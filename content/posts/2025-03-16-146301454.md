---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f64756e64756e6d6d2f:61727469636c652f64657461696c732f313436333031343534"
layout: post
title: "论文阅读Cross-View-Fusion-for-Multi-View-Clustering"
date: 2025-03-16 21:30:54 +08:00
description: "多视图聚类近年来备受关注，因其能够利用多视图的一致性与互补性信息提升聚类性能。然而，如何有效融合多视图信息并平衡其一致性与互补性，是多视图聚类面临的共性挑战。现有方法多聚焦于加权求和融合或拼接融合，但这些方式难以充分融合潜在信息，且未考虑多视图一致性与互补性的平衡。为此，本文提出一种跨视图融合多视图聚类方法（CFMVC）。具体而言，CFMVC结合深度神经网络与图卷积网络实现跨视图信息融合，充分融合多视图的特征信息与结构信息。为平衡多视图的一致性与互补性，CFMVC通过增强同类样本间的相关性以。"
keywords: "【论文阅读】Cross-View Fusion for Multi-View Clustering"
categories: ['论文阅读', '深度学习', '数据挖掘']
tags: ['论文阅读', '聚类', '深度聚类', '深度学习', '数据挖掘', '多视图聚类', '人工智能']
artid: "146301454"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146301454
    alt: "论文阅读Cross-View-Fusion-for-Multi-View-Clustering"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146301454
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146301454
cover: https://bing.ee123.net/img/rand?artid=146301454
image: https://bing.ee123.net/img/rand?artid=146301454
img: https://bing.ee123.net/img/rand?artid=146301454
---

# 【论文阅读】Cross-View Fusion for Multi-View Clustering

![](https://i-blog.csdnimg.cn/direct/268ac9130949460189ae643a5c4b5aa5.png)

论文地址：
[Cross-View Fusion for Multi-View Clustering | IEEE Journals & Magazine | IEEE Xplore](https://ieeexplore.ieee.org/document/10833824 "Cross-View Fusion for Multi-View Clustering | IEEE Journals & Magazine | IEEE Xplore")

---

## 摘要

**多视图聚类**
近年来备受关注，因其能够利用多视图的一致性与互补性信息提升聚类性能。然而，如何有效融合多视图信息并平衡其一致性与互补性，是多视图聚类面临的共性挑战。现有方法多聚焦于
**加权求和融合**
或
**拼接融合**
，但这些方式难以充分融合潜在信息，且未考虑多视图一致性与互补性的平衡。为此，本文提出一种
**跨视图融合多视图聚类方法（CFMVC）**
。
  
具体而言，CFMVC结合
**深度神经网络**
与
**图卷积网络**
实现跨视图信息融合，充分融合多视图的特征信息与结构信息。为平衡多视图的一致性与互补性，CFMVC通过增强同类样本间的相关性以
**最大化一致性信息**
，同时强化不同样本间的独立性以
**最大化互补性信息**
。在多个多视图数据集上的实验表明，CFMVC在多视图聚类任务中具有显著有效性。

## 引言

**多视图聚类**
（Multi-view Clustering, MVC）作为机器学习的新范式，旨在通过多视图联合学习提取有价值的语义信息[1]–[4]。传统MVC方法主要包括：

1. **协同训练方法**
   （如[5]–[7]），利用先验信息或视图间知识交互最大化视图一致性；
2. **多视图子空间聚类方法**
   （如[8]–[11]），从多子空间或潜在空间学习统一表征；
3. **多视图图聚类方法**
   （如[12]–[14]），学习跨视图的融合图结构。
     
   然而，传统方法存在
   **表征能力弱**
   、
   **计算复杂度高**
   的问题，导致聚类性能受限。

近年来，
**深度多视图聚类方法**
[15]–[20]凭借深度神经网络强大的特征表征与非线关系处理能力，可从多视图中学习高表达能力表征。例如：

* [23]设计自适应特征金字塔网络，实现空间位置与通道间的平衡融合；
* [24]提出高效图推理模块，保持特征多样性以学习判别性描述；
* [25]结合拉普拉斯正则化与多样性策略，学习一致且多样的深度潜在表征；
* [26]利用带拉普拉斯正则的自编码器构建单视图相似图并提出融合策略。

尽管现有深度MVC方法取得显著进展，仍面临以下挑战：

1. **融合策略局限**
   ：主流方法依赖
   **加权求和**
   [15][16]或
   **拼接融合**
   [19][27]，难以充分融合多视图底层信息并获取紧凑公共表征；
2. **信息平衡缺失**
   ：多数方法仅关注一致性或互补性最大化，未平衡二者关系[27][28]。

针对上述问题，本文提出
**跨视图融合多视图聚类方法（CFMVC）**
（见图1），其目标包括：

1. 融合多视图特征与结构信息以获取丰富语义；
2. 有效平衡多视图一致性与互补性。具体实现如下：

* **跨视图信息融合模块**
  ：结合深度神经网络与图卷积网络，逐层提取视图特征后跨视图传播特征及结构信息；
* **平衡特征融合模块**
  ：基于冗余缩减原理[29]，通过增强同类样本相关性
  **最大化一致性**
  ，同时强化异类样本独立性
  **最大化互补性**
  。

本文主要贡献包括：

1. 提出深度神经网络与图卷积网络结合的跨视图信息融合模块，充分融合多视图特征与结构信息；
2. 设计平衡特征融合模块，通过协调一致性与互补性获取紧凑且判别性强的公共表征；
3. 提出新型多视图融合策略，在多视图融合与信息平衡中表现优异。实验证明CFMVC在多视图聚类任务中具有显著有效性。

## 模型

![](https://i-blog.csdnimg.cn/direct/18b4552bcd6b4b4fa862ffd0122e30c2.png)

**所提出的CFMVC框架**
包含三个核心模块：
**跨视图信息融合**
、
**平衡特征融合**
与
**自训练聚类**
。其总体损失函数定义为：

𝐿=𝐿𝑟𝑒𝑐+𝜆1𝐿𝑏𝑓𝑓+𝜆2𝐿𝑐𝑙𝑢

其中：

* 𝐿𝑟𝑒𝑐为
  **重构损失**
  ，用于约束数据重建精度；
* 𝐿𝑏𝑓𝑓为
  **平衡特征融合模块的损失**
  ，用于协调多视图一致性与互补性信息；
* 𝐿𝑐𝑙𝑢为
  **聚类损失**
  ，优化聚类目标；
* 𝜆1​ 与 𝜆2为权衡参数，调节不同损失的贡献权重。

---

#### **A. 跨视图信息融合模块（CIF）**

本模块旨在通过融合多视图的
**特征信息**
与
**结构信息**
，生成富含语义的跨视图融合表征。具体流程如下：

##### **1. 结构信息提取**

* 基于原始数据 𝑋𝑚，采用
  **K近邻（KNN）方法**
  构建邻接矩阵 𝐴𝑚：

  + 计算样本间相似度
    ![](https://i-blog.csdnimg.cn/direct/446b347f33d94ad4ac7174cf9351543d.png)
    ，选择相似度最高的 𝑘 个样本作为邻居节点；
  + 构建KNN图并生成邻接矩阵 𝐴𝑚。

##### **2. 特征信息提取**

* 使用自编码器（Autoencoder）逐层提取视图特征：

  + 编码器第 𝑙 层特征表示为 𝐻(𝑚,𝑙)=𝐸𝑚(𝑋𝑚;𝜃𝑒𝑚)，捕获层级特异性信息；
  + 解码器重建数据 𝑋^𝑚=𝐷𝑚(𝐻(𝑚,𝑙);𝜃𝑑𝑚)，重构损失定义为：![](https://i-blog.csdnimg.cn/direct/7708c1c3a37e4fd38f84e10ecab8c8ca.png)

##### **3. 跨视图信息传递**

* **结构信息融合**
  ：融合双视图的邻接矩阵（含自连接 𝐴~𝑚=𝐴𝑚+𝐼）以增强全局结构表征：

  ![](https://i-blog.csdnimg.cn/direct/e0813713b8b7404ab579197139a2e76d.png)
* **特征信息融合**
  ：将自编码器第 𝑙 层特征 𝐻(𝑚,𝑙)与图卷积网络（GCN）的层级表示 𝑍(𝑙) 结合：

  ![](https://i-blog.csdnimg.cn/direct/ab0ff1756b8d4257b0bc728ea60ef42d.png)

  其中 𝛼为
  **传递算子**
  ，用于耦合自编码器与GCN。

##### **4. 层级传播与对称融合**

* 融合后的邻接矩阵 𝐴^与特征表示 𝑍~(𝑙) 输入至下一层GCN：

  ![](https://i-blog.csdnimg.cn/direct/b6b6aee659d84495a5ad8d998763db91.png)

  其中 𝜎为激活函数，𝐷~ 为度矩阵，𝑊 为可训练权重。
* **对称输出**
  ：以双视图互为输入进行对称融合，最终输出跨视图融合表征 𝑍1与 𝑍2。

#### **B. 平衡特征融合模块**

受文献[29]启发，本文扩展
**冗余缩减原理**
以平衡多视图的
**一致性**
与
**互补性**
信息。具体实现如下：

1. **样本相关性计算**
     
   计算跨视图融合表征 𝑍1​ 与 𝑍2 的
   **样本相关性矩阵**
   𝐶∈𝑅𝑛×𝑛：​​​​​​​
   ![](https://i-blog.csdnimg.cn/direct/d871e4d634214616ae75a8163f88fd3b.png)
   其中 𝐶𝑖𝑗表示 𝑍1​ 中第 𝑖 个样本与 𝑍2​ 中第 𝑗个样本的
   **余弦相似度**
   。
2. **平衡损失函数**
     
   通过优化目标使相关性矩阵 𝐶 逼近单位矩阵 𝐼：

   ![](https://i-blog.csdnimg.cn/direct/88b4fc9bda3240aebd81f93bdcaec98b.png)
   * **第一项**
     ：强制对角元素 𝐶𝑖𝑖→1，通过
     **最大化同类样本相似度**
     增强视图间一致性；
   * **第二项**
     ：强制非对角元素 𝐶𝑖𝑗→0，通过
     **最小化异类样本相似度**
     提升视图间互补性。
3. **公共表征生成**
     
   线性融合 𝑍1​ 与 𝑍2​ 得到平衡后的公共表征：

   ![](https://i-blog.csdnimg.cn/direct/f279849512434fbcabe830686a77adff.png)

---

#### **C. 自训练聚类模块**

为构建
**聚类友好空间**
，基于KL散度设计聚类损失函数：

1. **软分配概率**
     
   采用
   **学生t分布**
   度量样本 𝑧𝑖zi​ 与聚类中心 𝜇𝑗的相似性：

   ![](https://i-blog.csdnimg.cn/direct/e86e679bc7664fd296fcd2ad436a3e0b.png)
2. **目标分布优化**
     
   通过
   **高频增强策略**
   生成辅助目标分布 𝑝𝑖𝑗：

   ![](https://i-blog.csdnimg.cn/direct/01bfd666f21540e784eee66133732945.png)
3. **KL散度损失**
     
   通过最小化 𝑝𝑖𝑗 与 𝑞𝑖𝑗的KL散度优化聚类：

   ![](https://i-blog.csdnimg.cn/direct/c73ad3d68dab4232b45e84b9d641cf2f.png)

   此过程迫使样本向聚类中心紧致聚集，最终获得适合聚类的公共表征。

## 实验

![](https://i-blog.csdnimg.cn/direct/6dcff9afe65c47ab8bfe57d266b25626.png)

---

从跨视图的角度出发解决视图信息融合问题。