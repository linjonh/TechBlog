---
layout: post
title: "transformer-学习王木头学科学"
date: 2025-09-11T18:10:28+0800
description: "这个叫自注意力，也来源于Q,K的数据都是相同的，每一个词，都要与包括自己在内的所有其他词向量进行内积。我们得到的A矩阵到底是什么呢，相当于就是看Q和K的关系到底相关性（匹配度）大还是小，，A的值越大，则相关性越大，值越小，则相关性越小。为什么要用QK两个矩阵，而不直接训练矩阵A呢，这是因为直接弄的话，呈线性的，而两个矩阵的话，就是非线性了，它的表达能力会更强，让模型表达更复杂的情况。那为什么不用相同的矩阵，自己和自己的转置相乘，而非要用不同的矩阵相乘呢，最关键，从数学上来讲，他们的计算结果是等价的啊？"
keywords: "transformer 学习——王木头学科学"
categories: ['未分类']
tags: ['计算机视觉', '深度学习', '机器学习', '人工智能']
artid: "151580448"
arturl: "https://blog.csdn.net/m0_56487684/article/details/151580448"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151580448
    alt: "transformer-学习王木头学科学"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151580448
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151580448
cover: https://bing.ee123.net/img/rand?artid=151580448
image: https://bing.ee123.net/img/rand?artid=151580448
img: https://bing.ee123.net/img/rand?artid=151580448
---



# transformer 学习——王木头学科学

独热编码维度过高，全是靠维度之间的关系去体现语义关系的，没有充分利用空间的长度

而分词器，则是把语义变成长度关系，没有利用好空间维度。

![](https://i-blog.csdnimg.cn/direct/41ce7c31ff724dff9fe48ed6c64e43bd.png)

![](https://i-blog.csdnimg.cn/direct/291f1a7163864f31889ac39d19532325.png)

transformer除以D_out是从概率角度得出的  （根号下d_out 就是Q,K相乘后的标准差。）

相当于就是为了他们的相对关系进行归一  （这样让数值也在合理区间，避免softmax处理的时候出现梯度消失或梯度爆炸）

值得注意的是，这里对A矩阵进行softmax，是对每一行进行相加，然后softmax的，而不是全部A一起进行softmax。

![](https://i-blog.csdnimg.cn/direct/63edc70dff3646b68f8ffbcab2b2287a.png)

再次明确我们的目标。

词向量本身就有词意，而我们引入注意力，是为了找到词与词之间的关系，词与词之间的距离。让词之间的组合能更有意义，更有不同的词句上下文含义。让组合的词句更有主观性。

如：“美女，可以加一下微信吗” 和 “美女，麻烦让一让” 这里的美女后面相关的词，使得美女 该词的含义侧重有所不同了。 （即前一个美丽程度更加侧重，而后者就要弱很多）

我们得到的A矩阵到底是什么呢，相当于就是看Q和K的关系到底相关性（匹配度）大还是小，，A的值越大，则相关性越大，值越小，则相关性越小。可以看下面这个向量图理解下。

这个叫自注意力，也来源于Q,K的数据都是相同的，每一个词，都要与包括自己在内的所有其他词向量进行内积。  内积的结果，就是他们之间的关系到底是什么。如果是垂直的，就是无关，如果是负数就是负相关，如果是正的就是正相关。

![](https://i-blog.csdnimg.cn/direct/4f59690fdeb9438da7d09967e320ec4f.png)

为什么要用QK两个矩阵，而不直接训练矩阵A呢，这是因为直接弄的话，呈线性的，而两个矩阵的话，就是非线性了，它的表达能力会更强，让模型表达更复杂的情况。

![](https://i-blog.csdnimg.cn/direct/e7c97c9d49984fb2bd1172447f2b8787.png)

那为什么不用相同的矩阵，自己和自己的转置相乘，而非要用不同的矩阵相乘呢，最关键，从数学上来讲，他们的计算结果是等价的啊？

![](https://i-blog.csdnimg.cn/direct/bd2167853c0b47fabfcebff731cb40c6.png)

可以这样理解，其实Q,K分别可以代表两种语义，设定语义和表达语义，即一个设定的语义条件环境下，而另一个代表所表达出来的语义意思，两者相乘，得到更合适的权重矩阵。（当然Q和K代表的表达语义。

![](https://i-blog.csdnimg.cn/direct/d6fef209a1a74f1fa81fdb10a8bc50fd.png)

![](https://i-blog.csdnimg.cn/direct/56ec023f2f88441b93e930772908f1ee.png)

通过上图方式，解决seq2seq的问题（即 输入的token的数目和输出数目往往不相同的情况）

多头注意力为什么要分别计算再拼接，而不直接拼接好 直接运算得出呢？？

分块训练出来的内容是相似的，而整体处理的话，就不具有相似的含义了。

![](https://i-blog.csdnimg.cn/direct/1ce1e16861e64b18be9a863a9b1b9fc8.png)



