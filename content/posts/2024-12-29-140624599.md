---
layout: post
title: "SenseVoice-实测,阿里开源语音大模型,识别效果和效率优于-Whisper,居然还能检测掌声笑声5分钟带你部署体验"
date: 2024-12-29 22:23:47 +0800
description: "阿里开源语音大模型：语音识别效果和性能强于 Whisper，还能检测掌声、笑声、咳嗽等_sensev"
keywords: "sensevoicesmall开源"
categories: ['Ai']
tags: ['开源', 'Xcode', 'Whisper']
artid: "140624599"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=140624599
    alt: "SenseVoice-实测,阿里开源语音大模型,识别效果和效率优于-Whisper,居然还能检测掌声笑声5分钟带你部署体验"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=140624599
featuredImagePreview: https://bing.ee123.net/img/rand?artid=140624599
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     SenseVoice 实测，阿里开源语音大模型，识别效果和效率优于 Whisper，居然还能检测掌声、笑声！5分钟带你部署体验
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     前段时间，带着大家捏了一个对话机器人：
     <br/>
     <a href="https://blog.csdn.net/u010522887/article/details/139668478">
      手把手带你搭建一个语音对话机器人，5分钟定制个人AI小助手（新手入门篇）
     </a>
    </p>
    <p>
     其中语音识别（ASR）方案，采用的是阿里开源的 FunASR，这刚不久，阿里又开源了一个更强的音频基础模型，该模型具有如下能力：
    </p>
    <ul>
     <li>
      语音识别（ASR）
     </li>
     <li>
      语种识别（LID）
     </li>
     <li>
      语音情感识别（SER）
     </li>
     <li>
      声学事件分类（AEC）
     </li>
     <li>
      声学事件检测（AED）
     </li>
    </ul>
    <blockquote>
     <p>
      传送门：
      <a href="https://github.com/FunAudioLLM/SenseVoice">
       https://github.com/FunAudioLLM/SenseVoice
      </a>
     </p>
    </blockquote>
    <p>
     今天就带着大家体验一番~
    </p>
    <h2>
     <a id="0__15">
     </a>
     0. 项目简介
    </h2>
    <p>
     模型结构如下图所示：
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/2eb3e7e8eb5b193d245e89a595ed93a3.png"/>
    </p>
    <p>
     模型亮点：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        多语言语音识别
       </strong>
       ：
       <br/>
       经过超过40万小时的数据训练，支持50多种语言，其识别性能超越了Whisper模型。
      </p>
     </li>
     <li>
      <p>
       <strong>
        丰富的转录能力
       </strong>
       ：
       <br/>
       具备出色的情感识别能力，在测试数据上超越了当前最佳模型。
       <br/>
       提供声音事件检测能力，支持检测各种常见的人机交互事件，如背景音乐、掌声、笑声、哭泣、咳嗽和打喷嚏。
      </p>
     </li>
     <li>
      <p>
       <strong>
        高效推理
       </strong>
       ：
       <br/>
       SenseVoice-Small模型采用非自回归的端到端框架，具有极低的推理延迟。处理10秒音频仅需70毫秒，比Whisper-Large快15倍。
      </p>
     </li>
     <li>
      <p>
       <strong>
        便捷的微调
       </strong>
       ：
       <br/>
       提供便捷的微调脚本和策略，使用户能够根据业务场景轻松解决长尾样本问题。
      </p>
     </li>
    </ul>
    <h2>
     <a id="1__35">
     </a>
     1. 在线体验
    </h2>
    <blockquote>
     <p>
      在线体验地址：
      <a href="https://github.com/FunAudioLLM/SenseVoice">
       https://www.modelscope.cn/studios/iic/SenseVoice
      </a>
     </p>
    </blockquote>
    <p>
     语音识别：支持中、粤、英、日、韩语等 50 多种语言。
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/69ba122d669891407cd8ea6f6b8b2b5e.png"/>
    </p>
    <p>
     情感识别：比如积极 or 消极，以 Emoji 表情输出。
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/68014ba84e5996ce99876f52267e81f8.png"/>
    </p>
    <p>
     音频事件检测：同样以 Emoji 表情输出。
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/b467769bec1f775e707a5dbc417594fb.png"/>
    </p>
    <h2>
     <a id="2__53">
     </a>
     2. 本地部署
    </h2>
    <h3>
     <a id="21____54">
     </a>
     2.1 安装 &amp; 测试
    </h3>
    <p>
     首先 git 下载到本地，然后安装必要的包：
    </p>
    <pre><code>git clone https://github.com/FunAudioLLM/SenseVoice.git
pip install -r requirements.txt
</code></pre>
    <p>
     注意：
    </p>
    <ul>
     <li>
      本项目依赖的
      <code>
       funasr
      </code>
      版本要
      <code>
       &gt;=1.1.2
      </code>
      ，这个和
      <code>
       funasr
      </code>
      语音识别模型的版本是不匹配的，如果要同时使用这两个模型，会出现版本冲突，所以最好采用 conda 管理 python 环境。
     </li>
     <li>
      本项目依赖的
      <code>
       torchaudio
      </code>
      需要更新到最新版本，否则会出现报错。
     </li>
    </ul>
    <p>
     接下来，我们采用官方脚本进行测试：
    </p>
    <pre><code>from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess

model_dir = "iic/SenseVoiceSmall"
model = AutoModel(
    model=model_dir,
    trust_remote_code=True,
    remote_code="./model.py",
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

res = model.generate(
    input=f"{model.model_path}/example/en.mp3",
    cache={},
    language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
    use_itn=True,
    batch_size_s=60,
    merge_vad=True,  #
    merge_length_s=15,
)
text = rich_transcription_postprocess(res[0]["text"])
print(text)
</code></pre>
    <p>
     首次使用，会下载模型，默认保存在你的根目录下：
     <code>
      ~/.cache/modelscope/
     </code>
     。
    </p>
    <h3>
     <a id="22_FastAPI__99">
     </a>
     2.2 FastAPI 部署
    </h3>
    <p>
     测试成功后，我们采用 FastAPI 把模型部署成一个服务，方便提供给其他应用调用。
    </p>
    <h4>
     <a id="221__102">
     </a>
     2.2.1 服务端
    </h4>
    <p>
     首先准备好服务端代码
     <code>
      speech_server.py
     </code>
     ：
    </p>
    <pre><code>import torch
import base64
import uvicorn
from fastapi import FastAPI
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess
from pydantic import BaseModel

# asr model
model = AutoModel(
    model="iic/SenseVoiceSmall",
    trust_remote_code=True,
    remote_code="./model.py",
    vad_model="fsmn-vad",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
)

# 定义asr数据模型，用于接收POST请求中的数据
class ASRItem(BaseModel):
    wav : str # 输入音频

app = FastAPI()
@app.post("/asr")
async def asr(item: ASRItem):
    try:
        data = base64.b64decode(item.wav)
        with open("test.wav", "wb") as f:
            f.write(data)
        res = model.generate("test.wav", 
                            language="auto",  # "zn", "en", "yue", "ja", "ko", "nospeech"
                            use_itn=True,
                            batch_size_s=60,
                            merge_vad=True,  #
                            merge_length_s=15,)
        text = rich_transcription_postprocess(res[0]["text"])
        result_dict = {"code": 0, "msg": "ok", "res": text}
    except Exception as e:
        result_dict = {"code": 1, "msg": str(e)}
    return result_dict

if __name__ == '__main__':
    uvicorn.run(app, host='0.0.0.0', port=2002)

</code></pre>
    <h4>
     <a id="222__151">
     </a>
     2.2.2 服务启动
    </h4>
    <pre><code>CUDA_VISIBLE_DEVICES=0 python speech_server.py &gt; log.txt 2&gt;&amp;1 &amp;
</code></pre>
    <p>
     服务成功启动，可以发现显存只占用 1202 M，比上一篇的
     <code>
      FunASR
     </code>
     更轻量~
    </p>
    <pre><code>+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    2   N/A  N/A   3178377      C   python                                       1202MiB |
+-----------------------------------------------------------------------------------------+
</code></pre>
    <h4>
     <a id="223__167">
     </a>
     2.2.3 客户端
    </h4>
    <p>
     最后，我们来编写客户端代码：
    </p>
    <pre><code>import base64
import requests

url = "http://10.18.32.170:2002/"

def asr_damo_api(wav_path):
    headers = {'Content-Type': 'application/json'}
    with open(wav_path, "rb") as f:
        wav = base64.b64encode(f.read()).decode()
    data = {"wav": wav}
    response = requests.post(url+"asr", headers=headers, json=data)
    response = response.json()
    if response['code'] == 0:
        res = response['res']
        return res
    else:
        return response['msg']

if __name__ == '__main__':
    res = asr_damo_api("xxx/.cache/modelscope/hub/iic/SenseVoiceSmall/example/en.mp3")
    print(res)
</code></pre>
    <h2>
     <a id="_193">
     </a>
     写在最后
    </h2>
    <p>
     本文通过对 SenseVoice 模型的实操，带领大家快速上手语音识别模型。
    </p>
    <p>
     希望能激发你的更多创作灵感，打造自己的 AI 助手。
    </p>
    <p>
     如果你对本项目感兴趣，欢迎
     <strong>
      点赞收藏
     </strong>
     并分享给更多朋友！
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f753031303532323838372f:61727469636c652f64657461696c732f313430363234353939" class_="artid" style="display:none">
 </p>
</div>


