---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35323538323731302f:61727469636c652f64657461696c732f313231353133353738"
layout: post
title: "bert模型笔记"
date: 2025-03-06 23:51:40 +08:00
description: "1.各预训练模型说明BERT模型在英文数据集上提供了两种大小的模型，Base和Large。Uncased是意味着输入的词都会转变成小写，cased是意味着输入的词会保存其大写（在命名实体识别等项目上需要）。Multilingual是支持多语言的，最后一个是中文预训练模型。在这里我们选择BERT-Base，Uncased。下载下来之后是一个zip文件，解压后有ckpt文件，一个模型参数的json文件，一个词汇表txt文件。..."
keywords: "bert模型笔记"
categories: ['未分类']
tags: ['笔记', '人工智能', 'Bert']
artid: "121513578"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=121513578
    alt: "bert模型笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=121513578
featuredImagePreview: https://bing.ee123.net/img/rand?artid=121513578
cover: https://bing.ee123.net/img/rand?artid=121513578
image: https://bing.ee123.net/img/rand?artid=121513578
img: https://bing.ee123.net/img/rand?artid=121513578
---

# bert模型笔记

## 1.各预训练模型说明

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/6ba337cec60b72faee277e5823305141.png)
  
BERT模型在英文数据集上提供了两种大小的模型，Base和Large。Uncased是意味着输入的词都会转变成小写，cased是意味着输入的词会保存其大写（在命名实体识别等项目上需要）。Multilingual是支持多语言的，最后一个是中文预训练模型。

```
在这里我们选择BERT-Base，Uncased。下载下来之后是一个zip文件，解压后有ckpt文件，一个模型参数的json文件，一个词汇表txt文件。

```

## 2.参数错误

```
当输出出现 args = parser.parse_args()标红时，将 args = parser.parse_args() 替换为：args, unknown = parser.parse_known_args()

```

## 3.命令行转换模型（tf到pytorch）chinese\_L-12\_H-768\_A-12

```
安装：pip install pytorch-pretrained-bert
解压地址：export BERT_BASE_DIR=/path/to/bert/chinese_L-12_H-768_A-12

```

（地址拼接）转换模型：pytorch\_pretrained\_bert convert\_tf\_checkpoint\_to\_pytorch $BERT\_BASE\_DIR/bert\_model.ckpt $BERT\_BASE\_DIR/bert\_config.json $BERT\_BASE\_DIR/pytorch\_model.bin

## 4.bert模型的标签

```
标签默认为0，1，2...n的方式标注，否则需转换。

```