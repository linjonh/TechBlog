---
layout: post
title: "基于PyTorch的深度学习3非标量反向传播"
date: 2025-03-09 14:34:17 +0800
description: "以下内容由大语言模型生成：张量，边代表从输入到输出的操作。当我们对一个标量调用backward()函数时，PyTorch能够直接计算该标量关于参与运算的所有张量的梯度。这是因为导数的概念最开始是针对标量值定义的。"
keywords: "基于PyTorch的深度学习3——非标量反向传播"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Pytorch']
artid: "146131243"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146131243
    alt: "基于PyTorch的深度学习3非标量反向传播"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146131243
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146131243
cover: https://bing.ee123.net/img/rand?artid=146131243
image: https://bing.ee123.net/img/rand?artid=146131243
img: https://bing.ee123.net/img/rand?artid=146131243
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     基于PyTorch的深度学习3——非标量反向传播
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     以下内容由大语言模型生成：
    </p>
    <p>
     张量，边代表从输入到输出的操作。当我们对一个标量调用
     <code>
      backward()
     </code>
     函数时，PyTorch能够直接计算该标量关于参与运算的所有张量的梯度。这是因为导数的概念最开始是针对标量值定义的。
    </p>
    <h4>
     为什么需要传入gradient参数？
    </h4>
    <p>
     当你有一个非标量（即形状不为1的张量）并希望对其求导时，实际上你是在尝试计算这个张量关于某些变量的雅可比矩阵（Jacobian Matrix）。然而，PyTorch并不直接支持这种操作，因为它复杂化了自动微分的流程。为了简化这个问题，PyTorch要求在这种情况下提供一个额外的
     <code>
      gradient
     </code>
     参数，这个参数同样是一个张量，并且其形状必须与调用
     <code>
      backward()
     </code>
     的那个张量相同。这个
     <code>
      gradient
     </code>
     实际上扮演了一个“权重”的角色，用于将多维导数的问题转化为标量导数的问题。
    </p>
    <h4>
     如何理解这个过程？
    </h4>
    <p>
     考虑你的例子：假设你有一个损失向量
     <code>
      loss=(y_1, y_2, ..., y_m)
     </code>
     ，你想要根据它来更新一些模型参数。由于
     <code>
      loss
     </code>
     不是一个标量，你不能直接对它调用
     <code>
      backward()
     </code>
     。这时，你可以引入一个向量
     <code>
      v=(v_1, v_2, ..., v_m)
     </code>
     ，然后将
     <code>
      loss
     </code>
     与
     <code>
      v
     </code>
     进行点乘得到一个新的标量
     <code>
      loss*v^T
     </code>
     （这里
     <code>
      v^T
     </code>
     表示
     <code>
      v
     </code>
     的转置，虽然在实际代码中我们不会这样写，这只是为了表达数学概念）。这个新生成的标量可以被用来调用
     <code>
      backward()
     </code>
     方法，从而触发梯度的计算。
    </p>
    <p>
     具体来说，这样做实际上是计算了
     <code>
      loss
     </code>
     的雅可比矩阵与
     <code>
      v
     </code>
     的乘积。换句话说，原本你需要计算的是雅可比矩阵，但现在通过点乘转换后，你只需计算一个标量关于所需变量的梯度。这使得PyTorch的自动微分机制能够处理这种情况，而不需要直接支持张量对张量的求导。
    </p>
    <p>
     1)定义叶子节点及计算节点。
    </p>
    <pre><code class="language-python">import torch

#定义叶节点张量x，形状为1x2
x=torch.tensor([[2.3]],dtype=torch.float,requires_grad=True)

#初始化Jacobian矩阵
J=torch.zeros(2,2)

#初始化目标张量，形状为1x2
y=torch.zeros(1,2)

#定义y与x之间的映射关系：
#y1=x1**2+3*x2,y2=x2**2+2*x1
y[0,0]=x[0,0]**2+3*x[0,1]
y[0,1]=x[0,1]**2+2*x[0,0]

</code></pre>
    <p>
     2)手工计算y对x的梯度
    </p>
    <p>
     y对x的梯度是一个雅可比矩阵，可以通过手动计算值
    </p>
    <pre><code class="language-python">#生成y1对x的梯度
y.backward(torch.Tensor([[1, 0]]),retain_graph=True)
##gradient的作用：传入的gradient张量扮演了一个权重的角色，它决定了每个元素在最终梯度计算中的重要
##本质上，这是将雅可比矩阵乘以这个gradient向量，从而将多维导数的问题简化为一维标量导数的问题。

J[0]=x.grad

#梯度是累加的，故需要对x的梯度清零
x.grad = torch.zeros_like(x.grad)

#生成y2对x的梯度
y.backward(torch.Tensor([[0, 1]]))

J[1]=x.grad
#显示jacobian矩阵的值
print(J)</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="687474:70733a2f2f626c6f672e6373646e2e6e65742f57697334652f:61727469636c652f64657461696c732f313436313331323433" class_="artid" style="display:none">
 </p>
</div>


