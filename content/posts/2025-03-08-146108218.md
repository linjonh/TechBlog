---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34323338303731312f:61727469636c652f64657461696c732f313436313038323138"
layout: post
title: "MuMu-LLaMA通过大型语言模型进行多模态音乐理解和生成Python代码实现论文"
date: 2025-03-08 00:24:41 +08:00
description: "文本到音乐生成（T2M-Gen）面临的主要障碍是缺乏带有自然语言描述的大规模公开音乐数据集。为此，我们提出音乐理解大语言模型（MU-LLaMA）​，该模型能够回答音乐相关问题并为音乐文件生成字幕。我们的模型使用预训练的MERT模型提取音频表征作为音乐特征。然而，获取适合训练MU-LLaMA模型的数据集仍具挑战，因为现有公开音频问答数据集缺乏开放式音乐问答所需的深度。为填补这一空白，我们提出从现有音频字幕数据集生成问答对的方法，并推出专为回答开放式音乐问题设计的MusicQA数据集。"
keywords: "MuMu-LLaMA：通过大型语言模型进行多模态音乐理解和生成（Python代码实现+论文）"
categories: ['未分类']
tags: ['语言模型', '人工智能', 'Llama']
artid: "146108218"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146108218
    alt: "MuMu-LLaMA通过大型语言模型进行多模态音乐理解和生成Python代码实现论文"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146108218
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146108218
cover: https://bing.ee123.net/img/rand?artid=146108218
image: https://bing.ee123.net/img/rand?artid=146108218
img: https://bing.ee123.net/img/rand?artid=146108218
---

# MuMu-LLaMA：通过大型语言模型进行多模态音乐理解和生成（Python代码实现+论文）

> MuMu-LLaMA 模型是一种音乐理解和生成模型，能够进行音乐问答以及从文本、图像、视频和音频生成音乐，以及音乐编辑。该模型利用了用于音乐理解的 MERT、用于图像理解的 ViT 和用于视频理解的 ViViT 等编码器，以及作为音乐生成模型（音乐解码器）的 MusicGen/AudioLDM2 模型，再加上适配器和 LLaMA 2 模型，使模型具有多种能力。模型架构以
> [mumu\_llama.py](https://github.com/shansongliu/MuMu-LLaMA/blob/main/llama/mumu_llama.py "mumu_llama.py")
> 给出。

由于缺乏带有自然语言字幕的大规模公开可用的音乐数据集，文本到音乐生成 （T2M-Gen） 面临重大障碍。为了解决这个问题，我们提出了音乐理解 LLaMA （MU-LLaMA），它能够回答与音乐相关的问题并为音乐文件生成字幕。我们的模型利用来自预训练 MERT 模型的音频表示来提取音乐特征。然而，获得一个合适的数据集来训练 MU-LLaMA 模型仍然具有挑战性，因为现有的可公开访问的音频问答数据集缺乏开放式音乐问答的必要深度。为了填补这一空白，我们提出了一种从现有音频字幕数据集生成问答对的方法，并介绍了专为回答开放式音乐相关问题而设计的 MusicQA 数据集。实验表明，在我们设计的 MusicQA 数据集上训练的所提出的 MU-LLaMA 模型在各种指标上都取得了出色的音乐问答和音乐字幕生成方面的出色表现，在这两个领域的表现都优于当前最先进的 （SOTA） 模型，并在 T2M-Gen 研究领域提供了有希望的进步。

### 🤖 模型设置

我们在这个项目中使用 Python 3.9.17，库要求在 requirements.txt 中给出。使用 创建 conda 环境

```
conda create --name <env> --file requirements.txt

```

确保 NVIDIA 驱动程序为 12 或更高版本，以便与 PyTorch 2.1.0 兼容。

为了使我们的模型工作，需要 Facebook 的 LLaMA-2 模型权重，有关获取这些权重的详细信息，请访问
[HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/llama "HuggingFace")
。

我们模型的训练检查点可在此处获得：

* [MuMu-LlaMA 与 MusicGen Small](https://huggingface.co/M2UGen/M2UGen-MusicGen-small "MuMu-LlaMA 与 MusicGen Small")
* [含 MusicGen 培养基的 MuMu-LlaMA](https://huggingface.co/M2UGen/M2UGen-MusicGen-medium "含 MusicGen 培养基的 MuMu-LlaMA")
* [MuMu-LlaMA 与 AudioLDM2](https://huggingface.co/M2UGen/M2UGen-AudioLDM2 "MuMu-LlaMA 与 AudioLDM2")

可以在此处找到所需的预训练多模态编码器和音乐解码器模型：

* [MERT](https://huggingface.co/m-a-p/MERT-v1-330M "MERT")
* [ViT](https://huggingface.co/google/vit-base-patch16-224-in21k "ViT")
* [ViViT](https://huggingface.co/google/vivit-b-16x2-kinetics400 "ViViT")
* [MusicGen](https://huggingface.co/facebook/musicgen-medium "MusicGen")
* [AudioLDM 2](https://huggingface.co/cvssp/audioldm2-music "AudioLDM 2")

### 🧰 系统硬件要求

对于训练，第 1 阶段和第 2 阶段使用单个 32GB V100 GPU，而第 3 阶段使用 2 个 32GB V100 GPU。对于推理，使用单个 32GB V100 GPU。要加载模型检查点，大约需要 49GB 的 CPU 内存。

#### 摘要

文本到音乐生成（T2M-Gen）面临的主要障碍是缺乏带有自然语言描述的大规模公开音乐数据集。为此，我们提出
**音乐理解大语言模型（MU-LLaMA）​**
，该模型能够回答音乐相关问题并为音乐文件生成字幕。我们的模型使用预训练的MERT模型提取音频表征作为音乐特征。然而，获取适合训练MU-LLaMA模型的数据集仍具挑战，因为现有公开音频问答数据集缺乏开放式音乐问答所需的深度。为填补这一空白，我们提出从现有音频字幕数据集生成问答对的方法，并推出专为回答开放式音乐问题设计的
**MusicQA数据集**
。实验表明，基于MusicQA数据集训练的MU-LLaMA模型在音乐问答和字幕生成任务中均取得卓越性能，在多项指标上超越当前最优模型（SOTA），为T2M-Gen研究提供了重要突破。

**关键词**
：MU-LLaMA、MusicQA数据集、音乐问答、文本到音乐生成

---

#### 1. 引言

音乐生成任务需要大量带字幕的音乐数据支持。然而，由于版权限制，大多数含字幕的音乐数据集未公开[1-3]。目前最大的公开数据集MusicCaps[4]仅包含约28.52小时的音乐及字幕，远小于音频分类或标签任务的数据规模。因此，迫切需要开发大规模生成公开文本-音乐配对数据的方法。

Song Describer¹等少数尝试通过众包收集文本-音乐对，但其耗时且不可控，难以满足T2M-Gen研究的大数据需求。为此，我们提出利用大语言模型（LLM）为公开音乐资源自动生成字幕。

现有音乐字幕生成模型包括MusCaps[5]、音频字幕Transformer[6]、基于音频-文本检索预训练的模型[7]、Whisper微型音频字幕²和LP-MusicCaps[8]。其中，MusCaps采用卷积网络与循环神经网络的混合架构，LP-MusicCaps使用跨模态编码器-解码器架构。另一种思路是利用音频问答模型生成音乐字幕。

近期，LLaMA Adapter[9]、UniVAL[10]和LTU[11]等支持音频理解的模型相继出现。然而，这些模型缺乏音乐文本数据训练，或依赖非音乐音频数据，限制了其在音乐领域的表现。

本文提出一种创新方法，通过构建基于LLaMA的
**MU-LLaMA模型**
解决音乐问答与字幕生成问题。核心贡献包括：

1. 提出性能卓越的MU-LLaMA模型；
2. 设计MusicQA数据集构建方法；
3. 展示MU-LLaMA生成多样化字幕的能力，支持T2M-Gen模型开发。

论文结构如下：第2章对比音乐特征表示方法；第3章介绍MusicQA数据集生成方法；第4章详解MU-LLaMA架构；第5章展示实验结果；第6章总结与展望。

---

#### 2. 音乐特征表示

为实现音乐理解，我们对比了以下预训练音频表征模型在MagnaTagATune数据集[15]上的音乐标签任务性能：

1. ​
   **ImageBind**
   [12]：跨6模态的联合嵌入模型；
2. ​
   **JukeBox**
   [17]：基于VQ-VAE的自回归Transformer；
3. ​
   **OpenL3**
   [18]：基于音频-视觉对应的无监督嵌入；
4. ​
   **CLAP**
   [19]：对比语言-音频预训练模型；
5. ​
   **Wav2CLIP**
   [20]：CLIP模型的音频蒸馏版本；
6. ​
   **MERT**
   [22]：通过自监督训练实现SOTA音乐理解的声学模型。

表1显示，MERT在音乐标签任务中的平均精度（59.57%）显著优于其他模型，故选择其作为MU-LLaMA的音乐编码器。

**表1：音频表征模型对比**

| 模型 | MTTAUC | MTTAP |
| --- | --- | --- |
| ImageBind | 88.55% | 40.19% |
| JukeBox | 91.5% | 41.4% |
| ​ **MERT** | 93.91% | 59.57% |

---

#### 3. MusicQA数据集生成

为训练MU-LLaMA，我们利用MPT-7B模型[16]从MusicCaps（含描述）和MagnaTagATune（含标签）生成问答对：

1. ​
   **指令集1**
   ：根据描述/标签生成答案（如“描述这段音乐”“从音频中能推断什么”）；
2. ​
   **指令集2**
   ：生成5个开放式问答对（如“音频的情绪如何”“使用了哪些乐器”）。
     
   生成示例如下（完整指令与样例见项目页面⁴）。

---

#### 4. MU-LLaMA模型架构

如图1所示，MU-LLaMA基于LLaMA-2 7B模型[14]，集成MERT编码器提取音乐特征：

1. ​
   **特征提取**
   ：MERT的24个隐藏层与1个输出层（共25×1024维）经卷积层聚合为1024维向量；
2. ​
   **投影与适配器**
   ：投影至4096维后，通过含3个子块的稠密网络（公式1）处理，输出注入LLaMA最后19层的注意力查询中；
3. ​
   **训练策略**
   ：冻结MERT与LLaMA参数，仅微调解码器适配器。

**公式1**
：适配器前向计算
  
![](https://i-blog.csdnimg.cn/direct/6b92e77317e3406c861c6e883aa545e3.png)

---

#### 5. 实验与结果

##### 5.1 实验设置

* ​
  **数据集**
  ：从MTG-Jamendo[25]选取500首音乐生成4500 QA对（MTG-eval-QA）和1000字幕（MTG-eval-Cap）；
* ​
  **评估指标**
  ：BLEU、METEOR、ROUGE-L、BERTScore；
* ​
  **训练参数**
  ：学习率0.0001，批次大小1，预训练150轮，微调20轮。

##### 5.2 音乐问答性能

表2显示，MU-LLaMA在METEOR（0.385）和ROUGE-L（0.466）上显著优于LLaMA Adapter与LTU。

**表2：音乐问答模型对比**

| 模型 | BLEU | METEOR | ROUGE-L | BERTScore |
| --- | --- | --- | --- | --- |
| LTU | 0.242 | 0.274 | 0.326 | 0.887 |
| ​ **MU-LLaMA** | 0.306 | 0.385 | 0.466 | 0.901 |

##### 5.3 音乐字幕生成

表3显示，MU-LLaMA在BLEU（0.278）和ROUGE-L（0.312）上超越LP-MusicCaps等专用模型。

**表3：音乐字幕模型对比**

| 模型 | BLEU | METEOR | ROUGE-L | BERTScore |
| --- | --- | --- | --- | --- |
| LP-MusicCaps | 0.227 | 0.177 | 0.227 | 0.877 |
| ​ **MU-LLaMA** | 0.278 | 0.261 | 0.312 | 0.888 |

---

#### 6. 结论

本文提出的MU-LLaMA模型在音乐问答与字幕生成任务中均实现SOTA性能。未来工作将扩展至歌词理解，并利用该模型构建T2M-Gen所需的大规模数据集。

![](https://i-blog.csdnimg.cn/direct/46cebee1d4674ee786525eae8a392833.png)

图 1：音乐理解大语言模型（MU-LLaMA）。该模型分为 3 个部分：（1）预训练的 MERT 编码器，用于生成音乐表示；（2）音乐理解适配器，用于将该表示融合到 LLaMA 模型中；（3）LLaMA 模型，它从适配器获取输入，在最后\((L - 1)\)层中学习音乐上下文信息。

![](https://i-blog.csdnimg.cn/direct/2c03fef327864ad09ffe6e6c24615785.png)

### 介绍

MU-LLaMA 模型是音乐理解语言模型，旨在回答基于音乐的问题。我们的模型还旨在为音乐文件添加字幕以生成文本到音乐生成数据集。该模型使用 MERT + LLaMA 作为主干，并采用适配器来连接音乐上下文信息以指导 LLaMA 的输出。在比较了不同的音乐表示模型后，MERT 被选为我们模型的音乐编码器，可以
[在这里](https://github.com/crypto-code/Music-Representation-Comparison "在这里")
查看。我们还提供了用于从
[MusicCaps](https://www.kaggle.com/datasets/googleai/musiccaps "MusicCaps")
和
[MagnaTagATune](https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset "MagnaTagATune")
数据集生成 MusicQA 数据集的代码。

[![](https://img-home.csdnimg.cn/images/20230724024159.png?origin_url=https%3A%2F%2Fgithub.com%2Fshansongliu%2FMU-LLaMA%2Fraw%2FLLaMA-2%2Fassets%2FMU-LLaMA.png&pos_id=AQWpHxvE)](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/assets/MU-LLaMA.png)

### MU-LLaMA 演示

为了使我们的模型工作，需要 Facebook 的 LLaMA-2 模型权重，有关获取这些权重的详细信息，请访问
[HuggingFace](https://huggingface.co/docs/transformers/main/model_doc/llama "HuggingFace")
。我们针对 MU-LLaMA 模型的预训练权重，从
**LLaMA 7B-2**
微调，可以
[在此处](https://huggingface.co/mu-llama/MU-LLaMA/tree/main "在此处")
下载。下载后，将文件存储在 MU-LLaMA 目录下的 ckpts 文件夹中。

下载后，目录结构将如下所示。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>.
├── ...
├── MU-LLaMA                
│   ├── ckpts
│   │   │── LLaMA
│   │   │   │── 7B
│   │   │   │   │── checklist.chk
│   │   │   │   │── consolidated.00.pth
│   │   │   │   │── params.json
│   │   │   │── llama.sh
│   │   │   │── tokenizer.model
│   │   │   │── tokenizer_checklist.chk
│   │   │── 7B.pth
│   │   ├── checkpoint.pth
└── ...
</code></span></span></span></span>
```

我们在这个项目中使用 Python 3.9.17，库要求在
[requirements.txt](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/requirements.txt "requirements.txt")
中给出。可以使用
[gradio\_app.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MU-LLaMA/gradio_app.py "gradio_app.py")
运行演示。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>python gradio_app.py --model ./ckpts/checkpoint.pth --llama_dir ./ckpts/LLaMA
</code></span></span></span></span>
```

### 训练 MU-LLaMA

要训练 MU-LLaMA 模型，请按照以下步骤作。

#### MusicQA 数据集

我们使用
[MusicCaps](https://www.kaggle.com/datasets/googleai/musiccaps "MusicCaps")
和
[MagnaTagATune](https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset "MagnaTagATune")
数据集来生成我们的训练 MusicQA 数据集和
[MTG-Jamendo](https://github.com/MTG/mtg-jamendo-dataset "MTG-Jamendo")
进行评估。您可以
[在此处](https://huggingface.co/datasets/mu-llama/MusicQA "在此处")
下载生成的 MusicQA 数据集。

要自己生成数据集，请先下载 MusicCaps、MTT 和 MTG 数据集。下载后，目录结构将如图所示。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>.
├── ...
├── MusicQA                
│   ├── MTT
│   │   ├── audios
│   │   │   │── ...
│   │   ├── annotations_final.csv
│   ├── MusicCaps
│   │   ├── audios
│   │   │   │── ...
│   │   ├── musiccaps-public.csv
│   ├── MTG
│   │   ├── audios
│   │   │   │── 00
│   │   │   │── 01
│   │   │   │── ...
│   │   ├── raw_30s_cleantags_50artists.tsv
│   ├── MTT_process.py
│   ├── musiccaps_process.py
│   ├── MTG_process.py
│   ├── generate_dataset.py
└── ...
</code></span></span></span></span>
```

MusicQA 数据集生成是一个计算量非常大的过程，在 Tesla V100-SXM2-32GB GPU 上，每个数据集大约需要 8 天，因此建议下载我们生成的数据集。

> 📝
> **注意**
> ： 运行以下命令，将 MTT 音频文件结构压缩并提取后，

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>find ./MTT/audios -mindepth 2 -type f -exec mv -t ./MTT/audios -i '{}' +
</code></span></span></span></span>
```

> 我们只使用 MTG 数据集中的文件夹 00 到 09

通过运行
[musiccaps\_process.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MusicQA/musiccaps_process.py "musiccaps_process.py")
、
[MTT\_process.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MusicQA/MTT_process.py "MTT_process.py")
和
[MTG\_process.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MusicQA/MTG_process.py "MTG_process.py")
，您可以从每个数据集生成问答对，并通过运行
[generate\_dataset.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MusicQA/generate_dataset.py "generate_dataset.py")
生成用于预训练、微调和评估的最终数据集。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>usage: generate_dataset.py [-h] --mtt MTT --mtg MTG --musiccaps MUSICCAPS --musicqa MUSICQA

optional arguments:
  -h, --help            show this help message and exit
  --mtt MTT             Directory of the MTT dataset
  --mtg MTG             Directory of the MTG dataset
  --musiccaps MUSICCAPS
                        Directory of the MusicCaps dataset
  --musicqa MUSICQA     Directory of the MusicQA dataset to be generated
</code></span></span></span></span>
```

#### MU-LLaMA 预训练

为了预训练 MU-LLaMA 模型，我们使用 MusicQA 数据集的 MusicCaps 部分和
[Alpaca](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MU-LLaMA/pretrain.sh "Alpaca")
Instruction 数据集以及 pretrain.sh 脚本。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>./pretrain.sh ./ckpts/LLaMA-2 ./configs/pretrain.yaml ./ckpts/MU-LLaMA_Pretrain
</code></span></span></span></span>
```

这将对 MU-LLaMA 模型进行 150 个 epoch 的预训练。可以在
[pretrain.sh](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MU-LLaMA/pretrain.sh "pretrain.sh")
文件中修改超参数。

#### MU-LLaMA 微调

为了微调 MU-LLaMA 模型，我们将 MusicQA 数据集的 MTT 部分与
[finetune.sh](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MU-LLaMA/finetune.sh "finetune.sh")
脚本结合使用。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>./finetune.sh ./ckpts/LLaMA-2 ./ckpts/MU-LLaMA_Pretrain/checkpoint.pth ./configs/finetune.yaml ./ckpts/MU-LLaMA_Finetune
</code></span></span></span></span>
```

这将对 MU-LLaMA 模型进行 20 个 epoch 的微调。可以在
[finetune.sh](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MU-LLaMA/finetune.sh "finetune.sh")
文件中修改超参数。具有 7B 参数的 MU-LLaMA 模型在 Tesla V100-SXM2-32GB GPU 上训练大约需要 2 天时间。训练后，可以使用 Gradio 演示对模型进行测试。

#### MU-LLaMA 推理

要在没有 Gradio 的情况下测试模型，可以使用
[inference.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/MU-LLaMA/inference.py "inference.py")
脚本。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>usage: inference.py [-h] [--model MODEL] [--llama_type LLAMA_TYPE] [--llama_dir LLAMA_DIR] [--mert_path MERT_PATH] --audio_path AUDIO_PATH [--question QUESTION]

optional arguments:
  -h, --help            show this help message and exit
  --model MODEL         Name of or path to the trained checkpoint
  --llama_type LLAMA_TYPE
                        Type of llama original weight
  --llama_dir LLAMA_DIR
                        Path to LLaMA pretrained checkpoint
  --mert_path MERT_PATH
                        Path to MERT pretrained checkpoint
  --audio_path AUDIO_PATH
                        Path to the input music file
  --question QUESTION   Question to ask the model
</code></span></span></span></span>
```

#### MU-LLaMA 评估

我们的模型与支持音频的模型进行了比较，例如在我们的 MusicQA 数据集上训练的 Listen， Think and Understand （LTU） 模型和 LLaMA Adapter 模型。我们使用 BLEU （B-U）、METEOR （M-R）、ROUGE 评估模型L（R-L） 和 BERT-Score （BERT-S） 是文本生成的常用评估指标。对于 BLEU 分数，加权平均值为 BLEU1、 BLEU2、 BLEU3和 BLEU4（每个 weight = 0.25）。

评估脚本在 ModelEvaluations 文件夹中给出。生成脚本用于生成数据集中所有问题的答案。

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>usage: generate_mullama.py [-h] [--model MODEL] [--knn KNN] [--llama_type LLAMA_TYPE] [--llama_dir LLAMA_DIR] [--mert_path MERT_PATH]

optional arguments:
  -h, --help            show this help message and exit
  --model MODEL         Name of or path to the trained checkpoint
  --knn KNN             Name of or path to the directory with knn checkpoint
  --llama_type LLAMA_TYPE
                        Type of llama original weight
  --llama_dir LLAMA_DIR
                        Path to LLaMA pretrained checkpoint
  --mert_path MERT_PATH
                        Path to MERT pretrained checkpoint
</code></span></span></span></span>
```

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>usage: generate_ltu.py [-h] [--demo DEMO]

optional arguments:
  -h, --help   show this help message and exit
  --demo DEMO  Link to the LTU Demo Page
</code></span></span></span></span>
```

```notranslate
<span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><span style="color:#1f2328"><span style="color:var(--fgColor-default, var(--color-fg-default))"><span style="background-color:var(--bgColor-muted, var(--color-canvas-subtle))"><code>usage: generate_llama-adapter.py [-h] [--model MODEL] [--llama_dir LLAMA_DIR]

optional arguments:
  -h, --help            show this help message and exit
  --model MODEL         Name of or path to the trained checkpoint
  --llama_dir LLAMA_DIR
                        Path to LLaMA pretrained checkpoint
</code></span></span></span></span>
```

生成后，
[evaluate.py](https://github.com/shansongliu/MU-LLaMA/blob/LLaMA-2/ModelEvaluations/evaluate.py "evaluate.py")
可用于评估为三个模型生成的答案。结果如下所示。

| **型** | **B-U ↑** | **M-R ↑** | **R-L ↑** | **BERT-S ↑** |
| --- | --- | --- | --- | --- |
| LTU | 0.242 | 0.274 | 0.326 | 0.887 |
| LLaMA 适配器 | 0.273 | 0.334 | 0.413 | 0.895 |
| **MU-LLaMA （英语）** | **0.306** | **0.385** | **0.466** | **0.901** |

### 确认

此代码包含来自以下存储库的元素：

* [OpenGVLab/LLaMA 适配器](https://github.com/OpenGVLab/LLaMA-Adapter "OpenGVLab/LLaMA 适配器")
* [伊兹希尔/MERT](https://github.com/yizhilll/MERT "伊兹希尔/MERT")