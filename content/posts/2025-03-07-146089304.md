---
layout: post
title: "DeepSeek-R1本地化部署Mac"
date: 2025-03-07 16:05:54 +0800
description: "DeepSeek-R1本地化部署（Mac）"
keywords: "requires macos 11 big sur or late"
categories: ['技术实践']
tags: ['Macos', 'Deepseek', 'Ai']
artid: "146089304"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146089304
    alt: "DeepSeek-R1本地化部署Mac"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146089304
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146089304
cover: https://bing.ee123.net/img/rand?artid=146089304
image: https://bing.ee123.net/img/rand?artid=146089304
img: https://bing.ee123.net/img/rand?artid=146089304
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     DeepSeek-R1本地化部署（Mac）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h4>
     <strong>
      一、下载 Ollama
     </strong>
    </h4>
    <p>
     本地化部署需要用到 Ollama，它能支持很多大模型。官方网站：https://ollama.com/
    </p>
    <p>
     <img alt="" height="252" src="https://i-blog.csdnimg.cn/direct/62f61b20ebc645ad89a3532fc46c7e2e.png" width="245"/>
    </p>
    <p>
     点击 Download 即可，支持macOS,Linux 和 Windows；我下载的是 mac 版本，要求macOS 11 Big Sur or later，Ollama是跳转到github去下载的，如果下载不了可能要借助科学上网。
    </p>
    <p>
     下载的是个压缩包，直接双击就可以解压出Ollama.app，点击运行即可安装
    </p>
    <p>
     <img alt="" height="315" src="https://i-blog.csdnimg.cn/direct/f21c218bfb2a4f7bab346290464f7a85.png" width="236"/>
    </p>
    <p>
     安装成功之后，ollama会在后台运行，启动命令行，输入ollama
    </p>
    <p>
     <img alt="" height="333" src="https://i-blog.csdnimg.cn/direct/bce7ff52453e413ca276ddaf42890ee7.png" width="411"/>
    </p>
    <p>
     出现以上页面即表示安装成功
    </p>
    <h4>
     <strong>
      二、下载DeepSeek-R1
     </strong>
    </h4>
    <p>
     还是进入ollama.com的页面，点击Models
    </p>
    <p>
     <img alt="" height="261" src="https://i-blog.csdnimg.cn/direct/f5de70fe6bea4076a82cabb70b74de98.png" width="517"/>
    </p>
    <p>
     下载deepseek-r1，
    </p>
    <p>
     <img alt="" height="431" src="https://i-blog.csdnimg.cn/direct/abd7e473c038444c91dd2f0a87dd5ee6.png" width="558"/>
    </p>
    <p>
     deepseek-r1有很多个版本，1.5b，7b，8b，14b，32b，70b，671b，分别代表模型不同的参数数量。
    </p>
    <ul>
     <li>
      B = Billion（十亿参数）：表示模型的参数量级，直接影响计算复杂度和显存占用。
      <ul>
       <li>
        DeepSeek 1.5B：15亿参数（小型模型，适合轻量级任务）
       </li>
       <li>
        DeepSeek 7B：70亿参数（主流规模，平衡性能与资源）
       </li>
       <li>
        DeepSeek 70B：700亿参数（高性能需求场景）
       </li>
       <li>
        DeepSeek 671B：6710亿参数（超大规模，对标PaLM/GPT-4）
       </li>
      </ul>
     </li>
    </ul>
    <p>
     每个版本对应所需的内存大小都不一样，如果你电脑运行内存为8G那可以下载1.5b，7b，8b的蒸馏后的模型；如果你电脑运行内存为16G那可以下载14b的蒸馏后的模型，我这里选择14b的模型。
    </p>
    <p>
     使用ollama run deepseek-r1:14b 进行下载，在命令行里面输入：
    </p>
    <pre><code class="language-bash">ollama run deepseek-r1:14b</code></pre>
    <p>
     <img alt="" height="248" src="https://i-blog.csdnimg.cn/direct/18fc272b66ac49b080ad0354c5ed2d70.png" width="628"/>
    </p>
    <p>
     使用ollama list 查看是否成功下载了模型
    </p>
    <p>
     <img alt="" height="46" src="https://i-blog.csdnimg.cn/direct/371362214b2e42a98de5d2f2a82dfd2c.png" width="526"/>
    </p>
    <p>
     输入ollama run deepseek-r1:14b运行模型，启动成功后，就可以输入我们想问的问题，模型首先会进行深度思考（也就是think标签包含的地方），思考结束后会反馈我们问题的结果。在&gt;&gt;&gt;之后输入想要咨询的 问题，模型回答的速度取决电脑的性能。
    </p>
    <p>
     <img alt="" height="240" src="https://i-blog.csdnimg.cn/direct/128983f54ecc487b9a1d8934639c57a2.png" width="1060"/>
    </p>
    <p>
     使用快捷键Ctrl + d 或者在&gt;&gt;&gt;之后输入 /bye即可退出对话模式。
    </p>
    <pre><code class="language-bash">## 删除模型
ollama rm deepseek-r1:14b
## 停止模型
ollama stop deepseek-r1:14b</code></pre>
    <h4>
     <strong>
      三、web页面的访问
     </strong>
    </h4>
    <p>
     我们通过ollama下载模型后，可以在命令行使用deepseek了，但是命令行的形式还是有些不友好，我们可以借助chatBox，或者Open-WebUI，只要接入ollama的Api就可以使用了。
    </p>
    <h5>
     <strong>
      1、Open-WebUI
     </strong>
    </h5>
    <p>
     Open WebUI是一个可扩展、功能丰富、用户友好的自托管AI平台，旨在完全离线运行。它支持各种LLM运行程序，如Ollama和OpenAI兼容的API，内置RAG推理引擎，使其成为一个强大的AI部署解决方案，本地需要安装Python3（版本3.11～3.13以下）。
    </p>
    <p>
     安装 Open-WebUI需要使用pip进行安装，安装需要一定时间
    </p>
    <pre name="code"><code class="language-bash">pip install open-webui
### 如网络太差，可以使用国内的镜像下载
pip install open-webui -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com</code></pre>
    <p>
     如果 pip 版本较低，可以更新下
    </p>
    <pre name="code"><code class="language-bash">python3 -m pip install --upgrade pip</code></pre>
    <p>
     使用如下命令启动open-webui服务，启动需要一定时间
    </p>
    <pre name="code"><code>open-webui serve</code></pre>
    <p>
     后使用浏览器输入
     <a href="http://127.0.0.1:8080/" rel="nofollow" title="http://127.0.0.1:8080/">
      http://127.0.0.1:8080/
     </a>
     登录服务，注意端口的占用冲突，页面如下：
    </p>
    <p>
     <img alt="" height="272" src="https://i-blog.csdnimg.cn/direct/bbbc94cbb83041e795887905904191f7.png" width="419"/>
    </p>
    <p>
     点击开始使用，第一次使用需要注册用户名、邮件以及密码，这都是存在本地的，可以放心填写。
    </p>
    <p>
     <img alt="" height="318" src="https://i-blog.csdnimg.cn/direct/d29227cac5de4fe4ba625f65b33f87fe.png" width="415"/>
    </p>
    <p>
     注册完毕后，如果本地已经运行了deepseek-r1，它可以自动识别本地已经安装的deepseek r1大模型，
    </p>
    <p>
     <img alt="" height="1086" src="https://i-blog.csdnimg.cn/direct/7091fc0b3bef410fa597b9a36630976d.png" width="2262"/>
    </p>
    <p>
     在对话框里面输入内容，即可与deepseek-r1展开对话
    </p>
    <p>
     <img alt="" height="986" src="https://i-blog.csdnimg.cn/direct/7a5d60fa1b48477d8b14efc491afcdcc.png" width="2760"/>
    </p>
    <h5>
     <strong>
      2、ChatBox
     </strong>
    </h5>
    <p>
     Chatbox AI 是一款 AI 客户端应用和智能助手，支持众多先进的 AI 模型和 API，可在 Windows、MacOS、Android、iOS、Linux 和网页版上使用。
    </p>
    <p>
     <img alt="" height="1076" src="https://i-blog.csdnimg.cn/direct/49b84f619ca4498bbb75e0ea31bb4a09.png" width="2546"/>
     我这里下载的mac版本，成功安装启动后，点击左下角的
     <strong>
      设置
     </strong>
    </p>
    <p>
     <img alt="" height="1520" src="https://i-blog.csdnimg.cn/direct/4ded4864ce924e2faf9b0aba98f3adb6.png" width="2036"/>
    </p>
    <p>
     模型提供方选择Ollama API
    </p>
    <p>
     <img alt="" height="437" src="https://i-blog.csdnimg.cn/direct/63cb7395b53245d0b8b12108e0803ef1.png" width="452"/>
    </p>
    <p>
     模型选择本地部署好的deepseek-r1:14b，点击保存，即可以开始对话
    </p>
    <p>
     <img alt="" height="792" src="https://i-blog.csdnimg.cn/direct/9962bd96fcdc41e5975cdc4b5ad5b666.png" width="1544"/>
    </p>
    <p>
     <strong>
      最后：蒸馏模型不同规格的选择，需要结合自己电脑的配置来选择，不合适的模型会导致电脑过载，对话回答的速度和效果问题都会很差。我电脑内存16GB，以为14b能扛得住，结果安装之后，对话巨慢！后面安装了8b，运行起来速度就快多了，但是通过页面的返回速度会变慢。
     </strong>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6b7869616f7a68756b2f:61727469636c652f64657461696c732f313436303839333034" class_="artid" style="display:none">
 </p>
</div>


