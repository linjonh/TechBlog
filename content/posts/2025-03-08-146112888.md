---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f707a6231393834313131362f:61727469636c652f64657461696c732f313436313132383838"
layout: post
title: "强化学习赵世钰版-学习笔记3.最优策略与贝尔曼最优方程"
date: 2025-03-08 11:07:30 +08:00
description: "这个方程中的r，p（r|s,a），p（s'|s,a）等都是已知的，这表明整个环境信息是已知的，各状态的状态值是未知的，状态值的作用就是用于衡量两个策略之间的优劣，如果对于同一个环境下，对于所有的状态s，策略Pi1的状态值均大于策略Pi2，则可以说策略Pi1由于策略Pi2，以下是相关的数学描述。贝尔曼最优方程，可以记作一个关于状态值v的方程，就是下面的v=f（v），因为前面的R_pi是当前状态下动作值的最大值（前面证过了）。可以据此优化现有的策略，将策略在S1上的行为进行修改，从而增加整个策略的状态值。"
keywords: "强化学习（赵世钰版）-学习笔记（3.最优策略与贝尔曼最优方程）"
categories: ['深度学习', '数学学习', '人工智能']
tags: ['算法', '笔记', '学习']
artid: "146112888"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146112888
    alt: "强化学习赵世钰版-学习笔记3.最优策略与贝尔曼最优方程"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146112888
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146112888
cover: https://bing.ee123.net/img/rand?artid=146112888
image: https://bing.ee123.net/img/rand?artid=146112888
img: https://bing.ee123.net/img/rand?artid=146112888
---

# 强化学习（赵世钰版）-学习笔记（3.最优策略与贝尔曼最优方程）

这是本章在课程中的位置，属于基础工具中的最后一章，主要讨论了最优状态值（Optimal State Value）与最优策略（Optimal Policy），并介绍了对应的计算方法-贝尔曼最优方程（Bellman Optimality Equation，BOE）。

![](https://i-blog.csdnimg.cn/direct/846329df552049419980363e12e29da6.png)

如果一个现有的策略不是最优的，那么如何提升呢？结论就是对策略进行改进。

![](https://i-blog.csdnimg.cn/direct/25e2137ebad7455988d8985f44c66f42.png)

比如起始状态是S1，当前的策略是在S1状态100%向右移动（S1上的右箭头）。但是通过对S1状态上各行为值的计算，可以看出来右移（A2）行为值取值并非最大，下移（A3）的行为值取值最大。

![](https://i-blog.csdnimg.cn/direct/25d46b8c67a74c0faee6f72f6e26add1.png)

可以据此优化现有的策略，将策略在S1上的行为进行修改，从而增加整个策略的状态值。

![](https://i-blog.csdnimg.cn/direct/0b18dacb8c6141b49bf091a8111bb221.png)

状态值的作用就是用于衡量两个策略之间的优劣，如果对于同一个环境下，对于所有的状态s，策略Pi1的状态值均大于策略Pi2，则可以说策略Pi1由于策略Pi2，以下是相关的数学描述。

![](https://i-blog.csdnimg.cn/direct/fc226e1daebe4a5286c37bdab896ab84.png)

既然有了对比，那么会有一个策略Pi\_star，它的状态值优于同一个环境下其他所有的策略，那么这个策略就叫做最优策略。

![](https://i-blog.csdnimg.cn/direct/c36c66a8795b4d6d81bdd89e62f62fa8.png)

而计算获取最优策略的方法，就是贝尔曼最优方程，贝尔曼最优方程的形式如下。

![](https://i-blog.csdnimg.cn/direct/b074d1991e834013b4085dc4ffd4ebc0.png)

与贝尔曼方程相比，前面多了个max符号，作用是从后续各种参数算出的结果中，找到最大的作为结果。这个跟最优化理论中的argmax不太一样，argmax指的是当后续函数取最大值时对应的参数。这个方程中的r，p（r|s,a），p（s'|s,a）等都是已知的，这表明整个环境信息是已知的，各状态的状态值是未知的，

贝尔曼最优方程的矩阵-向量形式为

![](https://i-blog.csdnimg.cn/direct/7b6e07c09f454fdfb8b362588a5818c1.png)

对整个运算求max，等价于对每一行的运算求max

![](https://i-blog.csdnimg.cn/direct/9dd6b17e0bbf4d6ca84d02c58c063216.png)

后面需要讨论的是，如何解这个方程，这个方程的解是否存在，方程的解是否唯一，以及如何将解转换成最优策略。

计算最优策略的方法是，首先固定住未来的状态值v(s')，仅计算当前状态值的最大值。

![](https://i-blog.csdnimg.cn/direct/b98d1e63ca4a45e78eae10d68b918ad2.png)

最后简化成了计算当前状态s下行为值期望的最大值，很显然最大值就是几个候选行为中，行为值最大的那个，下面是证明过程：

![](https://i-blog.csdnimg.cn/direct/4af8c1f7d32442da9a7fc99128d4f96d.png)

所以贝尔曼最优方程，这里通过简化，可以变成对当前状态值最大值的求解。

![](https://i-blog.csdnimg.cn/direct/61633b10c9d24fbdb59863d0240c193b.png)

然后根据对相关行为值的计算，修改策略在当前状态下的行为选择。

![](https://i-blog.csdnimg.cn/direct/e21964fb46334c49ad5d55aa31acc94b.png)

贝尔曼最优方程，可以记作一个关于状态值v的方程，就是下面的v=f（v），因为前面的R\_pi是当前状态下动作值的最大值（前面证过了）。

![](https://i-blog.csdnimg.cn/direct/f9341149c9574ee8bdaa8bb33a85ff40.png)

这里提出了一个新的概念，压缩映射或者压缩函数，指的是函数映射之后能缩小两个自变量之间的距离，数学表达如下

![](https://i-blog.csdnimg.cn/direct/c63446ba5bba44409ee4120d0ee91fa4.png)

这个压缩函数或者压缩映射，存在一个固定点x\_star，这个固定点的含义是x*=f（x*）。证明过程如下，主要是将自变量映射出的因变量，继续映射或者传入函数（一回事），这么不停的迭代，最后会逐渐收敛，整个迭代不会发生变化了。

![](https://i-blog.csdnimg.cn/direct/cc1d0c77b5314c558a313ff590d822af.png)

回到贝尔曼最优方程，它的形式是

![](https://i-blog.csdnimg.cn/direct/a304fbcf775446f599983255be9f58d8.png)

可以证明这个贝尔曼最优方程是一个压缩映射（证明过程用的是豆包）

![](https://i-blog.csdnimg.cn/direct/fa603d0b42154db89b93740685c9f94c.png)

那么根据压缩映射的性质，通过一个初始状态的状态值，通过迭代总会到一个固定点，既是最优点，这个方法叫做值迭代算法。

![](https://i-blog.csdnimg.cn/direct/ba8989831056489589a488df7eff3561.png)

贝尔曼最优方程的解，其对应的策略就是整个环境的最优策略

![](https://i-blog.csdnimg.cn/direct/342d39a9693b4a6382d2d2b3de5c2024.png)

最优策略的形式，就是每个状态都会100%选择行为值最大的策略。

![](https://i-blog.csdnimg.cn/direct/d82b70b6bf2040529833abccbc5ae36c.png)

那么对最优策略和最优状态值的决定因素是什么，是环境系统模型，奖励设置和折扣率。

![](https://i-blog.csdnimg.cn/direct/9e1da22871fb45069d226c0b2d170cc1.png)

奖励值决定了对一些行为的取舍，且奖励的绝对值大小并不重要，主要是各行为奖励的相对大小。而折扣率决定了策略的侧重，折扣率越接近于零则策略越短视，极端情况下降到零则可能永远到不了目标状态。

同时，最优策略也避免了绕路刷分的情况，因为通过折扣率完成了对绕路的惩罚，路径越长惩罚的越严重。

![](https://i-blog.csdnimg.cn/direct/f1fb4874aba14d85b0d9708aab22d425.png)

最后是本章的总结，贝尔曼最优方程的两种形式：元素形式与矩阵-向量形式。

![](https://i-blog.csdnimg.cn/direct/48597b24e7444eb289eb5f9e2fc033b8.png)

通过压缩映射的性质，决定了贝尔曼最优方程可解，且解是唯一的。解贝尔曼最优方程的方法就是通过迭代法，结果会最终收敛。解贝尔曼最优方程的目的，是通过最优解找到最优状态值和最优策略。