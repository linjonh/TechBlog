---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34323437393332372f:61727469636c652f64657461696c732f313436313131353433"
layout: post
title: "qwen2.5-vlå¤šæœºå¤šå¡åˆ†å¸ƒå¼éƒ¨ç½²"
date: 2025-03-15 17:52:48 +08:00
description: "è®°å½•ä¸€ä¸‹å·¥ä½œä¸­è¿›è¡Œå¤šæœºå¤šå¡éƒ¨ç½²qwen2.5-vlå¤šæ¨¡æ€å¤§æ¨¡å‹è¸©è¿‡çš„å‘ç¬¬ä¸€ä¸ªå¤©å‘å°±æ˜¯å®˜æ–¹æä¾›çš„é•œåƒqwenllm/qwenvl:2.5-cu121æœ‰é—®é¢˜ï¼Œåœ¨titanæ˜¾å¡ä¼šæŠ›å‡ºcuda error:no kernel image is availabe for execution on the device. è¿™æ˜¯cudaå†…æ ¸ä¸GPUä¸å…¼å®¹çš„é—®é¢˜ï¼Œå¯æ˜¯æ‰‹åŠ¨åˆ¶ä½œçš„å…¶ä»–cuda12é•œåƒå°±èƒ½è·‘ã€‚"
keywords: "qwen2.5-vlå¤šæœºå¤šå¡åˆ†å¸ƒå¼éƒ¨ç½²"
categories: ['æœªåˆ†ç±»']
tags: ['è¯­è¨€æ¨¡å‹', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'æ·±åº¦å­¦ä¹ ', 'åˆ†å¸ƒå¼', 'äººå·¥æ™ºèƒ½', 'Transformer', 'Chatgpt']
artid: "146111543"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146111543
    alt: "qwen2.5-vlå¤šæœºå¤šå¡åˆ†å¸ƒå¼éƒ¨ç½²"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146111543
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146111543
cover: https://bing.ee123.net/img/rand?artid=146111543
image: https://bing.ee123.net/img/rand?artid=146111543
img: https://bing.ee123.net/img/rand?artid=146111543
---

# qwen2.5-vlå¤šæœºå¤šå¡åˆ†å¸ƒå¼éƒ¨ç½²

è®°å½•ä¸€ä¸‹å·¥ä½œä¸­è¿›è¡Œå¤šæœºå¤šå¡éƒ¨ç½²qwen2.5-vlå¤šæ¨¡æ€å¤§æ¨¡å‹è¸©è¿‡çš„å‘
  
ç¬¬ä¸€ä¸ªå¤©å‘å°±æ˜¯å®˜æ–¹æä¾›çš„é•œåƒqwenllm/qwenvl:2.5-cu121æœ‰é—®é¢˜ï¼Œåœ¨titanæ˜¾å¡ä¼šæŠ›å‡ºcuda error:no kernel image is availabe for execution on the device. è¿™æ˜¯cudaå†…æ ¸ä¸GPUä¸å…¼å®¹çš„é—®é¢˜ï¼Œå¯æ˜¯æ‰‹åŠ¨åˆ¶ä½œçš„å…¶ä»–cuda12é•œåƒå°±èƒ½è·‘ã€‚åœ¨å®˜ç½‘é•œåƒåŸºç¡€ä¸Šé‡è£…cuda11.8ï¼Œå€’æ˜¯å¯ä»¥ç”¨äº†ï¼Œä½†åœ¨gradioé¡µé¢è°ƒç”¨æ—¶ä¼šå‡ºç°å¡æ­»æƒ…å†µï¼Œæœ€ç»ˆè¿˜æ˜¯è‡ªå·±ä»å¤´é…ç½®ç¯å¢ƒã€‚å…³äºå¦‚ä½•é…ç½®dockeré•œåƒï¼Œæ²¡å•¥æŠ€æœ¯ç‚¹ï¼Œè¿™é‡Œä¸è¯´äº†ã€‚

## ä¸€ å®¹å™¨é…ç½®

ä½¿ç”¨ray+vllmæ–¹å¼è¿›è¡Œåˆ†å¸ƒå¼éƒ¨ç½²ï¼Œé‡‡ç”¨hostæ¨¡å¼ã€‚å½“ä¸€å°æœºå™¨ä¸æ˜¯æ‰€æœ‰æœºå™¨éƒ½å¯ç”¨æ—¶ï¼Œéœ€è¦é€šè¿‡CUDA\_VISIBLE\_DEVICESå˜é‡æŒ‡å®šrayåˆ†å¸ƒå¼ä½¿ç”¨çš„æ˜¾å¡ç¼–å·ï¼Œé€šå¸¸åœ¨èµ·å®¹å™¨æ—¶æŒ‡å®šï¼Œå½“ä½¿ç”¨æ˜¾å¡ç¼–å·æœ‰å˜åŒ–æ—¶ï¼Œåœæ‰åˆ†å¸ƒå¼ï¼Œåœ¨å®¹å™¨å†…æ‰‹åŠ¨æŒ‡å®šæ˜¾å¡åå†é‡æ–°æ­å»ºåˆ†å¸ƒå¼é›†ç¾¤ã€‚

å¦å¤–ï¼Œæœºå™¨ä¹‹é—´çš„é€šä¿¡è¦æŒ‡å®šç½‘å¡ï¼Œhostæ¨¡å¼å®¹å™¨ç›´æ¥ä½¿ç”¨å®¿ä¸»æœºç½‘å¡ï¼Œé€šè¿‡ifconfigæŸ¥çœ‹å½“å‰æœºå™¨å¯ç”¨ç½‘å¡ï¼Œæ¯å°æœºå™¨ç½‘å¡å¯èƒ½ä¸åŒï¼Œæ‰€ä»¥åœ¨å¯åŠ¨å®¹å™¨æ—¶å°†å½“å‰æœºå™¨çš„ç½‘å¡ä»¥ç¯å¢ƒå˜é‡çš„å½¢å¼ä¼ å…¥ã€‚ä¸‹é¢å°±æ˜¯é›†ç¾¤ä¸­ä¸¤å°æœºå™¨ä½¿ç”¨ä¸åŒç½‘å¡çš„æ¡ˆä¾‹ã€‚
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1c32ef7dadc0413785df0dead7dde8f3.png)

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯åŠ¨å®¹å™¨çš„docker-composeå‘½ä»¤ï¼Œå®¹å™¨ä¸åˆ†headä¸workèŠ‚ç‚¹ï¼Œåœ¨å®¹å™¨å†…æ­å»ºé›†ç¾¤æ—¶æŒ‡å®šã€‚

```yaml
version: "3"

services:
	# æœåŠ¡å,å¯ä»¥ä¸ä¸container_nameåŒå, åœ¨Nginxä¸­ä½¿ç”¨æœåŠ¡ååšè°ƒåº¦,
    yblir_qwen2.5-vl:
        image: qwen2.5-vl:cu12-torch25-vllm073-dist
        container_name: dist_head_qwen2.5-vl
        
        # è·å–å®¿ä¸»æœºrootæƒé™,è¦æ˜ å°„ç«¯å£ä¹Ÿéœ€è¦true
        privileged: true
        # è®¾ç½®å…±äº«å†…å­˜å¤§å°ä¸º16g,é˜²æ­¢åˆ†å¸ƒå¼è®­ç»ƒdataloaderæ—¶å‡ºé”™
        shm_size: "32gb"
		
		# è·¯å¾„æ˜ å°„, ä¸»æœºç›®å½•:å®¹å™¨å†…ç›®å½•
        volumes:
            - /home/data/liyabin_project:/home
        #ports:
        #    - "10086:22"        # å®¹å™¨å†…22ç«¯å£æ˜ å°„åˆ°å¤–éƒ¨10086,22ç«¯å£é€šå¸¸ç”¨äºåœ¨å¤–éƒ¨è°ƒç”¨å®¹å™¨å†…Pythonç¯å¢ƒ
        #    - "10087:10087"     # ç”¨é€”å¾…å®š,æ¯”å¦‚å¯ä»¥tensorboard --logdir=è·¯å¾„ --port 10087  
        # ä»å¤–éƒ¨ä¼ å…¥å˜é‡
        environment:
             - NVIDIA_VISIBLE_DEVICES=0,3,4
             - GLOO_SOCKET_IFNAME=enp134s0f0
             - NCCL_SOCKET_IFNAME=enp134s0f0

        # å®¹å™¨é‡æ–°å¯åŠ¨,æ¯”å¦‚å½“å®¹å™¨è¢«å…¶ä»–äººkilläº†,ä¼šè‡ªåŠ¨é‡å¯
        restart: always
        entrypoint: /bin/bash
        # command: ["-c", "ray start --head --dashboard-host=0.0.0 --port 6379"]
        tty: true
        
        network_mode:
            "bridge"    # é»˜è®¤æ¡¥æ¥æ¨¡å¼,å®¹å™¨ä¹‹é—´ä¸é¡»è¦äº’ç›¸é€šè®¯.
            "host"    # å½“å‰æ¨¡å¼ä¸‹portsç«¯å£æ˜ å°„å¤±æ•ˆ, å®¹å™¨ç¯å¢ƒä¸éš”ç¦»ï¼Œå°†ä½¿ç”¨ä¸»æœºçš„ç«¯å£å’Œip.

```

æ³¨æ„ï¼Œæ‰€æœ‰å®¹å™¨çš„ç¯å¢ƒï¼Œå®¹å™¨å†…æ¨¡å‹æ–‡ä»¶è·¯å¾„å¿…é¡»ä¸€è‡´ã€‚

vllmå®˜æ–¹ç»™äº†ä¸€ä¸ªéƒ¨ç½²è„šæœ¬ï¼Œä¸å¥½ç”¨ï¼Œè¿™é‡Œä½¿ç”¨æ›´çµæ´»çš„æ‰‹åŠ¨æ­å»ºé›†ç¾¤çš„æ–¹æ³•ã€‚å‰é¢è¯´åœ¨æ­å»ºé›†ç¾¤æ—¶ç¡®å®šheadä¸workèŠ‚ç‚¹ï¼Œ
  
åœ¨æƒ³åšheadèŠ‚ç‚¹çš„å®¹å™¨å†…æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```bash
ray start --head --dashboard-host=0.0.0.0 --port=6379

```

åœ¨workå®¹å™¨èŠ‚ç‚¹æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼š

```bash
ray start --address='xx.xx.xx.xx:6379'

```

åªèƒ½æœ‰ä¸€ä¸ªheadèŠ‚ç‚¹ï¼Œworkå¯ä»¥æœ‰å¤šä¸ªã€‚

åœ¨ä»»æ„å®¹å™¨èŠ‚ç‚¹æ‰§è¡Œray status,å¯ä»¥çœ‹åˆ°é›†ç¾¤ä¸Šæ‰€æœ‰èŠ‚ç‚¹è®¾å¤‡æƒ…å†µ
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/2e87d60925e84633a15871a5ffca4c94.png)
rayå¦ä¸€ä¸ªå¸¸ç”¨å‘½ä»¤æ˜¯ray stop, åœæ­¢é›†ç¾¤ã€‚åœ¨headèŠ‚ç‚¹ä¸Šæ‰§è¡Œä¼šæ€æ‰æ•´ä¸ªé›†ç¾¤ï¼Œåœ¨workèŠ‚ç‚¹ä¸Šæ‰§è¡Œåˆ™ä»…ä¼šå¸è½½å½“å‰èŠ‚ç‚¹ã€‚
  
å…³äºray, ä¼šä½¿ç”¨è¿™ä¸¤ä¸ªå‘½ä»¤å°±èƒ½åº”å¯¹ç»å¤§éƒ¨åˆ†é›†ç¾¤é—®é¢˜ã€‚

åœ¨headèŠ‚ç‚¹æ‰§è¡Œå¯åŠ¨é›†ç¾¤æ—¶ï¼Œä¼šæš´éœ²ä¸€ä¸ªURLï¼Œåœ¨æµè§ˆå™¨é¡µé¢ç›‘æ§è®¾å¤‡ä½¿ç”¨æƒ…å†µï¼š
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e9ae5363539a4eb59b852a709a62ba26.png)
  
å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œåˆ°æ­¤å®Œæˆé›†ç¾¤æ­å»ºå·¥ä½œäº†ï¼Œä¸è¿‡è¿‡ç¨‹ä¸­éš¾å…ä¼šå‡ºç°å„ç§å„æ ·çš„å‘ï¼Œæ¯”å¦‚æˆ‘æ›¾åœ¨2x8 2080tiè®¾å¤‡ä¸Šæ­å»ºé›†ç¾¤ï¼ŒworkèŠ‚ç‚¹ä»‹å…¥é›†ç¾¤åå¾ˆå¿«å°±æ‰çº¿ï¼Œæ¨æµ‹å¤§æ¦‚ç‡æ—¶æœºå™¨é—´é€šä¿¡ä¸ç¨³å®šé€ æˆçš„ï¼Œæ‰€ä»¥ï¼Œæ­å»ºé›†ç¾¤çš„æœºå™¨å°½é‡å¤„åœ¨åŒä¸€ç½‘æ®µã€‚

## äºŒ ç¯å¢ƒæµ‹è¯•

æ­å»ºå¥½é›†ç¾¤åï¼Œè¿˜éœ€è¦å¯¹ç¯å¢ƒåšæµ‹è¯•ï¼Œä¿è¯ä»£ç èƒ½è·‘é€šã€‚
  
éƒ¨ç½²qwen2.5-vlæœ€ä½ç¯å¢ƒè¦æ±‚ä¸ºvllm>=0.7.2, transformers>=4.49

ä½¿ç”¨vllmå‘½ä»¤è¡Œå¯åŠ¨3bé‡åŒ–ç‰ˆæ¥åšæµ‹è¯•ï¼š

```bash
vllm serve /home/Qwen2.5-VL-3B-Instruct-AWQ --tensor-parallel-size 2 --pipeline-parallel-size 2 --dtype float16 \
--port 8811 --gpu-memory-untilization 0.5 --max-num-seqs 2 --max-model-len 4096 --enforce-eager

```

* é—®é¢˜1 cuda error: cuda error:no kernel image is availabe for execution on the device
    
  é—®é¢˜çœ‹èµ·æ¥æ˜¯cudaå†…æ ¸ä¸GPUä¸å…¼å®¹ï¼Œå¼€å§‹ä»¥ä¸ºæ˜¯å„ç§ä¾èµ–åŒ…å’Œcudaè£…çš„ç‰ˆæœ¬å¤ªé«˜ï¼Œä¸å…¼å®¹titanæ˜¾å¡äº†ï¼Œå¤šæ¬¡æ’æŸ¥æ— æœï¼Œåªå¾—é‡è£…cuda11.8,é—®é¢˜è§£å†³ã€‚ä¹‹ååˆä»é›¶æ„å»ºäº†cuda12.1çš„é•œåƒï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œåªèƒ½æŠŠé”…ç”©ç»™qwenå®˜æ–¹æä¾›çš„é•œåƒæœ‰é—®é¢˜ã€‚
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/7280a616ba6f4bcbbc3d87d3e38438e1.png)
* é—®é¢˜2 attendtionè®¡ç®—åç«¯ä¸å…¼å®¹é—®é¢˜
    
  xformers wasnâ€™t build with CUDA suport
    
  requires device with capability > (8, 0) but your GPU has capability (7, 5)
    
  åŸå› ï¼šè®¡ç®—åç«¯è¦åœ¨å½“å‰è®¾å¤‡ä¸Šç°åœºç¼–è¯‘ï¼Œæ¯”å¦‚ä¸‹é¢è¿™ä¸ªï¼Œxformersæ˜¯åœ¨å…¶ä»–æ˜¾å¡é¢„ç¼–è¯‘å¥½çš„ï¼Œä½¿ç”¨æ—¶å°±ä¼šæŠ¥é”™ã€‚
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/bf3b72329aed4fd69b3c85574c53a178.png)
    
  ä¸‹è½½xformersæºç ç¼–è¯‘å®‰è£…ï¼Œä¹‹åæŸ¥çœ‹xformersè¯¦ç»†ä¿¡æ¯ã€‚

```python
python -m xformers.info

```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1fec8912474c498eabf7040c01ed7e32.png)

* é—®é¢˜3 é‡åŒ–é—®é¢˜
    
  The input size is not aligned with the quantized weight shape. This can be casused by too large tensor parallel size
    
  å®˜æ–¹æ—©èµ·Qwen2.5-VL-72B-Instruct-AWQæ²¡åšå¥½ï¼Œè®°å¾—æ˜¯FFNå±‚å‚æ•°ä¸èƒ½æ•´é™¤çš„åŸå› ï¼Œä¸æ”¯æŒâ€“tensor-parallel-size 2,4,8ç­‰è®¾ç½®ï¼Œä¸è¿‡ç°åœ¨æ–°ç‰ˆæ¨¡å‹å®˜æ–¹å·²ç»åšå¥½ï¼Œä¸çº ç»“è¿™ä¸ªé—®é¢˜äº†ã€‚
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/a8b79d4d10684475947357421789fa95.png)
  - é—®é¢˜4 max\_model\_lenè®¾ç½®é—®é¢˜
    
  valueerror: The modelâ€™s max seq len (10240) is larger than the maximum number of tokens that can be stored in KV cache (3408). Try increasing â€˜gpu\_memory\_utilizationâ€™ or decreasing max\_model\_len when initializing the engine.
    
  åŸå› : è¿™æ˜¯è®¾å®šæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦è¶…è¿‡å¯å­˜å‚¨çš„æœ€å¤§çš„kv-cacheæ•°é‡, æ˜¯æ˜¾å­˜ä¸è¶³çš„è¡¨ç°ä¹‹ä¸€,å¢åŠ gpu\_memory\_utilizationæˆ–å‡å°‘max\_model\_lenå¯è§£å†³
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e203acd692b446849a6249f596233033.png)
* é—®é¢˜5 æ˜¾å­˜ä¸è¶³çš„å¦ä¸€ä¸­è¡¨ç°
    
  torch.outofmemoryerror:cuda out of memory. tried to allocate 640.00MiB GPU 0 has a total capacity â€¦
    
  è§£å†³æ–¹æ¡ˆåŒé—®é¢˜4, å½“ä¿®æ”¹gpu\_memory\_utilizationå,æŠ¥é”™ä¿¡æ¯æ— å˜åŒ–(è¿æŠ¥é”™æ•°å€¼éƒ½æ²¡å˜åŒ–), åŸºæœ¬å¯ç¡®å®šæ˜¯max\_model\_lenå¼•èµ·çš„. è¿™æ˜¯å› ä¸ºvllmåˆå§‹åŒ–æ—¶ä¼šæå‰å¼€è¾Ÿä¸€å—æ˜¾å­˜ç©ºé—´å­˜å‚¨max\_model\_lenä¸ªå…¨0 kv-cacheå¤‡ç”¨, å¼€å¯è¿™å—ç©ºé—´æ—¶æ˜¾å­˜ä¸å¤Ÿå¼•èµ·äº†æŠ¥é”™. å…³äºmax\_model\_lenä¸kv-cacheçš„å…³ç³»åœ¨é—®é¢˜4ä¸­æœ‰è¯´æ˜.
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/6114877cbb344ff9bebee7faff8889af.png)
* é—®é¢˜6 headæ•´é™¤é—®é¢˜
    
  total number of attention heads(16) must be divisible by tensor parallel size(6)
    
  è¿™æ˜¯â€“tensor-parallel-sizeå€¼æ²¡è®¾å®šå¥½,è¿™ä¸ªå€¼å¿…é¡»è¢«attention heads æ•´æ•°. æ¨¡å‹headæ•°é‡åœ¨æ¨¡å‹æ–‡ä»¶config.jsonä¸­å¯æŸ¥çœ‹.
    
  vllm ä¸­tensor-parallel-size=Næ˜¯æŒ‡å¼ é‡å¹¶è¡Œ,æ ‡å‡†è¯´æ³•æ˜¯å°† QKV æŠ•å½±çš„å‚æ•°çŸ©é˜µä¼šè¢«åˆ‡åˆ†æˆ N ä»½ï¼Œæ¯ä¸ª GPU åªå­˜å‚¨å¹¶è®¡ç®—å…¶ä¸­çš„ä¸€éƒ¨åˆ†. è¯´äººè¯å°±æ˜¯ä¸€ä¸ªattentionç”±å¤šä¸ªhead attentionç»„æˆ, å°†æ‰€æœ‰headså‡åˆ†åˆ°Nä¸ªgpuä¸Šåšè®¡ç®—(æœ‰è¡Œå¹¶è¡Œå’Œåˆ—å¹¶è¡Œçš„ä¸¤ç§æ–¹å¼), ç„¶ååˆå¹¶ç»“æœ.
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/03d07a51ce254f00a7c4cab45bf0ef10.png)
* é—®é¢˜7 åˆ†å¸ƒå¼å¯åŠ¨é—®é¢˜
    
  runtimeError: cannot re-initialize cuda in forked subprocess. To use CUDA with multiprocessing. you must use the â€˜spawnâ€™ start method
    
  åŸå› : vllmä¸torchä¸¤ä¸ªç»„ä»¶åˆ†å¸ƒå¼æ¨¡å—å†²çª, åœ¨ç¯å¢ƒå˜é‡æˆ–å¯åŠ¨è„šæœ¬ä¸­æŒ‡å®šåˆ†å¸ƒå¼æ–¹å¼å°±å¥½.

```python
import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD']='spawn'

```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/6cc19f231e91417f9063869ca883bc28.png)

## ä¸‰ æœåŠ¡ä»£ç æ”¹å†™åŠéƒ¨ç½²

ä½¿ç”¨vllm serveå¯åŠ¨æœåŠ¡,ç„¶åé€šè¿‡requestè¯·æ±‚è°ƒç”¨, è¿™æ˜¯å¸¸ç”¨çš„æ–¹å¼,åœ¨ä¸Šä¸€ä»½å·¥ä½œ
[qwen2.5-vlä½¿ç”¨vllméƒ¨ç½²gradioé¡µé¢è°ƒç”¨](https://blog.csdn.net/weixin_42479327/article/details/146199434)
å·²ç»è¯¦ç»†æè¿°è¿‡

å·¥ä½œä¸­æœ¬åœ°éƒ¨ç½²è¿˜å°è¯•äº†å¦ä¸€æ–¹æ¡ˆ,å³æ”¹é€ qwen2.5-vlå®˜æ–¹ç»™çš„æœ¬åœ°gradioé¡µé¢è°ƒç”¨çš„æ–¹å¼. vllm+gradioå±äºå¼‚æ­¥è°ƒç”¨, è¦ä½¿ç”¨vllmçš„AsyncLLMEngineæ¥æ„å»ºæ¨ç†æ¨¡å‹.

```python
# -*- coding: utf-8 -*-
# @Time    : 2025/3/10 ä¸‹åˆ19:31
# @Author  : yblir
# @File    : dist_demo_mm.py
# explain  :
# =======================================================

import copy
import re
from argparse import ArgumentParser

# from threading import Thread
import threading
from vllm import SamplingParams, AsyncLLMEngine
from vllm.engine.arg_utils import AsyncEngineArgs

import gradio as gr
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, TextIteratorStreamer

# DEFAULT_CKPT_PATH = '/home/Qwen2.5-VL-7B-Instruct'
# DEFAULT_CKPT_PATH = r'E:\PyCharm\PreTrainModel\Qwen2.5-VL-7B-Instruct'
DEFAULT_CKPT_PATH = '/home/Qwen25-VL-3B-Instruct-AWQ'


def _get_args():
    parser = ArgumentParser()

    parser.add_argument('-c',
                        '--checkpoint-path',
                        type=str,
                        default=DEFAULT_CKPT_PATH,
                        help='Checkpoint name or path, default to %(default)r')
    parser.add_argument('--cpu-only', action='store_true', help='Run demo with CPU only')

    parser.add_argument('--flash-attn2',
                        action='store_true',
                        default=False,
                        help='Enable flash_attention_2 when loading the model.')
    parser.add_argument('--share',
                        action='store_true',
                        default=False,
                        help='Create a publicly shareable link for the interface.')
    parser.add_argument('--inbrowser',
                        action='store_true',
                        default=False,
                        help='Automatically launch the interface in a new tab on the default browser.')
    # ===================================================================================================================
    parser.add_argument('--server-port', type=int, default=7860, help='Demo server port.')
    parser.add_argument('--server-name', type=str, default='0.0.0.0', help='Demo server name.')
    parser.add_argument('--max-image-nums',
                        type=int,
                        default=10,
                        help='å¤šè½®å¯¹è¯æ—¶å¯æ¥å—çš„æœ€å¤§å›¾ç‰‡æ•°é‡')
    parser.add_argument('--dtype',
                        type=str,
                        default='float16',
                        help='æ¨¡å‹å¤„ç†çš„æ•°æ®ç±»å‹,è¿™é‡Œé»˜è®¤ä½¿ç”¨flaot16,å‡å°‘æ˜¾å­˜')
    parser.add_argument('--gpu-memory-utilization',
                        type=float,
                        default=0.15,
                        help='vllmè¦å ç”¨çš„gpuæ˜¾å­˜æ¯”ä¾‹,è‹¥æœ‰å…¶ä»–ç¨‹åºä¹Ÿåœ¨ä½¿ç”¨æ˜¾å¡, éœ€è¦æŠŠè¯¥å€¼è®¾å°')
    parser.add_argument('--enforce-eager',
                        action='store_true',
                        default=True,
                        help='falseæ—¶ä½¿ç”¨cuda graph, ä¼šå¢åŠ æ˜¾å­˜å ç”¨')
    parser.add_argument('--max-model-len',
                        type=int,
                        default=4096,
                        help='å¯å¤„ç†çš„æœ€å¤§è¾“å…¥tokenæ•°é‡, å¦‚æœè¿›è¡Œå¤šè½®å¯¹è¯,éœ€è¦è®¾ç½®å¤§å€¼,ä¸ç„¶tokenæ•°é‡ä¼šè¶…æ ‡,'
                             'ä¹Ÿå¯ä»¥ç¼ºçœ,ä¸è®¾ä¸Šé™, ä¸è¿‡è¿™æ ·æœ‰æŠ¥æ˜¾å­˜é£é™©')
    parser.add_argument('--tensor-parallel-size',
                        type=int,
                        default=2,
                        help='ä¸€å°æœºå™¨ä¸Šä½¿ç”¨çš„gpuæ•°é‡, è¯¥å€¼çš„è®¾å®šéœ€è¦è¢«attentionçš„headæ•°é‡æ•´é™¤')
    parser.add_argument('--pipeline-parallel-size',
                        type=int,
                        default=2,
                        help='åˆ†å¸ƒå¼ç³»ç»Ÿä½¿ç”¨çš„æœºå™¨æ•°é‡')
    args = parser.parse_args()
    return args


def _parse_text(text):
    lines = text.split('\n')
    lines = [line for line in lines if line != '']
    count = 0
    for i, line in enumerate(lines):
        if '```' in line:
            count += 1
            items = line.split('`')
            if count % 2 == 1:
                lines[i] = f'<pre><code class="language-{items[-1]}">'
            else:
                lines[i] = '<br></code></pre>'
        else:
            if i > 0:
                if count % 2 == 1:
                    line = line.replace('`', r'\`')
                    line = line.replace('<', '&lt;')
                    line = line.replace('>', '&gt;')
                    line = line.replace(' ', '&nbsp;')
                    line = line.replace('*', '&ast;')
                    line = line.replace('_', '&lowbar;')
                    line = line.replace('-', '&#45;')
                    line = line.replace('.', '&#46;')
                    line = line.replace('!', '&#33;')
                    line = line.replace('(', '&#40;')
                    line = line.replace(')', '&#41;')
                    line = line.replace('$', '&#36;')
                lines[i] = '<br>' + line
    text = ''.join(lines)
    return text


def _remove_image_special(text):
    text = text.replace('<ref>', '').replace('</ref>', '')
    return re.sub(r'<box>.*?(</box>|$)', '', text)


def _is_video_file(filename):
    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg']
    return any(filename.lower().endswith(ext) for ext in video_extensions)


def _gc():
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


def _transform_messages(original_messages):
    transformed_messages = []
    for message in original_messages:
        new_content = []
        for item in message['content']:
            if 'image' in item:
                new_item = {'type': 'image', 'image': item['image']}
            elif 'text' in item:
                new_item = {'type': 'text', 'text': item['text']}
            elif 'video' in item:
                new_item = {'type': 'video', 'video': item['video']}
            else:
                continue
            new_content.append(new_item)

        new_message = {'role': message['role'], 'content': new_content}
        transformed_messages.append(new_message)

    return transformed_messages


async def async_generate(request_id, engine, inputs):
    sampling_params = SamplingParams(
            temperature=0.1,
            top_p=0.001,
            repetition_penalty=1.05,
            max_tokens=512,
            stop_token_ids=[]
    )
    response_gen = engine.generate(
            request_id=request_id,
            prompt=inputs,
            sampling_params=sampling_params
    )

    async for result in response_gen:
        for output in result.outputs:
            yield output.text


def _launch_demo(args, model, processor):
    async def call_local_model(model, processor, messages):
        request_id = f"req-{threading.get_ident()}"

        messages = _transform_messages(messages)
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        # image_inputs, video_inputs = process_vision_info(messages)
        image_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)
        mm_data = {}
        if image_inputs is not None:
            mm_data['image'] = image_inputs
        if video_inputs is not None:
            mm_data["video"] = video_inputs
        ll_inputs = {
            'prompt'             : text,
            'multi_modal_data'   : mm_data,
            # FPS will be returned in video_kwargs
            "mm_processor_kwargs": video_kwargs
        }

        async for new_next in async_generate(request_id, model, ll_inputs):
            yield new_next

    def create_predict_fn():
        async def predict(_chatbot, task_history):
            nonlocal model, processor
            chat_query = _chatbot[-1][0]
            query = task_history[-1][0]
            if len(chat_query) == 0:
                _chatbot.pop()
                task_history.pop()
                yield _chatbot
            print('User: ' + _parse_text(query))
            history_cp = copy.deepcopy(task_history)
            full_response = ''
            messages = []
            content = []
            for q, a in history_cp:
                if isinstance(q, (tuple, list)):
                    if _is_video_file(q[0]):
                        content.append({'video': f'file://{q[0]}'})
                    else:
                        content.append({'image': f'file://{q[0]}'})
                else:
                    content.append({'text': q})
                    messages.append({'role': 'user', 'content': content})
                    messages.append({'role': 'assistant', 'content': [{'text': a}]})
                    content = []
            messages.pop()

            async for response in call_local_model(model, processor, messages):
                # print("response=",response)
                _chatbot[-1] = (_parse_text(chat_query), _remove_image_special(_parse_text(response)))

                yield _chatbot
                full_response = _parse_text(response)

            task_history[-1] = (query, full_response)
            print('Qwen-VL-Chat: ' + _parse_text(full_response))
            yield _chatbot

        return predict

    def create_regenerate_fn():
        async def regenerate(_chatbot, task_history):
            nonlocal model, processor
            if not task_history:
                yield _chatbot
            item = task_history[-1]
            if item[1] is None:
                yield _chatbot
            task_history[-1] = (item[0], None)
            chatbot_item = _chatbot.pop(-1)
            if chatbot_item[0] is None:
                _chatbot[-1] = (_chatbot[-1][0], None)
            else:
                _chatbot.append((chatbot_item[0], None))
            _chatbot_gen = predict(_chatbot, task_history)
            async for _chatbot in _chatbot_gen:
                yield _chatbot

        return regenerate

    predict = create_predict_fn()
    regenerate = create_regenerate_fn()

    def add_text(history, task_history, text):
        task_text = text
        history = history if history is not None else []
        task_history = task_history if task_history is not None else []
        history = history + [(_parse_text(text), None)]
        task_history = task_history + [(task_text, None)]
        return history, task_history, ''

    def add_file(history, task_history, file):
        history = history if history is not None else []
        task_history = task_history if task_history is not None else []
        history = history + [((file.name,), None)]
        task_history = task_history + [((file.name,), None)]
        return history, task_history

    def reset_user_input():
        return gr.update(value='')

    def reset_state(_chatbot, task_history):
        task_history.clear()
        _chatbot.clear()
        _gc()
        return []

    with gr.Blocks() as demo:
        gr.Markdown("""\
<p align="center"><img src="https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png" style="height: 80px"/><p>"""
                    )
        gr.Markdown("""<center><font size=8>Qwen2.5-VL</center>""")
        gr.Markdown("""\
<center><font size=3>This WebUI is based on Qwen2.5-VL, developed by Alibaba Cloud.</center>""")
        gr.Markdown("""<center><font size=3>æœ¬WebUIåŸºäºQwen2.5-VLï¿½?/center>""")

        chatbot = gr.Chatbot(label='Qwen2.5-VL', elem_classes='control-height', height=500)
        query = gr.Textbox(lines=2, label='Input')
        task_history = gr.State([])

        with gr.Row():
            addfile_btn = gr.UploadButton('ğŸ“ Upload (ä¸Šä¼ æ–‡ä»¶)', file_types=['image', 'video'])
            submit_btn = gr.Button('ğŸš€ Submit (å‘é€)')
            regen_btn = gr.Button('ğŸ¤”ï¸ Regenerate (é‡è¯•)')
            empty_bin = gr.Button('ğŸ§¹ Clear History (æ¸…é™¤å†å²)')

        submit_btn.click(add_text, [chatbot, task_history, query],
                         [chatbot, task_history]).then(predict, [chatbot, task_history], [chatbot], show_progress=True)
        submit_btn.click(reset_user_input, [], [query])
        empty_bin.click(reset_state, [chatbot, task_history], [chatbot], show_progress=True)
        regen_btn.click(regenerate, [chatbot, task_history], [chatbot], show_progress=True)
        addfile_btn.upload(add_file, [chatbot, task_history, addfile_btn], [chatbot, task_history], show_progress=True)

        gr.Markdown("""\
<font size=2>Note: This demo is governed by the original license of Qwen2.5-VL. \
We strongly advise users not to knowingly generate or allow others to knowingly generate harmful content, \
including hate speech, violence, pornography, deception, etc. \
(æ³¨ï¼šæœ¬æ¼”ç¤ºå—Qwen2.5-VLçš„è®¸å¯åè®®é™åˆ¶ã€‚æˆ‘ä»¬å¼ºçƒˆå»ºè®®ï¼Œç”¨æˆ·ä¸åº”ä¼ æ’­åŠä¸åº”å…è®¸ä»–äººä¼ æ’­ä»¥ä¸‹å†…å®¹ï¼Œ\
åŒ…æ‹¬ä½†ä¸é™äºä»‡æ¨è¨€è®ºã€æš´åŠ›ã€è‰²æƒ…ã€æ¬ºè¯ˆç›¸å…³çš„æœ‰å®³ä¿¡æ¯ï¿½?""")

    demo.queue().launch(
            share=args.share,
            inbrowser=args.inbrowser,
            server_port=args.server_port,
            server_name=args.server_name,
    )


def main():
    args = _get_args()
    engine_args = AsyncEngineArgs(
            model=args.checkpoint_path,
            limit_mm_per_prompt={"image": args.max_image_nums, "video": 10},
            dtype=args.dtype,
            gpu_memory_utilization=args.gpu_memory_utilization,
            enforce_eager=args.enforce_eager,
            max_model_len=args.max_model_len,
            tensor_parallel_size=args.tensor_parallel_size,
            pipeline_parallel_size=args.pipeline_parallel_size
    )
    model = AsyncLLMEngine.from_engine_args(engine_args)
    processor = AutoProcessor.from_pretrained(args.checkpoint_path)
    _launch_demo(args, model, processor)


# ç®€å•æè¿°ä¸‹è¿™å¼ å›¾ç‰‡
if __name__ == '__main__':
    main()


```

åœ¨2å° 2080tiæœºå™¨,å…±16å¼ æ˜¾å¡ä¸Šéƒ¨ç½²Qwen2.5-VL-72B-Instructæ¨¡å‹, æ˜¾å­˜å…±å ç”¨çº¦149GB
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/afabc2e89c144b3a9681e9261092acfb.png)

å…³äºbfloat16ä¸float16çš„åŒºåˆ«
  
bfloat16åªæ”¯æŒAmpereæ¶æ„,å…¬å¸å¤§å¤šæ•°æ—§å¡2080ti, rtx titanéƒ½ç”¨ä¸äº†. äºæ˜¯ç‰¹åœ°åœ¨RTX 4060ä¸Šæµ‹è¯•è¿™ä¸¤ä¸ªæ•°æ®ç±»å‹å¯¹æ˜¾å­˜çš„å ç”¨æƒ…å†µ.

ä¸‹å›¾æ˜¯åœ¨å•æœº4å¼ 4060æ˜¾å¡ä¸Š,åˆ†åˆ«ä½¿ç”¨ä¸¤ç§ç±»å‹éƒ¨ç½²Qwen2.5-VL-7B-Instructçš„æ˜¾å­˜å ç”¨æƒ…å†µ:
  
(27966-17938)/1024=9.8GB

bfloat16è¦æ¯”float16è¦æ•´ä½“å¢åŠ çº¦9.8Gæ˜¾å­˜. å½“æ˜¾å­˜èµ„æºç´§å¼ æ—¶, å»ºè®®ä½¿ç”¨float16ç±»å‹.
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/c64279b8e08f4bdc859e2c247d4056a6.png)