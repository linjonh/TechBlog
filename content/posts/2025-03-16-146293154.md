---
layout: post
title: "机器学习扫盲系列1-序"
date: 2025-03-16 21:26:26 +08:00
description: "深度学习扫盲系列"
keywords: "机器学习扫盲系列（1） - 序"
categories: ['机器学习扫盲系列从机器学习到大模型之路']
tags: ['深度学习', '人工智能']
artid: "146293154"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146293154
    alt: "机器学习扫盲系列1-序"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146293154
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146293154
cover: https://bing.ee123.net/img/rand?artid=146293154
image: https://bing.ee123.net/img/rand?artid=146293154
img: https://bing.ee123.net/img/rand?artid=146293154
---

# 机器学习扫盲系列（1） - 序
## 序
### 为什么要写这一系列？
#### AI 概念科普
在各种AI概念铺天盖地向我们袭来的今天，我们作为其他方向的工作者，如果想转行做深度学习/模型训练算法工程师的难度很大，基本是不太可能，但是能够理解AI中的一些基本概念，如正向传播、反向传播、卷积、Transformer、Attention等，对我们本身的工作和思维的锻炼也是很有帮助的，也能让我们顺着AI大潮一起前进，不至于完全脱节。
另一方面，笔者自己作为平台开发者，从一开始学习神经网络时也遇到入门较困难的问题，希望这一系列能从一名普通平台开发者的角度带大家了解AI相关的概念。
#### 原理性思维
随着技术迭代越来越快，软件系统的复杂度越来越高，我们所面对的场景也越来越多，各种工具应用层出不穷，各种AI概念方法也是扑面而来，对开发者来说，学习的速度似乎永远跟不上知识爆炸的速度，那怎么办？
其实，如果能够意识到，原理性知识是可控的这一点，我们就不必慌张。也就是说，掌握知识背后的原理可以大幅度降低我们对于知识的记忆量，知识量是爆炸的，但是原理绝对是可控的！
对于AI概念也是一样，虽然现在各种神经网络结构和训练方法层出不穷，但绝对离不开很久之前一些原理性的工作，比如卷积神经网络其实本质也是从图像处理发展而来，如果你了解视频图像的压缩方法，其实也不难理解卷积。
#### 转行
作为一个已经有过多年平台开发经验的开发者，现在转行去做模型训练算法工程师的难度很大，但是否有其他相关方向值得我们探索或者转行呢？笔者认为，跟我们工程开发最相关的AI
方向有两个，一个是模型训练与推理的优化，另一个是AI Agent的开发。这两者其实都依赖于本系列所要向大家展示的AI基础概念。
#### 立flag
作为普通的平台开发者，可能会担心被汹涌的AI浪潮席卷，所以也希望通过这一系列大家一起交流想法，立个持续更新的flag，欢迎评论，请轻喷。
### 这一系列的计划
这一系列希望能从以下两方面带大家熟悉 AI 相关的概念。两者没有先后顺序，互为补充。
#### 神经网络
1. 基础知识入门，包括链式法则、反向传播等基础的神经网络概念
2. 经典网络介绍，包括rnn、cnn等
3. 大模型概念介绍，包括 Transformer、Attention机制等
#### 训练框架
1. pytorch的基本功能，用法，原理等， 可能穿插cuda算子开发的相关内容
2. mmengine训练框架的基本功能，用法，原理等