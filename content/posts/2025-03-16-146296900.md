---
layout: post
title: "LLM论文笔记-25-Chain-of-Thought-Reasoning-without-Prompting"
date: 2025-03-16 16:22:50 +0800
description: "注：本系列不包括基础的知识点讲解，为笔记/大纲性质而非教程，用于论文知识点和思想和快速记忆和回顾，更多细节建议阅读论文原文。1. LLMs 不需要prompting就可以生成链式推理路径，prompting只是将这些能力显性化的一种手段。2. cot path 往往与更高的model confidence相关，可以用作可靠性的metric。3. 探索多样化的解码路径能有效挖掘模型的内在推理能力，而不仅仅依赖于模型规模或训练数据的多样性。模型未经过指令调优时的推理能力缺陷，并在指令调优的模型中。"
keywords: "LLM论文笔记 25: Chain-of-Thought Reasoning without Prompting"
categories: ['未分类']
tags: ['论文阅读', '深度学习', '机器学习', '人工智能', 'Chatgpt']
artid: "146296900"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146296900
    alt: "LLM论文笔记-25-Chain-of-Thought-Reasoning-without-Prompting"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146296900
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146296900
cover: https://bing.ee123.net/img/rand?artid=146296900
image: https://bing.ee123.net/img/rand?artid=146296900
img: https://bing.ee123.net/img/rand?artid=146296900
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     LLM论文笔记 25: Chain-of-Thought Reasoning without Prompting
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <blockquote>
     <ul>
      <li>
       Arxiv日期：2024.5.31
      </li>
      <li>
       机构：Google DeepMind
      </li>
     </ul>
    </blockquote>
    <h3>
     关键词
    </h3>
    <ul>
     <li>
      cot-decoding
     </li>
     <li>
      推理路径
     </li>
     <li>
      pretrain
     </li>
    </ul>
    <p>
    </p>
    <h3>
     核心结论
    </h3>
    <p>
     1. LLMs 不需要prompting就可以生成链式推理路径，prompting只是将这些能力显性化的一种手段
    </p>
    <p>
     2. cot path 往往与更高的model confidence相关，可以用作可靠性的metric
    </p>
    <p>
     3. 探索多样化的解码路径能有效挖掘模型的内在推理能力，而不仅仅依赖于模型规模或训练数据的多样性
    </p>
    <p>
     4.
     <span style="color:#fe2c24">
      CoT-Decoding
     </span>
     可以
     <strong>
      弥补
     </strong>
     模型未经过指令调优时的推理能力缺陷，并在指令调优的模型中
     <strong>
      进一步优化
     </strong>
     性能
    </p>
    <p>
     5. Cot-Decoding适用于多种任务和语言模型，显示出显著的通用性和鲁棒性
    </p>
    <p>
    </p>
    <h3>
     主要方法
    </h3>
    <p>
     （验证了内在推理能力的存在）使用pretrain模型，
     <strong>
      不使用greedy decoding，而是在第一个token预测使用top-k
     </strong>
     ：
     <span style="color:#fe2c24">
      <strong>
       发现内化cot推理能力，且带cot的答案置信度更高
      </strong>
     </span>
    </p>
    <p>
     <img alt="" height="518" src="https://i-blog.csdnimg.cn/direct/f66940e479bd4f13a36ecf19cee83217.png" width="1280"/>
    </p>
    <p>
     置信度衡量标准：
    </p>
    <p>
     <strong>
      answer中每一个token在NTP时当前token和下一个token的概率差异
     </strong>
    </p>
    <p class="img-center">
     <img alt="" height="63" src="https://i-blog.csdnimg.cn/direct/78a19f54dc3f4c6e9229c2a215accbb2.png" width="393"/>
    </p>
    <p>
    </p>
    <blockquote>
     <p>
      注：本系列不包括基础的知识点讲解，为笔记/大纲性质而非教程，用于论文知识点和思想和快速记忆和回顾，更多细节建议阅读论文原文
     </p>
    </blockquote>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36353330353134322f:61727469636c652f64657461696c732f313436323936393030" class_="artid" style="display:none">
 </p>
</div>


