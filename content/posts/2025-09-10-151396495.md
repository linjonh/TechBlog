---
layout: post
title: "用生成式方法摸清动态页面小红书热点追踪实践"
date: 2025-09-10T10:28:43+0800
description: "本文介绍了使用浏览器自动化工具（Selenium/Playwright）结合代理服务和生成式方法抓取小红书等平台数据的解决方案。文章详细说明了环境安装、代理设置、浏览器启动等前期准备步骤，并提供了基于Playwright的完整代码示例，包括动态页面获取和智能解析方法。针对常见问题给出了应对策略，提醒需要注意滚动加载、登录验证等关键点。最后指出该方案可扩展用于其他社交平台数据采集，为市场热点分析提供有效支持。"
keywords: "用生成式方法“摸清”动态页面：小红书热点追踪实践"
categories: ['爬虫代理', 'Seleuium', 'Python']
tags: ['爬虫代理', '热点追踪', '浏览器自动化', '小红书', '动态网页', 'Selenium', 'Playwright']
artid: "151396495"
arturl: "https://blog.csdn.net/ip16yun/article/details/151396495"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151396495
    alt: "用生成式方法摸清动态页面小红书热点追踪实践"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151396495
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151396495
cover: https://bing.ee123.net/img/rand?artid=151396495
image: https://bing.ee123.net/img/rand?artid=151396495
img: https://bing.ee123.net/img/rand?artid=151396495
---



# 用生成式方法“摸清”动态页面：小红书热点追踪实践



## 爬虫代理

### 开篇思路

很多刚入门做数据分析的人，在处理热门平台（例如小红书）时都会遇到类似的困难：

* 页面并不是传统的静态 HTML，而是需要脚本渲染后才能看到核心信息；
* 标签结构更新很快，前几天能用的规则可能很快失效；
* 初学者往往不知道该如何稳妥地获取标题、点赞数、评论等关键指标。

这篇文章结合一个真实的业务场景——**市场热点追踪**，分享如何通过 **浏览器自动化工具（Selenium/Playwright）** 配合 **生成式方法** 来推断页面结构，并抓取出有价值的数据。我们还会顺便引入代理服务，保证访问过程更加稳定。

### 前期准备

如果你打算自己动手，可以先具备以下条件：

1. 基础的 Python 编程知识；
2. 对浏览器驱动或 Playwright 有大致了解；
3. 能够配置代理账号，确保访问不会轻易被封。

### 实操过程

#### 1. 环境安装

你可以选择 Selenium 或 Playwright。

```bash
pip install selenium

```

或：

```bash
pip install playwright
playwright install

```

#### 2. 设置代理

以常见的爬虫代理服务为例，写法类似：

```python
#设置爬虫代理（参考亿牛云示例www.16yun.cn）
proxy_host = "proxy.16yun.cn"
proxy_port = "3100"
proxy_user = "16YUN"
proxy_pass = "16IP"

proxy = f"http://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}"

```

这样请求时就会通过中转，提高稳定性。

#### 3. 启动浏览器

**Selenium 方式**

```python
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument(f'--proxy-server={proxy}')
chrome_options.add_argument("--headless")

driver = webdriver.Chrome(options=chrome_options)
driver.get("https://www.xiaohongshu.com/explore")

```

**Playwright 方式**

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(proxy={"server": proxy}, headless=True)
    page = browser.new_page()
    page.goto("https://www.xiaohongshu.com/explore")

```

#### 4. 生成式推断页面结构

与其死记硬背固定 XPath，不如让程序基于上下文“猜测”可能的路径。示例函数：

```python
def ai_like_guess(html_snippet, target="title"):
    if target == "title":
        return '//div[contains(@class,"note-title")]/text()'
    elif target == "likes":
        return '//span[contains(@class,"like")]/text()'
    elif target == "comments":
        return '//span[contains(@class,"comment")]/text()'
    return None

```

#### 5. 组合代码（Playwright 示例）

```python
# -*- coding: utf-8 -*-
from playwright.sync_api import sync_playwright
from lxml import etree

#设置爬虫代理（参考亿牛云示例www.16yun.cn）
proxy_host = "proxy.16yun.cn"
proxy_port = "3100"
proxy_user = "16YUN"
proxy_pass = "16IP"
proxy = f"http://{proxy_user}:{proxy_pass}@{proxy_host}:{proxy_port}"

def ai_like_guess(html_snippet, target="title"):
    if target == "title":
        return '//div[contains(@class,"note-title")]/text()'
    elif target == "likes":
        return '//span[contains(@class,"like")]/text()'
    elif target == "comments":
        return '//span[contains(@class,"comment")]/text()'
    return None

def scrape_hot_notes(url="https://www.xiaohongshu.com/explore"):
    with sync_playwright() as p:
        browser = p.chromium.launch(proxy={"server": proxy}, headless=True)
        page = browser.new_page()
        page.goto(url, timeout=30000)
        
        html = page.content()
        dom = etree.HTML(html)

        titles = dom.xpath(ai_like_guess(html, "title"))
        likes = dom.xpath(ai_like_guess(html, "likes"))
        comments = dom.xpath(ai_like_guess(html, "comments"))

        results = []
        for i in range(min(len(titles), len(likes), len(comments))):
            results.append({
                "标题": titles[i].strip(),
                "点赞数": likes[i].strip(),
                "评论数": comments[i].strip()
            })
        
        return results

if __name__ == "__main__":
    data = scrape_hot_notes()
    for d in data[:5]:
        print(d)

```

### 常见问题与对策

* **只抓到空白页面**：动态内容没加载，需要增加等待逻辑。
* **代理不可用**：检查账号密码或尝试更换。
* **解析不到数据**：生成式函数可能猜错，需要人工兜底。

### 踩坑提醒

很多人一开始会犯几个典型错误：

* 只停留在首页，忽略了滚动加载；
* 没有处理登录问题，导致只能看到部分内容；
* 盲目依赖自动生成的规则，没有写备用方案。

更稳妥的做法是：模拟滚动加载更多条目，提前准备 Cookie，人工校验生成规则是否靠谱。

### 后续提升

* 加入关键词搜索，获取某类产品的热度；
* 导出数据，用 Excel 或可视化工具做统计；
* 进一步做评论情感分类，支持市场调研。

### 收尾小结

整套方案的思路是：**用浏览器驱动拿到动态页面 → 借助生成式推断结构 → 结合代理提高成功率**。  
它最大的价值在于，可以快速获取社交平台上的热门信息，并做基础的市场热点分析。

如果你把思路拓展一下，这套方法同样适用于其他平台，比如微博热搜、短视频评论区等。



