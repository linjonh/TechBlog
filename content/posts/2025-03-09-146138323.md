---
layout: post
title: "2022IJCAISparseTT,使用稀疏Transformers进行视觉跟踪"
date: 2025-03-09 20:46:28 +0800
description: "在这项工作中，作者通过一种新颖的稀疏 Transformer 跟踪器来增强基于 Transformer 的视觉跟踪。Transformer 中的稀疏自注意力机制缓解了普通自注意力机制因集中于全局背景而忽略最相关信息的问题，从而突出了搜索区域中的潜在目标。此外，引入双头预测器来提高分类和回归的准确性。实验表明，本文方法在以实时速度运行时，可以在多个数据集上显著优于最先进的方法，这证明了该方法的优越性和适用性。此外，本文方法的训练时间仅为 TransT 的 25%。总的来说，这是进一步研究的新的良好基线。"
keywords: "2022IJCAI：SparseTT，使用稀疏Transformers进行视觉跟踪"
categories: ['目标检测跟踪论文精读专栏']
tags: ['计算机视觉', '视觉跟踪', '目标跟踪', '目标检测', 'Transformer']
artid: "146138323"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146138323
    alt: "2022IJCAISparseTT,使用稀疏Transformers进行视觉跟踪"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146138323
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146138323
cover: https://bing.ee123.net/img/rand?artid=146138323
image: https://bing.ee123.net/img/rand?artid=146138323
img: https://bing.ee123.net/img/rand?artid=146138323
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     2022IJCAI：SparseTT，使用稀疏Transformers进行视觉跟踪
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <div>
     <div style="margin-left:0in">
      <div style="margin-left:0in">
       <p style="margin-left:0; margin-right:0">
        <strong>
         原文标题：SparseTT: Visual Tracking with Sparse Transformers
        </strong>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         中文标题：SparseTT：使用稀疏Transformers进行视觉跟踪
        </strong>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         代码地址：
        </strong>
        <a href="https://github.com/fzh0917/SparseTT" title='GitHub - fzh0917/SparseTT: The official implementation for paper "SparseTT: Visual Tracking with Sparse Transformers"'>
         GitHub - fzh0917/SparseTT: The official implementation for paper "SparseTT: Visual Tracking with Sparse Transformers"
        </a>
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h2 style="margin-left:0px; margin-right:0px">
        <strong>
         Abstract
        </strong>
       </h2>
       <p style="margin-left:0; margin-right:0">
        Transformers 已成功应用于视觉跟踪任务并显着提升了跟踪性能。 旨在模拟远程依赖关系的自注意力机制是 Transformers 成功的关键。然而，自注意力缺乏对搜索区域中最相关信息的关注，因此很容易被背景分散注意力。在本文中，我们通过将最相关的信息集中在搜索区域中，通过稀疏注意力机制来缓解这个问题，从而实现更准确的跟踪。 此外，我们引入了双头预测器来提高前景背景分类和目标边界框回归的准确性，从而进一步提高了跟踪性能。大量实验表明，在没有附加功能的情况下，我们的方法在以 40 FPS 运行时，显着优于 LaSOT、GOT-10k、TrackingNet 和 UAV123 上最先进的方法。值得注意的是，与 TransT 相比，我们方法的训练时间减少了 75%。
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h2 style="margin-left:0px; margin-right:0px">
        <strong>
         1
        </strong>
        <strong>
         .
        </strong>
        <strong>
         Introduction
        </strong>
       </h2>
       <p style="margin-left:0; margin-right:0">
        视觉跟踪旨在根据目标的初始状态预测其未来状态。它的应用非常广泛，例如人机交互、视频监控、自动驾驶等。大多数现有方法通过序列预测框架解决跟踪问题，其中它们根据初始状态和先前状态估计当前状态。因此，在每个时间片中给出准确的状态非常重要，否则错误会累积并导致跟踪失败。人们付出了巨大的努力来提高跟踪精度，即目标边界框的精度。然而，目标变形、部分遮挡和尺度变化等挑战仍然是阻碍它们完美跟踪的巨大障碍。原因可能是这些方法大多采用互相关运算来衡量目标模板与搜索区域之间的相似性，这可能会陷入局部最优。
       </p>
       <p style="margin-left:0; margin-right:0">
        最近，TransT [Chen et al., 2021] 和 DTT [Yu et al., 2021] 通过用 Transformer [Vaswani et al., 2017] 替换相关性来提高跟踪性能。然而，用 Transformers 构建跟踪器会带来一个新问题：Transformers 中 self-attention 的全局视角导致主要信息（例如搜索区域中的目标）聚焦不足，而次要信息（例如搜索区域中的背景）过度聚焦，使前景和背景之间的边缘区域变得模糊，从而降低跟踪性能。
       </p>
       <p style="margin-left:0; margin-right:0">
        在本文中，我们通过专注于搜索区域最相关的信息来解决这个问题，这是通过稀疏 Transformer 实现的。与之前作品中使用的普通　Transformer 不同，稀疏 Transformer 旨在关注主要信息，即使在严重的目标变形、部分遮挡、尺度变化等情况下，也使目标更具判别力，目标的边界框也更加准确 ，如图1所示。
       </p>
       <p class="img-center">
        <img alt="" height="337" src="https://i-blog.csdnimg.cn/direct/af23d8ec09ff4c26b80484a23c9442e2.png" width="472"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        总之，这项工作的主要贡献有三个方面。
       </p>
       <p style="margin-left:0; margin-right:0">
        （１）我们提出了一个目标聚焦网络，它能够
        <strong>
         聚焦搜索区域中感兴趣的目标，并突出显示最相关信息的特征
        </strong>
        ，以便更好地估计目标的状态。
       </p>
       <p style="margin-left:0; margin-right:0">
        （２）提出了一种基于稀疏Transformer的Siamese跟踪框架，该框架
        <strong>
         具有很强的处理目标变形、局部遮挡、尺度变化等问题的能力
        </strong>
        。
       </p>
       <p style="margin-left:0; margin-right:0">
        （３）广泛的实验表明，我们的方法优于在LaSOT，GOT10k，TrackingNet和UAV123上最先进的方法，同时以40 FPS运行，证明了我们方法的优越性。
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h2 style="margin-left:0px; margin-right:0px">
        <strong>
         2. Rel
        </strong>
        <strong>
         ated Work
        </strong>
       </h2>
       <p style="margin-left:0; margin-right:0">
        <strong>
         Siamese Trackers.
        </strong>
        在Siamese视觉跟踪器中，
        <strong>
         相互关系被广泛用于衡量目标模板与搜索区域之间的相似度
        </strong>
        。如
        <strong>
         朴素互相关
        </strong>
        [Bertinetto等人，2016]、
        <strong>
         深度互相关
        </strong>
        [Li等人，2019;Xu et al.，2020]，逐像素互关[Yan et al.， 2021b]，像素到全局匹配互关[Liao et al.，2020]等。然而，相互关联执行的是局部线性匹配过程，容易陷入局部最优[Chen等，2021]。此外，相互关联破坏了输入特征的语义信息，这不利于准确感知目标边界。大多数Siamese跟踪器在处理目标变形、局部遮挡、尺度变化等方面仍然存在困难。
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         Transformer in Visual Tracking.
        </strong>
        近年来，Transformer 已成功应用于视觉跟踪领域。STARK [Yan等人，2021a]借鉴DETR [Carion等人，2020]的灵感，将目标跟踪作为一个边界框预测问题，并使用编码器-解码器transformer来解决这个问题，其中
        <strong>
         编码器对目标和搜索区域之间的全局时空特征依赖关系进行建模
        </strong>
        ，
        <strong>
         解码器学习查询嵌入来预测目标的空间位置
        </strong>
        。它在视觉跟踪方面取得了优异的性能。TrDiMP [Wang等人，2021]设计了一个类似Siamese的跟踪管道，其中两个分支分别由CNN主干网后接transformer编码器和transformer解码器构建。Transformer 用于增强目标模板和搜索区域。与之前的Siamese跟踪器类似，TrDiMP 应用互相关来测量目标模板和搜索区域之间的相似性，这可能会妨碍跟踪器的高性能跟踪。注意到这一缺点，TransT 和 DTT 提出用 Transformer 代替互相关，从而生成融合特征而不是响应分数。由于融合特征包含比响应分数更丰富的语义信息，因此这些方法比以前的连体跟踪器实现了更准确的跟踪。
       </p>
       <p style="margin-left:0; margin-right:0">
        Transformers 中的
        <strong>
         Self-attention 专门用于对远程依赖关系进行建模，使其擅长捕获全局信息
        </strong>
        ，
        <strong>
         但缺乏对搜索区域中最相关信息的关注
        </strong>
        。为了进一步增强 Transformer 跟踪器，我们通过稀疏注意力机制缓解了上述自注意力的缺点。这个想法的灵感来自[Zhao et al., 2019]。 我们采用了[Zhao et al., 2019]中的稀疏Transformer 来适应视觉跟踪任务，并提出了一种带有编码器-解码器稀疏 Transformer 的全新端到端Siamese跟踪器。在稀疏注意力机制的驱动下，稀疏 Transformer 聚焦于搜索区域中最相关的信息，从而更有效地抑制干扰跟踪的干扰背景。
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h2 style="margin-left:0px; margin-right:0px">
        <strong>
         3. Met
        </strong>
        <strong>
         hod
        </strong>
       </h2>
       <p style="margin-left:0; margin-right:0">
        我们提出了一种用于视觉跟踪的Siamese架构，该架构由特征提取网络、目标聚焦网络和双头预测器组成，如下图所示。
        <strong>
         特征提取网络是一个权重共享的骨干网络
        </strong>
        。
        <strong>
         利用稀疏transformer构建目标聚焦网络
        </strong>
        ，
        <strong>
         生成目标聚焦特征
        </strong>
        。
        <strong>
         双头预测器区分前景和背景，输出目标的边界框
        </strong>
        。
       </p>
       <p class="img-center">
        <img alt="" height="237" src="https://i-blog.csdnimg.cn/direct/baac753a35a540b599b8d40c7bf8b8b0.png" width="537"/>
       </p>
       <h3 style="margin-left:0px; margin-right:0px">
        <strong>
         3.1 Target Focus Network
        </strong>
       </h3>
       <p style="margin-left:0; margin-right:0">
        目标聚焦网络采用稀疏Transformer构建，具有编码器-解码器架构，如下图所示。
        <strong>
         编码器负责对目标模板特征进行编码
        </strong>
        。
        <strong>
         解码器负责解码搜索区域特征以生成目标聚焦特征
        </strong>
        。
       </p>
       <p class="img-center">
        <img alt="" height="406" src="https://i-blog.csdnimg.cn/direct/efdbc6b69aaa423ea977638bf26e6873.png" width="397"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         Encoder.
        </strong>
        在目标聚焦网络中，编码器是一个重要且必需的组成部分。它由N个编码器层组成，其中
        <strong>
         每个编码器层都将其前一个编码器层的输出作为输入
        </strong>
        。 注意，为了使网络具有空间位置信息的感知，我们在目标模板特征中加入空间位置编码，并将其加入到编码器中。因此，
        <strong>
         第一编码器层以具有空间位置编码的目标模板特征作为输入
        </strong>
        。简而言之，它可以正式表示为：
       </p>
       <p class="img-center">
        <img alt="" height="65" src="https://i-blog.csdnimg.cn/direct/a2ab45be869d4b5baabd3a3006732afb.png" width="501"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        其中Z∈RHtWt×C表示目标模板特征，Penc∈RHtWt×C表示空间位置编码，f(i)enc表示第i层编码器，Y(i−1)enc∈RHtWt×C表示第（i−1）层编码器的输出。Ht和Wt分别为目标模板的特征图的高度和宽度。
       </p>
       <p style="margin-left:0; margin-right:0">
        在每个编码器层中，使用多头自注意力（MSA）来显式建模目标模板特征的所有像素对之间的关​​系。
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         Decoder.
        </strong>
        与编码器类似，解码器由 M 个解码器层组成。然而，与编码器层不同的是，
        <strong>
         每个解码器层不仅输入经过空间位置编码的搜索区域特征或其前一解码器层的输出，还输入编码器输出的编码后的目标模板特征
        </strong>
        。简而言之，它可以正式表示为：
       </p>
       <p class="img-center">
        <img alt="" height="92" src="https://i-blog.csdnimg.cn/direct/9e40042bb3c14c7994a70ec1d9a8491d.png" width="471"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        其中，X∈RHsWs×C表示搜索区域特征，Pdec∈RHsWs×C表示空间位置编码，y(n)enc∈RHtWt×C表示编码器输出的编码后的目标模板特征，f(i-1)dec表示第 i - 1层解码器，Y(i−1)dec∈RHsWs×C表示第(i−1)层解码器的输出。Hs和Ws分别为搜索区域特征图的高度和宽度。
       </p>
       <p style="margin-left:0; margin-right:0">
        与普通 Transformer 的解码器层不同，所提出的稀疏 Transformer 的每个解码器层首先使用
        <strong>
         稀疏多头自注意力
        </strong>
        <strong>
         (
        </strong>
        <strong>
         SMSA
        </strong>
        <strong>
         )
        </strong>
        计算 X 上的自注意力，然后使用朴素多头交叉注意(MCA)计算 Z 和 X 之间的交叉注意力。其他操作与普通 Transformer 的解码器层相同。形式上，所提出的稀疏 Transformer 的每个解码器层可以表示为：
       </p>
       <p class="img-center">
        <img alt="" height="108" src="https://i-blog.csdnimg.cn/direct/99856ad3e99d421c89d734f6eedb9e5d.png" width="390"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         Sparse Multi-Head Self-Attention.
        </strong>
        稀疏多头自注意力旨在提高前景-背景的辨别力并减轻前景边缘区域的模糊性。具体来说，在普通的MSA中，
        <strong>
         注意力特征的每个像素值是由输入特征的所有像素值计算的，这使得前景的边缘区域变得模糊
        </strong>
        。在我们
        <strong>
         提出的 SMSA 中，注意力特征的每个像素值仅由与其最相似的 K 个像素值决定，这使得前景更加集中，并且前景的边缘区域更具辨别力
        </strong>
        。
       </p>
       <p style="margin-left:0; margin-right:0">
        具体来说，如图4中间所示，给定
        <strong>
         一个查询
        </strong>
        <strong>
         Q
        </strong>
        <strong>
         ∈R
        </strong>
        <strong>
         HW×C
        </strong>
        <strong>
         ，一个键
        </strong>
        <strong>
         K
        </strong>
        <strong>
         ∈R
        </strong>
        <strong>
         C×H
        </strong>
        <strong>
         'W'
        </strong>
        <strong>
         ，一个值
        </strong>
        <strong>
         V
        </strong>
        <strong>
         ∈R
        </strong>
        <strong>
         H
        </strong>
        <strong>
         'W'
        </strong>
        <strong>
         ×C
        </strong>
        ，我们
        <strong>
         首先计算查询
        </strong>
        <strong>
         Q
        </strong>
        <strong>
         和键
        </strong>
        <strong>
         K
        </strong>
        <strong>
         之间所有像素对的相似度得到相似度矩阵
        </strong>
        ，并在相似度矩阵中屏蔽掉不必要的标记。然后，与图4左侧所示的朴素缩放点积注意力不同，我们
        <strong>
         只使用softmax函数对相似性矩阵每行中最大的K个元素进行归一化
        </strong>
        。对于其他元素，我们将它们替换为0。最后用矩阵乘法将相似矩阵和值相乘得到最终结果。右图中可以看到，
        <strong>
         朴素缩放点积注意力放大了相对较小的相似性权重，这使得输出特征容易受到噪声和分散注意力的背景的影响
        </strong>
        。然而，这个问题可以通过稀疏缩放点积注意力来显著缓解。
       </p>
       <p class="img-center">
        <img alt="" height="181" src="https://i-blog.csdnimg.cn/direct/689e25cfc9c240c9990a01fe5558b894.png" width="557"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         图 4：左边是 MSA 中缩放点积自注意力的图示，中间是 SMSA 中稀疏缩放点积自注意力的图示，其中函数 scatter 意味着将给定值填充到给定索引处的
        </strong>
        <strong>
         0
        </strong>
        <strong>
         值
        </strong>
        <strong>
         矩阵中。右上和右下分别是在朴素缩放点积注意力和稀疏缩放点积注意力中标准化相似性矩阵的行向量的示例。
        </strong>
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h3 style="margin-left:0px; margin-right:0px">
        <strong>
         3.2
        </strong>
        <strong>
         .
        </strong>
        <strong>
         Double-Head predictor
        </strong>
       </h3>
       <p style="margin-left:0; margin-right:0">
        现有的跟踪器大多采用全连接网络或卷积网络来进行前景和背景的分类以及目标边界框的回归，而没有根据分类和回归任务的特点对头部网络进行深入的分析或设计。我们
        <strong>
         引入双头预测器来提高分类和回归的准确性
        </strong>
        。具体来说，如下图所示，它由
        <strong>
         一个由两个全连接层组成的
        </strong>
        <strong>
         fc-head
        </strong>
        <strong>
         和一个由L个卷积块组成的
        </strong>
        <strong>
         conv-head
        </strong>
        组成。在推理阶段，对于分类任务，融合 fc-head 输出的分类分数和 conv-head 输出的分类分数；对于回归任务，只采用 conv-head 输出的预测偏移量。
       </p>
       <p class="img-center">
        <img alt="" height="303" src="https://i-blog.csdnimg.cn/direct/8821ac80925d48599fab91ccb558b48a.png" width="372"/>
       </p>
       <h3 style="margin-left:0px; margin-right:0px">
        <strong>
         3.3
        </strong>
        <strong>
         .
        </strong>
        <strong>
         Training Loss
        </strong>
       </h3>
       <p style="margin-left:0; margin-right:0">
        为了端到端训练整个网络，
        <strong>
         目标函数为分类损失和回归损失的加权和
        </strong>
        ，如下所示:
       </p>
       <p class="img-center">
        <img alt="" height="68" src="https://i-blog.csdnimg.cn/direct/5c9279cf05e449f889e14411a4899371.png" width="449"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        其中 ωfc、λfc、ωconv 和 λconv 是超参数。实际上，我们设置 ωfc = 2.0，λfc = 0.7，ωconv = 2.5，λconv = 0.8。函数Lclass fc和Lclass conv都是通过focal loss实现的，函数Lbox fc和Lbox conv都是通过IoU loss实现的。
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h2 style="margin-left:0px; margin-right:0px">
        <strong>
         4
        </strong>
        <strong>
         .
        </strong>
        <strong>
         Experiments
        </strong>
       </h2>
       <h3 style="margin-left:0px; margin-right:0px">
        <strong>
         4.1
        </strong>
        <strong>
         .
        </strong>
        <strong>
         Implementation Details
        </strong>
       </h3>
       <p style="margin-left:0; margin-right:0">
        我们使用 Swin Transformer [Liu et al., 2021] (Swin-Tiny) 的微型版本作为主干φ。在MSA、SMSA和MCA中，注意力头数设置为8，FFN隐藏层通道数设置为2048，dropout率设置为0.1。编码器层数N和解码器层数M设置为2，并且SMSA中的稀疏度K设置为32，当K=H'W'，SMSA变成MSA。在双头预测器的 conv-head 中，第一个卷积块被设置为残差块，其他 L − 1 个被设置为瓶颈块，其中 L = 8。
       </p>
       <p style="margin-left:0; margin-right:0">
        我们使用AdamW优化器进行了20个epoch的训练。在每个epoch中，从所有训练数据集中采样600,000对图像。批量大小设置为32，学习率和权值衰减都设置为1 ×10−4。经过10次和15次的训练，学习率分别下降到1×10−5和1×10−6。在4块NVIDIA RTX 2080Ti gpu上，整个训练过程大约需要60个小时。需要注意的是，TransT的训练时间约为10天（240小时），是我们方法的4倍。
       </p>
       <h3 style="margin-left:0px; margin-right:0px">
        <strong>
         4.2.
        </strong>
        <strong>
         Comparison with the state-of-the-art
        </strong>
       </h3>
       <p style="margin-left:0; margin-right:0">
        <strong>
         LaSOT.
        </strong>
        LaSOT是一个具有高质量注释的大规模长片段数据集。它的测试集由280个序列组成，平均长度超过2500帧。我们在LaSOT的测试集上进行了评估，并与其他有竞争力的方法进行了比较。如下表所示，我们的方法在成功率、精度和归一化精度指标方面达到了最佳性能。
       </p>
       <p class="img-center">
        <img alt="" height="270" src="https://i-blog.csdnimg.cn/direct/d5373b3b157e42bd8f86b42e16d44f0d.png" width="436"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        我们还在具有变形、局部遮挡和尺度变化属性的测试子集上评估了我们的方法。结果如下表所示。可以看出，我们的方法在上述具有挑战性的场景中表现最好，明显优于其他竞争方法。这些挑战带来了确定目标精确边界的模糊性，使得跟踪器难以对目标边界框进行定位和估计。然而，我们的方法很好地应对了这些挑战。
       </p>
       <p class="img-center">
        <img alt="" height="143" src="https://i-blog.csdnimg.cn/direct/12a7893546e44727a32d3e3197efddbd.png" width="511"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         GOT-10k.
        </strong>
        GOT-10k包含9335个用于训练的序列和180个用于测试的序列。与其他数据集不同，GOT10k只允许使用训练集来训练跟踪器。我们按照这个协议来训练我们的方法，并在测试集上测试它，然后在下表中报告性能。我们看到，我们的方法大大超过了第二好的跟踪器TransT，这表明当带注释的训练数据有限时，我们的方法优于其他方法。
       </p>
       <p class="img-center">
        <img alt="" height="280" src="https://i-blog.csdnimg.cn/direct/5a44e99ec10e481bafc7c17dbb0ee00f.png" width="489"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         UAV123.
        </strong>
        UAV123是无人机拍摄的低空航拍数据集，包含123个序列，平均每个序列915帧。由于航空图像的特点，该数据集中的许多目标分辨率较低，并且容易出现快速运动和运动模糊。尽管如此，我们的方法仍然能够很好地应对这些挑战。如下表所示，我们的方法超越了其他竞争方法，在UAV123上达到了最先进的性能，证明了我们的方法的泛化和适用性。
       </p>
       <p class="img-center">
        <img alt="" height="167" src="https://i-blog.csdnimg.cn/direct/6f1f521476344361974423a44f5afee4.png" width="502"/>
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         OTB2015.
        </strong>
        OTB2015是一个经典的视觉跟踪测试数据集。 它包含100个短期跟踪序列，涵盖了目标变形、遮挡、尺度变化、旋转、光照变化、背景杂波等11种常见挑战。 我们报告了我们的方法在OTB2015上的性能。虽然标注不是很准确，如上表所示，但我们的方法仍然优于优秀的跟踪器TransT，达到了相当的性能。
       </p>
       <p style="margin-left:0; margin-right:0">
        <strong>
         TrackingNet.
        </strong>
        TrackingNet是一个大规模的数据集，它的测试集包括511个序列，涵盖了各种对象类别和跟踪场景。我们报告了在TrackingNet测试集上的性能。 如下表所示，我们的方法在成功率方面达到了最好的性能。
       </p>
       <p class="img-center">
        <img alt="" height="250" src="https://i-blog.csdnimg.cn/direct/1235aa03275e4ebc84816cf422ff3009.png" width="507"/>
       </p>
       <p style="margin-left:0; margin-right:0">
       </p>
       <h2 style="margin-left:0px; margin-right:0px">
        <strong>
         5
        </strong>
        <strong>
         . Conclusion
        </strong>
       </h2>
       <p style="margin-left:0; margin-right:0">
        在这项工作中，我们通过一种新颖的稀疏 Transformer 跟踪器来增强基于 Transformer 的视觉跟踪。Transformer 中的稀疏自注意力机制缓解了普通自注意力机制因集中于全局背景而忽略最相关信息的问题，从而突出了搜索区域中的潜在目标。此外，引入双头预测器来提高分类和回归的准确性。实验表明，我们的方法在以实时速度运行时，可以在多个数据集上显著优于最先进的方法，这证明了我们方法的优越性和适用性。此外，我们方法的训练时间仅为 TransT 的 25%。总的来说，这是进一步研究的新的良好基线。
       </p>
      </div>
     </div>
    </div>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36333239343530342f:61727469636c652f64657461696c732f313436313338333233" class_="artid" style="display:none">
 </p>
</div>


