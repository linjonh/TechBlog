---
layout: post
title: "聊一聊binder传递文件fd原理及新版本性能优化"
date: 2025-03-13 15:45:19 +0800
description: "上面可以看出与老版本巨大差别在于，新版本根本没有直接在binder_translate_fd中获取target_fd和install target_fd到file，只是构造了binder_txn_fd_fixup对象，赋值file后，然后加入到事物t的fd_fixups列表中。‌收集阶段‌：在源进程的 Binder 线程中，通过 binder_translate_fd 收集所有待映射的 fd，形成 fd_fixups 链表。下面看看真正干活的binder_apply_fd_fixups。"
keywords: "聊一聊binder传递文件fd原理及新版本性能优化"
categories: ['Binder']
tags: ['系统开发', '开发语言', 'Java', 'Framework', 'Binder', 'Android']
artid: "146224227"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146224227
    alt: "聊一聊binder传递文件fd原理及新版本性能优化"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146224227
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146224227
cover: https://bing.ee123.net/img/rand?artid=146224227
image: https://bing.ee123.net/img/rand?artid=146224227
img: https://bing.ee123.net/img/rand?artid=146224227
---

# 聊一聊binder传递文件fd原理及新版本性能优化
### 背景：
近期有vip学员朋友，问道了一个问题，那就是关于binder跨进程传递fd具体是怎么做到的呢？这块其实binder跨进程传递文件fd，本质的原理的实现在binder驱动中，所以给这个同学找一下相关的binder驱动中的代码，但是发现这块代码还和我以前看过的跨进程传递fd代码有较大的差别了，这个差别还不是简单改变一下方法，而是设计思路上就有变化，所以这里整理一下给大家分享出来。
### 老版本binder驱动代码传递fd
安卓kernel内核版本：
VERSION = 4
PATCHLEVEL = 4
SUBLEVEL = 302
EXTRAVERSION =
一般在client端调用调用了Parcel.writeFileDescriptor(fd)进行fd传递，接下来跨进程通信会一路调用到binder内核的binder\_transaction代码：
static void binder\_transaction(struct binder\_proc \*proc,
struct binder\_thread \*thread,
struct binder\_transaction\_data \*tr, int reply,
binder\_size\_t extra\_buffers\_size)
{
//省略
hdr = (struct binder\_object\_header \*)(t->buffer->data + \*offp);
off\_min = \*offp + object\_size;
switch (hdr->type) {
//省略
//针对binder传递的是fd类型
case BINDER\_TYPE\_FD: {
struct binder\_fd\_object \*fp = to\_binder\_fd\_object(hdr);
int target\_fd = binder\_translate\_fd(fp->fd, t, thread,
in\_reply\_to);
fp->pad\_binder = 0;
fp->fd = target\_fd;
} break;
老版本内核处理fd转换：
static int binder\_translate\_fd(int fd,
struct binder\_transaction \*t,
struct binder\_thread \*thread,
struct binder\_transaction \*in\_reply\_to)
{
struct binder\_proc \*proc = thread->proc;
struct binder\_proc \*target\_proc = t->to\_proc;
int target\_fd;
struct file \*file;
int ret;
//这个fd的是源进程的，通过fd获取到对应的file结构
file = fget(fd);
//这里只是安全坚持，file是否可以传递过去
ret = security\_binder\_transfer\_file(proc->cred, target\_proc->cred, file);
//这里会从目标进程中获取没有使用的fd，赋值给target\_fd
target\_fd = task\_get\_unused\_fd\_flags(target\_proc, O\_CLOEXEC);
//直接把从目标进程获取的空闲target\_fd与file进行绑定
task\_fd\_install(target\_proc, target\_fd, file);
return target\_fd;
}
也看看fget和task\_fd\_install这两个核心方法：
fget其实是调用到\_\_fget
static struct file \*\_\_fget(unsigned int fd, fmode\_t mask, unsigned int refs)
{
struct files\_struct \*files = current->files;
struct file \*file;
rcu\_read\_lock();
loop:
file = fcheck\_files(files, fd);
//省略
rcu\_read\_unlock();
return file;
}
再看看task\_fd\_install方法：
void \_\_fd\_install(struct files\_struct \*files, unsigned int fd,
struct file \*file)
{
struct fdtable \*fdt;
might\_sleep();
rcu\_read\_lock\_sched();
while (unlikely(files->resize\_in\_progress)) {
rcu\_read\_unlock\_sched();
wait\_event(files->resize\_wait, !files->resize\_in\_progress);
rcu\_read\_lock\_sched();
}
/\* coupled with smp\_wmb() in expand\_fdtable() \*/
smp\_rmb();
fdt = rcu\_dereference\_sched(files->fdt);
BUG\_ON(fdt->fd[fd] != NULL);
rcu\_assign\_pointer(fdt->fd[fd], file);//这里就是源进程file与新进程fd进行了绑定
rcu\_read\_unlock\_sched();
}
其实老版本的binder传递fd还是比较好理解的，主要就以下几个步骤：
1、根据源进程fd获取内核中对应的file
2、在目标进程中获取空闲没有使用的fd值
3、把第1步获取的file对象与第2步获取的fd值进行绑定既可以
老版本binder驱动的fd传递基本工作都直接在binder\_translate\_fd这个方法中完成，但是新版本binder驱动这块就有了较大的差异。
总结一个简单图给大家更容易理解：
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9d7996a71ebd43e9b47e3a242a3c310e.png)
### 新版本binder驱动代码传递fd
安卓kernel内核版本：
VERSION = 6
PATCHLEVEL = 1
SUBLEVEL = 90
EXTRAVERSION =
NAME = Curry Ramen
下面来看看这个6.1对应的源码是怎么传递fd的
static int binder\_translate\_fd(u32 fd, binder\_size\_t fd\_offset,
struct binder\_transaction \*t,
struct binder\_thread \*thread,
struct binder\_transaction \*in\_reply\_to)
{
struct binder\_proc \*proc = thread->proc;
struct binder\_proc \*target\_proc = t->to\_proc;
struct binder\_txn\_fd\_fixup \*fixup;
struct file \*file;
//这里也是通过源进程fd获取对应的file结构
file = fget(fd);
//检验传递安全性
ret = security\_binder\_transfer\_file(proc->cred, target\_proc->cred, file);
/\*
\* Add fixup record for this transaction. The allocation
\* of the fd in the target needs to be done from a
\* target thread.
\*/
//以下是差异部分
//构造出一个binder\_txn\_fd\_fixup结构
fixup = kzalloc(sizeof(\*fixup), GFP\_KERNEL);
//检验传递安全性
fixup->file = file;//获取file
fixup->offset = fd\_offset;
fixup->target\_fd = -1;//注意这里开始构造时候没有值
//该代码将 fixup 结构体（类型为 binder\_txn\_fd\_fixup）的成员 fixup\_entry
//添加到事务 t的 fd\_fixups 链表尾部，用于‌延迟处理文件描述符（fd）的跨进程映射‌。
list\_add\_tail(&fixup->fixup\_entry, &t->fd\_fixups);
return ret;
}
上面可以看出与老版本巨大差别在于，新版本根本没有直接在binder\_translate\_fd中获取target\_fd和install
target\_fd到file，只是构造了binder\_txn\_fd\_fixup对象，赋值file后，然后加入到事物t的fd\_fixups列表中。
下面来看看‌binder\_txn\_fd\_fixup 结构体‌
表示一个待处理的 fd 映射修正项，包含以下关键字段：
struct binder\_txn\_fd\_fixup {
struct file \*file; // 源进程的 file 对象
binder\_size\_t offset; // fd 在事务数据缓冲区中的偏移
int target\_fd; // 目标进程的 fd（初始为 -1）
struct list\_head fixup\_entry; // 链表节点（通过 list\_add\_tail 链接）
};
上面看完后最大疑问肯定是：
1、没有看到有target\_fd在目标进行进行获取啊？
2、也没有看到file与target\_fd进行映射？
那么新版本到底是如何通过binder\_txn\_fd\_fixup就可以实现的fd的跨进程传递呢？又为什么要这样做呢？
按照上面源码分析可以看到最后的binder\_txn\_fd\_fixup被放到了传递事物binder\_transaction中去了，那么什么时候执行处理这个呢？
目标进程处理事务‌，这里处理事物肯定在目标进程的binder\_thread\_read时候：
在 binder\_thread\_read 或事务处理函数中，遍历 t->fd\_fixups 链表：
static int binder\_thread\_read(struct binder\_proc \*proc,
struct binder\_thread \*thread,
binder\_uintptr\_t binder\_buffer, size\_t size,
binder\_size\_t \*consumed, int non\_block)
{
//省略
//使用binder\_apply\_fd\_fixups方法来专门处理fd相关
ret = binder\_apply\_fd\_fixups(proc, t);
//省略
return 0;
}
下面看看真正干活的binder\_apply\_fd\_fixups
/\*\*
\* binder\_apply\_fd\_fixups() - finish fd translation
\* @proc: binder\_proc associated @t->buffer
\* @t: binder transaction with list of fd fixups
\*
\* Now that we are in the context of the transaction target
\* process, we can allocate and install fds. Process the
\* list of fds to translate and fixup the buffer with the
\* new fds first and only then install the files.
\*
\* If we fail to allocate an fd, skip the install and release
\* any fds that have already been allocated.
\*/
static int binder\_apply\_fd\_fixups(struct binder\_proc \*proc,
struct binder\_transaction \*t)
{
struct binder\_txn\_fd\_fixup \*fixup, \*tmp;
int ret = 0;
list\_for\_each\_entry(fixup, &t->fd\_fixups, fixup\_entry) {
//获取目标进程的空闲fd
int fd = get\_unused\_fd\_flags(O\_CLOEXEC);
fixup->target\_fd = fd;//赋值给target\_fd
if (binder\_alloc\_copy\_to\_buffer(&proc->alloc, t->buffer,
fixup->offset, &fd,
sizeof(u32))) {
}
}
list\_for\_each\_entry\_safe(fixup, tmp, &t->fd\_fixups, fixup\_entry) {
//把源file与上面赋值的fixup->target\_fd进行关联
fd\_install(fixup->target\_fd, fixup->file);
list\_del(&fixup->fixup\_entry);
kfree(fixup);
}
return ret;
}
上面源码就很清楚看出来，fd的获取和关联file结构都是在目标进程进行read的时候才操作的，而不是和老版本那样在源进程进行binder\_transaction时候做的，那么这样做是基于什么考虑呢？
个人认为有以下几个部分：
\*\*‌延迟处理设计‌\*\*
原因‌：目标进程的 fd 分配必须在其自身上下文中执行（涉及进程的 fd 表），避免竞态或权限问题。
‌流程‌：
‌收集阶段‌：在源进程的 Binder 线程中，通过 binder\_translate\_fd 收集所有待映射的 fd，形成 fd\_fixups 链表。
‌处理阶段‌：当目标进程的线程处理该事务时，遍历 fd\_fixups 链表，为每个 fixup 分配 target\_fd，并修改事务数据缓冲区中的 fd
值。
有以下几个优势：
安全性‌
‌权限隔离‌：目标进程自行分配 fd，避免源进程直接操作目标资源。
‌策略检查‌：在 binder\_translate\_fd 阶段已通过 SELinux 检查（security\_binder\_transfer\_file）。
原子性‌
所有 fd 修正项集中处理，确保事务数据的一致性。
性能优化‌
延迟分配减少上下文切换，结合 Binder 的 mmap 机制实现零拷贝。
文章参考来源：
更多framework实战开发，关注下面“千里马学框架”