---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f323330335f38313133333831312f:61727469636c652f64657461696c732f313435373536373935"
layout: post
title: "网络爬虫-1发送请求维持会话代理设置超时设置"
date: 2025-03-12 15:48:35 +08:00
description: "1.基于get发送请求2.基于post发送请求3.维持会话4.代理设置/超时设置"
keywords: "网络爬虫-1:发送请求+维持会话+代理设置/超时设置"
categories: ['网络爬虫']
tags: ['爬虫', 'Python']
artid: "145756795"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145756795
    alt: "网络爬虫-1发送请求维持会话代理设置超时设置"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145756795
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145756795
cover: https://bing.ee123.net/img/rand?artid=145756795
image: https://bing.ee123.net/img/rand?artid=145756795
img: https://bing.ee123.net/img/rand?artid=145756795
---

# 网络爬虫-1:发送请求+维持会话+代理设置/超时设置

> 1.基于get发送请求
>   
> 2.基于post发送请求
>   
> 3.维持会话
>   
> 4.代理设置/超时设置

## 一.基于get发送请求

### 1.获取网页源码1

**使用json库中的json.loads(),将json格式的字符串变为Python的字典形式**

以下通过
[http://httpbin.org/get](http://httpbin.org/get "http://httpbin.org/get")
网址进行基本练习操作

```python
import requests
import json
url='http://httpbin.org/get'  #目标站点url
r=requests.get(url)           #发送get请求
dict_data=json.loads(r.text)
print("网页源码:\n",r.text)
print("网页源码的类型:\n",type(r.text))
print("键值对形式:\n",dict_data)
print("响应状态码:\n",r.status_code)     
```

1.网页源码通常为 HTML、JSON 或其他文本格式。

2.通过type函数可知,返回的网页源码是
**JSON 格式的字符串**
。

3.响应状态码一般有:200(请求成功),404(未找到),500(服务器内部错误)。

通过以上操作就获得了一个简单的网页源码爬取

```python
网页源码:
 {
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.32.3", 
    "X-Amzn-Trace-Id": "Root=1-67b6f01c-2e488d805a9188994c10307a"
  }, 
  "origin": "112.240.55.236", 
  "url": "http://httpbin.org/get"
}

网页源码的类型:
 <class 'str'>
键值对形式:
 {'args': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-67b6f01c-2e488d805a9188994c10307a'}, 'origin': '112.240.55.236', 'url': 'http://httpbin.org/get'}
响应状态码:
 200
```

### 2.获取网页源码2

**使用r.json()**

```python
import requests
import json
url='http://httpbin.org/get'  #目标站点url
r=requests.get(url)           #发送get请求
dict_data=r.json()
print("键值对形式:\n",dict_data)

```

```python
键值对形式:
 {'args': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-67b6f1fb-77329c48412b3c3c3c0aa26e'}, 'origin': '112.240.55.236', 'url': 'http://httpbin.org/get'}
```

### 3.网页源码(为图片或音频时)

**使用r.content**

```python
import requests

url='https://www.baidu.com/img/baidu_jgylogo3.gif'  #以图片为例
r = requests.get(url)                               #发送get请求
print(r.content)                                    #获取为二进制形式,图片或音频
print(type(r.content))                              #<class 'bytes'>表示二进制
#保存二进制图片
with open('test.jpg','wb') as f:
    f.write(r.content)
```

### 4.设置参数1

该练习网站可以自己设置参数args,
**通过在网址后面加?,参数后面加&**
,会在args这一栏显示设置的参数

```python
import requests
url='http://httpbin.org/get?age=12&time=11.9&name=12'#目标站点
r=requests.get(url)
print(r.text)

```

在返回的网页源码中,会出现我们在url后面设置的参数

```python
{
  "args": {
    "age": "12", 
    "name": "12", 
    "time": "11.9"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.32.3", 
    "X-Amzn-Trace-Id": "Root=1-67b6e74e-2ecdb89860e0144f3c399d39"
  }, 
  "origin": "112.240.55.236", 
  "url": "http://httpbin.org/get?age=12&time=11.9&name=12"
}
```

### 5.设置参数2

推荐使用的参数添加方式:使用字典,并在
**params参数中加上data**
,即可实现在网址后加上参数的操作

```python
url='http://httpbin.org/get?'
data={
    'age':12,
    'time':11.9
}
r=requests.get(url,params=data)
print(r.text)
```

```python
{
  "args": {
    "age": "12", 
    "time": "11.9"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.32.3", 
    "X-Amzn-Trace-Id": "Root=1-67a08088-56abf4e11348b1622190c6ca"
  }, 
  "origin": "123.13.93.50", 
  "url": "http://httpbin.org/get?age=12&time=11.9"
}
```

### 6.爬虫伪装-1

```python
import requests
import random
url='https://www.zhihu.com/explore'
#构建用户信息
USER_AGENTS = [
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.11 TaoBrowser/2.0 Safari/536.11",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.71 Safari/537.1 LBBROWSER",
"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; LBBROWSER)",
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 LBBROWSER",
"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)",
"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E; QQBrowser/7.0.3698.400)",
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)",
"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SV1; QQDownload 732; .NET4.0C; .NET4.0E; 360SE)",
"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E)",
"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)",
"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.89 Safari/537.1",
"Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5",
"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:2.0b13pre) Gecko/20110307 Firefox/4.0b13pre",
"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:16.0) Gecko/20100101 Firefox/16.0",
"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11",
"Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10"
]
UA={
    'User-Agent': random.choice(USER_AGENTS),
}
#参数设置
data={
    "age":12,
    "time":11.9
}
response=requests.get(url,params=data,headers=UA)
print("响应状态码:\n",response.status_code)
print("网页源码前200:\n",response.text[:200])

```

## 二.基于post发送请求

### 1.获取网页源码1

**使用json库中的json.loads(),将json格式的字符串变为Python的字典形式**

```python
import requests
import json
url='http://httpbin.org/post'    #目标站点
r = requests.post(url)           #发送post请求
dict_data=json.loads(r.text)     #将JSON 格式的字符串转换为 Python 字典(方便后续通过键名操作)
url_=dict_data["url"]            #获取网页源码中的url
print("输出json形式的网页源码:\n",dict_data)
print(url_)
```

```python
输出json形式的网页源码:
 {'args': {}, 'data': '', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '0', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-67b6ea62-559651c6763bbd4a2659f360'}, 'json': None, 'origin': '112.240.55.236', 'url': 'http://httpbin.org/post'}
http://httpbin.org/post
```

### 2.获取网页源码2

**使用r.json()**

```python
import requests
import json
url='http://httpbin.org/post'#目标站点
r = requests.post(url)
dict_data=r.json()
print(dict_data)
print(dict_data["url"])
```

```python
{'args': {}, 'data': '', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '0', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-67b6ee93-0580883d7de493eb16e3b93a'}, 'json': None, 'origin': '112.240.55.236', 'url': 'http://httpbin.org/post'}
http://httpbin.org/post

```

### 3.设置参数

**json形式,基于post请求增加参数,使用data=data进行参数的增加**

```python
import requests

url='http://httpbin.org/post'  #目标站点post请求
data={
    "age":12,
    "time":11.9
}                              #参数
response = requests.post(url,data=data)
print("网页源码:\n",response.text)
print("网页源码(键值对形式):\n",response.json())
```

```python
网页源码:
 {
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {
    "age": "12", 
    "time": "11.9"
  }, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "16", 
    "Content-Type": "application/x-www-form-urlencoded", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.32.3", 
    "X-Amzn-Trace-Id": "Root=1-67b6f31c-37e0c7ac11daa02a0d09daad"
  }, 
  "json": null, 
  "origin": "112.240.55.236", 
  "url": "http://httpbin.org/post"
}

网页源码(键值对形式):
 {'args': {}, 'data': '', 'files': {}, 'form': {'age': '12', 'time': '11.9'}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '16', 'Content-Type': 'application/x-www-form-urlencoded', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-67b6f31c-37e0c7ac11daa02a0d09daad'}, 'json': None, 'origin': '112.240.55.236', 'url': 'http://httpbin.org/post'}
```

## 三.维持会话

**用途:**
登录网站使用 Cookies 或 Session 维持登录状态，访问需要登录的页面。

### 3.1通过cookies维持会话

```python
import requests
#好处:容易请求到需要登录的请求
#坏处:容易被反爬封号
head={
    "cookie":"...............",
    
    "user-agent":"................"
}
url='https://www.jianshu.com'
response=requests.get(url,headers=head)

```

### 3.2通过session维持会话

```python
import requests

s=requests.Session()
url='https://www.jianshu.com'
response1 = s.get(url,headers=head)

```

#### **Cookies 和 Session 的区别**

| **特性** | **Cookies** | **Session** |
| --- | --- | --- |
| **存储位置** | 客户端（浏览器或爬虫） | 服务器端 |
| **数据安全性** | 较低（可以被客户端修改） | 较高（存储在服务器） |
| **生命周期** | 可以设置过期时间 | 通常随会话结束而失效 |
| **性能影响** | 每次请求都会携带 Cookies，增加带宽 | 服务器需要存储 Session 数据 |
| **适用场景** | 简单的状态维持（如登录状态） | 复杂的状态管理（如购物车、多步操作） |

## 四.代理设置/超时设置

使用proxies携带ip地址,使用timeout进行超时反馈,当ip不能用时,防止一直访问而不自知

超时（Timeout）是指爬虫在发送请求后，等待服务器响应的最长时间。如果超过这个时间仍未收到响应，爬虫会停止等待并抛出异常。

```python
url='https://www.jianshu.com'
ip = {
    "https":"172.16.17.32:8000",
}
response2=requests.get(url,proxies=ip,timeout=4)
```

**超时参数详解**

* `timeout=5`
  ：表示总超时时间为 5 秒（包括连接时间和读取时间）。
* `timeout=(3, 5)`
  ：表示连接超时为 3 秒，读取超时为 5 秒。