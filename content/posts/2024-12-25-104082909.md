---
layout: post
title: 蒙特卡洛树搜索算法MCTS
date: 2024-12-25 18:01:13 +08:00
categories: ['Machinelearning']
tags: ['无标签']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=104082909
    alt: 蒙特卡洛树搜索算法MCTS
artid: 104082909
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=104082909
featuredImagePreview: https://bing.ee123.net/img/rand?artid=104082909
---

# 蒙特卡洛树搜索算法(MCTS)

## 蒙特卡洛树搜索(MCTS)

*参考网址：https://zhuanlan.zhihu.com/p/30458774*

### 定义

**Monte Carlo Tree Search**
， 是一类树搜索算法的统称。

* 蒙特卡洛树搜索是一种基于
  **树数据结构**
  、能在
  **搜索空间巨大**
  仍然
  **比较有效**
  的
  **启发式**
  搜索算法
* **MCTS**
  是一种
  **逼近纳什均衡**
  的搜索策略。

###### 应用场景

* 搜索空间巨大
* *zero-sum、fully information、determinism、sequential、discrete*
* 第二点即：场景能分出输赢、游戏信息完全公开、每一个操作结果没有随机因素、操作按顺序执行、没有操作是一种连续值
* 只能解决Combinatorial Game的问题

###### 四大阶段

*Selection*
、
*Expansion*
、
*Simulation_和_Backpropagation（
**选择、扩展、模拟、回溯**
）*

### 理论基础

#### 一、Game Theory（博弈论）

##### 1. 纳什均衡点

**定义**

**minmax算法**
最终达到的
**平衡点**

##### 2. minmax算法

![img](https://i-blog.csdnimg.cn/blog_migrate/5c71499450d630191e4f107a14fc4e09.png)

图1 minmax算法示意图

##### 应用

**在搜索树中，每次轮到黑棋走时，走对黑棋最有利的；轮到白棋走时，走对黑棋最不利的。**

#### 二、Black Box Optimization（黑盒优化）

无法得知场景内部的函数或模型结果，只能通过输入和输出对算法进行优化。

**示例**

进化算法、贝叶斯优化、MCTS

#### 三、UCB算法基础

##### 与蒙特卡洛搜索算法关系说明

* **UCB**
  : 指UCB公式 （Upper Confidence Bound），公式为：

V
i
+
C
l
n
N
n
i
V_i + C \sqrt{\frac{lnN}{n_i}}






V









i

​




+





C





















n









i

​












l

n

N

​


​

* **UCT 算法**
  ：UCB for Tree的算法，最经典的蒙特卡罗树搜索算法

  **UCT = MCTS + UCB**
* **UCB1**
  ： 一种简单而广泛使用的UCB公式

V
i
+
2
l
n
N
n
i
V_i + \sqrt{\frac{2 lnN}{n_i}}






V









i

​




+

























n









i

​












2

l

n

N

​


​

#### 四、MCTS过程

![img](https://i-blog.csdnimg.cn/blog_migrate/ca63887d46f777190ae75af48e046365.png)

图2 MSTC 1次迭代的 4个步骤

##### UCT (UCB for Tree)算法

蒙特卡罗树搜索大概可以被分成四步。
**选择(Selection)，拓展(Expansion)，模拟(Simulation)，反向传播(Backpropagation)。**

在开始阶段，搜索树只有一个节点，也就是我们需要决策的局面。

搜索树中的每一个
**节点**
包含了
**三个基本信息**
：
**代表的局面，被访问的次数，累计评分。**

**[1]选择(Selection)**

在选择阶段，需要从根节点，也就是要做决策的局面R出发向下选择出一个最急迫需要被拓展的节点N，局面R是是每一次迭代中第一个被检查的节点；

​ 对于被检查的局面而言，他可能有三种可能：

​ 1)该节点所有可行动作都已经被拓展过

​ 2)该节点有可行动作还未被拓展过

​ 3)这个节点游戏已经结束了(例如已经连成五子的五子棋局面)

​ 对于这三种可能：

​ 1)如果所有可行动作都已经被拓展过了，那么我们将使用UCB公式计算该节点所有子节点的UCB值，并找到值最大的一个子节点继续检查。反复向下迭代。

​ 2)如果被检查的局面依然存在没有被拓展的子节点(例如说某节点有20个可行动作，但是在搜索树中才创建了19个子节点)，那么我们认为这个节点就是本次迭代的的目标节点N，并找出N还未被拓展的动作A。执行步骤[2]

​ 3)如果被检查到的节点是一个游戏已经结束的节点。那么从该节点直接执行步骤{4]。

每一个被检查的节点的被访问次数在这个阶段都会自增。

在反复的迭代之后，我们将在搜索树的底端找到一个节点，来继续后面的步骤。

**[2]拓展(Expansion)**

在选择阶段结束时候，我们查找到了一个最迫切被拓展的节点N，以及他一个尚未拓展的动作A。在搜索树中创建一个新的节点Nn作为N的一个新子节点。Nn的局面就是节点N在执行了动作A之后的局面。

**[3]模拟(Simulation)**

为了让Nn得到一个初始的评分。我们从Nn开始，让游戏随机进行，直到得到一个游戏结局，这个结局将作为Nn的初始评分。一般使用胜利/失败来作为评分，只有1或者0。

**[4]反向传播(Backpropagation)**

在Nn的模拟结束之后，它的父节点N以及从根节点到N的路径上的所有节点都会根据本次模拟的结果来添加自己的累计评分。如果在[1]的选择中直接发现了一个游戏结局的话，根据该结局来更新评分。

每一次迭代都会拓展搜索树，随着迭代次数的增加，搜索树的规模也不断增加。当到了一定的迭代次数或者时间之后结束，选择根节点下最好的子节点作为本次决策的结果。

上面描述的是
**UCT (UCB for Tree)算法**
，可以说是最经典的蒙特卡罗树搜索算法了。但随着算法的发展，MCTS已经有了非常大的改变。例如很多围棋AI都已经不再使用纯粹的UCB公式而改用效果更好的UCB1-Tuned了[2]，而搜索方法上也有了非常多的改动了。

### MCTS 和 UCT

Kocsis 和 Szepervari 在 2006 年首先构建了一个完备的 MCTS 算法，通过
**扩展 UCB**
到
**minimax 树搜索**
，并将其命名为 Upper Confidence Bounds for Trees（UCT）方法。这其实是用在当前众多 MCTS 实现中的算法版本。

**UCT 可以被描述为 MCTS 的一个特例：UCT = MCTS + UCB。**