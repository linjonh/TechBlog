---
layout: post
title: "03特征值分解"
date: 2025-03-08 17:51:09 +0800
description: "从方程 A v = λ v 变形得到：(A - λI) v = 0 为了求解 v，需要 A - λI 是。将每个特征值代入(A - λI) v = 0 求解对应的特征向量。v称为矩阵A的特征向量Eigenvector。λ称为矩阵A的特征值Eigenvalue。"
keywords: "03特征值分解"
categories: ['计算机视觉', '大模型']
tags: ['线性代数', '矩阵']
artid: "146119712"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146119712
    alt: "03特征值分解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146119712
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146119712
cover: https://bing.ee123.net/img/rand?artid=146119712
image: https://bing.ee123.net/img/rand?artid=146119712
img: https://bing.ee123.net/img/rand?artid=146119712
---

# 03特征值分解

#### **特征值分解（Eigenvalue Decomposition）**

##### **课程目标：**

  * 理解 **特征值（Eigenvalue）** 和 **特征向量（Eigenvector）** 的概念
  * 掌握 **特征值分解（Eigendecomposition）** 的基本原理
  * 学会计算 **矩阵的特征值和特征向量**
  * 了解特征值分解在 **数据降维（PCA）、物理、计算机视觉** 等领域的应用

* * *

### **第一部分：特征值和特征向量**

#### **1.1 特征值和特征向量的定义**

对于一个 n x n 方阵 A ，如果存在一个非零向量 v 和一个标量 λ，使得：  
A v = λ v A v = \lambda v Av=λv  
那么：  
v称为矩阵A的特征向量Eigenvector  
λ称为矩阵A的特征值Eigenvalue

##### **直观理解：**

  * **特征向量** 是在矩阵变换时 **方向不变** 的向量（只会被放大或缩小）。
  * **特征值** 描述了变换时向量的缩放比例。

* * *

#### **1.2 计算特征值**

从方程 A v = λ v 变形得到：(A - λI) v = 0 为了求解 v，需要 A - λI 是**奇异矩阵** （即行列式为 0）：  
det ⁡ ( A − λ I ) = 0 \det(A - \lambda I) = 0 det(A−λI)=0  
这个方程称为 **特征方程（Characteristic Equation）** ，解出 λ 即得到 **特征值** 。

##### **示例：计算 2×2 矩阵的特征值**

A = [ 4 2 1 3 ] A = \begin{bmatrix} 4 & 2 \\\ 1 & 3 \end{bmatrix} A=[41​23​]  
求特征方程：  
det ⁡ [ 4 − λ 2 1 3 − λ ] = 0 ( 4 − λ ) ( 3 − λ ) − ( 2 × 1 ) = 0 12 − 4 λ − 3
λ \+ λ 2 − 2 = 0 λ 2 − 7 λ \+ 10 = 0 \det \begin{bmatrix} 4 - \lambda & 2 \\\
1 & 3 - \lambda \end{bmatrix} = 0 \\\\\ (4 - \lambda)(3 - \lambda) - (2 \times
1) = 0 \\\\\ 12 - 4\lambda - 3\lambda + \lambda^2 - 2 = 0 \\\\\ \lambda^2 -
7\lambda + 10 = 0 det[4−λ1​23−λ​]=0 (4−λ)(3−λ)−(2×1)=0 12−4λ−3λ+λ2−2=0
λ2−7λ+10=0  
解这个二次方程：  
( λ − 5 ) ( λ − 2 ) = 0 (\lambda - 5)(\lambda - 2) = 0 (λ−5)(λ−2)=0  
所以特征值：  
λ 1 = 5 , λ 2 = 2 \lambda_1 = 5, \quad \lambda_2 = 2 λ1​=5,λ2​=2

* * *

#### **1.3 计算特征向量**

将每个特征值代入(A - λI) v = 0 求解对应的特征向量。

对于 λ1 = 5：  
( A − 5 I ) v = 0 [ − 1 2 1 − 2 ] [ x y ] = [ 0 0 ] (A - 5I) v = 0 \\\
\begin{bmatrix} -1 & 2 \\\ 1 & -2 \end{bmatrix} \begin{bmatrix} x \\\
y\end{bmatrix} =\begin{bmatrix} 0 \\\ 0 \end{bmatrix}
(A−5I)v=0[−11​2−2​][xy​]=[00​]  
解得特征向量：  
v 1 = k [ 2 1 ] v_1 = k \begin{bmatrix} 2 \\\ 1 \end{bmatrix} v1​=k[21​]

对于 λ1 = 2：  
( A − 2 I ) v = 0 [ 2 2 1 1 ] [ x y ] = [ 0 0 ] (A - 2I) v = 0 \\\\\
\begin{bmatrix} 2 & 2 \\\ 1 & 1 \end{bmatrix} \begin{bmatrix} x \\\
y\end{bmatrix}= \begin{bmatrix} 0 \\\ 0 \end{bmatrix} (A−2I)v=0
[21​21​][xy​]=[00​]  
解得特征向量：  
v 2 = k [ − 1 1 ] v_2 = k \begin{bmatrix} -1 \\\ 1 \end{bmatrix} v2​=k[−11​]

* * *

### **第二部分：特征值分解（Eigendecomposition）**

#### **2.1 特征值分解的定义**

如果矩阵 A **可对角化** ，则它可以分解为：  
A = P D P − 1 A = P D P^{-1} A=PDP−1  
其中：

  * P 是由 **特征向量** 组成的矩阵
  * D 是对角矩阵，其对角线元素是 **特征值**
  * P-1 是 P 的逆矩阵

#### **2.2 示例：计算特征值分解**

设：  
A = [ 4 2 1 3 ] A = \begin{bmatrix} 4 & 2 \\\ 1 & 3 \end{bmatrix} A=[41​23​]  
已经求得特征值：  
λ 1 = 5 , λ 2 = 2 \lambda_1 = 5, \quad \lambda_2 = 2 λ1​=5,λ2​=2  
以及特征向量：  
v 1 = [ 2 1 ] , v 2 = [ − 1 1 ] v_1 = \begin{bmatrix} 2 \\\ 1 \end{bmatrix},
\quad v_2 = \begin{bmatrix} -1 \\\ 1 \end{bmatrix} v1​=[21​],v2​=[−11​]  
构造矩阵P：  
P = [ 2 − 1 1 1 ] P = \begin{bmatrix} 2 & -1 \\\ 1 & 1 \end{bmatrix}
P=[21​−11​]  
构造对角矩阵D：  
D = [ 5 0 0 2 ] D = \begin{bmatrix} 5 & 0 \\\ 0 & 2 \end{bmatrix} D=[50​02​]  
计算 P-1，最终验证：  
A = P D P − 1 A = P D P^{-1} A=PDP−1

* * *

### **第三部分：特征值分解的应用**

  * **主成分分析（PCA）** ：用于数据降维
  * **图像处理** ：特征分解可以用于压缩、去噪
  * **振动分析** ：用于研究物理系统的固有频率
  * **微分方程** ：求解动态系统

* * *

### **第四部分：Python 实现**

    
    
    import numpy as np
    
    # 定义矩阵
    A = np.array([[4, 2], [1, 3]])
    
    # 计算特征值和特征向量
    eigenvalues, eigenvectors = np.linalg.eig(A)
    
    print("特征值：", eigenvalues)
    print("特征向量：\n", eigenvectors)
    

* * *

### **总结**

✅ **掌握了特征值和特征向量的计算方法**  
✅ **理解了特征值分解的公式：( A = P D P^{-1} )**  
✅ **应用于 PCA、图像处理、微分方程等领域**  
✅ **Python 代码实现特征值分解**

🚀 **下一步** ：学习 **奇异值分解（SVD）** 和 **主成分分析（PCA）** ！



