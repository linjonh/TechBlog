---
layout: post
title: "基于Ollama平台部署的Qwen大模型实现聊天机器人"
date: 2025-03-11 23:40:34 +0800
description: "本案例旨在构建一个基于Python的交互式系统，前端通过Streamlit框架实现简洁易用的用户界面，后端基于Ollama平台部署Qwen模型，提供自然语言处理（NLP）能力。用户可以通过前端界面与Qwen模型进行交互，获取模型的响应结果。"
keywords: "基于Ollama平台部署的Qwen大模型实现聊天机器人"
categories: ['Ollama']
tags: ['聊天机器人', '大模型', 'Ollama']
artid: "146190639"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146190639
    alt: "基于Ollama平台部署的Qwen大模型实现聊天机器人"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146190639
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146190639
cover: https://bing.ee123.net/img/rand?artid=146190639
image: https://bing.ee123.net/img/rand?artid=146190639
img: https://bing.ee123.net/img/rand?artid=146190639
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     基于Ollama平台部署的Qwen大模型实现聊天机器人
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <p>
    </p>
    <h2>
     <a id="OllamaQwen_1">
     </a>
     基于Ollama平台部署的Qwen大模型实现聊天机器人
    </h2>
    <h2>
     <a id="1__3">
     </a>
     1 概述
    </h2>
    <p>
     本案例旨在构建一个基于Python的交互式系统，前端通过Streamlit框架实现简洁易用的用户界面，后端基于Ollama平台部署Qwen模型，提供自然语言处理（NLP）能力。用户可以通过前端界面与Qwen模型进行交互，获取模型的响应结果。
    </p>
    <h2>
     <a id="2__5">
     </a>
     2 技术栈
    </h2>
    <h3>
     <a id="21__6">
     </a>
     2.1 开发技术
    </h3>
    <ul>
     <li>
      前端：Streamlit 1.42.2（轻量级Web应用框架）
     </li>
     <li>
      后端：Ollama 0.5.12（模型部署平台）
     </li>
     <li>
      模型：Qwen2:0.5b（自然语言处理模型）
     </li>
     <li>
      编程语言：Python 3.12.8
     </li>
     <li>
      模块：requests 2.32.3、ollama 0.4.7
     </li>
     <li>
      开发工具：PyCharm
     </li>
    </ul>
    <blockquote>
     <p>
      说明：
      <br/>
      安装requests 2.32.3、ollama 0.4.7，只是为了演示两种访问方式，开发场景中，只要实现其中之一。
     </p>
    </blockquote>
    <h3>
     <a id="22__15">
     </a>
     2.2 环境
    </h3>
    <ul>
     <li>
      系统：Ubuntu 24.04.2 LTS
     </li>
     <li>
      系统服务：WSL 2.4.11.0
     </li>
    </ul>
    <blockquote>
     <p>
      <strong>
       说明：
      </strong>
      <br/>
      Windows Subsystem for Linux（简称WSL）是一个在Windows 10\11上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。它是由微软与Canonical公司合作开发，开发人员可以在 Windows 计算机上同时访问 Windows 和 Linux 的强大功能。 通过
      <code>
       适用于 Linux 的 Windows 子系统 (WSL)
      </code>
      ，开发人员可以安装 Linux 发行版（例如 Ubuntu、OpenSUSE、Kali、Debian、Arch Linux 等），并直接在 Windows 上使用 Linux 应用程序、实用程序和 Bash 命令行工具，不用进行任何修改，也无需承担传统虚拟机或双启动设置的费用。
     </p>
    </blockquote>
    <h2>
     <a id="3__21">
     </a>
     3 实现步骤
    </h2>
    <h3>
     <a id="31__22">
     </a>
     3.1 环境搭建
    </h3>
    <h4>
     <a id="311_WSLUbuntu_23">
     </a>
     3.1.1 WSL配置及Ubuntu安装
    </h4>
    <p>
     WSL配置以及Ubuntu系统安装，可参考文章
     <strong>
      WSL安装及问题
     </strong>
     <a href="https://blog.csdn.net/mh942408056/article/details/145053974">
      https://blog.csdn.net/mh942408056/article/details/145053974
     </a>
    </p>
    <blockquote>
     <p>
      注意：
      <br/>
      如果不准备在Liunx中安装Ollama，可省略此步骤，Ollama同时支持Windows安装。
     </p>
    </blockquote>
    <h4>
     <a id="312_Ollama_27">
     </a>
     3.1.2 Ollama安装及模型部署
    </h4>
    <p>
     Ollama安装以及模型的部署，可参考文章
     <strong>
      Ollama安装与使用
     </strong>
     <a href="https://blog.csdn.net/mh942408056/article/details/146038905">
      https://blog.csdn.net/mh942408056/article/details/146038905
     </a>
    </p>
    <blockquote>
     <p>
      注意：
      <br/>
      如果Windows中安装Ollama，请去
      <a href="https://ollama.com/download/windows" rel="nofollow">
       Ollama官网
      </a>
      下载Windows版本。
     </p>
    </blockquote>
    <h3>
     <a id="32__31">
     </a>
     3.2 模块安装
    </h3>
    <h4>
     <a id="321_Streamlit_1422_32">
     </a>
     3.2.1 安装Streamlit 1.42.2
    </h4>
    <pre><code class="prism language-Bash">pip install streamlit
</code></pre>
    <blockquote>
     <p>
      说明：
      <br/>
      Steamlit帮助文档地址为：
      <a href="https://docs.streamlit.io/" rel="nofollow">
       https://docs.streamlit.io/
      </a>
     </p>
    </blockquote>
    <h4>
     <a id="322_requests_2323_38">
     </a>
     3.2.2 安装requests 2.32.3
    </h4>
    <pre><code class="prism language-Bash">pip install requests
</code></pre>
    <blockquote>
     <p>
      说明：
      <br/>
      通过requests模块实现远程调用接口访问Ollama中的大模型。
     </p>
    </blockquote>
    <h4>
     <a id="323_ollama_047_44">
     </a>
     3.2.3 安装ollama 0.4.7
    </h4>
    <pre><code class="prism language-Bash">pip install ollama
</code></pre>
    <blockquote>
     <p>
      说明：
      <br/>
      通过ollama模块实现本地调用接口访问Ollama中的大模型。
     </p>
    </blockquote>
    <h3>
     <a id="33__50">
     </a>
     3.3 后端实现
    </h3>
    <blockquote>
     <p>
      <strong>
       注意
      </strong>
      ：
     </p>
     <ul>
      <li>
       使用Ollama加载Qwen模型（如 ollama run qwen2:0.5b）。
      </li>
      <li>
       确保API服务可用（默认地址为 http://localhost:11434/api/chat）。
      </li>
     </ul>
    </blockquote>
    <p>
     文件名称为：chat_utils.py
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> ollama
<span class="token keyword">import</span> requests
<span class="token keyword">import</span> json


<span class="token keyword">def</span> <span class="token function">get_response</span><span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    调用ollama聊天接口，并返回结果（方式一：本地访问）
    :param prompt: 历史对话、提示词
    :return:执行结果
    """</span>
    <span class="token comment"># 获取最后50个会话信息传送给模型，模型会根据上下文回答最后一个问题</span>
    response <span class="token operator">=</span> ollama<span class="token punctuation">.</span>chat<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">'qwen2:0.5b'</span><span class="token punctuation">,</span> messages<span class="token operator">=</span>prompt<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> stream<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> response<span class="token punctuation">[</span><span class="token string">'message'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span>


<span class="token keyword">def</span> <span class="token function">get_response_requests</span><span class="token punctuation">(</span>url<span class="token punctuation">,</span> prompt<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    调用ollama聊天接口，并返回结果（方式二：远程访问）
    :param url: 访问的接口
    :param prompt: 历史对话、提示词
    :return: 执行结果
    """</span>
    <span class="token comment"># 1 定义请求头</span>
    headers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"Content-Type"</span><span class="token punctuation">:</span> <span class="token string">"application/json"</span><span class="token punctuation">}</span>
    <span class="token comment"># 2 请求并返回结果</span>
    response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> data<span class="token operator">=</span>json<span class="token punctuation">.</span>dumps<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 3 判断返回结果状态</span>
    <span class="token keyword">if</span> response<span class="token punctuation">.</span>status_code <span class="token operator">==</span> <span class="token number">200</span><span class="token punctuation">:</span>
        <span class="token comment"># 3.1 将文本转换成字典</span>
        msg <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
        <span class="token comment"># 3.2 返回消息</span>
        <span class="token keyword">return</span> msg<span class="token punctuation">[</span><span class="token string">"message"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> response<span class="token punctuation">.</span>status_code<span class="token punctuation">,</span> response<span class="token punctuation">.</span>text


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment"># 提示词</span>
    prompt <span class="token operator">=</span> <span class="token string">'学习streamlit的注意事项'</span>
    <span class="token comment"># 组装接口消息内容</span>
    prompt_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span><span class="token punctuation">]</span>
    <span class="token comment"># # 通过ollama模块访问Ollama平台中的大模型</span>
    <span class="token comment"># response = get_response(prompt_list)</span>
    <span class="token comment"># 通过requests模块访问Ollama平台中的大模型</span>
    response <span class="token operator">=</span> get_response_requests<span class="token punctuation">(</span><span class="token string">'http://localhost:11434/api/chat'</span><span class="token punctuation">,</span> <span class="token punctuation">{<!-- --></span><span class="token string">'model'</span><span class="token punctuation">:</span> <span class="token string">'qwen2:0.5b'</span><span class="token punctuation">,</span> <span class="token string">'messages'</span><span class="token punctuation">:</span> prompt_list<span class="token punctuation">,</span><span class="token string">'stream'</span><span class="token punctuation">:</span><span class="token boolean">False</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">)</span>
</code></pre>
    <h3>
     <a id="34__106">
     </a>
     3.4 前端访问
    </h3>
    <p>
     文件名称为：chat_main.py
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> streamlit <span class="token keyword">as</span> st
<span class="token keyword">import</span> chat_utils

<span class="token comment"># 1 页面配置</span>
st<span class="token punctuation">.</span>set_page_config<span class="token punctuation">(</span>
    page_title<span class="token operator">=</span><span class="token string">'智聊机器人'</span><span class="token punctuation">,</span>  <span class="token comment"># 页面标题</span>
    page_icon<span class="token operator">=</span><span class="token string">':pirate_flag:'</span><span class="token punctuation">,</span>  <span class="token comment"># 页面图标</span>
    initial_sidebar_state<span class="token operator">=</span><span class="token string">'expanded'</span><span class="token punctuation">,</span>  <span class="token comment"># 初始状态侧边栏</span>
    menu_items<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">'Get Help'</span><span class="token punctuation">:</span> <span class="token string">'https://www.csdn.net/'</span><span class="token punctuation">,</span>
        <span class="token string">'Report a Bug'</span><span class="token punctuation">:</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        <span class="token string">'About'</span><span class="token punctuation">:</span> <span class="token string">"# 智聊机器人"</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">)</span>

<span class="token comment"># 2 主界面主标题</span>
st<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'智聊机器人'</span><span class="token punctuation">)</span>

<span class="token comment"># 3 判断聊天记录是否存在会话状态中</span>
<span class="token keyword">if</span> <span class="token string">'messages'</span> <span class="token keyword">not</span> <span class="token keyword">in</span> st<span class="token punctuation">.</span>session_state<span class="token punctuation">:</span> 
    <span class="token comment"># 3.1 增加欢迎语</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">[</span><span class="token string">'messages'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'assistant'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> <span class="token string">'你好，我是智聊机器人，有什么可以帮助您的吗？ 	:santa:'</span><span class="token punctuation">}</span><span class="token punctuation">]</span>

<span class="token comment"># 4 循环遍历会话状态中的消息</span>
<span class="token keyword">for</span> message <span class="token keyword">in</span> st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">:</span>
    <span class="token comment"># 4.1 按角色将消息输出到页面</span>
    <span class="token keyword">with</span> st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span>message<span class="token punctuation">[</span><span class="token string">'role'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 4.1.1 输出消息</span>
        st<span class="token punctuation">.</span>markdown<span class="token punctuation">(</span>message<span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 5.创建一个会话框</span>
prompt <span class="token operator">=</span> st<span class="token punctuation">.</span>chat_input<span class="token punctuation">(</span><span class="token string">'请输入您要咨询的问题：'</span><span class="token punctuation">)</span>
<span class="token comment"># 6.判断是否有新的消息</span>
<span class="token keyword">if</span> prompt<span class="token punctuation">:</span>
    <span class="token comment"># 6.1 将消息追加到会话状态中</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">[</span><span class="token string">'messages'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> prompt<span class="token punctuation">}</span><span class="token punctuation">)</span>
    <span class="token comment"># 6.2 输出会话消息</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">'user'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>markdown<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span>
    <span class="token comment"># 7 增加旋转等待组件</span>
    <span class="token keyword">with</span> st<span class="token punctuation">.</span>spinner<span class="token punctuation">(</span><span class="token string">':hourglass: AI小助手正在思考中...'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 7.1 调用Ollama聊天接口，并接收返回结果</span>
        content <span class="token operator">=</span> chat_utils<span class="token punctuation">.</span>get_response<span class="token punctuation">(</span>st<span class="token punctuation">.</span>session_state<span class="token punctuation">[</span><span class="token string">'messages'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 8 记录assistant返回的消息</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">[</span><span class="token string">'messages'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'assistant'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> content<span class="token punctuation">}</span><span class="token punctuation">)</span>
    <span class="token comment"># 9 将返回消息输出到页面</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">'assistant'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>markdown<span class="token punctuation">(</span>content<span class="token punctuation">)</span>
</code></pre>
    <h3>
     <a id="35__157">
     </a>
     3.5 代码执行
    </h3>
    <p>
     进入
     <code>
      chat_main.py
     </code>
     根目录，运行以下命令：
    </p>
    <pre><code class="prism language-Bash">streamlit run chat_main.py
</code></pre>
    <h3>
     <a id="36__162">
     </a>
     3.6 实现效果
    </h3>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7409e3352cd44560bfa6906d5a97177c.png"/>
    </p>
    <h2>
     <a id="4_curl_164">
     </a>
     4 通过curl命令行工具进行访问
    </h2>
    <p>
     如果只是简单访问，而不用开发代码，可通过
     <code>
      ollama run 模型名称:标签
     </code>
     或
     <code>
      curl
     </code>
     实现快速访问。
    </p>
    <ul>
     <li>
      ollama访问方式参考文章
      <strong>
       Ollama安装与使用
      </strong>
      <a href="https://blog.csdn.net/mh942408056/article/details/146038905">
       https://blog.csdn.net/mh942408056/article/details/146038905
      </a>
      。
     </li>
     <li>
      curl访问方式可参考
      <strong>
       官网API
      </strong>
      <a href="https://github.com/ollama/ollama/blob/main/docs/api.md">
       https://github.com/ollama/ollama/blob/main/docs/api.md
      </a>
     </li>
    </ul>
    <h3>
     <a id="41_apichat__168">
     </a>
     4.1 /api/chat 聊天对话接口案例
    </h3>
    <pre><code class="prism language-Bash">curl http://localhost:11434/api/chat -d "{\"model\": \"qwen2:0.5b\",\"messages\": [ {\"role\": \"user\",\"content\": \"天空为何这么蓝？\"}]}"
</code></pre>
    <blockquote>
     <p>
      注意：
      <br/>
      <code>
       \
      </code>
      用于转义引号，如果不带，将无法访问。
     </p>
    </blockquote>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d683934323430383035362f:61727469636c652f64657461696c732f313436313930363339" class_="artid" style="display:none">
 </p>
</div>


