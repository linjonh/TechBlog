---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d683934323430383035362f:61727469636c652f64657461696c732f313436313930363339"
layout: post
title: "基于Ollama平台部署的Qwen大模型实现聊天机器人"
date: 2025-03-11 23:40:34 +08:00
description: "本案例旨在构建一个基于Python的交互式系统，前端通过Streamlit框架实现简洁易用的用户界面，后端基于Ollama平台部署Qwen模型，提供自然语言处理（NLP）能力。用户可以通过前端界面与Qwen模型进行交互，获取模型的响应结果。"
keywords: "基于Ollama平台部署的Qwen大模型实现聊天机器人"
categories: ['Ollama']
tags: ['聊天机器人', '大模型', 'Ollama']
artid: "146190639"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146190639
    alt: "基于Ollama平台部署的Qwen大模型实现聊天机器人"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146190639
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146190639
cover: https://bing.ee123.net/img/rand?artid=146190639
image: https://bing.ee123.net/img/rand?artid=146190639
img: https://bing.ee123.net/img/rand?artid=146190639
---

# 基于Ollama平台部署的Qwen大模型实现聊天机器人

## 基于Ollama平台部署的Qwen大模型实现聊天机器人

## 1 概述

本案例旨在构建一个基于Python的交互式系统，前端通过Streamlit框架实现简洁易用的用户界面，后端基于Ollama平台部署Qwen模型，提供自然语言处理（NLP）能力。用户可以通过前端界面与Qwen模型进行交互，获取模型的响应结果。

## 2 技术栈

### 2.1 开发技术

* 前端：Streamlit 1.42.2（轻量级Web应用框架）
* 后端：Ollama 0.5.12（模型部署平台）
* 模型：Qwen2:0.5b（自然语言处理模型）
* 编程语言：Python 3.12.8
* 模块：requests 2.32.3、ollama 0.4.7
* 开发工具：PyCharm

> 说明：
>   
> 安装requests 2.32.3、ollama 0.4.7，只是为了演示两种访问方式，开发场景中，只要实现其中之一。

### 2.2 环境

* 系统：Ubuntu 24.04.2 LTS
* 系统服务：WSL 2.4.11.0

> **说明：**
>   
> Windows Subsystem for Linux（简称WSL）是一个在Windows 10\11上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。它是由微软与Canonical公司合作开发，开发人员可以在 Windows 计算机上同时访问 Windows 和 Linux 的强大功能。 通过
> `适用于 Linux 的 Windows 子系统 (WSL)`
> ，开发人员可以安装 Linux 发行版（例如 Ubuntu、OpenSUSE、Kali、Debian、Arch Linux 等），并直接在 Windows 上使用 Linux 应用程序、实用程序和 Bash 命令行工具，不用进行任何修改，也无需承担传统虚拟机或双启动设置的费用。

## 3 实现步骤

### 3.1 环境搭建

#### 3.1.1 WSL配置及Ubuntu安装

WSL配置以及Ubuntu系统安装，可参考文章
**WSL安装及问题**
<https://blog.csdn.net/mh942408056/article/details/145053974>

> 注意：
>   
> 如果不准备在Liunx中安装Ollama，可省略此步骤，Ollama同时支持Windows安装。

#### 3.1.2 Ollama安装及模型部署

Ollama安装以及模型的部署，可参考文章
**Ollama安装与使用**
<https://blog.csdn.net/mh942408056/article/details/146038905>

> 注意：
>   
> 如果Windows中安装Ollama，请去
> [Ollama官网](https://ollama.com/download/windows)
> 下载Windows版本。

### 3.2 模块安装

#### 3.2.1 安装Streamlit 1.42.2

```Bash
pip install streamlit

```

> 说明：
>   
> Steamlit帮助文档地址为：
> <https://docs.streamlit.io/>

#### 3.2.2 安装requests 2.32.3

```Bash
pip install requests

```

> 说明：
>   
> 通过requests模块实现远程调用接口访问Ollama中的大模型。

#### 3.2.3 安装ollama 0.4.7

```Bash
pip install ollama

```

> 说明：
>   
> 通过ollama模块实现本地调用接口访问Ollama中的大模型。

### 3.3 后端实现

> **注意**
> ：
>
> * 使用Ollama加载Qwen模型（如 ollama run qwen2:0.5b）。
> * 确保API服务可用（默认地址为 http://localhost:11434/api/chat）。

文件名称为：chat_utils.py

```python
import ollama
import requests
import json


def get_response(prompt):
    """
    调用ollama聊天接口，并返回结果（方式一：本地访问）
    :param prompt: 历史对话、提示词
    :return:执行结果
    """
    # 获取最后50个会话信息传送给模型，模型会根据上下文回答最后一个问题
    response = ollama.chat(model='qwen2:0.5b', messages=prompt[-50:], stream=False)
    return response['message']['content']


def get_response_requests(url, prompt):
    """
    调用ollama聊天接口，并返回结果（方式二：远程访问）
    :param url: 访问的接口
    :param prompt: 历史对话、提示词
    :return: 执行结果
    """
    # 1 定义请求头
    headers = {"Content-Type": "application/json"}
    # 2 请求并返回结果
    response = requests.post(url, headers=headers, data=json.dumps(prompt))
    # 3 判断返回结果状态
    if response.status_code == 200:
        # 3.1 将文本转换成字典
        msg = json.loads(response.text)
        # 3.2 返回消息
        return msg["message"]["content"]
    else:
        return response.status_code, response.text


if __name__ == '__main__':
    # 提示词
    prompt = '学习streamlit的注意事项'
    # 组装接口消息内容
    prompt_list = [{'role': 'user', 'content': prompt}]
    # # 通过ollama模块访问Ollama平台中的大模型
    # response = get_response(prompt_list)
    # 通过requests模块访问Ollama平台中的大模型
    response = get_response_requests('http://localhost:11434/api/chat', {'model': 'qwen2:0.5b', 'messages': prompt_list,'stream':False})
    print(response)

```

### 3.4 前端访问

文件名称为：chat_main.py

```python
import streamlit as st
import chat_utils

# 1 页面配置
st.set_page_config(
    page_title='智聊机器人',  # 页面标题
    page_icon=':pirate_flag:',  # 页面图标
    initial_sidebar_state='expanded',  # 初始状态侧边栏
    menu_items={
        'Get Help': 'https://www.csdn.net/',
        'Report a Bug': None,
        'About': "# 智聊机器人",
    }
)

# 2 主界面主标题
st.title('智聊机器人')

# 3 判断聊天记录是否存在会话状态中
if 'messages' not in st.session_state: 
    # 3.1 增加欢迎语
    st.session_state['messages'] = [
        {'role': 'assistant', 'content': '你好，我是智聊机器人，有什么可以帮助您的吗？ 	:santa:'}]

# 4 循环遍历会话状态中的消息
for message in st.session_state.messages:
    # 4.1 按角色将消息输出到页面
    with st.chat_message(message['role']):
        # 4.1.1 输出消息
        st.markdown(message['content'])

# 5.创建一个会话框
prompt = st.chat_input('请输入您要咨询的问题：')
# 6.判断是否有新的消息
if prompt:
    # 6.1 将消息追加到会话状态中
    st.session_state['messages'].append({'role': 'user', 'content': prompt})
    # 6.2 输出会话消息
    st.chat_message('user').markdown(prompt)
    # 7 增加旋转等待组件
    with st.spinner(':hourglass: AI小助手正在思考中...'):
        # 7.1 调用Ollama聊天接口，并接收返回结果
        content = chat_utils.get_response(st.session_state['messages'])
    # 8 记录assistant返回的消息
    st.session_state['messages'].append({'role': 'assistant', 'content': content})
    # 9 将返回消息输出到页面
    st.chat_message('assistant').markdown(content)

```

### 3.5 代码执行

进入
`chat_main.py`
根目录，运行以下命令：

```Bash
streamlit run chat_main.py

```

### 3.6 实现效果

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7409e3352cd44560bfa6906d5a97177c.png)

## 4 通过curl命令行工具进行访问

如果只是简单访问，而不用开发代码，可通过
`ollama run 模型名称:标签`
或
`curl`
实现快速访问。

* ollama访问方式参考文章
  **Ollama安装与使用**
  <https://blog.csdn.net/mh942408056/article/details/146038905>
  。
* curl访问方式可参考
  **官网API**
  <https://github.com/ollama/ollama/blob/main/docs/api.md>

### 4.1 /api/chat 聊天对话接口案例

```Bash
curl http://localhost:11434/api/chat -d "{\"model\": \"qwen2:0.5b\",\"messages\": [ {\"role\": \"user\",\"content\": \"天空为何这么蓝？\"}]}"

```

> 注意：
>   
> `\`
> 用于转义引号，如果不带，将无法访问。