---
layout: post
title: "基于YOLO8的垃圾识别检测系统数据集源码文章"
date: 2025-08-30T22:07:36+0800
description: "本系统通过先进的视觉识别技术（涵盖静态图片分析、动态视频流解析及实时摄像头监控），构建了全方位、多层次的智能垃圾检测与分类体系。系统依托深度学习领域的YOLOv8图像识别算法，能够精准识别各类垃圾特征，对公共场所、社区环境中的垃圾违规堆放实现毫秒级响应与智能分类提示，有效提升环境监管效率。这一技术解决方案不仅为破解&quot;垃圾识别与分类&quot;难题提供了智能化路径，更通过自动化监管显著降低了人工分拣成本，推动环境治理从被动清理转向主动防控。"
keywords: "基于YOLO8的垃圾识别检测系统（数据集+源码+文章）"
categories: ['Yolo']
tags: ['人工智能']
artid: "151026227"
arturl: "https://blog.csdn.net/qq_42681787/article/details/151026227"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151026227
    alt: "基于YOLO8的垃圾识别检测系统数据集源码文章"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151026227
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151026227
cover: https://bing.ee123.net/img/rand?artid=151026227
image: https://bing.ee123.net/img/rand?artid=151026227
img: https://bing.ee123.net/img/rand?artid=151026227
---



# 基于YOLO8的垃圾识别检测系统（数据集+源码+文章）



> 文末附下载链接

![](https://i-blog.csdnimg.cn/direct/987362ecfdd94a77b1e13a3651840872.gif)

## 1.系统开发目的

基于 YOLO8 开发垃圾检测系统，核心目的是解决传统垃圾处理流程中的效率瓶颈与精度缺陷，具有以下目的和意义：​

* 突破人工分拣的局限性：传统垃圾分拣依赖人工识别，存在效率低（单人日均分拣量不足 1 吨）、误判率高（易混淆可回收物与有害垃圾）、作业环境恶劣（易接触有毒有害垃圾）等问题，系统旨在通过 AI 自动化检测，将分拣效率提升 3 倍以上，同时降低人工成本与健康风险。​
* 实现垃圾全流程智能化管理：针对垃圾 “产生 - 运输 - 处理” 全链条数据断层问题，系统可实时识别垃圾种类、统计产量，并联动物联网设备（如智能垃圾桶、清运车 GPS），构建可视化管理平台，为垃圾分类政策落地提供数据支撑。​
* 适配复杂场景的检测需求：传统机器视觉模型在光照变化（如夜间垃圾站）、遮挡（如塑料袋包裹玻璃）、小目标（如纽扣电池）等场景下检测精度骤降，YOLO8 的强鲁棒性可解决此类问题，确保在社区、商场、工业区等多场景下稳定运行。

## 2. YOLO8介绍

YOLOv8 由 Ultralytics 公司开发，是 YOLO (You Only Look Once) 系列目标检测模型的最新成员。它在之前版本的基础上进行了多项架构改进和优化，使其在精度、速度和易用性上都有显著提升。

![](https://i-blog.csdnimg.cn/direct/759104d9c06f45d9a2d69a9af530f5b3.png)

主要创新点包括：

1.无锚点 (Anchor-Free) 设计​​：

**改变**​​： YOLOv5、v7 等早期版本使用基于锚点（Anchor）的检测方式，需要预先定义大量不同尺寸和比例的锚框。YOLOv8 抛弃了这一点，转而使用​​无锚点​​机制。  
 **优势​​**： 简化了训练过程，减少了超参数的数量（无需再聚类分析数据集的锚点值），使模型更易于训练和调优，同时提升了检测性能。  
 2.新的主干网络 (Backbone) 和 Neck​​：

**CSPDarknet53 的增强版**​​： 主干网络仍然基于CSP结构，但在具体层结构上进行了优化，使用了更高效的跨阶段局部网络设计，提升了特征提取能力。  
 ​​**SPPF 的继承与优化**​​： 继续使用了空间金字塔池化快速（SPPF）模块，有效融合多尺度特征。  
 **新的特征融合网络 (Neck)​​**： 采用了 ​​PAN-FPN​​ 结构（Path Aggregation Network + Feature Pyramid Network）的改进版，更好地实现了自底向上和自顶向下的特征融合，增强了多尺度目标的检测能力。  
 3.​​损失函数​​： 使用了 ​​CIoU Loss​​ 或 ​​DFL (Distribution Focal Loss) + CIoU​​ 的组合。DFL 让模型更好地学习边界框的分布，从而获得更精确的预测框。  
 4.​​解耦头 (Decoupled Head)​​：将分类头和回归头分离开，而不是像之前版本那样共享参数。 让模型可以分别专注于分类和定位这两个任务，提升了整体性能。  
 5.Mosaic 数据增强的灵活运用​​：YOLOv8 在训练末期（最后10个epoch）会​​关闭 Mosaic 增强​​，转而使用更传统的缩放和翻转增强。这有助于模型更好地收敛，学习到更真实的特征，提升验证精度。  
 6.​​先进的训练技巧和模型缩放​​：整合了模型缩放（Model Scaling）技术，可以更容易地导出不同规模的模型（n, s, m, l, x），平衡速度和精度。提供了非常完善和用户友好的训练、验证、推理和导出管道。

## 3.系统开发思路

### 3.1 系统开发流程

系统开发设计主要包含三个步骤：（1）数据集准备（2）模型训练（3）系统开发。首先准备垃圾图片数据集并利用labelimg标注软件对图片中的垃圾进行标注，之后采用YOLO8目标检测网络对图片数据集进行训练，得到训练好的模型权重，最后编写模型推理代码并分别开发前端、后端，最终完成整套系统的开发。

![](https://i-blog.csdnimg.cn/direct/05e07729b4a24b64a1c6ba17ad11b455.png)

### 3.2 数据集收集

使用的垃圾图片数据集为自制数据集。数据集制作的具体步骤是，实地拍摄和网络爬虫收集4526张垃圾图片。然后使用labeling标注图片，将图片分为了一类：垃圾。数据集格式保存为YOLO格式，并按80%、20%的比例划分为训练集和验证集。数据集样张如图3.1所示。转化为txt格式如图3.2所示。

![](https://i-blog.csdnimg.cn/direct/234311ff8de94d69919cead877c2af61.png)

图3.1 数据集样张

![](https://i-blog.csdnimg.cn/direct/5a8b709a65b941e7b7b030b0a12fe7e7.png)

图3.2 txt格式样张

### ******3.3********数据处理******

本项目涉及到的数据增强方法主要有以下几种：

1.对原图做数据增强

①像素级：HSV增强、旋转、缩放、平移、剪切、透视、翻转等

②图片级：MixUp、Cutout、CutMix、Mosaic、Copy-Paste(Segment)等

2.对标签做同样的增强

①变换后的坐标偏移量

②防止标签坐标越界

除了上述最基本的数据增强方法外，还使用了 Mosaic 数据增强方法，其主要思想就是将4张图片进行随机裁剪、缩放后，再随机排列拼接形成一张图片，实现丰富数据集的同时，增加了小样本目标，提升网络的训练速度[1]。在进行归一化操作时会一次性计算4张图片的数据，因此模型对内存的需求降低。Mosaic数据增强的流程如图所示。

![](https://i-blog.csdnimg.cn/direct/3514e9aa35fb4dfd8d91742119ccdce4.png)

图3.3 Mosaic数据增强的流程

Mosaic数据增强的代码主要流程如下：

①假设模型输入尺寸为s，首先初始化一幅尺寸为2s*2s的灰色大图

②在大图中从点A（s/2, s/2）和点B（3s/2, 3s/2）限定的矩形内随机选择一点作为拼接点

③随机选择四张图，取其部分拼入大图，超出的部分将被舍弃

④根据原图坐标的偏移量，重新计算GT框的坐标，并使用np.clip防止更新后的标签坐标越界

## 

### 3.4 模型训练

YOLOv8 的核心是其极简的API设计。我们使用 `ultralytics` 这个库来完成所有操作。

环境安装

首先，安装必需的库：

> `pip install ultralytics`

训练需要准备符合YOLO格式的数据集。本系统中涉及的垃圾数据集结构如下：

> my_dataset/  
>  ├── images/  
>  │   ├── train/  
>  │   └── val/  
>  └── labels/  
>      ├── train/  
>      └── val/

每个图像对应一个 `.txt` 标签文件，格式为：`class_id center_x center_y width height`（归一化坐标）。

​**​训练代码 (`train.py`)​**​:

```
from ultralytics import YOLO

# 加载一个预训练模型
# 可选模型: yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt
model = YOLO('yolov8s.pt')  # 加载一个小型预训练模型

# 训练模型
results = model.train(
    data='my_dataset/data.yaml',  # 数据配置文件路径
    epochs=100,                   # 训练轮数
    imgsz=640,                   # 输入图像尺寸
    batch=16,                    # 批次大小（根据GPU内存调整）
    name='yolov8s_custom',       # 本次训练运行的名称
    device='cuda',               # 设备： 'cuda', 'cpu', 或 'cuda:0'
    workers=4,                   # 数据加载的工作线程数
    lr0=0.01,                    # 初始学习率
    lrf=0.01,                    # 最终学习率 (lr0 * lrf)
    dropout=0.2                  # 使用dropout正则化（仅适用于大型模型，如l和x）
)
```

​**​`data.yaml` 文件示例​**​:

```
# data.yaml
path: /path/to/my_dataset  # 数据集根目录
train: images/train        # 训练集路径（相对于path）
val: images/val           # 验证集路径（相对于path）
names:                    # 类别名称列表
  0: garbage
```

运行 `python train.py` 即可开始训练。训练结果和模型权重会保存在 `runs/train`目录下

![](https://i-blog.csdnimg.cn/direct/44057e97fdba4660af620dc09fd5d9af.png)

模型训练过程中，为了记录模型训练的情况，比如模型是否收敛、是否训练充分、模型当前精度等，我们会记录一系列的指标，包括loss曲线、map曲线等，如下图所示。

![](https://i-blog.csdnimg.cn/direct/4a4e6637f9eb42268686eaffd24b9d30.png)

模型训练好之后，需要用评估指标对训练好的模型效果进行定量评估。一般通过PR曲线即可看出模型的效果。P-R曲线是根据模型的预测结果对样本进行排序，把最有可能是正样本的个体排在前面，而后面的则是模型认为最不可能为正例的样本，再按此顺序逐个把样本作为“正例”进行预测并计算出当前的准确率和召回率得到的曲线。PR曲线中的P代表的是Precision（精准率）,R代表的是Recall（召回率），其代表的是精准率与召回率的关系，一般情况下，将recall设置为横坐标，precision设置为纵坐标。PR曲线下围成的面积即AP，所有类别AP平均值即map。如下图所示：

![](https://i-blog.csdnimg.cn/direct/1fdca119f50b48f481902eea69695005.png)

### 3.5 模型推理(系统后端开发)

训练完成后，使用最佳模型 `runs/train/.../weights/best.pt` 进行推理。我们依然使用ultralytics框架进行推理。

a. 图片推理

```
from ultralytics import YOLO
import cv2

# 加载自定义训练的最佳模型
model = YOLO('runs/train/yolov8s_custom/weights/best.pt')

# 单张图片推理
results = model('path/to/your/image.jpg',
                conf=0.25,  # 置信度阈值
                iou=0.7,    # NMS的IoU阈值
                show=False   # 是否显示结果（在支持的环境下）
                )

# 处理结果列表（一张图片所以取第一个）
result = results[0]

# 使用OpenCV绘制结果
annotated_frame = result.plot()  # 返回一个带标注的BGR图像数组

# 显示图片
cv2.imshow('YOLOv8 Inference', annotated_frame)
cv2.waitKey(0)
cv2.destroyAllWindows()

# 或者保存图片
cv2.imwrite('output.jpg', annotated_frame)
```

b. 视频推理

```
from ultralytics import YOLO
import cv2

model = YOLO('runs/train/yolov8s_custom/weights/best.pt')

# 打开视频文件
video_path = "path/to/your/video.mp4"
cap = cv2.VideoCapture(video_path)

# 获取视频属性，为输出视频做准备
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = int(cap.get(cv2.CAP_PROP_FPS))
output_path = 'output_video.mp4'

# 定义编码器并创建VideoWriter对象
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# 循环读取视频帧
while cap.isOpened():
    success, frame = cap.read()
    if not success:
        break

    # 在当前帧上运行YOLOv8推理
    results = model(frame, conf=0.25, iou=0.7, verbose=False) # verbose=False减少输出

    # 可视化结果
    annotated_frame = results[0].plot()

    # 将带标注的帧写入输出视频
    out.write(annotated_frame)

    # （可选）实时显示带标注的流
    cv2.imshow('YOLOv8 Inference', annotated_frame)

    # 按 'q' 退出循环
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# 释放资源
cap.release()
out.release()
cv2.destroyAllWindows()
```

### 3.6 系统前端开发

前端采用Streamlit框架开发。Streamlit是一个专门为机器学习和数据科学团队设计的应用开发框架，它的主要目标是帮助数据科学家和开发人员更快速地创建和部署交互式数据应用程序，以便将研究成果转化为实际应用。 这个框架的主要优势在于其易用性和高效性。使用Streamlit，开发者只需要几行代码就可以创建出交互式应用，无需编写HTML、CSS或JavaScript。同时，Streamlit支持大量表格、图表、数据表等对象的渲染，并封装了大量互动组件，如滑动条、单选框、复选框等，使得应用界面丰富且用户友好。 Streamlit也支持实时预览，即在编辑代码时，应用会自动重新加载，开发者可以实时查看更改的效果。此外，Streamlit还可以自动调整布局和大小，以适应不同的屏幕和设备，实现栅格化响应式布局。 Streamlit的另一个显著特点是对Python数据科学库的广泛支持，如Pandas、Numpy、Matplotlib、Scikit-Learn等，这使得开发者可以利用这些库的功能，直接在Streamlit应用中实现数据分析和可视化。 Streamlit的应用场景非常广泛，比如可以用于搭建预测模型的Web应用，或者用于自动化处理Excel文件的Web应用等。它可以帮助用户将复杂的机器学习或者深度学习模型或数据处理流程转化为易于理解和使用的Web界面，从而方便非专业人员的使用。 总的来说，Streamlit是一个功能强大且易于使用的应用开发框架，特别适合于机器学习和数据科学领域的快速应用开发。

核心框架​​：Streamlit（轻量级Web应用框架）  
 辅助库​​：OpenCV（视频处理）、Pillow（图像处理）、Pandas（数据展示）  
 响应式设计​​：自适应PC/移动端，布局采用Centered模式

![](https://i-blog.csdnimg.cn/direct/944fd09f4ebc4356b5e8c07b9b2f5bbd.png)

侧边栏控制面板​：

```
with st.sidebar:
    st.header("配置面板")
    
    # 模型选择器
    model_type = st.selectbox("选取模型", MODEL_LIST)
    
    # 双参数滑动调节
    confidence = st.slider("置信度阈值", 10, 100, 25) / 100
    iou = st.slider("IOU阈值", 10, 100, 45) / 100
    
    # 检测模式切换
    detection_mode = st.radio('检测类型', 
                             ("图片检测", "视频检测", '本地摄像头检测'))
```

图片检测模块：

```
if detection_mode == "图片检测":
    uploaded_file = st.file_uploader("上传垃圾图片", type=['png', 'jpeg', 'jpg'])
    
    if uploaded_file:
        # 显示原图
        st.sidebar.image(uploaded_file, caption="原始图片")
        
        # 执行推理
        with st.spinner('检测中...'):
            annotated_img, stats, detections = infer_image(model, Image.open(uploaded_file))
            
            # 显示结果
            st.image(annotated_img, channels='BGR', caption="检测结果")
            
            # 结构化数据展示
            st.subheader("检测明细")
            st.dataframe(pd.DataFrame(detections, 
                           columns=['序号', '类别', '置信度', '坐标位置']))
            
            st.subheader("类别统计")
            st.dataframe(pd.DataFrame(list(stats.items()), 
                           columns=['垃圾类别', '数量']))
```

视频检测模块：

```
elif detection_mode == "视频检测":
    video_file = st.file_uploader("上传垃圾视频", type=['mp4'])
    
    if video_file:
        st.sidebar.video(video_file)
        
        # 创建临时视频文件
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(video_file.read())
        
        # 视频流处理
        video_placeholder = st.empty()
        cap = cv2.VideoCapture(tfile.name)
        
        with st.spinner('视频分析中...'):
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret: break
                
                # 跳帧处理（性能优化）
                if frame_count % 2 == 0:
                    processed_frame = infer_video_frame(model, frame)
                    video_placeholder.image(processed_frame, 
                                          channels='BGR', 
                                          caption="实时检测")
```

## 4. 系统功能介绍

可进行各种垃圾进行检测与识别

支持图片、视频及摄像头进行检测

界面可实时显示目标位置、目标总数、置信度等信息

具体功能见如下视频演示所示：

![](https://i-blog.csdnimg.cn/direct/122f5abd9f434530bfc4ee29f4f0c771.gif)

## 5.结语

综上所述，本系统通过先进的视觉识别技术（涵盖静态图片分析、动态视频流解析及实时摄像头监控），构建了全方位、多层次的智能垃圾检测与分类体系。系统依托深度学习领域的YOLOv8图像识别算法，能够精准识别各类垃圾特征，对公共场所、社区环境中的垃圾违规堆放实现毫秒级响应与智能分类提示，有效提升环境监管效率。这一技术解决方案不仅为破解"垃圾识别与分类"难题提供了智能化路径，更通过自动化监管显著降低了人工分拣成本，推动环境治理从被动清理转向主动防控。随着系统的落地应用，将切实提升垃圾分类效率与准确率，为智慧城市建设和人居环境治理提供关键技术支撑。

另外，限于本篇文章的篇幅，无法一一细致讲解系统原理、项目代码、模型训练等细节，需要数据集、项目源码、训练代码、系统原理说明文章的小伙伴可以从下面的链接中下载：

[【数据集+源码+论文】yolov8 垃圾检测识别系统](https://mbd.pub/o/bread/YZWXkp1raQ== "【数据集+源码+论文】yolov8 垃圾检测识别系统")



