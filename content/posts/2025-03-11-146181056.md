---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f47756f5f507974686f6e2f:61727469636c652f64657461696c732f313436313831303536"
layout: post
title: "å¤šæ¨¡æ€å¤§æ¨¡å‹Qwen2.5-vlæœ¬åœ°éƒ¨ç½²æŒ‡å—"
date: 2025-03-11 16:59:16 +08:00
description: "Qwen2.5-VL æ˜¯é€šä¹‰åƒé—®ç³»åˆ—çš„æœ€æ–°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå…·å¤‡å›¾æ–‡ç†è§£ã€è§†è§‰æ¨ç†ã€æ–‡æ¡£è§£æç­‰å¼ºå¤§èƒ½åŠ›ï¼Œå¹¿æ³›åº”ç”¨äºæ™ºèƒ½æœç´¢ã€å†…å®¹ç”Ÿæˆã€ä¼ä¸šæ–‡æ¡£å¤„ç†ç­‰é¢†åŸŸã€‚ğŸ”¹ ä¸»è¦åŠŸèƒ½âœ… å¤šæ¨¡æ€é—®ç­”ï¼šè§£æå›¾ç‰‡ã€å›¾è¡¨ã€æ–‡æ¡£ï¼Œå›ç­”é—®é¢˜ï¼Œæ”¯æŒ OCR è¯†åˆ«ã€‚âœ… å¤æ‚æ–‡æ¡£è§£æï¼šæå–å‘ç¥¨ã€åˆåŒã€PPTã€è¡¨æ ¼ç­‰æ–‡ä»¶ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚âœ… é«˜çº§è§†è§‰æ¨ç†ï¼šç†è§£å›¾åƒä¸­çš„å…³ç³»ï¼Œå¦‚å› æœæ¨ç†ã€æ•°æ®åˆ†æã€‚âœ… æ™ºèƒ½æ‘˜è¦ä¸ç”Ÿæˆï¼šè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°ã€æ–‡æ¡£æ‘˜è¦ï¼Œæé«˜ä¿¡æ¯è·å–æ•ˆç‡ã€‚"
keywords: "å¤šæ¨¡æ€å¤§æ¨¡å‹Qwen2.5 vlæœ¬åœ°éƒ¨ç½²æŒ‡å—"
categories: ['å¤§æ¨¡å‹']
tags: ['äººå·¥æ™ºèƒ½']
artid: "146181056"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146181056
    alt: "å¤šæ¨¡æ€å¤§æ¨¡å‹Qwen2.5-vlæœ¬åœ°éƒ¨ç½²æŒ‡å—"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146181056
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146181056
cover: https://bing.ee123.net/img/rand?artid=146181056
image: https://bing.ee123.net/img/rand?artid=146181056
img: https://bing.ee123.net/img/rand?artid=146181056
---

# å¤šæ¨¡æ€å¤§æ¨¡å‹Qwen2.5 vlæœ¬åœ°éƒ¨ç½²æŒ‡å—

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/9b8e39aceb4b4a9387d3cb3ab1f4d4b2.png)
  
Qwen2.5-VL æ˜¯é€šä¹‰åƒé—®ç³»åˆ—çš„æœ€æ–°å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œå…·å¤‡å›¾æ–‡ç†è§£ã€è§†è§‰æ¨ç†ã€æ–‡æ¡£è§£æç­‰å¼ºå¤§èƒ½åŠ›ï¼Œå¹¿æ³›åº”ç”¨äºæ™ºèƒ½æœç´¢ã€å†…å®¹ç”Ÿæˆã€ä¼ä¸šæ–‡æ¡£å¤„ç†ç­‰é¢†åŸŸã€‚

ğŸ”¹ ä¸»è¦åŠŸèƒ½
  
âœ… å¤šæ¨¡æ€é—®ç­”ï¼šè§£æå›¾ç‰‡ã€å›¾è¡¨ã€æ–‡æ¡£ï¼Œå›ç­”é—®é¢˜ï¼Œæ”¯æŒ OCR è¯†åˆ«ã€‚
  
âœ… å¤æ‚æ–‡æ¡£è§£æï¼šæå–å‘ç¥¨ã€åˆåŒã€PPTã€è¡¨æ ¼ç­‰æ–‡ä»¶ä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ã€‚
  
âœ… é«˜çº§è§†è§‰æ¨ç†ï¼šç†è§£å›¾åƒä¸­çš„å…³ç³»ï¼Œå¦‚å› æœæ¨ç†ã€æ•°æ®åˆ†æã€‚
  
âœ… æ™ºèƒ½æ‘˜è¦ä¸ç”Ÿæˆï¼šè‡ªåŠ¨ç”Ÿæˆå›¾ç‰‡æè¿°ã€æ–‡æ¡£æ‘˜è¦ï¼Œæé«˜ä¿¡æ¯è·å–æ•ˆç‡ã€‚
  
âœ… ä»£ç ä¸ UI è§£æï¼šè¯†åˆ«æˆªå›¾ä¸­çš„ä»£ç /UI è®¾è®¡ï¼Œç”Ÿæˆå¯æ‰§è¡Œä»£ç æˆ–äº¤äº’è¯´æ˜ã€‚

### ä¸€. ç¯å¢ƒå‡†å¤‡

æœºå™¨ï¼š4090
  
python: 3.10
  
cuda: 12.2

```bash
# ç½‘ç»œä¸å¥½ï¼Œå¯èƒ½éœ€è¦å°è¯•å‡ æ¬¡
pip install git+https://github.com/huggingface/transformers accelerate
pip install qwen-vl-utils[decord]

# è·‘ä»£ç æ—¶ç¼ºå°‘åŒ…
pip install torchvision==0.19.0

```

### äºŒ. ä¸‹è½½æ¨¡å‹

```python
from modelscope import snapshot_download
model_dir = snapshot_download('Qwen/Qwen2.5-VL-7B')

```

### ä¸‰. æ¨ç†ä»£ç å°è£…

```python
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch


class QwenVLModel:
    def __init__(self, model_path="./Qwen2.5-VL-7B-Instruct", use_flash_attention=False):
        """
        åˆå§‹åŒ–Qwen VLæ¨¡å‹
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            use_flash_attention: æ˜¯å¦ä½¿ç”¨flash attentionåŠ é€Ÿ
        """
        # åŠ è½½æ¨¡å‹
        if use_flash_attention:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_path,
                torch_dtype=torch.bfloat16,
                attn_implementation="flash_attention_2",
                device_map="auto",
            )
        else:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_path, torch_dtype="auto", device_map="auto"
            )

        # åˆå§‹åŒ–å¤„ç†å™¨
        min_pixels = 256*28*28
        max_pixels = 1280*28*28
        self.processor = AutoProcessor.from_pretrained(
            model_path, 
            min_pixels=min_pixels, 
            max_pixels=max_pixels, 
            use_fast=True
        )

    def process_image(self, image_path, prompt):
        """
        å¤„ç†å›¾ç‰‡å¹¶ç”Ÿæˆè¾“å‡º
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            prompt: æç¤ºæ–‡æœ¬
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬è¾“å‡º
        """
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "image": image_path,
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        # å‡†å¤‡æ¨ç†è¾“å…¥
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.model.device)

        # ç”Ÿæˆè¾“å‡º
        generated_ids = self.model.generate(**inputs, max_new_tokens=512)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )
        return output_text



if __name__ == "__main__":
    model = QwenVLModel()
    img_path = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg"
    output_text = model.process_image(
        img_path,
        "è¯·ç”¨ä¸­æ–‡æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡"
    )
    print(f"è¾“å‡ºä¿¡æ¯: {output_text}")

```

### å››. æµ‹è¯•æ•ˆæœ

å›¾ç‰‡
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/6f9823500cd5429da097732ddf0fd12d.jpeg)
  
æ¨¡å‹è¾“å‡ºç»“æœï¼š

```bash
è¾“å‡ºä¿¡æ¯: ['è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€ä½å¥³å£«å’Œä¸€åªç‹—åœ¨æµ·æ»©ä¸Šäº’åŠ¨çš„åœºæ™¯ã€‚å¥³å£«ååœ¨æ²™æ»©ä¸Šï¼Œç©¿ç€æ ¼å­è¡¬è¡«å’Œé»‘è‰²è£¤å­ï¼Œé¢å¸¦å¾®ç¬‘ï¼Œä¼¼ä¹åœ¨ä¸ç‹—è¿›è¡Œå‹å¥½äº’åŠ¨ã€‚ç‹—æˆ´ç€å½©è‰²çš„é¡¹åœˆï¼Œæ­£ä¼¸å‡ºå‰çˆªä¸å¥³å£«çš„æ‰‹ç›¸è§¦ç¢°ï¼Œæ˜¾å¾—éå¸¸äº²å¯†å’Œæ„‰å¿«ã€‚èƒŒæ™¯æ˜¯å¹¿é˜”çš„æµ·æ´‹å’Œå¤©ç©ºï¼Œå¤•é˜³çš„ä½™æ™–æ´’åœ¨æ²™æ»©ä¸Šï¼Œè¥é€ å‡ºä¸€ç§æ¸©é¦¨å’Œè°çš„æ°›å›´ã€‚æ•´ä¸ªç”»é¢ç»™äººä¸€ç§è½»æ¾æ„‰å¿«çš„æ„Ÿè§‰ã€‚']

```