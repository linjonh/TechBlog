---
layout: post
title: "DeepSeek-V3-源码从入门到放弃"
date: 2025-03-06 20:00:00 +0800
description: "这段代码定义了一个多头注意力层（MLA）类。在初始化时，根据传入的参数设置各种维度、低秩投影的秩等，并初始化相应的线性层和归一化层，同时根据注意力实现方式注册不同的缓存。在前向传播过程中，对输入进行处理得到查询、键和值，应用旋转位置编码，根据不同的注意力实现方式计算注意力分数，最后通过注意力分数和缓存得到输出并进行线性变换。这段代码定义了一个混合专家（MoE）模块。在初始化时，根据传入的参数设置专家的数量、门控机制、专家模块列表和共享专家模块。"
keywords: "deepseek源码分析"
categories: ['机器学习笔记']
tags: ['人工智能']
artid: "146063624"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146063624
    alt: "DeepSeek-V3-源码从入门到放弃"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146063624
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146063624
cover: https://bing.ee123.net/img/rand?artid=146063624
image: https://bing.ee123.net/img/rand?artid=146063624
img: https://bing.ee123.net/img/rand?artid=146063624
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     DeepSeek V3 源码：从入门到放弃！
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="_0">
     </a>
     从入门到放弃
    </h2>
    <p>
     花了几天时间，看懂了DeepSeek V3 源码的逻辑。源码的逻辑是不难的，但为什么模型结构需要这样设计，为什么参数需要这样设置呢？知其然，但不知其所以然。除了模型结构以外，模型的训练数据、训练脚本和训练经验，也是DeepSeek V3能够训练出来的关键，但这些是DeepSeek母公司的核心机密，我们无从得知。
     <br/>
     因此，看懂了源码，算是
     <strong>
      入门
     </strong>
     了DeepSeek V3，因为没有条件知道更多重要细节，因此不得不
     <strong>
      放弃
     </strong>
     重现整个模型的训练。
    </p>
    <h2>
     <a id="Paper__4">
     </a>
     Paper 和源码
    </h2>
    <p>
     Paper URL: https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
     <br/>
     Code URL: https://github.com/deepseek-ai/DeepSeek-V3
    </p>
    <h2>
     <a id="_9">
     </a>
     模型逻辑
    </h2>
    <p>
     下面这张图，代表了DeepSeek的核心逻辑。左边是Transformer的逻辑结构，可以认为有N个左边这样的Block结构不断重复，组成Transformer模型。每个Block中，分成两个部分，Attention 和 Feed-Forward Network。对这两个部分使用不同的网络结构，我们就得到了不同的模型。
     <br/>
     DeepSeek V3 的 Attention 用的是 Multi-Head Latent Attention（MLA) ，Feed-Forward Network 用的是DeepSeekMoE。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fa13e1ff224a422180a727b3b1b15108.png"/>
    </p>
    <h3>
     <a id="MLA_13">
     </a>
     MLA
    </h3>
    <p>
     Multi-Head Latent Attention（MLA）即多头潜在注意力，是DeepSeek模型中引入的一种创新注意力机制，旨在优化传统多头注意力（Multi-Head Attention，MHA）的计算效率和内存占用。具体介绍如下：
    </p>
    <h4>
     <a id="_15">
     </a>
     核心创新点
    </h4>
    <ul>
     <li>
      <strong>
       低秩键值压缩
      </strong>
      <ul>
       <li>
        <strong>
         KV的低秩压缩
        </strong>
        ：不直接存储原始的Key和Value，而是先将隐藏状态投影到一个更小的压缩潜在向量。在推理时，只需缓存该压缩潜在向量，而不是完整的Key和Value，从而大大降低了KV缓存的存储需求。
       </li>
       <li>
        <strong>
         Query的低秩压缩
        </strong>
        ：对Query也进行低秩压缩，虽然不会减少KV缓存的大小，但可以减少训练时的激活存储需求，进而降低计算成本。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       解耦旋转位置嵌入（RoPE）
      </strong>
      <ul>
       <li>
        <strong>
         额外引入“解耦查询”
        </strong>
        ：将查询拆分为两个部分，一部分不经过RoPE变换，代表非位置敏感的特征信息；另一部分专门用于嵌入RoPE位置编码信息。
       </li>
       <li>
        <strong>
         共享RoPE变换的Key
        </strong>
        ：所有注意力头共用一个旋转变换后的Key，减少了计算开销，也减小了KV缓存大小，降低了GPU内存占用，提高了推理速度，特别适用于长序列任务和大规模Transformer。
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="_22">
     </a>
     推理过程中的优化
    </h4>
    <p>
     将上投影矩阵吸收到里面，简化查询计算，并优化注意力分数的计算，减少了计算步骤，提升了计算效率。避免了先计算Value向量，减少了矩阵运算的开销，使推理更快。
    </p>
    <h4>
     <a id="_24">
     </a>
     整体优势
    </h4>
    <ul>
     <li>
      <strong>
       降低内存占用
      </strong>
      ：通过对键值进行低秩联合压缩以及解耦RoPE等策略，显著减少了KV缓存的存储需求，降低了GPU内存占用。
     </li>
     <li>
      <strong>
       提高计算效率
      </strong>
      ：减少了训练和推理过程中的计算量，加快了模型的推理速度，在保持甚至提高模型性能的同时，提升了模型的运行效率。
     </li>
     <li>
      <strong>
       增强模型适应性
      </strong>
      ：特别适用于长序列任务和大规模Transformer模型，能够更好地处理长序列输入，提高模型在各种自然语言处理任务中的表现。
     </li>
    </ul>
    <h3>
     <a id="MLA__29">
     </a>
     MLA 有物理意义吗？
    </h3>
    <p>
     Multi-Head Latent Attention（MLA）能够起作用主要源于其独特的技术设计，在数学和信息处理层面有清晰的逻辑，不过它是一种抽象的算法概念，并不直接对应具体的物理意义，以下是对其作用原理的分析：
    </p>
    <h4>
     <a id="_32">
     </a>
     起作用的原因
    </h4>
    <ul>
     <li>
      <strong>
       低秩压缩的有效性
      </strong>
      <ul>
       <li>
        <strong>
         信息浓缩与降噪
        </strong>
        ：通过低秩键值压缩，MLA将高维的Key和Value信息投影到低维的潜在向量空间，这一过程类似于对原始信息进行浓缩，提取出最关键、最具代表性的特征，去除了一些可能的噪声和冗余信息，使得模型能够更聚焦于重要信息，从而提高信息处理的效率和准确性。
       </li>
       <li>
        <strong>
         减少计算量和存储需求
        </strong>
        ：低秩压缩大大降低了数据的维度，减少了模型训练和推理过程中的计算量和存储需求，使得模型能够更高效地运行，尤其是在处理大规模数据和长序列数据时，这种优势更为明显。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       解耦旋转位置嵌入的优势
      </strong>
      <ul>
       <li>
        <strong>
         位置信息与内容信息的分离
        </strong>
        ：传统的位置编码方式将位置信息和内容信息混合在一起进行处理，而MLA的解耦旋转位置嵌入将查询拆分为位置敏感和非位置敏感两部分，使模型能够更清晰地分离和处理位置信息与内容信息，更好地捕捉文本中的长距离依赖关系。
       </li>
       <li>
        <strong>
         共享RoPE变换的Key
        </strong>
        ：所有注意力头共用一个旋转变换后的Key，不仅减少了计算开销，还使得模型能够从更宏观的角度利用位置信息，增强了模型对序列数据整体结构的理解和把握能力。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       多头机制的协同作用
      </strong>
      <ul>
       <li>
        <strong>
         捕捉多维度信息
        </strong>
        ：MLA中的多头机制允许模型同时从多个不同的角度和维度去捕捉输入数据中的信息，每个头可以关注到输入序列的不同方面，通过多个头的并行计算和协同工作，模型能够更全面、更深入地理解输入数据，提高模型的表示能力和泛化能力。
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="_42">
     </a>
     难以直接赋予物理意义的原因及近似理解
    </h4>
    <ul>
     <li>
      <strong>
       抽象的算法概念
      </strong>
      ：MLA是一种基于数学和计算机科学的算法概念，主要用于处理和分析数据中的模式和关系，它不像物理概念那样具有直接可观测的物理实体或现象与之对应，更多地是在数据空间和计算逻辑中发挥作用。
     </li>
     <li>
      <strong>
       类比物理现象理解
      </strong>
      ：可以进行一些类比来帮助理解。比如低秩压缩类似于物理中的能量聚集，将分散的能量（信息）聚集到关键的“点”上；解耦旋转位置嵌入有点像物理中对不同性质力的分解，将位置信息和内容信息这两种“力”分开处理；多头机制如同多个物理传感器从不同方向和角度对环境进行感知，然后综合这些感知信息来对整个系统进行理解和判断。
     </li>
    </ul>
    <h3>
     <a id="DeepSeekMoE_46">
     </a>
     DeepSeekMoE
    </h3>
    <p>
     DeepSeekMoE是由深度求索（DeepSeek）研发的基于混合专家系统（Mixture of Experts，MoE）的技术架构，以下是具体介绍：
    </p>
    <h4>
     <a id="_49">
     </a>
     架构原理
    </h4>
    <ul>
     <li>
      <strong>
       混合专家系统核心
      </strong>
      ：采用MoE架构，核心在于通过动态路由机制，把输入数据分配给最相关的专家处理。比如在自然语言处理中，有的专家专门处理情感分析，有的处理主题建模。
     </li>
     <li>
      <strong>
       结合多头潜在注意力机制
      </strong>
      ：与MLA相结合，MLA通过引入潜在向量，减少键值缓存（KV cache）需求，提升推理效率。
     </li>
     <li>
      <strong>
       Transformer架构基础
      </strong>
      ：以Transformer架构为基础，每个Transformer块由一个注意力模块和一个前馈网络（FFN）组成，在注意力机制和FFN方面采用创新架构。
     </li>
    </ul>
    <h4>
     <a id="_53">
     </a>
     技术优势
    </h4>
    <ul>
     <li>
      <strong>
       降低算力需求
      </strong>
      ：MoE的动态分配机制和MLA减少KV缓存需求等特点，使模型在训练和推理时对算力的要求降低。
     </li>
     <li>
      <strong>
       保持高性能
      </strong>
      ：在参数量减少的情况下仍能保持高性能，例如DeepSeek-V2以236B总参数、21B激活，大致可以达到70B-110B Dense的模型能力。
     </li>
     <li>
      <strong>
       减少计算量
      </strong>
      ：自研Sparse结构DeepSeekMoE进一步降低了计算量。
     </li>
     <li>
      <strong>
       长上下文理解能力强
      </strong>
      ：支持超100万token的上下文窗口，显著优于行业平均水平，适用于长文档分析、代码开发等复杂场景的连贯交互。
     </li>
    </ul>
    <h3>
     <a id="DeepSeekMoE_59">
     </a>
     DeepSeekMoE的物理意义是什么？
    </h3>
    <p>
     DeepSeekMoE作为一种人工智能技术架构，没有严格意义上的物理意义，但可以从一些角度进行类比和理解：
    </p>
    <h4>
     <a id="_61">
     </a>
     从系统资源分配角度
    </h4>
    <ul>
     <li>
      <strong>
       资源按需分配类比
      </strong>
      ：可以将DeepSeekMoE的专家网络和动态路由机制类比为一个智能电力分配系统。在这个系统中，不同的电器设备（任务）需要不同的电量（计算资源）来运行。专家网络就像不同功率的发电机，而动态路由机制则像是智能电表和分配器，它会根据每个电器设备的实际需求，将电力（计算资源）精准地分配给需要的设备，避免了资源的浪费，提高了整个系统的能源利用效率。
     </li>
     <li>
      <strong>
       负载均衡类比
      </strong>
      ：类似于在一个大型物流中心，不同的仓库区域（专家）负责存储和处理不同类型的货物（数据）。当有货物运输任务时，调度系统（动态路由）会根据货物的特点和仓库的负载情况，合理地安排货物存储到哪个仓库，确保每个仓库都能在其承载能力范围内高效运作，不会出现某个仓库过度拥挤而其他仓库闲置的情况，实现了负载均衡，提高了物流中心的整体运营效率。
     </li>
    </ul>
    <h4>
     <a id="_65">
     </a>
     从信息处理角度
    </h4>
    <ul>
     <li>
      <strong>
       多维度信息处理类比
      </strong>
      ：可以把DeepSeekMoE处理信息的过程想象成一个由多个不同专业的侦探（专家）组成的侦探团队在调查一个复杂案件。每个侦探都有自己独特的专业技能和视角，比如有的擅长调查线索，有的擅长分析人物关系，有的擅长破解密码等。当面对案件（输入数据）时，队长（路由器）会根据案件的具体情况，分配合适的侦探去处理相应的部分，最后将各个侦探的调查结果综合起来，形成对整个案件的全面了解和判断，从而更高效地解决复杂问题。
     </li>
     <li>
      <strong>
       特征提取与融合类比
      </strong>
      ：如同在一个化学实验中，不同的化学试剂（专家）可以与不同的物质发生反应，提取出特定的化学特征。DeepSeekMoE中的专家网络就像这些化学试剂，它们各自对输入数据进行处理，提取出不同的特征。然后通过融合机制，将这些特征像混合化学物质一样进行整合，得到更全面、更有价值的信息，用于后续的分析和决策。
     </li>
    </ul>
    <h4>
     <a id="_69">
     </a>
     从模型架构角度
    </h4>
    <ul>
     <li>
      <strong>
       积木搭建类比
      </strong>
      ：把DeepSeekMoE的架构比作搭建积木。每个专家网络就像不同形状和功能的积木块，有的积木块负责搭建基础结构，有的负责构建上层建筑，有的负责添加装饰等。路由器则像是搭建者的手，根据要搭建的目标模型的需求，选择合适的积木块进行组合，最终搭建出一个复杂而功能强大的模型结构，实现对各种自然语言处理任务的高效处理。
     </li>
     <li>
      <strong>
       人体神经系统类比
      </strong>
      ：可以将DeepSeekMoE类比为人体的神经系统。专家网络类似于人体的不同神经细胞或神经中枢，它们各自负责处理特定类型的信息，如视觉神经细胞负责处理视觉信息，听觉神经细胞负责处理听觉信息等。路由器就像神经系统中的神经递质或信号传导机制，它负责将外界的刺激信号（输入数据）准确地传递给相应的神经细胞，并将各个神经细胞处理后的信号进行整合和传递，使人体能够做出协调的反应和决策，实现对外部世界的感知和交互。
     </li>
    </ul>
    <h2>
     <a id="_73">
     </a>
     代码逻辑
    </h2>
    <h3>
     <a id="__Transformer_74">
     </a>
     整体 - Transformer
    </h3>
    <p>
     下面这段代码是典型的 Transformer 实现，核心可以看 forward 函数逻辑：
    </p>
    <ol>
     <li>
      进行 Embeding；
     </li>
     <li>
      经过各个 Block；
     </li>
     <li>
      归一化并输出。
      <br/>
      对应的代码：
     </li>
    </ol>
    <pre><code class="prism language-dart"># 通过嵌入层将输入标记转换为向量表示
h <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">embed</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
# 依次通过每个<span class="token class-name">Transformer</span>块进行处理
<span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
    h <span class="token operator">=</span> <span class="token function">layer</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
# 对输出进行层归一化，并取最后一个时间步的输出
h <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">norm</span><span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
# 通过输出投影层得到对数概率
logits <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token function">head</span><span class="token punctuation">(</span>h<span class="token punctuation">)</span>
</code></pre>
    <p>
     完整代码：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 定义Transformer类，继承自PyTorch的nn.Module类</span>
<span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Transformer模型，包含位置嵌入、多个层以及输出投影。

    属性:
        max_seq_len (int): Transformer允许的最大序列长度。
        embed (nn.Module): 用于输入标记的嵌入层，将输入的标记转换为向量表示。
        layers (torch.nn.ModuleList): 存储多个Transformer块的列表，每个块包含多头注意力和前馈网络。
        norm (nn.Module): 层归一化层，在所有Transformer块之后应用，用于稳定训练。
        head (nn.Module): 输出投影层，将模型的输出映射到词汇表大小，用于预测下一个标记。
        freqs_cis (torch.Tensor): 预计算的复指数值，用于旋转位置嵌入，帮助模型捕捉序列中的位置信息。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化Transformer模型。

        参数:
            args: 模型参数对象，包含Transformer的各种参数，如词汇表大小、维度、层数等。
        """</span>
        <span class="token comment"># 获取全局变量world_size和rank，分别表示分布式训练中的进程总数和当前进程的编号</span>
        <span class="token keyword">global</span> world_size<span class="token punctuation">,</span> rank
        <span class="token comment"># 如果分布式训练已初始化，则获取进程总数，否则默认为1</span>
        world_size <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_world_size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> dist<span class="token punctuation">.</span>is_initialized<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">1</span>
        <span class="token comment"># 如果分布式训练已初始化，则获取当前进程编号，否则默认为0</span>
        rank <span class="token operator">=</span> dist<span class="token punctuation">.</span>get_rank<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> dist<span class="token punctuation">.</span>is_initialized<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">0</span>
        <span class="token comment"># 根据参数设置线性层的数据类型</span>
        Linear<span class="token punctuation">.</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float8_e4m3fn <span class="token keyword">if</span> args<span class="token punctuation">.</span>dtype <span class="token operator">==</span> <span class="token string">"fp8"</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>bfloat16
        <span class="token comment"># 调用父类的初始化方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存最大序列长度</span>
        self<span class="token punctuation">.</span>max_seq_len <span class="token operator">=</span> args<span class="token punctuation">.</span>max_seq_len
        <span class="token comment"># 初始化嵌入层，将输入标记转换为向量表示</span>
        self<span class="token punctuation">.</span>embed <span class="token operator">=</span> ParallelEmbedding<span class="token punctuation">(</span>args<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 初始化一个空的ModuleList，用于存储Transformer块</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 循环创建指定数量的Transformer块，并添加到layers列表中</span>
        <span class="token keyword">for</span> layer_id <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>n_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Block<span class="token punctuation">(</span>layer_id<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 初始化层归一化层</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 初始化输出投影层，将模型的输出映射到词汇表大小</span>
        self<span class="token punctuation">.</span>head <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>vocab_size<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>get_default_dtype<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 预计算旋转位置嵌入所需的复指数值，并将其注册为缓冲区，不参与模型参数的更新</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"freqs_cis"</span><span class="token punctuation">,</span> precompute_freqs_cis<span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@torch<span class="token punctuation">.</span>inference_mode</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> start_pos<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Transformer模型的前向传播过程。

        参数:
            tokens (torch.Tensor): 输入的标记ID张量，形状为 (batch_size, seq_len)。
            start_pos (int, 可选): 旋转位置嵌入的起始位置，默认为0。

        返回:
            torch.Tensor: 对数概率张量，形状为 (batch_size, vocab_size)，表示每个标记的预测概率。
        """</span>
        <span class="token comment"># 获取输入序列的长度</span>
        seqlen <span class="token operator">=</span> tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 通过嵌入层将输入标记转换为向量表示</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>embed<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
        <span class="token comment"># 从预计算的复指数值中截取当前序列所需的部分</span>
        freqs_cis <span class="token operator">=</span> self<span class="token punctuation">.</span>freqs_cis<span class="token punctuation">[</span>start_pos<span class="token punctuation">:</span>start_pos <span class="token operator">+</span> seqlen<span class="token punctuation">]</span>
        <span class="token comment"># 初始化掩码为None</span>
        mask <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token comment"># 如果序列长度大于1，则创建一个上三角掩码，用于屏蔽未来的标记</span>
        <span class="token keyword">if</span> seqlen <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>full<span class="token punctuation">(</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> seqlen<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>tokens<span class="token punctuation">.</span>device<span class="token punctuation">)</span><span class="token punctuation">.</span>triu_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 依次通过每个Transformer块进行处理</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            h <span class="token operator">=</span> layer<span class="token punctuation">(</span>h<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
        <span class="token comment"># 对输出进行层归一化，并取最后一个时间步的输出</span>
        h <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>h<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 通过输出投影层得到对数概率</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>head<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        <span class="token comment"># 如果使用分布式训练，则收集所有进程的对数概率</span>
        <span class="token keyword">if</span> world_size <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># 创建一个列表，用于存储所有进程的对数概率</span>
            all_logits <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>empty_like<span class="token punctuation">(</span>logits<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span><span class="token punctuation">]</span>
            <span class="token comment"># 收集所有进程的对数概率</span>
            dist<span class="token punctuation">.</span>all_gather<span class="token punctuation">(</span>all_logits<span class="token punctuation">,</span> logits<span class="token punctuation">)</span>
            <span class="token comment"># 将所有进程的对数概率拼接在一起</span>
            logits <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>all_logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits
</code></pre>
    <h3>
     <a id="__Block_181">
     </a>
     单个 - Block
    </h3>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/6c4c139fb2964168ac02b924dc743ad1.png">
      <br/>
      核心代码非常简单MLA(attention) + MOE(Feed-Forward Network)：
     </img>
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 首先对输入进行层归一化，然后通过注意力层进行计算，最后将结果与输入进行残差连接</span>
x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
<span class="token comment"># 接着对上述结果进行层归一化，再通过前馈网络层进行计算，最后将结果与之前的结果进行残差连接</span>
x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ffn_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     全部代码：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 定义一个Transformer块类，继承自PyTorch的nn.Module类</span>
<span class="token keyword">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Transformer块，结合了注意力层和前馈网络层。

    属性:
        attn (nn.Module): 注意力层（采用多头潜在注意力机制，即MLA），用于捕捉输入序列中不同位置之间的依赖关系。
        ffn (nn.Module): 前馈网络层（可以是多层感知机MLP或者混合专家模型MoE），对注意力层的输出进行非线性变换。
        attn_norm (nn.Module): 用于注意力层的层归一化层，对输入到注意力层的数据进行归一化处理，稳定训练过程。
        ffn_norm (nn.Module): 用于前馈网络层的层归一化层，对输入到前馈网络层的数据进行归一化处理。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer_id<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化Transformer块。

        参数:
            layer_id (int): 当前块在Transformer模型中的层索引，用于确定使用哪种前馈网络结构。
            args: 模型参数对象，包含了块的各种参数，如维度、层数等。
        """</span>
        <span class="token comment"># 调用父类的初始化方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 初始化注意力层，使用多头潜在注意力机制（MLA）</span>
        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> MLA<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 根据当前层的索引来决定使用MLP还是MoE作为前馈网络</span>
        <span class="token comment"># 如果当前层索引小于密集层的数量，则使用MLP</span>
        <span class="token comment"># 否则使用混合专家模型（MoE）</span>
        self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> MLP<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>inter_dim<span class="token punctuation">)</span> <span class="token keyword">if</span> layer_id <span class="token operator">&lt;</span> args<span class="token punctuation">.</span>n_dense_layers <span class="token keyword">else</span> MoE<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 初始化用于注意力层的层归一化层</span>
        self<span class="token punctuation">.</span>attn_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 初始化用于前馈网络层的层归一化层</span>
        self<span class="token punctuation">.</span>ffn_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Transformer块的前向传播过程。

        参数:
            x (torch.Tensor): 输入张量，包含了序列的特征信息。
            start_pos (int): 序列中的起始位置，用于旋转位置嵌入。
            freqs_cis (torch.Tensor): 预计算的复指数值，用于旋转位置嵌入，帮助模型捕捉序列中的位置信息。
            mask (Optional[torch.Tensor]): 掩码张量，用于在注意力计算中排除某些位置，避免模型关注到不应该关注的信息。

        返回:
            torch.Tensor: 经过当前Transformer块计算后的输出张量。
        """</span>
        <span class="token comment"># 首先对输入进行层归一化，然后通过注意力层进行计算，最后将结果与输入进行残差连接</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>attn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attn_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
        <span class="token comment"># 接着对上述结果进行层归一化，再通过前馈网络层进行计算，最后将结果与之前的结果进行残差连接</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ffn_norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre>
    <h3>
     <a id="Attention__245">
     </a>
     Attention 模块
    </h3>
    <p>
     经典的QKV计算公式。解释可以自行搜索，或者参考：
     <a href="https://blog.csdn.net/weixin_44153630/article/details/145993548">
      Transformer结构和注意力机制
     </a>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/39e5340ac2bc4ed9ac2bfd0ac19fa69d.png">
      <br/>
      和传统的QKV相比，可以认为是做了压缩，主要是为了减小 KV Cache。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/47d30652e49b4711bae2cf795b5a5bc2.png">
       <br/>
       代码，就是做了一堆下采样上采样和矩阵的组合变换。最终目的是减少计算量和显存使用量。
      </img>
     </img>
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 定义多头注意力层类，继承自PyTorch的nn.Module类</span>
<span class="token keyword">class</span> <span class="token class-name">MLA</span><span class="token punctuation">(</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    多头注意力层（MLA）。

    属性:
        dim (int): 输入特征的维度。
        n_heads (int): 注意力头的数量。
        n_local_heads (int): 分布式系统中本地注意力头的数量。
        q_lora_rank (int): 查询（query）的低秩投影的秩。
        kv_lora_rank (int): 键（key）和值（value）的低秩投影的秩。
        qk_nope_head_dim (int): 非位置相关的查询/键投影的维度。
        qk_rope_head_dim (int): 旋转位置编码的查询/键投影的维度。
        qk_head_dim (int): 查询/键投影的总维度。
        v_head_dim (int): 值投影的维度。
        softmax_scale (float): 注意力计算中softmax函数的缩放因子。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 调用父类的初始化方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存输入特征的维度</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim
        <span class="token comment"># 保存注意力头的数量</span>
        self<span class="token punctuation">.</span>n_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads
        <span class="token comment"># 计算分布式系统中本地注意力头的数量</span>
        self<span class="token punctuation">.</span>n_local_heads <span class="token operator">=</span> args<span class="token punctuation">.</span>n_heads <span class="token operator">//</span> world_size
        <span class="token comment"># 保存查询的低秩投影的秩</span>
        self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">=</span> args<span class="token punctuation">.</span>q_lora_rank
        <span class="token comment"># 保存键和值的低秩投影的秩</span>
        self<span class="token punctuation">.</span>kv_lora_rank <span class="token operator">=</span> args<span class="token punctuation">.</span>kv_lora_rank
        <span class="token comment"># 保存非位置相关的查询/键投影的维度</span>
        self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>qk_nope_head_dim
        <span class="token comment"># 保存旋转位置编码的查询/键投影的维度</span>
        self<span class="token punctuation">.</span>qk_rope_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>qk_rope_head_dim
        <span class="token comment"># 计算查询/键投影的总维度</span>
        self<span class="token punctuation">.</span>qk_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> args<span class="token punctuation">.</span>qk_rope_head_dim
        <span class="token comment"># 保存值投影的维度</span>
        self<span class="token punctuation">.</span>v_head_dim <span class="token operator">=</span> args<span class="token punctuation">.</span>v_head_dim

        <span class="token comment"># 如果查询的低秩投影的秩为0，直接使用列并行线性层进行查询投影</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>wq <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
        <span class="token comment"># 否则，使用低秩分解的方式进行查询投影</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>wq_a <span class="token operator">=</span> Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>q_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>wq_b <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
        <span class="token comment"># 对输入进行线性变换得到键和值的低秩表示</span>
        self<span class="token punctuation">.</span>wkv_a <span class="token operator">=</span> Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank <span class="token operator">+</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">)</span>
        <span class="token comment"># 对键和值的低秩表示进行归一化</span>
        self<span class="token punctuation">.</span>kv_norm <span class="token operator">=</span> RMSNorm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span>
        <span class="token comment"># 对归一化后的键和值进行线性变换</span>
        self<span class="token punctuation">.</span>wkv_b <span class="token operator">=</span> ColumnParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 对多头注意力的输出进行行并行线性变换</span>
        self<span class="token punctuation">.</span>wo <span class="token operator">=</span> RowParallelLinear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_heads <span class="token operator">*</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 计算softmax函数的缩放因子</span>
        self<span class="token punctuation">.</span>softmax_scale <span class="token operator">=</span> self<span class="token punctuation">.</span>qk_head_dim <span class="token operator">**</span> <span class="token operator">-</span><span class="token number">0.5</span>
        <span class="token comment"># 如果最大序列长度大于原始序列长度，对缩放因子进行调整</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>max_seq_len <span class="token operator">&gt;</span> args<span class="token punctuation">.</span>original_seq_len<span class="token punctuation">:</span>
            mscale <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token operator">*</span> args<span class="token punctuation">.</span>mscale <span class="token operator">*</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>args<span class="token punctuation">.</span>rope_factor<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1.0</span>
            self<span class="token punctuation">.</span>softmax_scale <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax_scale <span class="token operator">*</span> mscale <span class="token operator">*</span> mscale

        <span class="token comment"># 如果注意力实现方式为朴素方式</span>
        <span class="token keyword">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
            <span class="token comment"># 注册键缓存</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"k_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            <span class="token comment"># 注册值缓存</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"v_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token comment"># 否则</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 注册键值缓存</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"kv_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
            <span class="token comment"># 注册位置编码缓存</span>
            self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"pe_cache"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>args<span class="token punctuation">.</span>max_batch_size<span class="token punctuation">,</span> args<span class="token punctuation">.</span>max_seq_len<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> persistent<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> start_pos<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        多头注意力层（MLA）的前向传播过程。

        参数:
            x (torch.Tensor): 输入张量，形状为 (batch_size, seq_len, dim)。
            start_pos (int): 序列中用于缓存的起始位置。
            freqs_cis (torch.Tensor): 预计算的复指数值，用于旋转位置编码。
            mask (Optional[torch.Tensor]): 掩码张量，用于在注意力计算中排除某些位置。

        返回:
            torch.Tensor: 输出张量，形状与输入相同。
        """</span>
        <span class="token comment"># 获取输入张量的批次大小、序列长度</span>
        bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 计算序列的结束位置</span>
        end_pos <span class="token operator">=</span> start_pos <span class="token operator">+</span> seqlen
        <span class="token comment"># 如果查询的低秩投影的秩为0，直接通过线性层得到查询</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>q_lora_rank <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            q <span class="token operator">=</span> self<span class="token punctuation">.</span>wq<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 否则，通过低秩分解的方式得到查询</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            q <span class="token operator">=</span> self<span class="token punctuation">.</span>wq_b<span class="token punctuation">(</span>self<span class="token punctuation">.</span>q_norm<span class="token punctuation">(</span>self<span class="token punctuation">.</span>wq_a<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 调整查询的形状，将其划分为多个头</span>
        q <span class="token operator">=</span> q<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_head_dim<span class="token punctuation">)</span>
        <span class="token comment"># 将查询划分为非位置相关部分和位置相关部分</span>
        q_nope<span class="token punctuation">,</span> q_pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>q<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对位置相关部分应用旋转位置编码</span>
        q_pe <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>q_pe<span class="token punctuation">,</span> freqs_cis<span class="token punctuation">)</span>
        <span class="token comment"># 通过线性层得到键和值的低秩表示</span>
        kv <span class="token operator">=</span> self<span class="token punctuation">.</span>wkv_a<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 将键和值的低秩表示划分为低秩部分和位置编码部分</span>
        kv<span class="token punctuation">,</span> k_pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>kv<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_rope_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对位置编码部分应用旋转位置编码</span>
        k_pe <span class="token operator">=</span> apply_rotary_emb<span class="token punctuation">(</span>k_pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> freqs_cis<span class="token punctuation">)</span>

        <span class="token comment"># 如果注意力实现方式为朴素方式</span>
        <span class="token keyword">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
            <span class="token comment"># 将非位置相关部分和位置相关部分拼接得到完整的查询</span>
            q <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>q_nope<span class="token punctuation">,</span> q_pe<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 对键和值的低秩表示进行归一化和线性变换</span>
            kv <span class="token operator">=</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">(</span>self<span class="token punctuation">.</span>kv_norm<span class="token punctuation">(</span>kv<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token comment"># 调整键和值的形状，将其划分为多个头</span>
            kv <span class="token operator">=</span> kv<span class="token punctuation">.</span>view<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>qk_nope_head_dim <span class="token operator">+</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">)</span>
            <span class="token comment"># 将键和值划分为非位置相关部分和值部分</span>
            k_nope<span class="token punctuation">,</span> v <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>kv<span class="token punctuation">,</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 将非位置相关部分和位置编码部分拼接得到完整的键</span>
            k <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>k_nope<span class="token punctuation">,</span> k_pe<span class="token punctuation">.</span>expand<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 将键存入缓存</span>
            self<span class="token punctuation">.</span>k_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> k
            <span class="token comment"># 将值存入缓存</span>
            self<span class="token punctuation">.</span>v_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> v
            <span class="token comment"># 计算注意力分数</span>
            scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshd,bthd-&gt;bsht"</span><span class="token punctuation">,</span> q<span class="token punctuation">,</span> self<span class="token punctuation">.</span>k_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>softmax_scale
        <span class="token comment"># 否则</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 获取键和值的线性变换层的权重</span>
            wkv_b <span class="token operator">=</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>weight <span class="token keyword">if</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>scale <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> weight_dequant<span class="token punctuation">(</span>self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>wkv_b<span class="token punctuation">.</span>scale<span class="token punctuation">,</span> block_size<span class="token punctuation">)</span> 
            <span class="token comment"># 调整权重的形状</span>
            wkv_b <span class="token operator">=</span> wkv_b<span class="token punctuation">.</span>view<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_local_heads<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_lora_rank<span class="token punctuation">)</span>
            <span class="token comment"># 计算非位置相关部分的注意力分数</span>
            q_nope <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshd,hdc-&gt;bshc"</span><span class="token punctuation">,</span> q_nope<span class="token punctuation">,</span> wkv_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>qk_nope_head_dim<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment"># 将键和值的低秩表示归一化后存入缓存</span>
            self<span class="token punctuation">.</span>kv_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>kv_norm<span class="token punctuation">(</span>kv<span class="token punctuation">)</span>
            <span class="token comment"># 将位置编码部分存入缓存</span>
            self<span class="token punctuation">.</span>pe_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> start_pos<span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span> <span class="token operator">=</span> k_pe<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
            <span class="token comment"># 计算注意力分数</span>
            scores <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshc,btc-&gt;bsht"</span><span class="token punctuation">,</span> q_nope<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span>
                      torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshr,btr-&gt;bsht"</span><span class="token punctuation">,</span> q_pe<span class="token punctuation">,</span> self<span class="token punctuation">.</span>pe_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>softmax_scale

        <span class="token comment"># 如果存在掩码，将掩码加到注意力分数上</span>
        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">+=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 对注意力分数应用softmax函数</span>
        scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># 如果注意力实现方式为朴素方式</span>
        <span class="token keyword">if</span> attn_impl <span class="token operator">==</span> <span class="token string">"naive"</span><span class="token punctuation">:</span>
            <span class="token comment"># 通过注意力分数和值缓存计算输出</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bsht,bthd-&gt;bshd"</span><span class="token punctuation">,</span> scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>v_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 否则</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># 通过注意力分数和键值缓存计算中间结果</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bsht,btc-&gt;bshc"</span><span class="token punctuation">,</span> scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>kv_cache<span class="token punctuation">[</span><span class="token punctuation">:</span>bsz<span class="token punctuation">,</span> <span class="token punctuation">:</span>end_pos<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment"># 通过中间结果和权重计算输出</span>
            x <span class="token operator">=</span> torch<span class="token punctuation">.</span>einsum<span class="token punctuation">(</span><span class="token string">"bshc,hdc-&gt;bshd"</span><span class="token punctuation">,</span> x<span class="token punctuation">,</span> wkv_b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>v_head_dim<span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 对输出进行线性变换</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>wo<span class="token punctuation">(</span>x<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre>
    <h4>
     <a id="_418">
     </a>
     代码解释总结
    </h4>
    <p>
     这段代码定义了一个多头注意力层（MLA）类。在初始化时，根据传入的参数设置各种维度、低秩投影的秩等，并初始化相应的线性层和归一化层，同时根据注意力实现方式注册不同的缓存。在前向传播过程中，对输入进行处理得到查询、键和值，应用旋转位置编码，根据不同的注意力实现方式计算注意力分数，最后通过注意力分数和缓存得到输出并进行线性变换。
    </p>
    <h3>
     <a id="FeedForward_Network_422">
     </a>
     Feed-Forward Network
    </h3>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/92877a968d0845c58bd1ed0dee521fe7.png">
      <br/>
      这个MoE分成两个部分，左边是一些可以分享的专家，就是每次都需要去计算的，右边的是根据分数来选择的。如何选择，是通过一个门控机制来选择。这个门控是如何设计的，代码里有实现，但论文和代码都没有对它物理意义的解释。门控机制，简单来说，就是设计了一个网络，来选出K个候选。
     </img>
    </p>
    <p>
     核心逻辑：
    </p>
    <ol>
     <li>
      通过门控确定本轮要用到的本地专家：
      <code>
       weights, indices = self.gate(x)
      </code>
     </li>
     <li>
      用选择的每个本地专家进行计算：
      <code>
       y[idx] += expert(x[idx]) * weights[idx, top, None]
      </code>
     </li>
     <li>
      用共享专家进行计算：
      <code>
       z = self.shared_experts(x)
      </code>
     </li>
     <li>
      将本地专家的输出和共享专家的输出相加，并恢复到原始形状：
      <code>
       return (y + z).view(shape)
      </code>
     </li>
    </ol>
    <p>
     全部代码：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 定义混合专家（Mixture-of-Experts, MoE）模块类，继承自PyTorch的nn.Module类</span>
<span class="token keyword">class</span> <span class="token class-name">MoE</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    混合专家（Mixture-of-Experts, MoE）模块。

    属性:
        dim (int): 输入特征的维度。
        n_routed_experts (int): 模型中专家的总数。
        n_local_experts (int): 在分布式系统中本地处理的专家数量。
        n_activated_experts (int): 每个输入激活的专家数量。
        gate (nn.Module): 门控机制，用于将输入路由到不同的专家。
        experts (nn.ModuleList): 专家模块列表，包含多个专家网络。
        shared_experts (nn.Module): 共享专家模块，应用于所有输入。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化MoE模块。

        参数:
            args: 模型参数对象，包含MoE模块的相关参数。
        """</span>
        <span class="token comment"># 调用父类的初始化方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存输入特征的维度</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim
        <span class="token comment"># 确保专家总数能被分布式系统中的进程数整除</span>
        <span class="token keyword">assert</span> args<span class="token punctuation">.</span>n_routed_experts <span class="token operator">%</span> world_size <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"专家数量必须能被进程数整除 (进程数=</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>world_size<span class="token punctuation">}</span></span><span class="token string">)"</span></span>
        <span class="token comment"># 保存模型中专家的总数</span>
        self<span class="token punctuation">.</span>n_routed_experts <span class="token operator">=</span> args<span class="token punctuation">.</span>n_routed_experts
        <span class="token comment"># 计算本地处理的专家数量</span>
        self<span class="token punctuation">.</span>n_local_experts <span class="token operator">=</span> args<span class="token punctuation">.</span>n_routed_experts <span class="token operator">//</span> world_size
        <span class="token comment"># 保存每个输入激活的专家数量</span>
        self<span class="token punctuation">.</span>n_activated_experts <span class="token operator">=</span> args<span class="token punctuation">.</span>n_activated_experts
        <span class="token comment"># 计算本地专家在所有专家中的起始索引</span>
        self<span class="token punctuation">.</span>experts_start_idx <span class="token operator">=</span> rank <span class="token operator">*</span> self<span class="token punctuation">.</span>n_local_experts
        <span class="token comment"># 计算本地专家在所有专家中的结束索引</span>
        self<span class="token punctuation">.</span>experts_end_idx <span class="token operator">=</span> self<span class="token punctuation">.</span>experts_start_idx <span class="token operator">+</span> self<span class="token punctuation">.</span>n_local_experts
        <span class="token comment"># 初始化门控机制</span>
        self<span class="token punctuation">.</span>gate <span class="token operator">=</span> Gate<span class="token punctuation">(</span>args<span class="token punctuation">)</span>
        <span class="token comment"># 初始化专家模块列表，本地负责的专家使用Expert模块，其他位置置为None</span>
        self<span class="token punctuation">.</span>experts <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>Expert<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>moe_inter_dim<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>experts_start_idx <span class="token operator">&lt;=</span> i <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>experts_end_idx <span class="token keyword">else</span> <span class="token boolean">None</span>
                                      <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token comment"># 初始化共享专家模块</span>
        self<span class="token punctuation">.</span>shared_experts <span class="token operator">=</span> MLP<span class="token punctuation">(</span>args<span class="token punctuation">.</span>dim<span class="token punctuation">,</span> args<span class="token punctuation">.</span>n_shared_experts <span class="token operator">*</span> args<span class="token punctuation">.</span>moe_inter_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        MoE模块的前向传播过程。

        参数:
            x (torch.Tensor): 输入张量。

        返回:
            torch.Tensor: 经过专家路由和计算后的输出张量。
        """</span>
        <span class="token comment"># 保存输入张量的原始形状</span>
        shape <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 将输入张量展平为二维张量，方便后续处理</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>dim<span class="token punctuation">)</span>
        <span class="token comment"># 通过门控机制得到每个输入分配到各个专家的权重和对应的专家索引</span>
        weights<span class="token punctuation">,</span> indices <span class="token operator">=</span> self<span class="token punctuation">.</span>gate<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 初始化输出张量，形状与输入相同，初始值全为0</span>
        y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 统计每个专家被分配到的输入数量</span>
        counts <span class="token operator">=</span> torch<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>indices<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> minlength<span class="token operator">=</span>self<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 遍历本地负责的专家</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>experts_start_idx<span class="token punctuation">,</span> self<span class="token punctuation">.</span>experts_end_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 如果该专家没有被分配到输入，则跳过</span>
            <span class="token keyword">if</span> counts<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">continue</span>
            <span class="token comment"># 获取当前专家模块</span>
            expert <span class="token operator">=</span> self<span class="token punctuation">.</span>experts<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            <span class="token comment"># 找出分配到当前专家的输入的索引</span>
            idx<span class="token punctuation">,</span> top <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>indices <span class="token operator">==</span> i<span class="token punctuation">)</span>
            <span class="token comment"># 将这些输入通过当前专家模块进行计算，并乘以对应的权重，累加到输出张量中</span>
            y<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">+=</span> expert<span class="token punctuation">(</span>x<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> weights<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> top<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>
        <span class="token comment"># 将输入通过共享专家模块进行计算</span>
        z <span class="token operator">=</span> self<span class="token punctuation">.</span>shared_experts<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># 如果使用分布式训练，对本地专家的输出进行全局归约操作</span>
        <span class="token keyword">if</span> world_size <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            dist<span class="token punctuation">.</span>all_reduce<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        <span class="token comment"># 将本地专家的输出和共享专家的输出相加，并恢复到原始形状</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>y <span class="token operator">+</span> z<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_519">
     </a>
     代码解释总结
    </h4>
    <p>
     这段代码定义了一个混合专家（MoE）模块。在初始化时，根据传入的参数设置专家的数量、门控机制、专家模块列表和共享专家模块。在前向传播过程中，首先通过门控机制将输入路由到不同的专家，然后对本地负责的专家进行计算并累加结果，同时将输入通过共享专家模块进行计算，最后将两部分结果相加并恢复原始形状。如果使用分布式训练，还会对本地专家的输出进行全局归约操作。
    </p>
    <h3>
     <a id="Gate_521">
     </a>
     Gate
    </h3>
    <p>
     不考虑分组路由来看看它的核心逻辑，实际上就是线下变换，然后激活，选择K个极值（如果用了分组，就是选择K的方式发生了一些变化）：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 通过线性变换计算每个输入对应各个专家的分数</span>
scores <span class="token operator">=</span> linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># 根据评分函数类型对分数进行处理</span>
<span class="token keyword">if</span> self<span class="token punctuation">.</span>score_func <span class="token operator">==</span> <span class="token string">"softmax"</span><span class="token punctuation">:</span>
    scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 选择分数最高的若干专家</span>
indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>topk<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token comment"># 根据选择的专家索引，从原始分数中获取对应的权重</span>
weights <span class="token operator">=</span> scores<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> indices<span class="token punctuation">)</span>
<span class="token comment"># 如果评分函数是sigmoid，对权重进行归一化</span>
<span class="token keyword">if</span> self<span class="token punctuation">.</span>score_func <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>
    weights <span class="token operator">/=</span> weights<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token comment"># 对权重进行缩放</span>
weights <span class="token operator">*=</span> self<span class="token punctuation">.</span>route_scale
<span class="token keyword">return</span> weights<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> indices
</code></pre>
    <pre><code class="prism language-python"><span class="token comment"># 定义门控机制类，用于在混合专家（MoE）模型中对输入进行路由</span>
<span class="token keyword">class</span> <span class="token class-name">Gate</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    混合专家（MoE）模型中用于输入路由的门控机制。

    属性:
        dim (int): 输入特征的维度。
        topk (int): 每个输入激活的顶级专家数量。
        n_groups (int): 用于路由的分组数量。
        topk_groups (int): 输入将被路由到的分组数量。
        score_func (str): 评分函数，取值为 'softmax' 或 'sigmoid'。
        route_scale (float): 路由权重的缩放因子。
        weight (torch.nn.Parameter): 门控机制的可学习权重。
        bias (Optional[torch.nn.Parameter]): 门控机制的可选偏置项。
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        初始化门控机制模块。

        参数:
            args: 模型参数对象，包含门控机制的相关参数。
        """</span>
        <span class="token comment"># 调用父类的初始化方法</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存输入特征的维度</span>
        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> args<span class="token punctuation">.</span>dim
        <span class="token comment"># 保存每个输入激活的顶级专家数量</span>
        self<span class="token punctuation">.</span>topk <span class="token operator">=</span> args<span class="token punctuation">.</span>n_activated_experts
        <span class="token comment"># 保存用于路由的分组数量</span>
        self<span class="token punctuation">.</span>n_groups <span class="token operator">=</span> args<span class="token punctuation">.</span>n_expert_groups
        <span class="token comment"># 保存输入将被路由到的分组数量</span>
        self<span class="token punctuation">.</span>topk_groups <span class="token operator">=</span> args<span class="token punctuation">.</span>n_limited_groups
        <span class="token comment"># 保存评分函数类型</span>
        self<span class="token punctuation">.</span>score_func <span class="token operator">=</span> args<span class="token punctuation">.</span>score_func
        <span class="token comment"># 保存路由权重的缩放因子</span>
        self<span class="token punctuation">.</span>route_scale <span class="token operator">=</span> args<span class="token punctuation">.</span>route_scale
        <span class="token comment"># 初始化可学习权重</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>args<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">,</span> args<span class="token punctuation">.</span>dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 根据输入特征维度决定是否初始化偏置项</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>args<span class="token punctuation">.</span>n_routed_experts<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>dim <span class="token operator">==</span> <span class="token number">7168</span> <span class="token keyword">else</span> <span class="token boolean">None</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        门控机制的前向传播过程。

        参数:
            x (torch.Tensor): 输入张量。

        返回:
            Tuple[torch.Tensor, torch.Tensor]: 路由权重和选择的专家索引。
        """</span>
        <span class="token comment"># 通过线性变换计算每个输入对应各个专家的分数</span>
        scores <span class="token operator">=</span> linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
        <span class="token comment"># 根据评分函数类型对分数进行处理</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>score_func <span class="token operator">==</span> <span class="token string">"softmax"</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 保存原始分数，后续计算权重时使用</span>
        original_scores <span class="token operator">=</span> scores
        <span class="token comment"># 如果存在偏置项，将其加到分数上</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            scores <span class="token operator">=</span> scores <span class="token operator">+</span> self<span class="token punctuation">.</span>bias
        <span class="token comment"># 如果分组数量大于1，进行分组路由操作</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>n_groups <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
            <span class="token comment"># 调整分数的形状，以便按组处理</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_groups<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 根据是否有偏置项，计算每个组的分数表示</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>bias <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                group_scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>amax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                group_scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>topk<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 选择分数最高的若干组</span>
            indices <span class="token operator">=</span> group_scores<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>self<span class="token punctuation">.</span>topk_groups<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token comment"># 创建掩码，用于屏蔽未选择的组</span>
            mask <span class="token operator">=</span> scores<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>n_groups<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> indices<span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
            <span class="token comment"># 将屏蔽组的分数设为负无穷</span>
            scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment"># 选择分数最高的若干专家</span>
        indices <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> self<span class="token punctuation">.</span>topk<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token comment"># 根据选择的专家索引，从原始分数中获取对应的权重</span>
        weights <span class="token operator">=</span> original_scores<span class="token punctuation">.</span>gather<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> indices<span class="token punctuation">)</span>
        <span class="token comment"># 如果评分函数是sigmoid，对权重进行归一化</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>score_func <span class="token operator">==</span> <span class="token string">"sigmoid"</span><span class="token punctuation">:</span>
            weights <span class="token operator">/=</span> weights<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token comment"># 对权重进行缩放</span>
        weights <span class="token operator">*=</span> self<span class="token punctuation">.</span>route_scale
        <span class="token keyword">return</span> weights<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> indices
</code></pre>
    <h4>
     <a id="_635">
     </a>
     代码解释总结
    </h4>
    <p>
     这段代码定义了一个门控机制（Gate）类，用于在混合专家（MoE）模型中对输入进行路由。在初始化时，根据传入的参数设置各种属性，如输入维度、激活专家数量、分组数量等，并初始化可学习的权重和偏置项。在前向传播过程中，首先计算每个输入对应各个专家的分数，然后根据评分函数类型进行处理，接着根据分组情况进行分组路由操作，选择激活的专家并计算对应的权重，最后返回路由权重和选择的专家索引。
    </p>
    <h2>
     <a id="B_638">
     </a>
     B站大牛详解视频链接
    </h2>
    <p>
     B站大牛有详细视频讲解：
     <br/>
     https://www.bilibili.com/video/BV1RtNLeqEeu/
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34343135333633302f:61727469636c652f64657461696c732f313436303633363234" class_="artid" style="display:none">
 </p>
</div>


