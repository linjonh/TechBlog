---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f323330315f37393435363239342f:61727469636c652f64657461696c732f313436313035323532"
layout: post
title: "深度学习笔记神经网络"
date: 2025-03-07 23:32:48 +0800
description: "模拟生物神经元，人工神经元有1个或者多个输入（模拟多个树突或者多个神经元向该神经元传递神经冲动）；线性层(Linear Layer)又称全连接层(Full-connected Layer)，其每个神经元与上一层所有神经元相连，实现对前一层的线性组合/线性变换。每个神经元都和前一层中的所有神经元相连，每个神经元的计算方式是对上一层的加权求和的过程。ReLU，全称为：Rectified Linear Unit，是一种人工神经网络中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即f(x)=max(0,x)"
keywords: "深度学习笔记——神经网络"
categories: ['深度学习']
tags: ['笔记', '神经网络', '深度学习', '人工智能', 'Python']
artid: "146105252"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146105252
    alt: "深度学习笔记神经网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146105252
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146105252
cover: https://bing.ee123.net/img/rand?artid=146105252
image: https://bing.ee123.net/img/rand?artid=146105252
img: https://bing.ee123.net/img/rand?artid=146105252
---

# 深度学习笔记——神经网络

本文为在拓尔思智能举办的训练营中学习内容的总结，部分内容摘自百度百科

个人在这里推荐一个好用的软件，Trae，主要是免费。

**人工神经元**
是人工神经网络的基本单元。模拟生物神经元，人工神经元有1个或者多个输入（模拟多个树突或者多个神经元向该神经元传递神经冲动）；对输入进行加权求和（模拟细胞体将神经信号进行积累和树突强度不同）；对输入之和使用激活函数计算活性值（模拟细胞体产生兴奋或者抑制）；输出活性值并传递到下一个人工神经元（模拟生物神经元通过轴突将神经冲动输入到下一个神经元）。

**nn.Linear线性层**
  
线性层(Linear Layer)又称全连接层(Full-connected Layer)，其每个神经元与上一层所有神经元相连，实现对前一层的线性组合/线性变换。每个神经元都和前一层中的所有神经元相连，每个神经元的计算方式是对上一层的加权求和的过程。因此，线性层可以采用矩阵乘法来实现。

```python
# nn.Linear(in_features, out_features, bias=True)
layer = nn.Linear(20, 30)
x = torch.randn(128, 20)
y = layer(x)
y.shape
```

输出：

torch.Size([128, 30])

这个代码实现从20个特征点向30个的自由转移，也就是线性层的作用。

**relu函数：**

ReLU，全称为：Rectified Linear Unit，是一种人工神经网络中常用的激活函数，通常意义下，其指代数学中的斜坡函数，即f(x)=max(0,x)

他的应用其实就是把小于0的数值归零

**激活函数**
：是对特征进行非线性的变化，赋予多层神经网络具有深度的意义。

参考了该文章一些内容：

[深入理解ReLU函数（ReLU函数的可解释性）-CSDN博客](https://kanny.blog.csdn.net/article/details/112253138?fromshare=blogdetail&sharetype=blogdetail&sharerId=112253138&sharerefer=PC&sharesource=2301_79456294&sharefrom=from_link "深入理解ReLU函数（ReLU函数的可解释性）-CSDN博客")