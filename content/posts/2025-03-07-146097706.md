---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f5a4e426173652f:61727469636c652f64657461696c732f313436303937373036"
layout: post
title: "Paper-Reading-AI-数据库融合经典论文回顾"
date: 2025-03-07 16:04:52 +0800
description: "人工智能（AI）和数据库（DB）在过去的50年里得到了广泛的研究，随着数据库近年来的不断发展，数据库开始与人工智能结合，数据库和人工智能（AI）可以相互促进。一方面，AI 可以使数据库更加智能化（AI4DB）。例如，传统的数据库优化技术无法满足大规模数据库实例、各种应用程序和多样化用户的高性能要求，尤其是在云上。幸运的是，基于机器学习的技术可以缓解这个问题。另一方面，数据库技术可以优化AI模型（DB4AI）。例如，人工智能在实际应用中难以部署，因为它要求开发人员编写复杂的代码并训练复杂的模型。数据库技术可以"
keywords: "Paper Reading | AI & 数据库融合经典论文回顾"
categories: ['未分类']
tags: ['论文阅读', '数据库']
artid: "146097706"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146097706
    alt: "Paper-Reading-AI-数据库融合经典论文回顾"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146097706
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146097706
cover: https://bing.ee123.net/img/rand?artid=146097706
image: https://bing.ee123.net/img/rand?artid=146097706
img: https://bing.ee123.net/img/rand?artid=146097706
---

# Paper Reading | AI & 数据库融合经典论文回顾

人工智能（AI）和数据库（DB）在过去的50年里得到了广泛的研究，随着数据库近年来的不断发展，数据库开始与人工智能结合，数据库和人工智能（AI）可以相互促进。一方面，AI 可以使数据库更加智能化（AI4DB）。例如，传统的数据库优化技术无法满足大规模数据库实例、各种应用程序和多样化用户的高性能要求，尤其是在云上。幸运的是，基于机器学习的技术可以缓解这个问题。另一方面，数据库技术可以优化AI模型（DB4AI）。例如，人工智能在实际应用中难以部署，因为它要求开发人员编写复杂的代码并训练复杂的模型。数据库技术可以用来降低使用 AI 模型的复杂性，加速 AI 算法并在数据库内部提供 AI 能力。在这篇技术博客中，我们将对清华大学李国良教授团队的综述性论文《Database Meets AI:A Survey》进行详细研读。这篇综述论文从AI4DB和DB4AI两方面展开，全面回顾了工业和学术界关于AI与DB 结合的研究工作。

数据库和人工智能（AI）可以相互促进。一方面，AI 可以使数据库更加智能化（AI4DB）。例如，传统的基于经验的数据库优化技术，如成本估计、连接顺序选择、参数调整、索引和视图选择等，已无法满足大规模数据库实例、多样化应用和用户的高性能需求，尤其是在云环境中。基于学习的技术可以缓解这一问题。另一方面，数据库技术可以优化 AI 模型（DB4AI）。AI 在实际应用中难以部署，因为它要求开发者编写复杂代码并训练复杂的模型。数据库技术可以用来简化 AI 模型的使用，加速 AI 算法，并在数据库内部提供AI功能。论文回顾了 AI4DB 和 DB4AI 方面的现有研究。对于 AI4DB，论文回顾了基于学习的技术，包括配置调整、优化器、索引/视图顾问和安全性。对于 DB4AI，论文回顾了面向 AI 的声明式语言、面向 AI 的数据治理、训练加速和推理加速。最后，论文提出了研究挑战和未来的发展方向。

引言

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5af92200fe4c4be5b32c18b8d60e8df3.png#pic_center)

AI for DB
  
传统数据库设计依赖于经验方法和规范，需要数据库管理员（DBAs）手动调整大量参数（称为旋钮或Knobs），以适应不同的工作负载和场景。这种方法在面对云数据库上数以百万计的数据库实例时显得不可扩展。使用机器学习技术可以自动化数据库配置，减少对 DBAs 的依赖，提高数据库性能，并适应不断变化的工作负载。目前基于学习的数据库配置研究可以做如下分类：

• Knob Tuning（参数调整）：自动调整数据库参数以适应不同场景，使用机器学习算法探索参数组合空间，推荐高质量的参数值。

• Index/View Advisor（索引/视图顾问）：自动推荐和维护索引和视图，以提高查询性能。传统数据库依赖 DBA 来构建和维护索引/视图，而基于学习的方法可以处理大量列/表组合，减少人工干预。

• SQL Rewriter（SQL重写）：自动重写 SQL 查询以提高性能。例如，将嵌套查询转换为连接查询，使用深度强化学习选择适当的重写规则并按正确顺序应用。

此外，许多研究工作探索使用机器学习技术来解决数据库查询优化中的一些关键问题，可以分为以下几类：

• 基数/成本估计（Cardinality/Cost Estimation）：数据库优化器依赖于成本和基数估计来选择最优的查询执行计划。传统方法在处理多列或多表连接时，由于无法有效捕捉数据之间的相关性，导致估计质量下降。基于深度学习的方法被提出来改善估计质量，通过深度神经网络捕捉数据间的复杂相关性。

• 连接顺序选择（Join Order Selection）：对于 SQL 查询，可能存在数百万甚至数十亿种潜在的执行计划，有效地找到一个好的计划非常关键。传统优化器在面对大量表的查询时，由于巨大的计划空间，难以找到好的计划。基于深度强化学习的方法被提出以自动探索并选择优秀的查询计划。

• 端到端优化器（End-to-End Optimizer）：一个完整的优化器不仅需要依赖成本和基数估计，还需要考虑索引和视图等因素。基于学习的优化器使用深度神经网络来全面优化 SQL 查询，包括结构和执行计划。

• 学习型索引（Learned Indexes）：提出了基于学习模型的索引结构，这些结构旨在减少索引大小并提高索引性能。与传统的B树或哈希索引不同，学习型索引使用机器学习算法来预测数据分布和查询模式。

• 学习型数据结构设计（Learned Data Structure Design）：不同的数据结构可能适用于不同的环境，例如不同的硬件或不同的读写应用场景。提出了数据结构炼金术（Data Structure Alchemy），旨在创建一个数据推理引擎，用于推荐和设计适应不同场景的数据结构。

• 学习型事务管理（Learning-Based Transaction Management）：传统事务管理技术侧重于事务协议，例如OCC（乐观并发控制）、PCC（悲观并发控制）、MVCC（多版本并发控制）、2PC（两阶段提交）。学习型技术尝试通过分析现有数据模式来预测和调度事务，通过平衡冲突率和并发性来有效管理事务。

• 自设计数据库技术（Self-Design Techniques）：传统数据库设计依赖于数据库架构师的经验和知识，但这些人为设计可能无法覆盖所有可能的设计空间。学习型自设计技术通过自动化的方式探索更多的设计可能性，以适应不断变化的数据和查询模式。

DB for AI
  
尽管人工智能（AI）能够解决许多现实世界的问题，但目前并没有广泛部署的AI系统能够像数据库管理系统（DBMS）那样在不同领域中被普遍使用。现有的AI系统复制性差，普通用户难以使用。为了解决这个问题，可以使用数据库技术来降低使用AI的障碍。

1.AI 模型的声明式查询
  
SQL 在数据库系统中相对容易使用，并被广泛接受。研究者们提出将 SQL 扩展以支持 AI 模型。这意味着可以在 SQL 中直接使用 AI 算法，而无需编写复杂的代码。幸运的是，SQL 可以扩展到支持 AI 模型，我们还可以设计用户友好的工具来支持 SQL 语句中的 AI 模型。

2.数据治理
  
数据质量对于机器学习至关重要。高质量的数据可以显著提高模型的训练效果和预测准确性。数据治理包括一系列流程和实践，用于提高数据质量。这包括数据发现、数据清洗、数据整合、数据标注和数据血统。其中，基于学习的数据发现提高了查找相关数据的能力，有效地在大量数据源中找到相关数据。肮脏或不一致的数据会严重影响训练表现，数据清理和集成技术可以检测和修复脏数据，并集成来自多个来源的数据，生成高质量的数据。数据标注通过专家知识或众包方式为ML算法标记大量的训练数据。数据血统则通过追踪数据流动和转换，确保了模型的可靠性和可解释性。通过连接和图形映射等数据库技术，可以向和向前跟踪数据关系。

3.模型训练
  
模型训练的目的是训练一个好的模型，并将其用于在线推理。模型训练是机器学习中的核心环节，它涉及到从大量数据中学习并构建一个能够进行准确预测的模型。这个过程包括多个关键步骤，如特征选择、模型选择、模型管理和硬件加速。特征选择的目的是筛选出对模型性能有显著影响的特征，以提高模型的准确性和效率。模型选择则是从众多可能的模型中找到最合适的模型架构和参数配置。模型管理关注于如何跟踪、存储和搜索大量的机器学习模型，以便于迭代开发和知识共享。硬件加速则是通过使用 GPU、FPGA 等硬件资源来提高模型训练的速度。数据库技术在模型训练中扮演着重要角色。例如，数据库可以提供高效的数据存储和索引机制，加速数据的访问和处理。此外，数据库系统内建的并行处理和分布式计算能力可以用来加速模型的训练过程。数据库还可以用来管理训练过程中的各种元数据，包括特征、模型参数和训练结果等。

4.模型推理
  
模型推理的目的是利用训练好的模型有效地推断结果，而数据库内的优化技术包括操作符支持、操作符选择和执行加速。与传统数据库操作符（如过滤、连接、排序）不同，AI 模型涉及更复杂的操作符类型，包括标量、向量、矩阵和张量操作。数据库系统可以原生支持这些操作符，并优化它们的执行。此外，同一个机器学习模型可以转换为不同的物理操作符。数据库优化器可以高效地估计执行成本并原生地优化操作符选择，而不需要依赖硬件如 GPU 进行操作符选择。与模型训练不同，模型推理需要选择机器学习模型并执行前向传播来对不同问题进行预测。推理加速旨在提高执行效率。内存数据库压缩内存内的样本/模型数据，并进行内存优化。分布式数据库通过向不同的节点发布任务来提高执行效率。

AI for DB
  
基于学习的数据库配置
  
基于学习的数据库配置旨在利用机器学习技术来自动化数据库配置，例如，旋钮调优、索引推荐、视图推荐以及 SQL 重写。下面将主要对旋钮调节、索引推荐和 SQL 重写等相关重要工作进行阐述。

1.旋钮调节
  
数据库旋钮的数量庞大且关系复杂。DBA 通常擅长于特定的数据库，并且需要相对较长的时间。显然，DBA 无法扩展到云数据库上的数百万个数据库实例。但是通过基于学习的技术自动调整旋钮，可以探索更多旋钮组合空间并推荐高质量旋钮值，从而获得比 DBA 更好的结果。如表1所示，论文将现有的旋钮调优技术分为四类，包括基于搜索的调优、传统的基于 ML 的调优、基于深度学习的调优和基于强化学习的调优。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/23ae1e0a3ee64b129b18ecf9b3298e8b.png#pic_center)

论文首先介绍了基于搜索的调优：为了减少人力，《BestConfig: Tapping the performance potential of systems via automatic configuration tuning》提出了一种递归绑定和搜索优化方法BestConfig，给定一个查询工作负载，从历史数据中找到类似的工作负载，并返回相应的旋钮值。具体来说，给定 n 个旋钮，BestConfig 将每个旋钮的值范围划分为 k 个区间，这些旋钮区间形成一个具有 k 的 n 次方个子空间的离散空间（有界空间）。

然后，在每次迭代中，BestConfig从有界空间中随机选择 k 个样本，并从所选择的k个样本中选择性能最好的样本，记为 C1。在下一次迭代中，它只从接近 C1 的有界空间中获取样本。这样，最佳配置迭代地减小有界空间，最终得到一个很好的旋钮组合。然而，这种基于搜索的方法有几个局限性。首先，它是启发式的，可能在有限的时间内找到最优的旋钮值。其次，它不能实现高性能，因为它需要搜索整个空间。

《Automatic database management system tuning through large-scale machine learning》提出了一个基于机器学习的数据库调整系统 OtterTune。OtterTune 使用高斯过程（GP）为不同的工作负载推荐合适的旋钮。首先，它选择一些查询模板，每个查询模板包含一个查询工作负载及其相应的合适旋钮值。其次，它提取数据库的内部状态（例如，读取/写入的页面数，查询缓存的利用率）来反映工作负载特征。从内部状态特征中，OtterTune 使用因子分析过滤不相关的特征，然后使用简单的无监督学习方法（例如，K均值）选择与调整问题最相关的K个特征。OtterTune 使用这些 K 个特征来描绘工作负载的特征。第三，它使用这些选定的特征将当前工作负载映射到最相似的模板。OtterTune 直接推荐这个模板的旋钮配置作为最优配置。并且它还将查询工作负载输入到 GP 模型中，以学习新配置并更新模型。

正式地说，它的模型训练如下。给定训练数据（W，W0，C0，R），其中 W 是工作负载，W0 是 W 的相似工作负载模板，C0 是 W0 推荐的配置，C00 是 GP 模型推荐的配置，R 是 C0 和 C00 之间的性能差异。它通过最小化 C0 和 C00 之间的差异来训练模型。该方法具有良好的泛化能力，能够适应不同的数据库环境，并有效利用历史任务中学习到的经验。此外，它可以有效利用从历史任务中学到的经验，并将这些经验应用于未来的推理和训练。但该方法也存在一些局限性，包括依赖于复杂的流水线架构、需要大量高质量样本进行训练，以及在处理高维和连续空间的参数调整时存在困难。

一些研究试图通过强化学习来解决旋钮调节问题，《An end-to-end automatic cloud database tuning system using deep reinforcement learning》提出了一个基于深度强化学习（DRL）的数据库调整系统 CDBTune，该系统通过与数据库环境的持续交互来优化参数调整。CDBTune 将数据库调整问题映射为强化学习框架中的五个模块：环境（云数据库实例）、状态（实例的内部指标）、代理（调优模型）、动作（参数调整）、奖励（调整后性能变化）。

CDBTune 采用神经网络（Actor）作为调优策略，输入状态指标并输出参数值。同时，使用另一个神经网络（Critic）来调整 Actor，输出奖励值。在调优过程中，Agent 根据数据库实例的状态输出调优动作，应用到数据库实例上，执行工作负载，根据性能变化获得奖励，并用以更新 Critic 和 Actor。CDBTune 能够处理在线工作负载，通过Actor模型推荐参数值，并通过实时监控和调优来优化数据库性能。尽管 CDBTune 在粗粒度调优方面表现出色，但它无法提供细粒度调优，例如针对特定查询的调优。此外，CDBTune 主要基于现有的模型，如 Q-learning 和 DDPG，这些模型在处理连续值输入/输出和考虑工作负载特征方面存在限制。

2.索引选择
  
在 DBMS 中，索引对于加速查询执行非常关键，选择适当的索引以实现高性能至关重要。我们首先定义索引选择问题。考虑一组表，设 C 表示这些表中的列集合，size(c ∈ C) 表示列 c 的索引大小。给定一个查询工作负载Q，设benefit(q ∈ Q; c ∈ C)表示为查询 q 在列 c 上建立索引的好处，即有索引和没有索引时执行查询q的成本差异。给定一个空间预算 B ，索引选择问题的目标是找到一个列的子集C’来建立索引，以在保持总索引大小在 B 以内的同时最大化好处，即最大化 ∑(q ∈ Q; c ∈ C’) benefit(q, c)，条件是 ∑(c ∈ C’) size© ≤ B。

这里存在几个挑战：首先是如何估计 benefit(q, c)，一个经典的方法是使用假设索引，它将索引信息添加到 DBMS 的数据字典中，而不是创建实际索引，这样 DBMS 的查询优化器可以识别索引的存在，并在不建立真实索引的情况下估计查询执行的成本。索引的估计 benefit 是有无假设索引时查询执行成本的减少。其次是解决上述优化问题。主要有两种解决方案来应对这些挑战——离线索引选择和在线索引选择（见表2）。离线索引选择方法要求 DBA 提供代表性的工作负载，并通过分析此工作负载来选择索引方案。在线索引选择方法监控 DBMS，并根据工作负载的变化选择合适的索引方案。离线方法和在线方法之间的主要区别在于，离线方法只选择和实现索引计划，而在线方法根据工作负载的变化动态创建或删除一些索引。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dfb6137d4b47477c98c345470eb56a94.png#pic_center)

离线索引选择依赖于 DBA 从查询日志中选择一些频繁查询作为代表性工作负载，并使用该工作负载来选择索引。《An efficient cost-driven index selection tool for microsoft SQL server》为 Microsoft SQL Server提出了一个索引选择工具 AutoAdmin，其主要思想是为每个查询选择表现良好的索引方案，然后扩展到 Q 中的多个查询。首先，对于每个查询 qi ∈ Q，AutoAdmin 从 SQL 查询中提取可索引的列。其次，AutoAdmin 使用一个简单的枚举算法来枚举一组可索引的列作为候选项，例如 I = {i1, i2, i3, i4}; {i3, i4, i5, i6}; …。然后 AutoAdmin 在 I 中选择对查询有最高 benefit 的索引方案。第三，对于每个查询，都有一个相应的最优索引策略，然后对于 Q 中的所有查询， AutoAdmin 根据 benefit 选择前 k 个方案。然后对于每个前k个方案， AutoAdmin 使用贪心算法逐步添加可索引的列，直到大小等于阈值 B。最后，选择 benefit 最高且在存储预算内的方案。

《DB2 advisor: An optimizer smart enough to recommend its own indexes》将索引选择问题建模为背包问题，提出了 DB2 Advisor。与 AutoAdmin 类似，DB2 Advisor 首先枚举索引方案及其 benefit。然后，它将索引选择问题建模为背包问题。更具体地说，它将每个候选索引方案视为一个项目，方案的大小视为项目重量，方案的 benefit 视为价值。然后 DB2 使用动态规划来解决这个问题。离线方法的缺点是它们不够灵活，无法处理工作负载的动态变化。更糟糕的是，选择代表性工作负载将增加 DBA 的负担。为了解决这些问题，提出了一些在线方法，如下所述。

在线索引选择可以分为三类：传统的在线索引选择方法、半自动索引选择方法和基于机器学习的索引选择方法。传统的在线索引选择方法不断分析工作负载，并根据工作负载的变化实时更新索引方案。《Autonomous management of soft indexes》提出了一种基于“观察-预测-反应”周期的软索引自治管理方法。首先，它从查询中提取可索引的列，并枚举候选索引方案。然后，它使用贪心策略选择具有最高估计好处的方案，并将它们添加到最终结果中。最后，当 DBMS 的负载不重时，将所选的索引方案创建实现。

《On-line index selection for shifting workloads》提出了COLT，它支持根据当前索引方案自动在线实现新索引。它将索引选择问题建模为我们在离线索引选择（DB2）中描述的背包问题，并应用动态规划来获得索引方案。一旦得出最终索引方案，它将立即被创建并实现。然而，传统的在线方法没有考虑 DBA 的经验。此外，索引方案的持续变化可能会影响 DBMS 的稳定性并导致高开销。

《Semi-automatic index tuning: Keeping DBAs in the loop》提出了一种半自动索引选择算法WFIT。WFIT考虑了 DBA 的反馈。它实时监控 DBMS，动态分析工作负载并枚举一些候选方案来调整索引结构。但在实施索引方案之前，WFIT 需要 DBA 判断是否应该对某个列建立索引。然后在后续的索引选择过程中， WFIT 将根据 DBA 的经验从索引方案中排除不应该被索引的列。同样，它也可以将应该被索引的列添加到索引方案中。与传统方法相比，WFIT 不需要选择代表性工作负载，从而减轻了 DBA 的负担。尽管半自动索引选择方法考虑了 DBA 的经验，但这些经验可能并不总是有用。基于机器学习的索引选择方法可以自动从历史数据中学习经验，而不是依赖 DBA 的反馈，并将这些经验应用于验证索引的有用性。

基于机器学习的索引选择方法自动从历史数据中学习经验，而不是依赖 DBA 的反馈。《An adaptive approach for index tuning with learning classifier systems on hybrid storage environments》提出了一种基于学习分类器系统（LCS）和遗传算法的索引选择方法 ITLCS。首先，ITLCS 使用 LCS 在列级别生成索引规则。每条规则由两部分组成：(i) 与索引相关的信息来自 DBA，例如，“列中的空元组百分比”，“列中的数据类型”；(ii) 一个动作表示是创建还是删除索引。其次，ITLCS使用遗传算法来消除 LCS 规则，并生成复合规则作为最终的索引策略。然而，生成规则是困难的。Sadri 等人[117]提出了一种基于强化学习的索引选择方法。首先，无需专家规则，他们将工作负载特征表示为查询的到达率，列特征表示为每个列的访问频率和选择性。其次，他们使用马尔可夫决策过程模型（MDP）从查询、列的特征中学习，并输出一组动作，表示创建/删除索引。

3.SQL 重写
  
许多数据库用户，特别是云用户，可能不会编写高质量的 SQL 查询，而 SQL 重写器的目标是将 SQL 查询转换为等效的形式（例如，过滤下推、将嵌套查询转换为连接查询），这些形式可以在数据库中更高效地执行。大多数现有的SQL 重写方法都采用基于规则的技术，给定一组查询重写规则，找到可以应用于查询的规则，并使用该规则重写查询。然而，评估重写操作的各种组合的代价很高，而且传统的方法往往无法处理次优化问题。此外，重写规则与应用程序高度相关，并且很难在新的场景上有效地识别规则。

因此，机器学习可以从两个方面来优化 SQL 的重写。

(1)规则选择：由于有许多重写规则，因此可以使用一个强化模型来做出重写决策。在每一步中，Agent 估计不同的重写方法的执行成本，并选择成本最低的方法。该模型迭代生成一个重写方案，并根据结果更新其决策策略。

(2)规则生成：根据不同场景下的重写规则集，使用 LSTM 模型来学习查询、编译器、硬件特性和相应规则之间的相关性。然后，对于一个新的场景，LSTM 模型捕获门单元内的特征，并预测合适的重写规则。

基于学习的数据库优化
  
基于学习的数据库优化旨在利用机器学习技术来解决数据库优化中的难题，例如成本/基数估计、连接顺序选择和端到端的优化器。

1.基数/成本估算
  
基数估计是数据库中最具挑战性的问题之一，常被称为现代查询优化器的“Achilles Heel（用以形容致命弱点）”，并且已经被研究了数十年。传统的基数估计方法可以分为三类：数据草图（data sketching）、直方图和抽样。然而，它们存在以下缺点：首先，数据草图和基于直方图的方法只能处理单列的数据分布，而对于多列表或多表连接，它们可能会由于列之间的相关性而产生较大的误差。其次，尽管基于抽样的方法可以通过使用索引捕捉多列和多表的相关性，但由于0元组问题它们在处理稀疏或复杂查询时可能效果不佳。成本估计会预测查询的物理执行计划的资源使用情况，包括I/O使用情况和 CPU 使用情况。传统的成本估计方法利用建立在基数估计基础上的成本模型来选择物理算子。与基数估计相比，成本估计为指导计划选择提供了更直接的运行开销。

最近，数据库研究人员提出使用深度学习技术来估计基数和成本。基于学习的基数估计可以分为有监督方法和无监督方法，相关方法分类详见表4。有监督方法：可以根据它们所采用的模型将监督方法进一步分为以下类别。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c625068fdcc546b483db4f0c9914e5f4.png#pic_center)

2.混合模型

论文《QuickSel: Quick selectiv ity learning with mixture models》提出了一种基于查询的方法。该方法使用混合模型通过最小化模型和可观测分布之间的差异来拟合观察到的谓词和选择性。它可以避免多维直方图的开销。然而，这些方法不支持LIKE、EXISTS和ANY等关键字。

3.全连接神经网络

论文《Learning state representations for query optimization with deep reinforce ment learning》构建了一个使用全连接神经网络的基数估计模型，并将编码的查询作为输入特征。《Selectivity estimation for range predicates using lightweight models》提出了一个针对多维数值范围谓词的选择性估计回归模型。该回归模型可以在较小的空间和时间开销下给出选择性估计，并且它可以学习多列之间的相关性，其性能优于假设属性值独立性的方法。《Towards a learning optimizer for shared clouds》提出了一种用于共享云工作负载的学习方法 CardLeaner。该方法从全部工作负载中提取重叠的子查询，并根据结构对其进行分类。每个类别的子查询都是一个模板，CardLeaner 为每个模板训练一个基数估计模型。通过这种方式，CardLeaner 可以用共享模板取代传统的子查询基数估计。然而，这种方法的有效性受到工作负载的限制。

4.卷积神经网络

《Learned cardinalities: Estimating correlated joins with deep learning》提出了一个多集合CNN算法来学习连接基数。该模型将一个查询分为三个部分，即选定的表、连接条件和过滤谓词。每个部分由一个卷积网络表示，并在平均池化后进行连接。这是第一种以端到端方式表示整个查询和学习基数的方法，它能够以较低的计算开销给出准确的基数。但是，这种方法很难直接在基于计划的查询优化场景中使用，因为查询计划中的父节点不能使用子节点的表示。《Neo: A learned query optimizer》提出了一个名为 Neo 的端到端查询优化器。Neo 使用了一个同时包含查询编码和部分计划编码的神经网络来评估当前查询的最佳性能。对于计划编码，使用 CNN 逐层聚合连接；对于谓词编码，利用 word2vec 模型训练得到行向量，并通过利用所选行的平均值对每个谓词进行编码。然而，该方法的缺点主要在于行向量表示对于在线编码来说是很耗时的。

5.无监督方法

一些研究使用无监督密度模型拟合数据集的基础分布，但它们很难支持像多连接这样的复杂查询。《Self-tuning, GPU-acceler ated kernel density models for multidimensional selectivity estimation》提出了一种基于核密度估计器（KDE）的选择性估计器。该方法的构建和维护比较轻量级，它通过选择最优带宽参数对核密度估计器模型进行数值优化，以获得更好的估计质量。KDE 可以快速拟合潜在的数据分布，易于构建和维护，并且对数据相关性非常稳健。《Multi-attribute selectivity estimation using deep learning》利用自回归密度模型来表示列间的联合数据分布。对于一个输入元组，该模型返回一个由链式规则表示的条件密度列表。为了支持范围谓词，该模型采用渐进抽样，利用学习到的密度模型来选择有意义的样本，甚至适用于倾斜的数据。但是，它不能支持高维数据或多表连接。

上述方法都有了很大的改进，但它们只支持简单/固定的查询，而且对于含有 DISTINCT 关键字的查询支持较差。因此，《NN-based transformation of any SQL cardinality estimator for handling distinct, and, OR and NOT》使用了两种方法来处理通用的查询。首先，使用深度学习方案来预测查询结果集中的 unique rate R。若结果集中存在重复行，其查询则被表示为（属性、表、连接、谓词）的集合。其次，通过将unique rate与重复结果集大小相乘来扩展现有的基数方法。

6.连接顺序选择
  
连接顺序选择在数据库系统中扮演着非常重要的角色，并且已经被研究了许多年。传统的方法通常基于基数估计和成本模型，使用一些剪枝技术来搜索所有可能的连接顺序解决方案空间。例如，基于动态规划（DP）的算法经常用于选择最佳计划，但计算开销很高。此外，由于错误的成本估计，由DP算法生成的计划可能成本很高。启发式方法，如GEQO、QuickPick-1000和GOO，可能更快地生成计划，但可能不产生良好的计划。

为了解决这些问题，近年来提出了基于机器学习的方法来提高连接顺序选择的性能，这些方法利用机器学习从以往的示例中学习，克服了由于估计不准确引起的偏差。而且，基于学习的方法可以在短时间内高效地选择更好的计划。现有的方法可以分为离线学习方法和在线学习方法。

离线学习方法：一些研究从以往的查询中学习，以提高未来查询的性能。论文《LEO DB2’s learning optimizer》提出了一个名为 LEO 的学习优化器，它利用查询执行的反馈来改进查询优化器的成本模型。它采用一种双层方法来指导计划搜索。其中，一层代表来自数据库的统计信息，另一层是从历史执行中分析得到的 System Catalog。当一个查询到来时，LEO 使用统计信息和 System catalog 的组合来估计基数和成本，并生成一个计划。在查询执行后，LEO 将与这个查询计划的准确成本和估计成本进行比较，然后更新 System catalog。

《Learning to optimize join queries with deep reinforcement learning》提出的 DQ 和《Deep reinforcement learn ing for join order enumeration》提出的 ReJoin 受到 LEO 基于反馈的方法启发，使用神经网络和强化学习来优化连接顺序，采用以前计划的成本作为训练数据来训练神经网络，以评估每个连接顺序。具体的，DQ 使用一个独热向量（one-hot vector）G 来表示一个连接状态。向量中的每个单元格指示连接树中每个表的存在性和操作选择，然后使用多层感知器（MLP）以G 作为输入来衡量每个连接状态，并使用深度 Q 网络（DQN）指导连接顺序的选择。一旦生成一个计划，计划的成本将作为反馈来训练神经网络。与 DQ 不同，ReJoin的输入由深度向量和查询向量组成。深度向量表示连接树中每个表的深度信息，查询向量表示查询信息。ReJoin使用近端策略优化（Proximal Policy Optimization，PPO）指导连接顺序的选择。结果表明，与PostgreSQL的优化器相比，DQ 和 ReJoin 都能在保持高效率的同时以低成本生成好的连接顺序。然而，ReJoin 和 DQ 中的神经网络很简单，不能充分表示连接树的结构，它们不能学习查询计划的执行延迟。

此外，上述两种方法不支持数据库模式的更新。为了解决这个问题，《Reinforcement learning with tree-LSTM for join order selection》提出了 RTOS，RTOS 具有设计良好的神经网络结构，使用两阶段训练来生成具有低延迟的更好连接顺序。为了解决ReJoin和DQ中的DNN设计不能捕捉连接树结构的问题，RTOS 提出了一个模型利用 TreeLSTM 来表示连接状态。具体的，RTOS 首先为列、表和连接树设计表示向量。在获得表示向量后使用 DQN 算法来指导生成连接的顺序。接下来，RTOS 首先使用成本来预训练 DNN，然后使用执行延迟信息在线训练神经网络。论文实验结果表明，它可以生成具有低延迟的更好连接顺序。

在线学习方法：这类方法专注于使用自适应查询处理来学习连接顺序，即使在查询执行期间也可以改变连接顺序。《Eddies: Continuously adaptive query processing》提出了一个名为 Eddy 的自适应查询处理机制。它结合了查询的执行和优化，在查询的在线执行期间学习和生成连接顺序。Eddy 将查询处理分解为多个操作符，例如在三个关系之间的两个连接操作符。Eddy 使用两种路由方法来控制这些操作符的顺序以处理传入的元组：Naive Eddy 和Lottery Eddy。Naive Eddy 可以将更多的元组路由到成本更低的操作符；而 Lottery Eddy 可以将更多的元组路由到选择性更小的操作符。然而，这两种路由方法都是为特定场景设计的，需要设计通用的路由算法来处理更复杂的情况。

《A reinforcement learn ing approach for adaptive query processing》将 Eddy 的查询执行建模为强化学习问题（Eddy-RL），并自动从查询执行中学习路由方法，无需任何人为设计。Eddy 将自身定义为代理，元组的进度定义为状态，查询中的操作符定义为动作，执行时间定义为奖励（成本）。该RL方法主要用于求解决定操作顺序的路由方法，它定义了一个 Q 函数作为所有操作符成本的总和。通过最小化 Q 函数，它指导每次选择哪个操作符。然而，上述风格的优化器没有分析预期执行时间和最优解之间的关系，也不会丢弃中间结果。

《SkinnerDB: Regret-bounded query evaluation via reinforcement learning》提出了一个基于 Eddy-RL 的强化学习模型 SkinnerDB。SkinnerDB 使用 Upper Confidence Bounds to Trees（UCT）代替 Eddy-RL 中的 Q 学习，因为 UCT 对所有选择上的累积遗憾上提供了理论保证。SkinnerDB 将查询执行分成许多小时间段，在每个时间段中，它选择一个连接顺序来执行。使用时间段内连接顺序的真实性能，Skinner-DB 训练 UCT 以指导更好的连接顺序选择。最后，Skinner-DB 合并每个时间段产生的元组以生成最终结果。

7.端到端优化器
  
虽然许多研究者尝试使用机器学习方法来解决成本/基数估计和连接顺序选择问题，但在物理计划优化中仍需要考虑许多因素，如索引和视图选择。连接顺序选择方法提供了逻辑计划，然后依靠数据库优化器来选择物理操作符和索引，并利用成本模型来生成最终的物理计划。《Neo: A learned query optimizer,” Proc. VLDBEndowment》提出了一种端到端优化器 NEO。NEO 是一种基于 ReJoin 的离线学习方法，它不使用任何成本模型和基数估计来生成最终的物理计划。

与RTOS 类似，NEO 也使用 Tree-CNN 来捕获结构信息，并使用行向量来表示谓词。为了生成物理计划，NEO使用一个独热向量来表示神经网络中每个物理算子的选择和索引的选择。然后 NEO 执行一个 DNN 引导的搜索，以最小的值不断扩展状态，以找到物理计划。此外，在没有任何成本模型信息的情况下，NEO 使用 PostgreSQL 生成的计划来对神经网络进行预训练，并使用延迟作为反馈来训练神经网络。这种端到端方法从延迟中学习以生成整个物理计划，它可以应用于任何环境，并对估计误差具有鲁棒性。

基于学习的数据库设计

数据结构学习：数据库社区和机器学习社区研究基于学习的结构，如学习的 B 树，使用基于学习的模型来取代传统的索引，以减少索引的大小，提高性能。

学习型 B+ 树索引：B+ 树索引可以看作是一个将每个查询键映射到其页面的模型。对于一个排序数组，位置 ID 越大意味着键值越大，范围索引应有效近似累积分布函数（CDF）。基于这一观察，《The case for learned index structures》提出了一个递归模型索引，该索引使用学习模型来估计一个键的页面ID。这种方法在内存环境中的表现优于 B+ 树。然而，它不支持数据更新，并且对于二级索引没有显示出有效性。

《Fiting-tree: Adata-aware index structure》提出了另一种学习型索引，Fitting 树。它提供了严格的误差界限和可预测的性能，并支持两种数据插入策略。对于就地插入策略，Fitting 树在每个页面的末端保留额外的插入空间，以使就地插入不违反页面误差。然而，对于大段数据，插入成本可能很高。增量插入策略在每个段中保留一个固定大小的缓冲区，键将被插入到缓冲区并保持排序。一旦缓冲区溢出，它就必须分割和合并段。与 Fitting 树类似，《ALEX: an updatable adaptive learned index》所提出的 Alex-index 索引也为插入的键预留空间。不同之处在于，Alex-index 中预留的空间是分散的，插入的键将直接被放置到模型预测的位置。如果该位置被占用，则会插入更多的间隙或扩展自身。Alex-index可以更灵活地平衡空间和效率之间的权衡。

《“Learned indexes for dynamic workloads》提出了一个工作负载感知的学习型索引，称为 Doraemon。Doraemon 可以通过创建多个频繁访问查询的副本来将读取访问模式纳入训练数据，这种方式可以使得频繁查询对误差做出更多贡献，并相比其他查询得到更多的优化。

学习型二级索引：《Designing succinct secondary indexing mechanism by exploiting column correlations》提出了一种简洁的二级索引机制，称为 Hermit，它利用分层回归搜索树（TRS-Tree）来捕获列相关性和异常值。TRS 树是一个机器学习增强的树索引。每个叶节点包含一个学习型线性回归模型，该模型将目标值映射到相关值。每个内部节点维护固定数量的指针指向子节点。Hermit 使用三个步骤来回答查询，首先搜索 TRS-Tree 以将目标列映射到现有索引，利用现有索引获取候选元组，最后验证元组。实验证明，Hermit 在内存和基于磁盘的关系数据库管理系统中都有效。

学习型索引 for 空间数据：传统的空间索引，例如 R树、kd树、G树，不能捕获底层数据的分布，查找时间和空间开销可以通过基于学习的技术进一步优化。例如，《“Learned index for spatial queries》提出了一种学习型 ZM 索引，该索引首先使用 Z 排序将多维地理空间点映射到一维向量，然后构建一个神经网络索引来拟合分布并预测查询的位置。

基于学习的事务管理

随着CPU核心数的增加，针对重负载的并发控制变得更加具有挑战性。有效的工作负载调度可以通过避免冲突显著提高性能。该论文从两个方面介绍了学习型事务管理技术：事务预测和事务调度。

事务预测：对数据库优化（例如，资源控制、事务调度）非常重要。传统的工作负载预测方法是基于规则的。例如，一种基于规则的方法使用数据库引擎的领域知识（例如，内部延迟、资源利用）来识别与工作负载特征相关的信号t，如内存利用率。并且这种方法直接使用内存利用率来预测未来的工作负载趋势。然而，基于规则的方法在工作负载变化时重建统计模型会耗费大量时间，因此，《Query-based workload forecasting for self-driving database management systems》提出了一个基于机器学习的系统 QB5000，它预测不同工作负载的未来趋势。QB5000 主要由三个组件组成，预处理器、聚类器和预测器。首先，预处理器记录传入的查询特征（例如，语法树、到达率）并将具有相同模板的查询聚合在一起，以近似工作负载特征。其次，聚类器使用修改后的DBSCAN算法将具有相似到达率的模板进行聚类。第三，预测器预测每个聚类的查询到达率模式。QB5000 尝试了六种不同的预测模型，训练数据来自过去观察的历史工作负载。

事务调度：传统数据库系统要么按顺序调度工作负载，要么基于数据库优化器预测的执行成本来调度工作负载。顺序调度无法考虑潜在的冲突，而成本调度由于传统的数据库优化器基于诸如均匀性和独立性等假设来估计成本，当存在连接属性之间的相关性时，这些假设可能是错误的。因此，《Scheduling OLTP transactions via machine learning》提出了一种基于机器学习的事务调度方法，它可以在并发和冲突率之间取得平衡。首先，使用监督算法估计冲突概率：构建一个分类器M来识别任何一对事务(Ti, Tj)是否会中止，其中Ti是事务查询的向量表示。训练数据是通过观察系统日志收集的，日志收集了事务提交或中止时的信息，每个样本是抽象的三元组，如 (f(Ti), f(Tj), 标签：中止)，其中 f(T) 是事务T的向量表示。其次，在可接受的中止率下将事务分配到执行队列，以实现最大吞吐量。假设(Ti’, Tj’)具有最高的中止概率，他们将 Ti’ 放入 Tj’ 之后的队列中，以便它们永远不会同时执行。

数据库监控
  
数据库监控记录系统运行状态，并检查工作负载以确保数据库的稳定性，这对数据库优化和故障诊断至关重要。例如，参数调整依赖于数据库监控指标，如系统负载、读写块数。该论文将数据库监控广泛地分为三种情况——数据库健康监控、活动监控和性能预测。

数据库健康监控：数据库健康监控记录与数据库健康相关的指标，例如每秒查询数、查询延迟，以优化数据库或诊断故障。在《Diagnosing Root Causes of Intermittent Slow Queries in Cloud Databases》中，作者假设具有相似关键性能指标（例如，cpu:usage, mysql:tps）的间歇性慢查询具有相同的根本原因。因此，采用两阶段诊断：

（i）离线阶段: 从故障记录中提取慢 SQL，根据KPI状态对它们进行聚类，并要求 DBA 为每个聚类分配根本原因；

（ii）在线阶段:对于一个传入的慢SQL，根据KPI状态的相似性分数将其与聚类 C 匹配。如果匹配成功，使用 C的根本原因来通知 DBA。否则，生成一个新的聚类，并要求DBA分配根本原因。然而，这种监控无法防止潜在的数据库故障，并且它高度依赖于 DBA 的经验。此外，随着监控指标的增加，数据库用于监控的资源相应增加，导致监控成本过高。为了解决这个问题，《P-Store: An elastic database system with predictive provisioning》提出了一个弹性数据库系统 P-Store，它将数据库监控与工作负载预测结合起来。基本思想是主动监控数据库以适应工作负载变化。

数据库活动监控（Database Activity Monitor，DAM）： 与健康监控不同，数据库活动监控用于外部监控并控制数据库活动（例如，创建新账户、查看敏感信息），这对保护敏感数据至关重要。该论文将DAM分为两类，活动选择和活动跟踪。对于活动选择，有不同级别的数据库活动（例如，DBA 活动、用户事务包括DML、DDL和DCL）。传统的 DAM 方法需要根据触发规则在额外的系统上记录所有活动。例如，公司可能会创建一个规则，每当DBA对信用卡列执行返回超过5条结果的 select 查询时，就生成一个警报。然而，记录所有活动仍然是一个沉重的负担，这带来了数据库和监控系统之间频繁的数据交换。因此，为了自动选择和记录风险活动，《Diversifying database activity monitoring with bandits》将数据库监控视为一个多臂老虎机问题（MAB）。MAB 模型是一种决策算法，通过利用当前策略和探索新策略来选择风险数据库活动。目标是训练一个具有最大风险分数的最优策略。因此，对于MAB 模型，在每一步中，它都会对一些具有最高风险的用户进行抽样，以利用策略，并抽样一些用户以探索更好的策略。对于活动跟踪，决定要监控哪些活动后，需要跟踪高风险活动并优化数据库性能。

性能预测： 传统的预测方法只捕获逻辑I/O指标（例如，页面访问时间），忽略了许多与资源相关的特征，并且无法获得准确的结果。因此，《Plan-structured deep neural network models for query performance prediction》使用深度学习来预测并发场景下的查询延迟，包括子/父操作符之间的交互和并行计划。然而，它采用了流水线结构（导致信息丢失），并且未能捕获操作符到操作符的关系，如数据共享/冲突特征。因此，《Query performance prediction for concurrent queries using graph embedding》提出了一种基于图嵌入的绩效预测方法。该方法使用图模型来表征并发查询，其中顶点是操作符，边捕获操作符之间的相关性（例如，数据传递、访问冲突、资源竞争）。使用图卷积网络来嵌入工作负载图，从图中提取与性能相关的特征，并基于这些特征进行性能预测。

基于学习的数据库安全
  
基于学习的数据库安全目标是使用机器学习技术来保护数据库中的敏感数据、访问控制和 SQL 注入攻击。

1.基于学习的敏感数据发现

由于敏感数据泄露可能导致巨大的财务和个人信息损失，因此在数据库中自动检测和保护机密数据非常重要。传统的方法使用用户定义的规则（例如，ID、信用卡和密码）来检测敏感数据。然而，这种方法存在几个限制。首先，搜索所有数据成本很高，并且需要用户指定候选搜索列以缩减搜索空间。其次，它无法自动更新新数据的规则，因此如果某些未知的敏感数据没有用户定义的规则，可能会错过敏感数据。最近，《Discovering frequent patterns in sensitive data》提出了一种使用机器学习发现敏感数据模式的算法。该算法采用拉普拉斯模型来学习数据记录的实际访问频率，然后将频繁访问的记录视为候选敏感数据。

2.访问控制

访问控制旨在防止未经授权的用户访问数据，包括表级和记录级访问控制。传统的方法主要基于静态规则，例如基于协议、角色、查询和目的的访问控制。然而，这些方法无法有效防止高级技术（例如身份冒充、元数据修改、非法入侵）伪造访问优先级。《Efficient enforcement of action aware purpose-based access control within relational database management systems》提出了一种基于目的的访问控制模型，该模型通过定制控制策略来调节数据请求。由于不同的操作和数据内容可能导致不同的隐私问题，该方法旨在学习合法访问目的。

3.SQL 注入

SQL 注入是数据库中一种常见且有害的漏洞。攻击者可以通过绕过额外的信息或干扰 SQL 语句来修改或查看超出其权限的数据，例如检索隐藏数据、颠覆应用逻辑、联合攻击等。传统的方法基于规则（例如，参数化查询），存在两个限制：首先，扫描非法参数耗时较长；其次，非法参数的变体众多，无法枚举，因此传统的特征匹配方法无法识别所有攻击。《Collaborative SQL-injections detection system with machine learning》提出了一种检测 SQL 注入的分类算法。由于查询参数中的逻辑故障或错误的过滤器导致的 SQL 攻击频繁发生。因此，该方法基于从 SQL 查询中提取的令牌构建一个分类器树，以预测可能的SQL注入。训练样本是具有典型 SQL 注入和风险级别（危险/正常/无）的查询，它们是从数据库日志中收集起来的。然而，该方法需要大量的训练数据，不能将知识推广到不同的检测目标。为了解决训练样本有限的问题，《Fuzzy neural networks to create an expert system for detecting attacks by SQL injection》提出了一种用于SQL攻击的模糊神经网络（FNN），其基本思想是用模糊规则来识别攻击模式，并在神经网络中记忆这些规则。

DB for AI
  
现有的机器学习平台很难使用，因为用户必须编写代码（例如，Python）来利用 AI 算法进行数据发现/清理、建模训练和模型推理。为了降低使用人工智能的障碍，数据库社区扩展了数据库技术，以封装人工智能算法的复杂性，并允许用户使用声明性语言，如 SQL，来利用人工智能算法。在本节中，该论文总结了用于降低人工智能复杂性的数据库技术。

声明性语言模型
  
传统的机器学习算法大多使用编程语言（例如Python、R）实现，存在一些限制。首先，它们需要工程技能来定义完整的执行逻辑，例如模型训练的迭代模式，以及像矩阵乘法和展平这样的张量操作。其次，机器学习算法必须从数据库系统中加载数据，而数据导入/导出的成本可能很高。

因此，研究人员提出了面向 AI 的声明式语言模型，通过扩展 SQL 语法来支持机器学习。综述论文将声明式语言模型分为两类：混合语言模型、统一语言模型，以及拖放方法，如下所述。

1.混合语言模型（Hybrid Language Model）

例如 BigQuery ML，包含 AI 和数据库操作。通常，对于每个查询，它将语句分割为AI操作和数据库操作。然后，它在 AI 平台（例如TensorFlow、Keras）上执行 AI 操作，在数据库上执行数据库操作。混合语言模型的优点是它们易于实现，但缺点是它们需要频繁在数据库和 AI 平台之间迁移数据，导致效率低下。

2.统一语言模型（Unified Language Model）

为了充分利用数据管理技术，提出了在数据库中本地支持AI查询的统一语言模型，无需数据迁移。《The MADlib Analytics Library or MAD Skills, the SQL》提出了一种数据库内分析方法MADlib，它提供了一套基于 SQL 的机器学习算法，分为三个步骤：

(1) AI 操作需要频繁的矩阵计算，包括乘法、转置等，MADlib 在 PostgreSQL 中实现了定制的稀疏矩阵库。

(2) MADlib 在数据库中抽象了许多AI操作，包括数据获取、访问、采样和模型定义、训练、推理。

(3) MADlib 支持数据库中的迭代训练。对于一个有n次迭代的训练过程，MADlib 首先声明一个虚拟表。然后对于每次迭代，它将 m 个样本的训练结果（例如，神经单元的梯度）作为视图进行维护，并与虚拟表连接以更新模型参数。

3.拖放界面（Drag-and-Drop Interface）

一些 SQL 语句仍然复杂（例如，嵌套结构、多重连接），对于没有经验的用户来讲非常难以理解。因此，一些研究使用拖放方法来使用AI技术。电子表格是许多数据分析师不可或缺的工具。BigQuery ML 提供了一种虚拟电子表格技术 Connected Sheets，它将电子表格界面的简单性与数据库系统中的机器学习算法结合起来。首先，它以虚拟电子表格的形式向用户呈现数十亿行数据，用户可以在该电子表格上探索数据。然后，它自动将用户执行的数据操作转换为 SQL 语句并发送到数据库。这样，数据可以使用传统的工作表函数（例如，公式、数据透视表和图表）进行分析。

数据治理
  
人工智能模型依赖于高质量的数据，而数据治理的目标是发现、清理、集成、标记数据以获得高质量的数据，这对部署人工智能模型非常重要。

数据发现：假设一名人工智能开发人员打算建立一个人工智能模型。给定一个数据集语料库，用户需要找到相关的数据集来建立一个AI模型。数据发现的目的是考虑应用程序和用户需求，从数据仓库自动查找相关数据集。许多公司提出了数据发现系统，比如微软的 Infogather 谷歌的 Goods 。前者主要关注属性级别的模式补充，旨在从大量的 Web 语料库中丰富表中的属性。后者则是一个数据集级别的方法，存储类似于数据集方案、数据集之间相似度和来源等信息，然后用户可以搜索和管理他们想要的数据集。然而这些系统的发现过程是内置的，而且存储的信息是预定义的。由于可以表示的数据集之间的关系有限，因此它们不能泛化到通用用例。面对上述问题，Fernandez 等人在《Aurum: A data discovery system》中提出了 Aurum，这是一个提供灵活查询的数据发现系统，可以根据用户需求搜索数据集。它利用企业知识图谱（EKG）捕获各种关系以支持广泛的查询。EKG 是一个超图，其中每个节点表示一个表列，每条边代表两个节点（表列）之间的关系，超边连接层次相关的节点，比如在同一个表中的列。

1.数据清洗

值得注意的是，现实中大多数的数据都是脏数据和不一致数据，会导致不可靠的决策和有偏差的分析。因此，清洗数据是必要的。数据清洗的流程包括错误检测和错误修复。大多数数据清洗方法主要关注于清洗整个数据集。然而，数据清洗是任务依赖的，不同的任务可能使用不同的清洗技术来修复数据的不同部分。为了解决这个问题，Wang 等人在《Activeclean: Interactive data cleaning for statistical modeling》中提出了一个面向机器学习任务的清洗框架 ActiveClean。给定一个数据集和具有凸损失的机器学习模型，它选择可以最大程度提高模型性能的记录进行迭代清洗。ActiveClean 由四个模块组成：采样器、清洗器、更新器和估计器。采样器用于选择要清洗的一批记录。选择标准是通过清洗记录后可以做出的改进程度来衡量的，即梯度的变化，由估计器估计。然后，所选记录将由清洗器进行检查和修复。接下来，更新器基于这些已修复的脏数据更新梯度。上述四个步骤重复执行，直到用完预算。

2.数据标注

如今，一些复杂的机器学习算法，如深度学习，通常需要大量的训练数据来训练一个好模型。因此，如何获得如此大量的标注数据是一个挑战。获得训练数据主要有三种方式：领域专家、非专家和远程监督。首先，领域专家可以提供高质量的标签，这将产生表现良好的模型。然而，要求专家标记大量数据总是成本过高，因此，主动学习技术被扩展以利用较少的标记数据来训练模型。其次，归功于诸如亚马逊 Mechanical Turk（AMT）之类的商业公共众包平台，它利用成百上千的工人来标记数据，也是解决这类任务的一种有效方式。第三，远程监督利用诸如如 Freebase 或特定领域的知识自动进行数据标注。监督学习总是需要大量的标注训练数据，这使得其成本非常昂贵。而无监督方法虽然不需要训练数据，但它们通常表现不佳。远程监督方法实现了它们之间的优势互补。

3.数据血统

数据血统旨在描述机器学习流水线之间的关系。例如，假设我们检测到工作流的错误结果然后想要调试它。回顾生成错误的源数据是非常有益的。此外，如果有脏的输入数据，识别相应的输出数据以防止错误的结果也是有帮助的。总之，数据血统描述的关系可以分为两类：

(1) 反向关系返回生成给定输出记录的输入数据子集；

(2) 正向关系返回从给定输入记录生成的输出记录子集。目前有多种数据血统方法在输入和输出数据之间建立关系，包括lazy approach、eager linear capture approach 和细粒度数据血统。lazy approach 将血统查询视为关系查询，并直接在输入关系上执行它们。优点是基本查询不会产生开销，但缺点是血统查询执行的计算成本。eager linear capture approach 构建一个血统图以加速血统查询，并使用图上的路径支持反向和正向查询。细粒度数据血统系统将血统捕获逻辑和数据库中的物理操作紧密集成，并使用轻量级和写入效率高的索引来实现低开销的血统捕获。

模型训练
  
模型训练是应用人工智能算法中不可或缺的一步，它包括特征选择、模型选择、模型管理和模型加速，该论文总结了现有的模型训练技术，如下所述。

特征选择：特征选择（Feature Selection, FS）选择可能显著影响模型性能的特征。为了便于表述，给定关系表R的数据集，我们定义整个特征集为F = {f1, f2, …, fn}。FS的目标是选择一个最优的特征子集F’ ⊆ F，以便训练具有最佳性能的模型。此外，我们用RF’ ⊆ R表示对应于特征子集F’的子数据集。一般来说，FS 过程包括以下步骤：

(1) 生成一个特征子集F0和相应的数据集RF0；

(2) 通过在 RF0 上构建机器学习模型来评估 F0；

(3) 重复上述两个步骤，直到达到预定的模型性能或评估了所有可能的特征子集。由于特征子集的数量是 O(2^n)，在大搜索空间中使用蛮力方法枚举每个子集成本过高。因此，在机器学习社区中，提出了许多方法来通过生成一些候选特征子集来减少搜索空间，此类工作综述详见《AutoML: A survey of the state-of-the-art》。最近，《Learning to optimize join queries with deep reinforcement learning》提出了利用数据库优化技术来加速FS过程，包括特征子集枚举和特征子集评估。具体来说，使用了批处理和物化技术来减少特征子集枚举成本，将基于主动学习的方法用于加速特征子集评估过程。

模型选择：模型选择的目标是在给定特定测量标准的情况下，生成模型并设置其超参数以最大化模型性能。目前有两类模型选择方法：传统模型选择和神经架构搜索（NAS）。前者侧重于从像 SVM、随机森林、KNN 等传统机器学习模型中选择最佳模型。后者旨在自动构建性能良好的神经网络架构，包括模型结构设计和超参数设置，这在机器学习和数据库社区都是当前的热门话题。该论文重点关注了基于数据库的 NAS 技术。在机器学习社区中，提出了许多自动化机器学习技术，以实现高性能模型或减少一次训练一个模型的延迟，如网格/随机搜索、强化学习方法、贝叶斯优化等。然而，这个问题的一个关键瓶颈是模型选择吞吐量，即每单位时间测试的训练配置数量。高吞吐量允许用户在固定期间内测试更多配置，这使得整个训练过程更加高效。提高吞吐量的一个直接方法是并行性，流行的并行策略包括任务并行、批量同步并行、模型跳跃并行和参数服务器。

模型管理：数据分析师通常以迭代方式构建机器学习模型。给定一个机器学习任务，他们首先从一些小巧的模型开始，指定训练/测试数据和损失函数。其次，模型在数据上进行评估。基于结果，分析师修改模型并重复上述步骤，直到得到一个表现良好的模型。然而，对于数据分析师来说，回顾以前的评估结果以获得一些见解是很困难的，因为之前构建的模型没有被记录。因此，提出了模型管理来跟踪、存储和搜索大量的机器学习模型，以便人们可以方便地分析、修订和共享他们的模型。在这一部分，该论文将模型管理系统分为两类，即基于 GUI 的系统和基于命令的系统。

硬件加速：计算机架构社区一直研究如何利用硬件加速器，如FPGA，来加速机器学习模型。给定一个在RDBMS 中存储数据的长训练时间的机器学习任务，数据科学家可以使用硬件来加速任务。为了使 RDBMS 中的硬件加速对机器学习易于使用，《In-RDBMS hardware acceleration of advanced analytics》提出了一个框架 DAnA，它将 ML 查询作为输入，调用FPGA加速器自动获取数据并进行ML计算。更具体地说，DAnA 首先设计了一种结合SQL和Python的高级编程语言来定义ML算法和所需数据，其中 SQL 部分指定如何检索和管理数据，Python 部分描述了ML算法。其次，DAnA 解析查询并使用 Striders（一种连接 FPGA 和数据库的硬件机制）。具体来说，Striders 可以跳过CPU直接从缓冲池到加速器检索训练数据，然后派生特征向量和标签。最后，设计了一个执行模型结合线程级和数据级并行性来加速 ML 算法。对于列存储数据库，《ColumnML:Column store machine learning with on-the-fly data transformation》提出了一个框架 ColumnML，研究如何利用硬件来加速广义线性模型（GLMs）的训练。对于列式存储的数据，《Stochastic methods for l1 regularized loss minimization》应用了随机坐标下降（SCD）来解决GLM 算法。ColumnML 提出了一种基于分区的 SCD 来提高缓存局部性。此外，由于列存储数据被压缩和加密，CPU 转换训练数据的效率很低。因此，ColumnML 利用 FPGA 来即时转换数据。

加速模型推理的数据库技术
  
算子支持：数据库社区研究支持数据库内置的模型推理，并利用优化技术来加速模型推理。与传统的数据库运算符（例如，过滤器、连接、排序）不同，AI模型涉及更复杂的运算符类型，包括标量（0维）/向量（一维）/矩阵（二维）/张量（N维）运算。首先，数据库原生地支持标量操作，并可以优化执行。其次，向量和张量数据可以转换为矩阵：向量是一种特殊的一维矩阵，张量可以划分为多个矩阵。因此，现有的研究大多集中在优化矩阵数据上。模型推理中的矩阵操作包括数据预处理、前向传播和模型训练，这些通常是耗时的。传统的人工智能系统依赖于 GPU 等新的硬件来提高执行性能。《SystemML: Declarative machine learning on spark》提出了一个数据库内的机器学习系统。SystemML 支持采用用户自定义聚合函数的方式进行矩阵操作，它在列级提供并行数据处理。首先，SystemML 根据其访问模式对操作进行分类，如单元格级矩阵的添加和转置。然后SystemML通过代数重写来优化这些操作，调整矩阵乘法链的算符顺序，并将它们编译成一个低级聚合算符的 DAG，如分组、求和和计数。因此，SystemML 可以在数据库中有效地执行矩阵运算符。

算子选择：相同的 ML 模型可以转换为不同的物理操作符。例如，线性回归可以解释为线性正则化操作或导数操作。然而，人工智能系统并没有直接考虑操作符的选择，将这项工作留给 GPU 这样的硬件，这可以使稀疏张量变平，并将张量分解转换为更简单的矩阵乘法。但硬件级的选择往往落入局部优化，无法估计整体的资源需求，从而产生错误估计。数据库优化器可以有效地估计执行成本，并自然优化操作的选择。

SystemML提出了一种在数据库内选择操作的方法。首先，根据数据稀疏性、集群大小或内存的资源估计函数来选择操作，例如，估计每个操作的内存消耗M(X)（表示单块矩阵的内存估计）以及M(Xp)（表示一个块分区矩阵的内存估计）。该方法的目标是在内存约束下选择操作组合，以最小化总执行时间。其次，在 Spark 系统中，它通过尽可能将选定的操作替换为 Spark 的操作符（例如，Map、Reduce、Shuffle）来进一步提高执行效率。第三，为了避免重复从HDFS读取、文本解析和shuffle这些耗时操作，在每次持久读取后注入检查点。如果任何检查点报告了这三个操作符中的一个，Spark 中的优化器将在下一次迭代中移除/替换该操作符。

执行加速：与模型训练不同，模型推理需要选择机器学习模型并执行前向传播来对不同问题进行预测。现有的执行加速技术包括内存方法和分布式方法。前者的目标是将数据压缩到内存中，并尽可能进行内存计算。后者通过将任务路由到不同的节点，并使用并行计算减少数据处理和模型计算的负担。

未来挑战
  
利用人工智能技术优化数据库仍然存在一些挑战。

大规模、高质量、多样化的训练数据获取困难。例如，在数据库旋钮调优中，训练样本需要基于 DBA 经验获得，因此很难获取非常大数量的样本。此外，为了构建有效的模型，训练数据需要涵盖不同场景、不同硬件环境和不同工作负载，因此迫切需要一个新的方法用小型的训练数据集去获得高质量模型。

适应性是一个巨大的挑战。如何使数据集上经过训练的模型适应其他数据集？如何使硬件环境中经过训练的模型适应其他硬件环境？如何使经过训练的数据库模型适应其他数据库？如何使经过训练的模型支持动态数据更新？

数据治理。模型能否收敛至关重要，如果模型不能被收敛，需要用其它方式规避延迟和不准确的决策。例如，在旋钮调优中，如果模型不收敛，就不能利用模型提供在线旋钮推荐。

传统 OLAP 任务聚焦关系型数据分析，然而大数据时代，图数据、时序数据、空间数据层出不穷，需要新的数据分析技术去分析这些多模数据。

OLTP 学习 事务模型和调度对 OLTP 系统很重要，因为不同事务间可能存在冲突。利用学习技术优化OLTP 查询是很有希望的，例如一致性快照等。

利用数据库优化AI模型也存在许多挑战。

库内训练方面: 在数据库中支持人工智能训练是一项挑战，包括模型存储、模型更新和并行训练。首先，在多租户可以训练和使用模型时，库内存储模型的安全和隐私问题是一个挑战；其次，数据动态更新时去更新模型也是一大挑战。

利用数据库技术加速AI训练： 目前大多数研究都集中在人工智能算法的有效性上，而对算法的效率关注不多，需要利用数据库技术提高算法的表现性能。

AI 优化器： 当前研究主要利用用户定义函数（UDF）来支持 AI 模型，但这些模型没有得到有效优化。需要将 AI 模型作为算子在库内执行，同时还需要为每个算子设计物理算子，最重要的是，需要将AI算子下推并预估 cost/cardinality 值，AI 优化器应支持优化AI训练和推理，此外，对于分布式环境下 AI 算子的有效支持也非常重要。

容错学习： 现有的学习模式训练不考虑容错度。一个分布式训练执行中出现一个进程崩溃，整个任务就会失败，需要将现有的容错技术以提高库内训练的健壮性。为了确保在可预测和不可预测的灾难下的业务连续性，数据库系统必须保证容错和灾难恢复能力。

结论
  
该论文综述了 AI4DB 和 DB4AI 的最新技术。前者侧重于利用人工智能技术解决计算复杂度高的数据处理问题，例如数据库旋钮调整、成本估算、连接顺序选择、索引顾问和视图顾问。后者侧重于使用数据库技术来降低使用人工智能的复杂性和加速人工智能模型，例如，声明式 AI，以及加速 AI 训练和推理。数据库和人工智能都能从对方那里获得各方面的提升，数据库与人工智能结合势必成为未来二者发展的一大趋势。

展望
  
随着大模型技术及产品的蓬勃发展，人工智能领域迎来新的里程碑式突破。人工智能的高速发展离不开海量数据的支撑，而数据库作为存储和管理数据的基础底座，也将是人工智能技术不可或缺的组成部分。因此，人工智能与数据库的融合已是大势所趋。

根据信通院发布的《数据库发展研究报告（2024年）》报告，截止目前，人工智能技术赋能数据库系统在以下三个方面取得一定的成绩：

（1）数据库智能运维： 数据库运维管理人员利用机器学习和人工智能算法优化数据库系统，在分析、配置、调优、SQL 诊断以及优化等各个环节发力，朝着自感知、自配置、自优化、自诊断和自转换的全链路查询优化方向迈进；

（2）自然语言查询： 通过大语言模型，用户能够直接将自然语言描述转化为对应的 SQL 查询语句，有效辅助非专业用户进行数据查询，降低数据库使用门槛。

（3）数据库自我管理： 随着应用上云，云原生数据库成为趋势。人工智能技术在云环境下实现数据库的全生命周期自动化管理。

未来，人工智能赋能的数据库系统将在数据库资源按需伸缩，数据库正常运行、性能和安全性，数据库手动管理任务和日常任务自动化，提升DBA管理效率等方面做进一步的突破。

根据信通院发布的《数据库发展研究报告（2024年）》报告，截止目前，数据库系统赋能人工智能在内置机器学习引擎、支撑大模型部署方面起到了举足轻重的作用。

（1）内置机器学习引擎： 通过在数据库内部原生或集成机器学习库的形式帮助用户快速构建人工智能/机器学习模型，简化了机器学习生命周期管理操作，提升了数据计算、模型开发和测试性能，确保了数据安全性。

（2）支撑大模型落地部署： 以向量数据库为代表的数据库技术有效提升了人工智能中非结构化数据的处理能力，增强了大模型的检索能力，推动了人工智能领域的创新和应用，在智能搜索、内容推荐、自然语言处理等多个领域发挥了关键的作用。

未来，数据库系统赋能人工智能将在向量数据库处理能力增强、库内原生大模型或机器学习引擎搭建等方向做进一步创新。