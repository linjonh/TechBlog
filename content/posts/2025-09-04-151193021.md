---
layout: post
title: "零样本学习与少样本学习"
date: 2025-09-04T20:03:55+0800
description: "零样本学习和少样本学习"
keywords: "零样本学习与少样本学习"
categories: ['未分类']
tags: ['学习']
artid: "151193021"
arturl: "https://blog.csdn.net/weixin_62726761/article/details/151193021"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151193021
    alt: "零样本学习与少样本学习"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151193021
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151193021
cover: https://bing.ee123.net/img/rand?artid=151193021
image: https://bing.ee123.net/img/rand?artid=151193021
img: https://bing.ee123.net/img/rand?artid=151193021
---



# 零样本学习与少样本学习

在人工智能（尤其是机器学习与深度学习）领域，零样本学习（Zero-Shot Learning, ZSL） 和少样本学习（Few-Shot Learning, FSL）是针对“数据稀缺场景”提出的核心技术范式，核心目标是解决传统监督学习对“大量标注数据”的依赖问题，让模型在“见过极少甚至没见过目标任务样本”的情况下，仍能完成有效学习和预测。

## 一、核心定义与核心差异

二者的本质区别在于模型在“目标任务”上可获取的标注样本数，具体定义如下：

| 维度 | 零样本学习（ZSL） | 少样本学习（FSL） |

|---------------------|----------------------------------------------------|----------------------------------------------------| | 核心定义 | 模型在训练时完全没见过“目标类别/任务”的标注样本，仅依赖“源类别/任务”的知识和“目标与源的关联信息”，完成对目标任务的预测。 | 模型在训练时仅能获取“目标类别/任务”的极少量标注样本（通常1-5个，称为“支持集”），结合“源任务”的预训练知识，快速适配目标任务。 |

| 标注样本数量 | 目标任务标注样本数 = 0 | 目标任务标注样本数 = 极少量（通常k=1/5/10，对应1-shot/5-shot/10-shot） |

| 核心挑战 | 如何建立“源知识”与“目标任务”的有效关联（避免“语义鸿沟”）。 | 如何在样本极少的情况下，高效利用有限信息，避免模型过拟合或泛化能力差。 |

| 典型场景类比 | 教孩子认识“苹果”后，仅通过描述“外形像苹果、表皮带刺、口感酸甜”，让孩子第一次见到“菠萝”就能认出它。 | 教孩子认识“猫”时，只给孩子看1张猫的照片，孩子就能在后续场景中区分“猫”和其他动物。 |

## 二、零样本学习（ZSL）：“没见过也能认”

零样本学习的核心逻辑是：利用“已知类别（源类别）”的标注数据，结合“源类别与未知类别（目标类别）的关联信息”，让模型具备识别“未知类别”的能力。

### 1. 关键组成要素

- 源类别（Source Classes）：有大量标注样本的已知类别（如“狗、猫、鸟”）。

- 目标类别（Target Classes）：无任何标注样本的未知类别（如“企鹅、鸵鸟”）。

- 关联信息（Auxiliary Information）：连接源与目标的“桥梁”，是ZSL的核心，常见形式包括： - 语义描述：如目标类别的文本定义（“企鹅：生活在南极、会游泳、不会飞的鸟类”）。

- 属性向量：如用“有羽毛、会飞、体型小”等属性描述类别，源和目标共享属性空间。

- 知识图谱：通过实体（类别）间的关系（如“企鹅属于鸟类”）传递知识。

### 2. 典型技术思路（以图像分类为例）

1. 特征映射：将源类别的图像特征（如CNN提取的视觉特征）与对应的语义信息（如属性向量）映射到同一个“公共空间”，让“视觉特征”与“语义描述”对齐（例如，“鸟”的视觉特征与“有羽毛、会飞”的语义向量靠近）。

2. 目标推理：对于目标类别的图像（如企鹅图片），先提取其视觉特征，再在公共空间中找到与该特征最匹配的“目标类别语义向量”，从而完成分类。

### 3. 常见应用场景

- 图像识别：识别训练时未见过的物体类别（如识别罕见动物、新型产品）。

- 自然语言处理（NLP）：零样本情感分析（仅训练“积极/消极”，预测“中性”）、零样本机器翻译（仅训练“中-英”，预测“中-日”）。

- 推荐系统：向用户推荐其从未点击过的“新类型商品”。

## 三、少样本学习（FSL）：“看几眼就能学”

少样本学习的核心逻辑是：**模型先在“大规模源任务”上预训练，获取通用知识；再利用“目标任务的极少量标注样本”进行快速适配（Fine-tuning），最终完成目标任务**。它更贴近人类“举一反三”的学习能力。

### 1. 关键组成要素

- 预训练阶段（Pre-training）：在大规模“源数据集”（如ImageNet、GPT的海量文本）上训练模型，让模型学习通用特征（如视觉领域的“边缘、纹理”，NLP领域的“语言规律”）。

- 适配阶段（Adaptation）：目标任务仅提供极少量标注样本（称为“支持集，Support Set”），模型通过“元学习（Meta-Learning）”或“小样本微调”，快速将预训练知识迁移到目标任务。

- 任务形式：常见“N-way K-shot”任务（即“从N个类别中，每个类别给K个样本，让模型区分这N个类别”），例如“5-way 1-shot”（5个类别，每个类别1个样本）。

### 2. 核心技术范式

目前FSL的主流技术分为两类：

#### - 元学习（Meta-Learning，“学会如何学习”）：

- 核心思路：让模型在“大量模拟的小样本任务”上训练，学习“快速适配新任务”的通用能力（即“学习策略”），而非仅学习某个特定任务的知识。

- 典型方法：MAML（Model-Agnostic Meta-Learning，模型无关元学习）——通过在多个小样本任务上迭代，让模型参数处于“微调少量步就能适配新任务”的最优初始状态。

#### - 迁移学习+小样本微调（Transfer Learning + Few-Shot Fine-tuning）：

- 核心思路：先在大规模数据上预训练一个“基础模型”（如BERT、ResNet），再用目标任务的少量样本对模型的“上层网络”进行微调（冻结底层通用特征，仅更新分类头），避免过拟合。

### 3. 常见应用场景

- 医疗影像诊断：某些罕见疾病的医学影像样本极少（如某些癌症的CT影像），用FSL训练模型辅助诊断。

- 机器人操作：让机器人通过几次演示（少量样本），学会新动作（如抓取新形状的物体）。

- 语音识别：为方言、小众语言构建识别模型（这类语言的标注语音数据极少）。

## 四、总结：零样本 vs. 少样本的核心区别

| 对比维度 | 零样本学习（ZSL） | 少样本学习（FSL） |

|------------------|--------------------------------------------|--------------------------------------------|

| 目标任务样本量 | 0个标注样本 | k个标注样本（k=1/5/10） |

| 依赖核心 | 源与目标的“关联信息”（语义、属性等） | 预训练的“通用知识”+ 少量样本的“快速适配” |

| 核心目标 | “从未见过”的类别/任务识别 | “见过少数几次”的类别/任务快速学习 |

| 典型问题 | 语义鸿沟（源与目标的关联不充分） | 过拟合（少量样本导致模型学偏） |

简言之：零样本是“无数据也能学”，依赖“知识关联”；少样本是“少数据能学”，依赖“通用知识+快速适配”，二者都是为了解决人工智能在“数据稀缺场景”下的落地难题，是通往更通用AI的重要方向。



