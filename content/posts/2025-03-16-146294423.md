---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f626974655f7a77792f:61727469636c652f64657461696c732f313436323934343233"
layout: post
title: "QWQ-32B-于蓝耘-MaaS-平台基于-Transformer-架构的-AIGC-推理优化策略"
date: 2025-03-16 15:11:39 +08:00
description: "在 AIGC 发展浪潮中，推理效率与准确性一直是大家关注的重点。《QWQ-32B 于蓝耘 MaaS 平台：基于 Transformer 架构的 AIGC 推理优化策略》这篇文章，深入探讨了在蓝耘 MaaS 平台上，运用 Transformer 架构对 QWQ-32B 模型开展推理优化的相关内容。对于正在探索提升模型性能方法的开发者，或是寻求更高效 AIGC 解决方案的企业而言，文中思路也许能为您带来一些启发，助力您在 AIGC 领域探索前行，不妨一读。"
keywords: "QWQ-32B 于蓝耘 MaaS 平台：基于 Transformer 架构的 AIGC 推理优化策略"
categories: ['人工智能Ai']
tags: ['自然语言处理', '深度学习', '机器学习', '人工智能', 'Aigc']
artid: "146294423"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146294423
    alt: "QWQ-32B-于蓝耘-MaaS-平台基于-Transformer-架构的-AIGC-推理优化策略"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146294423
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146294423
cover: https://bing.ee123.net/img/rand?artid=146294423
image: https://bing.ee123.net/img/rand?artid=146294423
img: https://bing.ee123.net/img/rand?artid=146294423
---

# QWQ-32B 于蓝耘 MaaS 平台：基于 Transformer 架构的 AIGC 推理优化策略

## 通义千问QwQ-32B

### 模型介绍

* **通义千问 QwQ-32B 是阿里云于 2025 年 3 月 6 日发布并开源的全新推理模型。**
* **通义千问QwQ推理模型增强版，基于Qwen2.5模型训练的QwQ推理模型，通过强化学习大幅度提升了模型推理能力。模型数学代码等核心指标（
  `AIME24/25、livecodebench`
  ）以及部分通用指标（
  `IFEval、LiveBench`
  ）达到DeepSeek-R1 满血版水平。**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b27254f093a545d296d416cd711bde5e.png)

**参数规模与性能表现：**

* 该模型仅有 320 亿参数，但性能表现出色，在多个基准测试中比肩拥有 6710 亿参数（其中 370 亿被激活）的 DeepSeek -R1。例如，在测试数学能力的 AIME24 评测集上，QwQ - 32B 取得 79.5 分，仅微弱落后于 DeepSeek - R1 的79.8 分；
* 在评估代码能力的
  `LiveCodeBench`
  中，QwQ - 32B 获得 63.4 分，接近
  `DeepSeek - R1`
  的 65.9 分。
* 而在
  **`Meta`**
  首席科学家杨立昆领衔的 “最难 LLMs 评测榜” LiveBench、谷歌等提出的指令遵循能力 IFEval 评测集、由加州大学伯克利分校等提出的评估准确调用函数或工具方面的 BFCL 测试中，QwQ - 32B 的得分均超越了 DeepSeek - R1。同时，它几乎完全超越了
  `OpenAI - o1 - mini`
  模型。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ede5ea32fd5f495999b39eff58761b6b.png)

**技术特点：**

* 强化学习的创新应用：QwQ - 32B采用结果导向型强化学习体系，在冷启动基础上，针对数学和编程任务、通用能力分别进行了两轮大规模强化学习，实现了推理能力的质变。通过两阶段动态反馈系统，模型能够根据任务结果反哺训练，结合动态奖励模型与规则验证双引擎，优化计算资源消耗与性能。
* 动态稀疏注意力机制：QwQ - 32B应用了动态稀疏注意力机制，在处理长文本时，能根据输入重要性有选择地关注关键信息，有效减少计算量，提升模型性能和计算效率，使其能够迅速提取长篇文章、学术论文等长文本中的关键信息。

**模型优势：**

* 低资源消耗与易部署：QwQ - 32B能满足更低的资源消耗需求，适合快速响应或对数据安全要求高的应用场景。开发者和企业可以在消费级显卡上将其部署到本地设备中，也可通过阿里云百炼平台直接调用模型API 服务，对于云端部署需求，还可通过
  **阿里云 PAI 平台**
  完成快速部署，或是选择容器服务 ACK 搭配阿里云 GPU 算力实现模型容器化部署和高效推理。
* 集成智能体能力：模型中集成了与智能体 Agent相关的能力，使其能够在使用工具的同时进行批判性思考，并根据环境反馈调整推理过程，为复杂场景推理提供了底层能力支撑，适用于数据分析、实验模拟等科研场景。
* 多语言支持与框架兼容：QwQ - 32B支持多语言，涵盖中文、英文、德语、法语、西班牙语等数十种语言，满足全球不同地区用户的需求。此外，它兼容
  **`PyTorch、TensorFlow`**
  等主流深度学习框架，为开发者提供了极大便利。
* 开源与应用：QwQ - 32B 基于宽松的
  **`Apache2.0`**
  协议在魔搭社区
  **`HuggingFace`**
  等平台开源，所有人都可免费下载及商用。其开源推动了端侧 AI生态构建，降低了企业接入智能计算的门槛，为工业、消费电子等领域提供低功耗、高精度的 AI 解决方案，加速了通用人工智能的普惠化进程。

---

### API示例

**API 使用前提：已获取API Key并完成配置API Key到环境变量。如果通过SDK调用，需要安装
`OpenAI`
或
`DashScope SDK`
（
DashScope Java SDK 版本需要不低于2.18.2
）。**

**注意：**

* QwQ 模型当前仅支持通过流式输出方式调用。

**如果您使用
`DashScope`
方式调用 QwQ 模型：**

* **`incremental_output`
  参数默认为
  `true`
  ，且不支持设置为
  `false`
  ，仅支持增量流式返回；**
* **`response_format`
  参数默认为"
  `message`
  "。**

**OpenAI兼容：**

示例代码：

```python
from openai import OpenAI
import os

# 初始化OpenAI客户端
client = OpenAI(
    # 如果没有配置环境变量，请用百炼API Key替换：api_key="sk-xxx"
    api_key = os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

reasoning_content = ""  # 定义完整思考过程
answer_content = ""     # 定义完整回复
is_answering = False   # 判断是否结束思考过程并开始回复

# 创建聊天完成请求
completion = client.chat.completions.create(
    model="qwq-32b",  # 此处以 qwq-32b 为例，可按需更换模型名称
    messages=[
        {"role": "user", "content": "9.9和9.11谁大"}
    ],
    stream=True,
    # 解除以下注释会在最后一个chunk返回Token使用量
    # stream_options={
    #     "include_usage": True
    # }
)

print("\n" + "=" * 20 + "思考过程" + "=" * 20 + "\n")

for chunk in completion:
    # 如果chunk.choices为空，则打印usage
    if not chunk.choices:
        print("\nUsage:")
        print(chunk.usage)
    else:
        delta = chunk.choices[0].delta
        # 打印思考过程
        if hasattr(delta, 'reasoning_content') and delta.reasoning_content != None:
            print(delta.reasoning_content, end='', flush=True)
            reasoning_content += delta.reasoning_content
        else:
            # 开始回复
            if delta.content != "" and is_answering is False:
                print("\n" + "=" * 20 + "完整回复" + "=" * 20 + "\n")
                is_answering = True
            # 打印回复过程
            print(delta.content, end='', flush=True)
            answer_content += delta.content

# print("=" * 20 + "完整思考过程" + "=" * 20 + "\n")
# print(reasoning_content)
# print("=" * 20 + "完整回复" + "=" * 20 + "\n")
# print(answer_content)

```

返回结果：

```bash
====================思考过程====================

嗯，用户问的是9.9和9.11谁大。首先我要确定这两个数字的具体含义，因为有时候在不同的上下文中可能会有不同的解释。不过通常来说，这里的数字应该是指十进制的小数，也就是数学上的数值比较。

首先，我需要比较这两个小数的大小。9.9和9.11。先看整数部分，都是9，所以整数部分相同。接下来比较小数部分。小数点后的第一位，9.9的小数点后第一位是9，而9.11的小数点后第一位是1。这时候，9比1大，所以9.9的小数部分在第一位就已经更大了，那么整个数自然更大。不过，可能有人会疑惑，因为9.11的小数点后有两位，而9.9只有两位？或者说是不是应该补零来比较？

比如，9.9可以写成9.90，而9.11是9.11，这样比较的话，第二位小数的话，9.90的第二位是0，而9.11的是1，这时候第二位的话，0比1小。不过这时候第一位已经分出胜负了，第一位9比1大，所以不管后面的小数位如何，第一位已经决定了。因此，9.9比9.11大。

不过，也有可能用户在这里的写法有其他意思，比如9.9和9.11可能指的是日期，比如9月9日和9月11日，这样的话比较的话，9.11会更大，因为11日比9日晚。但通常数字比较的话，如果没有特别说明是日期的话，应该按数值来比较。不过用户的问题用的是“谁大”，也就是数值大小，而不是日期先后，所以应该还是数值比较。

再仔细检查一遍，9.9和9.11的数值比较：

9.9 = 9 + 0.9 = 9.900...

9.11 = 9 + 0.11 = 9.110...

比较的时候，先比较整数部分，都是9，相同。然后比较小数点后的第一位，0.9的十分位是9，而0.11的十分位是1，所以9.9的十分位更大，因此整个数更大。后面的百分位虽然9.11有1，但因为十分位已经分出高下了，所以不影响结果。

所以结论应该是9.9比9.11大。不过可能有人会因为小数点后的位数不同而产生混淆，比如认为9.11有两位小数，所以更精确或者数值更大，但其实不是的，数值的大小不取决于小数点后的位数，而是每一位的数值比较。

比如，比如比较1.5和1.499，虽然1.499有三位小数，但数值上还是小于1.5。同理，9.9虽然写成一位小数，但等价于9.90，所以比9.11大。

不过，也有可能用户在这里的写法有其他意思，比如货币单位？比如9.9元和9.11元，这时候同样是9.9元更多。或者其他的上下文，但如果没有特别说明的话，应该按数值比较。

所以最终结论是9.9比9.11大。不过，为了确保正确，再举个例子，比如用分数来比较：

9.9 = 99/10 = 9.9

9.11 = 911/100 = 9.11

将它们通分比较：

99/10 = 990/100

911/100 = 911/100

显然，990 > 911，所以9.9更大。

这样确认下来，答案应该是9.9更大。不过用户可能在输入时有笔误，或者有其他意图，但按照常规数学比较的话，结论应该是正确的。
====================完整回复====================

9.9比9.11大。以下是详细分析：

1. **整数部分比较**：  
   两个数的整数部分均为 **9**，因此整数部分相等。

2. **小数部分比较**：  
   - **第一位小数**：  
     9.9 的小数点后第一位是 **9**，而 9.11 的小数点后第一位是 **1**。  
     **9 > 1**，因此仅凭这一位即可判断 **9.9 > 9.11**。

   - **后续小数位**（补充说明）：  
     即使补零为相同位数（如 9.9 = 9.90），比较第二位小数时：  
     - 9.90 的第二位是 **0**，而 9.11 的第二位是 **1**。  
     但此时第一位小数已分出胜负，后续位数不影响最终结果。

3. **分数形式验证**：  
   - 9.9 = \( \frac{99}{10} = \frac{990}{100} \)  
   - 9.11 = \( \frac{911}{100} \)  
   显然，\( 990 > 911 \)，因此 **9.9 > 9.11**。

**结论**：  
在数值大小比较中，**9.9 大于 9.11**。

```

---

**多轮对话**

**QwQ 模型 API 默认不会记录您的历史对话信息。多轮对话功能可以让大模型“拥有记忆”，满足如追问、信息采集等需要连续交流的场景。您在使用 QwQ 模型时，会接收到
`reasoning_content`
字段（思考过程）与
`content`
（回复内容），您可以将
`content`
字段通过
`{'role': 'assistant', 'content':`
拼接后的流式输出
`content}`
添加到上下文，无需添加
`reasoning_content`
字段。**

示例代码：

```python
from openai import OpenAI
import os

# 初始化OpenAI客户端
client = OpenAI(
    # 如果没有配置环境变量，请用百炼API Key替换：api_key="sk-xxx"
    api_key = os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

reasoning_content = ""  # 定义完整思考过程
answer_content = ""     # 定义完整回复
is_answering = False   # 判断是否结束思考过程并开始回复

messages = []
conversation_idx = 1
while True:
    print("="*20+f"第{conversation_idx}轮对话"+"="*20)
    conversation_idx += 1
    user_msg = {"role": "user", "content": input("请输入你的消息：")}
    messages.append(user_msg)
    # 创建聊天完成请求
    completion = client.chat.completions.create(
        model="qwq-32b",  # 此处以 qwq-32b 为例，可按需更换模型名称
        messages=messages,
        stream=True
    )
    print("\n" + "=" * 20 + "思考过程" + "=" * 20 + "\n")
    for chunk in completion:
        # 如果chunk.choices为空，则打印usage
        if not chunk.choices:
            print("\nUsage:")
            print(chunk.usage)
        else:
            delta = chunk.choices[0].delta
            # 打印思考过程
            if hasattr(delta, 'reasoning_content') and delta.reasoning_content != None:
                print(delta.reasoning_content, end='', flush=True)
                reasoning_content += delta.reasoning_content
            else:
                # 开始回复
                if delta.content != "" and is_answering is False:
                    print("\n" + "=" * 20 + "完整回复" + "=" * 20 + "\n")
                    is_answering = True
                # 打印回复过程
                print(delta.content, end='', flush=True)
                answer_content += delta.content
    messages.append({"role": "assistant", "content": answer_content})
    print("\n")
    # print("=" * 20 + "完整思考过程" + "=" * 20 + "\n")
    # print(reasoning_content)
    # print("=" * 20 + "完整回复" + "=" * 20 + "\n")
    # print(answer_content)

```

### PAI SDK

**`PAI Python SDK`
提供了一组易用的
`HighLevel API`
，以简化用户通过SDK的方式来使用PAI服务的过程。**

* SDK安装

**您可以通过以下命令安装和配置PAI Python SDK：**

```python
# 安装PAI Python SDK
pip install alipai>=0.4.5

# 配置阿里云相关服务信息
python -m pai.toolkit.config

```

* 训练

**通过 PAI Python SDK，使用默认数据或自己的业务数据进行模型训练，从而得到您的专属场景模型**
。

```python
import pprint
from pai.model import RegisteredModel

# 获取 QwQ-32B 最新的模型版本
m = RegisteredModel(
    model_name="QwQ-32B",
    model_provider="pai",
)

# 由 RegisteredModel 生成 AlgorithmEstimator
# 默认信息由 RegisteredModel 提供，用户可以根据需要修改实例类型、实例数量等信息
est = m.get_estimator()
print("实例信息：")
print(est.instance_type)

# 由 RegisteredModel 生成 AlgorithmEstimator 的默认输入
# 默认输入中所提供的训练数据集、验证数据集等信息，用户可以根据需要修改
inputs = m.get_estimator_inputs()
print("输入信息：")
pprint.pprint(inputs)

# 查看训练超参的默认值，用户可以根据需要修改
print("超参信息：")
pprint.pprint(est.hyperparameters)

# 使用默认输入提交训练任务到PAI平台
# 请注意，提交训练任务会产生DLC费用账单
est.fit(
    inputs=inputs
)

# 训练结束后，查看输出的模型地址
pprint.pprint(est.model_data())

# 查看所有输出信息（如果有多个输出）
pprint.pprint(est.get_outputs_data())

```

* 部署

**通过 PAI Python SDK，使用预置开源的模型并将其部署为在线服务，以进行实时的推理调用。**

```python
import pprint
from pai.model import RegisteredModel

# 获取 QwQ-32B 最新的模型版本
m = RegisteredModel(
    model_name="QwQ-32B",
    model_provider="pai",
)

# 查看部署信息
pprint.pprint(m.inference_spec)

# 使用默认配置部署服务，用户可以根据需要修改服务名称、实例类型等信息
# 请注意，部署服务会产生EAS费用账单
p = m.deploy(
    # service_name='xxx',
    # instance_type='xxx',
    # ... 其他参数
)

# 服务成功部署后，用户可以通过 `Predictor.predict()` 方法进行推理
p.predict(
    # 输入数据
)

# 在使用完服务后，用户可以通过 `Predictor.delete_service()` 方法删除服务，释放资源。

```

## 蓝耘MaaS平台

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/84101747f7564f919af365355b16dd20.png)

### 什么是MaaS平台？

> **模型即服务（MaaS）平台**
> 面向企业开发者、创业者及非技术背景用户，提供开箱即用的热门AI模型服务，支持零代码体验、API快速集成与灵活计费，降低AI应用开发门槛，加速业务创新。允许用户通过API接口或其他方式访问和使用预先训练好的机器学习模型，无需自己从头开始训练模型，使得即使没有深厚机器学习背景的用户也能享受到高水平的AI技术支持。

**MaaS平台的典型特点包括：**

* 模型多样性：提供多种类型的机器学习模型，适应不同的业务需求；
* 易用性：用户通过简单API调用或对话框即可使用模型；
* 可扩展性：能够根据业务的发展需求，快速扩展或更新模型服务；
* 性能保障：平台通常提供高性能计算资源，确保模型运行的效率和稳定性；
* 数据隐私与安全：保障用户数据在使用过程中的安全性和隐私性。

MaaS平台正在成为越来越多企业和开发者首选的AI解决方案，尤其适用于那些需要快速迭代和灵活应用AI技术的场景。

---

### 支持的大模型

| 模型 | API 调用模型名 | 免费赠送token | 单价（元/百万token） | 支持上下文长度 | 输入 token 上限 | 输出 token 上限 |
| --- | --- | --- | --- | --- | --- | --- |
| DeepSeek-R1 | /maas/deepseek-ai/DeepSeek-R1 | 5百万 | 8.00 元 | 60K | 65536 | 9536 |
| DeepSeek-V3 | /maas/deepseek-ai/DeepSeek-V3 | 5百万 | 4.00 元 | 60K | 65536 | 9536 |
| QwQ-32B | /maas/qwen/QwQ-32B | 1百万 | 4.00 元 | 40K | 40960 | 9536 |

> Token是指模型处理和生成文本时的基本单元，中文的Token通常是一个字或词，英文的Token通常是一个单词、子词或词组。

**除了上述文生文的deepseek，Maas平台还支持图像理解与生成、音视频理解与生成、数学领域、法律领域等方面的大模型。平台后续将纳管众多主流的第三方大模型，例如
`Llama`
、
`ChatGLM`
、零一万物、
`Stable Diffusion`
等大模型，敬请期待。**

### 优势在哪？

**为什么要选择MaaS平台调用大模型？？**

* 云原生架构设计：采用
  **`Kubernetes`**
  作为核心架构，具备高度的可扩展性和灵活性，能自动管理容器生命周期，实现资源动态分配和优化，确保大规模部署时的稳定性和高效性。
* 强大的硬件支持：支持 NVIDIA 的 A100、V100、A800 等多种主流 GPU 型号，构建大规模 GPU 集群，为用户提供充足算力资源，无论是高并发训练单个项目还是并行运行多个项目，都能确保 AI 项目顺利进行。
* 全栈服务支持：硬件资源层提供高性能 GPU 和稳定存储设备，保障硬件高效运行。软件平台层预装
  **`TensorFlow`
  、
  `PyTorch`
  、
  `MindSpore`**
  等多种主流深度学习框架，以及丰富的 AI开发工具和库，用户无需自行安装和配置复杂软件环境。
* 丰富的模型支持：除支持 DeepSeek 系列文生文模型外，还具备图像、音视频理解与生成等多模态大模型能力，并将持续集成
  **`Llama`
  、
  `ChatGLM`
  、
  `Stable Diffusion`**
  等第三方主流模型，满足多样化应用场景需求，开发者无需在多个平台间切换。

---

## MaaS平台调用QWQ-32B

### 如何部署

我们无需上面复杂的部署流程，只需在蓝耘MaaS平台上一键调用即可
  
看到这里是否也跃跃欲试，接下来我们为大家介绍如何注册蓝耘平台。
  
[点击一键跳转注册🔥](https://cloud.lanyun.net//#/registerPage?promoterCode=0131)

跳转到如下界面：我们根据需要填写对应信息就可以注册成功。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/86d425dc69b94e939ab7fae588a9f9d8.png)

**注册成功后进入主页面，点击
`MaaS`
平台**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3869ea38a41e40d2b06ca730d3a72ae2.png)

**点击左上角，选择文本模型：**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/80ecc0c77f87427a9726ee224a78d75c.png)

**继续选择QWQ-32B模型，进而切换到通义千问QWQ模型。**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/344a5f8e153349938ffb287d2a9bbabd.png)

**可以选择深度思考和联网搜索两种推理模式：**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e03eb2ef9e58431cb80477145f3ca164.png)
  
**到这里就可以尽情使用QWQ-32B完成你需要协助的任务了**

---

### 应用案例

**我们选择深度思考模式，问它一些简单的问题，比如：**

**- 帮我用C++写一个冒泡排序算法**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/16ec2214ffdf42cf9db119440d3df0ae.png)

* **思考过程：**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/aa2b570cdcea418b97e4cf0d83af1dcc.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3e61210152c5406aa1d2518990fa2101.png)

* **输出结果：**

```cpp
#include <iostream>
using namespace std;

// 冒泡排序函数
void bubbleSort(int arr[], int n) {
    for (int i = 0; i < n - 1; i++) {
        // 每一轮将最大的元素"冒泡"到正确位置
        for (int j = 0; j < n - i - 1; j++) {
            if (arr[j] > arr[j + 1]) {
                // 交换相邻元素
                int temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }
}

int main() {
    // 示例测试数组
    int arr[] = {64, 34, 25, 12, 22, 11, 90};
    int n = sizeof(arr) / sizeof(arr[0]);

    cout << "原始数组：";
    for (int i = 0; i < n; i++) {
        cout << arr[i] << " ";
    }
    cout << endl;

    // 调用冒泡排序
    bubbleSort(arr, n);

    cout << "排序后的数组：";
    for (int i = 0; i < n; i++) {
        cout << arr[i] << " ";
    }
    cout << endl;

    return 0;
}


```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c5de3e1b53fc41858938dc99f9ecc2b0.png)

---

**我们接着刚才的问题，测试一下它的上下文回答能力**

* **询问它如何优化到最佳版本？**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/67d75aceabfd4fc397b0b7dd77c4e5f6.png)

* **思考过程：**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ca14b23f2ea6422ba2565634c12d85b8.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e9539e15773c4936820c471fa15b90f2.png)

* **输出结果：**

```cpp
#include <iostream>
using namespace std;

void bubbleSort(int arr[], int n) {
    bool swapped = true;          // 标志位，判断是否发生过交换
    int lastSwapIndex = n - 1;    // 记录最后一次交换的位置

    while (swapped) {
        swapped = false;
        int newLastSwap = 0;      // 新的最后一次交换的位置

        // 遍历到 lastSwapIndex 的位置即可
        for (int i = 0; i < lastSwapIndex; i++) {
            if (arr[i] > arr[i + 1]) {
                swap(arr[i], arr[i + 1]);    // 交换相邻元素
                swapped = true;
                newLastSwap = i;             // 更新最后一次交换的位置
            }
        }

        // 缩小下次遍历的范围到 newLastSwap 的位置
        lastSwapIndex = newLastSwap;
    }
}

int main() {
    int arr[] = {64, 34, 25, 12, 22, 11, 90};
    int n = sizeof(arr) / sizeof(arr[0]);

    cout << "原始数组：";
    for (int num : arr) cout << num << " ";
    cout << endl;

    bubbleSort(arr, n);

    cout << "排序后的数组：";
    for (int num : arr) cout << num << " ";
    cout << endl;

    return 0;
}


```

**优化说明：**

提前终止（
**`Early Termination`**
）：

* 通过 swapped 标志位，当某一轮遍历中没有发生任何交换时，说明数组已经有序，直接跳出循环。
* 时间复杂度优化：在数组已排序的情况下，时间复杂度从原来的 (O(n^2)) 降为 (O(n))。

追踪最后一次交换的位置：

* 使用
  **`lastSwapIndex`**
  记录每一轮遍历中最后一次交换的位置。
* 下一轮遍历只需要处理到 lastSwapIndex 的位置，因为最后一次交换的位置之后的元素已经是有序的。
* 减少比较次数：例如，若某次遍历中最大的元素被移动到位置 (i)，则后续遍历无需再检查 (i) 之后的元素。

---

**进一步优化：双向冒泡排序（鸡尾酒排序）**

* 如果希望进一步减少比较次数，可以采用双向遍历（鸡尾酒排序），交替从左到右和从右到左遍历，同时移动最大值和最小值：

```cpp
void cocktailSort(int arr[], int n) {
    bool swapped = true;
    int start = 0, end = n - 1;

    while (swapped) {
        swapped = false;

        // 从左往右遍历，移动最大值
        for (int i = start; i < end; i++) {
            if (arr[i] > arr[i + 1]) {
                swap(arr[i], arr[i + 1]);
                swapped = true;
            }
        }

        if (!swapped) break;

        swapped = false;
        end--; // 最大值已到正确位置

        // 从右往左遍历，移动最小值
        for (int i = end - 1; i >= start; i--) {
            if (arr[i] > arr[i + 1]) {
                swap(arr[i], arr[i + 1]);
                swapped = true;
            }
        }

        start++; // 最小值已到正确位置
    }
}


```

| 算法 | 最坏时间复杂度 | 平均时间复杂度 | 空间复杂度 | 稳定性 |
| --- | --- | --- | --- | --- |
| 标准冒泡排序 | O ( n 2 ) O(n^2)      O  (   n          2  ) | O ( n 2 ) O(n^2)      O  (   n          2  ) | O ( 1 ) O(1)      O  (  1  ) | 稳定 |
| 优化后的冒泡排序 | O ( n 2 ) O(n^2)      O  (   n          2  ) | O ( n 2 ) O(n^2)      O  (   n          2  ) | O ( 1 ) O(1)      O  (  1  ) | 稳定 |
| 鸡尾酒排序 | O ( n 2 ) O(n^2)      O  (   n          2  ) | O ( n 2 ) O(n^2)      O  (   n          2  ) | O ( 1 ) O(1)      O  (  1  ) | 稳定 |

---

## 写在最后

**综上所述，MaaS平台下的QWQ-32B模型确实实至名归，强大的思考能力和推理能力完全可以与DeepSeek-R1满血版相媲美。
  
本文到这里就结束了，有关QWQ-32B模型的其它强大功能还等着你亲手去发现，快去试试吧，我们下期再见！**

```cpp
蓝耘平台注册链接：
https://cloud.lanyun.net//#/registerPage?promoterCode=0131

```