---
layout: post
title: "PyTorch深度学习框架进阶学习计划-第21天自然语言处理基础"
date: 2025-03-12 09:01:02 +0800
description: "PyTorch深度学习框架进阶学习计划 - 第21天：自然语言处理基础！如果文章对你有帮助，还请给个三连好评，感谢感谢！"
keywords: "PyTorch深度学习框架进阶学习计划 - 第21天：自然语言处理基础"
categories: ['未分类']
tags: ['自然语言处理', '深度学习', '学习', '人工智能', 'Pytorch', 'Aigc', 'Ai']
artid: "146195526"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146195526
    alt: "PyTorch深度学习框架进阶学习计划-第21天自然语言处理基础"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146195526
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146195526
cover: https://bing.ee123.net/img/rand?artid=146195526
image: https://bing.ee123.net/img/rand?artid=146195526
img: https://bing.ee123.net/img/rand?artid=146195526
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     PyTorch深度学习框架进阶学习计划 - 第21天：自然语言处理基础
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="PyTorch__21_0">
     </a>
     PyTorch深度学习框架进阶学习计划 - 第21天
    </h2>
    <h2>
     <a id="_1">
     </a>
     自然语言处理基础
    </h2>
    <p>
     今天我们将深入学习自然语言处理(NLP)的基础概念，重点关注词嵌入技术、序列建模原理以及主流模型之间的区别和优缺点。通过理解这些基础知识，你将能够更好地应用PyTorch构建NLP应用。
    </p>
    <h4>
     <a id="1__4">
     </a>
     1. 词嵌入原理与实现
    </h4>
    <p>
     <strong>
      词嵌入(Word Embeddings)
     </strong>
     是NLP中的核心概念，它将单词映射到连续向量空间，使得语义相似的词在向量空间中距离较近。
    </p>
    <h5>
     <a id="_8">
     </a>
     为什么需要词嵌入？
    </h5>
    <p>
     传统的独热编码(One-Hot Encoding)存在维度灾难和语义鸿沟问题：
    </p>
    <ul>
     <li>
      对于拥有百万级词汇的语言，独热向量非常稀疏且维度巨大
     </li>
     <li>
      独热编码无法表达单词之间的语义关系
     </li>
    </ul>
    <p>
     词嵌入解决了这些问题，它能够：
    </p>
    <ul>
     <li>
      将单词表示为低维稠密向量（通常30-300维）
     </li>
     <li>
      捕捉词之间的语义和句法关系
     </li>
     <li>
      支持词向量的算术运算（如 king - man + woman ≈ queen）
     </li>
    </ul>
    <p>
     让我们使用PyTorch实现简单的词嵌入：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>manifold <span class="token keyword">import</span> TSNE

<span class="token comment"># 定义一个简单的数据集</span>
sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"I love deep learning"</span><span class="token punctuation">,</span>
    <span class="token string">"I love PyTorch"</span><span class="token punctuation">,</span>
    <span class="token string">"I enjoy natural language processing"</span><span class="token punctuation">,</span>
    <span class="token string">"I like neural networks"</span><span class="token punctuation">,</span>
    <span class="token string">"Neural networks are fascinating"</span>
<span class="token punctuation">]</span>

<span class="token comment"># 数据预处理</span>
<span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 构建词汇表</span>
    word_list <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vocab <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">)</span>
    word_to_idx <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>word<span class="token punctuation">:</span> idx <span class="token keyword">for</span> idx<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span>
    idx_to_word <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>idx<span class="token punctuation">:</span> word <span class="token keyword">for</span> idx<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">}</span>
    
    <span class="token comment"># 构建训练样本 (上下文词, 目标词)</span>
    window_size <span class="token operator">=</span> <span class="token number">2</span>
    data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        words <span class="token operator">=</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> center_idx<span class="token punctuation">,</span> center_word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 上下文窗口内的词</span>
            context_words <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> center_idx <span class="token operator">-</span> window_size<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">min</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">,</span> center_idx <span class="token operator">+</span> window_size <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">if</span> i <span class="token operator">!=</span> center_idx<span class="token punctuation">:</span>
                    context_words<span class="token punctuation">.</span>append<span class="token punctuation">(</span>words<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 创建样本对</span>
            <span class="token keyword">for</span> context_word <span class="token keyword">in</span> context_words<span class="token punctuation">:</span>
                data<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>word_to_idx<span class="token punctuation">[</span>center_word<span class="token punctuation">]</span><span class="token punctuation">,</span> word_to_idx<span class="token punctuation">[</span>context_word<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> data<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">,</span> idx_to_word<span class="token punctuation">,</span> vocab

<span class="token comment"># 准备数据</span>
data<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">,</span> idx_to_word<span class="token punctuation">,</span> vocab <span class="token operator">=</span> preprocess<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span>
vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span>

<span class="token comment"># 定义Skip-Gram模型</span>
<span class="token keyword">class</span> <span class="token class-name">SkipGramModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SkipGramModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> vocab_size<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        embeds <span class="token operator">=</span> self<span class="token punctuation">.</span>embeddings<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>embeds<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output

<span class="token comment"># 训练模型</span>
<span class="token keyword">def</span> <span class="token function">train_skipgram</span><span class="token punctuation">(</span>data<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> SkipGramModel<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
    criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        total_loss <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> center_word<span class="token punctuation">,</span> context_word <span class="token keyword">in</span> data<span class="token punctuation">:</span>
            center_word <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>center_word<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
            context_word <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>context_word<span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 前向传播</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            log_probs <span class="token operator">=</span> model<span class="token punctuation">(</span>center_word<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>log_probs<span class="token punctuation">,</span> context_word<span class="token punctuation">)</span>
            
            <span class="token comment"># 反向传播</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            total_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epochs<span class="token punctuation">}</span></span><span class="token string">, Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>total_loss<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> model

<span class="token comment"># 训练模型并可视化词嵌入</span>
embedding_dim <span class="token operator">=</span> <span class="token number">10</span>
model <span class="token operator">=</span> train_skipgram<span class="token punctuation">(</span>data<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># 提取词嵌入</span>
embeddings <span class="token operator">=</span> model<span class="token punctuation">.</span>embeddings<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 使用t-SNE可视化词嵌入</span>
<span class="token keyword">def</span> <span class="token function">visualize_embeddings</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> idx_to_word<span class="token punctuation">)</span><span class="token punctuation">:</span>
    tsne <span class="token operator">=</span> TSNE<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>
    embeddings_tsne <span class="token operator">=</span> tsne<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>embeddings_tsne<span class="token punctuation">)</span><span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span>idx_to_word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Word Embeddings Visualization"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 可视化词嵌入</span>
visualize_embeddings<span class="token punctuation">(</span>embeddings<span class="token punctuation">,</span> idx_to_word<span class="token punctuation">)</span>

<span class="token comment"># 展示一些词向量之间的相似度</span>
<span class="token keyword">def</span> <span class="token function">cosine_similarity</span><span class="token punctuation">(</span>v1<span class="token punctuation">,</span> v2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>v1<span class="token punctuation">,</span> v2<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v1<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v2<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 计算"pytorch"和"learning"的相似度</span>
word1<span class="token punctuation">,</span> word2 <span class="token operator">=</span> <span class="token string">"pytorch"</span><span class="token punctuation">,</span> <span class="token string">"neural"</span>
<span class="token keyword">if</span> word1 <span class="token keyword">in</span> word_to_idx <span class="token keyword">and</span> word2 <span class="token keyword">in</span> word_to_idx<span class="token punctuation">:</span>
    idx1<span class="token punctuation">,</span> idx2 <span class="token operator">=</span> word_to_idx<span class="token punctuation">[</span>word1<span class="token punctuation">]</span><span class="token punctuation">,</span> word_to_idx<span class="token punctuation">[</span>word2<span class="token punctuation">]</span>
    similarity <span class="token operator">=</span> cosine_similarity<span class="token punctuation">(</span>embeddings<span class="token punctuation">[</span>idx1<span class="token punctuation">]</span><span class="token punctuation">,</span> embeddings<span class="token punctuation">[</span>idx2<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Cosine similarity between '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>word1<span class="token punctuation">}</span></span><span class="token string">' and '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>word2<span class="token punctuation">}</span></span><span class="token string">': </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>similarity<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># 查找与给定词最相似的词</span>
<span class="token keyword">def</span> <span class="token function">find_most_similar</span><span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">,</span> idx_to_word<span class="token punctuation">,</span> embeddings<span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> word <span class="token keyword">not</span> <span class="token keyword">in</span> word_to_idx<span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    word_idx <span class="token operator">=</span> word_to_idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span>
    word_vec <span class="token operator">=</span> embeddings<span class="token punctuation">[</span>word_idx<span class="token punctuation">]</span>
    
    similarities <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> vec <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> i <span class="token operator">!=</span> word_idx<span class="token punctuation">:</span>
            similarity <span class="token operator">=</span> cosine_similarity<span class="token punctuation">(</span>word_vec<span class="token punctuation">,</span> vec<span class="token punctuation">)</span>
            similarities<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>idx_to_word<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> similarity<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>similarities<span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>top_k<span class="token punctuation">]</span>

<span class="token comment"># 查找与"deep"最相似的三个词</span>
word <span class="token operator">=</span> <span class="token string">"deep"</span>
<span class="token keyword">if</span> word <span class="token keyword">in</span> word_to_idx<span class="token punctuation">:</span>
    most_similar <span class="token operator">=</span> find_most_similar<span class="token punctuation">(</span>word<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">,</span> idx_to_word<span class="token punctuation">,</span> embeddings<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Words most similar to '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>word<span class="token punctuation">}</span></span><span class="token string">':"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> similar_word<span class="token punctuation">,</span> similarity <span class="token keyword">in</span> most_similar<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"  </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>similar_word<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>similarity<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre>
    <p>
     上面的代码实现了一个简单的Skip-Gram模型，它是Word2Vec的一种实现方式。此模型通过预测上下文词来学习中心词的向量表示。在实际应用中，我们通常会使用预训练好的词嵌入，如GloVe、FastText或Word2Vec。
    </p>
    <h4>
     <a id="2_Word2VecBERT_170">
     </a>
     2. Word2Vec与BERT语义表示对比
    </h4>
    <p>
     Word2Vec和BERT代表了两代不同的词嵌入技术，它们在语义捕获能力上有显著差异。
    </p>
    <h5>
     <a id="Word2Vec_174">
     </a>
     Word2Vec特点：
    </h5>
    <ol>
     <li>
      <strong>
       静态词嵌入
      </strong>
      ：每个词只有一个固定的向量表示
     </li>
     <li>
      <strong>
       上下文无关
      </strong>
      ：无法处理一词多义的问题
     </li>
     <li>
      <strong>
       浅层模型
      </strong>
      ：基于简单的神经网络架构
     </li>
     <li>
      <strong>
       训练方式
      </strong>
      ：基于词的共现统计，使用Skip-Gram或CBOW模型
     </li>
     <li>
      <strong>
       无监督学习
      </strong>
      ：利用大规模非标注文本进行训练
     </li>
    </ol>
    <h5>
     <a id="BERT_182">
     </a>
     BERT特点：
    </h5>
    <ol>
     <li>
      <strong>
       动态词嵌入
      </strong>
      ：根据上下文生成不同的词表示
     </li>
     <li>
      <strong>
       上下文相关
      </strong>
      ：能够处理一词多义的问题
     </li>
     <li>
      <strong>
       深层模型
      </strong>
      ：基于Transformer的多层双向架构
     </li>
     <li>
      <strong>
       训练方式
      </strong>
      ：使用掩码语言模型(MLM)和下一句预测(NSP)任务
     </li>
     <li>
      <strong>
       迁移学习
      </strong>
      ：预训练+微调的范式
     </li>
    </ol>
    <p>
     让我们通过代码实现，对比这两种模型的语义表示能力：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> gensim<span class="token punctuation">.</span>models <span class="token keyword">import</span> Word2Vec
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertModel
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> PCA

<span class="token comment"># 准备测试句子展示一词多义问题</span>
sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"The bank by the river is eroding."</span><span class="token punctuation">,</span>
    <span class="token string">"I deposited money in the bank yesterday."</span><span class="token punctuation">,</span>
    <span class="token string">"The bank approved my loan application."</span>
<span class="token punctuation">]</span>

<span class="token comment"># 1. 使用Word2Vec训练静态词嵌入</span>
<span class="token comment"># 准备训练数据</span>
training_data <span class="token operator">=</span> <span class="token punctuation">[</span>s<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> sentences<span class="token punctuation">]</span>

<span class="token comment"># 训练Word2Vec模型</span>
word2vec_model <span class="token operator">=</span> Word2Vec<span class="token punctuation">(</span>sentences<span class="token operator">=</span>training_data<span class="token punctuation">,</span> vector_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> window<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>

<span class="token comment"># 2. 使用BERT获取上下文相关的词嵌入</span>
<span class="token comment"># 加载预训练的BERT模型和分词器</span>
tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>
bert_model <span class="token operator">=</span> BertModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>

<span class="token comment"># 设置为评估模式</span>
bert_model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 函数: 使用BERT获取词在特定上下文中的嵌入</span>
<span class="token keyword">def</span> <span class="token function">get_bert_embedding</span><span class="token punctuation">(</span>sentence<span class="token punctuation">,</span> target_word<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 标记化句子</span>
    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 找到目标词的索引</span>
    <span class="token keyword">try</span><span class="token punctuation">:</span>
        target_idx <span class="token operator">=</span> tokens<span class="token punctuation">.</span>index<span class="token punctuation">(</span>target_word<span class="token punctuation">)</span>
    <span class="token keyword">except</span> ValueError<span class="token punctuation">:</span>
        <span class="token comment"># 处理分词器可能将词分成子词的情况</span>
        sub_tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>tokenize<span class="token punctuation">(</span>target_word<span class="token punctuation">)</span>
        <span class="token keyword">if</span> sub_tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">in</span> tokens<span class="token punctuation">:</span>
            target_idx <span class="token operator">=</span> tokens<span class="token punctuation">.</span>index<span class="token punctuation">(</span>sub_tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Warning: '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>target_word<span class="token punctuation">}</span></span><span class="token string">' not found in tokenized sentence."</span></span><span class="token punctuation">)</span>
            <span class="token keyword">return</span> <span class="token boolean">None</span>
    
    <span class="token comment"># 转换为模型输入</span>
    indexed_tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>convert_tokens_to_ids<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
    tokens_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>indexed_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 获取BERT输出</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> bert_model<span class="token punctuation">(</span>tokens_tensor<span class="token punctuation">)</span>
        hidden_states <span class="token operator">=</span> outputs<span class="token punctuation">.</span>last_hidden_state
    
    <span class="token comment"># 提取目标词的嵌入</span>
    <span class="token comment"># BERT使用WordPiece分词，可能会将单词分成多个子词</span>
    <span class="token comment"># 这里我们简单地取第一个子词的嵌入</span>
    token_embedding <span class="token operator">=</span> hidden_states<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> target_idx<span class="token punctuation">]</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> token_embedding

<span class="token comment"># 比较Word2Vec和BERT对"bank"一词的表示</span>
target_word <span class="token operator">=</span> <span class="token string">"bank"</span>

<span class="token comment"># Word2Vec的表示（静态）</span>
<span class="token keyword">if</span> target_word <span class="token keyword">in</span> word2vec_model<span class="token punctuation">.</span>wv<span class="token punctuation">:</span>
    word2vec_embedding <span class="token operator">=</span> word2vec_model<span class="token punctuation">.</span>wv<span class="token punctuation">[</span>target_word<span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Word2Vec embedding of '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>target_word<span class="token punctuation">}</span></span><span class="token string">': Shape = </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>word2vec_embedding<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>target_word<span class="token punctuation">}</span></span><span class="token string">' not in vocabulary"</span></span><span class="token punctuation">)</span>

<span class="token comment"># BERT的表示（上下文相关）</span>
bert_embeddings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
contexts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

<span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
    bert_embedding <span class="token operator">=</span> get_bert_embedding<span class="token punctuation">(</span>sentence<span class="token punctuation">,</span> target_word<span class="token punctuation">)</span>
    <span class="token keyword">if</span> bert_embedding <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        bert_embeddings<span class="token punctuation">.</span>append<span class="token punctuation">(</span>bert_embedding<span class="token punctuation">)</span>
        contexts<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>

<span class="token keyword">if</span> bert_embeddings<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"BERT embeddings of '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>target_word<span class="token punctuation">}</span></span><span class="token string">' in different contexts: Shape = </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>bert_embeddings<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

<span class="token comment"># 可视化不同上下文中的词表示</span>
<span class="token keyword">def</span> <span class="token function">visualize_embeddings</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 使用PCA将高维向量降至2维</span>
    pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 对BERT嵌入进行降维</span>
    <span class="token keyword">if</span> bert_embeddings<span class="token punctuation">:</span>
        bert_2d <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>bert_embeddings<span class="token punctuation">)</span>
        
        plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>context<span class="token punctuation">,</span> embed_2d<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>contexts<span class="token punctuation">,</span> bert_2d<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>embed_2d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embed_2d<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'o'</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Context </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span> <span class="token punctuation">(</span>embed_2d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> embed_2d<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 如果有Word2Vec嵌入，也进行可视化</span>
        <span class="token keyword">if</span> <span class="token string">'word2vec_embedding'</span> <span class="token keyword">in</span> <span class="token builtin">locals</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            word2vec_2d <span class="token operator">=</span> pca<span class="token punctuation">.</span>transform<span class="token punctuation">(</span><span class="token punctuation">[</span>word2vec_embedding<span class="token punctuation">]</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>word2vec_2d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> word2vec_2d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> marker<span class="token operator">=</span><span class="token string">'x'</span><span class="token punctuation">,</span> s<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">)</span>
            plt<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span><span class="token string">"Word2Vec (static)"</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>word2vec_2d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> word2vec_2d<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>
        
        plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Embeddings of '</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>target_word<span class="token punctuation">}</span></span><span class="token string">' in Different Contexts"</span></span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 添加上下文说明</span>
        plt<span class="token punctuation">.</span>figtext<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token string">"\n"</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"Context </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ctx<span class="token punctuation">}</span></span><span class="token string">"</span></span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> ctx <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>contexts<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
                   ha<span class="token operator">=</span><span class="token string">"center"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bbox<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">"facecolor"</span><span class="token punctuation">:</span><span class="token string">"orange"</span><span class="token punctuation">,</span> <span class="token string">"alpha"</span><span class="token punctuation">:</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token string">"pad"</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
        
        plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
        plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 执行可视化</span>
visualize_embeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 计算BERT嵌入之间的余弦相似度</span>
<span class="token keyword">def</span> <span class="token function">cosine_similarity</span><span class="token punctuation">(</span>v1<span class="token punctuation">,</span> v2<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>v1<span class="token punctuation">,</span> v2<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v1<span class="token punctuation">)</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>v2<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 打印不同上下文中BERT嵌入的相似度</span>
<span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>bert_embeddings<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nBERT embedding similarities between different contexts:"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>bert_embeddings<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>bert_embeddings<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            sim <span class="token operator">=</span> cosine_similarity<span class="token punctuation">(</span>bert_embeddings<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> bert_embeddings<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Similarity between Context </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string"> and Context </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>sim<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre>
    <p>
     上述代码对比了Word2Vec和BERT在处理一词多义问题上的差异。使用"bank"这个词作为例子，可以看到：
    </p>
    <ol>
     <li>
      Word2Vec为"bank"仅提供一个静态向量表示，无论它出现在河岸、金融机构还是其他上下文中
     </li>
     <li>
      BERT为"bank"在不同上下文中生成不同的向量表示，能够捕捉到词的多义性
     </li>
     <li>
      通过余弦相似度可以看到，BERT表示出的不同语义上下文中的"bank"向量之间的相似度较低
     </li>
    </ol>
    <p>
     <strong>
      BERT的优势：
     </strong>
    </p>
    <ul>
     <li>
      能够处理一词多义问题
     </li>
     <li>
      捕捉丰富的上下文语义信息
     </li>
     <li>
      支持微调适应下游任务
     </li>
     <li>
      性能在多种NLP任务上表现优异
     </li>
    </ul>
    <p>
     <strong>
      Word2Vec的优势：
     </strong>
    </p>
    <ul>
     <li>
      计算效率高，训练和推理速度快
     </li>
     <li>
      资源需求小，即使在普通硬件上也能运行
     </li>
     <li>
      模型简单，易于理解和实现
     </li>
     <li>
      对于某些简单任务效果足够好
     </li>
    </ul>
    <h4>
     <a id="3_RNN_345">
     </a>
     3. RNN及其变体的序列建模能力
    </h4>
    <p>
     循环神经网络(RNN)是处理序列数据的经典模型，它通过维护一个隐藏状态来捕捉序列中的依赖关系。
    </p>
    <h5>
     <a id="RNN_348">
     </a>
     RNN及其变体的对比
    </h5>
    <table>
     <thead>
      <tr>
       <th>
        模型类型
       </th>
       <th>
        结构特点
       </th>
       <th>
        优势
       </th>
       <th>
        劣势
       </th>
       <th>
        适用场景
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        标准RNN
       </td>
       <td>
        简单的循环结构
       </td>
       <td>
        结构简单，参数少
       </td>
       <td>
        难以捕捉长距离依赖，梯度问题严重
       </td>
       <td>
        简短序列，实时性要求高的场景
       </td>
      </tr>
      <tr>
       <td>
        LSTM
       </td>
       <td>
        包含遗忘门、输入门和输出门
       </td>
       <td>
        能捕捉长距离依赖，缓解梯度问题
       </td>
       <td>
        参数较多，计算复杂
       </td>
       <td>
        长序列建模，如机器翻译、语音识别
       </td>
      </tr>
      <tr>
       <td>
        GRU
       </td>
       <td>
        包含更新门和重置门
       </td>
       <td>
        性能接近LSTM，但参数更少
       </td>
       <td>
        在某些任务上不如LSTM
       </td>
       <td>
        计算资源受限时的长序列建模
       </td>
      </tr>
      <tr>
       <td>
        双向RNN
       </td>
       <td>
        同时考虑过去和未来信息
       </td>
       <td>
        对上下文建模更全面
       </td>
       <td>
        不适用于实时场景，需要完整序列
       </td>
       <td>
        文本分类、命名实体识别等
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     让我们使用PyTorch实现一个简单的RNN语言模型，并分析其中的梯度问题：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> time

<span class="token comment"># 设置随机种子，确保结果可复现</span>
torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>

<span class="token comment"># 定义一个简单的字符级RNN语言模型</span>
<span class="token keyword">class</span> <span class="token class-name">CharRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> model_type<span class="token operator">=</span><span class="token string">'rnn'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CharRNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>model_type <span class="token operator">=</span> model_type<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 字符嵌入层</span>
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        
        <span class="token comment"># 根据模型类型选择RNN层</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>model_type <span class="token operator">==</span> <span class="token string">'rnn'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>model_type <span class="token operator">==</span> <span class="token string">'gru'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"Unsupported model type. Use 'rnn', 'lstm', or 'gru'."</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 输出层</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> hidden<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 初始化隐藏状态（如果没有提供）</span>
        <span class="token keyword">if</span> hidden <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
                h0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
                c0 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
                hidden <span class="token operator">=</span> <span class="token punctuation">(</span>h0<span class="token punctuation">,</span> c0<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
        
        <span class="token comment"># 前向传播</span>
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>
            hidden_for_grad <span class="token operator">=</span> hidden
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>
            hidden_for_grad <span class="token operator">=</span> hidden
        
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>output<span class="token punctuation">)</span>
        
        <span class="token comment"># 保存隐藏状态的梯度范数（用于分析梯度问题）</span>
        self<span class="token punctuation">.</span>hidden_grad_norm <span class="token operator">=</span> <span class="token boolean">None</span>
        hidden_for_grad<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> self<span class="token punctuation">.</span>_save_grad_norm<span class="token punctuation">(</span>grad<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> output<span class="token punctuation">,</span> hidden
    
    <span class="token keyword">def</span> <span class="token function">_save_grad_norm</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>hidden_grad_norm <span class="token operator">=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>grad<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 创建字符级语言建模的数据集</span>
<span class="token keyword">def</span> <span class="token function">create_dataset</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> seq_length<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    chars <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    char_to_idx <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>ch<span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> ch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span><span class="token punctuation">}</span>
    idx_to_char <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>i<span class="token punctuation">:</span> ch <span class="token keyword">for</span> i<span class="token punctuation">,</span> ch <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span><span class="token punctuation">}</span>
    
    <span class="token comment"># 将文本转换为索引</span>
    text_encoded <span class="token operator">=</span> <span class="token punctuation">[</span>char_to_idx<span class="token punctuation">[</span>ch<span class="token punctuation">]</span> <span class="token keyword">for</span> ch <span class="token keyword">in</span> text<span class="token punctuation">]</span>
    
    <span class="token comment"># 创建输入-输出对：每个输入是一个序列，输出是序列之后的字符</span>
    X<span class="token punctuation">,</span> y <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>text_encoded<span class="token punctuation">)</span> <span class="token operator">-</span> seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_encoded<span class="token punctuation">[</span>i<span class="token punctuation">:</span>i<span class="token operator">+</span>seq_length<span class="token punctuation">]</span><span class="token punctuation">)</span>
        y<span class="token punctuation">.</span>append<span class="token punctuation">(</span>text_encoded<span class="token punctuation">[</span>i<span class="token operator">+</span>seq_length<span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 转换为PyTorch张量</span>
    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>X<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> chars<span class="token punctuation">,</span> char_to_idx<span class="token punctuation">,</span> idx_to_char

<span class="token comment"># 训练模型并分析梯度</span>
<span class="token keyword">def</span> <span class="token function">train_and_analyze</span><span class="token punctuation">(</span>model_type<span class="token operator">=</span><span class="token string">'rnn'</span><span class="token punctuation">,</span> seq_length<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 使用一段文本数据</span>
    text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""It was a bright cold day in April, and the clocks were striking thirteen. 
    Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, 
    slipped quickly through the glass doors of Victory Mansions, though not quickly enough to 
    prevent a swirl of gritty dust from entering along with him."""</span>
    
    <span class="token comment"># 创建数据集</span>
    X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> chars<span class="token punctuation">,</span> char_to_idx<span class="token punctuation">,</span> idx_to_char <span class="token operator">=</span> create_dataset<span class="token punctuation">(</span>text<span class="token punctuation">,</span> seq_length<span class="token punctuation">)</span>
    
    <span class="token comment"># 模型参数</span>
    input_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span>  <span class="token comment"># 词汇表大小</span>
    hidden_size <span class="token operator">=</span> <span class="token number">128</span>
    output_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span>  <span class="token comment"># 词汇表大小</span>
    
    <span class="token comment"># 创建模型</span>
    model <span class="token operator">=</span> CharRNN<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> model_type<span class="token punctuation">)</span>
    
    <span class="token comment"># 损失函数和优化器</span>
    criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 用于记录梯度范数</span>
    grad_norms <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token comment"># 训练模型</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 记录每个位置的梯度范数</span>
        epoch_grad_norms <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        epoch_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        
        <span class="token comment"># 初始化隐藏状态</span>
        <span class="token keyword">if</span> model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        
        <span class="token comment"># 每个样本独立训练（为了分析梯度流动）</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># 获取样本</span>
            input_seq <span class="token operator">=</span> X<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            target <span class="token operator">=</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            
            <span class="token comment"># 前向传播</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> model<span class="token punctuation">(</span>input_seq<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>
            
            <span class="token comment"># 计算损失</span>
            loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>
            
            <span class="token comment"># 反向传播</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 记录每个位置的梯度范数</span>
            <span class="token keyword">if</span> model<span class="token punctuation">.</span>hidden_grad_norm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                epoch_grad_norms<span class="token punctuation">.</span>append<span class="token punctuation">(</span>model<span class="token punctuation">.</span>hidden_grad_norm<span class="token punctuation">)</span>
            
            <span class="token comment"># 记录损失</span>
            epoch_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 更新权重</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            <span class="token comment"># 分离隐藏状态（防止梯度在序列之间流动）</span>
            <span class="token keyword">if</span> model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
                hidden <span class="token operator">=</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> hidden<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                hidden <span class="token operator">=</span> hidden<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 保存本轮的梯度范数</span>
        grad_norms<span class="token punctuation">.</span>append<span class="token punctuation">(</span>epoch_grad_norms<span class="token punctuation">)</span>
        losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>epoch_losses<span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>num_epochs<span class="token punctuation">}</span></span><span class="token string">, Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>losses<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> grad_norms<span class="token punctuation">,</span> losses

<span class="token comment"># 分析不同RNN变体的梯度问题</span>
<span class="token keyword">def</span> <span class="token function">analyze_gradients</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 训练不同类型的RNN并收集梯度信息</span>
    models <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'rnn'</span><span class="token punctuation">,</span> <span class="token string">'lstm'</span><span class="token punctuation">,</span> <span class="token string">'gru'</span><span class="token punctuation">]</span>
    all_grad_norms <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
    all_losses <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
    
    <span class="token keyword">for</span> model_type <span class="token keyword">in</span> models<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\nTraining </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_type<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> model..."</span></span><span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> grad_norms<span class="token punctuation">,</span> losses <span class="token operator">=</span> train_and_analyze<span class="token punctuation">(</span>model_type<span class="token operator">=</span>model_type<span class="token punctuation">,</span> num_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
        all_grad_norms<span class="token punctuation">[</span>model_type<span class="token punctuation">]</span> <span class="token operator">=</span> grad_norms
        all_losses<span class="token punctuation">[</span>model_type<span class="token punctuation">]</span> <span class="token operator">=</span> losses
    
    <span class="token comment"># 可视化梯度范数</span>
    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 第一个子图：每个模型最后一个epoch的梯度分布</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> model_type <span class="token keyword">in</span> models<span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>all_grad_norms<span class="token punctuation">[</span>model_type<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_type<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Gradient Norms in Last Epoch"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Sequence Position"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Gradient Norm"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 第二个子图：每个模型的损失曲线</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> model_type <span class="token keyword">in</span> models<span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>all_losses<span class="token punctuation">[</span>model_type<span class="token punctuation">]</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_type<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Training Loss"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Epoch"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Loss"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 执行分析</span>
analyze_gradients<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 梯度裁剪演示</span>
<span class="token keyword">def</span> <span class="token function">demonstrate_gradient_clipping</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 使用标准RNN训练，分别展示有无梯度裁剪的区别</span>
    text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""To be, or not to be, that is the question:
    Whether 'tis nobler in the mind to suffer
    The slings and arrows of outrageous fortune,
    Or to take arms against a sea of troubles
    And by opposing end them."""</span>
    
    <span class="token comment"># 创建数据集</span>
    X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> chars<span class="token punctuation">,</span> char_to_idx<span class="token punctuation">,</span> idx_to_char <span class="token operator">=</span> create_dataset<span class="token punctuation">(</span>text<span class="token punctuation">,</span> seq_length<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 模型参数</span>
    input_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span>
    hidden_size <span class="token operator">=</span> <span class="token number">128</span>
    output_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>chars<span class="token punctuation">)</span>
    
    <span class="token comment"># 训练函数</span>
    <span class="token keyword">def</span> <span class="token function">train_with_clipping</span><span class="token punctuation">(</span>use_clipping<span class="token punctuation">,</span> max_norm<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model <span class="token operator">=</span> CharRNN<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> <span class="token string">'rnn'</span><span class="token punctuation">)</span>
        criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
        
        losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        grad_norms <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        
        <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            epoch_loss <span class="token operator">=</span> <span class="token number">0</span>
            epoch_grad_norm <span class="token operator">=</span> <span class="token number">0</span>
            hidden <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
            
            <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                input_seq <span class="token operator">=</span> X<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                target <span class="token operator">=</span> y<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
                
                output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> model<span class="token punctuation">(</span>input_seq<span class="token punctuation">,</span> hidden<span class="token punctuation">)</span>
                loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span>
                
                loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
                
                <span class="token comment"># 计算梯度范数</span>
                total_norm <span class="token operator">=</span> <span class="token number">0</span>
                <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    <span class="token keyword">if</span> p<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                        param_norm <span class="token operator">=</span> p<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>norm<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
                        total_norm <span class="token operator">+=</span> param_norm<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">2</span>
                total_norm <span class="token operator">=</span> total_norm <span class="token operator">**</span> <span class="token number">0.5</span>
                grad_norms<span class="token punctuation">.</span>append<span class="token punctuation">(</span>total_norm<span class="token punctuation">)</span>
                
                <span class="token comment"># 应用梯度裁剪（如果启用）</span>
                <span class="token keyword">if</span> use_clipping<span class="token punctuation">:</span>
                    torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> max_norm<span class="token punctuation">)</span>
                
                optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                hidden <span class="token operator">=</span> hidden<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
                
                epoch_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>epoch_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/10, Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>losses<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        
        <span class="token keyword">return</span> losses<span class="token punctuation">,</span> grad_norms
    
    <span class="token comment"># 训练带梯度裁剪和不带梯度裁剪的模型</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nTraining RNN without gradient clipping..."</span><span class="token punctuation">)</span>
    no_clip_losses<span class="token punctuation">,</span> no_clip_grads <span class="token operator">=</span> train_with_clipping<span class="token punctuation">(</span>use_clipping<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nTraining RNN with gradient clipping..."</span><span class="token punctuation">)</span>
    clip_losses<span class="token punctuation">,</span> clip_grads <span class="token operator">=</span> train_with_clipping<span class="token punctuation">(</span>use_clipping<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_norm<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 可视化结果</span>
    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 梯度范数</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>no_clip_grads<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Without Clipping'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>clip_grads<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'With Clipping'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>yscale<span class="token punctuation">(</span><span class="token string">'log'</span><span class="token punctuation">)</span>  <span class="token comment"># 使用对数尺度更好地显示范围差异</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Gradient Norms During Training'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Training Step'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Gradient Norm (log scale)'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 损失</span>
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>no_clip_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'Without Clipping'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>clip_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'With Clipping'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 执行梯度裁剪演示</span>
demonstrate_gradient_clipping<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="4_RNN_667">
     </a>
     4. RNN的梯度问题分析与解决方案
    </h4>
    <p>
     RNN在处理长序列时面临的主要挑战是梯度消失和梯度爆炸问题，这严重影响了模型捕捉长距离依赖的能力。
    </p>
    <h5>
     <a id="_671">
     </a>
     梯度消失问题
    </h5>
    <p>
     当误差信号向前传播时，由于重复的矩阵乘法和激活函数的导数（如sigmoid函数导数最大值为0.25），梯度会指数级减小，导致长距离依赖的信息几乎无法影响模型参数更新。
    </p>
    <p>
     例如，对于标准RNN，如果隐藏层转换矩阵的最大特征值小于1，那么梯度会随着时间步的增加呈指数级衰减：
    </p>
    <pre><code>∂L/∂h_t = ∏(i=t+1 to T) (W^T * diag(f'(h_i)))
</code></pre>
    <p>
     当这个连乘积中的每个项小于1时，乘积会随着序列长度增加而迅速趋向于0。
    </p>
    <h5>
     <a id="_683">
     </a>
     梯度爆炸问题
    </h5>
    <p>
     相反，如果隐藏层转换矩阵的最大特征值大于1，梯度会随着时间步的增加呈指数级增长，导致参数更新过大，模型无法收敛。
    </p>
    <h5>
     <a id="_687">
     </a>
     解决方案
    </h5>
    <ol>
     <li>
      <strong>
       梯度裁剪
      </strong>
      : 当梯度范数超过某个阈值时，按比例缩小梯度，防止梯度爆炸
     </li>
     <li>
      <strong>
       使用ReLU激活函数
      </strong>
      : 避免sigmoid和tanh在饱和区域的梯度消失问题
     </li>
     <li>
      <strong>
       使用LSTM/GRU
      </strong>
      : 这些变体通过门控机制和额外的记忆单元缓解梯度问题
     </li>
     <li>
      <strong>
       残差连接
      </strong>
      : 类似ResNet的跳跃连接可以帮助梯度直接流动
     </li>
     <li>
      <strong>
       Transformer架构
      </strong>
      : 完全抛弃循环结构，使用自注意力机制建模序列关系
     </li>
    </ol>
    <h5>
     <a id="LSTM_695">
     </a>
     LSTM如何缓解梯度问题
    </h5>
    <p>
     LSTM通过以下机制缓解梯度问题：
    </p>
    <ol>
     <li>
      <strong>
       记忆单元(Cell State)
      </strong>
      : 提供了一条信息高速公路，允许梯度无阻碍地流动
     </li>
     <li>
      <strong>
       遗忘门
      </strong>
      : 控制丢弃哪些信息，减少不相关信息的干扰
     </li>
     <li>
      <strong>
       输入门
      </strong>
      : 控制新信息的添加，使模型能够选择性地更新状态
     </li>
     <li>
      <strong>
       输出门
      </strong>
      : 控制哪些信息会输出，进一步优化信息流
     </li>
    </ol>
    <h4>
     <a id="5__704">
     </a>
     5. 动手实践：构建基础文本分类器
    </h4>
    <p>
     现在，让我们将所学知识应用到实际问题中，构建一个简单的文本分类器：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 定义一个简单的数据集</span>
<span class="token comment"># 使用一些示例句子和情感标签（0=负面，1=正面）</span>
sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"I love this movie so much!"</span><span class="token punctuation">,</span>
    <span class="token string">"This film is amazing and wonderful"</span><span class="token punctuation">,</span>
    <span class="token string">"The acting was great and the plot was engaging"</span><span class="token punctuation">,</span>
    <span class="token string">"I enjoyed watching this show"</span><span class="token punctuation">,</span>
    <span class="token string">"This is my favorite movie of all time"</span><span class="token punctuation">,</span>
    <span class="token string">"The story was captivating from start to finish"</span><span class="token punctuation">,</span>
    <span class="token string">"I hate this movie, it was terrible"</span><span class="token punctuation">,</span>
    <span class="token string">"This film is boring and predictable"</span><span class="token punctuation">,</span>
    <span class="token string">"The acting was poor and the plot made no sense"</span><span class="token punctuation">,</span>
    <span class="token string">"I regret watching this show"</span><span class="token punctuation">,</span>
    <span class="token string">"This is the worst movie I've ever seen"</span><span class="token punctuation">,</span>
    <span class="token string">"The story was confusing and uninteresting"</span>
<span class="token punctuation">]</span>

labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>

<span class="token comment"># 数据预处理</span>
<span class="token keyword">def</span> <span class="token function">preprocess_data</span><span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 创建词汇表</span>
    vocab <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            vocab<span class="token punctuation">.</span>add<span class="token punctuation">(</span>word<span class="token punctuation">)</span>
    
    word_to_idx <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>word<span class="token punctuation">:</span> i<span class="token operator">+</span><span class="token number">1</span> <span class="token keyword">for</span> i<span class="token punctuation">,</span> word <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">sorted</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span>  <span class="token comment"># 0保留为padding</span>
    word_to_idx<span class="token punctuation">[</span><span class="token string">'&lt;PAD&gt;'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
    
    <span class="token comment"># 将句子转换为索引序列</span>
    indexed_sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        indexed_sentence <span class="token operator">=</span> <span class="token punctuation">[</span>word_to_idx<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        indexed_sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>indexed_sentence<span class="token punctuation">)</span>
    
    <span class="token comment"># 填充序列到相同长度</span>
    max_length <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span> <span class="token keyword">for</span> s <span class="token keyword">in</span> indexed_sentences<span class="token punctuation">)</span>
    padded_sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> sentence <span class="token keyword">in</span> indexed_sentences<span class="token punctuation">:</span>
        padded <span class="token operator">=</span> sentence <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_length <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
        padded_sentences<span class="token punctuation">.</span>append<span class="token punctuation">(</span>padded<span class="token punctuation">)</span>
    
    <span class="token comment"># 转换为PyTorch张量</span>
    X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>padded_sentences<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>

<span class="token comment"># 准备数据</span>
X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">,</span> vocab_size <span class="token operator">=</span> preprocess_data<span class="token punctuation">(</span>sentences<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>

<span class="token comment"># 定义模型</span>
<span class="token keyword">class</span> <span class="token class-name">TextClassifier</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">,</span> model_type<span class="token operator">=</span><span class="token string">'lstm'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TextClassifier<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model_type <span class="token operator">=</span> model_type<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">elif</span> model_type <span class="token operator">==</span> <span class="token string">'gru'</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>GRU<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>rnn <span class="token operator">=</span> nn<span class="token punctuation">.</span>RNN<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> hidden_dim<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># text shape: [batch_size, sequence_length]</span>
        
        embedded <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
        <span class="token comment"># embedded shape: [batch_size, sequence_length, embedding_dim]</span>
        
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>model_type <span class="token operator">==</span> <span class="token string">'lstm'</span><span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> <span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> cell<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            output<span class="token punctuation">,</span> hidden <span class="token operator">=</span> self<span class="token punctuation">.</span>rnn<span class="token punctuation">(</span>embedded<span class="token punctuation">)</span>
        
        <span class="token comment"># 使用最后一个时间步的隐藏状态</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>hidden<span class="token punctuation">,</span> <span class="token builtin">tuple</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            hidden <span class="token operator">=</span> hidden<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        
        hidden <span class="token operator">=</span> hidden<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token comment"># hidden shape: [batch_size, hidden_dim]</span>
        
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>

<span class="token comment"># 初始化模型和训练参数</span>
EMBEDDING_DIM <span class="token operator">=</span> <span class="token number">100</span>
HIDDEN_DIM <span class="token operator">=</span> <span class="token number">128</span>
OUTPUT_DIM <span class="token operator">=</span> <span class="token number">1</span>  <span class="token comment"># 二分类，输出维度为1</span>

model <span class="token operator">=</span> TextClassifier<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> HIDDEN_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> model_type<span class="token operator">=</span><span class="token string">'lstm'</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>BCEWithLogitsLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 训练模型</span>
<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    accuracies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 前向传播</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        predictions <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>predictions<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
        
        <span class="token comment"># 反向传播</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        <span class="token comment"># 计算准确率</span>
        predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">round</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>predictions<span class="token punctuation">)</span><span class="token punctuation">)</span>
        correct <span class="token operator">=</span> <span class="token punctuation">(</span>predictions <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        accuracy <span class="token operator">=</span> correct <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>
        
        losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        accuracies<span class="token punctuation">.</span>append<span class="token punctuation">(</span>accuracy<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epochs<span class="token punctuation">}</span></span><span class="token string">, Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">, Accuracy: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>accuracy<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> losses<span class="token punctuation">,</span> accuracies

<span class="token comment"># 训练模型</span>
losses<span class="token punctuation">,</span> accuracies <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>

<span class="token comment"># 可视化训练过程</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>losses<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>accuracies<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Accuracy'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 测试模型</span>
<span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> sentence<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 预处理句子</span>
    tokens <span class="token operator">=</span> sentence<span class="token punctuation">.</span>lower<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
    indexed <span class="token operator">=</span> <span class="token punctuation">[</span>word_to_idx<span class="token punctuation">.</span>get<span class="token punctuation">(</span>token<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> tokens<span class="token punctuation">]</span>  <span class="token comment"># 使用0处理未知词</span>
    
    <span class="token comment"># 转换为张量</span>
    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>indexed<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 预测</span>
    prediction <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>model<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> prediction<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"Positive"</span> <span class="token keyword">if</span> prediction<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0.5</span> <span class="token keyword">else</span> <span class="token string">"Negative"</span>

<span class="token comment"># 测试一些新句子</span>
test_sentences <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"I really enjoyed this film"</span><span class="token punctuation">,</span>
    <span class="token string">"This movie is terrible and boring"</span><span class="token punctuation">,</span>
    <span class="token string">"The acting was quite good"</span><span class="token punctuation">,</span>
    <span class="token string">"I fell asleep during the movie"</span>
<span class="token punctuation">]</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nTesting the model on new sentences:"</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> sentence <span class="token keyword">in</span> test_sentences<span class="token punctuation">:</span>
    score<span class="token punctuation">,</span> sentiment <span class="token operator">=</span> predict_sentiment<span class="token punctuation">(</span>model<span class="token punctuation">,</span> sentence<span class="token punctuation">,</span> word_to_idx<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>sentence<span class="token punctuation">}</span></span><span class="token string">" - Sentiment: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>sentiment<span class="token punctuation">}</span></span><span class="token string">, Score: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>score<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

<span class="token comment"># 比较不同RNN变体的性能</span>
<span class="token keyword">def</span> <span class="token function">compare_rnn_variants</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    models <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string">'rnn'</span><span class="token punctuation">:</span> TextClassifier<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> HIDDEN_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> model_type<span class="token operator">=</span><span class="token string">'rnn'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'lstm'</span><span class="token punctuation">:</span> TextClassifier<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> HIDDEN_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> model_type<span class="token operator">=</span><span class="token string">'lstm'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token string">'gru'</span><span class="token punctuation">:</span> TextClassifier<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> EMBEDDING_DIM<span class="token punctuation">,</span> HIDDEN_DIM<span class="token punctuation">,</span> OUTPUT_DIM<span class="token punctuation">,</span> model_type<span class="token operator">=</span><span class="token string">'gru'</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span>
    
    results <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>
    
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> model <span class="token keyword">in</span> models<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"\nTraining </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>name<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> model..."</span></span><span class="token punctuation">)</span>
        optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
        losses<span class="token punctuation">,</span> accuracies <span class="token operator">=</span> train<span class="token punctuation">(</span>model<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> epochs<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span>
        results<span class="token punctuation">[</span>name<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>losses<span class="token punctuation">,</span> accuracies<span class="token punctuation">)</span>
    
    <span class="token comment"># 可视化比较</span>
    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> <span class="token punctuation">(</span>losses<span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token keyword">in</span> results<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>losses<span class="token punctuation">,</span> label<span class="token operator">=</span>name<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Loss Comparison'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> <span class="token punctuation">(</span>_<span class="token punctuation">,</span> accuracies<span class="token punctuation">)</span> <span class="token keyword">in</span> results<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>accuracies<span class="token punctuation">,</span> label<span class="token operator">=</span>name<span class="token punctuation">.</span>upper<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Accuracy Comparison'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Accuracy'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 比较不同RNN变体</span>
compare_rnn_variants<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h3>
     <a id="_938">
     </a>
     总结与关键点回顾
    </h3>
    <p>
     今天我们深入学习了自然语言处理的基础概念，特别是词嵌入技术、序列建模以及RNN的梯度问题。以下是关键知识点的总结：
    </p>
    <h4>
     <a id="_942">
     </a>
     词嵌入技术
    </h4>
    <ol>
     <li>
      <strong>
       词嵌入定义
      </strong>
      ：将单词映射到低维稠密向量空间的技术
     </li>
     <li>
      <strong>
       主要优势
      </strong>
      ：解决了独热编码的维度灾难和语义鸿沟问题
     </li>
     <li>
      <strong>
       实现方式
      </strong>
      ：通过神经网络训练上下文预测任务来学习表示
     </li>
    </ol>
    <h4>
     <a id="Word2VecBERT_948">
     </a>
     Word2Vec与BERT对比
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        特性
       </th>
       <th>
        Word2Vec
       </th>
       <th>
        BERT
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        词表示类型
       </td>
       <td>
        静态词嵌入
       </td>
       <td>
        动态上下文相关表示
       </td>
      </tr>
      <tr>
       <td>
        一词多义
       </td>
       <td>
        不支持
       </td>
       <td>
        支持
       </td>
      </tr>
      <tr>
       <td>
        模型复杂度
       </td>
       <td>
        浅层模型
       </td>
       <td>
        深层双向Transformer
       </td>
      </tr>
      <tr>
       <td>
        训练方法
       </td>
       <td>
        上下文词预测
       </td>
       <td>
        掩码语言模型
       </td>
      </tr>
      <tr>
       <td>
        资源需求
       </td>
       <td>
        低
       </td>
       <td>
        高
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     BERT能够根据上下文生成不同的词向量表示，而Word2Vec为每个词只提供一个固定的向量，无法处理一词多义的情况。
    </p>
    <h4>
     <a id="RNN_960">
     </a>
     RNN及其变体
    </h4>
    <ol>
     <li>
      <strong>
       标准RNN
      </strong>
      ：简单的循环结构，但存在严重的梯度问题
     </li>
     <li>
      <strong>
       LSTM
      </strong>
      ：通过门控机制和记忆单元解决长距离依赖问题
     </li>
     <li>
      <strong>
       GRU
      </strong>
      ：LSTM的简化版本，性能相近但参数更少
     </li>
     <li>
      <strong>
       双向RNN
      </strong>
      ：考虑序列的过去和未来信息，提供更全面的上下文
     </li>
    </ol>
    <h4>
     <a id="RNN_967">
     </a>
     RNN梯度问题
    </h4>
    <ol>
     <li>
      <strong>
       梯度消失
      </strong>
      ：梯度随着时间步呈指数级衰减，导致长距离依赖无法学习
     </li>
     <li>
      <strong>
       梯度爆炸
      </strong>
      ：梯度随着时间步呈指数级增长，导致训练不稳定
     </li>
     <li>
      <strong>
       解决方案
      </strong>
      ：
      <ul>
       <li>
        使用LSTM/GRU架构
       </li>
       <li>
        梯度裁剪
       </li>
       <li>
        残差连接
       </li>
       <li>
        使用ReLU激活函数
       </li>
       <li>
        采用Transformer架构
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <p>
     <strong>
      清华大学全三版的《DeepSeek教程》完整的文档需要的朋友，关注我私信：deepseek 即可获得。
     </strong>
    </p>
    <p>
     <strong>
      怎么样今天的内容还满意吗？再次感谢朋友们的观看，关注GZH：凡人的AI工具箱，回复666，送您价值199的AI大礼包。最后，祝您早日实现财务自由，还请给个赞，谢谢！
     </strong>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34303738303137382f:61727469636c652f64657461696c732f313436313935353236" class_="artid" style="display:none">
 </p>
</div>


