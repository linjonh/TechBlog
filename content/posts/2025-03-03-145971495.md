---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f616e68656963616e672f:61727469636c652f64657461696c732f313435393731343935"
layout: post
title: "通往-AI-之路Python-机器学习入门-线性代数"
date: 2025-03-03 16:15:00 +0800
description: "线性代数是机器学习的核心数学工具，广泛用于数据表示、模型计算和优化算法。本章将从基本概念开始，介绍标量、向量、矩阵及其运算，最后深入特征值、特征向量和奇异值分解（SVD）。"
keywords: "通往 AI 之路：Python 机器学习入门-线性代数"
categories: ['从0开始学习机器学习']
tags: ['线性代数', '机器学习', '开发语言', '后端', '人工智能', 'Python']
artid: "145971495"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145971495
    alt: "通往-AI-之路Python-机器学习入门-线性代数"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145971495
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145971495
cover: https://bing.ee123.net/img/rand?artid=145971495
image: https://bing.ee123.net/img/rand?artid=145971495
img: https://bing.ee123.net/img/rand?artid=145971495
---

# 通往 AI 之路：Python 机器学习入门-线性代数

## **2.1 线性代数（机器学习的核心）**

线性代数是机器学习的基础之一，许多核心算法都依赖矩阵运算。本章将介绍线性代数中的基本概念，包括标量、向量、矩阵、矩阵运算、特征值与特征向量，以及奇异值分解（SVD）。

---

### **2.1.1 标量、向量、矩阵**

#### **1. 标量（Scalar）**

标量是一个单独的数，例如：

```plaintext
a = 5

```

在 Python 中：

```python
a = 5  # 标量

```

#### **2. 向量（Vector）**

向量是由多个数值组成的一维数组，例如：

```plaintext
v = [2, 3, 5]

```

Python 实现：

```python
import numpy as np
v = np.array([2, 3, 5])  # 一维数组表示向量
print(v)

```

#### **3. 矩阵（Matrix）**

矩阵是一个二维数组，例如：

```plaintext
A = [[1, 2],
     [3, 4]]

```

Python 实现：

```python
A = np.array([[1, 2], [3, 4]])  # 二维数组表示矩阵
print(A)

```

---

### **2.1.2 矩阵运算**

#### **1. 矩阵加法**

两个相同形状的矩阵可以相加：

```plaintext
A + B = [[1, 2],    +   [[5, 6],    =   [[6,  8],
         [3, 4]]         [7, 8]]         [10, 12]]

```

Python 计算：

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A + B
print(C)

```

#### **2. 矩阵乘法**

* **逐元素相乘（Hadamard 乘积）**

```plaintext
A ⊙ B = [[1×5,  2×6],
         [3×7,  4×8]]
       = [[5, 12],
          [21, 32]]

```

Python 实现：

```python
C = A * B  # 逐元素相乘
print(C)

```

* **矩阵乘法（点积）**

```plaintext
A × B = [[1×5 + 2×7,  1×6 + 2×8],
         [3×5 + 4×7,  3×6 + 4×8]]
       = [[19, 22],
          [43, 50]]

```

Python 实现：

```python
C = np.dot(A, B)  # 矩阵乘法
print(C)

```

#### **3. 矩阵转置**

矩阵转置是将行变成列：

```plaintext
A^T = [[1, 3],
       [2, 4]]

```

Python 计算：

```python
A_T = A.T  # 计算转置
print(A_T)

```

#### **4. 逆矩阵**

如果矩阵
`A`
是可逆的（即
`det(A) ≠ 0`
），那么存在一个矩阵
`A^(-1)`
使得：

```plaintext
A × A^(-1) = I  (单位矩阵)

```

Python 计算：

```python
A_inv = np.linalg.inv(A)  # 计算逆矩阵
print(A_inv)

```

---

### **2.1.3 特征值与特征向量**

特征值（Eigenvalue）和特征向量（Eigenvector）在机器学习中用于主成分分析（PCA）等算法。

#### **1. 定义**

对于矩阵
`A`
，如果存在一个向量
`v`
和一个数
`λ`
使得：

```plaintext
A × v = λ × v

```

那么
`v`
是
`A`
的特征向量，
`λ`
是对应的特征值。

#### **2. Python 计算特征值和特征向量**

```python
eigenvalues, eigenvectors = np.linalg.eig(A)
print("特征值:", eigenvalues)
print("特征向量:", eigenvectors)

```

---

### **2.1.4 SVD（奇异值分解）**

奇异值分解（Singular Value Decomposition, SVD）是矩阵分解的一种重要方法，它可以表示为：

```plaintext
A = U × Σ × V^T

```

其中：

* `U`
  是左奇异向量矩阵
* `Σ`
  是对角矩阵，对角线上的元素称为奇异值
* `V^T`
  是右奇异向量矩阵的转置

#### **Python 计算 SVD**

```python
U, S, V_T = np.linalg.svd(A)
print("U 矩阵:", U)
print("Σ 矩阵:", S)
print("V^T 矩阵:", V_T)

```

SVD 在降维（如 PCA）中有重要应用，后续章节将深入介绍。

---

### **总结**

本章介绍了机器学习中常用的线性代数知识，包括：

* **标量、向量、矩阵**
  及其表示方式
* **矩阵运算**
  （加法、乘法、转置、逆矩阵）
* **特征值与特征向量**
  （PCA 等算法的基础）
* **SVD（奇异值分解）**
  （在数据降维中的应用）

掌握这些内容，有助于理解机器学习的数学基础！建议多实践代码，加深理解！