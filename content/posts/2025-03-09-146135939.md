---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34313230343436342f:61727469636c652f64657461696c732f313436313335393339"
layout: post
title: "复现-MODEST-机器人抓取透明物体-单目-ICRA-2025"
date: 2025-03-09 18:47:31 +0800
description: "MODEST 单目透明物体抓取算法，来自ICRA 2025，本文分享它的复现过程。输入单个视角的RGB图像，模型需要同时处理深度和分割任务，输出透明物体的分割结果和场景深度预测。将算法迁移到真实机器人平台，开展了透明物体抓取实验。实验平台主要由UR机械臂和深度相机组成。在借助MODEST方法对透明物体进行分割和深度预测，生成点云数据作为输入，进而采用GraspNet生成抓取位姿。"
keywords: "复现 MODEST 机器人抓取透明物体 单目 ICRA 2025"
categories: ['机器人']
tags: ['透明物体', '深度预测', '机器人', '抓取', '分割']
artid: "146135939"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146135939
    alt: "复现-MODEST-机器人抓取透明物体-单目-ICRA-2025"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146135939
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146135939
cover: https://bing.ee123.net/img/rand?artid=146135939
image: https://bing.ee123.net/img/rand?artid=146135939
img: https://bing.ee123.net/img/rand?artid=146135939
---

# 复现 MODEST 机器人抓取透明物体 单目 ICRA 2025

MODEST 单目透明物体抓取算法，来自ICRA 2025，本文分享它的复现过程。

输入单个视角的RGB图像，模型需要同时处理深度和分割任务，输出透明物体的分割结果和场景深度预测。

论文地址：
[Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion](https://arxiv.org/pdf/2502.14616 "Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion")

代码地址：
[https://github.com/D-Robotics-AI-Lab/MODEST](https://github.com/D-Robotics-AI-Lab/MODEST "https://github.com/D-Robotics-AI-Lab/MODEST")

![](https://i-blog.csdnimg.cn/direct/bae14d6fd6664520b2991f988d0742de.png)

将算法迁移到真实机器人平台，开展了透明物体抓取实验。实验平台主要由UR机械臂和深度相机组成。

在借助MODEST方法对透明物体进行分割和深度预测，生成点云数据作为输入，进而采用GraspNet生成抓取位姿。

![](https://i-blog.csdnimg.cn/direct/a0986c0800b047689a7684dcf9e14e5d.png)

### 1、创建Conda环境

使用conda创建一个虚拟环境，名字为modest，指定使用python3.8

然后进入modest环境

```bash
conda create -n modest python=3.8
conda activate modest
```

### 2、安装torch和CUDA

需要安装torch==1.10.1+cu111，执行下面命令：

```bash
pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html
```

然后安装其他依赖

```bash
sudo apt-get install openexr libopenexr-dev
```

### 3、安装依赖库requirements.txt

下载MODEST代码到本地，然后解压

打开requirements.txt，注释torch==1.10.1+cu111、torchvision==0.11.2+cu111，因为上面安装了

![](https://i-blog.csdnimg.cn/direct/a8bb2810490d492e914dd075625ee987.png)

然后执行命令，安装依赖库

```bash
pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### 4、准备数据集ClearPose

ClearPose 数据集是使用 RealSense L515 摄像头在室内环境中捕获的，捕获了 63 个透明物体。

它包含 RGB、原始深度、地面真实深度、地面真实表面法线图像以及所有物体实例6D位姿。

代码地址：
[https://github.com/opipari/ClearPose](https://github.com/opipari/ClearPose "https://github.com/opipari/ClearPose")

下载地址：
[点击下载clearpose](https://www.dropbox.com/scl/fo/dzc777gazch9309g4lebn/AHUr5d8z5NLuw8Sc_stV3Nk?rlkey=i0kpvh3792kvrumpsh9c51qim&e=1&dl=0 "点击下载clearpose")

![](https://i-blog.csdnimg.cn/direct/623fe0000db24554902549418aae6600.png)

ClearPose 被分成 9 个集合，其中 Set1 只包含化学透明物体，Set2-7 只包含家居物品，Set8-9 还包含其他对抗因素。

文件夹结构如下：

> ```
> <dataset_path>
> |-- set1
>     |-- scene1
>         |-- metadata.mat            # 
>         |-- 000000-color.png        # RGB image
>         |-- 000000-depth.png        # Raw depth image
>         |-- 000000-depth_true.png   # Ground truth depth image
>         |-- 000000-label.png        #
>         |-- 000000-normal_true.png  #
>         ...
> |-- model
>     |-- <object1>
>         |-- <object1>.obj
>     |-- <object2>
>         |-- <object2>.obj
>         ...
>
> ```

示例数据：

![](https://i-blog.csdnimg.cn/direct/99576c73b62f491f8a4c0232cf2b2301.png)

### 5、下载 **模型权重**

Syn-TODD 数据集上预先训练的模型权重：
[https://drive.google.com/file/d/1haxiir4PdBNE9Zr1AA4D9bVJ4KCzqa8v/view](https://drive.google.com/file/d/1haxiir4PdBNE9Zr1AA4D9bVJ4KCzqa8v/view "https://drive.google.com/file/d/1haxiir4PdBNE9Zr1AA4D9bVJ4KCzqa8v/view")

真实世界数据集 ClearPose 的模型权重：
[https://drive.google.com/file/d/1798AE\_u6KrMV6mpUGBxz\_jaLrg\_21A39/view](https://drive.google.com/file/d/1798AE_u6KrMV6mpUGBxz_jaLrg_21A39/view "https://drive.google.com/file/d/1798AE_u6KrMV6mpUGBxz_jaLrg_21A39/view")

然后创建文件夹ckpt，放到里面：

![](https://i-blog.csdnimg.cn/direct/2fda87f5a22a41f79345444dd8cb01e8.png)

### 6、进行推理

首先配置文件：config/config.json，指定预训练权重ISGNet\_clearpose.p

![](https://i-blog.csdnimg.cn/direct/2b9610fe7a6b42fc998d7e84ce9d833d.png)

使用CPU运行，"device":"cpu"；如果使用GPU，"device":"cuda"

![](https://i-blog.csdnimg.cn/direct/2b3889d4be9b452d8bac540acc559689.png)

然后在推理代码inference.py中，需要修改图片路径，比如：

image\_path = "./datasets/clearpose\_downsample\_100/set1/scene1/000000-color.png"

推理代码如下

```python
import json
from models.Trainer import Trainer
from utils.visualize import *


image_path = "./datasets/clearpose_downsample_100/set1/scene1/000000-color.png"

################ load the config file ##################
with open('config/config.json', 'r') as f:
    config = json.load(f)

############### load the trainer ###############
trainer = Trainer(config)

############### start inference ##############
trainer.inference(image_path)
```

执行代码：

![](https://i-blog.csdnimg.cn/direct/a28d0895afb54868a04886fb062a28f0.png)

运行结果，在results目录保存了

![](https://i-blog.csdnimg.cn/direct/e42cbdc5d3a942da9fe13aa4e7acd0a8.png)

原图是这样的

![](https://i-blog.csdnimg.cn/direct/b6ad1e52e3134aed8e06e67335150bd6.png)

模型预测的深度图：

![](https://i-blog.csdnimg.cn/direct/f1adb796ae4d4ce69b97a188452ef922.png)

模型预测的分割效果：

![](https://i-blog.csdnimg.cn/direct/b6367ac57f5e48b3b7eabed233358c56.png)

MODEST对透明物体进行分割和深度预测，生成点云数据作为输入，进而采用GraspNet生成抓取位姿。

分享完成～