---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36313336303730312f:61727469636c652f64657461696c732f313436323239313432"
layout: post
title: "神经网络分类任务"
date: 2025-03-13 14:41:31 +0800
description: "每个隐藏层后面都接了一个 ReLU 激活函数，增加了模型的非线性能力。这是一个多层神经网络，包含两个隐藏层和一个输出层。使用了 Dropout 层来防止过拟合。但不知道数据集里面是什么打印。下载mnist数据集。"
keywords: "神经网络分类任务"
categories: ['深度学习']
tags: ['神经网络', '分类']
artid: "146229142"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146229142
    alt: "神经网络分类任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146229142
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146229142
cover: https://bing.ee123.net/img/rand?artid=146229142
image: https://bing.ee123.net/img/rand?artid=146229142
img: https://bing.ee123.net/img/rand?artid=146229142
---

# 神经网络分类任务

```
import torch
%matplotlib inline
from pathlib import Path
import requests

import torchvision
mnist_dataset = torchvision.datasets.MNIST(root='./data', download=True)
```

下载mnist数据集

但不知道数据集里面是什么打印

```
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# 下载并加载 MNIST 数据集
transform = transforms.Compose([transforms.ToTensor()])  # 将图像转换为张量
mnist_dataset = torchvision.datasets.MNIST(root='./data', download=True, transform=transform)

# 获取一个数据样本
image, label = mnist_dataset[0]  # 获取第一个样本

# 打印图像和标签
print(f"Label: {label}")
plt.imshow(image.squeeze(), cmap='gray')  # 显示图像
plt.title(f"Label: {label}")
plt.show()
```

![](https://i-blog.csdnimg.cn/direct/1aefe0430db84ad8be3f372ab4b9a6d5.png)

```
image.shape#图片的大小
```

```
torch.Size([1, 28, 28])

```

```
train_data = torch.stack([mnist_dataset[i][0] for i in range(64)])  # 图像数据
train_labels = torch.tensor([mnist_dataset[i][1] for i in range(64)])  # 标签数据

# 定义模型参数
num_inputs = 28 * 28  # 输入特征数量（28x28图像）
num_outputs = 10  # 输出类别数量（0-9）

# 初始化权重和偏置
weights = torch.randn(num_inputs, num_outputs, requires_grad=True)  # 随机初始化权重
bias = torch.zeros(num_outputs, requires_grad=True)  # 初始化偏置为0

# 定义模型
def model(xb):
    return xb.view(xb.size(0), -1).mm(weights) + bias  # 将输入展平为二维张量后进行线性变换

# 定义损失函数
loss_func = F.cross_entropy
```

```
# 训练模型
num_epochs = 10  # 训练轮数
for epoch in range(num_epochs):
    # 前向传播
    preds = model(train_data)  # 模型预测
    loss = loss_func(preds, train_labels)  # 计算损失

    # 反向传播和优化
    optimizer.zero_grad()  # 清空之前的梯度
    loss.backward()  # 反向传播计算梯度
    optimizer.step()  # 更新参数

    # 打印损失
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}")

print("Training complete.")
```

![](https://i-blog.csdnimg.cn/direct/2b4f59d9a05946b39abac43bba9f5a96.png)

这是单层

```
from torch import nn
import torch.nn.functional as F

class Mnist_NN(nn.Module):
    def __init__(self):
        super(Mnist_NN, self).__init__()  # 调用父类的构造函数
        self.hidden1 = nn.Linear(784, 128)  # 第一层：输入 784，输出 128
        self.hidden2 = nn.Linear(128, 256)  # 第二层：输入 128，输出 256
        self.out = nn.Linear(256, 10)       # 输出层：输入 256，输出 10
        self.dropout = nn.Dropout(0.5)      # Dropout 层，丢弃概率为 0.5

    def forward(self, x):
        x = F.relu(self.hidden1(x))         # 第一层 + ReLU 激活
        x = self.dropout(x)                 # 应用 Dropout
        x = F.relu(self.hidden2(x))         # 第二层 + ReLU 激活
        x = self.dropout(x)                 # 应用 Dropout
        x = self.out(x)                     # 输出层
        return x
```

```
# 初始化模型
model = Mnist_NN()

# 定义损失函数和优化器
loss_func = F.cross_entropy
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 使用模型的所有参数

# 训练模型
num_epochs = 10  # 训练轮数
for epoch in range(num_epochs):
    # 前向传播
    preds = model(train_data.view(-1, 784))  # 将输入展平为 [batch_size, 784]
    loss = loss_func(preds, train_labels)    # 计算损失

    # 反向传播和优化
    optimizer.zero_grad()  # 清空之前的梯度
    loss.backward()        # 反向传播计算梯度
    optimizer.step()       # 更新参数

    # 打印损失
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}")

print("Training complete.")
```

![](https://i-blog.csdnimg.cn/direct/b6460a71896d46e0b7e06dcaacb940e5.png)

* 这是一个多层神经网络，包含两个隐藏层和一个输出层。
* 每个隐藏层后面都接了一个 ReLU 激活函数，增加了模型的非线性能力。
* 使用了 Dropout 层来防止过拟合。

刚开始学，后续再看