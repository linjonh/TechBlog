---
layout: post
title: "动手实验TCP-orphan-socket-的产生与消亡"
date: 2025-03-07 10:40:47 +08:00
description: "没有在 Wiki 中找到关于孤儿 socket 的定义，在 StackOverflow 中找到了一个比较通俗易懂的解释：下面一段是 DeepSeek 给出的解释：总的来说就是当程序调用 close() 函数关闭 socket 后，socket 就和应用程序无关了，但此时 TCP 终止流程还没有走完，所以内核里还有这些 socket 的数据，这些 socket 就是所谓的孤儿 socket。下面我们结合源码和实验分析下哪些情况下的 socket 会被视为孤儿 socket。可以用ss -s或者。"
keywords: "【动手实验】TCP orphan socket 的产生与消亡"
categories: ['未分类']
tags: ['网络协议', '网络', 'Tcp']
artid: "146089453"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146089453
    alt: "动手实验TCP-orphan-socket-的产生与消亡"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146089453
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146089453
cover: https://bing.ee123.net/img/rand?artid=146089453
image: https://bing.ee123.net/img/rand?artid=146089453
img: https://bing.ee123.net/img/rand?artid=146089453
---

# 【动手实验】TCP orphan socket 的产生与消亡
之前在实验中提到了 tcp\_max\_orphans 和 tcp\_orphan\_retries 两个参数，我们使用 `ss -s` 命令查看当前系统中的
socket 状态也有 orphan 状态的 socket，本篇文章我们就来分析下到底什么情况下的 socket 才会被视为 orphan socket。
# ubuntu @ node1 in ~ [12:07:15]
$ ss -s
Total: 184
TCP: 14 (estab 5, closed 1, orphaned 0, timewait 1)
### 什么是孤儿(Orphan) socket？
没有在 Wiki 中找到关于孤儿 socket 的定义，在 StackOverflow 中找到了一个比较通俗易懂的解释：
> Do you have “too many” orphan sockets?
>
> First of all: what’s an orphan socket? It’s simply a socket that isn’t
> associated to a file descriptor. For instance, after you close() a socket,
> you no longer hold a file descriptor to reference it, but it still exists
> because the kernel has to keep it around for a bit more until TCP is done
> with it. Because orphan sockets aren’t very useful to applications (since
> applications can’t interact with them), the kernel is trying to limit the
> amount of memory consumed by orphans, and it does so by limiting the number
> of orphans that stick around. If you’re running a frontend web server (or an
> HTTP load balancer), then you’ll most likely have a sizeable number of
> orphans, and that’s perfectly normal.
下面一段是 DeepSeek 给出的解释：
> Orphan Sockets refer to TCP sockets that no longer have an associated user
> process but remain in the operating system’s kernel network stack. These
> sockets persist because the owning process terminated (e.g., crashed or
> exited abruptly) without properly closing the socket, leaving the kernel to
> manage cleanup
总的来说就是当程序调用 close() 函数关闭 socket 后，socket 就和应用程序无关了，但此时 TCP 终止流程还没有走完，所以内核里还有这些
socket 的数据，这些 socket 就是所谓的孤儿 socket。下面我们结合源码和实验分析下哪些情况下的 socket 会被视为孤儿 socket。
### Orphan socket 的产生
上面提到内核是在程序调用 close() 函数后的状态视为 orphan socket，因此内核中 orphan socket 的产生主要是在
`tcp\_close` 函数中，其内部调用 `\_\_tcp\_close` 函数执行主要逻辑：我们来分析下。
// https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2910
void tcp\_close(struct sock *sk, long timeout)
{
lock\_sock(sk);
\_\_tcp\_close(sk, timeout);
release\_sock(sk);
sock\_put(sk);
}
void \_\_tcp\_close(struct sock *sk, long timeout)
{
struct sk\_buff *skb;
int data\_was\_unread = 0;
int state;
WRITE\_ONCE(sk->sk\_shutdown, SHUTDOWN\_MASK);
// 处于 listen 状态，直接进入 TCP\_CLOSE 状态
if (sk->sk\_state == TCP\_LISTEN) {
tcp\_set\_state(sk, TCP\_CLOSE);
/* Special case. */
inet\_csk\_listen\_stop(sk);
goto adjudge\_to\_death;
}
// ... 代码省略
}
可以看到，在 \_\_tcp\_close 函数中，有一个 `adjudge\_to\_death`的 goto 标签，这部分代码执行了**将 socket 变为
orphan 状态的逻辑，并增加 orphan socket 计数** ，然后执行一系列状态变迁逻辑后，如果 socket 处于 TCP\_CLOSE
状态，则清理掉 socket 并减少 orphan socket 计数。
* 执行清理，增加 orphan socket 计数
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2828
adjudge\_to\_death:
state = sk->sk\_state;
sock\_hold(sk);
// 执行清理，将 socket 变为 orphan 状态
sock\_orphan(sk);
local\_bh\_disable();
bh\_lock\_sock(sk);
/* remove backlog if any, without releasing ownership. */
\_\_release\_sock(sk);
// 增加 orphan socket 计数
this\_cpu\_inc(tcp\_orphan\_count);
// ...代码省略：状态变迁逻辑
* 具体的清理任务
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/include/net/sock.h#L1992
/* Detach socket from process context.
* Announce socket dead, detach it from wait queue and inode.
* Note that parent inode held reference count on this struct sock,
* we do not release it in this function, because protocol
* probably wants some additional cleanups or even continuing
* to work with this socket (TCP).
*/
static inline void sock\_orphan(struct sock *sk)
{
write\_lock\_bh(&sk->sk\_callback\_lock);
// 设置 SOCK\_DEAD 标志，标记 socket 已死亡，不在于任何进程有关系
sock\_set\_flag(sk, SOCK\_DEAD);
// 设置 socket 为 NULL，不在于文件描述符有关系
sk\_set\_socket(sk, NULL);
// 清空等待队列
sk->sk\_wq = NULL;
write\_unlock\_bh(&sk->sk\_callback\_lock);
}
* 减少 orphan socket 计数
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2890
void \_\_tcp\_close(struct sock *sk, long timeout)
{
// ... 代码省略
// 如果 socket 处于 TCP\_CLOSE 状态，则执行清理
if (sk->sk\_state == TCP\_CLOSE) {
// ... 代码省略
// 销毁 socket，减少 orphan socket 计数
inet\_csk\_destroy\_sock(sk);
}
}
// 具体的销毁任务
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/inet\_connection\_sock.c#L1005
void inet\_csk\_destroy\_sock(struct sock *sk)
{
// ... 代码省略
// 减少 orphan socket 计数
this\_cpu\_dec(*sk->sk\_prot->orphan\_count);
sock\_put(sk);
}
由此我们知道，内核在执行 close() 时会先将 socket 视为 orphan 状态，增加计数，执行完 TCP 状态转换后，如果 socket 进入
CLOSE 状态，那么 orphan socket 计数会减少，也就是说此时不会有 orphan socket
了。我们在来分析下具体的状态转换逻辑，看哪些情况下 socket 会进入 CLOSE 状态。
1. TCP 主动关闭连接。如果 socket 处于 TCP\_LISTEN 状态，则直接进入 TCP\_CLOSE 状态，然后执行 adjudge\_to\_death 标签，增加 orphan socket 计数，但最终会走到 `inet\_csk\_destroy\_sock` 函数，减少 orphan socket 计数，并销毁 socket。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2747
if (sk->sk\_state == TCP\_LISTEN) {
// 设置 socket 为 TCP\_CLOSE 状态
tcp\_set\_state(sk, TCP\_CLOSE);
/* Special case. */
inet\_csk\_listen\_stop(sk);
goto adjudge\_to\_death;
}
2. 如果接收队列中存在未被应用读取的数据，则进入 TCP\_CLOSE 状态，并向对端发送 RST 报文，然后销毁 socket。正常情况下，应用应该会在处理完数据后调用 close()，所以如果调用 close() 时还没有处理完数据，会被视为异常关闭，比如在应用异常崩溃、网络中断时会出现这种情况，因此会直接发送 RST 报文，并销毁 socket。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2784
else if (data\_was\_unread) {
/* Unread data was tossed, zap the connection. */
NET\_INC\_STATS(sock\_net(sk), LINUX\_MIB\_TCPABORTONCLOSE);
// 设置 socket 为 TCP\_CLOSE 状态
tcp\_set\_state(sk, TCP\_CLOSE);
tcp\_send\_active\_reset(sk, sk->sk\_allocation);
3. 系统开启了 so\_linger 并设置为0，表示 0 延迟关闭 ，socket 会丢弃数据并发送 RST 报文，然后进入 TCP\_CLOSE 状态，并销毁 socket。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2789
else if (sock\_flag(sk, SOCK\_LINGER) && !sk->sk\_lingertime) {
/* Check zero linger \_after\_ checking for unread data. */
sk->sk\_prot->disconnect(sk, 0);
NET\_INC\_STATS(sock\_net(sk), LINUX\_MIB\_TCPABORTONDATA);
}
disconnect 调用会执行 `tcp\_disconnect` 函数，该函数会发送 RST 报文，并设置 socket 为 TCP\_CLOSE
状态，然后销毁 socket。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2967
int tcp\_disconnect(struct sock *sk, int flags)
{
struct inet\_sock *inet = inet\_sk(sk);
struct inet\_connection\_sock *icsk = inet\_csk(sk);
struct tcp\_sock *tp = tcp\_sk(sk);
int old\_state = sk->sk\_state;
u32 seq;
/* Deny disconnect if other threads are blocked in sk\_wait\_event()
* or inet\_wait\_for\_connect().
*/
if (sk->sk\_wait\_pending)
return -EBUSY;
// 设置 socket 为 TCP\_CLOSE 状态
if (old\_state != TCP\_CLOSE)
tcp\_set\_state(sk, TCP\_CLOSE);
除了以上 3 种情况，后面就是正常的 TCP
状态转换逻辑了，代码从状态机映射中，获取到下个状态值做状态转换，我们最常看到的关闭时的状态转换就是在这里执行的，可以结合图例加深理解。
![](https://i-blog.csdnimg.cn/img\_convert/80261e68f36acbf45272bda0d343a013.png)
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2793
else if (tcp\_close\_state(sk)) {
// 完成状态转换后，发送 FIN 报文
tcp\_send\_fin(sk);
}
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2664
static const unsigned char new\_state[16] = {
/* current state: new state: action: */
[0 /* (Invalid) */] = TCP\_CLOSE,
[TCP\_ESTABLISHED] = TCP\_FIN\_WAIT1 | TCP\_ACTION\_FIN,
[TCP\_SYN\_SENT] = TCP\_CLOSE,
[TCP\_SYN\_RECV] = TCP\_FIN\_WAIT1 | TCP\_ACTION\_FIN,
[TCP\_FIN\_WAIT1] = TCP\_FIN\_WAIT1,
[TCP\_FIN\_WAIT2] = TCP\_FIN\_WAIT2,
[TCP\_TIME\_WAIT] = TCP\_CLOSE,
[TCP\_CLOSE] = TCP\_CLOSE,
[TCP\_CLOSE\_WAIT] = TCP\_LAST\_ACK | TCP\_ACTION\_FIN,
[TCP\_LAST\_ACK] = TCP\_LAST\_ACK,
[TCP\_LISTEN] = TCP\_CLOSE,
[TCP\_CLOSING] = TCP\_CLOSING,
[TCP\_NEW\_SYN\_RECV] = TCP\_CLOSE, /* should not happen ! */
};
static int tcp\_close\_state(struct sock *sk)
{
int next = (int)new\_state[sk->sk\_state];
int ns = next & TCP\_STATE\_MASK;
tcp\_set\_state(sk, ns);
return next & TCP\_ACTION\_FIN;
}
结合状态变迁和后续的代码执行，我们知道最终 socket 的状态只会是
CLOSE，FIN\_WAIT1、FIN\_WAIT\_2、CLOSING、LAST\_ACK中的一种。除了 CLOSE 状态，其他状态下都不会执行
`inet\_csk\_destroy\_sock` 函数，也就不会减少 orphan socket 计数，因此这些状态的 socket 都会被视为 orphan
socket。CLOSING 状态的构造相对麻烦，这里我们做实验看下 socket 在 FIN\_WAIT\_1，LAST\_ACK 状态时 orphan
socket 的计数变化。FIN\_WAIT\_2 状态的处理有一些特殊，我们在下一节做更详细的探讨。
首先服务端和客户端查看 socket 统计信息，orphan 计数为 0。
$ ss -s
Total: 176
TCP: 15 (estab 5, closed 5, orphaned 0, timewait 5)
#### 1\. FIN\_WAIT\_1 状态
这里用 Go 程序做服务端和客户端，代码如下：
* 服务端程序
package main
import (
"log"
"net"
"time"
)
func main() {
l, err := net.Listen("tcp", ":8888")
if err != nil {
log.Printf("failed to listen due to %v", err)
}
defer l.Close()
log.Println("listen :8888 success")
for {
time.Sleep(time.Second * 100)
}
}
* 客户端程序
package main
import (
"context"
"log"
"net"
"os"
"os/signal"
"sync"
"syscall"
"time"
)
var wg sync.WaitGroup
func establishConn(ctx context.Context, i int) {
defer wg.Done()
conn, err := net.DialTimeout("tcp", "172.19.0.12:8888", time.Second*50)
if err != nil {
log.Printf("%d, dial error: %v", i, err)
return
}
log.Printf("%d, dial success", i)
\_, err = conn.Write([]byte("hello world"))
if err != nil {
log.Printf("%d, send error: %v", i, err)
return
}
select {
case <-ctx.Done():
log.Printf("%d, dail close", i)
}
}
func main() {
ctx, cancel := context.WithCancel(context.Background())
// 并发请求 10 次服务端，连接建立成功后发送数据
for i := 0; i < 10; i++ {
wg.Add(1)
time.Sleep(1 * time.Millisecond)
go establishConn(ctx, i)
}
go func() {
sc := make(chan os.Signal, 1)
signal.Notify(sc, syscall.SIGINT)
select {
case <-sc:
cancel()
}
}()
wg.Wait()
log.Printf("client exit")
}
我们使用 iptables DROP 掉服务端返回的 ACK 包，使客户端处于 FIN\_WAIT\_1 状态，然后查看 socket 统计信息，预期
orphan 计数为 10。
服务端 DROP 客户端的 FIN 包
$ sudo iptables -A INPUT -p tcp --dport 8888 --tcp-flags FIN FIN -j DROP
我们运行客户端命令后按下 ctrl+c 中断客户端程序，查看 socket 统计信息，orphan 计数为 10，符合预期。
$ ss -ant | grep -E "Recv|8888"
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process
FIN-WAIT-1 0 1 172.19.0.15:48344 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48290 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48300 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48322 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48276 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48302 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48330 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48288 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48310 172.19.0.12:8888
FIN-WAIT-1 0 1 172.19.0.15:48334 172.19.0.12:8888
# ubuntu @ node3 in ~ [18:03:09]
$ ss -s
Total: 209
TCP: 30 (estab 10, closed 4, orphaned 10, timewait 4)
#### 2\. LAST\_ACK 状态
这里我用 nc 启动服务端，然后在客户端发起两次连接:
# 服务端
$ nc -k -l 172.19.0.12 8888
# 客户端，开两个终端发起两次
$ nc 172.19.0.12 8888
因为 nc 没有数据需要处理，所以服务端收到客户端的 FIN 包后会将 ACK-FIN 包一起发，然后进入 LAST\_ACK 状态，我们在客户端设置
iptables 规则将包丢弃，这样客户端就不会收到 ACK-FIN 包，服务端就会一直处于 LAST\_ACK 状态。
# DROP 源端口为 8888 的 ACK-FIN 包
sudo iptables -A INPUT -p tcp --sport 8888 --tcp-flags ACK,FIN ACK,FIN -j DROP
设置完成后客户端按下 ctrl+c 中断连接，查看服务端的 socket 统计信息，可以看到有 2 个 LAST\_ACK 状态的 socket，orphan
计数为 2，符合预期。
$ ss -ant | grep -E "Recv|:8888" && ss -s
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process
LISTEN 0 1 172.19.0.12:8888 0.0.0.0:*
LAST-ACK 0 1 172.19.0.12:8888 172.19.0.15:48566
LAST-ACK 0 1 172.19.0.12:8888 172.19.0.15:36574
Total: 212
TCP: 21 (estab 10, closed 2, orphaned 2, timewait 2)
### Orphan socket 的消亡
知道了哪些状态的 socket 会被视为 orphan socket，我们就可以分析下这些 socket 是如何消失的。
#### FIN-WAIT-2 下的 socket 处理
我们上面分析了 FIN\_WAIT\_2 状态下的 socket 应该也算是 orphan，但如果执行正常连接关闭操作时，查看 `ss -s` 统计会发现
orphan 为 0。下面用实验验证下：
* 服务端使用 nc 监听 8888，客户端发起三个连接
# 服务端
$ nc -k -l 172.19.0.12 8888
# 客户端，开三个窗口连接服务端
$ nc 172.19.0.12 8888
连接建立成功后，使用 iptables 将服务端的 FIN 包 drop 点然后 ctrl + c 终止客户端连接，查看统计。
$ sudo iptables -A INPUT -p tcp --sport 8888 --tcp-flags FIN FIN -j DROP
可以看到客户端有 3 条 FIN-WAIT-2 的 socket，但 orphaned 计数为 0。
$ ss -ant | grep -E "Recv|:8888" && ss -s
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process
ESTAB 0 0 172.19.0.15:42190 172.19.0.12:8888
ESTAB 0 0 172.19.0.15:42174 172.19.0.12:8888
ESTAB 0 0 172.19.0.15:41654 172.19.0.12:8888
Total: 222
TCP: 27 (estab 15, closed 6, orphaned 0, timewait 6)
Transport Total IP IPv6
RAW 1 0 1
UDP 9 7 2
TCP 21 20 1
INET 31 27 4
FRAG 0 0 0
# ubuntu @ node3 in ~ [15:46:37]
$ ss -ant | grep -E "Recv|:8888" && ss -s
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process
FIN-WAIT-2 0 0 172.19.0.15:42190 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:42174 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:41654 172.19.0.12:8888
Total: 219
TCP: 29 (estab 12, closed 10, orphaned 0, timewait 10)
我们结合源码来分析下内核对 FIN-WAIT-2 状态 socket 的处理。
##### 1\. FIN-WAIT-2 状态下的 0 延迟关闭，不计入 orphan
当 socket 处于 FIN-WAIT-2 状态并且设置了零延迟关闭时，会直接将 socket 设置为 TCP\_CLOSE 状态，并发送 RST
报文，然后销毁 socket，最终减少 orphan socket 计数。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2858
static void tcp\_close(struct sock *sk, long timeout)
if (sk->sk\_state == TCP\_FIN\_WAIT2) {
struct tcp\_sock *tp = tcp\_sk(sk);
// 如果 linger2 小于 0，则设置 socket 为 TCP\_CLOSE 状态，并发送 RST 报文
if (tp->linger2 < 0) {
tcp\_set\_state(sk, TCP\_CLOSE);
tcp\_send\_active\_reset(sk, GFP\_ATOMIC);
\_\_NET\_INC\_STATS(sock\_net(sk),
LINUX\_MIB\_TCPABORTONLINGER);
} else {
// 计算 超时时间
const int tmo = tcp\_fin\_time(sk);
if (tmo > TCP\_TIMEWAIT\_LEN) {
inet\_csk\_reset\_keepalive\_timer(sk,
tmo - TCP\_TIMEWAIT\_LEN);
} else {
tcp\_time\_wait(sk, TCP\_FIN\_WAIT2, tmo);
goto out;
}
}
}
关于零延迟关闭这里多提一点，这个设置指的是当应用调用 close 函数时，如果应用还有数据没有发送完成，则需要等待一定的延迟时间，如果设置了为 0
，socket 的数据就会被丢弃
掉并向对方发送一个 RST 包来中断连接，因此对于服务端来说最好永远不要设置为
0；但对于探活类的程序则是一种非常好的配置。具体可以参考皓哥的文章[从一次经历谈 TIME\_WAIT
的那些事](https://coolshell.cn/articles/22263.html)。
##### 2\. FIN-WAIT-2 下等待超时，启用 keepalive，计入 orphan
如果没有满足 0 延迟关闭的条件，内核转而会计算 socket 的超时时间，如果大于 60s 则启用 keepalive 定时器，注意这里是没有将
socket 状态设置为 CLOSE 的，因此可以认为超时时间大于 60s 的 FIN-WAIT-2 状态的 socket 被计入 orphan。
否则会执行 tcp\_time\_wait 代码将 socket 视为 timewait 状态。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2866
const int tmo = tcp\_fin\_time(sk);
if (tmo > TCP\_TIMEWAIT\_LEN) {
inet\_csk\_reset\_keepalive\_timer(sk,
tmo - TCP\_TIMEWAIT\_LEN);
} else {
tcp\_time\_wait(sk, TCP\_FIN\_WAIT2, tmo);
goto out;
}
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/include/net/tcp.h#L123
#define TCP\_TIMEWAIT\_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT
* state, about 60 seconds */
这里的超时计算是根据 tcp\_fin\_timeout 来的，默认是 60s，我们把这个值改大然后在客户端屏蔽掉服务端的 FIN 包后，在查看客户端 FIN-
WAIT-2 状态的 socket 就已经被计入 orphan socket了。如果没有设置，则会被计入 timewait 状态。
# 没有修改时会被认为是 timewait 计数
$ ss -ant | grep -E "Recv|:8888" && ss -s
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process
FIN-WAIT-2 0 0 172.19.0.15:48172 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48116 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48130 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48220 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48206 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48194 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48162 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48182 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48126 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:48146 172.19.0.12:8888
Total: 221
TCP: 41 (estab 14, closed 16, orphaned 0, timewait 16)
# 客户端修改 tcp\_fin\_timeout
sudo sysctl -w net.ipv4.tcp\_fin\_timeout=120
# 查看客户端统计信息
$ ss -ant | grep -E "Recv|:8888" && ss -s
State Recv-Q Send-Q Local Address:Port Peer Address:Port Process
FIN-WAIT-2 0 0 172.19.0.15:59016 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58996 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58966 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58936 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58956 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:59030 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58940 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58984 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:59012 172.19.0.12:8888
FIN-WAIT-2 0 0 172.19.0.15:58982 172.19.0.12:8888
Total: 243
TCP: 37 (estab 14, closed 5, orphaned 11, timewait 5)
Transport Total IP IPv6
RAW 1 0 1
UDP 9 7 2
TCP 32 31 1
INET 42 38 4
FRAG 0 0 0
# ubuntu @ node3 in ~ [17:33:06]
$ sudo netstat -anpo | grep -E "Recv-Q|8888"
Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name Timer
tcp 0 0 172.19.0.15:59016 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58996 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58966 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58936 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58956 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:59030 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58940 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58984 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:59012 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
tcp 0 0 172.19.0.15:58982 172.19.0.12:8888 FIN\_WAIT2 - keepalive (13.92/0/0)
可以看到每个 FIN\_WAIT\_2 状态的 socket 都有一个 keeplive 定时器，并且 orphan 的统计数发生了变化。
#### 2\. orphan socket 过多清理资源
完成了 FIN-FIN\_WAIT\_2 状态的判断后，如果 Socket 状态还不是 CLOSE，内核会判断当前 orphan 的 socket
数量是否过多，如果过多的话会执行资源清理。内核会根据 `net.ipv4.tcp\_max\_orphans`
参数判断 orphan socket 的数量是否过多。计算的时候内核会传一个 shift 参数，取值范围是 0~2，因此内核实际可能会拿当前 orphan
数量的 2 倍甚至 4 倍去做比较。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2877
if (sk->sk\_state != TCP\_CLOSE) {
sk\_mem\_reclaim(sk);
if (tcp\_check\_oom(sk, 0)) {
tcp\_set\_state(sk, TCP\_CLOSE);
tcp\_send\_active\_reset(sk, GFP\_ATOMIC);
\_\_NET\_INC\_STATS(sock\_net(sk),
LINUX\_MIB\_TCPABORTONMEMORY);
} else if (!check\_net(sock\_net(sk))) {
/* Not possible to send reset; just close */
tcp\_set\_state(sk, TCP\_CLOSE);
}
}
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp.c#L2719
static bool tcp\_too\_many\_orphans(int shift)
{
return READ\_ONCE(tcp\_orphan\_cache) << shift >
READ\_ONCE(sysctl\_tcp\_max\_orphans);
}
#### 3\. 定时器执行资源清理
系统有两个参数和 orphan 有关：
* `net.ipv4.tcp\_max\_orphans`：上面提到的 orphan socket 的最大数量。
* `net.ipv4.tcp\_orphan\_retries`：该字段控制 FIN 包的最大重传字数，设置默认为 0。如果为 0，内核会重置为 8。
当内核处于 FIN\_WAIT 状态时，会有定时器执行包的重传或者探活，此时也会执行上面提到的资源清理操作。此时如果重传次数已到，或者 orphan
socket 数量过多，也会执行清理操作。
// 源码地址：https://elixir.bootlin.com/linux/v5.15.130/source/net/ipv4/tcp\_timer.c#L231
static int tcp\_write\_timeout(struct sock *sk)
{
// 代码省略
if (sock\_flag(sk, SOCK\_DEAD)) {
const bool alive = icsk->icsk\_rto < TCP\_RTO\_MAX;
// 基于重试次数判断是否 reset
retry\_until = tcp\_orphan\_retries(sk, alive);
do\_reset = alive ||
!retransmits\_timed\_out(sk, retry\_until, 0);
// 判断是否需要释放资源
if (tcp\_out\_of\_resources(sk, do\_reset))
return 1;
}
}
static int tcp\_out\_of\_resources(struct sock *sk, bool do\_reset)
{
struct tcp\_sock *tp = tcp\_sk(sk);
int shift = 0;
/* If peer does not open window for long time, or did not transmit
* anything for long time, penalize it. */
if ((s32)(tcp\_jiffies32 - tp->lsndtime) > 2*TCP\_RTO\_MAX || !do\_reset)
shift++;
/* If some dubious ICMP arrived, penalize even more. */
if (sk->sk\_err\_soft)
shift++;
// 计算移位（0 - 2），判断 orphan socket 是否过多
if (tcp\_check\_oom(sk, shift)) {
/* Catch exceptional cases, when connection requires reset.
* 1. Last segment was sent recently. */
if ((s32)(tcp\_jiffies32 - tp->lsndtime) <= TCP\_TIMEWAIT\_LEN ||
/* 2. Window is closed. */
(!tp->snd\_wnd && !tp->packets\_out))
do\_reset = true;
if (do\_reset)
tcp\_send\_active\_reset(sk, GFP\_ATOMIC);
tcp\_done(sk);
\_\_NET\_INC\_STATS(sock\_net(sk), LINUX\_MIB\_TCPABORTONMEMORY);
return 1;
}
}
### 简要总结
1. 可以用 `ss -s` 或者 `cat /proc/net/sockstat` 查看孤儿套接字是否过多。
2. 根据[文档](https://sysctl-explorer.net/net/ipv4/tcp\_max\_orphans/) 介绍，每个 orphan socket 大约占用 64KB 内存。
3. FIN\_WAIT\_1、LAST\_ACK、CLOSING 状态下的 socket 都可以归类为 orphan socket。
4. 超时时间小于 60s 的 FIN\_WAIT\_2 计入 timewait，而大约 60s 的则记为 orphan socket。
5. 相关参数：net.ipv4.tcp\_orphan\_retries、net.ipv4.tcp\_max\_orphans。
6. LAST\_ACK、FIN\_WAIT\_1 状态下的连接都在等待回复，因此可以像发起 SYN Flood 那样的攻击，客户端故意断开但不在返回 ACK 信息，这样会导致服务器资源被占用。因此可以适当调整
上面两个参数，减少重传次数以及 orphan 最大数量，从而可以更快的清理掉无用的 socket。
### 参考资料
* [结合案例深入解析orphan socket产生与消亡（一）](https://developer.aliyun.com/article/91966)
* [结合案例深入解析orphan socket产生与消亡（二）](https://developer.aliyun.com/article/92925)