---
arturl_encode: "6874747073:3a2f2f626c6f672e6373646e2e6e65742f4252454b4f4a492f:61727469636c652f64657461696c732f313436303636303836"
layout: post
title: "机器学习深度学习基本概念logistic-regression和softmax"
date: 2025-03-06 12:38:17 +08:00
description: "softmax用来处理多分类问题：比如llm在generate的时候，每个batch里面的一个样本的一个一次generate就是softmax生成一个大小为vocab_size的向量的概率分布，然后再采样。当W·x趋近于负无穷时sigmoid输出接近于0，当趋近于正无穷时，接近于1，来生成分类预测的概率。逻辑回归（logistic regression）的核心：sigmoid函数。softmax函数输出每个类别的概率，概率总和为1。逻辑回归用来处理二分类问题。对数损失作为损失函数。"
keywords: "机器学习深度学习基本概念：logistic regression和softmax"
categories: ['未分类']
tags: ['人工智能']
artid: "146066086"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146066086
    alt: "机器学习深度学习基本概念logistic-regression和softmax"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146066086
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146066086
cover: https://bing.ee123.net/img/rand?artid=146066086
image: https://bing.ee123.net/img/rand?artid=146066086
img: https://bing.ee123.net/img/rand?artid=146066086
---

# 机器学习深度学习基本概念：logistic regression和softmax

逻辑回归用来处理二分类问题

softmax用来处理多分类问题：比如llm在generate的时候，每个batch里面的一个样本的一个一次generate就是softmax生成一个大小为vocab\_size的向量的概率分布，然后再采样

逻辑回归（logistic regression）的核心：sigmoid函数

![](https://i-blog.csdnimg.cn/direct/d27bb5109c36469d807f74120bb4d088.png)

![](https://i-blog.csdnimg.cn/direct/159202b171194c59957998f77ebfb1a4.png)

当W·x趋近于负无穷时sigmoid输出接近于0，当趋近于正无穷时，接近于1，来生成分类预测的概率

![](https://i-blog.csdnimg.cn/direct/eb22360e26414f30aa2f8a23442251b8.png)

损失函数：

![](https://i-blog.csdnimg.cn/direct/847b9b74b52c484dbd72d5e5c42f2c7c.png)

对数损失作为损失函数

softmax：

![](https://i-blog.csdnimg.cn/direct/33a54d25aaf446d8abd60e088cc057eb.png)

softmax函数输出每个类别的概率，概率总和为1

损失函数：

![](https://i-blog.csdnimg.cn/direct/9fe33087c83f4b9d800b679097534ee8.png)

**log-softmax**

![](https://i-blog.csdnimg.cn/direct/9b9e20fa31434017b2a76cbedce3eaf8.png)