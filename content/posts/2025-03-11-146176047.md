---
layout: post
title: "仅仅使用pytorch来手撕transformer架构3编码器模块和编码器类的实现和向前传播"
date: 2025-03-11 13:13:32 +0800
description: "往期文章：仅仅使用pytorch来手撕transformer架构(1)：位置编码的类的实现和向前传播最适合小白入门的Transformer介绍仅仅使用pytorch来手撕transformer架构(2)：多头注意力MultiHeadAttention类的实现和向前传播1.编码器模块的实现这段代码实现了一个Transformer编码器模块（Transformer Block），它是Transformer架构的核心组件之一。Transformer架构是一种基于自注意力机制（Self-Attention）的"
keywords: "仅仅使用pytorch来手撕transformer架构(3)：编码器模块和编码器类的实现和向前传播"
categories: ['手撕系列', 'Transformer']
tags: ['深度学习', '架构', '机器学习', '人工智能', 'Transformer', 'Pytorch', 'Python']
artid: "146176047"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146176047
    alt: "仅仅使用pytorch来手撕transformer架构3编码器模块和编码器类的实现和向前传播"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146176047
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146176047
cover: https://bing.ee123.net/img/rand?artid=146176047
image: https://bing.ee123.net/img/rand?artid=146176047
img: https://bing.ee123.net/img/rand?artid=146176047
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     仅仅使用pytorch来手撕transformer架构(3)：编码器模块和编码器类的实现和向前传播
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="pytorchtransformer2_0">
     </a>
     仅仅使用pytorch来手撕transformer架构(2)：编码器模块和编码器类的实现和向前传播
    </h2>
    <p>
     往期文章：
     <br/>
     <a href="https://blog.csdn.net/2302_80236633/article/details/146138782?spm=1001.2014.3001.5501">
      仅仅使用pytorch来手撕transformer架构(1)：位置编码的类的实现和向前传播
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/2302_80236633/article/details/145813364?spm=1001.2014.3001.5501">
      最适合小白入门的Transformer介绍
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/2302_80236633/article/details/146151270?spm=1011.2124.3001.6209">
      仅仅使用pytorch来手撕transformer架构(2)：多头注意力MultiHeadAttention类的实现和向前传播
     </a>
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Transformer 编码器模块</span>
<span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> forward_expansion<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> forward_expansion <span class="token operator">*</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>forward_expansion <span class="token operator">*</span> embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> value<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>value<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>attention <span class="token operator">+</span> query<span class="token punctuation">)</span><span class="token punctuation">)</span>
        forward <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>forward <span class="token operator">+</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out

<span class="token comment"># 编码器</span>
<span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src_vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> device<span class="token punctuation">,</span> forward_expansion<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_size <span class="token operator">=</span> embed_size
        self<span class="token punctuation">.</span>device <span class="token operator">=</span> device
        self<span class="token punctuation">.</span>word_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>src_vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>position_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                TransformerBlock<span class="token punctuation">(</span>
                    embed_size<span class="token punctuation">,</span>
                    heads<span class="token punctuation">,</span>
                    dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span>
                    forward_expansion<span class="token operator">=</span>forward_expansion<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
                <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
            N<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>position_embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

            <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
                x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

            <span class="token keyword">return</span> x


</code></pre>
    <h3>
     <a id="1_67">
     </a>
     1.编码器模块的实现
    </h3>
    <p>
     这段代码实现了一个Transformer编码器模块（Transformer Block），它是Transformer架构的核心组件之一。Transformer架构是一种基于自注意力机制（Self-Attention）的深度学习模型，广泛应用于自然语言处理（NLP）任务，如机器翻译、文本生成等。以下是对代码的详细解释：
    </p>
    <hr/>
    <h4>
     <a id="11__73">
     </a>
     1.1
     <strong>
      类定义
     </strong>
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">TransformerBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre>
    <p>
     <code>
      TransformerBlock
     </code>
     是一个继承自 PyTorch 的
     <code>
      nn.Module
     </code>
     的类，表示一个Transformer编码器模块。
     <code>
      nn.Module
     </code>
     是 PyTorch 中所有神经网络模块的基类，用于定义和管理神经网络的结构。
    </p>
    <hr/>
    <h4>
     <a id="22__81">
     </a>
     2.2
     <strong>
      初始化方法
     </strong>
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> forward_expansion<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>TransformerBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> heads<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> forward_expansion <span class="token operator">*</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>forward_expansion <span class="token operator">*</span> embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_98">
     </a>
     <strong>
      参数解释
     </strong>
    </h5>
    <ul>
     <li>
      <code>
       embed_size
      </code>
      : 嵌入向量的维度，表示每个词或标记（token）的特征维度。
     </li>
     <li>
      <code>
       heads
      </code>
      : 多头注意力机制中的头数（
      <code>
       Multi-Head Attention
      </code>
      ）。
     </li>
     <li>
      <code>
       dropout
      </code>
      : Dropout比率，用于防止过拟合。
     </li>
     <li>
      <code>
       forward_expansion
      </code>
      : 前馈网络（Feed-Forward Network, FFN）中隐藏层的扩展因子。
     </li>
    </ul>
    <h5>
     <a id="_104">
     </a>
     <strong>
      组件解释
     </strong>
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        多头注意力机制（
        <code>
         MultiHeadAttention
        </code>
        ）
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>attention <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> heads<span class="token punctuation">)</span>
</code></pre>
      <p>
       这是Transformer的核心部分，实现了多头注意力机制。它允许模型在不同的表示子空间中学习信息。
       <code>
        MultiHeadAttention
       </code>
       的具体实现没有在这段代码中给出，但通常它会将输入分为多个“头”，分别计算注意力权重，然后将结果拼接起来。
      </p>
     </li>
     <li>
      <p>
       <strong>
        层归一化（
        <code>
         LayerNorm
        </code>
        ）
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>
self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embed_size<span class="token punctuation">)</span>
</code></pre>
      <p>
       层归一化（Layer Normalization）是一种归一化方法，用于稳定训练过程并加速收敛。它对每个样本的特征进行归一化，而不是像批量归一化（Batch Normalization）那样对整个批次进行归一化。
      </p>
     </li>
     <li>
      <p>
       <strong>
        前馈网络（
        <code>
         Feed-Forward Network
        </code>
        ）
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> forward_expansion <span class="token operator">*</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>forward_expansion <span class="token operator">*</span> embed_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre>
      <p>
       前馈网络是一个简单的两层全连接网络。它的作用是进一步处理多头注意力机制的输出。
       <code>
        forward_expansion
       </code>
       参数控制隐藏层的大小，通常设置为一个较大的值（如4），表示隐藏层的维度是输入维度的4倍。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Dropout
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</code></pre>
      <p>
       Dropout 是一种正则化技术，通过随机丢弃一部分神经元的输出来防止过拟合。
       <code>
        dropout
       </code>
       参数表示丢弃的概率。
      </p>
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="3__136">
     </a>
     3.
     <strong>
      前向传播方法
     </strong>
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> value<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>value<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>attention <span class="token operator">+</span> query<span class="token punctuation">)</span><span class="token punctuation">)</span>
    forward <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>forward <span class="token operator">+</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> out
</code></pre>
    <h5>
     <a id="_147">
     </a>
     <strong>
      参数解释
     </strong>
    </h5>
    <ul>
     <li>
      <code>
       value
      </code>
      : 值向量，用于计算注意力权重后的加权求和。
     </li>
     <li>
      <code>
       key
      </code>
      : 键向量，用于计算注意力权重。
     </li>
     <li>
      <code>
       query
      </code>
      : 查询向量，用于计算注意力权重。
     </li>
     <li>
      <code>
       mask
      </code>
      : 掩码，用于防止某些位置的信息泄露（如在自注意力中屏蔽未来信息）。
     </li>
    </ul>
    <h5>
     <a id="_153">
     </a>
     <strong>
      流程解释
     </strong>
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        多头注意力
       </strong>
      </p>
      <pre><code class="prism language-python">attention <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>value<span class="token punctuation">,</span> key<span class="token punctuation">,</span> query<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
</code></pre>
      <p>
       首先，使用多头注意力机制计算注意力输出。
       <code>
        value
       </code>
       、
       <code>
        key
       </code>
       和
       <code>
        query
       </code>
       是输入的三个部分，
       <code>
        mask
       </code>
       用于控制哪些位置的信息可以被关注。
      </p>
     </li>
     <li>
      <p>
       <strong>
        残差连接与层归一化
       </strong>
      </p>
      <pre><code class="prism language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>attention <span class="token operator">+</span> query<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
      <p>
       将注意力输出与输入的
       <code>
        query
       </code>
       进行残差连接（
       <code>
        attention + query
       </code>
       ），然后通过层归一化（
       <code>
        LayerNorm
       </code>
       ）。最后，应用 Dropout 防止过拟合。
      </p>
     </li>
     <li>
      <p>
       <strong>
        前馈网络
       </strong>
      </p>
      <pre><code class="prism language-python">forward <span class="token operator">=</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre>
      <p>
       将经过归一化的输出传递到前馈网络中进行进一步处理。
      </p>
     </li>
     <li>
      <p>
       <strong>
        第二次残差连接与层归一化
       </strong>
      </p>
      <pre><code class="prism language-python">out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>forward <span class="token operator">+</span> x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
      <p>
       将前馈网络的输出与之前的输出
       <code>
        x
       </code>
       进行残差连接，再次通过层归一化和 Dropout。
      </p>
     </li>
     <li>
      <p>
       <strong>
        返回结果
       </strong>
      </p>
      <pre><code class="prism language-python"><span class="token keyword">return</span> out
</code></pre>
      <p>
       最终返回处理后的输出。
      </p>
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="4__186">
     </a>
     4.
     <strong>
      总结
     </strong>
    </h4>
    <p>
     Transformer编码器模块，其核心包括：
    </p>
    <ul>
     <li>
      多头注意力机制（
      <code>
       MultiHeadAttention
      </code>
      ）。
     </li>
     <li>
      残差连接（Residual Connection）。
     </li>
     <li>
      层归一化（
      <code>
       LayerNorm
      </code>
      ）。
     </li>
     <li>
      前馈网络（
      <code>
       Feed-Forward Network
      </code>
      ）。
     </li>
     <li>
      Dropout 正则化。
     </li>
    </ul>
    <p>
     这些组件共同作用，使得Transformer能够高效地处理序列数据，并在许多NLP任务中取得了优异的性能。
    </p>
    <h3>
     <a id="2_198">
     </a>
     2.编码器的实现
    </h3>
    <p>
     这段代码定义了一个完整的
     <strong>
      Transformer 编码器（Encoder）
     </strong>
     ，它是 Transformer 架构中的一个重要组成部分。编码器的作用是将输入序列（如源语言文本）转换为上下文表示，这些表示可以被解码器（Decoder）用于生成目标序列（如目标语言文本）。以下是对代码的详细解释：
    </p>
    <hr/>
    <h4>
     <a id="1__203">
     </a>
     1.
     <strong>
      类定义
     </strong>
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
</code></pre>
    <p>
     <code>
      Encoder
     </code>
     是一个继承自 PyTorch 的
     <code>
      nn.Module
     </code>
     的类，用于定义 Transformer 编码器的结构。
     <code>
      nn.Module
     </code>
     是 PyTorch 中所有神经网络模块的基类，用于定义和管理神经网络的结构。
    </p>
    <hr/>
    <h4>
     <a id="2__211">
     </a>
     2.
     <strong>
      初始化方法
     </strong>
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src_vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> heads<span class="token punctuation">,</span> device<span class="token punctuation">,</span> forward_expansion<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>embed_size <span class="token operator">=</span> embed_size
    self<span class="token punctuation">.</span>device <span class="token operator">=</span> device
    self<span class="token punctuation">.</span>word_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>src_vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>position_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
        <span class="token punctuation">[</span>
            TransformerBlock<span class="token punctuation">(</span>
                embed_size<span class="token punctuation">,</span>
                heads<span class="token punctuation">,</span>
                dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span>
                forward_expansion<span class="token operator">=</span>forward_expansion<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
    <span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_235">
     </a>
     <strong>
      参数解释
     </strong>
    </h5>
    <ul>
     <li>
      <code>
       src_vocab_size
      </code>
      : 源语言词汇表的大小，即输入序列中可能的标记（token）数量。
     </li>
     <li>
      <code>
       embed_size
      </code>
      : 嵌入向量的维度，表示每个词或标记的特征维度。
     </li>
     <li>
      <code>
       num_layers
      </code>
      : 编码器中 Transformer 块（
      <code>
       TransformerBlock
      </code>
      ）的数量。
     </li>
     <li>
      <code>
       heads
      </code>
      : 多头注意力机制中的头数。
     </li>
     <li>
      <code>
       device
      </code>
      : 运行设备（如 CPU 或 GPU）。
     </li>
     <li>
      <code>
       forward_expansion
      </code>
      : 前馈网络（FFN）中隐藏层的扩展因子。
     </li>
     <li>
      <code>
       dropout
      </code>
      : Dropout 比率，用于防止过拟合。
     </li>
     <li>
      <code>
       max_length
      </code>
      : 输入序列的最大长度，用于位置编码。
     </li>
    </ul>
    <h5>
     <a id="_245">
     </a>
     <strong>
      组件解释
     </strong>
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        词嵌入（
        <code>
         word_embedding
        </code>
        ）
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>word_embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>src_vocab_size<span class="token punctuation">,</span> embed_size<span class="token punctuation">)</span>
</code></pre>
      <p>
       词嵌入层将输入的标记（token）索引映射为固定维度的嵌入向量。
       <code>
        src_vocab_size
       </code>
       是词汇表的大小，
       <code>
        embed_size
       </code>
       是嵌入向量的维度。
      </p>
     </li>
     <li>
      <p>
       <strong>
        位置编码（
        <code>
         position_embedding
        </code>
        ）
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>position_embedding <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>embed_size<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_length<span class="token punctuation">)</span>
</code></pre>
      <p>
       位置编码层用于为每个标记添加位置信息，使得模型能够捕捉序列中的顺序关系。
       <code>
        PositionalEncoding
       </code>
       的具体实现没有在这段代码中给出，但通常它会根据标记的位置生成一个固定维度的向量，并将其与词嵌入相加。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Transformer 块（
        <code>
         TransformerBlock
        </code>
        ）
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>
        TransformerBlock<span class="token punctuation">(</span>
            embed_size<span class="token punctuation">,</span>
            heads<span class="token punctuation">,</span>
            dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span>
            forward_expansion<span class="token operator">=</span>forward_expansion<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span>
    <span class="token punctuation">]</span>
<span class="token punctuation">)</span>
</code></pre>
      <p>
       编码器由多个 Transformer 块组成。每个 Transformer 块包含多头注意力机制和前馈网络。
       <code>
        num_layers
       </code>
       表示 Transformer 块的数量。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Dropout
       </strong>
      </p>
      <pre><code class="prism language-python">self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
</code></pre>
      <p>
       Dropout 是一种正则化技术，通过随机丢弃一部分神经元的输出来防止过拟合。
       <code>
        dropout
       </code>
       参数表示丢弃的概率。
      </p>
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="3__282">
     </a>
     3.
     <strong>
      前向传播方法
     </strong>
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    N<span class="token punctuation">,</span> seq_length <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>position_embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
        x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>

    <span class="token keyword">return</span> x
</code></pre>
    <h5>
     <a id="_294">
     </a>
     <strong>
      参数解释
     </strong>
    </h5>
    <ul>
     <li>
      <code>
       x
      </code>
      : 输入序列，形状为
      <code>
       (N, seq_length)
      </code>
      ，其中
      <code>
       N
      </code>
      是批次大小，
      <code>
       seq_length
      </code>
      是序列长度。
     </li>
     <li>
      <code>
       mask
      </code>
      : 掩码，用于防止某些位置的信息泄露（如在自注意力中屏蔽未来信息）。
     </li>
    </ul>
    <h5>
     <a id="_298">
     </a>
     <strong>
      流程解释
     </strong>
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        词嵌入与位置编码
       </strong>
      </p>
      <pre><code class="prism language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>position_embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>word_embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
      <ul>
       <li>
        首先，将输入序列
        <code>
         x
        </code>
        通过词嵌入层（
        <code>
         word_embedding
        </code>
        ）得到嵌入向量。
       </li>
       <li>
        然后，将嵌入向量与位置编码（
        <code>
         position_embedding
        </code>
        ）相加，为每个标记添加位置信息。
       </li>
       <li>
        最后，应用 Dropout 防止过拟合。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        逐层传递
       </strong>
      </p>
      <pre><code class="prism language-python"><span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
    x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
</code></pre>
      <ul>
       <li>
        输入序列
        <code>
         x
        </code>
        逐层传递到每个 Transformer 块中。在每个块中：
        <ul>
         <li>
          <code>
           value
          </code>
          、
          <code>
           key
          </code>
          和
          <code>
           query
          </code>
          都是
          <code>
           x
          </code>
          ，因为这是自注意力机制（Self-Attention）。
         </li>
         <li>
          <code>
           mask
          </code>
          用于控制哪些位置的信息可以被关注。
         </li>
        </ul>
       </li>
       <li>
        每个 Transformer 块的输出会作为下一层的输入。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        返回结果
       </strong>
      </p>
      <pre><code class="prism language-python"><span class="token keyword">return</span> x
</code></pre>
      <ul>
       <li>
        最终返回编码器的输出，形状为
        <code>
         (N, seq_length, embed_size)
        </code>
        ，表示每个位置的上下文表示。
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="4__325">
     </a>
     4.
     <strong>
      总结
     </strong>
    </h4>
    <p>
     Transformer 编码器，其主要功能包括：
    </p>
    <ol>
     <li>
      <strong>
       词嵌入与位置编码
      </strong>
      ：将输入标记转换为嵌入向量，并添加位置信息。
     </li>
     <li>
      <strong>
       多层 Transformer 块
      </strong>
      ：通过多头注意力机制和前馈网络逐层处理输入序列。
     </li>
     <li>
      <strong>
       掩码机制
      </strong>
      ：通过掩码控制注意力的范围，避免信息泄露。
     </li>
     <li>
      <strong>
       Dropout 正则化
      </strong>
      ：防止过拟合。
     </li>
    </ol>
    <p>
     编码器的输出是一个上下文表示，可以被解码器用于生成目标序列。这种架构在机器翻译、文本生成等任务中表现出色。
    </p>
    <p>
     作者码字不易，觉得有用的话不妨点个赞吧，关注我，持续为您更新AI的优质内容。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f323330325f38303233363633332f:61727469636c652f64657461696c732f313436313736303437" class_="artid" style="display:none">
 </p>
</div>


