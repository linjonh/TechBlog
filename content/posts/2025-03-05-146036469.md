---
layout: post
title: "神经网络为什么要用-ReLU-增加非线性"
date: 2025-03-05 10:53:25 +0800
description: "非线性激活函数（如 ReLU、Sigmoid、Tanh）能够打破线性关系，使神经网络能够学习复杂的非线性模式。在 Sigmoid 或 Tanh 激活函数中，当输入值较大或较小时，梯度会趋近于零，导致梯度消失问题。ReLU 的计算非常简单，只需要比较和取最大值操作，计算速度远快于 Sigmoid 和 Tanh。ReLU 的梯度在正区间恒为 1，避免了梯度消失问题，使得深层网络的训练更加稳定。将 Leaky ReLU 的斜率 α 作为可学习参数，动态调整负区间的输出。其中 α 是一个小的正数（如 0.01）。"
keywords: "relu为什么非线性"
categories: ['人工智能']
tags: ['神经网络', '深度学习', '人工智能']
artid: "146036469"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146036469
    alt: "神经网络为什么要用-ReLU-增加非线性"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146036469
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146036469
cover: https://bing.ee123.net/img/rand?artid=146036469
image: https://bing.ee123.net/img/rand?artid=146036469
img: https://bing.ee123.net/img/rand?artid=146036469
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     神经网络为什么要用 ReLU 增加非线性？
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     在神经网络中使用
     <strong>
      ReLU（Rectified Linear Unit）
     </strong>
     作为激活函数的主要目的是引入
     <strong>
      非线性
     </strong>
     ，这是神经网络能够学习复杂模式和解决非线性问题的关键。
    </p>
    <p style="text-align:center">
     <img alt="" src="https://i-blog.csdnimg.cn/direct/6de15083b5884096bf64007f312e9c15.jpeg"/>
    </p>
    <hr/>
    <h3>
     <strong>
      1. 为什么需要非线性？
     </strong>
    </h3>
    <h4>
     <strong>
      1.1 线性模型的局限性
     </strong>
    </h4>
    <p>
     如果神经网络只使用线性激活函数（如
     <img alt="f(x)=x" class="mathcode" src="https://latex.csdn.net/eq?f%28x%29%3Dx">
      ），那么无论网络有多少层，整个模型仍然是一个线性模型。这是因为多个线性变换的组合仍然是线性变换：
     </img>
    </p>
    <p style="text-align:center">
     <img alt="f(f(f(x)))=W_{3}(W_{2}(W_{1}x+b_{1})+b_{2}) + b_{3}={W}'x+{b}'" class="mathcode" src="https://latex.csdn.net/eq?f%28f%28f%28x%29%29%29%3DW_%7B3%7D%28W_%7B2%7D%28W_%7B1%7Dx&amp;plus;b_%7B1%7D%29&amp;plus;b_%7B2%7D%29%20&amp;plus;%20b_%7B3%7D%3D%7BW%7D%27x&amp;plus;%7Bb%7D%27"/>
    </p>
    <p>
     这样的模型无法学习复杂的非线性关系，表达能力非常有限。
    </p>
    <h4>
     <strong>
      1.2 非线性激活函数的作用
     </strong>
    </h4>
    <p>
     非线性激活函数（如 ReLU、Sigmoid、Tanh）能够打破线性关系，使神经网络能够学习复杂的非线性模式。通过堆叠多个非线性层，神经网络可以逼近任意复杂的函数。
    </p>
    <hr/>
    <h3>
     <strong>
      2. ReLU 的定义
     </strong>
    </h3>
    <p>
     ReLU 的定义非常简单：
    </p>
    <p style="text-align:center">
     <img alt="ReLU(x)=max(0,x)" class="mathcode" src="https://latex.csdn.net/eq?ReLU%28x%29%3Dmax%280%2Cx%29"/>
    </p>
    <ul>
     <li>
      <p>
       当输入 x&gt;0 时，输出 x。
      </p>
     </li>
     <li>
      <p>
       当输入 x≤0 时，输出 0。
      </p>
     </li>
    </ul>
    <hr/>
    <h3>
     <strong>
      3. ReLU 的优势
     </strong>
    </h3>
    <h4>
     <strong>
      3.1 缓解梯度消失问题
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       在 Sigmoid 或 Tanh 激活函数中，当输入值较大或较小时，梯度会趋近于零，导致梯度消失问题。
      </p>
     </li>
     <li>
      <p>
       ReLU 的梯度在正区间恒为 1，避免了梯度消失问题，使得深层网络的训练更加稳定。
      </p>
     </li>
    </ul>
    <h4>
     <strong>
      3.2 计算高效
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       ReLU 的计算非常简单，只需要比较和取最大值操作，计算速度远快于 Sigmoid 和 Tanh。
      </p>
     </li>
     <li>
      <p>
       在训练大规模神经网络时，ReLU 的高效计算能够显著加快训练速度。
      </p>
     </li>
    </ul>
    <h4>
     <strong>
      3.3 稀疏激活
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       ReLU 会将负值输出为零，这使得神经网络的激活变得稀疏。
      </p>
     </li>
     <li>
      <p>
       稀疏激活可以减少参数之间的依赖性，提高模型的泛化能力。
      </p>
     </li>
    </ul>
    <hr/>
    <h3>
     <strong>
      4. ReLU 的变体
     </strong>
    </h3>
    <p>
     尽管 ReLU 有很多优点，但它也存在一些问题（如神经元“死亡”问题）。因此，研究者提出了多种 ReLU 的变体：
    </p>
    <h4>
     <strong>
      4.1 Leaky ReLU
     </strong>
    </h4>
    <p>
     在负区间引入一个小的斜率，避免神经元“死亡”：
    </p>
    <p style="text-align:center">
     <img alt="Leaky ReLU(x)=\begin{cases} x&amp; \text{ if } x&gt; 0 \\ ax&amp; \text{ if } x\leq 0 \end{cases}" class="mathcode" src="https://latex.csdn.net/eq?Leaky%20ReLU%28x%29%3D%5Cbegin%7Bcases%7D%20x%26%20%5Ctext%7B%20if%20%7D%20x%3E%200%20%5C%5C%20ax%26%20%5Ctext%7B%20if%20%7D%20x%5Cleq%200%20%5Cend%7Bcases%7D"/>
    </p>
    <p>
     其中 α 是一个小的正数（如 0.01）。
    </p>
    <h4>
     <strong>
      4.2 Parametric ReLU (PReLU)
     </strong>
    </h4>
    <p>
     将 Leaky ReLU 的斜率 α 作为可学习参数，动态调整负区间的输出。
    </p>
    <h4>
     <strong>
      4.3 Exponential Linear Unit (ELU)
     </strong>
    </h4>
    <p>
     在负区间引入指数函数，平滑过渡：
    </p>
    <p style="text-align:center">
     <img alt="ELU(x)=\begin{cases} x &amp; \text{ if } x&gt; 0 \\ a(e^{x}-1) &amp; \text{ if } x\leq 0 \end{cases}" class="mathcode" src="https://latex.csdn.net/eq?ELU%28x%29%3D%5Cbegin%7Bcases%7D%20x%20%26%20%5Ctext%7B%20if%20%7D%20x%3E%200%20%5C%5C%20a%28e%5E%7Bx%7D-1%29%20%26%20%5Ctext%7B%20if%20%7D%20x%5Cleq%200%20%5Cend%7Bcases%7D"/>
    </p>
    <hr/>
    <h3>
     <strong>
      5. ReLU 的代码实现
     </strong>
    </h3>
    <p>
     以下是 ReLU 及其变体的 PyTorch 展示：
    </p>
    <pre><code class="language-python">import torch
import torch.nn as nn

# 标准 ReLU
relu = nn.ReLU()
x = torch.tensor([-1.0, 2.0, -3.0, 4.0])
print(relu(x))  # 输出: tensor([0., 2., 0., 4.])

# Leaky ReLU
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
print(leaky_relu(x))  # 输出: tensor([-0.0100,  2.0000, -0.0300,  4.0000])

# ELU
elu = nn.ELU(alpha=1.0)
print(elu(x))  # 输出: tensor([-0.6321,  2.0000, -0.9502,  4.0000])</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f753031323933353434352f:61727469636c652f64657461696c732f313436303336343639" class_="artid" style="display:none">
 </p>
</div>


