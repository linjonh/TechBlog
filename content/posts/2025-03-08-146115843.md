---
layout: post
title: "第七课Python反爬攻防战HeadersIP代理与验证码"
date: 2025-03-08 15:05:41 +0800
description: "解析如何通过随机User-Agent生成、代理IP池搭建以及验证码识别来应对这些反爬策略。"
keywords: "第七课：Python反爬攻防战：Headers/IP代理与验证码"
categories: ['未分类']
tags: ['开发语言', 'Tcp', 'Python', 'Proxy', 'Ocr', 'Beautifulsoup']
artid: "146115843"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146115843
    alt: "第七课Python反爬攻防战HeadersIP代理与验证码"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146115843
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146115843
cover: https://bing.ee123.net/img/rand?artid=146115843
image: https://bing.ee123.net/img/rand?artid=146115843
img: https://bing.ee123.net/img/rand?artid=146115843
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     第七课：Python反爬攻防战：Headers/IP代理与验证码
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     在爬虫开发过程中，反爬虫机制成为了我们必须面对的挑战。本文将深入探讨Python爬虫中常见的反爬机制，并详细解析如何通过随机User-Agent生成、代理IP池搭建以及验证码识别来应对这些反爬策略。文章将包含完整的示例代码，帮助读者更好地理解和应用这些技术。
    </p>
    <p style="text-align:center">
     <img alt="" height="248" src="https://i-blog.csdnimg.cn/direct/e9f3fde183bf41dca2da357362cc8dd8.png" width="441"/>
    </p>
    <h4>
     一、常见反爬机制解析
    </h4>
    <h5>
     1.1 基于Headers的反爬
    </h5>
    <p>
     许多网站通过检查请求头（Headers）中的User-Agent字段来判断请求是否来自爬虫。如果User-Agent字段不符合预期，网站可能会拒绝服务或返回错误页面。
    </p>
    <h5>
     1.2 基于IP的反爬
    </h5>
    <p>
     为了限制爬虫对网站的访问频率，网站通常会记录访问者的IP地址。当某个IP地址在短时间内发送大量请求时，网站可能会暂时或永久封禁该IP地址。
    </p>
    <h5>
     1.3 基于验证码的反爬
    </h5>
    <p>
     验证码是网站用来区分人类用户和自动化脚本的一种有效手段。当检测到异常访问模式时，网站可能会要求访问者输入验证码以验证其身份。
    </p>
    <h4>
     二、随机User-Agent生成
    </h4>
    <p>
     为了绕过基于Headers的反爬机制，我们可以使用随机User-Agent来模拟不同浏览器的访问请求。Python中的fake_useragent库可以帮助我们轻松实现这一点。
    </p>
    <p>
     <strong>
      安装命令
     </strong>
    </p>
    <pre><code class="language-bash">pip install fake-useragent</code></pre>
    <p>
     <strong>
      示例代码
     </strong>
    </p>
    <pre><code class="language-python">import requests
from fake_useragent import UserAgent
 
# 生成一个随机的User-Agent
ua = UserAgent()
random_user_agent = ua.random
 
# 设置请求头
headers = {
    'User-Agent': random_user_agent
}
 
# 发送请求
response = requests.get('https://www.example.com', headers=headers)
print(response.text)</code></pre>
    <h4>
     三、代理IP池搭建实战
    </h4>
    <p>
     为了绕过基于IP的反爬机制，我们可以使用代理IP来隐藏真实的IP地址。搭建一个代理IP池，并随机选择代理IP进行请求，可以大大降低被封禁的风险。
    </p>
    <p>
     <strong>
      示例代码
     </strong>
    </p>
    <h5>
     3.1 爬取代理IP
    </h5>
    <p>
     首先，我们需要从一些提供免费代理IP的网站爬取代理IP信息。
    </p>
    <pre><code class="language-python">import requests
from bs4 import BeautifulSoup
 
def get_proxy_ips():
    # 替换为实际代理IP网站
    url = "https://www.example-proxy-website.com"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    proxy_ips = []
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # 假设代理IP信息在一个表格中，通过查找表格行(tr)来获取数据
            rows = soup.find_all('tr')

            # 跳过表头行
            for row in rows[1:]:
                cols = row.find_all('td')
                ip = cols[0].text
                port = cols[1].text
                proxy = f"{ip}:{port}"
                proxy_ips.append(proxy)
        return proxy_ips
    except requests.RequestException as e:
        print(f"请求错误: {e}")
        return []
 
proxy_ips = get_proxy_ips()
print(proxy_ips)</code></pre>
    <h5>
     3.2 验证代理IP
    </h5>
    <p>
     爬取到的代理IP不一定都能正常使用，因此我们需要进行可用性验证。
    </p>
    <pre><code class="language-python">def check_proxy(proxy):
    test_url = "https://www.baidu.com"
    
    # 可以代理的字典数据
    proxies = {
        "http": f"http://{proxy}",
        "https": f"https://{proxy}"
    }
    try:

        # 测试代理地址
        response = requests.get(test_url, proxies=proxies, timeout=5)
        if response.status_code == 200:
            return True
        return False
    except requests.RequestException:
        return False
 
valid_proxy_ips = []
for proxy in proxy_ips:
    if check_proxy(proxy):
        valid_proxy_ips.append(proxy)

# 输出可以进行代理的正确地址
print(valid_proxy_ips)</code></pre>
    <h5>
     3.3 使用代理IP进行请求
    </h5>
    <p>
     最后，我们可以使用验证通过的代理IP来发送请求。
    </p>
    <pre><code class="language-python">import random
 
# 随机选择一个可用的代理IP
proxy = random.choice(valid_proxy_ips)
proxies = {
    "http": f"http://{proxy}",
    "https": f"https://{proxy}"
}
 
# 设置请求头
headers = {
    'User-Agent': random_user_agent
}
 
# 发送请求
response = requests.get('https://www.example.com', headers=headers, proxies=proxies)
print(response.text)</code></pre>
    <h4>
     四、验证码识别基础方案
    </h4>
    <p>
     验证码识别是绕过基于验证码反爬机制的关键。虽然验证码识别技术相对复杂，但我们可以使用一些开源的
     <strong>
      OCR
     </strong>
     （文字识别）库来实现基本的验证码识别。
    </p>
    <p>
     <strong>
      示例代码
     </strong>
    </p>
    <h5>
     4.1 安装必要的库
    </h5>
    <p>
     从
     <a class="link-info" href="https://github.com/tesseract-ocr/tesseract" title="Tesseract-OCR官网">
      Tesseract-OCR官网
     </a>
     下载并安装
     <strong>
      Tesseract-OCR
     </strong>
    </p>
    <p>
     首先，我们需要安装Pillow和pytesseract库。Pillow用于图像处理，pytesseract是Tesseract-OCR的Python接口。
    </p>
    <pre><code class="language-bash">pip install pillow pytesseract</code></pre>
    <p>
     <strong>
      注意：
     </strong>
     你还需要从Tesseract-OCR官网下载并安装
     <strong>
      Tesseract-OCR
     </strong>
     ，并设置环境变量
     <strong>
      TESSDATA_PREFIX
     </strong>
     指向包含
     <strong>
      tessdata
     </strong>
     的目录。
    </p>
    <h5>
     4.2 验证码识别
    </h5>
    <p>
     假设我们已经下载了一张验证码图片captcha.jpg，我们可以使用以下代码进行识别。
    </p>
    <pre><code class="language-python">from PIL import Image
import pytesseract
 
# 打开验证码图片
image = Image.open('captcha.jpg')
 
# 进行OCR识别
text = pytesseract.image_to_string(image, lang='eng')
 
print('识别结果:', text)</code></pre>
    <p>
     识别完成以后，根据前边学习的内容，把图片中的内容填写到输入框即可
    </p>
    <h4>
     总结
    </h4>
    <p>
     本文通过详细解析常见的反爬机制，并提供了随机User-Agent生成、代理IP池搭建以及验证码识别的基础方案，帮助读者更好地理解和应对Python爬虫中的反爬挑战。希望这些技术和示例代码能对大家的爬虫开发有所帮助。
    </p>
    <p>
     <strong>
      关注我！！🫵
     </strong>
     持续为你带来Python相关内容。
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35373137393639362f:61727469636c652f64657461696c732f313436313135383433" class_="artid" style="display:none">
 </p>
</div>


