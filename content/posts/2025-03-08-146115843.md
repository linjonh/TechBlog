---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35373137393639362f:61727469636c652f64657461696c732f313436313135383433"
layout: post
title: "第七课Python反爬攻防战HeadersIP代理与验证码"
date: 2025-03-08 15:05:41 +08:00
description: "解析如何通过随机User-Agent生成、代理IP池搭建以及验证码识别来应对这些反爬策略。"
keywords: "第七课：Python反爬攻防战：Headers/IP代理与验证码"
categories: ['未分类']
tags: ['开发语言', 'Tcp', 'Python', 'Proxy', 'Ocr', 'Beautifulsoup']
artid: "146115843"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146115843
    alt: "第七课Python反爬攻防战HeadersIP代理与验证码"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146115843
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146115843
cover: https://bing.ee123.net/img/rand?artid=146115843
image: https://bing.ee123.net/img/rand?artid=146115843
img: https://bing.ee123.net/img/rand?artid=146115843
---

# 第七课：Python反爬攻防战：Headers/IP代理与验证码

在爬虫开发过程中，反爬虫机制成为了我们必须面对的挑战。本文将深入探讨Python爬虫中常见的反爬机制，并详细解析如何通过随机User-Agent生成、代理IP池搭建以及验证码识别来应对这些反爬策略。文章将包含完整的示例代码，帮助读者更好地理解和应用这些技术。

![](https://i-blog.csdnimg.cn/direct/e9f3fde183bf41dca2da357362cc8dd8.png)

#### 一、常见反爬机制解析

##### 1.1 基于Headers的反爬

许多网站通过检查请求头（Headers）中的User-Agent字段来判断请求是否来自爬虫。如果User-Agent字段不符合预期，网站可能会拒绝服务或返回错误页面。

##### 1.2 基于IP的反爬

为了限制爬虫对网站的访问频率，网站通常会记录访问者的IP地址。当某个IP地址在短时间内发送大量请求时，网站可能会暂时或永久封禁该IP地址。

##### 1.3 基于验证码的反爬

验证码是网站用来区分人类用户和自动化脚本的一种有效手段。当检测到异常访问模式时，网站可能会要求访问者输入验证码以验证其身份。

#### 二、随机User-Agent生成

为了绕过基于Headers的反爬机制，我们可以使用随机User-Agent来模拟不同浏览器的访问请求。Python中的fake\_useragent库可以帮助我们轻松实现这一点。

**安装命令**

```bash
pip install fake-useragent
```

**示例代码**

```python
import requests
from fake_useragent import UserAgent
 
# 生成一个随机的User-Agent
ua = UserAgent()
random_user_agent = ua.random
 
# 设置请求头
headers = {
    'User-Agent': random_user_agent
}
 
# 发送请求
response = requests.get('https://www.example.com', headers=headers)
print(response.text)
```

#### 三、代理IP池搭建实战

为了绕过基于IP的反爬机制，我们可以使用代理IP来隐藏真实的IP地址。搭建一个代理IP池，并随机选择代理IP进行请求，可以大大降低被封禁的风险。

**示例代码**

##### 3.1 爬取代理IP

首先，我们需要从一些提供免费代理IP的网站爬取代理IP信息。

```python
import requests
from bs4 import BeautifulSoup
 
def get_proxy_ips():
    # 替换为实际代理IP网站
    url = "https://www.example-proxy-website.com"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    proxy_ips = []
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            # 假设代理IP信息在一个表格中，通过查找表格行(tr)来获取数据
            rows = soup.find_all('tr')

            # 跳过表头行
            for row in rows[1:]:
                cols = row.find_all('td')
                ip = cols[0].text
                port = cols[1].text
                proxy = f"{ip}:{port}"
                proxy_ips.append(proxy)
        return proxy_ips
    except requests.RequestException as e:
        print(f"请求错误: {e}")
        return []
 
proxy_ips = get_proxy_ips()
print(proxy_ips)
```

##### 3.2 验证代理IP

爬取到的代理IP不一定都能正常使用，因此我们需要进行可用性验证。

```python
def check_proxy(proxy):
    test_url = "https://www.baidu.com"
    
    # 可以代理的字典数据
    proxies = {
        "http": f"http://{proxy}",
        "https": f"https://{proxy}"
    }
    try:

        # 测试代理地址
        response = requests.get(test_url, proxies=proxies, timeout=5)
        if response.status_code == 200:
            return True
        return False
    except requests.RequestException:
        return False
 
valid_proxy_ips = []
for proxy in proxy_ips:
    if check_proxy(proxy):
        valid_proxy_ips.append(proxy)

# 输出可以进行代理的正确地址
print(valid_proxy_ips)
```

##### 3.3 使用代理IP进行请求

最后，我们可以使用验证通过的代理IP来发送请求。

```python
import random
 
# 随机选择一个可用的代理IP
proxy = random.choice(valid_proxy_ips)
proxies = {
    "http": f"http://{proxy}",
    "https": f"https://{proxy}"
}
 
# 设置请求头
headers = {
    'User-Agent': random_user_agent
}
 
# 发送请求
response = requests.get('https://www.example.com', headers=headers, proxies=proxies)
print(response.text)
```

#### 四、验证码识别基础方案

验证码识别是绕过基于验证码反爬机制的关键。虽然验证码识别技术相对复杂，但我们可以使用一些开源的
**OCR**
（文字识别）库来实现基本的验证码识别。

**示例代码**

##### 4.1 安装必要的库

从
[Tesseract-OCR官网](https://github.com/tesseract-ocr/tesseract "Tesseract-OCR官网")
下载并安装
**Tesseract-OCR**

首先，我们需要安装Pillow和pytesseract库。Pillow用于图像处理，pytesseract是Tesseract-OCR的Python接口。

```bash
pip install pillow pytesseract
```

**注意：**
你还需要从Tesseract-OCR官网下载并安装
**Tesseract-OCR**
，并设置环境变量
**TESSDATA\_PREFIX**
指向包含
**tessdata**
的目录。

##### 4.2 验证码识别

假设我们已经下载了一张验证码图片captcha.jpg，我们可以使用以下代码进行识别。

```python
from PIL import Image
import pytesseract
 
# 打开验证码图片
image = Image.open('captcha.jpg')
 
# 进行OCR识别
text = pytesseract.image_to_string(image, lang='eng')
 
print('识别结果:', text)
```

识别完成以后，根据前边学习的内容，把图片中的内容填写到输入框即可

#### 总结

本文通过详细解析常见的反爬机制，并提供了随机User-Agent生成、代理IP池搭建以及验证码识别的基础方案，帮助读者更好地理解和应对Python爬虫中的反爬挑战。希望这些技术和示例代码能对大家的爬虫开发有所帮助。

**关注我！！🫵**
持续为你带来Python相关内容。