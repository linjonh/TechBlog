---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34303731343934392f:61727469636c652f64657461696c732f313435373737343936"
layout: post
title: "每日Attention学习28Strip-Pooling"
date: 2025-03-16 21:14:37 +08:00
description: "【代码】每日Attention学习28——Strip Pooling。"
keywords: "每日Attention学习28——Strip Pooling"
categories: ['未分类']
tags: ['划水']
artid: "145777496"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145777496
    alt: "每日Attention学习28Strip-Pooling"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145777496
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145777496
cover: https://bing.ee123.net/img/rand?artid=145777496
image: https://bing.ee123.net/img/rand?artid=145777496
img: https://bing.ee123.net/img/rand?artid=145777496
---

# 每日Attention学习28——Strip Pooling

##### 模块出处

[CVPR 20]
[[link]](http://openaccess.thecvf.com/content_CVPR_2020/html/Hou_Strip_Pooling_Rethinking_Spatial_Pooling_for_Scene_Parsing_CVPR_2020_paper.html)
Strip Pooling: Rethinking Spatial Pooling for Scene Parsing

---

##### 模块名称

Strip Pooling (SP)

---

##### 模块结构

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5c6be84fd8494657a5326b979da2f766.png)

---

##### 模块特点

* 本质是空间注意力的一种
* 使用横/纵两个方向的条形池化获得一维方向上的重要程度，结合后便可以扩展至二维方向

---

##### 模块代码

```python
import torch
import torch.nn as nn
import torch.nn.functional as F


class SP(nn.Module):
    def __init__(self, in_channels, pool_size):
        super(SP, self).__init__()
        self.pool1 = nn.AdaptiveAvgPool2d(pool_size[0])
        self.pool2 = nn.AdaptiveAvgPool2d(pool_size[1])
        self.pool3 = nn.AdaptiveAvgPool2d((1, None))
        self.pool4 = nn.AdaptiveAvgPool2d((None, 1))
        inter_channels = int(in_channels/4)
        self.conv1_1 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                nn.BatchNorm2d(inter_channels),
                                nn.ReLU(True))
        self.conv1_2 = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 1, bias=False),
                                nn.BatchNorm2d(inter_channels),
                                nn.ReLU(True))
        self.conv2_0 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                nn.BatchNorm2d(inter_channels))
        self.conv2_1 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                nn.BatchNorm2d(inter_channels))
        self.conv2_2 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                nn.BatchNorm2d(inter_channels))
        self.conv2_3 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (1, 3), 1, (0, 1), bias=False),
                                nn.BatchNorm2d(inter_channels))
        self.conv2_4 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, (3, 1), 1, (1, 0), bias=False),
                                nn.BatchNorm2d(inter_channels))
        self.conv2_5 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                nn.BatchNorm2d(inter_channels),
                                nn.ReLU(True))
        self.conv2_6 = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, 1, 1, bias=False),
                                nn.BatchNorm2d(inter_channels),
                                nn.ReLU(True))
        self.conv3 = nn.Sequential(nn.Conv2d(inter_channels*2, in_channels, 1, bias=False),
                                nn.BatchNorm2d(in_channels))

    def forward(self, x):
        _, _, h, w = x.size()
        x1 = self.conv1_1(x)
        x2 = self.conv1_2(x)
        x2_1 = self.conv2_0(x1)
        x2_2 = F.interpolate(self.conv2_1(self.pool1(x1)), (h, w))
        x2_3 = F.interpolate(self.conv2_2(self.pool2(x1)), (h, w))
        x2_4 = F.interpolate(self.conv2_3(self.pool3(x2)), (h, w))
        x2_5 = F.interpolate(self.conv2_4(self.pool4(x2)), (h, w))
        x1 = self.conv2_5(F.relu_(x2_1 + x2_2 + x2_3))
        x2 = self.conv2_6(F.relu_(x2_5 + x2_4))
        out = self.conv3(torch.cat([x1, x2], dim=1))
        return F.relu_(x + out)
    

if __name__ == '__main__':
    x = torch.randn([1, 64, 44, 44])
    sp = SP(in_channels=64, pool_size=(8, 8))
    out = sp(x)
    print(out.shape) # [1, 64, 44, 44]

```

---