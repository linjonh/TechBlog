---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f616f6c616e3132332f:61727469636c652f64657461696c732f313430303732353131"
layout: post
title: "大概是最全的开源大模型LLM盘点了吧"
date: 2025-01-16 02:31:48 +08:00
description: "LLM(Large Language Model, 大型语言模型)是指那些规模庞大、参数数量众多的深"
keywords: "llm开源大模型"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '人工智能', 'Aigc']
artid: "140072511"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=140072511
  alt: "大概是最全的开源大模型LLM盘点了吧"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=140072511
featuredImagePreview: https://bing.ee123.net/img/rand?artid=140072511
---

# 大概是最全的开源大模型LLM盘点了吧！

LLM(Large Language Model, 大型语言模型)是指那些规模庞大、参数数量众多的深度神经网络模型，用于理解和生成自然语言文本。在自然语言处理（NLP）领域有着广泛的应用，因其强大的语言理解和生成能力，能够处理各种复杂的文本任务，包括但不限于翻译、问答、文本摘要、对话、文本分类、情感分析、代码生成、创作辅助等。其主要功能和特点如下：

**「架构特点：」**

LLM主要基于Transformer架构，该架构由Vaswani等人在2017年的论文《Attention is All You Need》中提出。Transformer通过自注意力机制（Self-Attention）来捕捉文本中的长距离依赖关系，无需像循环神经网络（RNN）那样逐词递归处理，从而实现了并行计算，大大提高了训练和推理速度。典型的LLM结构包括：

* Encoder-Decoder结构：如用于机器翻译的模型。Encoder将输入文本编码成一个固定长度的上下文向量，Decoder 则依据该上下文向量生成目标语言的文本输出。
* Encoder-only结构：如BERT等。主要用于文本理解任务，如文本分类、命名实体识别、问答系统中的问题理解等。Encoder-only模型通过双向编码整个输入文本，生成具有上下文信息的隐藏状态，这些隐藏状态可以被后续任务特定的层（如分类层、标记层等）利用来进行预测。
* Decoder-only结构：如GPT系列模型，用于生成文本、补全句子、撰写文章等任务。这类模型直接根据给定的提示（prompt）或前文上下文生成连续的文本输出。

**「参数规模」**

LLM的“大型”体现在其巨大的参数量，通常在数十亿到数千亿之间。例如，GPT-3（Generative Pretrained Transformer 3）拥有约1750亿个参数，而更近期的模型如 GPT-4、PaLM、Chinchilla、阿里云的通义千问等，参数量可能更大。大规模参数使得模型能够学习到更丰富的语言规律和模式，提高其泛化能力和表达复杂语言结构的能力。

**「预训练与微调」**

LLM通常遵循“预训练-微调”的范式：

* 预训练：模型首先在大规模无标注文本数据（如互联网抓取的文本、书籍、百科等）上进行自我监督学习，通过自回归语言建模任务（预测下一个词的概率）或掩码语言建模任务（预测被遮蔽词语的概率）来学习语言的通用表示。
* 微调：预训练后的模型可以针对特定任务进行微调，即在特定领域的有标注数据上进一步训练，调整模型参数以适应特定任务的需求，如问答系统的回答生成、文本分类任务的标签预测等。

**「应用场景」**

LLM在实际中主要应用在以下场景：

* 生成文本：创作诗歌、故事、新闻文章、代码片段等。
* 理解与问答：解答各类问题，提供精准的信息检索和知识解析能力。
* 对话交互：进行自然、流畅的人机对话，模拟人类对话风格，用于客户服务、虚拟助手、教育辅导等领域。
* 文本翻译：实现高质量的跨语言翻译，无需显式对齐的平行语料。
* 文本摘要：自动生成文本摘要，提炼关键信息。
* 代码生成：根据自然语言描述编写或补全代码，助力编程和软件开发。

笔者在这里对目前的开源大模型LLM进行了一个全面、系统的整理，与大家分享~

## 国外开源模型

| 模型链接 | 模型描述 |
| --- | --- |
| OpenSora | 高效复现类Sora视频生成的完全开源方案 |
| GROK | 3140亿参数的混合专家模型，迄今参数量最大的开源LLM |
| Gemma | 谷歌商场开源模型2B，7B免费商用，开源第一易主了 |
| Mixtral | Mistral AI的突破性大模型，超越GPT3.5，重新定义人工智能性能和多样性 |
| Mistral7B | “欧洲OpenAI”的“最强7B开源模型”，全面超越13B的Llama2 |
| LLama2 | Open Meta带着可商用开源的羊驼2模型来了~ |
| LLama | Meta开源指令微调LLM，规模70亿到650亿不等 |
| WizardLM | 微软新发布13B，登顶AlpacaEval开源模型Top3，使用ChatGPT对指令进行复杂度进化微调LLama2 |
| Falcon | 阿联酋技术研究所推出，3.5万亿token训练，性能直接碾压LLaMA2 |
| Vicuna | Alpaca前成员等开源以LLama13B为基础使用ShareGPT指令微调的模型，提出了用GPT4来评测模型效果 |
| OpenChat | 80k ShareGPT对话微调LLama-2 13B开源模型中的战斗机 |
| Guanaco | LLama 7B基座，在alpaca52K数据上加入534K多语言指令数据微调 |
| MPT | MosaicML开源的预训练+指令微调的新模型，可商用，支持84k tokens超长输入 |
| RedPajama | RedPajama项目既开源预训练数据后开源3B，7B的预训练+指令微调模型 |
| koala | 使用alpaca、HC3等开源指令集+ShareGPT等ChatGPT数据微调llama，在榜单上排名较高 |
| ChatLLaMA | 基于RLHF微调了LLaMA |
| Alpaca | 斯坦福开源的使用52k数据在7B的LLaMA上微调得到 |
| Alpaca-lora | LORA微调的LLaMA |
| Dromedary | IBM self-aligned model with the LLaMA base |
| ColossalChat | HPC-AI Tech开源的Llama+RLHF微调 |
| MiniGPT4 | Vicuna+BLIP2 文本视觉融合 |
| StackLLama | LLama使用Stackexchange数据+SFT+RL |
| Cerebras | Cerebras开源了1亿到130亿的7个模型，从预训练数据到参数全开源 |
| Dolly-v2 | 可商用7b指令微调开源模型在GPT-J-6B上微调 |
| OpenChatKit | openai研究员打造GPT-NoX-20B微调+6B审核模型过滤 |
| MetaLM | 微软开源的大规模自监督预训练模型 |
| Amazon Titan | 亚马逊在aws上增加自家大模型 |
| OPT-IML | Meta复刻GPT3，up to 175B, 不过效果并不及GPT3 |
| Bloom | BigScience出品，规模最大176B |
| BloomZ | BigScience出品, 基于Bloom微调 |
| Galacia | 和Bloom相似，更针对科研领域训练的模型 |
| T0 | BigScience出品，3B~11B的在T5进行指令微调的模型 |
| EXLLama | Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weight |
| LongChat | llama-13b使用condensing rotary embedding technique微调的长文本模型 |
| MPT-30B | MosaicML开源的在8Ktoken上训练的大模型 |

## 国内开源模型

| 模型链接 | 模型描述 |
| --- | --- |
| BayLing | 中科院开源，性能媲美GPT-3.5，基于LLama7B/13B，增强的语言对齐的英语/中文大语言模型 |
| GLM | 清华发布的中英双语双向密集模型，具有1300亿个参数，使用通用语言模型（GLM）算法进行预训练。它旨在支持在单台 A100（40G * 8）或V100（32G * 8）服务器上支持 130B 参数的推理任务。 |
| XWin-LM | 一款基于Llama2微调的语言模型,成功在斯坦福AlpacaEval上击败了GPT-4,成为新的榜首模型 |
| XVERSE | 元象科技自主研发的支持多语言的大语言模型（Large Language Model），参数规模为650亿，底座模型 XVERSE-65B |
| XVERSE-256K | 最大支持 256K 的上下文窗口长度，约 25w 字的输入内容，可以协助进行文献总结、报告分析等任务 |
| ChatGLM3 | 智谱AI训练的第三代大型语言模型，它不仅能理解和生成人类语言，还能执行代码、调用工具，并以markdown格式进行响应 |
| ChatGLM2 | 具备强大的问答和对话功能，拥有最大32K上下文，并且在授权后可免费商用！ |
| ChatGLM | 清华开源的、支持中英双语的对话语言模型，使用了代码训练，指令微调和RLHF |
| Orion-14B-Base | 具有140亿参数的多语种大模型，该模型在一个包含2.5万亿token的多样化数据集上进行了训练，涵盖了中文、英语、日语、韩语等多种语言。 |
| Baichuan2 | 百川第二代也出第二个版本了，提供了7B/13B Base和chat的版本 |
| Baichuan | 百川智能开源7B大模型可商用免费 |
| ziya2 | 基于Llama2训练的ziya2它终于训练完了 |
| ziya | IDEA研究院在7B/13B llama上继续预训练+SFT+RM+PPO+HFTT+COHFT+RBRS |
| Qwen1.5-MoE-A2.7B | Qwen推出MOE版本，推理更快 |
| Qwen1.5 | 通义千问升级1.5，支持32K上文 |
| Qwen1-7B+14B+70B | 阿里开源，可商用，通义千问7B,14B,70B Base和chat模型 |
| InternLM2 7B+20B | 商汤的书生模型2支持200K |
| Yuan-2.0 | 浪潮发布Yuan2.0 2B，51B，102B |
| YI-200K | 元一智能开源超长200K的6B，34B模型 |
| YI | 元一智能开源34B，6B模型 |
| DeepSeek-MOE | 深度求索发布的DeepSeekMoE 16B Base和caht模型 |
| DeepSeek | 深度求索发布的7B，67B大模型 |
| LLama2-chinese | 没等太久中文预训练微调后的llama2它来了~ |
| YuLan-chat2 | 高瓴人工智能基于Llama-2中英双语继续预训练+指令微调/对话微调 |
| BlueLM | Vivo人工智能实验室开源大模型 |
| zephyr-7B | HuggingFace 团队基于 UltraChat 和 UltraFeedback 训练了 Zephyr-7B 模型 |
| Skywork | 昆仑万维集团·天工团队开源13B大模型可商用 |
| Chinese-LLaMA-Alpaca | 哈工大中文指令微调的LLaMA |
| Moss | 为复旦正名！开源了预训练，指令微调的全部数据和模型。可商用 |
| InternLM | 书生浦语在过万亿 token 数据上训练的多语千亿参数基座模型 |
| Aquila2 | 智源更新Aquila2模型系列包括全新34B |
| Aquila | 智源开源7B大模型可商用免费 |
| UltraLM系列 | 面壁智能开源UltraLM13B，奖励模型UltraRM，和批评模型UltraCM |
| PandaLLM | LLAMA2上中文wiki继续预训练+COIG指令微调 |
| XVERSE | 据说中文超越llama2的元象开源模型13B模型 |
| BiLLa | LLama词表·扩充预训练+预训练和任务1比1混合SFT+指令样本SFT三阶段训练 |
| Phoenix | 港中文开源凤凰和奇美拉LLM，Bloom基座，40+语言支持 |
| Wombat-7B | 达摩院开源无需强化学习使用RRHF对齐的语言模型, alpaca基座 |
| TigerBot | 虎博开源了7B 180B的模型以及预训练和微调语料 |
| Luotuo-Chinese-LLM | 冷子昂@商汤科技, 陈启源@华中师范大学以及李鲁鲁@商汤科技发起的中文大语言模型开源项目，包含了一系列大语言模型、数据、管线和应用 |
| OpenBuddy | Llama 多语言对话微调模型 |
| Chinese Vincuna | LLama 7B基座，使用Belle+Guanaco数据训练 |
| Linly | Llama 7B基座，使用belle+guanaco+pclue+firefly+CSL+newscommentary等7个指令微调数据集训练 |
| Firefly | 中文2.6B模型，提升模型中文写作，古文能力，待开源全部训练代码，当前只有模型 |
| Baize | 使用100k self-chat对话数据微调的LLama |
| BELLE | 使用ChatGPT生成数据对开源模型进行中文优化 |
| Chatyuan | chatgpt出来后最早的国内开源对话模型，T5架构是下面PromptCLUE的衍生模型 |
| PromptCLUE | 多任务Prompt语言模型 |
| PLUG | 阿里达摩院发布超大规模语言模型PLUG，上能写诗词歌赋、下能对答如流 |
| CPM2.0 | 智源发布CPM2.0 |

**读者福利：如果大家对大模型感兴趣，这套大模型学习资料一定对你有用**

**对于0基础小白入门：**

> 如果你是零基础小白，想快速入门大模型是可以考虑的。
>
> 一方面是学习时间相对较短，学习内容更全面更集中。
>   
> 二方面是可以根据这些资料规划好学习计划和方向。

包括：大模型学习线路汇总、学习阶段，大模型实战案例，大模型学习视频，人工智能、机器学习、大模型书籍PDF。带你从零基础系统性的学好大模型！

😝有需要的小伙伴，可以保存图片到
**wx扫描二v码**
免费领取【
`保证100%免费`
】🆓
  
![](https://i-blog.csdnimg.cn/blog_migrate/563e12006cbf48d0525a9c539d4a5ab2.png)

##### 👉AI大模型学习路线汇总👈

大模型学习路线图，整体分为7个大的阶段：
**（全套教程文末领取哈）**
  
![                                                   ](https://i-blog.csdnimg.cn/blog_migrate/1dc8bc24c6d7992933d03140b841fe10.png#pic_center)
  
**第一阶段：**
从大模型系统设计入手，讲解大模型的主要方法；

**第二阶段：**
在通过大模型提示词工程从Prompts角度入手更好发挥模型的作用；

**第三阶段：**
大模型平台应用开发借助阿里云PAI平台构建电商领域虚拟试衣系统；

**第四阶段：**
大模型知识库应用开发以LangChain框架为例，构建物流行业咨询智能问答系统；

**第五阶段：**
大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型；

**第六阶段：**
以SD多模态大模型为主，搭建了文生图小程序案例；

**第七阶段：**
以大模型平台应用与开发为主，通过星火大模型，文心大模型等成熟大模型构建大模型行业应用。

#### 👉大模型实战案例👈

光学理论是没用的，要学会跟着一起做，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/31b6d0dd95b8d8fec9917b554a3cd9c9.jpeg#pic_center)

#### 👉大模型视频和PDF合集👈

观看零基础学习书籍和视频，看书籍和视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/08182a747176e52c2e20185242400093.png#pic_center)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/229901358c1a2ade26339ff0af840e62.png#pic_center)

#### 👉学会后的收获：👈

**• 基于大模型全栈工程实现**
（前端、后端、产品经理、设计、数据分析等），通过这门课可获得不同能力；

**• 能够利用大模型解决相关实际项目需求：**
大数据时代，越来越多的企业和机构需要处理海量数据，利用大模型技术可以更好地处理这些数据，提高数据分析和决策的准确性。因此，掌握大模型应用开发技能，可以让程序员更好地应对实际项目需求；

• 基于大模型和企业数据AI应用开发，
**实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能，**
学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握；

**• 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力：**
大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。

#### 👉获取方式：

😝有需要的小伙伴，可以保存图片到
**wx扫描二v码**
免费领取【
`保证100%免费`
】🆓
  
![](https://i-blog.csdnimg.cn/blog_migrate/563e12006cbf48d0525a9c539d4a5ab2.png)