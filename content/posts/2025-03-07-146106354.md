---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36353330353134322f:61727469636c652f64657461696c732f313436313036333534"
layout: post
title: "LLM论文笔记-19-On-Limitations-of-the-Transformer-Architecture"
date: 2025-03-07 22:43:26 +08:00
description: "CoT 只能通过增加大量的 token 来弥补 Transformer 的计算瓶颈，而不能从根本上提升 Transformer 的计算能力。注：本系列不包括基础的知识点讲解，为笔记/大纲性质而非教程，用于论文知识点和思想和快速记忆和回顾，更多细节建议阅读论文原文。3. CoT 可以减少 Transformer 计算错误的概率，但无法根本性突破其计算能力的上限。参考：https://zhuanlan.zhihu.com/p/682254725。2. Transformer 的计算能力受限于。"
keywords: "LLM论文笔记 19: On Limitations of the Transformer Architecture"
categories: ['大模型论文阅读']
tags: ['语言模型', '论文阅读', '自然语言处理', '笔记', '深度学习', '人工智能', 'Transformer']
artid: "146106354"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146106354
    alt: "LLM论文笔记-19-On-Limitations-of-the-Transformer-Architecture"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146106354
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146106354
cover: https://bing.ee123.net/img/rand?artid=146106354
image: https://bing.ee123.net/img/rand?artid=146106354
img: https://bing.ee123.net/img/rand?artid=146106354
---

# LLM论文笔记 19: On Limitations of the Transformer Architecture

> * Arxiv日期：2024.2.26
> * 机构：Columbia University / Google

### 关键词

* Transformer架构
* 幻觉问题
* 数学谜题

### 核心结论

1. Transformer 无法可靠地计算
**函数组合问题**

![](https://i-blog.csdnimg.cn/direct/2e0f915786634d95b9f9ff84db0216ae.png)

2. Transformer 的计算能力受限于
**信息瓶颈**

![](https://i-blog.csdnimg.cn/direct/9a249e096f2c476bb810a8fcd793aca4.png)

3. CoT 可以减少 Transformer 计算错误的概率，但无法根本性突破其计算能力的上限

4.
**CoT 需要生成指数级增长的 token。**
CoT 只能通过增加大量的 token 来弥补 Transformer 的计算瓶颈，而不能从根本上提升 Transformer 的计算能力。

![](https://i-blog.csdnimg.cn/direct/a80289195ad74a3d87047865bd93e6ba.png)

### 主要方法

核心目标是分析
**Transformer 在计算能力上的根本性限制**
，特别是在
**函数组合（Function Composition）、数学推理、逻辑推理**
等任务上的表现。

通过
**通信复杂度**
**（Communication Complexity）**
和
**计算复杂度（Computational Complexity）**
的分析

![](https://i-blog.csdnimg.cn/direct/e445bff138d845288d9f941ea80fa553.png)

![](https://i-blog.csdnimg.cn/direct/a909a8a76fa848589b972c3e387ca22c.png)

![](https://i-blog.csdnimg.cn/direct/7b997b6296f74def8946cd2b1b1d0ed7.png)

参考：https://zhuanlan.zhihu.com/p/682254725

> 注：本系列不包括基础的知识点讲解，为笔记/大纲性质而非教程，用于论文知识点和思想和快速记忆和回顾，更多细节建议阅读论文原文