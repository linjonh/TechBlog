---
layout: post
title: "支持向量机SVM学习笔记"
date: 2025-08-26T14:05:02+0800
description: "文章摘要 SVM（支持向量机）的核心目标是寻找最优划分超平面，以最大化分类间隔，提升模型泛化能力。其关键概念包括超平面（n-1维子空间）、支持向量（决定间隔的样本点）及间隔计算。通过拉格朗日乘子法将约束优化转化为对偶问题求解，确定超平面参数。针对复杂数据，引入软间隔（松弛因子）处理噪声，调整惩罚参数C平衡准确性与容错性；利用核函数（如高斯核）将低维不可分数据映射到高维空间，避免直接计算高维内积，有效解决非线性分类问题。SVM的优化目标与扩展方法使其兼具鲁棒性与灵活性。"
keywords: "支持向量机（SVM）学习笔记"
categories: ['未分类']
tags: ['机器学习', '支持向量机', '人工智能']
artid: "150853661"
arturl: "https://blog.csdn.net/2402_88703635/article/details/150853661"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150853661
    alt: "支持向量机SVM学习笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150853661
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150853661
cover: https://bing.ee123.net/img/rand?artid=150853661
image: https://bing.ee123.net/img/rand?artid=150853661
img: https://bing.ee123.net/img/rand?artid=150853661
---



# 支持向量机（SVM）学习笔记

### 一、SVM 核心目标与基本概念

#### （一）核心需求

在样本所在的空间里，找到一个能把不同类别样本分开的 “划分超平面”，这是 SVM 最基础的任务。

#### （二）理想超平面的标准

理想的划分超平面，要能对训练样本的局部扰动有最好的 “容忍性”。简单说，就算训练样本有微小的变动，这个超平面依然能准确分类，不容易因为样本的小波动就出现分类错误。

#### （三）关键概念解释

1. **超平面**
   * 本质：可以理解为从 n 维空间 “切” 出来的 n-1 维子空间，它由一个 n 维的方向向量（法向量）和一个代表位置的实数（截距）共同决定。
   * 维度对应关系：
     + 在我们熟悉的三维空间（比如日常生活中的立体空间）里，超平面就是二维的平面；
     + 在二维空间（比如一张纸的平面）里，超平面就是一维的直线。
2. **间隔（Margin）**
   * 定义：两类样本中，距离划分超平面最近的那些点，到超平面距离的 2 倍。
   * SVM 的核心优化方向：尽可能让这个间隔变大。因为间隔越大，超平面对样本变动的承受能力越强，模型在新样本上的分类效果（泛化能力）通常也越好。
3. **支持向量**
   * 定义：就是那些距离划分超平面最近的样本点，它们是决定间隔大小的关键。模型训练完成后，只有支持向量会影响超平面的位置和方向，其他样本不管怎么变动，只要不变成新的支持向量，就不会改变超平面。
4. **点到超平面的距离**
   * 可以类比我们学过的 “点到直线的距离”：在二维空间里，能计算点到直线的距离；推广到 n 维空间，就能计算样本点到超平面的距离，这个距离是判断样本与超平面远近的核心指标。

### 二、SVM 优化目标与求解思路

#### （一）数据与决策规则定义

1. **数据集标签**：假设我们有一组训练数据，每个数据都有对应的类别标签 —— 如果是正类样本，标签记为 + 1；如果是负类样本，标签记为 - 1。
2. **决策规则**：用一个函数来判断新样本的类别，这个函数会先对原始样本做一些特征变换（后面会讲为什么要变换），再结合超平面的参数，最终输出分类结果。

#### （二）优化目标转化

1. **初始目标**：找到超平面的参数（法向量和截距），让离超平面最近的点（支持向量）到超平面的距离最大。为了方便计算，我们会对参数做一些调整，让支持向量刚好满足 “到超平面的距离对应特定值”，从而把 “最大化距离” 的目标，转化为 “最小化某个关于参数的表达式”。
2. **核心约束**：所有样本都要在超平面的正确一侧 —— 正类样本在超平面的一边，负类样本在另一边，通过样本标签和决策函数的结果可以保证这一点。

#### （三）拉格朗日乘子法求解

1. **构建辅助函数**：针对 “在约束条件下求最小值” 这种问题，引入 “拉格朗日乘子” 构建一个辅助函数（拉格朗日函数），把带约束的优化问题转化为无约束的问题。
2. **转化对偶问题**：通过对辅助函数中的参数求偏导并令其等于 0，得到两个关键条件，再把这两个条件代入辅助函数，将原问题转化为 “对偶问题”—— 这种转化能让求解过程更简单，尤其是后续引入核函数时优势更明显。
3. **求解关键参数**：通过求解对偶问题，得到拉格朗日乘子的值，再结合之前得到的条件，就能算出超平面的法向量和截距，最终确定划分超平面。

### 三、SVM 求解实例（简化版）

1. **问题简化**：针对一组小规模的训练数据，先根据对偶问题的思路，把目标函数展开并简化。过程中会利用 “样本标签之和相关的约束”，减少需要求解的变量数量。
2. **寻找最优解**：对简化后的目标函数，计算相关变量的偏导数并令其为 0，找到可能的解。但有些解会不满足 “参数非负” 的约束，所以需要在约束边界上寻找符合条件的最优解。
3. **计算超平面参数**：根据找到的最优解，计算超平面的法向量和截距，最终写出超平面的表达式，完成分类模型的构建。

### 四、SVM 的扩展：应对复杂数据

#### （一）软间隔：处理噪声数据

1. **问题背景**：现实中的数据往往存在 “噪声点”（比如标注错误或异常的样本）。如果强行要求所有样本都严格符合 “在超平面正确一侧且距离满足要求”，会导致超平面过于 “迁就” 噪声点，反而在新样本上分类效果差（过拟合）。
2. **引入松弛因子**：为了解决这个问题，引入 “松弛因子”—— 允许少数样本不严格满足原始约束，松弛因子的大小代表样本偏离约束的程度。
3. **调整目标函数**：在原有的目标函数里，增加一项与松弛因子相关的惩罚项，惩罚项的权重由参数 C 控制：
   * 如果 C 很大：意味着对噪声的容忍度低，不允许太多样本偏离约束，要求分类尽可能准确；
   * 如果 C 很小：意味着对噪声的容忍度高，允许更多样本偏离约束，更看重间隔的大小。
4. **求解调整**：构建新的拉格朗日函数时，会结合松弛因子的约束，最终得到的参数约束会比原来多一个 “上限”，其他求解逻辑和之前一致。

#### （二）核函数：处理低维不可分数据

1. **问题背景**：有些数据在当前的低维空间里，无论怎么画超平面都无法把两类样本分开（比如呈环形分布的数据）。这时可以考虑把数据 “搬到” 更高维的空间，在高维空间里，这些样本可能就变得可以用超平面分开了。
2. **维度灾难问题**：直接把数据映射到高维空间，会面临 “计算量爆炸” 的问题 —— 高维空间里计算样本之间的内积（求解 SVM 的关键步骤），运算量会随着维度的增加急剧上升，甚至无法实现。
3. **核函数的作用**：引入 “核函数” 可以解决这个问题。核函数能直接在低维空间里，计算出数据映射到高维空间后的内积结果，不用真的把数据映射到高维，既达到了高维映射的效果，又避免了复杂的高维计算。
4. **常见核函数**
   * **线性核函数**：适用于本身在低维空间就能分开的数据，本质上和原始的线性 SVM 一样，计算简单。
   * **高斯核函数（RBF 核）**：适用范围很广，能把数据映射到无穷维空间，即使是复杂的非线性分布数据，也能通过它找到合适的超平面，灵活性非常强。
5. **实例验证**：以一组简单的三维数据为例，若将其映射到九维空间，直接计算高维内积会很繁琐，但用特定的核函数（如基于内积平方的核函数），在三维空间里就能快速算出和高维内积相同的结果，充分体现了核函数的优势。



