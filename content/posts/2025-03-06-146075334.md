---
layout: post
title: "大模型工程师学习日记十五Hugging-Face-模型微调训练基于-BERT-的中文评价情感分析"
date: 2025-03-06 17:33:53 +0800
description: "4.2 数据集信息 加载数据集后，可以查看数据集的基本信息，如数据集大小、字段名称等。3. 模型微调的基本概念与流程 微调是指在预训练模型的基础上，通过进一步的训练来适应特定的下游任务。微调过程中，通 常冻结 BERT 的预训练层，只训练与下游任务相关的层。初始化时，需要根据下游任务的需求，定义合适的输出维度。6. vocab 字典操作 在微调 BERT 模型之前，需要将模型的词汇表（vocab）与数据集中的文本匹配。DataLoader 自动处理数据的批处理和随机打乱，确保训练的高 效性和数据的多样性。"
keywords: "大模型工程师学习日记（十五）：Hugging Face 模型微调训练（基于 BERT 的中文评价情感分析）"
categories: ['未分类']
tags: ['自然语言处理', '深度学习', '学习', '全量微调', '人工智能', 'Bert']
artid: "146075334"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146075334
    alt: "大模型工程师学习日记十五Hugging-Face-模型微调训练基于-BERT-的中文评价情感分析"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146075334
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146075334
cover: https://bing.ee123.net/img/rand?artid=146075334
image: https://bing.ee123.net/img/rand?artid=146075334
img: https://bing.ee123.net/img/rand?artid=146075334
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     大模型工程师学习日记（十五）：Hugging Face 模型微调训练（基于 BERT 的中文评价情感分析）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     1. datasets 库核心方法
    </h2>
    <h3>
     1.1. 列出数据集 使用 d atasets 库，你可以轻松列出所有 Hugging Face 平台上的数据集：
    </h3>
    <pre><code class="language-python">from datasets import list_datasets
 # 列出所有数据集
all_datasets = list_datasets()
 print(all_datasets)
</code></pre>
    <h3>
     1.2. 加载数据集 你可以通过 load_dataset 方法加载任何数据集：
    </h3>
    <pre><code class="language-python">from datasets import load_dataset
 # 加载GLUE数据集
dataset = load_dataset("glue", "mrpc")
 print(dataset)</code></pre>
    <h3>
     1.3. 加载磁盘数据 你可以加载本地磁盘上的数据：
    </h3>
    <pre><code class="language-python">from datasets import load_from_disk
 # 从本地磁盘加载数据集
dataset = load_from_disk("./my_dataset")
 print(dataset)</code></pre>
    <h2>
     2. 分词工具与文字编码 2.1. 加载字典和分词工具 你可以使用 AutoTokenizer 自动加载分词工具：
    </h2>
    <pre><code class="language-python">from transformers import AutoTokenizer
 # 加载中文BERT模型的分词器
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")</code></pre>
    <h3>
     2.1. 批量编码句子 使用分词器，你可以批量对文本进行编码：
    </h3>
    <pre><code class="language-python"># 批量编码句子
sentences = ["我爱自然语言处理", "Hugging Face 很强大"]
 encoded_inputs = tokenizer(sentences, padding=True, truncation=True, 
return_tensors="pt")
 print(encoded_inputs)</code></pre>
    <h2>
     3. 模型微调的基本概念与流程
    </h2>
    <p>
     微调是指在预训练模型的基础上，通过进一步的训练来适应特定的下游任务。BERT 模型通过预训练来 学习语言的通用模式，然后通过微调来适应特定任务，如情感分析、命名实体识别等。微调过程中，通 常冻结 BERT 的预训练层，只训练与下游任务相关的层。本课件将介绍如何使用 BERT 模型进行情感分 析任务的微调训练。
    </p>
    <h2>
     4. 加载数据集 情感分析任务的数据通常包括文本及其对应的情感标签。使用 Hugging Face 的 datasets 库可以轻松地 加载和处理数据集
    </h2>
    <pre><code class="language-python">from datasets import load_dataset
 # 加载数据集
dataset = load_dataset('csv', data_files="data/ChnSentiCorp.csv")
 # 查看数据集信息
print(dataset)</code></pre>
    <h3>
     4.1 数据集格式
    </h3>
    <p>
     Hugging Face 的 datasets 库支持多种数据集格式，如 CSV、JSON、TFRecord 等。在本案例中，使用 CSV 格式，CSV 文件应包含两列：一列是文本数据，另一列是情感标签。
    </p>
    <h3>
     4.2 数据集信息
    </h3>
    <p>
     加载数据集后，可以查看数据集的基本信息，如数据集大小、字段名称等。这有助于我们了解数据的分 布情况，并在后续步骤中进行适当的处理。
    </p>
    <h2>
     5. 制作 Dataset 加载数据集后，需要对其进行处理以适应模型的输入格式。这包括数据清洗、格式转换等操作。
    </h2>
    <pre><code class="language-python">加载数据集后，需要对其进行处理以适应模型的输入格式。这包括数据清洗、格式转换等操作。
from datasets import Dataset
 # 制作 Dataset
 dataset = Dataset.from_dict({
 'text': ['位置尚可，但距离海边的位置比预期的要差的多', '5月8日付款成功，当当网显示5月10
日发货，可是至今还没看到货物，也没收到任何通知，简不知怎么说好！！！', '整体来说，本书还是不错
的。至少在书中描述了许多现实中存在的司法系统方面的问题，这是值得每个法律工作者去思考的。尤其是让
那些涉世不深的想加入到律师队伍中的年青人，看到了社会特别是中国司法界真实的一面。缺点是：书中引用
了大量的法律条文和司法解释，对于已经是律师或有一定工作经验的法律工作者来说有点多余，而且所占的篇
幅不少，有凑字数的嫌疑。整体来说还是不错的。不要对一本书提太高的要求。'],
 'label': [0, 1, 1]  # 0 表示负向评价，1 表示正向评价
})
 # 查看数据集信息
print(dataset)</code></pre>
    <h3>
     5.1 数据集字段
    </h3>
    <p>
     在制作 Dataset 时，需定义数据集的字段。在本案例中，定义了两个字段： text （文本）和 label （情感标签）。每个字段都需要与模型的输入和输出匹配。
    </p>
    <h3>
     5.2 数据集信息
    </h3>
    <p>
     制作 Dataset 后，可以通过 dataset.info 等方法查看其大小、字段名称等信息，以确保数据集的正确 性和完整性。
    </p>
    <h2>
     6. vocab 字典操作
    </h2>
    <p>
     在微调 BERT 模型之前，需要将模型的词汇表（vocab）与数据集中的文本匹配。这一步骤确保输入的 文本能够被正确转换为模型的输入格式。
    </p>
    <pre><code class="language-python">from transformers import BertTokenizer
 # 加载 BERT 模型的 vocab 字典
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
 # 将数据集中的文本转换为 BERT 模型所需的输入格式
dataset = dataset.map(lambda x: tokenizer(x['text'], return_tensors="pt"), 
batched=True)
 # 查看数据集信息
print(dataset)</code></pre>
    <h3>
     6.1 词汇表（vocab）
    </h3>
    <p>
     BERT 模型使用词汇表（vocab）将文本转换为模型可以理解的输入格式。词汇表包含所有模型已知的单 词及其对应的索引。确保数据集中的所有文本都能找到对应的词汇索引是至关重要的。
    </p>
    <h3>
     6.2 文本转换
    </h3>
    <p>
     使用 tokenizer 将文本分割成词汇表中的单词，并转换为相应的索引。此步骤需要确保文本长度、特殊 字符处理等都与 BERT 模型的预训练设置相一致。
    </p>
    <h2>
     7. 下游任务模型设计
    </h2>
    <p>
     在微调 BERT 模型之前，需要设计一个适应情感分析任务的下游模型结构。通常包括一个或多个全连接 层，用于将 BERT 输出的特征向量转换为分类结果。
    </p>
    <pre><code class="language-python">from transformers import BertModel
 import torch.nn as nn
 class SentimentAnalysisModel(nn.Module):
 def __init__(self):
 super().__init__()
 self.bert = BertModel.from_pretrained('bert-base-chinese')
 self.drop_out = nn.Dropout(0.3)
 self.linear = nn.Linear(768, 2)  # 假设情感分类为二分类
def forward(self, input_ids, attention_mask):
 _, pooled_output = self.bert(
 input_ids=input_ids,
 attention_mask=attention_mask,
 return_dict=False
 )
 output = self.drop_out(pooled_output)
 return self.linear(output)</code></pre>
    <h3>
     7.1 模型结构
    </h3>
    <p>
     下游任务模型通常包括以下几个部分： BERT 模型：用于生成文本的上下文特征向量。 Dropout 层：用于防止过拟合，通过随机丢弃一部分神经元来提高模型的泛化能力。 全连接层：用于将 BERT 的输出特征向量映射到具体的分类任务上。
    </p>
    <h3>
     7.2 模型初始化
    </h3>
    <p>
     使用 接层。初始化时，需要根据下游任务的需求，定义合适的输出维度。 BertModel.from_pretrained() 方法加载预训练的 BERT 模型，同时也可以初始化自定义的全连
    </p>
    <h2>
     8. 自定义模型训练
    </h2>
    <p>
     模型设计完成后，进入训练阶段。通过数据加载器（DataLoader）高效地批量处理数据，并使用优化器 更新模型参数
    </p>
    <pre><code class="language-python">from torch.utils.data import DataLoader
 from transformers import AdamW
 # 实例化 DataLoader
 data_loader = DataLoader(dataset, batch_size=16, shuffle=True)
 # 初始化模型和优化器
model = SentimentAnalysisModel()
 optimizer = AdamW(model.parameters(), lr=5e-5)
 # 训练循环
for epoch in range(3):  # 假设训练 3 个 epoch
 model.train()
 for batch in data_loader:
 optimizer.zero_grad()
 outputs = model(input_ids=batch['input_ids'], 
attention_mask=batch['attention_mask'])
 loss = nn.CrossEntropyLoss()(outputs, batch['labels'])
 loss.backward()
 optimizer.step()</code></pre>
    <h3>
     8.1 数据加载
    </h3>
    <p>
     使用 DataLoader 实现批量数据加载。 DataLoader 自动处理数据的批处理和随机打乱，确保训练的高 效性和数据的多样性。
    </p>
    <h3>
     8.2 优化器
    </h3>
    <p>
     AdamW 是一种适用于 BERT 模型的优化器，结合了 Adam 和权重衰减的特点，能够有效地防止过拟合。
    </p>
    <h3>
     8.3 训练循环
    </h3>
    <p>
     训练循环包含前向传播（forward pass）、损失计算（loss calculation）、反向传播（backward pass）、参数更新（parameter update）等步骤。每个 epoch 都会对整个数据集进行一次遍历，更新 模型参数。通常训练过程中会跟踪损失值的变化，以判断模型的收敛情况。
    </p>
    <h2>
     9. 最终效果评估与测试
    </h2>
    <p>
     在模型训练完成后，加载模型训练权重测试其效果：
    </p>
    <p>
     <img alt="" height="487" src="https://i-blog.csdnimg.cn/direct/fd44c04ca3f04f5a870e7851cb0bb46b.png" width="920"/>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f37353236393630302f:61727469636c652f64657461696c732f313436303735333334" class_="artid" style="display:none">
 </p>
</div>


