---
layout: post
title: "NN神经网络"
date: 2025-03-16 09:47:02 +0800
description: "预测好坏的判断标准：激活函数是为了得出结果，损失函数是为了让结果最佳ps：梯度下降法的步长（学习率lr）是一个超参数。"
keywords: "NN：神经网络"
categories: ['Deeplearning']
tags: ['神经网络', '深度学习', '人工智能']
artid: "146290983"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146290983
    alt: "NN神经网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146290983
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146290983
cover: https://bing.ee123.net/img/rand?artid=146290983
image: https://bing.ee123.net/img/rand?artid=146290983
img: https://bing.ee123.net/img/rand?artid=146290983
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     NN：神经网络
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     预测好坏的判断标准：
    </p>
    <p>
     三个公设（对f(X)函数值修饰后进行判断）：
    </p>
    <ol>
     <li>
      间隔最大
     </li>
     <li>
      似然值（w变化，不是x）（概率）最大&lt;–&gt;交叉熵最大
     </li>
     <li>
      方差最小
     </li>
    </ol>
    <p>
     激活函数是为了得出结果，损失函数是为了让结果最佳
    </p>
    <p>
     机器学习三大要素：
    </p>
    <ol>
     <li>
      模型（隐藏层）
     </li>
     <li>
      策略（公设）（输出层）
     </li>
     <li>
      算法（反向传播，关键为梯度下降法）
     </li>
    </ol>
    <p>
     <code>
      ps
     </code>
     ：梯度下降法的步长（学习率lr）是一个超参数
    </p>
    <h4>
     <a id="_18">
     </a>
     神经网络：
    </h4>
    <h5>
     <a id="1__20">
     </a>
     1. 可看做数据升维
    </h5>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/3db376df2b8a4928a5cd47236a0e87b8.png"/>
    </p>
    <ol>
     <li>
      输入层（数据处理）数据的维度
     </li>
    </ol>
    <p>
     一个平面的原因是，没激活函数的情况下，w1x1+w2x2+b=z是个线性函数
     <br/>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/fa76b2e9eefc495cb60ce41785c68c20.png"/>
    </p>
    <p>
     分界线：二维平面里的线到n维空间里的超平面
    </p>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/cf4c0a91748a482d91a75af9c523147a.png"/>
    </p>
    <p>
     <code>
      ps
     </code>
     ：
    </p>
    <ul>
     <li>
      <p>
       激活函数，非线性
      </p>
     </li>
     <li>
      <p>
       权重weight和偏置bias
      </p>
     </li>
     <li>
      <p>
       神经网络的复杂性来源于激活函数
      </p>
     </li>
    </ul>
    <ol start="2">
     <li>
      隐藏层（让模型更复杂）
     </li>
    </ol>
    <p>
     中间的神经元个数可以对数据进行升维操作，再找到一个超平面对数据进行划分
    </p>
    <p>
     <code>
      ps
     </code>
     ：升维操作，完成维度的映射
    </p>
    <ol start="3">
     <li>
      输出层
     </li>
    </ol>
    <p>
     神经网络可以有多个输出节点，处理多分类问题
    </p>
    <p>
     多分类问题，本质上是多个二分类问题，每个节点都在进行二分类判断
    </p>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/c6485053ea4344128763a8ddfcbd986f.png"/>
    </p>
    <p>
     输出结果使用softmax描述，进行归一处理（各分类的概率分布）
    </p>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/083a2092e63c492f805e6cbd5468ac0f.png"/>
    </p>
    <p>
     softmax可以看做是sigmoid函数的扩展和升级
    </p>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/8dfb7eb550bd420b9b9f148cee7fe937.png"/>
    </p>
    <p>
     分母是所有分类数值的和，分子是各个分类自己的数值，计算出的是各分类的概率（归一）
    </p>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/c92828ae24b5440a9b6777c7ec8afe7e.png"/>
    </p>
    <h5>
     <a id="_73">
     </a>
     可看做数据降维
    </h5>
    <p>
     <img alt="外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传" src="https://i-blog.csdnimg.cn/direct/e3a3dc13f98d429fb273638f7f34e0dc.png"/>
    </p>
    <p>
     进行数据的降维操作，提取特征，不需要原始数据的所有维度
    </p>
    <p>
     隐藏层越深，抽象程度越高
    </p>
    <p>
     王木头up主哔站视频：
    </p>
    <p>
     <a href="https://www.bilibili.com/video/BV1FP411j7oW/?vd_source=fe62ed91e1ba23d7af413bab2cc65903" rel="nofollow">
      学习分享一年，对神经网络的理解全都在这40分钟里了
     </a>
    </p>
    <p>
     梯度消失是由于深层神经网络在链式求导时，连乘项的绝对值小于1导致的（例如使用sigmoid激活函数时，远离原点处的梯度接近于0），不是由于学习率的高次方导致的。
     <br/>
     梯度消失是由于深层神经网络在链式求导时，连乘项的绝对值小于1导致的（例如使用sigmoid激活函数时，远离原点处的梯度接近于0），不是由于学习率的高次方导致的。
     <br/>
     并且在梯度下降过程中更新梯度时，每一层参数的梯度项乘的是同一个学习率，不存在次方的情况。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6e697369646a6e646e2f:61727469636c652f64657461696c732f313436323930393833" class_="artid" style="display:none">
 </p>
</div>


