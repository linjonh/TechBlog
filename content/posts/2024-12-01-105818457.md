---
layout: post
title: "机器学习LifeLong-Learning终身学习介绍"
date: 2024-12-01 07:31:26 +0800
description: "下面的文章转自（已获作者允许）：2020机器学习前沿技术----LifeLong learning "
keywords: "lifelong learning"
categories: ['Machine', 'Learning']
tags: ['深度学习', '机器学习', '数据挖掘']
artid: "105818457"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=105818457
    alt: "机器学习LifeLong-Learning终身学习介绍"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=105818457
featuredImagePreview: https://bing.ee123.net/img/rand?artid=105818457
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【机器学习】LifeLong Learning（终身学习）介绍
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     下面的文章转自（已获作者允许）：
     <br/>
     <a href="https://zhuanlan.zhihu.com/p/119324185" rel="nofollow">
      2020机器学习前沿技术----LifeLong learning - stephon的文章 - 知乎
     </a>
     <br/>
     文章介绍了关于机器学习比较前沿的一项技术，也就是LifeLong Learning。这篇文章也对LifeLong Learning与Multi-task Learning以及Transfer Learning进行了对比，所以我将其进行了转载，其中对其排版进行了一定修改。
    </p>
    <p>
    </p>
    <div class="toc">
     <h4>
      文章目录
     </h4>
     <ul>
      <li>
       <ul>
        <li>
         <ul>
          <li>
           <ul>
            <li>
             <a href="#1_Lifelong_learning_6" rel="nofollow">
              1. 为什么需要研究Lifelong learning?
             </a>
            </li>
            <li>
             <a href="#2__19" rel="nofollow">
              2. 有哪些别称？
             </a>
            </li>
            <li>
             <a href="#3__27" rel="nofollow">
              3. 有哪些处理方法及研究成果
             </a>
            </li>
            <li>
             <ul>
              <li>
               <a href="#a_EWC_Elastic_Weight_Consolidation_35" rel="nofollow">
                a) EWC: Elastic Weight Consolidation
               </a>
              </li>
              <li>
               <a href="#b_Multitask_learning_with_some_modify_37" rel="nofollow">
                b) Multi-task learning with some modify
               </a>
              </li>
              <li>
               <a href="#c_Model_Expansionbut_parameter_efficiency_47" rel="nofollow">
                c) Model Expansion(but parameter efficiency)
               </a>
              </li>
             </ul>
            </li>
            <li>
             <a href="#4_Multitask_learning_Transfer_Learning__LifeLong_learning_55" rel="nofollow">
              4. Multi-task learning, Transfer Learning ， LifeLong learning之间的区别
             </a>
            </li>
            <li>
             <a href="#5_Lifelong_Learning_61" rel="nofollow">
              5. 如何评价Life-long Learning的好坏
             </a>
            </li>
            <li>
             <a href="#6__73" rel="nofollow">
              6. 这个领域还有哪些前沿的研究方向
             </a>
            </li>
           </ul>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </div>
    <p>
    </p>
    <h5>
     <a id="1_Lifelong_learning_6">
     </a>
     1. 为什么需要研究Lifelong learning?
    </h5>
    <p>
     细数机器学习处理的问题，概括得讲，可以分为如下几大类：
    </p>
    <ul>
     <li>
      计算机视觉（CV）： object classification, object detection, object segmentation, style transfer, denoising, image generation, image caption
     </li>
     <li>
      语音（Speech) : speech recogniton， speech synthesis
     </li>
     <li>
      自然语言处理（NLP): Machine translation, text classfication， emotional recogniton
     </li>
     <li>
      推荐系统: Recommendation, CRT
     </li>
    </ul>
    <p>
     目前针对各个大类的不同子类问题， 都会去设计不同的网络结构，设计不同的loss, 采用不同的数据集去处理。 这使得机器学习“偏科严重”，比如前几年很火的AlphaGo, 虽然他是一个“围棋天才”，但是一旦让他去下象棋， 他就歇菜了。换句话说， 目前的人工智能，只能处理给定的任务，换一个任务就无能为力了，这距离我们所想达到的通用智能实在相差甚远。
    </p>
    <p>
     反观人脑，在人不断成长的过程中，他可以学习各种各样的技能。不仅会下棋，还会踢球，还会辩论等等，而人的大脑只有一个(相当于自始至终只有一个网络）。 虽然随着时间的流逝，以前学习的东西会渐渐淡忘，但这丝毫不影响人脑在不断学习，胜任一个又一个任务中所表现出来的强大。
    </p>
    <p>
     因此，我们是否可以只用一个网络结构（注意，这里的网络结构并非是固定的。也许随着任务的需要，得自行扩展网络），在不同的任务上分别训练，使得该网络能够胜任所有的任务呢。 这就是Life-long learning 所要研究的课题。
    </p>
    <h5>
     <a id="2__19">
     </a>
     2. 有哪些别称？
    </h5>
    <ul>
     <li>
      <p>
       Continuous Learning
      </p>
     </li>
     <li>
      <p>
       Never ending Learning
      </p>
     </li>
     <li>
      <p>
       Incremental Learning
      </p>
     </li>
    </ul>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/5d876ae7b723da6ac1dba0f762dfa81e.jpeg#pic_center"/>
    </p>
    <h5>
     <a id="3__27">
     </a>
     3. 有哪些处理方法及研究成果
    </h5>
    <p>
     <strong>
      Knowledge Retention（but Not Intransigence） and Knowledge Transfer
     </strong>
     知识保留(但不妥协）及知识迁移。 这意味着以前学到的知识，需要能够促进下一个任务Task B的学习； 学完Task B之后，要不仅可以很好得处理Task B, 同时在Task A 上不能有明显的下降， 最好Task A的性能，也能够有所提升或者是保持。
    </p>
    <p>
     一个遗忘的例子：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/22cc0b443d0270b36db3b4d998dfecad.jpeg#pic_center">
      <strong>
       How to solve Catastrophic Forgetting
      </strong>
      Little forgetting是允许的。
     </img>
    </p>
    <p>
     一些理论成果：
    </p>
    <h6>
     <a id="a_EWC_Elastic_Weight_Consolidation_35">
     </a>
     a) EWC: Elastic Weight Consolidation
    </h6>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/c71a653af35c491d9331c1121fd42e17.jpeg#pic_center">
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/d67f710b4ced5f2b501a56642feca688.jpeg#pic_center"/>
     </img>
    </p>
    <h6>
     <a id="b_Multitask_learning_with_some_modify_37">
     </a>
     b) Multi-task learning with some modify
    </h6>
    <p>
     刚才提到的问题，如果采用Multi-task learning：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/231746e27d0c5ecd59fa5c5be2822937.jpeg#pic_center">
      可以看到： 如果先学习Task1, 再学习Task2; 那么当Task 2学完之后， Task 1的准确率就下降了很多， 这就是Forget!!! 这就是Transfer Learning所带来的问题， 也是Lifelong Learning 所需要解决的问题。
     </img>
    </p>
    <p>
     如果采用Multi-task Learning，需要把Task 1 和 Task2的训练数据都存储起来，需要的时候放在一起训练。 虽然可以看到Task 1和 Task 2都达到了不错的效果， 但是所有数据的存储会是一个很大的问题。
    </p>
    <p>
     <strong>
      因此， 如何解决数据存储的问题呢？
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/d7d273be0a932af103a1b55d33314931.jpeg#pic_center">
      借助于GAN网络来完成， 减少了数据存储，但带来了更大的计算量。
     </img>
    </p>
    <h6>
     <a id="c_Model_Expansionbut_parameter_efficiency_47">
     </a>
     c) Model Expansion(but parameter efficiency)
    </h6>
    <p>
     Progressive Neural Network
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/675590edc4921519955a4b97211e8b0b.jpeg#pic_center">
      <br/>
      Net2Net
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/8569822bb6fbaf2ace01693a0c5db31d.jpeg#pic_center"/>
     </img>
    </p>
    <h5>
     <a id="4_Multitask_learning_Transfer_Learning__LifeLong_learning_55">
     </a>
     4. Multi-task learning, Transfer Learning ， LifeLong learning之间的区别
    </h5>
    <p>
     <strong>
      Transfer Learning VS LifeLong Learning:
     </strong>
     Transfer learning只考虑在当前任务上的效果； 而LifeLong Learning需要考虑在所有任务上的效果。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/9afa4289ac086d216a566db2400f7927.jpeg#pic_center"/>
     <strong>
      Multi-task Learning VS LifeLong Learning :
     </strong>
     LifeLong Learning训练时只用当前任务的数据； 而Multi-task Learning会用到之前所有任务的数据。这带来了数据存储以及计算量不断增大的问题； Multi-task learning可以看作是LifeLong learning的upper bound
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/2bb1f49e4188bf9b70a97aa55ea1ddd6.jpeg#pic_center"/>
    </p>
    <h5>
     <a id="5_Lifelong_Learning_61">
     </a>
     5. 如何评价Life-long Learning的好坏
    </h5>
    <p>
     3个指标：
    </p>
    <ul>
     <li>
      Accuracy 表征N个任务学完后总体的性能；
     </li>
     <li>
      Backward Transfer: 表征N个任务学完后，总体遗忘的程度；通常为负数，越大越好；
     </li>
     <li>
      Forward Transfer: 表征N个任务学完后，总体学习的程度；通常为正数，越大越好；
     </li>
    </ul>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/cad601e37174b60c7d843efc3c533f6b.jpeg#pic_center"/>
     R(0, i) 表示在随机初始化的情况下，在Task i上的准确率。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/bedc40e601ba429b9034f640a0d4757b.jpeg#pic_center"/>
    </p>
    <h5>
     <a id="6__73">
     </a>
     6. 这个领域还有哪些前沿的研究方向
    </h5>
    <p>
     <strong>
      Curriculum Learning
     </strong>
     类似于一个课程系的学习， 研究的是如何安排课程学习的先后顺序。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/54c93f2a6250c9ebfe10cc48b3bab942.jpeg#pic_center"/>
    </p>
    <p>
     <strong>
      Taskonomy
     </strong>
     研究的是各个任务之间的关系， 即该先学哪个，后学哪个。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/8ddff7f022367dbd5ed8233e9b5c3ed9.jpeg#pic_center"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f626c:6f672e6373646e2e6e65742f4672616e6b696548656c6c6f2f:61727469636c652f64657461696c732f313035383138343537" class_="artid" style="display:none">
 </p>
</div>


