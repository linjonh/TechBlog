---
layout: post
title: "尚硅谷爬虫note16"
date: 2025-03-10 14:12:49 +0800
description: "1. 安装scrapy终端中：pip install scrapy。"
keywords: "尚硅谷爬虫note16"
categories: ['未分类']
tags: ['爬虫']
artid: "146143719"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146143719
    alt: "尚硅谷爬虫note16"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146143719
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146143719
cover: https://bing.ee123.net/img/rand?artid=146143719
image: https://bing.ee123.net/img/rand?artid=146143719
img: https://bing.ee123.net/img/rand?artid=146143719
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     尚硅谷爬虫note16
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     一、crawlSpider
    </h2>
    <p>
     1. 安装scrapy
    </p>
    <p>
     终端中：pip install scrapy
    </p>
    <h3 style="background-color:transparent">
     2. 创建项目
    </h3>
    <p>
     1）创建项目
    </p>
    <p>
     scrapy startproject 项目名
    </p>
    <p>
     2）切换到spiders目录下
    </p>
    <p>
     cd 项目名\项目名\spiders
    </p>
    <p>
     3）创建文件
    </p>
    <p>
     scrapy genspider
     <span style="color:#fe2c24">
      <strong>
       -t crawl
      </strong>
     </span>
     文件名 网址
    </p>
    <p>
     4)运行
    </p>
    <p>
     scrapy crawl 文件名
    </p>
    <p>
    </p>
    <p>
     3. 对文件中的rule进行修改
    </p>
    <p>
     鼠标悬浮在地址页上——右键检查：
    </p>
    <p>
     <img alt="" height="394" src="https://i-blog.csdnimg.cn/direct/a2f95bb7bb0d474dbb04d5e54c4bbf0b.png" width="1348"/>
    </p>
    <p>
     对rule中的正则表达式allow进行修改： 修改完成后会提取当前起始url（
     <span style="white-space:pre-wrap">
      start_urls ）
      <span style="white-space:normal">
       的链接
      </span>
     </span>
    </p>
    <pre>allow=r'/book/1104_<span style="color:#fe2c24"><strong>\d</strong></span><strong><span style="color:#4da8ee">+</span><span style="color:#956fe7">\.</span></strong>html'</pre>
    <p>
     其中：
    </p>
    <p>
     \d：表示数字
    </p>
    <p>
     +：表示1~多
    </p>
    <p>
     \.：转义，使符号“.”生效
    </p>
    <p>
    </p>
    <p>
     对rule中的follow进行修改：
    </p>
    <p>
     follow =
     <span style="white-space:pre-wrap">
      False
     </span>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     1440（行）/40（行/页）/3（本/行）  = 12页（共13页），少了一页
    </p>
    <p>
     原因：规则里不包含首页
    </p>
    <p>
     只提取到2-13页
    </p>
    <p>
     <img alt="" height="271" src="https://i-blog.csdnimg.cn/direct/d183a0ddbc604163ae4dd18b51a198ab.png" width="1006"/>
    </p>
    <p>
     <img alt="" height="620" src="https://i-blog.csdnimg.cn/direct/76ef3ae6f9f448a0bb73ec38b7ae5f9f.png" width="1340"/>
    </p>
    <p>
    </p>
    <p>
     首页：
    </p>
    <pre># 首页：1104_1
start_urls = ["https://www.dushu.com/book/1104_1.html"]</pre>
    <p>
     1560行：
    </p>
    <p>
     <img alt="" height="316" src="https://i-blog.csdnimg.cn/direct/83503229ffe54c28b2f13a4e3098f9b1.png" width="1588">
     </img>
    </p>
    <p>
    </p>
    <h2>
     2. 读书网
    </h2>
    <h3>
     1）dsw.PY
    </h3>
    <pre><code class="hljs">import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
#导入
from demo_dsw.items import DemoDswItem


class DswSpider(CrawlSpider):
    name = "dsw"
    #修改 allowed_domains范围：只保留域名
    allowed_domains = ["www.dushu.com"]
    # 首页：1104_1
    start_urls = ["https://www.dushu.com/book/1104_1.html"]

    rules = (Rule(LinkExtractor(allow=r'/book/1104_\d+\.html'),
                  callback="parse_item",
                  follow=False),)

    def parse_item(self, response):
        img_list = response.xpath('//div[@class = "bookslist"]//img')

        for img in img_list:
            name = img.xpath('./@data-original').extract_first()
            src = img.xpath('./@alt').extract_first()

           #创建一本书
            book = DemoDswItem(name = name, src = src)
            #返回
            yield book


        #CTRL + alt + L：查看book.json文件


</code></pre>
    <h3>
     2）items.PY
    </h3>
    <pre><code class="hljs"># Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class DemoDswItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # pass

    #书名
    name = scrapy.Field()
    #图片
    src = scrapy.Field()</code></pre>
    <h3>
     3）pipelines.PY
    </h3>
    <pre><code class="hljs"># Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class DemoDswPipeline:
    # 开启
    def open_spider(self,spider):
        self.fp = open('book.json','w',encoding='utf-8')

    def process_item(self, item, spider):
        # 中间：只能写字符串
        self.fp.write(str(item))
        return item

    # 关闭
    def close_spider(self,spider):
        self.fp.close()
</code></pre>
    <h3>
     4）settings.PY
    </h3>
    <p>
     解除注释：
    </p>
    <pre><code class="hljs">ITEM_PIPELINES = {
   "demo_dsw.pipelines.DemoDswPipeline": 300,
}</code></pre>
    <h2>
     3. scrapy的post请求
    </h2>
    <p>
     scrapy_post.PY
    </p>
    <pre><code class="hljs">import json
from typing import Iterable

import scrapy
from scrapy import Request


class ScrapypostSpider(scrapy.Spider):
    name = "scrapyPost"
    allowed_domains = ["fanyi.baidu.com"]
    # post请求：
    # post请求必须依赖参数才能执行
    # 如果没有参数，post请求没有任何意义
    # status_urls也没用了
    # post方法也没用了
    # start_urls = ["https://fanyi.baidu.com/sug"]

    # def parse(self, response):

'
        # pass
def start_requests(self):
    url = 'https://fanyi.baidu.com/sug'
    data = {
            'kw': 'cat'
        }

    yield scrapy.FormRequest(url = url,formdata = data,callback = self.parse_second)

    def parse_second(self,response):
        content = response.text
        obj = json.loads(content,encodings = 'utf-8')
        print(obj)</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f4353444e793636363939392f:61727469636c652f64657461696c732f313436313433373139" class_="artid" style="display:none">
 </p>
</div>


