---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f323330325f37393133353938392f:61727469636c652f64657461696c732f313435343034333339"
layout: post
title: "DeepSeek本机部署基于Ollama和Docker管理"
date: 2025-01-31 11:41:47 +08:00
description: "基于Docker和Ollama管理的本地最强模型部署"
keywords: "docker部署deepseek"
categories: ['Deepseek', 'Ai']
tags: ['云原生', 'Spring', 'Eureka', 'Cloud']
artid: "145404339"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145404339
    alt: "DeepSeek本机部署基于Ollama和Docker管理"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145404339
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145404339
cover: https://bing.ee123.net/img/rand?artid=145404339
image: https://bing.ee123.net/img/rand?artid=145404339
img: https://bing.ee123.net/img/rand?artid=145404339
---

# DeepSeek本机部署(基于Ollama和Docker管理）

**目录**

[一、ollama 与 docker 简介](#%E4%B8%80%E3%80%81ollama%20%E4%B8%8E%20docker%20%E7%AE%80%E4%BB%8B)

[（一）ollama(Ollama)](#%EF%BC%88%E4%B8%80%EF%BC%89ollama%28Ollama%29)

[（二）docker](#%EF%BC%88%E4%BA%8C%EF%BC%89docker)

[二、利用 ollama 和 docker 配置 deepseek-r1 的准备工作](#%E4%BA%8C%E3%80%81%E5%88%A9%E7%94%A8%20ollama%20%E5%92%8C%20docker%20%E9%85%8D%E7%BD%AE%20deepseek-r1%20%E7%9A%84%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C)

[（一）硬件需求](#%EF%BC%88%E4%B8%80%EF%BC%89%E7%A1%AC%E4%BB%B6%E9%9C%80%E6%B1%82)

[（二）软件安装](#%EF%BC%88%E4%BA%8C%EF%BC%89%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85)

[三、配置 deepseek-r1 的详细步骤](#%E4%B8%89%E3%80%81%E9%85%8D%E7%BD%AE%20deepseek-r1%20%E7%9A%84%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4)

[（一）使用 ollama 获取 deepseek-r1 模型](#%EF%BC%88%E4%B8%80%EF%BC%89%E4%BD%BF%E7%94%A8%20ollama%20%E8%8E%B7%E5%8F%96%20deepseek-r1%20%E6%A8%A1%E5%9E%8B)

[（二）利用 docker 创建 deepseek-r1 容器](#%EF%BC%88%E4%BA%8C%EF%BC%89%E5%88%A9%E7%94%A8%20docker%20%E5%88%9B%E5%BB%BA%20deepseek-r1%20%E5%AE%B9%E5%99%A8)

[查看模型列表](#%E6%9F%A5%E7%9C%8B%E6%A8%A1%E5%9E%8B%E5%88%97%E8%A1%A8)

[下载模型命令](#%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%91%BD%E4%BB%A4)

[注意事项](#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9)

[（三）启动和测试 deepseek-r1 服务](#%EF%BC%88%E4%B8%89%EF%BC%89%E5%90%AF%E5%8A%A8%E5%92%8C%E6%B5%8B%E8%AF%95%20deepseek-r1%20%E6%9C%8D%E5%8A%A1)

[四、这种配置方式的优势](#%E5%9B%9B%E3%80%81%E8%BF%99%E7%A7%8D%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F%E7%9A%84%E4%BC%98%E5%8A%BF)

[（一）快速部署](#%EF%BC%88%E4%B8%80%EF%BC%89%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2)

[（二）环境隔离](#%EF%BC%88%E4%BA%8C%EF%BC%89%E7%8E%AF%E5%A2%83%E9%9A%94%E7%A6%BB)

[（三）易于扩展](#%EF%BC%88%E4%B8%89%EF%BC%89%E6%98%93%E4%BA%8E%E6%89%A9%E5%B1%95)

[五、可能遇到的问题及解决方法](#%E4%BA%94%E3%80%81%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95)

[（一）网络问题](#%EF%BC%88%E4%B8%80%EF%BC%89%E7%BD%91%E7%BB%9C%E9%97%AE%E9%A2%98)

[（二）资源冲突](#%EF%BC%88%E4%BA%8C%EF%BC%89%E8%B5%84%E6%BA%90%E5%86%B2%E7%AA%81)

---

在人工智能技术日新月异的时代，大语言模型的应用越来越广泛，DeepSeek 作为其中的佼佼者，备受开发者和技术爱好者的关注。通过在本机部署 DeepSeek，能够更灵活地利用其强大功能。而借助 ollama 和 docker 进行 deepseek - r1 的配置，能为我们带来更高效、便捷的部署体验。

### 一、ollama 与 docker 简介

#### （一）ollama( [Ollama](https://ollama.com/ "Ollama") )

ollama 是一个强大的工具，它为模型的管理和运行提供了便利。它可以简化模型的下载、配置和启动过程，让用户能够快速地将不同的模型集成到自己的工作流程中。例如，在处理多个不同类型的大语言模型时，ollama 可以轻松管理这些模型之间的切换和调用，提高开发效率。

#### （二）docker

docker 则是容器化技术的代表，它能够将应用程序及其依赖项打包成一个独立的容器。在 DeepSeek 部署中，使用 docker 可以确保 deepseek - r1 在不同环境中具有一致的运行状态。无论在开发环境、测试环境还是生产环境，只要安装了 docker，就可以运行相同的 deepseek - r1 容器，避免了因环境差异导致的兼容性问题。

### 二、利用 ollama 和 docker 配置 deepseek-r1 的准备工作

#### （一）硬件需求

同常规的 DeepSeek 部署类似，需要一台性能不错的计算机。内存建议 16GB 以上，这样在运行容器和模型时，能够保证系统的流畅性。同时，配备 NVIDIA GPU 会显著提升模型的推理速度，对于处理大规模文本任务非常关键。

#### （二）软件安装

1. **安装 docker**
   ：可以从 docker 官方网站获取适合你操作系统的安装包，按照官方指引进行安装。在安装完成后，确保 docker 服务正常运行，可通过简单的命令行测试来验证(sheel中输入docker)。

1. **安装 ollama**
   ：根据你使用的操作系统，选择合适的安装方式。例如，在 Linux 系统中，可以通过特定的脚本进行安装。安装完成后，配置好 ollama 的运行环境变量，确保其能够被系统正确识别。

### 三、配置 deepseek-r1 的详细步骤

![](https://i-blog.csdnimg.cn/direct/a1278e1ed4a540ffa0d56c755fedffb0.png)

可以看出DeepSeek-r1完全模型在各方面优于OpenAI，在某些方面评估甚至强于OpenAI，参数量适合于本地部署办公使用。

#### （一）使用 ollama 获取 deepseek-r1 模型

通过 ollama 的命令行工具，输入特定的命令来搜索和下载 deepseek - r1 模型。ollama 会自动从官方或指定的源获取模型文件，并将其存储在本地的模型库中。

#### （二）利用 docker 创建 deepseek-r1 容器

1. 基于下载好的 deepseek - r1 模型，使用 docker 命令创建一个新的容器。在创建容器时，需要指定容器的名称、挂载的目录（以便与本地文件系统进行交互）以及容器运行所需的环境变量。

#### 查看模型列表

可以访问 ollama 官方的模型仓库
[library](https://ollama.com/library "library")
查看支持的模型列表，点击浏览某个模型，可看到详细说明，如模型参数、大小、运行命令等信息。

#### 下载模型命令

使用
`ollama pull`
命令进行下载。例如，若要下载图片中的
`deepseek - r1`
7b 模型，在命令行中输入

```
ollama pull deepseek-r1:7b 
```

（若不指定具体版本如 7b 等，默认下载最新版本）。首次使用该命令运行模型时，ollama 也会自动从网上下载模型。

#### 注意事项

1. 下载速度可能受网络状况影响，如果网络不稳定，下载模型可能需要较长等待时间。

2.   部分模型对硬件资源有一定要求，如运行较大的模型（像 llama3 - 70b）可能会较慢，甚至出现硬件资源不足无法正常运行的情况，下载前可了解模型对硬件的需求。(主要是系统内存的要求)

配置容器的网络设置，确保容器能够与外部进行通信。可以根据实际需求，设置容器的端口映射，使本地应用能够访问到容器内运行的 deepseek - r1 服务。

#### （三）启动和测试 deepseek-r1 服务

1. （docker搭建请参照另一篇文章）完成容器创建后，使用 docker 命令启动 deepseek - r1 容器。容器启动后，ollama 会自动加载 deepseek - r1 模型，并启动相关的服务进程。

1. 通过编写简单的测试脚本，向运行在容器内的 deepseek - r1 服务发送请求，验证模型是否正常工作。例如，可以发送一段文本，请求模型生成回答，检查返回的结果是否符合预期。

**（四）WebUi的配置**

搭建部署Open WebUI有两种方式

1. Docker方式（官网推荐）
2. 源代码部署安装方式：（文档
   [🚀 Getting Started | Open WebUI](https://docs.openwebui.com/getting-started/ "🚀 Getting Started | Open WebUI")
   ）

Open WenUI 官网：
[GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)](https://github.com/open-webui/open-webui "GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)")

![](https://i-blog.csdnimg.cn/direct/e9a87f78bc214428bc2abff0e9cfb6e7.png)

```
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v D:devopen-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main

```

**此命令启动一个docker容器**

1. **`docker run`**
   ：这是 Docker 用于运行容器的基本命令，它会根据指定的镜像创建并启动一个新的容器实例。
2. **`-d`**
   ：表示以守护进程（detached）模式运行容器，即容器会在后台运行，不会占用当前命令行终端的输入输出流，方便执行其他命令。
3. **`-p 3000:8080`**
   ：端口映射参数，将容器内部的 8080 端口映射到主机的 3000 端口。这样，通过访问主机的 3000 端口，就可以访问到容器内运行在 8080 端口上的
   `open-webui`
   应用。
4. **`--add-host=host.docker.internal:host-gateway`**
   ：此参数用于向容器内的
   `/etc/hosts`
   文件中添加一条主机映射记录，将
   `host.docker.internal`
   映射到
   `host-gateway`
   。这在容器需要与主机进行通信时非常有用，特别是在一些特殊网络环境下，使得容器能够通过
   `host.docker.internal`
   这个域名访问到主机。
5. **`-v D:devopen-webui:/app/backend/data`**
   ：这是卷挂载（volume mount）参数，将主机上的
   `D:devopen-webui`
   目录挂载到容器内的
   `/app/backend/data`
   目录。这意味着主机和容器可以共享这个目录下的文件，主机目录中的任何更改都会实时反映到容器内，反之亦然。常用于数据持久化或在容器和主机之间传递数据。
6. **`--name open-webui`**
   ：为运行的容器指定一个名称为
   `open-webui`
   ，方便后续对容器进行管理和操作，例如使用
   `docker stop open-webui`
   停止容器，或
   `docker start open-webui`
   启动容器。
7. **`--restart always`**
   ：表示无论容器因为何种原因停止，Docker 都会自动尝试重新启动它，确保容器始终处于运行状态。
8. **`ghcr.io/open-webui/open-webui:main`**
   ：这是容器所使用的镜像名称和标签，指定从 GitHub Container Registry（
   `ghcr.io`
   ）上拉取
   `open-webui/open-webui`
   镜像的
   `main`
   版本。如果本地没有该镜像，Docker 会自动从指定的镜像仓库下载。

#### 启动ollama容器

1.使用该命令启动CPU版运行本地AI模型

```
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

2.此命令用于启动GPU版本运行AI模型

前提是笔记本已配置NVIDIA的GPU驱动，可在shell中输入nvidia-smi查看详细情况

```
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

​​​​
![](https://i-blog.csdnimg.cn/direct/c2cf5c15c3af459c817b714e6e303734.png)

然后就可以访问docker中给出的open webui的地址启动web界面，选择好模型就可以进行问答对话了，恭喜你拥有了自己的AI小助手！

### 四、这种配置方式的优势

#### （一）快速部署

ollama 和 docker 的结合，大大缩短了 deepseek - r1 的部署时间。通过简单的命令行操作，即可完成模型的获取和容器的创建，相比传统的手动配置方式，效率得到了极大提升。

#### （二）环境隔离

docker 的容器化技术实现了环境的隔离，使得 deepseek - r1 在独立的环境中运行，不会受到本地系统其他软件的干扰。同时，也方便对模型进行版本管理和维护，当需要更新或切换模型版本时，只需要重新创建或更新容器即可。

#### （三）易于扩展

在后续的应用中，如果需要增加模型的计算资源，或者部署多个 deepseek - r1 实例，可以轻松地通过 docker 的集群管理功能进行扩展。ollama 也能够方便地管理多个模型之间的协同工作，满足不同业务场景的需求。

### 五、可能遇到的问题及解决方法

#### （一）网络问题

在下载模型或容器通信过程中，可能会遇到网络不稳定的情况。解决方法是检查网络连接，尝试更换网络环境或使用代理服务器。同时，ollama 和 docker 都提供了相关的网络配置选项，可以根据实际情况进行调整。

#### （二）资源冲突

当本地系统中已经运行了其他占用端口或资源的服务时，可能会与 deepseek - r1 容器产生冲突。可以通过修改容器的端口映射或调整本地服务的配置，来避免资源冲突。

利用 ollama 和 docker 配置 deepseek - r1 实现 DeepSeek 本机部署，为我们提供了一种高效、便捷且稳定的部署方式。随着人工智能技术的不断发展，这种基于容器化和模型管理工具的部署方法，将在更多的应用场景中发挥重要作用，推动大语言模型技术在本地开发和应用中的普及。