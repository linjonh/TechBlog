---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f36323930343838332f:61727469636c652f64657461696c732f313436323836353339"
layout: post
title: "DataWhale大语言模型-大模型技术基础"
date: 2025-03-15 23:21:46 +08:00
description: "跟着datawhale学习大模型"
keywords: "DataWhale大语言模型-大模型技术基础"
categories: ['大模型学习']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "146286539"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146286539
    alt: "DataWhale大语言模型-大模型技术基础"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146286539
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146286539
cover: https://bing.ee123.net/img/rand?artid=146286539
image: https://bing.ee123.net/img/rand?artid=146286539
img: https://bing.ee123.net/img/rand?artid=146286539
---

# DataWhale大语言模型-大模型技术基础

## 什么是大语言模型

* 定义:通常是指具有
  **超大规模参数**
  的预训练语言模型

与传统的语言模型相比,大语言模型的构建过程涉及到更为复杂的训练方法,进而展现了强大的自然语言理解能力和复杂任务求解能力(通过文本生成的形式)

* 架构:主要作为Transformer解码器的架构
* 训练:训练的内容包括预训练(base model)和后训练(instruct model)
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4f63106e8efe486ba510c8a9010ebd27.png)

### 预训练和后训练之间的对比

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/549ad041a78145eeafced1a7d78900d2.png)

### 大模型预训练(Pre-training)

* 利用与下游任务无关的大规模数据进行模型参数的初始训练

主要的工作任务可以分成以下步骤:

* **解码器架构+预测下一个词**
  :这是由于GPT系列模型的出圈,该方法得到了有效的验证,已经成为了主流的大语言模型的技术路径
* **大量的高质量的数据**
  :为了预训练大语言模型需要进行大规模的文本数据,所以数据的数量,数据质量都是十分关键的

目前预训练的过程考虑各种细节,所以需要研发人员有丰富的训练经验和异常处理的能力,从而避免算力资源的浪费,提高模型预训练的成功几率

### 大语言模型后训练(Post-Training)

这一步可以理解为将一个泛化能力很强的模型不断去精修某一门技术,达到完成指定任务的能力,这一过程可以理解为大模型的微调过程
  
目前来讲比较常见的微调技术被称为
**指令微调SFT**

#### 指令微调(Instruction Tuning)

* 使用输入与输出配对的指令数据对模型进行微调
* 提升模型通过
  **问答模式**
  进行任务求解的能力
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8020f88375b4430c901faf30e2a22678.png)

#### 人类对齐(Human Alignment)

除了要提升任务的解决能力,还需要将大语言模型与人类的期望,需求以及价值观对齐,这对于大模型的部署与应用具有重要的意义

* 将大语言模型与人类的期望,需求以及价值观对齐
* 基于人类反馈的强化学习对齐方法(RLHF)

在RLHF算法当中,需要标注人员针对大语言模型所生成的多条输出进行
**偏好排序**
,并使用偏好数据训练奖励模型,用于判断模型的输出质量
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/cc30989c24054826b95f9c0915cec92b.png)

### 扩展定律

* 通过扩展参数规模以及数据规模和计算算力,大语言模型的能力会出现显著的提升
* 扩展定律在本次大模型的浪潮当中起到了至关重要的作用
    
  也就是说通过扩展带来的性能提升通常显著高于通过改进架构以及算法等方面所带来的改进,使得大语言模型的能力超越了小语言模型的能力
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c16868a063894f70aa20575a3525fedb.png)

#### KM扩展定律

由OpenAI团队所提出,首次建立了神经语言模型性能与
**参数规模(N)**
,**数据规模(D)
**和**
计算算力©**之间的幂律关系

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8f9619cfda194e7ab6e01247edf51ad8.png)
  



L
(
⋅
)
L(\cdot)





L

(

⋅

)
用来表示以

n
a
t
nat





na

t
(用来表示以

e
e





e
为底信息量的自然对数)为单位的交叉熵损失,其中

N
c
,
D
c
,
C
c
N_c,D_c,C_c






N









c

​


,




D









c

​


,




C









c

​

是实验性常数数值,分别对应于非嵌入参数的数量,训练数据数量以及实际的算力开销

#### Chinchilla扩展定律

由DeepMind团队所提出的另一种形式的扩展定律

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dfe39e3766de4add9ff625b4f95e11fc.png)
  
其中

a
a





a
和

b
b





b
决定了参数规模以及数据规模的资源分配优先级

* 当a>b时,应该用更多的算力来提高参数规模
* 当b>a时,应该利用更多的算力来提高数据规模

#### 深入讨论

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/729eb8e9c824431690f66c38377ee78e.png)
  
可预测扩展可以简单的理解为可以通过训练一个小模型去预测一个大模型的性能,这种方法其实很明显的问题就在于,模型的参数量大了的话那么很容易出现预测失败的问题
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e9ad251bb79648e4bd225c8ddf0e31a5.png)

## 涌现能力

非形式化的定义:在小型的模型当中并不存在但是在大模型当中出现的能力
  
这里可以我认为(不严谨哈)可以这样说:
**量变所引起的质变**
,由于模型的扩展超过一定的规模,使其能力得到了一定的提升(可以说是跃升)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/52d93e352b114bfe88d4d855746c26c5.png)

### 代表性能力

也就是具有普遍性的一些能力

#### 指令遵循(Instruction Following)

* 大语言模型能够按照自然语言的指令来执行对应的任务
* 可以通过高质量指令数据微调的方式习得一定的通用指令遵循能力
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/842e7576f4554dd7a1fc9c2281692545.png)

#### 上下文学习(In-context Learning)

* 在提示当中为语言模型提供自然语言指令和任务示例,无需显式的训练或者梯度更新,仅通过输入文本的单词序列就能为测试样本生成预测的输出
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4725d2995e00447c88e7cc657e01e6a7.png)

#### 逐步推理

在大语言模型当中利用思维链的提示策略来加强推理性能

* 在提示当中引入任务相关的中间推理步骤来加强复杂任务的求解,从而获得更加可靠的答案
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1b749ff45e5644c7840e6c3c5c97ce90.png)

## 涌现能力与扩展定律的关系

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/24f0fbf42f6142869c410f98a29a0cee.png)

## 参考资料

Datawhale学习链接:
<https://www.datawhale.cn/learn/content/107/3287>