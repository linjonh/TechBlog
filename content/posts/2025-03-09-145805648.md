---
layout: post
title: "NLP-39激活函数-Swish激活函数"
date: 2025-03-09 21:51:08 +0800
description: "基础形式Swish的标准表达式为：σ(x) 是Sigmoid函数：​        β 是可学习参数或固定值（通常默认设为1）​2.变体形式​：当β=1时，Swish退化为SILU​自适应Swish：通过训练学习 β 的值，允许激活函数根据任务动态调整形状。"
keywords: "【NLP 39、激活函数 ⑤ Swish激活函数】"
categories: ['Nlp']
tags: ['自然语言处理', '人工智能']
artid: "145805648"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145805648
    alt: "NLP-39激活函数-Swish激活函数"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145805648
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145805648
cover: https://bing.ee123.net/img/rand?artid=145805648
image: https://bing.ee123.net/img/rand?artid=145805648
img: https://bing.ee123.net/img/rand?artid=145805648
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【NLP 39、激活函数 ⑤ Swish激活函数】
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <blockquote>
     <p>
      <span style="color:#1c7892">
       <strong>
        我的孤独原本是座荒岛，直到你称成潮汐，原来爱是让个体失序的永恒运动
       </strong>
      </span>
     </p>
     <p>
      <span style="color:#1c7892">
       <strong>
        ——25.2.25
       </strong>
      </span>
     </p>
    </blockquote>
    <p>
     Swish激活函数是一种近年来在深度学习中广泛应用的激活函数，由Google Brain团队在2017年提出。其核心设计结合了Sigmoid门控机制和线性输入的乘积，通过引入平滑性和非单调性来提升模型性能。
    </p>
    <hr/>
    <h2>
     一、数学定义与变体
    </h2>
    <h3>
     1.​
     <strong>
      基础形式
     </strong>
    </h3>
    <p>
     Swish的
     <strong>
      标准表达式
     </strong>
     为：
     <span style="color:#fe2c24">
      Swish(x)=x ⋅ σ(βx)
     </span>
    </p>
    <p>
     <strong>
      其中：
     </strong>
    </p>
    <p>
     σ(x) 是Sigmoid函数：
     <img alt="" height="41" src="https://i-blog.csdnimg.cn/direct/78e78b4c0f7c4f8cb67d48da5ed84ddf.png" width="153"/>
    </p>
    <p>
     ​        β 是
     <strong>
      可学习参数
     </strong>
     或固定值（通常默认设为1）​
    </p>
    <h3>
     <strong>
      2.变体形式
     </strong>
    </h3>
    <p>
     ​
     <strong>
      SILU（Sigmoid-Weighted Linear Unit）​
     </strong>
     ：当β=1时，Swish退化为SILU​
    </p>
    <p>
     <strong>
      自适应Swish
     </strong>
     ：通过训练学习 β 的值，允许激活函数根据任务动态调整形状
    </p>
    <hr/>
    <h4>
     二、关键特性与优势
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        平滑性与非单调性
       </strong>
      </p>
      <ul>
       <li>
        ​
        <strong>
         平滑梯度
        </strong>
        ：Swish在全局连续可导（C∞），避免了ReLU在x=0处的梯度突变，缓解梯度消失问题
       </li>
       <li>
        ​
        <strong>
         非单调性
        </strong>
        ：当x&lt;0时，Swish允许部分负值传递（类似Leaky ReLU），增强模型对复杂模式的表达能力
       </li>
      </ul>
     </li>
     <li>
      <p>
       ​
       <strong>
        近似ReLU与自适应过渡
       </strong>
      </p>
      <ul>
       <li>
        当β→+∞时，Swish逼近ReLU；当β→0时，近似线性函数
       </li>
       <li>
        这种特性使其能灵活适应不同网络深度的需求。
        <strong style="color:#4d4d4d">
         梯度稳定性
        </strong>
       </li>
      </ul>
     </li>
     <li>
      <p>
       导数公式为：Swish′(x)=σ(βx)+βx⋅σ(βx)⋅(1−σ(βx))
      </p>
      <p>
       在正负输入区间均保持非零梯度，避免神经元死亡
      </p>
     </li>
    </ol>
    <hr/>
    <h4>
     三、与其他激活函数的对比
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        ​
        <strong>
         激活函数
        </strong>
       </th>
       <th>
        ​
        <strong>
         优势
        </strong>
       </th>
       <th>
        ​
        <strong>
         劣势
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        ​
        <strong>
         ReLU
        </strong>
       </td>
       <td>
        计算高效，缓解梯度消失
       </td>
       <td>
        负区间梯度为0，易导致神经元死亡
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         Leaky ReLU
        </strong>
       </td>
       <td>
        解决ReLU的死亡问题
       </td>
       <td>
        需人工设定斜率参数α
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         Swish
        </strong>
       </td>
       <td>
        平滑梯度，自适应参数，非单调性
       </td>
       <td>
        计算复杂度较高（涉及Sigmoid运算）
        <p>
         6
        </p>
        <p>
         7
        </p>
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         Sigmoid
        </strong>
       </td>
       <td>
        输出概率化，适合二分类
       </td>
       <td>
        梯度消失严重，输出非零中心化
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h4>
     四、应用场景与实验表现
    </h4>
    <ol>
     <li>
      <p>
       ​
       <strong>
        图像分类
       </strong>
      </p>
      <ul>
       <li>
        在ResNet、EfficientNet等模型中，Swish相比ReLU可提升Top-1准确率约0.5%-1%
       </li>
       <li>
        示例：MobileNetV3采用Swish作为隐藏层激活函数，优化了轻量级模型的表达能力
       </li>
      </ul>
     </li>
     <li>
      <p>
       ​
       <strong>
        自然语言处理
       </strong>
      </p>
      <ul>
       <li>
        在Transformer架构中，SwiGLU（Swish-Gated Linear Unit）结合Swish和GLU，显著提升机器翻译任务的BLEU分数
       </li>
      </ul>
     </li>
     <li>
      <p>
       ​
       <strong>
        强化学习
       </strong>
      </p>
      <ul>
       <li>
        Swish的非单调性使其在策略梯度方法中表现优异，能够处理更复杂的动作空间
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h4>
     五、实现与优化建议
    </h4>
    <ol>
     <li>
      <p>
       ​
       <strong>
        代码实现（PyTorch示例）​
       </strong>
      </p>
      <pre><code class="language-python">import torch
import torch.nn as nn

class Swish(nn.Module):
    def __init__(self, beta=1.0, trainable=False):
        super().__init__()
        self.beta = nn.Parameter(torch.tensor(beta)) if trainable else beta

    def forward(self, x):
        return x * torch.sigmoid(self.beta * x)</code></pre>
     </li>
     <li>
      <p>
       ​
       <strong>
        训练调参技巧
       </strong>
      </p>
      <ul>
       <li>
        ​
        <strong>
         初始化β
        </strong>
        ：建议从β=1开始，若训练不稳定可尝试固定为1
       </li>
       <li>
        ​
        <strong>
         混合精度训练
        </strong>
        ：使用FP16或BF16减少Sigmoid计算的开销
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h4>
     六、局限性及改进方向
    </h4>
    <ol>
     <li>
      <p>
       ​
       <strong>
        计算成本
       </strong>
       <br/>
       Swish的Sigmoid运算比ReLU多约30%的计算量，可通过算子融合优化（如NVIDIA的cuDNN加速）
      </p>
     </li>
     <li>
      <p>
       ​
       <strong>
        任务依赖性
       </strong>
       <br/>
       在简单任务（如MNIST分类）中，Swish可能无明显优势，需根据任务复杂度选择激活函数
      </p>
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37333938333730372f:61727469636c652f64657461696c732f313435383035363438" class_="artid" style="display:none">
 </p>
</div>


