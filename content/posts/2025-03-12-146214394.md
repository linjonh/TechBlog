---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f36343039313930302f:61727469636c652f64657461696c732f313436323134333934"
layout: post
title: "文献分享-对ColBERT段落多向量的剪枝基于学习的方法"
date: 2025-03-12 20:51:46 +08:00
description: "SIGIR'22"
keywords: "文献分享: 对ColBERT段落多向量的剪枝——基于学习的方法"
categories: ['文献阅读笔记']
tags: ['算法', '学习', '剪枝']
artid: "146214394"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146214394
    alt: "文献分享-对ColBERT段落多向量的剪枝基于学习的方法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146214394
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146214394
cover: https://bing.ee123.net/img/rand?artid=146214394
image: https://bing.ee123.net/img/rand?artid=146214394
img: https://bing.ee123.net/img/rand?artid=146214394
---

# 文献分享: 对ColBERT段落多向量的剪枝——基于学习的方法

[原论文](https://doi.org/10.1145/3477495.3531835)

## 1. 导论 & \textbf{\&} & 方法

> 1️⃣要干啥：在
>
> ColBERT
> \text{ColBERT}
>
>
>
>
>
>
> ColBERT
> 方法中，限制每个段落要保留的
>
> Token
> \text{Token}
>
>
>
>
>
>
> Token
> 的数量，或者说对段落
>
> Token
> \text{Token}
>
>
>
>
>
>
> Token
> 进行剪枝
>
> 2️⃣怎么干：注意以下方法都是整合进
>
> ColBERT
> \text{ColBERT}
>
>
>
>
>
>
> ColBERT
> 训练的顶层池化层，而非在后期交互中进行改进
>
> 1. 前
>
>    k
>    k
>
>
>
>
>
>    k
>    位置
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    ：只保留每个段落的前
>
>    k
>    k
>
>
>
>
>
>    k
>    个
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
> 2. 前
>
>    k
>    k
>
>
>
>
>
>    k
>    罕见
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    ：选择段落中最罕见的
>
>    k
>    k
>
>
>
>
>
>    k
>    个
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    ，所谓罕见的
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    即
>
>    IDF
>    \text{IDF}
>
>
>
>
>
>
>    IDF
>    高的
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
> 3. 前
>
>    k
>    k
>
>
>
>
>
>    k
>    闲置
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    ：在段落前添加
>
>    k
>    k
>
>
>
>
>
>    k
>    个特殊
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    ，这些
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    在
>
>    BERT
>    \text{BERT}
>
>
>
>
>
>
>    BERT
>    词汇表中标为闲置(
>    `unused`
>    )，最终只保留这
>
>    k
>    k
>
>
>
>
>
>    k
>    个
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
> 4. 前
>
>    k
>    k
>
>
>
>
>
>    k
>    得分
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    ：用预训练模型的最后一层注意力机制给所有
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    一个注意力评分，选取注意力机制最高的
>
>    k
>    k
>
>
>
>
>
>    k
>    个
>
>    Token
>    \text{Token}
>
>
>
>
>
>
>    Token
>    * 注意力张量：
>
>      P
>      =
>      {
>      p
>      1
>      ,
>      p
>      2
>      ,
>      .
>      .
>      .
>      ,
>      p
>      m
>      }
>      P\text{=}\{p_1,p_2,...,p_m\}
>
>
>
>
>
>      P
>
>
>      =
>
>      {
>
>
>      p
>
>
>
>
>
>
>
>
>
>      1
>
>      ​
>
>
>      ,
>
>
>
>
>      p
>
>
>
>
>
>
>
>
>
>      2
>
>      ​
>
>
>      ,
>
>
>
>      ...
>
>      ,
>
>
>
>
>      p
>
>
>
>
>
>
>
>
>
>      m
>
>      ​
>
>
>      }
>      的注意力为三维张量
>
>      A
>      (
>      h
>      ,
>      i
>      ,
>      j
>      )
>      A(h,i,j)
>
>
>
>
>
>      A
>
>      (
>
>      h
>
>      ,
>
>
>
>      i
>
>      ,
>
>
>
>      j
>
>      )
>      ，表示在
>
>      h
>      h
>
>
>
>
>
>      h
>      头注意力机制中
>
>      p
>      i
>      p_i
>
>
>
>
>
>
>      p
>
>
>
>
>
>
>
>
>
>      i
>
>      ​
>
>      与
>
>      p
>      j
>      p_j
>
>
>
>
>
>
>      p
>
>
>
>
>
>
>
>
>
>      j
>
>      ​
>
>      二者的注意力相关性
>        
>      ![image-20250312200743542](https://i-blog.csdnimg.cn/direct/dc7e9059154e4bdbb05fd9c355e375e4.png)
>    * 注意力评分：以
>
>      p
>      i
>      p_i
>
>
>
>
>
>
>      p
>
>
>
>
>
>
>
>
>
>      i
>
>      ​
>
>      为例，其注意力评分为每个注意力头中与
>
>      p
>      i
>      p_i
>
>
>
>
>
>
>      p
>
>
>
>
>
>
>
>
>
>      i
>
>      ​
>
>      有关行的总和，即
>
>      a
>      (
>      q
>      i
>      )
>      =
>      ∑
>      h
>      =
>      0
>      h
>      max
>      ⁡
>      ∑
>      j
>      =
>      0
>      m
>      A
>      (
>      h
>      ,
>      i
>      ,
>      j
>      )
>      a(q_i)\text{=}\displaystyle{}\sum_{h=0}^{h_{\max}}\sum_{j=0}^{m}A(h,i,j)
>
>
>
>
>
>      a
>
>      (
>
>
>      q
>
>
>
>
>
>
>
>
>
>      i
>
>      ​
>
>
>      )
>
>
>      =
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>      h
>
>      =
>
>      0
>
>
>
>
>
>      ∑
>
>
>
>
>
>
>
>      h
>
>
>
>
>
>
>
>
>
>
>
>      m
>
>      a
>
>      x
>
>      ​
>
>
>      ​
>
>
>
>
>
>
>
>
>
>
>
>
>
>      j
>
>      =
>
>      0
>
>
>
>
>
>      ∑
>
>
>
>
>
>
>      m
>
>      ​
>
>
>
>
>      A
>
>      (
>
>      h
>
>      ,
>
>
>
>      i
>
>      ,
>
>
>
>      j
>
>      )

## 2.   \textbf{2. } 2. 实验概要

> 1️⃣训练方法：
>
> ColBERT
> \text{ColBERT}
>
>
>
>
>
>
> ColBERT
> 使用
>
> Mini-LM
> \text{Mini-LM}
>
>
>
>
>
>
> Mini-LM
> 时无需归一化和查询扩展，大幅降低计算成本​
>
> 2️⃣检索性能：当
>
> k
> =
> 50
> k\text{=}50
>
>
>
>
>
> k
>
>
> =
>
> 50
> 时，剪枝可减少
>
> 30%
> \text{30\%}
>
>
>
>
>
>
> 30%
> 的段落索引，并且性能减少极小(
>
> nDCG@10
> \text{nDCG@10}
>
>
>
>
>
>
> nDCG@10
> 减小
>
> 0.01
> \text{0.01}
>
>
>
>
>
>
> 0.01
> )
>
> 3️⃣方法对比：当普通剪枝(
>
> k
> =50
> k\text{=50}
>
>
>
>
>
> k
>
>
> =50
> )时方法
>
> 1&3
> \text{1\&3}
>
>
>
>
>
>
> 1&3
> 最佳，剧烈剪枝(
>
> k
> =10
> k\text{=10}
>
>
>
>
>
> k
>
>
> =10
> )时方法
>
> 3
> 3
>
>
>
>
>
> 3
> 显著优于其它方法