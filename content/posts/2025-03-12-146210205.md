---
layout: post
title: "分布式训练中的-rank-和-local_rank"
date: 2025-03-12 17:24:21 +0800
description: "在分布式训练中， 和  是两个不同的概念，它们分别表示不同层次的进程标识符。理解这两者的区别和关系对于正确设置分布式训练环境至关重要。::关系：其中  是每个节点上的设备数量（例如，在一个有4个GPU的节点上， 就是4）。区别：假设你有一个由两台机器组成的集群，每台机器上有两个NPU：在这种情况下：解释:获取 :设置设备:以下是一个完整的示例，展示了如何在分布式训练中使用  和 ："
keywords: "分布式训练中的 rank 和 local_rank"
categories: ['未分类']
tags: ['分布式', '人工智能', 'Torch', 'Ai']
artid: "146210205"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146210205
    alt: "分布式训练中的-rank-和-local_rank"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146210205
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146210205
cover: https://bing.ee123.net/img/rand?artid=146210205
image: https://bing.ee123.net/img/rand?artid=146210205
img: https://bing.ee123.net/img/rand?artid=146210205
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     分布式训练中的 rank 和 local_rank
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-tomorrow-night-eighties" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     在分布式训练中，
     <code>
      rank
     </code>
     和
     <code>
      local_rank
     </code>
     是两个不同的概念，它们分别表示不同层次的进程标识符。理解这两者的区别和关系对于正确设置分布式训练环境至关重要。
    </p>
    <h4>
     <a id="rank__local_rank__2">
     </a>
     <code>
      rank
     </code>
     和
     <code>
      local_rank
     </code>
     的定义
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        <code>
         rank
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        全局唯一标识符（Global Rank），表示当前进程在整个分布式训练集群中的全局编号。
       </li>
       <li>
        范围是从
        <code>
         0
        </code>
        到
        <code>
         world_size - 1
        </code>
        ，其中
        <code>
         world_size
        </code>
        是参与分布式训练的所有进程的总数。
       </li>
       <li>
        每个进程都有一个唯一的
        <code>
         rank
        </code>
        ，用于区分不同的进程。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        <code>
         local_rank
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        本地设备标识符（Local Rank），表示当前进程在单个节点（即一台机器）上的设备编号。
       </li>
       <li>
        范围是从
        <code>
         0
        </code>
        到
        <code>
         num_devices_per_node - 1
        </code>
        ，其中
        <code>
         num_devices_per_node
        </code>
        是该节点上的设备数量（如 GPU 或 NPU 数量）。
       </li>
       <li>
        用于指定当前进程使用的具体设备（如 GPU 或 NPU）。
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="_14">
     </a>
     关系与区别
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        关系
       </strong>
       ：
      </p>
      <ul>
       <li>
        <code>
         rank
        </code>
        是全局的，标识了每个进程在整个集群中的位置。
       </li>
       <li>
        <code>
         local_rank
        </code>
        是局部的，标识了每个进程在其所在节点上的设备位置。
       </li>
       <li>
        可以通过
        <code>
         rank
        </code>
        和
        <code>
         world_size
        </code>
        计算出
        <code>
         local_rank
        </code>
        ，公式如下：
        <pre><code class="prism language-python">local_rank <span class="token operator">=</span> rank <span class="token operator">%</span> num_devices_per_node
</code></pre>
       </li>
       <li>
        其中
        <code>
         num_devices_per_node
        </code>
        是每个节点上的设备数量（例如，在一个有4个GPU的节点上，
        <code>
         num_devices_per_node
        </code>
        就是4）。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        区别
       </strong>
       ：
      </p>
      <ul>
       <li>
        <code>
         rank
        </code>
        是整个集群范围内的唯一标识符，而
        <code>
         local_rank
        </code>
        是节点范围内的标识符。
       </li>
       <li>
        <code>
         rank
        </code>
        通常用于控制全局操作（如同步所有进程），而
        <code>
         local_rank
        </code>
        用于选择特定的设备进行计算。
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="_29">
     </a>
     示例说明
    </h4>
    <p>
     假设你有一个由两台机器组成的集群，每台机器上有两个NPU：
    </p>
    <ul>
     <li>
      第一台机器：NPU 0 和 NPU 1
     </li>
     <li>
      第二台机器：NPU 0 和 NPU 1
     </li>
    </ul>
    <p>
     在这种情况下：
    </p>
    <ul>
     <li>
      <code>
       world_size
      </code>
      是 4（因为总共有4个进程）
     </li>
     <li>
      <code>
       num_devices_per_node
      </code>
      是 2（因为每台机器有两个NPU）
     </li>
    </ul>
    <h5>
     <a id="_41">
     </a>
     进程分配
    </h5>
    <table>
     <thead>
      <tr>
       <th>
        机器
       </th>
       <th>
        NPU 设备
       </th>
       <th>
        <code>
         rank
        </code>
       </th>
       <th>
        <code>
         local_rank
        </code>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        1
       </td>
       <td>
        NPU 0
       </td>
       <td>
        0
       </td>
       <td>
        0
       </td>
      </tr>
      <tr>
       <td>
        1
       </td>
       <td>
        NPU 1
       </td>
       <td>
        1
       </td>
       <td>
        1
       </td>
      </tr>
      <tr>
       <td>
        2
       </td>
       <td>
        NPU 0
       </td>
       <td>
        2
       </td>
       <td>
        0
       </td>
      </tr>
      <tr>
       <td>
        2
       </td>
       <td>
        NPU 1
       </td>
       <td>
        3
       </td>
       <td>
        1
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     <a id="_50">
     </a>
     代码解释
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> torch_npu

<span class="token comment"># 初始化过程组</span>
dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span><span class="token string">"nccl"</span><span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"tcp://</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>addr<span class="token punctuation">}</span></span><span class="token string">:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>port<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span>
                        rank<span class="token operator">=</span>rank_id<span class="token punctuation">,</span>
                        world_size<span class="token operator">=</span>world_size<span class="token punctuation">)</span>

<span class="token comment"># 获取 local_rank</span>
local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 设置 NPU 设备</span>
torch_npu<span class="token punctuation">.</span>npu<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_69">
     </a>
     解释
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        <code>
         dist.init_process_group
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        使用
        <code>
         rank=rank_id
        </code>
        来指定当前进程的全局编号。
       </li>
       <li>
        <code>
         world_size
        </code>
        是总的进程数，包括所有节点上的所有进程。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        获取
        <code>
         local_rank
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        通常通过环境变量
        <code>
         LOCAL_RANK
        </code>
        获取当前进程在本地节点上的设备编号。
       </li>
       <li>
        如果环境变量不存在，默认值为
        <code>
         0
        </code>
        。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        设置设备
       </strong>
       :
      </p>
      <ul>
       <li>
        使用
        <code>
         torch_npu.npu.set_device(local_rank)
        </code>
        来指定当前进程使用的 NPU 设备。
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="_82">
     </a>
     示例代码
    </h4>
    <p>
     以下是一个完整的示例，展示了如何在分布式训练中使用
     <code>
      rank
     </code>
     和
     <code>
      local_rank
     </code>
     ：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">import</span> torch_npu

<span class="token keyword">def</span> <span class="token function">setup</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">,</span> addr<span class="token punctuation">,</span> port<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 初始化进程组</span>
    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>
        backend<span class="token operator">=</span><span class="token string">"nccl"</span><span class="token punctuation">,</span>  <span class="token comment"># 使用 nccl 后端，适用于 GPU/NPU</span>
        init_method<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"tcp://</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>addr<span class="token punctuation">}</span></span><span class="token string">:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>port<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span>
        rank<span class="token operator">=</span>rank<span class="token punctuation">,</span>
        world_size<span class="token operator">=</span>world_size
    <span class="token punctuation">)</span>

    <span class="token comment"># 获取 local_rank</span>
    local_rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'LOCAL_RANK'</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 设置 NPU 设备</span>
    torch_npu<span class="token punctuation">.</span>npu<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>local_rank<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">cleanup</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    dist<span class="token punctuation">.</span>destroy_process_group<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">,</span> addr<span class="token punctuation">,</span> port<span class="token punctuation">)</span><span class="token punctuation">:</span>
    setup<span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">,</span> addr<span class="token punctuation">,</span> port<span class="token punctuation">)</span>

    <span class="token comment"># 设置设备</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"npu:</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>local_rank<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

    <span class="token comment"># 创建模型并移动到相应的设备</span>
    model <span class="token operator">=</span> SimpleModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    ddp_model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>local_rank<span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token comment"># 其他训练逻辑...</span>

    cleanup<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    <span class="token comment"># 配置信息</span>
    world_size <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># 总共4个进程</span>
    addr <span class="token operator">=</span> <span class="token string">"192.168.1.1"</span>  <span class="token comment"># 主节点的IP地址</span>
    port <span class="token operator">=</span> <span class="token string">"12355"</span>  <span class="token comment"># 通信使用的端口</span>

    <span class="token comment"># 启动每个进程</span>
    <span class="token keyword">import</span> multiprocessing <span class="token keyword">as</span> mp
    processes <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> rank <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>world_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        p <span class="token operator">=</span> mp<span class="token punctuation">.</span>Process<span class="token punctuation">(</span>target<span class="token operator">=</span>train<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">,</span> addr<span class="token punctuation">,</span> port<span class="token punctuation">)</span><span class="token punctuation">)</span>
        p<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
        processes<span class="token punctuation">.</span>append<span class="token punctuation">(</span>p<span class="token punctuation">)</span>

    <span class="token keyword">for</span> p <span class="token keyword">in</span> processes<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f66697273745f74687265655f73756e2f:61727469636c652f64657461696c732f313436323130323035" class_="artid" style="display:none">
 </p>
</div>


