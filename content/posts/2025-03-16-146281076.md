---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35353030393434382f:61727469636c652f64657461696c732f313436323831303736"
layout: post
title: "Towards-Universal-Soccer-Video-Understanding论文学习足球类"
date: 2025-03-16 17:58:37 +0800
description: "在这篇论文中，建立了一个统一的、可扩展的足球理解多模态框架。具体来说，本文介绍了SoccerReplay-1988，这是迄今为止最大、最全面的足球视频数据集，由自动管理管道注释。这为开发多模式足球理解模型提供了坚实的基础，并成为一个更具挑战性的基准。在此基础上，本文开发了第一个足球视觉语言基础模型，称为MatchVision，它有效地利用了足球视频中的时空信息，可以应用于各种任务，如事件分类和评论生成。"
keywords: "Towards Universal Soccer Video Understanding——论文学习（足球类）"
categories: ['未分类']
tags: ['论文阅读', '论文笔记', '计算机视觉', '算法', '深度学习', '学习', '人工智能']
artid: "146281076"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146281076
    alt: "Towards-Universal-Soccer-Video-Understanding论文学习足球类"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146281076
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146281076
cover: https://bing.ee123.net/img/rand?artid=146281076
image: https://bing.ee123.net/img/rand?artid=146281076
img: https://bing.ee123.net/img/rand?artid=146281076
---

# Towards Universal Soccer Video Understanding——论文学习（足球类）
## 一、主要内容
作为一项享誉全球的运动，足球吸引了世界各地球迷的广泛兴趣。
本文旨在为足球视频的理解开发一个全面的多模态框架
。具体来说，在本文中做出了以下贡献:(i)引入了
SoccerReplay
-1988，这是迄今为止
最大的多模式足球数据集
，具有1988场完整比赛的视频和详细注释，以及自动注释管道;(ii)
提出了足球领域的第一个视觉语言基础模型MatchVision
，它利用了足球视频中的时空信息，并在各种下游任务中表现出色;(iii)
在事件分类、评论生成和多视角犯规识别方面进行了广泛的实验和研究
。MatchVision在所有这些模型上都展示了最先进的性能，大大优于现有模型，这
突出了本文提出的数据和模型的优越性
。我们相信这项工作将为体育理解研究提供一个标准范例。
![](https://i-blog.csdnimg.cn/direct/ad11936a4a414daab40817b25806c8cf.png)
图 1 概述。展示了迄今为止最大的足球数据集，称为SoccerReplay-1988，以及第一个足球视觉语言基础模型MatchVision，能够完成各种任务，如事件分类和评论生成。
## 二、具体内容
### 1、数据集
在本文中，介绍了SoccerReplay-1988 迄今为止最大和最全面的多模式足球视频数据集，具有1988个完整的比赛视频和丰富的注释，如事件标签和文本注释。该数据集为开发足球领域的高级模型奠定了坚实的基础，并为评估足球理解模型建立了具有挑战性的新基准。此外，
本文已经协调了现有的数据集，使其与本文的数据集兼容，进一步扩展了该领域可用的数据资源。
考虑到原始数据中潜在的噪声，例如不相关的视频内容、不准确的时间戳和不完整的事件注释，本文
设计了一个用于管理的自动化数据管理管道，包括
\*\*(i)时间对齐\*\*
\*\*、\*\*
\*\*(ii)事件总结\*\*
\*\*和\*\*
\*\*(iii)匿名化\*\*
，如图2所示。
![](https://i-blog.csdnimg.cn/direct/b90d20401a884349bb8a2094138a85c9.png)
图 2 自动化数据管理管道
\*\*(i)
\*\*.\*\*
时间排列\*\*
在这里，
本文将比赛视频分成两部分，每一部分从开球时刻开始，并采用MatchTime[40]中公开的时间对齐模型，将文本解说时间戳与视频帧的时间戳同步
。
\*\*(ii). 事件总结\*\*
对于没有事件注释的示例，利用LLaMA-3-70B基于文本注释总结事件。
具体来说，
本文已经将SoccerNet[9]中的事件类别从17个扩展到24个类型，用于更细粒度的足球理解
，例如，将点球分为得分和未进球，并整合现代足球规则，如VAR。角球、进球、伤病、乌龙球、点球、罚失、红牌、第二张黄牌、换人、比赛开始(半场)、比赛结束(半场)、黄牌、界外球、任意球、被守门员扑救、射偏、解围、角球、越位、var、犯规(无牌)、统计和总结。“控球”和“球出界”等。
\*\*(iii). 匿名化\*\*
本文利用所有人从SoccerReplay-1988的元数据中获取球队实体名称，并将文本注释中任何匹配的名称替换为标准化的占位符
，例如“[PLAYER]”，“[team]”，“[COACH]”和“[裁判]”，以确保任务之间的一致性。
此外，
本文提出的数据管理管道可以无缝扩展到现有数据集
，将SoccerNet系列转换为我们统一的数据格式，称为SoccerNet-pro。这一扩展进一步扩大了用于足球理解任务的标准化数据集。
\*\*讨论\*\*
。总结一下，本文提出的SoccerReplay- 1988年的数据集展示了三个方面的进步: (i)它是
迄今为止最大的足球视频数据集，几乎 视频数量是现有数据集的四倍
;(二)
注释更加专业多样，更适合细粒度、综合性的足球理解任务
;(iii)它
采用自动化的注释管理管道，因此可扩展
，为未来的足球理解研究奠定坚实的数据基础。
### 2、MatchVision模型
#### （1）问题表述
在这项工作中，本文解决了分析足球视频片段的挑战，表示为
![V\epsilon R^{T\times 3\times H\times W}](https://latex.csdn.net/eq?V%5Cepsilon%20R%5E%7BT%5Ctimes%203%5Ctimes%20H%5Ctimes%20W%7D)
。本文的目标是利用视觉编码器(
![\Phi \_{MatchVision}](https://latex.csdn.net/eq?%5CPhi%20\_%7BMatchVision%7D)
)从这些片段中提取时空特征，然后由多个特定任务的头部进行处理，具体表述为:
![E,C,F= \Psi \left ( \Phi \_{MatchVision} \left (V\right )\right )](https://latex.csdn.net/eq?E%2CC%2CF%3D%20%5CPsi%20%5Cleft%20%28%20%5CPhi%20\_%7BMatchVision%7D%20%5Cleft%20%28V%5Cright%20%29%5Cright%20%29)
这里，
![\Psi = \left \{ \Psi \_{cls}, \Psi \_{Cmt},\Psi \_{Foul}\right \}](https://latex.csdn.net/eq?%5CPsi%20%3D%20%5Cleft%20%5C%7B%20%5CPsi%20\_%7Bcls%7D%2C%20%5CPsi%20\_%7BCmt%7D%2C%5CPsi%20\_%7BFoul%7D%5Cright%20%5C%7D)
表示特定于任务的头，而E、C和F分别表示输出事件类型、文本注释和犯规类型。这个统一的框架不仅可以有效地学习相关的时空特征，还可以促进各种下游任务的无缝集成，从而实现对足球比赛的全面理解。
#### （2）模型结构
利用文中数据集，
本文开发了第一个为各种足球理解任务量身定制的视觉语言基础模型，称为MatchVision
。它采用了前沿的
视觉语言基础模型作为主干
，例如SigLIP。通过在SoccerReplay-1988上对不同的视觉语言任务进行训练，进一步将框架视觉特征扩展为具有时间关注的时空特征，如图1所示。因此，MatchVision在包括事件分类和解说生成在内的各种任务中表现出很强的适应性，是全面理解足球视频的通用统一框架。
![](https://i-blog.csdnimg.cn/direct/6774eec654d6480e87afd2e454ba467f.png)
图 3 MatchVision概述:(a)模型体系结构及其时空特征提取过程;(b)视觉编码器预训练的细节，如监督训练和视频语言对比学习;(c)各种下游任务的具体头部的实现细节，包括评论生成、犯规识别和事件分类。
##### \*\*（a）Structure of MatchVision\*\*
MatchVision由三个关键部分组成:
\*\*(i)令牌嵌入\*\*
\*\*，\*\*
\*\*(ii)时空注意力块\*\*
\*\*，\*\*
\*\*(iii)聚合层\*\*
，如图3所示。
\*\*(i)令牌嵌入（Token Embedding）\*\*
按照Vision Transformer中的约定，将视频片段
![V= \left \{I \_{1},I \_{2}, \cdot \cdot \cdot ,I \_{T}\right \}](https://latex.csdn.net/eq?V%3D%20%5Cleft%20%5C%7BI%20\_%7B1%7D%2CI%20\_%7B2%7D%2C%20%5Ccdot%20%5Ccdot%20%5Ccdot%20%2CI%20\_%7BT%7D%5Cright%20%5C%7D)
中的每一帧
![\left ( I \_{i} \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20I%20\_%7Bi%7D%20%5Cright%20%29)
划分为
![M](https://latex.csdn.net/eq?M)
个大小为
![P\times P](https://latex.csdn.net/eq?P%5Ctimes%20P)
的不重叠的patch，这些patch跨越整个帧。
这些斑块被平面化成向量
![\left ( X\_{i}^{p} \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20X\_%7Bi%7D%5E%7Bp%7D%20%5Cright%20%29)
，其中
![p](https://latex.csdn.net/eq?p)
和
![i](https://latex.csdn.net/eq?i)
分别表示空间和时间位置。每个向量通过嵌入层
![\left ( \Phi \_{Emb} \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20%5CPhi%20\_%7BEmb%7D%20%5Cright%20%29)
转换为大小为
![R^{^{1\times D}}](https://latex.csdn.net/eq?R%5E%7B%5E%7B1%5Ctimes%20D%7D%7D)
的标记向量，然后添加空间位置嵌入
![\left (e\_{s}^{pos}\epsilon R^{M\times D} \right )](https://latex.csdn.net/eq?%5Cleft%20%28e\_%7Bs%7D%5E%7Bpos%7D%5Cepsilon%20R%5E%7BM%5Ctimes%20D%7D%20%5Cright%20%29)
。随后，我们将一个
![\left [ cls \right ]](https://latex.csdn.net/eq?%5Cleft%20%5B%20cls%20%5Cright%20%5D)
令牌与每一帧连接起来。最后，在所有帧的特征上添加一个时间位置嵌入
![\left (e\_{t}^{pos}\epsilon R^{T\times D} \right )](https://latex.csdn.net/eq?%5Cleft%20%28e\_%7Bt%7D%5E%7Bpos%7D%5Cepsilon%20R%5E%7BT%5Ctimes%20D%7D%20%5Cright%20%29)
，公式如下:
![y\_{i}= \left [ x\_{i}^{cls},\Phi \_{Emb}\left ( \left [ x\_{i}^{1},\cdot \cdot \cdot ,x\_{i}^{M} \right ] \right )+x\_{i}^{1},\cdot \cdot \cdot ,e\_{s}^{pos} \right ]](https://latex.csdn.net/eq?y\_%7Bi%7D%3D%20%5Cleft%20%5B%20x\_%7Bi%7D%5E%7Bcls%7D%2C%5CPhi%20\_%7BEmb%7D%5Cleft%20%28%20%5Cleft%20%5B%20x\_%7Bi%7D%5E%7B1%7D%2C%5Ccdot%20%5Ccdot%20%5Ccdot%20%2Cx\_%7Bi%7D%5E%7BM%7D%20%5Cright%20%5D%20%5Cright%20%29+x\_%7Bi%7D%5E%7B1%7D%2C%5Ccdot%20%5Ccdot%20%5Ccdot%20%2Ce\_%7Bs%7D%5E%7Bpos%7D%20%5Cright%20%5D)
![z= \left [ y\_{1},\cdot \cdot \cdot ,y\_{T} \right ]+e\_{t}^{pos}](https://latex.csdn.net/eq?z%3D%20%5Cleft%20%5B%20y\_%7B1%7D%2C%5Ccdot%20%5Ccdot%20%5Ccdot%20%2Cy\_%7BT%7D%20%5Cright%20%5D+e\_%7Bt%7D%5E%7Bpos%7D)
\*\*(ii)时空注意力块（Spatiotemporal Attention Block）\*\*
受TimeSformer的启发，
本文利用交错的时空注意力来整合足球视频中的时空信息
。具体来说，每个时空注意块包括一个时间自注意力机制层和一个空间自注意力机制层，分别为:
![\phi \_{t}\left ( \cdot \right )](https://latex.csdn.net/eq?%5Cphi%20\_%7Bt%7D%5Cleft%20%28%20%5Ccdot%20%5Cright%20%29)
和
![\phi \_{s}\left ( \cdot \right )](https://latex.csdn.net/eq?%5Cphi%20\_%7Bs%7D%5Cleft%20%28%20%5Ccdot%20%5Cright%20%29)
。
给定视频特征
![z\epsilon R^{T\times \left ( M+1 \right )\times D}](https://latex.csdn.net/eq?z%5Cepsilon%20R%5E%7BT%5Ctimes%20%5Cleft%20%28%20M+1%20%5Cright%20%29%5Ctimes%20D%7D)
，
本文交替使用时间和空间注意:时间注意促进不同帧内相同空间位置的令牌之间的交互，而空间注意使同一帧内令牌之间的交互成为可能
。
每一层都使用残差连接
。经过总共
![K](https://latex.csdn.net/eq?K)
个时空注意块后，得到的特征
![\left ( F \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20F%20%5Cright%20%29)
捕获了帧内和帧间的关系，即
![F=\left [ \phi \_{s}\left ( \phi \_{t}\left ( z \right ) \right ) \right ]^{K}\epsilon R^{T\times \left ( M+1 \right )\times D}](https://latex.csdn.net/eq?F%3D%5Cleft%20%5B%20%5Cphi%20\_%7Bs%7D%5Cleft%20%28%20%5Cphi%20\_%7Bt%7D%5Cleft%20%28%20z%20%5Cright%20%29%20%5Cright%20%29%20%5Cright%20%5D%5E%7BK%7D%5Cepsilon%20R%5E%7BT%5Ctimes%20%5Cleft%20%28%20M+1%20%5Cright%20%29%5Ctimes%20D%7D)
。
\*\*(iii)聚合层（Aggregation Layer）\*\*
为了获得视频级特征，本文在逐帧时空特征上使用了聚合层
。具体来说，对于第
![i](https://latex.csdn.net/eq?i)
帧，本文利用
空间自注意力机制将信息聚合
到它的
![\left [ cls \right ]](https://latex.csdn.net/eq?%5Cleft%20%5B%20cls%20%5Cright%20%5D)
令牌中，表示为
![\hat{F}\_{i}^{cls}=\Phi \_{Agg}\left ( F\_{i} \right )](https://latex.csdn.net/eq?%5Chat%7BF%7D\_%7Bi%7D%5E%7Bcls%7D%3D%5CPhi%20\_%7BAgg%7D%5Cleft%20%28%20F\_%7Bi%7D%20%5Cright%20%29)
。然后，本文将所有帧的
![\left [ cls \right ]](https://latex.csdn.net/eq?%5Cleft%20%5B%20cls%20%5Cright%20%5D)
标记连接起来，以获得最终的视频特征
![\left ( F\_{V} \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20F\_%7BV%7D%20%5Cright%20%29)
，该特征有效地封装了足球视频片段的时空特征，并使其适用于各种下游足球理解任务。这个过程可以表述为:
![F\_{V}=\Phi \_{MatchVision}\left ( V \right )=\left [ \hat{F}\_{1}^{cls},\cdot \cdot \cdot ,\hat{F}\_{T}^{cls} \right ]\epsilon R^{T\times D}](https://latex.csdn.net/eq?F\_%7BV%7D%3D%5CPhi%20\_%7BMatchVision%7D%5Cleft%20%28%20V%20%5Cright%20%29%3D%5Cleft%20%5B%20%5Chat%7BF%7D\_%7B1%7D%5E%7Bcls%7D%2C%5Ccdot%20%5Ccdot%20%5Ccdot%20%2C%5Chat%7BF%7D\_%7BT%7D%5E%7Bcls%7D%20%5Cright%20%5D%5Cepsilon%20R%5E%7BT%5Ctimes%20D%7D)
##### \*\*（b）MatchVision Pretraining\*\*
在这一部分中，
本文的目标是用三元组样本(
![\left \{ V,E,C \right \}](https://latex.csdn.net/eq?%5Cleft%20%5C%7B%20V%2CE%2CC%20%5Cright%20%5C%7D)
)预训练视觉编码器，包括视频，事件标签和文本注释
。具体来说，本文
研究了两种不同的预训练策略:
\*\*监督分类\*\*
和
\*\*视频语言对比学习\*\*
。
\*\*监督分类。\*\*
预训练视觉编码器的一种方法是对事件分类进行监督学习。具体来说，提取的视觉特征
![\left ( F\_{V} \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20F\_%7BV%7D%20%5Cright%20%29)
被一个时间自注意力机制层聚合
成一个可学习的
![\left [ cls \right ]](https://latex.csdn.net/eq?%5Cleft%20%5B%20cls%20%5Cright%20%5D)
令牌，记为
![F\_{V}^{cls}](https://latex.csdn.net/eq?F\_%7BV%7D%5E%7Bcls%7D)
，类似于上面提到的空间聚合。然后将该标记输入线性分类器，并使用交叉熵损失进行训练以进行事件分类。目标记为
![L\_{sup}](https://latex.csdn.net/eq?L\_%7Bsup%7D)
。
\*\*视频语言对比学习。\*\*
作为一种替代，也可以用视频文本预训练我们的视觉编码器对比学习。具体来说，本文对视频特征进行简单平均池化来获得聚合的视觉特征（
![F\_{V}^{Avg}](https://latex.csdn.net/eq?F\_%7BV%7D%5E%7BAvg%7D)
），并用文本编码器（
![\Phi \_{Text}](https://latex.csdn.net/eq?%5CPhi%20\_%7BText%7D)
）对文本评论（
![C](https://latex.csdn.net/eq?C)
）进行编码。本文使用SigLIP中使用的sigmoid损失（
![L\_{sigmoid}](https://latex.csdn.net/eq?L\_%7Bsigmoid%7D)
）训练模型。请注意，一些视频片段可能具有高度相似的评论，例如，“游戏开始”，在计算损失函数时，我们将同一批中具有高度相似度的评论视为正样本。可以表示为:
![L\_{contra}=L\_{sigmoid}\left (F\_{V}^{Avg},\Phi \_{Text}\left ( C \right ) \right )](https://latex.csdn.net/eq?L\_%7Bcontra%7D%3DL\_%7Bsigmoid%7D%5Cleft%20%28F\_%7BV%7D%5E%7BAvg%7D%2C%5CPhi%20\_%7BText%7D%5Cleft%20%28%20C%20%5Cright%20%29%20%5Cright%20%29)
##### \*\*（c）Downstream Tasks\*\*
在上述训练之后，MatchVision现在可以作为一个通用的视觉编码器，将足球视频片段映射到视觉特征
![\left ( F\_{V} \right )](https://latex.csdn.net/eq?%5Cleft%20%28%20F\_%7BV%7D%20%5Cright%20%29)
中，用于训练特定任务的头部
![\Psi = \left \{ \Psi \_{cls}, \Psi \_{Cmt},\Psi \_{Foul}\right \}](https://latex.csdn.net/eq?%5CPsi%20%3D%20%5Cleft%20%5C%7B%20%5CPsi%20\_%7Bcls%7D%2C%20%5CPsi%20\_%7BCmt%7D%2C%5CPsi%20\_%7BFoul%7D%5Cright%20%5C%7D)
，用于不同的
下游任务，包括:
(i)事件分类
，
(ii)评论生成
，以及
(iii)犯规识别
。
(i)事件分类（
![\Psi \_{cls}](https://latex.csdn.net/eq?%5CPsi%20\_%7Bcls%7D)
）
类似于上面提到的监督训练，本文连接一个可学习的
![\left [ cls \right ]](https://latex.csdn.net/eq?%5Cleft%20%5B%20cls%20%5Cright%20%5D)
令牌，通过时间自注意力机制来聚合帧相关的视觉特征。然后将该令牌输入线性分类器进行事件分类
。事件分类头(
![\Psi \_{cls}](https://latex.csdn.net/eq?%5CPsi%20\_%7Bcls%7D)
)可以在视觉编码器冻结的情况下使用
\*\*交叉熵损失\*\*
进行训练。
(ii)评论生成（
![\Psi \_{Cmt}](https://latex.csdn.net/eq?%5CPsi%20\_%7BCmt%7D)
）
在评论生成方面，本文遵循MatchTime的设计。具体来说，
评论生成头(
![\Psi \_{Cmt}](https://latex.csdn.net/eq?%5CPsi%20\_%7BCmt%7D)
)使用一个percepver聚合器来整合视觉特征，然后由一个可训练的MLP进行投影，作为大型语言模型(LLM)的前缀嵌入。随后，一个现成的大型语言模型将这些嵌入解码为文本注释
。本文采用
\*\*负对数似然损失\*\*
，通常用于自动回归下一个令牌预测。
(iii)犯规识别（
![\Psi \_{Foul}](https://latex.csdn.net/eq?%5CPsi%20\_%7BFoul%7D)
）
犯规识别任务将来自同一场景的多视图视频作为输入，每个样本都标注了犯规类别(8种类型)和严重程度(4个级别)。
本文用MatchVision对这些多视点视频进行编码，然后按照通常的做法，通过最大池化或平均池化，将提取的特征聚合到单个特征向量中。随后，犯规识别头(
![\Psi \_{Foul}](https://latex.csdn.net/eq?%5CPsi%20\_%7BFoul%7D)
)采用共享MLP和两个特定任务的线性分类器，分别预测犯规类型和严重程度
。与事件分类类似，本文结合犯规类型和严重程度分类上的
\*\*交叉熵损失\*\*
来联合训练
![\Psi \_{Foul}](https://latex.csdn.net/eq?%5CPsi%20\_%7BFoul%7D)
。
## 三、总结
在这篇论文中，建立了一个统一的、可扩展的足球理解多模态框架。具体来说，本文介绍了SoccerReplay-1988，这是迄今为止最大、最全面的足球视频数据集，由自动管理管道注释。这为开发多模式足球理解模型提供了坚实的基础，并成为一个更具挑战性的基准。在此基础上，本文开发了第一个足球视觉语言基础模型，称为MatchVision，它有效地利用了足球视频中的时空信息，可以应用于各种任务，如事件分类和评论生成。大量实验证明了本文提出的模型的优越性，MatchVision在现有基准和本文新建立的基准上都取得了最先进的性能。我们相信，这项工作将为未来体育理解的研究树立一个可行的、普遍的范式。