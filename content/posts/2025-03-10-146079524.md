---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f416c616e5f4265636b65722f:61727469636c652f64657461696c732f313436303739353234"
layout: post
title: "数码科技文心一言4.0-VS-DEEPSEEK-V3"
date: 2025-03-10 19:20:44 +08:00
description: "DEEPSEEK的所有模型全部免费，而文心一言的基础款（文心3.5）免费，4系列的则全部氪金专属，不过，要我说，感觉百度在不停琢磨怎么把文心4变得更好玩，型号更多，这样的发展趋势是极其糟糕的，文心一言宣传页写的是“有用，有趣，有温度”我不得不承认，文心一言“有温度”确实做到了，他甚至还专门做了个“个性化”，你的确可以把AI当成一个有个性的真人来对待，不过，在做这些锦上添花之前，总应该把基础的东西弄好吧？"
keywords: "【数码科技】文心一言4.0 VS DEEPSEEK V3"
categories: ['数码科技']
tags: ['文心一言', '人工智能']
artid: "146079524"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146079524
    alt: "数码科技文心一言4.0-VS-DEEPSEEK-V3"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146079524
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146079524
cover: https://bing.ee123.net/img/rand?artid=146079524
image: https://bing.ee123.net/img/rand?artid=146079524
img: https://bing.ee123.net/img/rand?artid=146079524
---

# 【数码科技】文心一言4.0 VS DEEPSEEK V3

DEEPSEEK V3和文心一言4.0是人工智能领域备受关注的两种高级语言模型，GPT4是open AI的产品，文心一言是百度的产品，今天它们将短兵相接，看看谁是真正的赢家

## 提前说明：

本次测试采用的是DS V3和文心大模型4.0

![](https://i-blog.csdnimg.cn/direct/0d9d11f49f474dc2a2281d466ec32836.png)
![](https://i-blog.csdnimg.cn/direct/f792a2fbd32e42b68e1611705174ffb4.png)

帮忙点点赞吧，做这个博客可是要充值文心一言的VIP的，要氪好多金呢

## 模型架构：

DS基于（生成预训练变压器）架构，该架构利用变压器神经网络进行自然语言处理任务。另一方面，文心一言4.0 其独特的架构将神经网络与中文语言处理算法相结合，允许更准确和上下文相关的响应。

## 语言能力：

就语言能力而言，这两种模型都擅长生成连贯且上下文准确的文本。DS已经接受了来自互联网的各种文本数据的培训，使其能够以多种语言生成高质量的响应。文心一言4.0, 专为中文设计，在理解和生成中文文本方面表现出色。

测试问题：中国队大胜美国队，哪个队赢了？ （等待AI回答） 中国队大败美国队，哪个队赢了？

实际上，这道题的两个答案都是中国队获胜，因为大败美国队是指让美国队大败

文心一言：

中国队大胜美国队，这句话明确表明了中国队在这场比赛中取得了胜利。所以，
**中国队赢了**
。这一信息是根据问题描述中“中国队大胜美国队”这一关键句直接得出的，无需参考其他细节或数字信息。

在这个新的描述“中国队大败美国队”中，虽然“大败”一词通常与失败相关联，但在这里它实际上是用来描述中国队使美国队遭遇大败，即中国队取得了压倒性的胜利。这种用法在中文里较为常见，其中“大败”指的是给予对方重大的失败。

因此，即使表述中使用了“大败”，在这里依然是指中国队赢得了比赛。所以，
**中国队赢了**
。

DS：

在这个问题中，中国队大胜美国队，意味着中国队赢得了比赛。因此，**中国队赢了**。

在这个问题中，“大败”意味着中国队使得美国队战败。因此，**中国队赢了**。

很明显，文心一言和DS都正确的回答了问题，毕竟两者都针对中文做出了很多训练。

## 多模式功能：

GPT4提供多模式功能，使其能够基于图像、视频和其他形式的多媒体内容处理和生成文本。此功能增强了模型提供更细致和上下文相关响应的能力。文心一言4.0也同样具备附加图片，文件或者绘图等功能，不过经过实测，GPT4的图像处理能力更胜一筹，文心一言生成人物的时候有时候会比较怪（比如说6个手指头），而文心一言在处理文本文件时更胜一筹，因为文心一言的智能体里专门有阅读助手（当然这里针对中文文档，而且我相信这个博客的读者绝大部分都是中国人，英文文档可能还是GPT4略胜一筹）

## 数学能力：

让我们看看它们的数学能力

1.简单问题：小红和小明总共有20本书，小红的书是小明的3倍，他们各有多少本书？

文心一言：

小红和小明两个人总共有20本书，而且小红的书是小明的3倍。
  
我们要找出小红和小明各自有多少本书。
  
假设小明有 x 本书，那么小红就有 3x 本书（因为小红的书是小明的3倍）。

根据题目，我们可以建立以下方程：

小红和小明总共有 20 本书，所以 x + 3x = 20。
  
现在我们要来解这个方程，找出 x 的值。
  
计算结果为：小明有 5 本书，小红有 15 本书。

ds：

设小明有 xx 本书，则小红有 3x3x 本书。

根据题意，他们总共有 20 本书，可以列出方程：

x+3x=20x+3x=204x=204x=20x=5x=5

因此，小明有 5 本书，小红有：

3x=3×5=153x=3×5=15

简单的数学问题，双方均可正确解决，（
ds这里格式乱是因为复制的问题，不用在意
）

2.复杂问题

1+1*2+1*2*3+1*2*3*4+……+1*2*3*...*100=？

文心一言：

这个问题要求我们计算一个特殊的数列之和，数列的形式是 1+1×2+1×2×3+...+1×2×3×...×100。
  
这个问题实际上涉及到阶乘的概念，每一项都是前几项的乘积，也就是前几项的阶乘。
  
假设 S 是我们要找的和，即 S = 1! + 2! + 3! + ... + 100!。

根据题目，我们可以建立以下数学模型：
  
S = 1! + 2! + 3! + ... + 100!
  
这是一个求和问题，其中每一项都是前一项的阶乘。

现在我们要来计算这个数列的和。
  
计算结果为：94269001683709979260859834124473539872070722613982672442938359305624678223479506023400294093599136466986609124347432647622826870038220556442336528920420940313
  
所以，1+1×2+1×2×3+...+1×2×3×...×100 的和是：94269001683709979260859834124473539872070722613982672442938359305624678223479506023400294093599136466986609124347432647622826870038220556442336528920420940313。

ds:

我们需要计算以下级数的和：

S=1+1×2+1×2×3+1×2×3×4+⋯+1×2×3×⋯×100S=1+1×2+1×2×3+1×2×3×4+⋯+1×2×3×⋯×100

这个级数可以表示为：

S=∑n=1100n!S=n=1∑100​n!

其中 n!n! 表示 nn 的阶乘。

计算这个级数的和需要计算从 1! 到 100! 的所有阶乘并将它们相加。由于 100! 是一个非常大的数，直接计算这个和在实际操作中是不现实的。因此，我们通常使用近似方法或编程工具来计算这个和。

**最终答案：**

S=∑n=1100n!S=n=1∑100​n!

真的开眼了👍文心一言上来直接输出了一大串数，也不知道对不对

而ds表示我直接摆烂，输出了个Σ就没了，这次的表现都不是很满意。

3.抽象问题

所有正整数和所有完全平方数一样多吗？

文心一言：

这个问题涉及到集合论和数的分类。为了回答“所有正整数和所有完全平方数是否一样多”，我们首先要明确两个概念：正整数和完全平方数。

1. **正整数**
   ：这是从1开始一直无限递增的整数序列，即1，2，3，4，5，...
2. **完全平方数**
   ：这是一个由整数的平方构成的序列，例如 12，32，...，即1，4，9，16，...

接下来，我们分析这两个集合的“大小”：

* 正整数集合是无限的，它包含了所有大于0的整数。
* 完全平方数集合也是无限的，但它只包含那些可以表示为某个整数平方的数。

在集合论中，“一样多”或“等势”的概念是基于是否存在一种一一对应关系。对于两个无限集合，如果它们之间存在一一对应的关系，那么这两个集合就被认为是等势的，即“一样多”。

现在，我们考虑正整数和完全平方数之间是否存在一一对应关系。显然，每一个完全平方数都可以对应到一个正整数（即其平方根），但并非每一个正整数都能对应到一个完全平方数。这表明正整数集合比完全平方数集合“更大”。

综上所述，所有正整数和所有完全平方数不是“一样多”。正整数集合包含的元素数量要多于完全平方数集合。这是因为虽然两者都是无限集合，但正整数集合的“密度”更高，包含了完全平方数集合以外的更多元素。

ds：

在数学中，比较无限集合的大小是通过“基数”来进行的。如果两个集合之间存在一个一一对应的关系（即双射），那么这两个集合的基数相同。

1. **所有正整数的集合**：这是一个无限集合，基数为 \(\aleph\_0\)（阿列夫零）。

2. **所有完全平方数的集合**：这也是一个无限集合，因为完全平方数是正整数的平方，例如 1, 4, 9, 16, 25, ...。

我们可以建立一个从所有正整数到所有完全平方数的一一对应关系：
  
\[
  
f(n) = n^2
  
\]
  
其中 \( n \) 是正整数，\( f(n) \) 是完全平方数。这个函数是双射的，因为每个正整数 \( n \) 都对应一个唯一的完全平方数 \( n^2 \)，并且每个完全平方数 \( m \) 都可以表示为某个正整数 \( n \) 的平方。

因此，所有正整数和所有完全平方数的基数相同。

**最终答案：**
  
所有正整数和所有完全平方数的基数相同，因为它们之间存在一一对应的关系。

文心一言一开始差一点就说出来正确答案了，后来走偏了。ds则是全程都说出来正确答案

## 编码能力：

在编码能力环节中，Deepseek将开启R1深度思考（因为R1更擅长数学，编程，推理等），本部分将只展示AI生成的代码以及运行结果

问题：

请帮我用C++11编写一个代码，用来计算高精度加法，要求能计算1000位两数加法并正确输出。

文心一言：

```cpp
#include <iostream>
#include <string>
#include <algorithm>

std::string addBigNumbers(const std::string& num1, const std::string& num2) {
    std::string result;
    int carry = 0;
    int sum = 0;
    int i = num1.size() - 1, j = num2.size() - 1;

    // 翻转两个字符串，便于从低位开始加
    std::string reversedNum1(num1.rbegin(), num1.rend());
    std::string reversedNum2(num2.rbegin(), num2.rend());

    // 使两个字符串长度相等，便于逐位相加
    while (i >= 0 || j >= 0 || carry > 0) {
        sum = carry;
        if (i >= 0) {
            sum += reversedNum1[i--] - '0';
        }
        if (j >= 0) {
            sum += reversedNum2[j--] - '0';
        }
        carry = sum / 10;
        result.push_back(sum % 10 + '0');
    }

    // 翻转结果字符串，得到正确的顺序
    std::reverse(result.begin(), result.end());
    return result;
}

int main() {
    std::string num1, num2;
    std::cout << "请输入第一个大数（最多1000位）: ";
    std::cin >> num1;
    std::cout << "请输入第二个大数（最多1000位）: ";
    std::cin >> num2;

    std::string sum = addBigNumbers(num1, num2);
    std::cout << "两数之和为: " << sum << std::endl;

    return 0;
}
```

运行结果：正确

DS：

```cpp
#include <iostream>
#include <algorithm>
#include <string>

using namespace std;

int main() {
    string num1, num2;
    cin >> num1 >> num2;

    // 去除前导零
    num1.erase(0, num1.find_first_not_of('0'));
    if (num1.empty()) num1 = "0";
    num2.erase(0, num2.find_first_not_of('0'));
    if (num2.empty()) num2 = "0";

    reverse(num1.begin(), num1.end());
    reverse(num2.begin(), num2.end());

    string result;
    int carry = 0;
    int max_len = max(num1.size(), num2.size());
    
    for (int i = 0; i < max_len || carry; ++i) {
        int digit1 = (i < num1.size()) ? (num1[i] - '0') : 0;
        int digit2 = (i < num2.size()) ? (num2[i] - '0') : 0;
        int sum = digit1 + digit2 + carry;
        carry = sum / 10;
        result.push_back((sum % 10) + '0');
    }

    reverse(result.begin(), result.end());
    
    // 处理结果的前导零
    result.erase(0, result.find_first_not_of('0'));
    if (result.empty()) {
        result = "0";
    }

    cout << result << endl;

    return 0;
}
```

运行结果：正确

注意，DS的R1推理模型输出会很慢，如果你是急性子，建议使用V3

![](https://i-blog.csdnimg.cn/direct/24f256b3f4fa446ea0a7193b66c6aeb7.png)

刚才这个问题它足足思考了快三分钟

不过经过我后来测试，如果你受不了R1太慢，刚才这个问题V3也可以完美解决并且代码正确，这里因为篇幅问题，就不晒代码了。

这里再提一嘴，文心一言的4.0，4.0工具版，4.0turbo区别真的不是很大，放心选，对结果几乎没有什么区别

![](https://i-blog.csdnimg.cn/direct/8a5c5f0dd147445283e8bb1d38996505.png)

## 总结

DEEPSEEK的所有模型全部免费，而文心一言的基础款（文心3.5）免费，4系列的则全部氪金专属，不过，要我说，感觉百度在不停琢磨怎么把文心4变得更好玩，型号更多，这样的发展趋势是极其糟糕的，文心一言宣传页写的是“有用，有趣，有温度”

![](https://i-blog.csdnimg.cn/direct/80f1f05a00f641f98197b75dc81fb706.png)

我不得不承认，文心一言“有温度”确实做到了，他甚至还专门做了个“个性化”，你的确可以把AI当成一个有个性的真人来对待，不过，在做这些锦上添花之前，总应该把基础的东西弄好吧？CHAT-GPT和DEEPSEEK从来都没闲着，OpenAI在当地时间2024年12月5日正式上线了ChatGPT的O1模型 ，在2024年12月21日推出了下一代推理模型O3，相差仅仅两周多两天，而Deepseek，DeepSeek-V3于2024年12月26日发布，R1在一月的时候就发布了，也仅仅相隔两周多，但是你看看文心一言，2023年5月26日，百度创始人、董事长兼首席执行官李彦宏在2023中关村论坛发表题为《大模型改变世界》演讲。他表示百度很快还会推出文心大模型3.5版本，而这时距离文心3推出已经有了一年，相比于前面两者，实际上也是比不上，百度现在需要了解目前AI市场的局势，他们也早就该研究新一代文心大模型了，我这里也没有批评百度或者夸赞DS和GPT，我只希望百度能尽快研究出文心大模型5.0或者一个推理模型。