---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f4a696d6d79476f6f6e672f:61727469636c652f64657461696c732f313436323333393937"
layout: post
title: "DeepSeek-R1深度解读"
date: 2025-03-13 16:48:21 +0800
description: "deepseek提出了一种通过强化学习（RL）激励大语言模型（LLMs）推理能力的方法，个人认为最让人兴奋的点是：通过RL发现了一个叫“Aha Moment”的现象，这个时刻发生在模型的中间版本中。在这个阶段，DeepSeek学会为问题分配更多的思考时间。性能直接达到国际顶流水平，这不仅实现了了大语言生成模型到推理模型0-1的越阶，而且成功打破美国对AI技术和高端芯片的封锁。"
keywords: "DeepSeek-R1深度解读"
categories: ['未分类']
tags: ['算法', '深度学习', '人工智能']
artid: "146233997"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146233997
    alt: "DeepSeek-R1深度解读"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146233997
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146233997
cover: https://bing.ee123.net/img/rand?artid=146233997
image: https://bing.ee123.net/img/rand?artid=146233997
img: https://bing.ee123.net/img/rand?artid=146233997
---

# DeepSeek-R1深度解读

![](https://i-blog.csdnimg.cn/direct/259f91a09c6b401bb20eda4fa3cd3908.png)

deepseek提出了一种通过强化学习（RL）激励大语言模型（LLMs）推理能力的方法，个人认为最让人兴奋的点是：通过RL发现了一个叫“Aha Moment”的现象，这个时刻发生在模型的中间版本中。在这个阶段，DeepSeek学会为问题分配更多的思考时间。性能直接达到国际顶流水平，这不仅实现了了大语言生成模型到推理模型0-1的越阶，而且成功打破美国对AI技术和高端芯片的封锁。

同时发布了 DeepSeek-R1-Zero 和 DeepSeek-R1 模型，通过纯 RL 训练和多阶段训练提升了模型在数学、编码等任务中的推理能力，并通过模型蒸馏将推理能力迁移到更小的模型。

#### 研究背景与目标

1. **LLM 推理能力的重要性**
   ：近年来，大型语言模型（LLMs）在推理能力上取得显著进展，如 OpenAI 的 o1 系列模型通过增加思维链（CoT）长度提升了数学、编码等任务的表现。然而，如何有效提升测试时的推理能力仍是研究热点。
2. **现有方法的局限性**
   ：现有方法如过程奖励模型、搜索算法等虽有一定效果，但未达到与 OpenAI o1 系列模型相当的通用推理性能。
3. **研究目标**
   ：探索纯强化学习（RL）在提升 LLM 推理能力中的潜力，无需监督微调（SFT），并通过多阶段训练和模型蒸馏进一步优化性能。

#### 模型架构与方法

1. **DeepSeek-R1-Zero**
   * **纯 RL 训练**
     ：直接在基础模型（DeepSeek-V3-Base）上应用 Group Relative Policy Optimization (GRPO) 算法，无需 SFT 数据。GRPO公式看着十分复杂，拆解开来看看并不难懂：
     ![](https://i-blog.csdnimg.cn/direct/13382c151a7c4aa99b66ebfa91be2728.png)
     其中：
     ![\theta](https://latex.csdn.net/eq?%5Ctheta)
     ：待优化的策略参数；
     ![G](https://latex.csdn.net/eq?G)
     ：每个问题生成的候选答案数量（组大小）；
     ![\pi _{\theta _{old}}](https://latex.csdn.net/eq?%5Cpi%20_%7B%5Ctheta%20_%7Bold%7D%7D)
     ：旧策略（即上一轮迭代的策略）；
     ![A_{i}](https://latex.csdn.net/eq?A_%7Bi%7D)
     ：优势函数（Advantage），反映第i个答案的相对质量，将原始奖励归一化；
     ![\varepsilon](https://latex.csdn.net/eq?%5Cvarepsilon)
     ：剪切阈值（通常取0.1-0.3）；
     ![\beta](https://latex.csdn.net/eq?%5Cbeta)
   * **奖励模型**
     ：基于规则的奖励系统，包括准确性奖励（验证答案正确性）和格式奖励（强制使用特定格式输出推理过程）。
   * **训练模板**
     ：引导模型生成推理过程和答案，结构化为 “推理过程” 和 “答案” 两部分。
   * **自进化与表现**
     ：在 AIME 2024 基准测试中，pass@1 从 15.6% 提升至 71.0%，多数投票后达 86.7%，接近 OpenAI-o1-0912 的水平。模型还表现出自我验证、反思等能力。
     ![](https://i-blog.csdnimg.cn/direct/1066eb61c28e4b4284bb1eefa67ff2a7.png)
     “顿悟时刻”。这个模型学会了用拟人化的语气重新思考。
2. **DeepSeek-R1**
   * **冷启动数据**
     ：收集数千条长 CoT 数据进行微调，解决 DeepSeek-R1-Zero 可读性差、语言混合等问题。
   * **多阶段训练**
     ：包括冷启动微调、推理导向的 RL（加入语言一致性奖励）、拒绝采样生成新 SFT 数据、多场景 RL（结合奖励信号优化有用性和无害性）。
   * **性能提升**
     ：在 AIME 2024 上 pass@1 达 79.8%，超过 OpenAI-o1-1217，MATH-500 达 97.3%，与 o1-1217 持平。
3. **模型蒸馏**
   * **方法**
     ：使用 DeepSeek-R1 生成的 800k 数据微调开源模型（如 Qwen、Llama 系列），仅进行 SFT 而不进行 RL。
   * **结果**
     ：蒸馏后的模型在多个基准测试中表现优异，如 DeepSeek-R1-Distill-Qwen-32B 在 AIME 2024 上 pass@1 达 72.6%，超过 o1-mini。

#### 实验结果

1. **基准测试表现**
   * **数学任务**
     ：DeepSeek-R1 在 AIME 2024（79.8%）和 MATH-500（97.3%）上接近或超过 OpenAI-o1-1217。
   * **编码任务**
     ：在 Codeforces 上 Elo 评分为 2029，超过 96.3% 的人类选手；LiveCodeBench pass@1 达 65.9%。
   * **知识问答**
     ：MMLU（90.8%）、GPQA Diamond（71.5%）等任务上优于 DeepSeek-V3，稍逊于 o1-1217。
2. **蒸馏模型对比**
   ：蒸馏后的小模型（如 14B、32B）在多个任务上显著优于同类开源模型，证明了大模型推理模式的可迁移性。

#### 讨论与结论

1. **蒸馏 vs. RL**
   ：蒸馏更高效，小模型通过学习大模型的推理模式即可获得优秀性能；而直接对小模型进行 RL 训练需大量计算资源且效果有限。
2. **未成功尝试**
   ：过程奖励模型（PRM）因难以定义细粒度步骤和奖励欺诈问题效果不佳；蒙特卡洛树搜索（MCTS）因搜索空间过大和价值模型训练困难未能显著提升性能。
3. **结论**
   ：纯 RL 可有效提升 LLM 推理能力，多阶段训练和冷启动数据进一步优化了模型表现。模型蒸馏为小模型赋予了强大的推理能力，开源模型将推动相关研究。

#### 未来工作方向

1. **通用能力扩展**
   ：提升在函数调用、多轮对话等任务上的表现。
2. **语言混合问题**
   ：优化非中 / 英文查询的处理能力。
3. **提示工程优化**
   ：减少模型对提示的敏感性，提升零样本性能。
4. **软件工程任务**
   ：增加相关 RL 训练数据，提高在软件工程项目中的表现。