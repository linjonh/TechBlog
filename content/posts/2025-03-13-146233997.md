---
layout: post
title: "DeepSeek-R1深度解读"
date: 2025-03-13 16:48:21 +0800
description: "deepseek提出了一种通过强化学习（RL）激励大语言模型（LLMs）推理能力的方法，个人认为最让人兴奋的点是：通过RL发现了一个叫“Aha Moment”的现象，这个时刻发生在模型的中间版本中。在这个阶段，DeepSeek学会为问题分配更多的思考时间。性能直接达到国际顶流水平，这不仅实现了了大语言生成模型到推理模型0-1的越阶，而且成功打破美国对AI技术和高端芯片的封锁。"
keywords: "DeepSeek-R1深度解读"
categories: ['未分类']
tags: ['算法', '深度学习', '人工智能']
artid: "146233997"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146233997
    alt: "DeepSeek-R1深度解读"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146233997
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146233997
cover: https://bing.ee123.net/img/rand?artid=146233997
image: https://bing.ee123.net/img/rand?artid=146233997
img: https://bing.ee123.net/img/rand?artid=146233997
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     DeepSeek-R1深度解读
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     <img alt="" height="613" src="https://i-blog.csdnimg.cn/direct/259f91a09c6b401bb20eda4fa3cd3908.png" width="970"/>
    </p>
    <p>
     deepseek提出了一种通过强化学习（RL）激励大语言模型（LLMs）推理能力的方法，个人认为最让人兴奋的点是：通过RL发现了一个叫“Aha Moment”的现象，这个时刻发生在模型的中间版本中。在这个阶段，DeepSeek学会为问题分配更多的思考时间。性能直接达到国际顶流水平，这不仅实现了了大语言生成模型到推理模型0-1的越阶，而且成功打破美国对AI技术和高端芯片的封锁。
    </p>
    <p>
     同时发布了 DeepSeek-R1-Zero 和 DeepSeek-R1 模型，通过纯 RL 训练和多阶段训练提升了模型在数学、编码等任务中的推理能力，并通过模型蒸馏将推理能力迁移到更小的模型。
    </p>
    <h4 id="%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87" name="%E7%A0%94%E7%A9%B6%E8%83%8C%E6%99%AF%E4%B8%8E%E7%9B%AE%E6%A0%87">
     研究背景与目标
    </h4>
    <ol>
     <li>
      <strong>
       LLM 推理能力的重要性
      </strong>
      ：近年来，大型语言模型（LLMs）在推理能力上取得显著进展，如 OpenAI 的 o1 系列模型通过增加思维链（CoT）长度提升了数学、编码等任务的表现。然而，如何有效提升测试时的推理能力仍是研究热点。
     </li>
     <li>
      <strong>
       现有方法的局限性
      </strong>
      ：现有方法如过程奖励模型、搜索算法等虽有一定效果，但未达到与 OpenAI o1 系列模型相当的通用推理性能。
     </li>
     <li>
      <strong>
       研究目标
      </strong>
      ：探索纯强化学习（RL）在提升 LLM 推理能力中的潜力，无需监督微调（SFT），并通过多阶段训练和模型蒸馏进一步优化性能。
     </li>
    </ol>
    <h4 id="%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%96%B9%E6%B3%95" name="%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%96%B9%E6%B3%95">
     模型架构与方法
    </h4>
    <ol>
     <li>
      <strong>
       DeepSeek-R1-Zero
      </strong>
      <ul>
       <li>
        <p>
         <strong>
          纯 RL 训练
         </strong>
         ：直接在基础模型（DeepSeek-V3-Base）上应用 Group Relative Policy Optimization (GRPO) 算法，无需 SFT 数据。GRPO公式看着十分复杂，拆解开来看看并不难懂：
         <img alt="" height="186" src="https://i-blog.csdnimg.cn/direct/13382c151a7c4aa99b66ebfa91be2728.png" width="918">
          其中：
          <img alt="\theta" class="mathcode" src="https://latex.csdn.net/eq?%5Ctheta">
           ：待优化的策略参数；
           <img alt="G" class="mathcode" src="https://latex.csdn.net/eq?G">
            ：每个问题生成的候选答案数量（组大小）；
            <img alt="\pi _{\theta _{old}}" class="mathcode" src="https://latex.csdn.net/eq?%5Cpi%20_%7B%5Ctheta%20_%7Bold%7D%7D">
             ：旧策略（即上一轮迭代的策略）；
             <img alt="A_{i}" class="mathcode" src="https://latex.csdn.net/eq?A_%7Bi%7D">
              ：优势函数（Advantage），反映第i个答案的相对质量，将原始奖励归一化；
              <img alt="\varepsilon" class="mathcode" src="https://latex.csdn.net/eq?%5Cvarepsilon">
               ：剪切阈值（通常取0.1-0.3）；
               <img alt="\beta" class="mathcode" src="https://latex.csdn.net/eq?%5Cbeta"/>
               ：KL散度正则化系数。
               <strong>
                <span style="color:#fe2c24">
                 红框公式
                </span>
                ：
               </strong>
               最原始的强化学习公式，衡量新策略与旧策略生成答案的概率差异。若概率比&gt;1，表示新策略更倾向于生成该答案。
               <span style="color:#ffd900">
                <strong>
                 黄框公式
                </strong>
               </span>
               ：剪切机制，设置奖励上下阈值clip一下，防止策略更新幅度过大，确保训练稳定性。
               <strong>
                <span style="color:#4da8ee">
                 蓝框公式
                </span>
                ：
               </strong>
               将原始奖励和clip后的奖励取最小值。
               <strong>
                <span style="color:#a2e043">
                 绿框公式
                </span>
                ：
               </strong>
               KL散度惩罚，该惩罚项避免模型过度拟合短期奖励，维持生成文本的多样性和安全性。
               <strong>
                黑色公式：
               </strong>
               对同一问题q生成G个答案，使用组内比较代替传统Critic模型。
              </img>
             </img>
            </img>
           </img>
          </img>
         </img>
        </p>
       </li>
       <li>
        <strong>
         奖励模型
        </strong>
        ：基于规则的奖励系统，包括准确性奖励（验证答案正确性）和格式奖励（强制使用特定格式输出推理过程）。
       </li>
       <li>
        <strong>
         训练模板
        </strong>
        ：引导模型生成推理过程和答案，结构化为 “推理过程” 和 “答案” 两部分。
       </li>
       <li>
        <strong>
         自进化与表现
        </strong>
        ：在 AIME 2024 基准测试中，pass@1 从 15.6% 提升至 71.0%，多数投票后达 86.7%，接近 OpenAI-o1-0912 的水平。模型还表现出自我验证、反思等能力。
        <img alt="" height="603" src="https://i-blog.csdnimg.cn/direct/1066eb61c28e4b4284bb1eefa67ff2a7.png" width="937"/>
        “顿悟时刻”。这个模型学会了用拟人化的语气重新思考。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       DeepSeek-R1
      </strong>
      <ul>
       <li>
        <strong>
         冷启动数据
        </strong>
        ：收集数千条长 CoT 数据进行微调，解决 DeepSeek-R1-Zero 可读性差、语言混合等问题。
       </li>
       <li>
        <strong>
         多阶段训练
        </strong>
        ：包括冷启动微调、推理导向的 RL（加入语言一致性奖励）、拒绝采样生成新 SFT 数据、多场景 RL（结合奖励信号优化有用性和无害性）。
       </li>
       <li>
        <strong>
         性能提升
        </strong>
        ：在 AIME 2024 上 pass@1 达 79.8%，超过 OpenAI-o1-1217，MATH-500 达 97.3%，与 o1-1217 持平。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       模型蒸馏
      </strong>
      <ul>
       <li>
        <strong>
         方法
        </strong>
        ：使用 DeepSeek-R1 生成的 800k 数据微调开源模型（如 Qwen、Llama 系列），仅进行 SFT 而不进行 RL。
       </li>
       <li>
        <strong>
         结果
        </strong>
        ：蒸馏后的模型在多个基准测试中表现优异，如 DeepSeek-R1-Distill-Qwen-32B 在 AIME 2024 上 pass@1 达 72.6%，超过 o1-mini。
       </li>
      </ul>
     </li>
    </ol>
    <h4 id="%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C" name="%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C">
     实验结果
    </h4>
    <ol>
     <li>
      <strong>
       基准测试表现
      </strong>
      <ul>
       <li>
        <strong>
         数学任务
        </strong>
        ：DeepSeek-R1 在 AIME 2024（79.8%）和 MATH-500（97.3%）上接近或超过 OpenAI-o1-1217。
       </li>
       <li>
        <strong>
         编码任务
        </strong>
        ：在 Codeforces 上 Elo 评分为 2029，超过 96.3% 的人类选手；LiveCodeBench pass@1 达 65.9%。
       </li>
       <li>
        <strong>
         知识问答
        </strong>
        ：MMLU（90.8%）、GPQA Diamond（71.5%）等任务上优于 DeepSeek-V3，稍逊于 o1-1217。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       蒸馏模型对比
      </strong>
      ：蒸馏后的小模型（如 14B、32B）在多个任务上显著优于同类开源模型，证明了大模型推理模式的可迁移性。
     </li>
    </ol>
    <h4 id="%E8%AE%A8%E8%AE%BA%E4%B8%8E%E7%BB%93%E8%AE%BA" name="%E8%AE%A8%E8%AE%BA%E4%B8%8E%E7%BB%93%E8%AE%BA">
     讨论与结论
    </h4>
    <ol>
     <li>
      <strong>
       蒸馏 vs. RL
      </strong>
      ：蒸馏更高效，小模型通过学习大模型的推理模式即可获得优秀性能；而直接对小模型进行 RL 训练需大量计算资源且效果有限。
     </li>
     <li>
      <strong>
       未成功尝试
      </strong>
      ：过程奖励模型（PRM）因难以定义细粒度步骤和奖励欺诈问题效果不佳；蒙特卡洛树搜索（MCTS）因搜索空间过大和价值模型训练困难未能显著提升性能。
     </li>
     <li>
      <strong>
       结论
      </strong>
      ：纯 RL 可有效提升 LLM 推理能力，多阶段训练和冷启动数据进一步优化了模型表现。模型蒸馏为小模型赋予了强大的推理能力，开源模型将推动相关研究。
     </li>
    </ol>
    <h4 id="%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%90%91" name="%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C%E6%96%B9%E5%90%91">
     未来工作方向
    </h4>
    <ol>
     <li>
      <strong>
       通用能力扩展
      </strong>
      ：提升在函数调用、多轮对话等任务上的表现。
     </li>
     <li>
      <strong>
       语言混合问题
      </strong>
      ：优化非中 / 英文查询的处理能力。
     </li>
     <li>
      <strong>
       提示工程优化
      </strong>
      ：减少模型对提示的敏感性，提升零样本性能。
     </li>
     <li>
      <strong>
       软件工程任务
      </strong>
      ：增加相关 RL 训练数据，提高在软件工程项目中的表现。
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f4a696d6d79476f6f6e672f:61727469636c652f64657461696c732f313436323333393937" class_="artid" style="display:none">
 </p>
</div>


