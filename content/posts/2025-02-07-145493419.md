---
layout: post
title: "macOS-和-Windows-系统的-DeepSeek详细教程"
date: 2025-02-07 13:45:32 +0800
description: "涵盖环境配置、模型加载、可视化交互及问题排查，按步骤操作即可完成部署。通过以上步骤，您已成功在本地部署。或 DeepSeek 的。"
keywords: "macos安装deepseek"
categories: ['Deepseek']
tags: ['深度学习', 'Chatgpt']
artid: "145493419"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145493419
    alt: "macOS-和-Windows-系统的-DeepSeek详细教程"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145493419
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145493419
cover: https://bing.ee123.net/img/rand?artid=145493419
image: https://bing.ee123.net/img/rand?artid=145493419
img: https://bing.ee123.net/img/rand?artid=145493419
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     macOS 和 Windows 系统的 DeepSeek详细教程
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     以下是针对
     <strong>
      macOS 和 Windows 系统
     </strong>
     的
     <strong>
      DeepSeek 本地部署超详细教程
     </strong>
     ，涵盖环境配置、模型加载、可视化交互及问题排查，按步骤操作即可完成部署。
    </p>
    <hr/>
    <h4>
     <a id="_4">
     </a>
     <strong>
      一、环境准备（硬件与系统）
     </strong>
    </h4>
    <h5>
     <a id="1_macOS__5">
     </a>
     <strong>
      1. macOS 用户
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       硬件要求
      </strong>
      ：
      <ul>
       <li>
        <strong>
         入门级
        </strong>
        ：Apple Silicon（M1/M2/M3 芯片），16GB 内存，30GB 存储空间（支持 1.5B/7B 模型）。
       </li>
       <li>
        <strong>
         高性能级
        </strong>
        ：M2 Ultra/M3 Max 芯片，64GB 内存，100GB 存储（支持 14B/32B 模型）。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       系统要求
      </strong>
      ：macOS Ventura（13.0）或更高版本，确保已安装 Xcode 命令行工具：
      <pre><code class="prism language-bash">xcode-select <span class="token parameter variable">--install</span>
</code></pre>
     </li>
    </ul>
    <h5>
     <a id="2_Windows__14">
     </a>
     <strong>
      2. Windows 用户
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       硬件要求
      </strong>
      ：
      <ul>
       <li>
        <strong>
         CPU 模式
        </strong>
        ：支持 AVX2 指令集的 Intel/AMD CPU（如 i5-8xxx 或 Ryzen 5 以上），16GB 内存，30GB 存储（仅运行 1.5B 模型）。
       </li>
       <li>
        <strong>
         GPU 加速
        </strong>
        ：NVIDIA RTX 3060 或更高（显存 ≥8GB），32GB 内存（支持 7B/14B 模型）。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       系统要求
      </strong>
      ：Windows 10/11 64位，需安装：
      <ul>
       <li>
        <a href="https://www.nvidia.com/Download/index.aspx" rel="nofollow">
         NVIDIA 显卡驱动
        </a>
        （GPU 用户必装）
       </li>
       <li>
        <a href="https://developer.nvidia.com/cuda-toolkit" rel="nofollow">
         CUDA Toolkit 12.1+
        </a>
        （GPU 加速依赖）
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_Ollama_24">
     </a>
     <strong>
      二、安装 Ollama（核心工具）
     </strong>
    </h4>
    <h5>
     <a id="1_macOS__25">
     </a>
     <strong>
      1. macOS 安装步骤
     </strong>
    </h5>
    <ol>
     <li>
      <strong>
       下载安装包
      </strong>
      ：
      <ul>
       <li>
        访问
        <a href="https://ollama.com/download" rel="nofollow">
         Ollama 官网
        </a>
        ，选择
        <strong>
         macOS（Apple Silicon）
        </strong>
        下载
        <code>
         .dmg
        </code>
        文件。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       安装与权限
      </strong>
      ：
      <ul>
       <li>
        双击下载的
        <code>
         .dmg
        </code>
        文件，将 Ollama 图标拖拽到
        <code>
         Applications
        </code>
        文件夹。
       </li>
       <li>
        首次运行时，系统可能提示“无法验证开发者”，需手动授权：
        <ul>
         <li>
          进入
          <strong>
           系统设置 → 隐私与安全性 → 安全性
          </strong>
          ，点击
          <strong>
           仍要打开
          </strong>
          。
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      <strong>
       验证安装
      </strong>
      ：
      <pre><code class="prism language-bash">ollama <span class="token parameter variable">--version</span>
<span class="token comment"># 输出示例：ollama version 0.1.20</span>
</code></pre>
     </li>
    </ol>
    <h5>
     <a id="2_Windows__38">
     </a>
     <strong>
      2. Windows 安装步骤
     </strong>
    </h5>
    <ol>
     <li>
      <strong>
       下载安装包
      </strong>
      ：
      <ul>
       <li>
        从
        <a href="https://ollama.com/download" rel="nofollow">
         Ollama 官网
        </a>
        下载
        <code>
         OllamaSetup.exe
        </code>
        。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       安装与配置
      </strong>
      ：
      <ul>
       <li>
        右键以管理员身份运行安装程序，默认路径安装（建议保持
        <code>
         C:\Program Files\Ollama
        </code>
        ）。
       </li>
       <li>
        安装完成后，Ollama 会自动启动并注册为系统服务。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       验证安装
      </strong>
      ：
      <ul>
       <li>
        打开 PowerShell 或 CMD，输入：
        <pre><code class="prism language-powershell">ollama <span class="token operator">-</span>v
<span class="token comment"># 输出示例：ollama version 0.1.20</span>
</code></pre>
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="_DeepSeek__53">
     </a>
     <strong>
      三、下载与加载 DeepSeek 模型
     </strong>
    </h4>
    <h5>
     <a id="1__54">
     </a>
     <strong>
      1. 模型选择建议
     </strong>
    </h5>
    <table>
     <thead>
      <tr>
       <th>
        模型版本
       </th>
       <th>
        适用场景
       </th>
       <th>
        最低显存/内存
       </th>
       <th>
        文件大小
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        1.5B
       </td>
       <td>
        轻量级文本生成
       </td>
       <td>
        4GB
       </td>
       <td>
        1.1GB
       </td>
      </tr>
      <tr>
       <td>
        7B
       </td>
       <td>
        通用任务（推荐）
       </td>
       <td>
        8GB
       </td>
       <td>
        5GB
       </td>
      </tr>
      <tr>
       <td>
        14B
       </td>
       <td>
        复杂推理与代码生成
       </td>
       <td>
        16GB
       </td>
       <td>
        12GB
       </td>
      </tr>
     </tbody>
    </table>
    <h5>
     <a id="2__61">
     </a>
     <strong>
      2. 命令行下载模型
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       通用命令
      </strong>
      （以 7B 模型为例）：
      <pre><code class="prism language-bash">ollama run deepseek-r1:7b
</code></pre>
      <ul>
       <li>
        首次运行会自动下载模型，终端显示进度条：
        <pre><code>pulling manifest...
downloading 7b...  [██████████████████] 100%
</code></pre>
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="3__72">
     </a>
     <strong>
      3. 国内网络加速（可选）
     </strong>
    </h5>
    <ul>
     <li>
      若下载缓慢，可配置镜像源：
      <pre><code class="prism language-bash"><span class="token comment"># macOS/Linux</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">OLLAMA_HOST</span><span class="token operator">=</span>mirror.ghproxy.com

<span class="token comment"># Windows（PowerShell）</span>
<span class="token variable">$env</span>:OLLAMA_HOST<span class="token operator">=</span><span class="token string">"mirror.ghproxy.com"</span>
</code></pre>
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_84">
     </a>
     <strong>
      四、测试模型基础功能
     </strong>
    </h4>
    <h5>
     <a id="1__85">
     </a>
     <strong>
      1. 命令行交互测试
     </strong>
    </h5>
    <ul>
     <li>
      启动模型交互模式：
      <pre><code class="prism language-bash">ollama run deepseek-r1:7b
</code></pre>
     </li>
     <li>
      输入测试指令：
      <pre><code class="prism language-text">&gt;&gt;&gt; 生成一个快速排序的Python代码
</code></pre>
      <ul>
       <li>
        预期输出：
        <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">quick_sort</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span> <span class="token operator">&lt;=</span> <span class="token number">1</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> arr
    pivot <span class="token operator">=</span> arr<span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">]</span>
    left <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> arr <span class="token keyword">if</span> x <span class="token operator">&lt;</span> pivot<span class="token punctuation">]</span>
    middle <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> arr <span class="token keyword">if</span> x <span class="token operator">==</span> pivot<span class="token punctuation">]</span>
    right <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> arr <span class="token keyword">if</span> x <span class="token operator">&gt;</span> pivot<span class="token punctuation">]</span>
    <span class="token keyword">return</span> quick_sort<span class="token punctuation">(</span>left<span class="token punctuation">)</span> <span class="token operator">+</span> middle <span class="token operator">+</span> quick_sort<span class="token punctuation">(</span>right<span class="token punctuation">)</span>
</code></pre>
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="2__GPU_Windows_106">
     </a>
     <strong>
      2. 验证 GPU 加速（Windows）
     </strong>
    </h5>
    <ul>
     <li>
      查看 Ollama 日志，确认是否启用 CUDA：
      <pre><code class="prism language-powershell">ollama serve
<span class="token comment"># 输出中出现 "CUDA capability detected" 表示 GPU 加速已启用</span>
</code></pre>
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="Chatbox_115">
     </a>
     <strong>
      五、配置可视化界面（Chatbox）
     </strong>
    </h4>
    <h5>
     <a id="1__Chatbox__116">
     </a>
     <strong>
      1. 安装 Chatbox 客户端
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       下载地址
      </strong>
      ：
      <a href="https://chatboxai.app/" rel="nofollow">
       Chatbox 官网
      </a>
      <ul>
       <li>
        选择对应系统的安装包（支持 macOS/Windows/Linux）。
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="2__120">
     </a>
     <strong>
      2. 连接本地模型
     </strong>
    </h5>
    <ol>
     <li>
      打开 Chatbox，进入
      <strong>
       Settings → Model
      </strong>
      。
     </li>
     <li>
      选择
      <strong>
       Ollama
      </strong>
      作为后端，填写模型名称（与下载的模型一致）：
      <pre><code>Model Name: deepseek-r1:7b
Base URL: http://localhost:11434
</code></pre>
     </li>
     <li>
      点击
      <strong>
       Test Connection
      </strong>
      ，显示绿色成功提示即配置完成。
     </li>
    </ol>
    <h5>
     <a id="3_Windows__129">
     </a>
     <strong>
      3. Windows 额外配置
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       环境变量设置
      </strong>
      （解决跨域问题）：
      <ol>
       <li>
        右键
        <strong>
         此电脑 → 属性 → 高级系统设置 → 环境变量
        </strong>
        。
       </li>
       <li>
        新建系统变量：
        <ul>
         <li>
          变量名：
          <code>
           OLLAMA_ORIGINS
          </code>
         </li>
         <li>
          变量值：
          <code>
           *
          </code>
         </li>
        </ul>
       </li>
       <li>
        重启 Ollama 服务：
        <pre><code class="prism language-powershell"><span class="token function">Restart-Service</span> Ollama
</code></pre>
       </li>
      </ol>
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_142">
     </a>
     <strong>
      六、常见问题与解决方案
     </strong>
    </h4>
    <h5>
     <a id="1__143">
     </a>
     <strong>
      1. 模型加载失败
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       现象
      </strong>
      ：
      <code>
       Error: model 'deepseek-r1:7b' not found
      </code>
     </li>
     <li>
      <strong>
       解决方法
      </strong>
      ：
      <pre><code class="prism language-bash"><span class="token comment"># 重新拉取模型</span>
ollama pull deepseek-r1:7b
</code></pre>
     </li>
    </ul>
    <h5>
     <a id="2__151">
     </a>
     <strong>
      2. 显存/内存不足
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       现象
      </strong>
      ：
      <code>
       CUDA out of memory
      </code>
      或程序崩溃
     </li>
     <li>
      <strong>
       解决方案
      </strong>
      ：
      <ul>
       <li>
        降低模型版本（如从 14B 切换到 7B）。
       </li>
       <li>
        添加
        <code>
         --num-gpu-layers
        </code>
        参数（仅限 GPU 用户）：
        <pre><code class="prism language-bash">ollama run deepseek-r1:7b --num-gpu-layers <span class="token number">20</span>
</code></pre>
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="3__160">
     </a>
     <strong>
      3. 响应速度慢
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       优化方法
      </strong>
      ：
      <ul>
       <li>
        <strong>
         macOS
        </strong>
        ：启用 Metal 加速（Apple Silicon 专用）：
        <pre><code class="prism language-bash"><span class="token assign-left variable">OLLAMA_METAL</span><span class="token operator">=</span><span class="token number">1</span> ollama run deepseek-r1:7b
</code></pre>
       </li>
       <li>
        <strong>
         Windows
        </strong>
        ：确保 CUDA 和显卡驱动为最新版本。
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_170">
     </a>
     <strong>
      七、高级部署选项
     </strong>
    </h4>
    <h5>
     <a id="1_Docker__171">
     </a>
     <strong>
      1. Docker 部署（跨平台）
     </strong>
    </h5>
    <ul>
     <li>
      使用 Docker 运行 Ollama：
      <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">11434</span>:11434 <span class="token parameter variable">--name</span> ollama ollama/ollama
</code></pre>
     </li>
     <li>
      拉取模型：
      <pre><code class="prism language-bash"><span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> ollama ollama pull deepseek-r1:7b
</code></pre>
     </li>
    </ul>
    <h5>
     <a id="2_Open_Web_UI__181">
     </a>
     <strong>
      2. Open Web UI 集成
     </strong>
    </h5>
    <ul>
     <li>
      部署开源交互界面：
      <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 --add-host<span class="token operator">=</span>host.docker.internal:host-gateway <span class="token parameter variable">-v</span> open-webui:/app/backend/data <span class="token parameter variable">--name</span> open-webui ghcr.io/open-webui/open-webui:main
</code></pre>
      <ul>
       <li>
        访问
        <code>
         http://localhost:3000
        </code>
        使用 Web 界面。
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_190">
     </a>
     <strong>
      总结
     </strong>
    </h4>
    <p>
     通过以上步骤，您已成功在本地部署
     <strong>
      DeepSeek 大模型
     </strong>
     。关键要点：
    </p>
    <ol>
     <li>
      根据硬件选择合适模型（
      <strong>
       7B 为平衡性能与资源的推荐版本
      </strong>
      ）。
     </li>
     <li>
      Windows 用户务必配置 CUDA 和显卡驱动以启用 GPU 加速。
     </li>
     <li>
      使用 Chatbox 或 Open Web UI 提升交互体验。
     </li>
    </ol>
    <p>
     如需进一步优化，可参考
     <a href="https://github.com/ollama/ollama">
      Ollama 官方文档
     </a>
     或 DeepSeek 的
     <a href="https://huggingface.co/deepseek-ai" rel="nofollow">
      Hugging Face 页面
     </a>
     。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34333239303337302f:61727469636c652f64657461696c732f313435343933343139" class_="artid" style="display:none">
 </p>
</div>


