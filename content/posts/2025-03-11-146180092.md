---
layout: post
title: "GRU门控循环单元"
date: 2025-03-11 17:47:08 +0800
description: "GRU（Gated Recurrent Unit）是RNN的一个变体，在2014年被提出。与LSTM类似，同样是为了解决RNN处理长序列时梯度爆炸或梯度消失的问题。与LSTM相比，GRU的结构更简单，参数更少，效果却与LSTM持平。"
keywords: "GRU门控循环单元"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Gru']
artid: "146180092"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146180092
    alt: "GRU门控循环单元"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146180092
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146180092
cover: https://bing.ee123.net/img/rand?artid=146180092
image: https://bing.ee123.net/img/rand?artid=146180092
img: https://bing.ee123.net/img/rand?artid=146180092
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     GRU门控循环单元
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <blockquote>
     <p>
      阅读本文前，建议读者先阅读
      <a href="https://blog.csdn.net/cufewxy1/article/details/146013615?spm=1001.2014.3001.5501" title="LSTM长短期记忆网络-CSDN博客">
       LSTM长短期记忆网络-CSDN博客
      </a>
     </p>
    </blockquote>
    <h2>
     1 简介
    </h2>
    <p>
     GRU（Gated Recurrent Unit）是RNN的一个变体，在2014年被提出。与LSTM类似，同样是为了解决RNN处理长序列时梯度爆炸或梯度消失的问题。与LSTM相比，GRU的结构更简单，参数更少，效果却能与LSTM接近。
    </p>
    <h2>
     2 模型结构
    </h2>
    <p>
     <img alt="" height="318" src="https://i-blog.csdnimg.cn/direct/46f1c9189c7747c68f8dc74198296a2c.png" width="923"/>
    </p>
    <h2>
     3 模型原理
    </h2>
    <p>
     同样举LSTM一文中柯南破案的例子，h(t)与x(t)的定义不变，取消了c(t)，新增了r(t)，表示根据新线索情况决定是否重新审视之前的线索。
    </p>
    <table border="1" cellpadding="1" cellspacing="1" style="width:500px">
     <tbody>
      <tr>
       <td>
        符号
       </td>
       <td>
        含义
       </td>
       <td>
        例子中类比对象
       </td>
      </tr>
      <tr>
       <td>
        h(t)
       </td>
       <td>
        t时刻隐藏状态
       </td>
       <td>
        之前调查过程中所积累的所有线索和经验，可以看作LSTM中的长期记忆和短期记忆合并成h(t)
       </td>
      </tr>
      <tr>
       <td>
        x(t)
       </td>
       <td>
        t时刻的输入
       </td>
       <td>
        t时刻接收到的线索
       </td>
      </tr>
      <tr>
       <td>
        r(t)
       </td>
       <td>
        t时刻重置比例
       </td>
       <td>
        记忆筛选器，指当前线索与记忆的关联性。从记忆中搜寻信息，生成候选线索
       </td>
      </tr>
      <tr>
       <td>
        z(t)
       </td>
       <td>
        t时刻更新比例
       </td>
       <td>
        记忆融合器，多大比例保留之前的调查思路。候选线索和上一时刻线索各自多少比例延续到当前时刻
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     4 重置门的计算
    </h2>
    <p>
     比如新线索是嫌疑人是左撇子，而此前观察到小红用左手使用筷子，那么重置门r(t)接近为1，表示从记忆中搜寻到相关信息。在生成候选线索时，将充分利用之前的线索，回顾当时观察到小红的其他举动。之前的隐藏状态大量流入到当前的候选线索。
    </p>
    <p>
     比如新线索是嫌疑人是左撇子，但此前并没有观察到各嫌疑人左右手使用的情况，那么重置门r(t)接近为0，表示记忆中并没有搜索到相关信息。在生成候选线索时，将只利用当前的线索。当前的输入大量流入到当前的候选线索。
    </p>
    <p>
     注意生成的候选线索只是备选，并不直接更新上一时刻隐藏状态。需要使用更新门控制有多大程度更新到隐藏状态中。重置门只关注当前的线索与历史线索的关联性，关联性强就关注历史线索，关联性差就只关注当前线索。至于该线索重不重要，是在更新门中处理的。
    </p>
    <p>
     计算公式为
    </p>
    <p>
     <img alt="r_t = \sigma (W_r \cdot [h_{t - 1}, x_t])" class="mathcode" src="https://latex.csdn.net/eq?r_t%20%3D%20%5Csigma%20%28W_r%20%5Ccdot%20%5Bh_%7Bt%20-%201%7D%2C%20x_t%5D%29"/>
    </p>
    <p>
     <img alt="\tilde{h}_t = \tanh(W_h[r_t \odot h_{t - 1}, x_t] + b_h)" class="mathcode" src="https://latex.csdn.net/eq?%5Ctilde%7Bh%7D_t%20%3D%20%5Ctanh%28W_h%5Br_t%20%5Codot%20h_%7Bt%20-%201%7D%2C%20x_t%5D%20&amp;plus;%20b_h%29"/>
    </p>
    <h2>
     5 更新门的计算
    </h2>
    <p>
     比如新线索是嫌疑人是左撇子，而此前认为嫌疑人是右撇子，或者此前的办案思路偏了，那么更新门z(t)接近为0，表示不再保留之前的办案思路，而是以当前生成的新线索为主。
    </p>
    <p>
     比如新线索是嫌疑人是左撇子，而此前的办案思路更重要，那么更新门z(t)接近为1，表示沿用之前的办案思路，当前新的线索顶多作为补充。
    </p>
    <p>
     更新门用于控制多大比例保留之前的调查思路。即新的隐藏状态h(t)有多大比例来自于上一时刻隐藏状态h(t-1)，有多大比例来自于当前时刻候选隐藏状态h'(t)。
    </p>
    <p>
     计算公式为
    </p>
    <p>
     <img alt="z_t = \sigma (W_z \cdot [h_{t - 1}, x_t])" class="mathcode" src="https://latex.csdn.net/eq?z_t%20%3D%20%5Csigma%20%28W_z%20%5Ccdot%20%5Bh_%7Bt%20-%201%7D%2C%20x_t%5D%29"/>
    </p>
    <p>
     <img alt="h_t = (1 - z_t) \odot \tilde{h}_t + z_t \odot h_{t - 1}" class="mathcode" src="https://latex.csdn.net/eq?h_t%20%3D%20%281%20-%20z_t%29%20%5Codot%20%5Ctilde%7Bh%7D_t%20&amp;plus;%20z_t%20%5Codot%20h_%7Bt%20-%201%7D"/>
    </p>
    <h2>
     6 GRU的特点
    </h2>
    <p>
     优势：
    </p>
    <p>
     第一，结构简单、参数较少、计算迅速。
    </p>
    <p>
     第二，能够解决RNN长序列依赖问题，且性能接近于LSTM。
    </p>
    <p>
     缺点：
    </p>
    <p>
     第一，在解决更复杂的信息时不如LSTM。LSTM设置的门控单元更多（遗忘门、输入门、输出门、细胞状态），因此可以处理更复杂的情况。GRU没有单独的遗忘门，更新门承担了LSTM的输入门和遗忘门两个角色，因此对于复杂问题相形见绌。
    </p>
    <p>
     第二，长时间记忆能力不如LSTM。LSTM有专门的长期记忆，但GRU还是用隐藏状态连接前后时间的。
    </p>
    <p>
     第三，不能并行计算，这是RNN、LSTM、GRU的通病。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f63756665777879312f:61727469636c652f64657461696c732f313436313830303932" class_="artid" style="display:none">
 </p>
</div>


