---
layout: post
title: "论文阅读Trustworthiness-in-Retrieval-Augmented-Generation-Systems-A-Survey"
date: 2025-03-11 12:28:52 +0800
description: "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey[2409.10102] Trustworthiness in Retrieval-Augmented Generation Systems: A Survey提出了一个统一的框架，该框架从六个关键维度评估 RAG 系统的可信度：事实性、鲁棒性、公平性、透明度、问责制和隐私。(1) 事实性：通过根据可靠来源验证来确保生成信息的准确性和真实性。 (2) 稳健性：确保系统在错误、"
keywords: "[论文阅读]Trustworthiness in Retrieval-Augmented Generation Systems: A Survey"
categories: ['论文精读']
tags: ['论文阅读']
artid: "146171150"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146171150
    alt: "论文阅读Trustworthiness-in-Retrieval-Augmented-Generation-Systems-A-Survey"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146171150
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146171150
cover: https://bing.ee123.net/img/rand?artid=146171150
image: https://bing.ee123.net/img/rand?artid=146171150
img: https://bing.ee123.net/img/rand?artid=146171150
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     [论文阅读]Trustworthiness in Retrieval-Augmented Generation Systems: A Survey
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     Trustworthiness in Retrieval-Augmented Generation Systems: A Survey
    </p>
    <p>
     <a href="http://arxiv.org/abs/2409.10102" rel="nofollow" title="[2409.10102] Trustworthiness in Retrieval-Augmented Generation Systems: A Survey">
      [2409.10102] Trustworthiness in Retrieval-Augmented Generation Systems: A Survey
     </a>
    </p>
    <h2>
     总览
    </h2>
    <p>
     提出了一个统一的框架，该框架从六个关键维度评估 RAG 系统的可信度：事实性、鲁棒性、公平性、透明度、问责制和隐私。
    </p>
    <p>
     <img alt="" height="631" src="https://i-blog.csdnimg.cn/direct/c6b28b69b9a244bda3ed6dfe9c416598.png" width="1260"/>
    </p>
    <p>
     (1) 事实性：通过根据可靠来源验证来确保生成信息的准确性和真实性。
    </p>
    <p>
     (2) 稳健性：确保系统在错误、对抗性攻击和其他外部威胁方面的可靠性。
    </p>
    <p>
     (3) 公平性：最大限度地减少检索和生成阶段的偏差，以确保公平的结果。
    </p>
    <p>
     (4) 透明度：使 RAG 系统流程和决策对用户清晰易懂，培养信任和问责制。
    </p>
    <p>
     (5) 问责制：实施机制，确保系统行为和输出负责任且可追溯。
    </p>
    <p>
     (6) 隐私：在整个检索和生成过程中保护个人数据和用户隐私。
    </p>
    <h2>
     RAG类型
    </h2>
    <h3>
     朴素RAG
    </h3>
    <p>
     朴素 RAG 遵循“检索-然后-阅读”的过程，包括一个简单的检索器和一个预训练的语言模型作为生成器。 它的工作流程包括两个简单的步骤：(1) 根据用户查询从预先构建的知识库中检索相关段落，以及 (2) 将检索到的信息与输入查询相结合以生成响应。
    </p>
    <p>
     局限性：首先，检索到的文档可能包含噪声或不相关的信息，这会干扰模型的响应。 其次，大型模型固有的高推理成本在 RAG 过程中进一步加剧；包含冗长的检索文档会减慢生成过程并消耗更多计算资源。
    </p>
    <h3>
     高级RAG
    </h3>
    <p>
     为了解决之前讨论的问题，在 RAG 过程中添加了额外的组件，使其更加复杂。 这些增强系统，称为高级 RAG，
     <strong>
      在检索和生成管道的不同阶段引入了专门的模块
     </strong>
     ，这些模块可以分为检索前和检索后组件。
    </p>
    <p>
     在检索前阶段，一个常见的问题是原始查询可能太短或太模糊，导致检索结果不相关。引入了重写器来澄清或扩展查询。 重写方法包括直接提示 LLM 或使用来自生成器的反馈训练重写器模型
    </p>
    <p>
     在检索后阶段，生成器通常会因检索内容的长度或噪声而面临挑战，这会影响生成质量。使用
     <strong>
      重新排序器
     </strong>
     对检索结果进行重新排序 [38]. 重新排序器通常使用交叉编码器架构，更好地衡量查询和检索文档之间的相似性，将更相关的文档推到前面，并删除不太相关的文档。
    </p>
    <p>
     另一个优化组件是优化器，它使用诸如提示 LLM 进行摘要或通过监督微调或强化学习训练摘要器等技术对检索到的内容进行摘要或压缩。 尽管高级 RAG 具有灵活性，但其顺序结构限制了其在复杂场景中的适应性，例如需要逐步推理的查询。
    </p>
    <h3>
     模块化RAG
    </h3>
    <p>
     模块化 RAG 阶段，其中组件被视为可以组合起来为不同场景创建定制管道，提供更大的灵活性和适应性的灵活模块。 研究现在集中在优化这些管道，它们主要有四种类型：顺序、条件、分支和循环。
    </p>
    <p>
     顺序管道以线性方式处理查询，类似于高级 RAG，具有检索前和检索后阶段。 条件管道根据查询类型沿着不同的执行路径路由查询。 例如，SKR [“Self-knowledge guided retrieval augmentation for large language models,” in Findings of the Association for Computational Linguistics] 识别出 LLM 可以无需检索即可回答的查询，而 Adaptive-RAG [Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity] 将查询分类为简单查询或复杂查询，对复杂查询使用多轮检索。
    </p>
    <p>
     分支管道同时为查询执行多个路径，将结果组合起来形成最终输出。 这可能涉及聚合生成概率 [REPLUG: retrieval-augmented black-box language models] 或生成多个答案并选择最佳答案 [Sure: Summarizing retrievals using answer candidates for open-domain QA of llms]。 这有助于解决单路径推理中的不稳定性。
    </p>
    <p>
     循环管道是最复杂的，它涉及检索器和生成器之间多轮交互。 ReAct [React: Synergizing reasoning and acting in language models] 等技术使用提示生成推理路径和搜索请求，而 Self-Ask [Measuring and narrowing the compositionality gap in language models,] 允许 LLM 提出并回答中间问题。 IRCOT [Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions] 在 CoT 路径生成期间引入了重复检索。 其他方法包括模型决定何时检索信息 [Self-rag: Learning to retrieve, generate, and critique through self-reflection]，使用外部工具 [Toolformer: Language models can teach themselves to use tools,]，或访问浏览器 [Webgpt: Browser-assisted question-answering with human feedback]。 这些模块化管道，具有迭代和多轮检索以及自我纠正等功能，创造了一个更智能的 RAG 流程。
    </p>
    <p>
     <img alt="" height="1066" src="https://i-blog.csdnimg.cn/direct/e0f4a706a83d4cd49863c0fc7c1bf0c8.png" width="1200"/>
    </p>
    <h2 style="background-color:transparent">
     可信RAG系统
    </h2>
    <p>
     <img alt="" height="319" src="https://i-blog.csdnimg.cn/direct/860f4d52d95f4fe39d279bc2467b6b61.png" width="600"/>
    </p>
    <p>
     一个完整的 RAG 系统包含三个主要阶段：将外部知识注入生成器、生成器生成答案以及对生成的答案进行评估。 每个阶段都存在与可信度相关的挑战。 在外部知识注入阶段，存在注入噪声或私人信息的风险。 在答案生成阶段，引入外部知识可能会导致偏差推理，并损害通过 RLHF 实现的对齐。 最后，在答案评估阶段，生成的答案可能包含事实错误或缺乏对外部知识的充分依据。
    </p>
    <p>
     <img alt="" height="483" src="https://i-blog.csdnimg.cn/direct/e849c7bc53ec43fcbb95c5ed37427a21.png" width="1200"/>
    </p>
    <h3>
     1.真实性
    </h3>
    <h4>
     1.1LLM中的真实性是什么
    </h4>
    <p>
     在大型语言模型的背景下，真实性指的是模型的
     <strong>
      输出是否包含准确的事实和信息
     </strong>
     。 真实性的关键方面包括：
    </p>
    <ul id="S3.I1">
     <li id="S3.I1.i1">
      真实性： 生成的信息必须与现实世界中的事实、数据和事件相符，模型应避免在响应中提供任何虚构或错误信息。
     </li>
     <li id="S3.I1.i2">
      逻辑一致性： 内容应保持逻辑正确性，确保句子内部和句子之间的一致性，防止自相矛盾和错误。 例如，如果之前的内容中提到了一个假设，那么以下内容需要在这个假设下写，不能有矛盾。
     </li>
     <li id="S3.I1.i3">
      时间感知： 它应考虑给定信息和自身知识的时间变化，并反映在给定时间最新的或指定的事件状态。 如果知识只能在某个时间点提供，则需要特殊的解释来避免误导用户。
     </li>
     <li id="S3.I1.i4">
      与指令一致性： 模型的响应必须遵循提供的指令，避免不相关的信息，即使信息是正确的。
     </li>
    </ul>
    <h4>
     1.2RAG中的真实性
    </h4>
    <p>
     在 RAG 场景中，大量的检索内容被输入到输入中，这会导致 LLM 出现额外的含义和挑战。 事实性的这种扩展定义要求模型综合内部和外部知识，以产生事实性的响应。 在这些情况下，会出现独特的挑战：
    </p>
    <ul id="S3.I2">
     <li id="S3.I2.i1">
      <p id="S3.I2.i1.p1.1">
       <strong>
        内部知识与外部知识之间的冲突
       </strong>
       ： 模型的内部知识基于从训练数据中学习到的模式，而检索到的外部知识则直接来自可靠的文档。 当这些来源在同一主题上提供相互矛盾的信息时，模型必须辨别并优先考虑更准确的来源。 未能这样做会导致生成的内容中出现事实上的不准确或逻辑错误。 例如，对于随着时间推移而不断变化的信息，例如当前事件或新闻，模型的内部知识可能已经过时，因此需要使用更新的外部知识。
      </p>
     </li>
     <li id="S3.I2.i2">
      <strong>
       检索文档中的噪声
      </strong>
      ： 由于检索系统并不完美，检索到的文档通常包含大量噪声，例如过时信息、与上下文不匹配的无关细节或措辞不同的冗余信息。 这种噪声会错误地引导模型的响应，直接影响生成的准确性并误导模型的输出。
     </li>
     <li id="S3.I2.i3">
      处理长上下文
      <strong>
       ：
      </strong>
      在 RAG 设置中，模型在深入理解和推理大量结构复杂的长上下文信息方面面临着重大挑战。 较长的文档要求模型增强信息过滤和理解能力，以避免遗漏关键细节。 此外，长文本通常涉及复杂的上下文和多个文档，要求模型不仅理解单个句子，还要理解整体逻辑和文档间信息。 在多跳问题中，确保生成事实的准确性需要基于多段信息进行推理。
     </li>
    </ul>
    <h4 style="background-color:transparent">
     1.3代表性研究
    </h4>
    <p>
     早期工作主要聚焦于检索器和生成器的联合训练或者只优化生成模型。
    </p>
    <p>
     而LLM规模扩大后联合训练这样的方式效率太低。
    </p>
    <p>
     <strong>
      整合内部和外部的知识：
     </strong>
    </p>
    <p>
     <strong>
      SAIL
     </strong>
     [SAIL: search-augmented instruction learning,]探索
     <strong>
      指令调整
     </strong>
     以微调生成模型，以增强事实性。 通过对搜索增强提示进行指令调优，模型可以区分复杂检索文档中的误导信息和相关信息，从而显著提高事实准确性。这种方式训练的较小模型在事实生成方面可以胜过 ChatGPT 等商业模型。
    </p>
    <p>
     <strong>
      Replug
     </strong>
     [REPLUG: retrieval-augmented black-box language models] 探索了一种针对
     <strong>
      黑盒模型
     </strong>
     的新方法。 它将每个搜索文档与查询一一连接起来，创建不同的生成路径。 然后，它合并来自这些路径的符元分布以生成最终输出。 这种方法避免了同时处理多个文档的挑战，并绕过了大型语言模型中的上下文限制。
    </p>
    <p>
     [Check your facts and try again: Improving large language models with external knowledge and automated feedback] 引入了一个即插即用模块来提高模型响应的事实准确性，评估响应的可靠性并提供改进反馈。
    </p>
    <p>
     [Merging generated and retrieved knowledge for open-domain QA]和[Generate rather than retrieve: Large language models are strong context generators]提示大型语言模型根据自身知识生成相关文档，明确提取内部知识以促进冲突解决和信息融合。
    </p>
    <p>
     <strong>
      自适应检索：
     </strong>
    </p>
    <p>
     传统RAG难以处理不够精炼的查询，这些查询无法检索到高度相关的文档，因此提出自适应检索，动态获取必要的内容。
    </p>
    <p id="S3.SS1.SSS3.p7.1">
     Self-Ask [Measuring and narrowing the compositionality gap in language models] 使用提示
     <strong>
      将复杂的查询逐步分解为子查询
     </strong>
     ，并通过检索和响应解决每个子查询。 这种方法确保更精确的知识检索，减少噪声并简化模型回答复杂问题的任务。 ReAct [React: Synergizing reasoning and acting in language models]
     <strong>
      将生成模型视为能够动态选择想法和行动的代理
     </strong>
     。 通过
     <strong>
      提示
     </strong>
     ，模型生成扩展的查询并规划后续步骤，利用自身的查询设计能力在整个过程中实现灵活性。
    </p>
    <p id="S3.SS1.SSS3.p8.1">
     FLARE [Active retrieval augmented generation]
     <strong>
      根据模型输出置信度调整检索
     </strong>
     。 当置信度较低时，系统将进行检索以增强事实准确性，而当
     <strong>
      置信度较高时，系统将依靠内部知识进行生成
     </strong>
     。 这在长篇问答中已被证明是有效的，确保了句子级别的真实性。
    </p>
    <p id="S3.SS1.SSS3.p9.1">
     IRCOT [Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions] 将链式思维推理与检索过程相结合，
     <strong>
      引导模型顺序生成推理路径并确定每个步骤需要哪些知识
     </strong>
     。 Self-RAG [Self-rag: Learning to retrieve, generate, and critique through self-reflection] 将
     <strong>
      自我反省与动态检索相结合
     </strong>
     ，生成指示检索必要的标记，并自主选择信息量最大的文档，避免引入无关文档。 实验结果表明，在各种任务中，事实准确性和响应质量都得到了提升。
    </p>
    <p id="S3.SS1.SSS3.p10.1">
     这些进步旨在通过改进外部知识的集成和利用，以及动态调整检索策略以更好地满足复杂信息搜索任务的需求，来完善 RAG 系统生成事实准确响应的能力。
    </p>
    <h3>
     2.鲁棒性
    </h3>
    <h4>
     2.1LLM中的鲁棒性
    </h4>
    <p>
     在各种输入条件和操作环境下保持稳定和可靠性能的能力。
    </p>
    <p id="S3.SS2.SSS1.p1.1">
     关键方面包括：
    </p>
    <ul id="S3.I3">
     <li id="S3.I3.i1">
      输入多样性： 大型语言模型能够解释和准确地响应各种输入的能力，这些输入在风格、结构和复杂性方面有所不同。
     </li>
     <li id="S3.I3.i2">
      噪声容忍度: 模型理解和处理包含错误、无关信息或失真输入的能力，同时性能不会大幅下降。
     </li>
     <li id="S3.I3.i3">
      对抗性抵抗: 抵御旨在欺骗或误导模型的故意操纵或攻击的能力。
     </li>
     <li id="S3.I3.i4">
      数据分布偏移: LLM 在遇到与训练集明显不同的数据时，需要可靠地执行，反映了现实世界中数据特征会随着时间推移而演变的情况。
     </li>
    </ul>
    <p>
     研究表明，大多数现有的 LLM 难以抵抗对抗性提示
    </p>
    <h4>
     2.2RAG中的鲁棒性
    </h4>
    <p>
     指 LLM 在呈现不同的检索信息输入时，始终如一地提取和利用相关知识的能力。
    </p>
    <ul id="S3.I4">
     <li id="S3.I4.i1">
      检索信息中的信噪比: RAG 中的鲁棒性涉及模型从检索到的文档中区分和优先考虑相关信息的能力，这些文档可能包含有用数据和噪声的混合。 模型应该有效地过滤掉无关内容，并专注于相关信息以生成准确且连贯的响应。
     </li>
     <li id="S3.I4.i2">
      检索信息的粒度: 这个维度考察了 LLM 在不同细节层次上处理信息的能力。 稳健的模型应该无缝地整合从检索到的文档中获得的细粒度细节和更广泛的上下文信息，根据所需的特定性调整其响应。
     </li>
     <li id="S3.I4.i3">
      检索信息的顺序： 稳健的 LLM 应该无论信息检索的顺序如何都保持性能。 能够准确地处理和综合信息，而与信息的顺序无关，对于确保动态检索场景中生成内容的可靠性至关重要。
     </li>
     <li id="S3.I4.i4">
      检索内容中的错误信息： RAG 系统的稳健性要求能够检测和管理检索到的文档中的错误信息。 该模型应该有效地识别并从其响应中排除不准确或误导性信息，确保生成的内容保持准确和可信。
     </li>
    </ul>
    <h4>
     2.3代表性研究
    </h4>
    <p>
     <strong>
      污染攻击
     </strong>
     ：利用自然语言生成和LLM中的漏洞，降低信息密度型应用程序的性能和可靠性。
    </p>
    <p>
     [Synthetic disinformation attacks on automated fact verification systems]探讨了自动化事实核查系统对
     <strong>
      合成对抗性证据
     </strong>
     的脆弱性，引入了对抗性添加和对抗性修改场景。 该研究表明，在多个基准测试中，事实核查模型的性能大幅下降，突出了由能够生成连贯的虚假信息的高级 NLG 系统带来的威胁。
    </p>
    <p>
     [On the risk of misinformation pollution with large language models]和[Attacking open-domain question answering by injecting misinformation]研究了 LLM 用于生成可信的虚假信息及其对开放域问答 (ODQA) 系统的影响的滥用可能性。 他们建立了威胁模型并模拟了误用场景，揭示了 LLM 可以显着降低 ODQA 性能。 作者提出了防御策略，如错误信息检测、警惕性提示和读者集成，强调了需要进行持续研究以减轻这些威胁。
    </p>
    <p>
     Corpus Poisoning和PoisonedRAG检查了密集检索系统和 RAG 系统对错误信息和知识中毒攻击的漏洞。 他们引入了新颖的攻击方法，这些方法会生成对抗性段落和中毒文本，并显示出很高的攻击成功率。 这些研究强调了需要强大的防御措施来保护这些系统免受此类漏洞的侵害。
    </p>
    <p>
     [Not what you’ve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection]探讨了间接提示注入 (IPI) 攻击，其中攻击者将提示注入到推断过程中可能检索到的数据源中，从而在没有直接访问的情况下远程控制 LLM。 该研究将这些攻击带来的各种威胁进行了分类，并证明了它们在现实世界系统中的实际可行性，主张改进安全性评估和缓解策略。
    </p>
    <p>
     GARAG对抗性攻击方法，解决了 RAG 系统对低级文本扰动（如错别字）的鲁棒性问题。 该研究揭示了 RAG 系统中的重大漏洞，表明即使是小的扰动也会大幅降低性能。
    </p>
    <p>
     <strong>
      防御措施
     </strong>
    </p>
    <p>
     [Why so gullible? enhancing the robustness of retrieval-augmented models against counterfactual noise]研究了检索增强型语言模型对检索到的文档中反事实和误导性信息的脆弱性。 本研究提出对检索增强模型和提示 GPT-3.5 的鉴别器进行微调，以激发其鉴别能力，证明了模型在对抗噪声方面的显著改进。
    </p>
    <p>
     [Defending against disinformation attacks in open-domain question answering]解决了防御 ODQA 系统免受对抗性中毒攻击的挑战。 作者提出了一种基于查询增强和一种名为“来自答案冗余的置信度”（CAR）的新型置信度方法的防御机制。 实验结果表明，这种方法可以将各种数据中毒水平下的精确匹配分数提高近 20%，从而增强系统对这种攻击的抵抗力。
    </p>
    <p>
     RobustRAG是用于保护 RAG 系统免受检索破坏攻击的防御框架。 RobustRAG 利用
     <strong>
      隔离然后聚合
     </strong>
     的策略，独立计算每个段落的 LLM 响应，然后安全地聚合这些响应以确保稳健性。 该框架证明了其在各种任务和数据集上的有效性，展示了其通用性和在现实世界应用中的潜力。
    </p>
    <h3>
     3.公平性
    </h3>
    <h4>
     3.1LLM中的公平性
    </h4>
    <p>
     LLM 的公平性是指确保模型不表现或传播偏见，并公平地对待所有个人和群体
    </p>
    <ul id="S3.I5">
     <li id="S3.I5.i1">
      数据公平性： 用于训练模型的训练数据需要具有代表性和多样性，以避免从不平衡的数据源中引入偏见。
     </li>
     <li id="S3.I5.i2">
      算法公平性： 算法的设计需要公平地对待所有人口统计，不偏袒或歧视任何特定社会群体。
     </li>
     <li id="S3.I5.i3">
      偏见检测： 偏见检测是指识别和量化 LLM 中偏见的过程，这是确定和理解 LLM 中偏见的存在和严重程度的关键步骤，也是随后偏见缓解工作的基础。
     </li>
     <li id="S3.I5.i4">
      偏见缓解： 偏见缓解是指应用技术来减少 LLM 中偏见的过程，包括以下三种方法：(1) 预处理 ：在训练前调整数据，例如重新加权或重新采样以纠正不平衡。 ; (2) 过程内处理 ：将公平性目标直接纳入学习算法，以在训练期间最大程度地减少偏见。 ; (3) 后处理: 在训练后修改模型的输出，以确保更公平的输出。
     </li>
    </ul>
    <h4>
     3.2RAG中的公平性
    </h4>
    <p id="S3.SS3.SSS2.p2.1">
     <strong>
      知识来源不平衡。
     </strong>
     如果外部知识库缺乏多样性，或代表特定的群体、文化视角或意识形态，那么 RAG 系统的输出将反映这些偏差。 这会导致某些观点的过度呈现，而边缘化其他观点。 此外，外部来源可能不成比例地呈现某些主题或观点，导致信息检索出现偏差，从而影响生成的內容。 例如，如果知识库过分偏向西方观点，那么 RAG 系统可能会产生忽略或歪曲非西方观点的输出。
    </p>
    <p id="S3.SS3.SSS2.p3.1">
     <strong>
      知识的可靠性。
     </strong>
     外部知识库可能包含错误或误导性信息。 如果RAG系统检索并整合了此类内容，它可能会延续偏差和错误。 外部知识库可能反映社会偏见和偏见。 通过整合此类有偏见的信息，RAG系统可能会无意间放大这些偏见，导致输出强化刻板印象和歧视性观点。 此外，不同的来源具有不同程度的可靠性和固有的偏见。 新闻机构、网站和数据库可能存在编辑偏见，RAG系统可能会在其输出中放大这些偏见。
    </p>
    <p id="S3.SS3.SSS2.p4.1">
     <strong>
      检索中的算法偏差。
     </strong>
     用于从外部知识库检索和排序信息的算法可能存在偏差。 它们可能会根据受欢迎程度、最新程度或其他因素偏好某些来源或类型的内容，这可能会在检索到的信息中引入偏差。 更糟糕的是，检索机制可能会通过持续呈现与用户过去偏好一致的信息来创建过滤泡，强化现有的偏见并限制对不同观点的接触。
    </p>
    <p id="S3.SS3.SSS2.p5.1">
     <strong>
      信息整合机制。
     </strong>
     生成模型可能会选择性地使用与自身既有偏见一致的检索信息，忽略其他可能提供更平衡观点的相关内容。 生成模型可能难以正确整合外部知识，尤其是在上下文或语义不一致的情况下。 当前模型在使用RAG技术时，只根据上下文相关性整合信息。 它无法判断外部知识的公平性，也无法选择性地整合公平信息并丢弃不公平信息。
    </p>
    <h4>
     3.3代表性研究
    </h4>
    <p>
     关于公平性的当前研究仍然非常有限。 FairRAG [Fairrag: Fair human generation via fair retrieval augmentation] 引入了一个新颖的框架，该框架解决了文本到图像生成模型中的公平性问题，特别关注减少人类图像生成中的偏差。 FairRAG 的关键贡献是其能够将预训练的生成模型与外部的、人口统计学上不同的参考图像进行条件化，以提高生成输出的公平性。 该框架采用了一个轻量级的线性模块，将参考图像投影到文本空间，并结合简单但有效的去偏策略来增强多样性。
    </p>
    <h3>
     4.透明度
    </h3>
    <h4>
     4.2LLM中的透明度
    </h4>
    <p>
     涉及了解和解释这些模型如何处理信息，做出决策 以及生成输出。
    </p>
    <p id="S3.SS4.SSS1.p1.1">
     关键领域：
    </p>
    <ul id="S3.I6">
     <li id="S3.I6.i1">
      数据透明度： 确保用于
      <strong>
       训练
      </strong>
      大型语言模型的
      <strong>
       数据集
      </strong>
      得到充分记录、公开访问并经过质量和偏差审查。 这也包括了解数据质量、多样性和偏差对模型性能的影响。
     </li>
     <li id="S3.I6.i2">
      模型透明度： 模型透明度研究涉及
      <strong>
       开发技术
      </strong>
      ，使人类能够理解大型语言模型的内部运作机制。 这些方法包括注意力可视化，激活最大化和逐层相关性传播，以查看模型如何处理输入以及它关注数据的哪些部分。
     </li>
     <li id="S3.I6.i3">
      算法透明度: 算法透明度需要理解和记录用于训练和微调大型语言模型 (LLM) 的
      <strong>
       算法和技术
      </strong>
      。 这包括模型开发中使用的架构设计、训练程序和超参数的透明度。
     </li>
     <li id="S3.I6.i4">
      解释生成: 创建可以为大型语言模型的决策和输出提供清晰简洁解释的工具和方法是提高透明度的另一种方式。 代理模型、特征归因方法和基于示例的解释等技术被用于阐明模型为何产生特定输出。
     </li>
    </ul>
    <h4>
     4.3RAG中的透明度
    </h4>
    <p id="S3.SS4.SSS2.p1.1">
     <strong>
      检索透明度。
     </strong>
     提高检索过程的透明度涉及调查检索组件如何从大型语料库中选择相关文档或段落。 这包括理解
     <strong>
      索引和排序算法
     </strong>
     ，以及用于
     <strong>
      选择最相关信息的标准
     </strong>
     。 此外，分析确定检索文档相关性的评分机制也有助于提高透明度。 这涉及研究为不同文本片段分配相关性评分的算法和启发式方法。
    </p>
    <p id="S3.SS4.SSS2.p2.1">
     <strong>
      信息整合透明度。
     </strong>
     提高信息整合的透明度需要理解检索到的信息是如何整合到答案生成过程中的。 这包括检查连接、注意机制或其他融合策略等技术，这些技术将检索到的文本与原始输入结合起来。 信息整合的透明度还包括研究检索到的信息如何影响生成的输出。 这涉及评估不同类型的检索文档对生成文本的质量、准确性和连贯性的影响。 创建工具来追踪生成的文本到特定的检索文档或段落，也为生成过程中使用信息提供清晰的传承。
    </p>
    <h4>
     4.4代表性研究
    </h4>
    <p>
     MetaRAG 框架[Metacognitive retrieval-augmented large language models]，该框架将检索增强生成与
     <strong>
      元认知策略
     </strong>
     相结合，以增强 LLM 在多跳问答任务中的推理能力。 MetaRAG 通过启用模型通过三步元认知调节管道（监控、评估和规划）对自身进行反省、评估和调整其推理过程，从而解决了现有检索增强模型的局限性。 这使模型能够诊断和纠正与知识不足、信息冲突和推理错误相关的错误。
    </p>
    <p>
     RAG-Ex[Ragex: A generic framework for explaining retrieval augmented generation]，这是一种与模型和语言无关的框架，旨在增强 RAG 系统的透明度和可解释性。 主要贡献包括开发了一种灵活的基于扰动的解释方法，适用于开源和专有 LLM，使用户能够理解模型在 QA 任务的背景下生成特定响应的原因。 该框架通过定量和定性方法进行了严格评估，证明了其在生成与用户期望高度一致的解释方面的有效性，并且几乎与模型内在方法的性能相匹配。
    </p>
    <p>
     RAGBench[Ragbench: Explainable benchmark for retrieval-augmented generation systems]，这是第一个专门
     <strong>
      为评估跨不同领域的 RAG 系统而设计的全面、大规模基准数据集
     </strong>
     。 作者提出了 TRACe 评估框架，该框架除了现有的度量标准（如上下文相关性和答案忠实度）之外，还包括新的度量标准，如上下文利用和答案完整性。 该基准包括来自行业特定领域的 100k 个示例，旨在为 RAG 系统提供可解释的和可操作的反馈。
    </p>
    <h3>
     5.问责制度
    </h3>
    <h4>
     5.1LLM中的问责制
    </h4>
    <p>
     指对这些系统及其开发人员和运营商负责其输出的能力。
    </p>
    <p id="S3.SS5.SSS1.p1.1">
     包含确保这些模型以对用户和利益相关者可解释和可辩护的方式运行的机制和策略。 大语言模型 (LLM) 的问责制至关重要，因为这些模型通常会影响决策过程，并生成影响公众舆论和个人认知的内容。
    </p>
    <p id="S3.SS5.SSS1.p2.1">
     LLM 问责制的根基在于
     <strong>
      创建用户可以质疑和理解的系统
     </strong>
     。 这包括实施模型设计、训练数据和决策过程的透明文档。 它还包括为模型产生的结果建立明确的责任划分，无论这些结果是直接输出还是受影响的决策。 审计跟踪和模型版本控制等机制对于追溯任何出现的错误或问题来源至关重要，从而能够有效地采取纠正措施。
    </p>
    <h4>
     5.2RAG中的问责制
    </h4>
    <p id="S3.SS5.SSS2.p1.1">
     不仅涉及生成的内容，还涉及用于告知该内容的来源和检索过程。 它是为了确保整个管道 - 检索、生成以及两者之间的接口 - 都受到
     <strong>
      监督和控制
     </strong>
     。
    </p>
    <p id="S3.SS5.SSS2.p2.1">
     对于 RAG 系统，问责制包括实施可以验证和确认检索过程中使用的信息来源的方法。 这确保了馈送到生成组件的信息是准确、相关且可信的。 问责制机制必须能够跟踪和报告哪些检索信息影响了生成内容的特定部分，从而提供清晰的信息流谱系。
    </p>
    <h4>
     5.3代表性研究
    </h4>
    <p id="S3.SS5.SSS3.p2.1">
     <strong>
      生成中的知识归属
     </strong>
     涉及在生成过程中将引文直接嵌入到模型的响应中。 早期尝试，如 WebGPT、LaMDA 和 WebBrain利用了大量的网页和维基百科资源来训练模型，生成附带引文的响应，从而增强信息的权威性和可追溯性。
    </p>
    <p id="S3.SS5.SSS3.p3.1">
     SearChain [Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks] 提出了一种新颖的方法，通过生成查询链 (CoQ)，每个节点代表一个逐步细化核心问题理解的查询。 此方法确保检索到的信息与手头的疑问密切相关，并生成完整的推理轨迹，通过其操作提高答案的可追溯性和可信度。
    </p>
    <p id="S3.SS5.SSS3.p4.1">
     VTG [Towards verifiable text generation with evolving memory and self-reflection] 将一个进化记忆系统与一个双层验证器集成在一起，专门设计用于生成可验证的文本。 该系统巧妙地结合了长期和短期记忆机制，以适应内容关注点的动态变化，并使用 NLI 模型来评估断言与潜在证据之间关系的逻辑强度。
    </p>
    <p id="S3.SS5.SSS3.p5.1">
     LLAtrieval [Llatrieval: Llm-verified retrieval for verifiable generation] 提出了一种迭代更新过程，不断检查检索到的文档是否充分支持生成的答案，有助于识别和纠正潜在的错误或遗漏，从而提高答案的准确性和完整性。 AGREE [Effective large language model adaptation for improved grounding] 将自然语言推理 (NLI) 模型作为验证工具，不仅增强了答案与检索内容之间的一致性检查，还采用了测试时自适应 (TTA) 策略。 这使得 LLM 能够在生成过程中主动寻找和引用最新信息，显着提高其响应的准确性和可靠性。
    </p>
    <p id="S3.SS5.SSS3.p6.1">
     通过引入细粒度的奖励机制， [Training language models to generate text with citations via finegrained rewards] 教会 LLM 如何准确地引用外部信息源。 此方法利用拒绝采样和强化学习算法，提供本地化和专门的奖励信号，显着提高了模型在生成带有引文的文本方面的性能。
    </p>
    <p id="S3.SS5.SSS3.p7.1">
     思想分层图 (HGoT) [HGOT: hierarchical graph of thoughts for retrieval-augmented incontext learning in factuality evaluation] 通过将复杂查询分解为更小的子查询，并利用 LLM 的规划能力逐步解决它们，从而提高了复杂查询的上下文学习，提高了检索效率和准确性。
    </p>
    <p id="S3.SS5.SSS3.p8.1">
     基于生成式检索，[Source-aware training enables knowledge attribution in language models] 使模型能够在预训练期间将 DocID 与知识关联起来，并在指令调优期间引入对支持证据的引用，从而大幅度提高 LLM 的知识归因能力，加强其问责制。
    </p>
    <p id="S3.SS5.SSS3.p9.1">
     ReClaim [Ground every sentence: Improving retrieval-augmented llms with interleaved referenceclaim generation] 提出了一种细粒度的属性文本生成方法，该方法在长篇问答任务中，交替地生成引用和答案。 这使得模型可以为每个答案句子添加句子级别的细粒度引用。 该论文还引入了解码约束，以防止引用和源段落之间出现不一致，从而降低了事实核查任务的复杂性。
    </p>
    <p id="S3.SS5.SSS3.p10.1">
     <strong>
      生成后的知识归因
     </strong>
     包含模型首先生成响应然后追溯添加引用方法。 RARR 模型 [RARR: researching and revising what language models say, using language models] 搜索外部证据并在语言模型的初始输出上执行后期编辑，以保持原文的本质，同时显著提高事实准确性，加强归因验证，而无需更改现有的模型架构。
    </p>
    <p id="S3.SS5.SSS3.p11.1">
     PURR [PURR: efficiently editing language model hallucinations by denoising language model corruptions] 采用了无监督学习途径，使 LLM 能够自主创建噪声文本，然后训练专门的编辑器来净化这些噪声，从而实现快速高效的文本优化循环。 该策略不仅增强了归因准确性，而且加速了内容生成，利用 LLM 的创造力来自动驱动训练数据的生成。
    </p>
    <p id="S3.SS5.SSS3.p12.1">
     此外，CEG [Citationenhanced generation for llm-based chatbots] 专注于通过搜索相关的支持性文档来增强生成的内容，并引入基于 NLI 的引用生成机制，确保每个陈述都有证据支持，从而提高文本的可信度和可信度。
    </p>
    <p id="S3.SS5.SSS3.p13.1">
     为了自动验证 LLM 生成的答案与支持证据的一致性，[Retrieving supporting evidence for generative question answering] 进行了两个简单的实验，发现 LLM 可以验证其生成的答案，准确率超过 80%，从而减少幻觉。 然而，验证过程可能会遗漏错误的生成答案，并且不能完全消除幻觉。
    </p>
    <h3>
     6隐私
    </h3>
    <h4>
     6.1LLM 的一般定义
    </h4>
    <p id="S3.SS6.SSS1.p1.1">
     涉及保护个人数据、身份保密和维护尊严 。 随着 LLM 在各个领域的广泛应用，它们在处理大量数据时不可避免地会遇到敏感和个人信息
    </p>
    <p id="S3.SS6.SSS1.p2.1">
     LLM 在训练过程中依赖于广泛的网络数据，其中可能包含个人信息，例如搜索日志和隐私数据。 如果 LLM 不能正确管理这些信息，它们可能会在响应查询时无意中泄露此类敏感数据。 此外，恶意行为者可能会利用特定提示来提取或推断 LLM 学习的私人信息，从而增加隐私泄露的风险。 因此，研究人员正在探索各种方法来增强 LLM 的隐私保护，包括将隐私保护机制纳入模型，以及开发用于检测和防止隐私泄露的工具和技术。
    </p>
    <h4>
     6.2RAG中的隐私
    </h4>
    <p id="S3.SS6.SSS2.p1.1">
     RAG 会改变 LLM 生成的输出的内在行为，从而导致新的隐私问题，尤其是在处理敏感和私人数据时。
    </p>
    <p>
     例如，检索数据库可能包含特定于医疗保健等领域的敏感信息，攻击者可以通过制作与特定疾病相关的查询来利用 RAG 系统，以访问患者处方信息或其他私人医疗记录。 此外，RAG 系统中的检索过程可能导致 LLM 输出包含在训练或微调数据集中的私人信息。
    </p>
    <p id="S3.SS6.SSS2.p2.1">
     研究人员提出了各种攻击方法来证明 RAG 系统易于泄露私人检索数据库信息 [TrojanRAG和BADRAG]。 他们发现，即使在黑盒攻击场景下，攻击者也可以通过制作特定提示有效地从 RAG 系统的检索数据库中提取信息 [Follow my instruction and spill the beans: Scalable data extraction from retrieval-augmented generation systems]。 这些攻击不仅揭示了 RAG 系统中的隐私保护缺陷，而且强调了在设计和部署 RAG 系统时考虑隐私保护措施的必要性 [Privacy implications of retrieval-based language models]。 因此，我们将深入研究 RAG 系统隐私的攻击和防御，以及对现有方法的评估。
    </p>
    <h4>
     6.3代表性研究
    </h4>
    <p id="S3.SS6.SSS3.p1.1">
     针对 RAG 系统的现有攻击和防御策略。 隐私攻击旨在识别和设计方法来利用现有 RAG 系统的安全弱点，揭示这些问题，帮助从业者和政策制定者认识到潜在的 RAG 安全问题，并为关于生成模型监管的讨论做出贡献；隐私防御旨在设计能够防御这些攻击的 RAG 系统，增强其安全性和隐私性。
    </p>
    <p id="S3.SS6.SSS3.p2.1">
     <strong>
      隐私攻击。
     </strong>
    </p>
    <p>
     对于知识中毒攻击，PoisonedRAG攻击者可以在知识数据库中注入少量“中毒文本”，导致 LLM 生成攻击者选择的输出。 实验表明，即使将少量中毒文本注入知识数据库，也会显著影响 LLM 通过 RAG 生成的输出。
    </p>
    <p id="S3.SS6.SSS3.p3.1">
     随后，Phantom提出了一种两步攻击框架：首先，攻击者创建一份有毒文档，该文档只有在受害者查询中存在特定对抗性触发器时才会被 RAG 系统检索到；然后，攻击者仔细构建有毒文档中的对抗性字符串，以触发 LLM 生成器中的各种对抗性攻击，包括拒绝服务、名誉损害、隐私侵犯和有害行为。 研究表明，攻击者只需一个恶意文档就可以有效地控制 RAG 系统。
    </p>
    <p id="S3.SS6.SSS3.p4.1">
     关于 RAG 系统中数据存储泄露的风险，[Follow my instruction and spill the beans: Scalable data extraction from retrieval-augmented generation systems] 表明，使用
     <strong>
      命令注入
     </strong>
     ，可以轻松地从使用命令调优的语言模型构建的 RAG 系统的数据存储中提取文本数据，利用语言模型的指令遵循能力。 该论文是对开源和生产 RAG 系统中数据泄漏问题进行的第一项综合研究，发现即使在黑盒 API 访问的情况下，也可以通过提示注入从 RAG 模型的非参数数据存储中提取数据。 此外，随着模型尺寸的增加，数据提取的漏洞也随之增加，特别是对于指令调优的语言模型。
    </p>
    <p id="S3.SS6.SSS3.p5.1">
     同样基于提示，[Neural exec: Learning (and learning from) execution triggers for prompt injection attacks] 引入了神经执行，它将执行触发器的创建视为一个可微分的搜索问题，并使用基于学习的方法自动生成它们，这与依赖手动设计的传统攻击不同。 因此，攻击者可以生成与已知攻击形式和形状明显不同的触发器，从而规避现有的基于黑名单的检测和清理方法。
    </p>
    <p id="S3.SS6.SSS3.p6.1">
     利用 RAG 中的后门攻击，TrojanRAG在一般攻击场景中操纵语言模型的性能。 研究人员构建了精心设计的目标上下文和触发器集，并通过对比学习优化了多个后门快捷方式，以提高匹配条件，将触发条件限制在参数子空间内。 该论文还从攻击者和用户的角度分析了语言模型中后门的真正危害，并进一步验证了上下文是破解模型的有益工具。
    </p>
    <p id="S3.SS6.SSS3.p7.1">
     此外，BadRAG通过将特定内容段落注入 RAG 数据库来实现检索后门攻击，这些段落在正常查询下表现良好，但在触发特定条件时会返回定制的恶意查询。 该论文描述了如何通过定制触发器和注入的对抗性段落来实施攻击。 作者证明，通过注入仅 10 个对抗性段落（总语料库的 0.04%），可以实现 98.2% 的成功率来检索对抗性段落。
    </p>
    <p id="S3.SS6.SSS3.p8.1">
     <strong>
      隐私防御。
     </strong>
    </p>
    <p>
     [Privacy implications of retrieval-based language models] 探讨了基于检索的语言模型的隐私风险，kNN-LMs [Generalization through memorization: Nearest neighbor language models]。 研究发现，与语言模型等参数化模型相比，kNN-LMs 更容易从其私有数据存储中泄露私人信息。 为了减轻隐私风险，当私人信息明确存在时，简单的清理步骤可以完全消除风险。 对于难以从数据中删除的非目标私人信息，本文考虑了在数据存储和编码器训练中混合公共和私人数据的策略。
    </p>
    <p id="S3.SS6.SSS3.p9.1">
     尽管 RAG 引入了与检索数据相关的新的风险，[The good and the bad: Exploring privacy issues in retrieval-augmented generation (RAG)] 发现 RAG 可以减少 LLM 训练数据的泄露。 对于攻击，提出了一种结构化提示攻击，通过提示语言模型在响应中包含检索到的数据，诱使检索器准确地检索目标信息。 为了防御，本文提出了三种策略：重新排序、与相关查询相关的摘要、设置距离阈值，以减轻数据提取风险。
    </p>
    <p id="S3.SS6.SSS3.p10.1">
     [Is my data in your retrieval database? membership inference attacks against retrieval augmented generation] 特别关注一种称为成员推理攻击 (MIA) 的隐私威胁。 攻击者可以通过观察 RAG 系统的输出，推断特定文本段是否出现在检索数据库中。 研究表明，在黑盒和灰盒设置中，通过设计合适的提示，可以有效地确定文档在检索数据库中的成员身份。
    </p>
    <p id="S3.SS6.SSS3.p11.1">
     这些研究展示了 RAG 系统在处理敏感信息时面临的重大隐私风险和安全挑战。 从知识中毒、数据提取到后门攻击和成员推理攻击，这些攻击不仅揭示了当前模型和数据存储策略的不足，而且强调了在设计和部署此类系统时加强安全和隐私保护的重要性。
    </p>
    <h2 style="background-color:transparent">
     评估
    </h2>
    <p>
     <img alt="" height="391" src="https://i-blog.csdnimg.cn/direct/bdd2dd7122b442c3b2e9f638370f99bc.png" width="800"/>
    </p>
    <p>
     在可信度方面，专有 LLM 通常优于大多数开放权重 LLM。
    </p>
    <p>
     与纯粹的预训练模型相比，经过指令调优和对齐的模型在大多数情况下往往表现出更高的可信度。
    </p>
    <p>
     参数规模更大的模型不一定表现出更高的可信度。
    </p>
    <p>
     与鲁棒性和问责制相比，隐私和公平对 LLM 构成了更大的挑战。
    </p>
    <p>
     <img alt="" height="357" src="https://i-blog.csdnimg.cn/direct/21897fed185b4b84b9360c7b5de428ef.png" width="600"/>
    </p>
    <p>
     总体而言，GPT-4o 和 GPT-3.5-turbo 表现出更高的综合可信度，隐私维度除外。 这突出了隐私保护的持续挑战。 其他开源模型往往在特定领域表现出色。 例如：Llama2-chat 系列模型在隐私保护方面表现尤为出色。 Baichuan2-chat 系列模型展现出高透明度。 GLM-chat 系列模型在问责制方面表现出色。 这项分析表明，实现全面的可信度是一项复杂的任务，需要付出更多努力。 改进的关键领域包括制定标准化基准、增强训练数据以及更严格的评估方法。 这些步骤对于确保模型能够在所有可信度维度上都能表现出色至关重要。
    </p>
    <h2>
     挑战
    </h2>
    <p id="S5.SS1.p2.1">
     <strong>
      静态模型知识与动态信息之间的冲突。
     </strong>
     确保 RAG 系统的真实性至关重要，因为它直接影响生成内容的可信度。 真实性挑战主要来自两个方面： 首先，
     <strong>
      知识的动态性
     </strong>
     。 虽然模型的参数捕获了截至某个截止日期的知识，但检索到的信息可能包含更当前的数据，从而导致潜在的冲突。 开发适应性机制以协调这些差异对于维护系统响应的准确性和相关性至关重要。 其次，需要对检索到的文本进行深入理解和推理。 处理长文本或复杂文本，无论是来自单个文档还是多个文档，都可能使 LLM 不堪重负，从而导致事实上的不准确。 必须制定有效的策略来管理和综合长文本，而不会影响生成信息的完整性。
    </p>
    <p id="S5.SS1.p3.1">
     <strong>
      在存在噪声数据的情况下保证可靠性。
     </strong>
     健壮性是确保 RAG 系统即使在不同条件下也能可靠地生成准确响应的基础。 主要挑战在于系统在检索到的证据中不同信噪比下保持一致性能的能力。 健壮的 RAG 系统还必须在输入数据中存在噪声的情况下保持其性能，无论内容、顺序或粒度如何。 必须不断改进检索和处理技术，以应对现实世界数据带来的各种挑战，确保系统保持可靠性和弹性。
    </p>
    <p id="S5.SS1.p4.1">
     <strong>
      嵌入在训练和检索数据中的偏差。
     </strong>
     RAG 系统的公平性是一个重大问题，主要是因为训练数据和检索到的内容中都存在偏差。 这些偏差可能会扭曲生成过程，导致不公平或歧视性的结果。 解决公平问题需要一个综合性的方法，包括严格审查和缓解训练和检索阶段的偏差。 确保公平还涉及评估外部知识来源，以防止引入额外的偏差。 开发健壮的策略来检测和最小化偏差对于确保 RAG 系统产生公平且无偏见的结果至关重要。
    </p>
    <p id="S5.SS1.p5.1">
     <strong>
      数据利用和决策过程中的不透明性。
     </strong>
     透明度对于建立对 RAG 系统的信任至关重要，它为用户提供关于系统如何运作以及如何做出决定的清晰见解。 透明度的挑战在于理解所使用的数据和知识来源，以及它们如何在系统中集成。 通过注意力可视化和解释生成等技术可以增强透明度，这些技术帮助用户了解生成答案的基础。
    </p>
    <p id="S5.SS1.p6.1">
     <strong>
      输出的可追溯性。
     </strong>
     RAG 的问责制对于确保信息的来源可以追溯并验证其准确性至关重要。 这项挑战涉及实施知识归因策略，这些策略在生成过程期间和之后将生成的内容与特定来源相关联。 有效的问责机制允许用户将错误追溯到其来源，从而促进纠正和改进。 加强问责制不仅建立了用户信任，而且提高了系统的可靠性和道德标准。
    </p>
    <p id="S5.SS1.p7.1">
     <strong>
      数据驱动过程中的敏感信息。
     </strong>
     保护用户隐私在 RAG 系统中至关重要，因为它在整个检索和生成过程中保护敏感信息。 隐私挑战包括在检索期间暴露个人数据的风险，这需要开发健壮的隐私保护机制。 这些机制应该可以防止未经授权的访问，并将数据泄露的风险降至最低。 此外，检测和防止隐私泄露的工具对于维护安全的数据处理实践至关重要。 通过优先考虑隐私保护，RAG 系统可以确保用户信任并符合数据保护法规。
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f35323931313130382f:61727469636c652f64657461696c732f313436313731313530" class_="artid" style="display:none">
 </p>
</div>


