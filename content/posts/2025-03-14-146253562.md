---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f6a756e6d696e67342f:61727469636c652f64657461696c732f313436323533353632"
layout: post
title: "如何手动使用下载并且运行-QwQ-32B-GGUF"
date: 2025-03-14 11:46:33 +08:00
description: "开始设置编译，根据不同的系统架构可以选择不同的编译指令。切换到目录，并且新增编译目录。"
keywords: "如何手动使用下载并且运行 QwQ-32B-GGUF"
categories: ['人工智能']
tags: ['神经网络', '目标检测', '深度学习', '机器学习', '人工智能']
artid: "146253562"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146253562
    alt: "如何手动使用下载并且运行-QwQ-32B-GGUF"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146253562
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146253562
cover: https://bing.ee123.net/img/rand?artid=146253562
image: https://bing.ee123.net/img/rand?artid=146253562
img: https://bing.ee123.net/img/rand?artid=146253562
---

# 如何手动使用下载并且运行 QwQ-32B-GGUF

首先使用安装

```
pip install ModelScope
```

使用
**ModelScope 下载对应的模型**

```
modelScope download --model Qwen/QwQ-32B-GGUF qwq-32b-q4_k_m.gguf
```

**第二步开始下载**
**ollama**

```
git clone https://githubfast.com/ggerganov/llama.cpp # githubfast.com 可以加速下载
```

![](https://i-blog.csdnimg.cn/direct/9a5e75b05c5d4d418d7c1e392ac34aeb.png)

切换到目录，并且新增编译目录

```
mkdir build 
cd build 
```

开始设置编译，根据不同的系统架构可以选择不同的编译指令

```
cd build 
# CPU 编译
cmake .. -DCMAKE_BUILD_TYPE=Release

# NVIDIA GPU 加速：
cmake .. -DCMAKE_BUILD_TYPE=Release -DLLAMA_CUDA=ON
# Apple Silicon 加速：
cmake .. -DCMAKE_BUILD_TYPE=Release -DLLAMA_METAL=ON
```

![](https://i-blog.csdnimg.cn/direct/5bb8bc2055cf417ebe0f1787cf3c4e61.png)

开始编译

```
 make -j$(nproc)
```

![](https://i-blog.csdnimg.cn/direct/ab453b8c69f54d148810c3dae66cbad5.png)

![](https://i-blog.csdnimg.cn/direct/5616c1f845074550b4f2d6dbc43ae405.png)

查询是否编译成功

```
# 查询是否编译成功，存在表示编译成功
ls -lh bin/llama-run
```

存在表示成功

可以运行模型

```
./bin/llama-run /mnt/workspace/.cache/modelscope/models/Qwen/QwQ-32B-GGUF/qwq-32b-q4_k_m.gguf
```

![](https://i-blog.csdnimg.cn/direct/a28d59d86ade4a379799e226e57d0303.png)