---
layout: post
title: "如何手动使用下载并且运行-QwQ-32B-GGUF"
date: 2025-03-14 11:46:33 +0800
description: "开始设置编译，根据不同的系统架构可以选择不同的编译指令。切换到目录，并且新增编译目录。"
keywords: "如何手动使用下载并且运行 QwQ-32B-GGUF"
categories: ['人工智能']
tags: ['神经网络', '目标检测', '深度学习', '机器学习', '人工智能']
artid: "146253562"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146253562
    alt: "如何手动使用下载并且运行-QwQ-32B-GGUF"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146253562
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146253562
cover: https://bing.ee123.net/img/rand?artid=146253562
image: https://bing.ee123.net/img/rand?artid=146253562
img: https://bing.ee123.net/img/rand?artid=146253562
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     如何手动使用下载并且运行 QwQ-32B-GGUF
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     首先使用安装
    </p>
    <pre><code>pip install ModelScope</code></pre>
    <p>
     使用
     <strong>
      ModelScope 下载对应的模型
     </strong>
    </p>
    <pre><code>modelScope download --model Qwen/QwQ-32B-GGUF qwq-32b-q4_k_m.gguf</code></pre>
    <p>
     <strong>
      第二步开始下载
     </strong>
     <strong>
      ollama
     </strong>
    </p>
    <pre><code>git clone https://githubfast.com/ggerganov/llama.cpp # githubfast.com 可以加速下载</code></pre>
    <p>
     <img alt="" height="270" src="https://i-blog.csdnimg.cn/direct/9a5e75b05c5d4d418d7c1e392ac34aeb.png" width="1844"/>
    </p>
    <p>
     切换到目录，并且新增编译目录
    </p>
    <pre><code>mkdir build 
cd build </code></pre>
    <p>
     开始设置编译，根据不同的系统架构可以选择不同的编译指令
    </p>
    <pre><code>cd build 
# CPU 编译
cmake .. -DCMAKE_BUILD_TYPE=Release

# NVIDIA GPU 加速：
cmake .. -DCMAKE_BUILD_TYPE=Release -DLLAMA_CUDA=ON
# Apple Silicon 加速：
cmake .. -DCMAKE_BUILD_TYPE=Release -DLLAMA_METAL=ON</code></pre>
    <p>
     <img alt="" height="902" src="https://i-blog.csdnimg.cn/direct/5bb8bc2055cf417ebe0f1787cf3c4e61.png" width="1990"/>
    </p>
    <p>
     开始编译
    </p>
    <pre><code> make -j$(nproc)</code></pre>
    <p>
     <img alt="" height="1350" src="https://i-blog.csdnimg.cn/direct/ab453b8c69f54d148810c3dae66cbad5.png" width="1684"/>
    </p>
    <p>
     <img alt="" height="1098" src="https://i-blog.csdnimg.cn/direct/5616c1f845074550b4f2d6dbc43ae405.png" width="1324"/>
    </p>
    <p>
     查询是否编译成功
    </p>
    <pre><code># 查询是否编译成功，存在表示编译成功
ls -lh bin/llama-run</code></pre>
    <p>
     存在表示成功
    </p>
    <p>
     可以运行模型
    </p>
    <pre><code>./bin/llama-run /mnt/workspace/.cache/modelscope/models/Qwen/QwQ-32B-GGUF/qwq-32b-q4_k_m.gguf</code></pre>
    <p>
     <img alt="" height="372" src="https://i-blog.csdnimg.cn/direct/a28d59d86ade4a379799e226e57d0303.png" width="2004"/>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f6a756e6d696e67342f:61727469636c652f64657461696c732f313436323533353632" class_="artid" style="display:none">
 </p>
</div>


