---
layout: post
title: "deepseek-GRPOç®—æ³•ä¿å§†çº§è®²è§£æ•°å­¦åŸç†æºç è§£ææ¡ˆä¾‹å®æˆ˜"
date: 2025-03-16 23:06:33 +0800
description: "GRPOï¼ˆGroupwise relative policy optimizationï¼‰ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–æ˜¯deepseekæå‡ºçš„ä¸€ç§RLHF(åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ )æŠ€æœ¯ã€‚é¦–æ¬¡æå‡ºæ˜¯åœ¨DeepSeekMathä¸­ï¼ŒRLè¢«è¯æ˜åœ¨SFTé˜¶æ®µåèƒ½æœ‰æ•ˆæå‡LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚åœ¨GRPOæå‡ºä¹‹å‰ï¼Œæœ‰ä¸¤ç§æµè¡Œçš„RLHFæŠ€æœ¯è¢«å¹¿æ³›ç”¨äºå¤§æ¨¡å‹çš„å¯¹é½è¿‡ç¨‹ä¸­ï¼Œåˆ†åˆ«æ˜¯PPOå’ŒDPOã€‚"
keywords: "deepseek GRPOç®—æ³•ä¿å§†çº§è®²è§£(æ•°å­¦åŸç†+æºç è§£æ+æ¡ˆä¾‹å®æˆ˜)"
categories: ['Llm']
tags: ['ç®—æ³•', 'äººå·¥æ™ºèƒ½']
artid: "146103697"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146103697
    alt: "deepseek-GRPOç®—æ³•ä¿å§†çº§è®²è§£æ•°å­¦åŸç†æºç è§£ææ¡ˆä¾‹å®æˆ˜"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146103697
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146103697
cover: https://bing.ee123.net/img/rand?artid=146103697
image: https://bing.ee123.net/img/rand?artid=146103697
img: https://bing.ee123.net/img/rand?artid=146103697
---

# deepseek GRPOç®—æ³•ä¿å§†çº§è®²è§£(æ•°å­¦åŸç†+æºç è§£æ+æ¡ˆä¾‹å®æˆ˜)

## ä»€ä¹ˆæ˜¯GRPO

**GRPOï¼ˆGroupwise relative policy optimizationï¼‰ç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–**
æ˜¯deepseekæå‡ºçš„ä¸€ç§RLHF(åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ )æŠ€æœ¯ã€‚é¦–æ¬¡æå‡ºæ˜¯åœ¨DeepSeekMathä¸­ï¼ŒRLè¢«è¯æ˜åœ¨SFTé˜¶æ®µåèƒ½æœ‰æ•ˆæå‡LLMçš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚  
åœ¨GRPOæå‡ºä¹‹å‰ï¼Œæœ‰ä¸¤ç§æµè¡Œçš„RLHFæŠ€æœ¯è¢«å¹¿æ³›ç”¨äºå¤§æ¨¡å‹çš„å¯¹é½è¿‡ç¨‹ä¸­ï¼Œåˆ†åˆ«æ˜¯PPOå’ŒDPOã€‚

### ç¾¤ç»„å½¢æˆ(Group Formation):è®©å¤§æ¨¡å‹åˆ›å»ºå¤šç§è§£å†³æ–¹æ¡ˆ

GRPOç®—æ³•çš„ç¬¬ä¸€æ­¥éå¸¸ç›´è§‚ï¼Œç±»ä¼¼äºå­¦ç”Ÿå°è¯•å¤šç§è§£æ³•å»è§£å†³åŒä¸€ä¸ªé—®é¢˜ã€‚å¯¹äºç»™å®šçš„promptï¼Œè®©å¤§æ¨¡å‹å°è¯•ç”Ÿæˆå¤šç§è§£ç­”(attempt)æ¥è§£å†³åŒä¸€ä¸ªé—®é¢˜(é€šå¸¸4ã€8æˆ–16ç§çš„ä¸åŒè§£ç­”)ã€‚ä¸åŒçš„è§£ç­”ç¤ºä¾‹å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/7c9539204d83475e9d220a57b8134784.png)

### åå¥½å­¦ä¹ (Preference Learning)ï¼šè®©å¤§æ¨¡å‹ç†è§£ä½•ä¸ºå¥½çš„è§£ç­”

ä¸å…¶ä»–çš„RLHFæ–¹æ³•éœ€è¦ä¸€ä¸ªå•ç‹¬çš„å¥–åŠ±æ¨¡å‹(reward
model)ä¸åŒï¼ŒGRPOæ–¹æ³•å¯ä»¥ä½¿ç”¨ä»»ä½•å‡½æ•°æˆ–æ¨¡å‹æ¥è¯„ä¼°LLMè§£ç­”(Solution)çš„è´¨é‡ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªé•¿åº¦å‡½æ•°ä½œä¸ºè¯„ä¼°å™¨æ¥å¥–åŠ±æ¨¡å‹ç”Ÿæˆæ›´ç®€çŸ­çš„è§£ç­”ï¼Œæˆ–æ˜¯ä½¿ç”¨ä¸€ä¸ªæ•°å­¦æ±‚è§£å™¨ï¼Œæ¥å¥–åŠ±æ¨¡å‹ç”Ÿæˆæ›´å‡†ç¡®çš„æ•°å­¦è§£ç­”æ–¹æ¡ˆã€‚  
è¯„ä¼°è¿‡ç¨‹ä¼šä»å¤šä¸ªè§’åº¦æ¥è¡¡é‡å¤§æ¨¡å‹ç”Ÿæˆè§£ç­”æ–¹æ¡ˆçš„è´¨é‡ï¼ŒåŒ…æ‹¬ï¼š  
1ï¼‰æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®ï¼›2ï¼‰ç­”æ¡ˆæ˜¯å¦æŒ‰æŒ‡å®šçš„æ ¼å¼è¾“å‡ºï¼ˆå¦‚XMLæ ‡ç­¾æ ¼å¼æ­£ç¡®æ€§ï¼‰ï¼›3ï¼‰æ¨ç†è¿‡ç¨‹ä¸æä¾›çš„ç­”æ¡ˆæ˜¯å¦ä¸€è‡´ã€‚

#### ç»„å†…ç›¸å¯¹ä¼˜åŠ¿

è¿™ç§å¤„ç†æ–¹å¼çš„å·§å¦™ä¹‹å¤„è¿˜åœ¨äºè¯„åˆ†æœºåˆ¶ï¼ŒGRPOä¸æ˜¯ç®€å•çš„ç»™å‡ºç»å¯¹åˆ†æ•°ï¼Œè€Œæ˜¯å¯¹æ¯ä¸ªç»„å†…çš„å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ã€‚å®ƒé‡‡ç”¨äº†ä¸€ä¸ªç®€å•ä½†æœ‰æ•ˆçš„æ–¹æ³•æ¥è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼š

A d v a t a g e = ( r e w a r d âˆ’ m e a n ( g r o u p r e w a r d s ) / s t d
( g r o u p r e w a r d s ) Advatage = (reward- mean(group_rewards)/
std(group_rewards) Advatage=(rewardâˆ’mean(grouprâ€‹ewards)/std(grouprâ€‹ewards)

è¿™ç§å½’ä¸€åŒ–æ–¹æ³•å¸®åŠ©å¤§æ¨¡å‹ç†è§£ï¼Œå“ªäº›è§£ç­”æ–¹æ¡ˆæ¯”åŒç»„å†…å…¶ä»–è§£ç­”æ–¹æ¡ˆæ›´å¥½æˆ–æ›´å·®ï¼Œè€Œä¸æ˜¯ç®€å•çš„åé¦ˆç»å¯¹å¾—åˆ†ç»™å¤§æ¨¡å‹ã€‚

### ä¼˜åŒ–(optimization): è®©å¤§æ¨¡å‹ä»ç»éªŒä¸­å­¦ä¹ (learning from experience)

GRPOçš„æœ€åä¸€æ­¥æ˜¯æ ¹æ®è§£å†³æ–¹æ¡ˆç»„çš„è¯„ä¼°ç»“æœï¼ŒæŒ‡å¯¼å¤§æ¨¡å‹è¿›è¡Œæ”¹è¿›ã€‚è¿™ä¸€è¿‡ç¨‹åŸºäºä¸¤ä¸ªä¸»è¦åŸåˆ™ï¼š

  1. é¼“åŠ±æ¨¡å‹ç”Ÿæˆæ›´å¤šç±»ä¼¼æˆåŠŸæ¡ˆä¾‹çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒæ—¶è¿œç¦»æ•ˆæœä¸ä½³çš„æ–¹æ³•ï¼›
  2. ä½¿ç”¨KLæ•£åº¦æƒ©ç½šä½œä¸ºå®‰å…¨æœºåˆ¶ï¼Œé˜²æ­¢æ¨¡å‹åœ¨è®­ç»ƒä¸­ä¸€æ¬¡å‘ç”Ÿè¿‡äºå‰§çƒˆçš„å˜åŒ–ï¼›

è¿™ç§æ–¹æ³•åœ¨å®è·µä¸­è¢«è¯æ˜æ¯”ä¼ ç»Ÿæ–¹æ³•æ›´ç¨³å®šï¼ŒåŸå› åœ¨äºï¼š

  * åŒæ—¶è€ƒè™‘å¤šä¸ªè§£å†³æ–¹æ¡ˆï¼Œè€Œä¸å±€é™äºä¸¤ä¸¤æ¯”è¾ƒï¼›
  * åŸºäºç»„çš„å½’ä¸€åŒ–æœ‰åŠ©äºé¿å…å¥–åŠ±ç¼©æ”¾é—®é¢˜ï¼ˆreward scalingï¼‰
  * KLæ•£åº¦æƒ©ç½šä½œä¸ºå®‰å…¨ç½‘ï¼Œä¿è¯æ¨¡å‹åœ¨å­¦ä¹ æ–°çŸ¥è¯†çš„åŒæ—¶ï¼Œä¸ä¼šå¿˜è®°å·²ç»æŒæ¡çš„å†…å®¹ï¼›

> æ€»ç»“GRPOçš„å…³é”®åˆ›æ–°åœ¨äºï¼š
>
>   1. ç›´æ¥ä»ä»»ä½•å‡½æ•°æˆ–æ¨¡å‹ä¸­å­¦ä¹ ï¼Œæ¶ˆé™¤äº†å¯¹å•ç‹¬å¥–åŠ±æ¨¡å‹çš„ä¾èµ–ã€‚
>   2. åŸºäºç»„çš„å­¦ä¹ æ–¹å¼ï¼Œæ¯”ä¼ ç»Ÿçš„æˆå¯¹æ¯”è¾ƒç­‰æ–¹æ³•æ›´åŠ ç¨³å®šå’Œé«˜æ•ˆã€‚
>

#### ç›®æ ‡å‡½æ•°

GRPOç®—æ³•ä»æ—§çš„ç­–ç•¥æ¨¡å‹ä¸­é‡‡æ ·ä¸€ç»„è¾“å‡º o 1 , o 2 , . . . , o G {o_1,o_2,...,o_G}
o1â€‹,o2â€‹,...,oGâ€‹ï¼Œå¹¶é€šè¿‡ä¸‹é¢çš„ç›®æ ‡å‡½æ•°æ¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ï¼š  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/241a8b2622fa4df6840a75a02c0630f3.png)  
GRPOçš„ç›®æ ‡å‡½æ•°é€šè¿‡ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡æ›¿ä»£ä¼ ç»Ÿä¼˜åŠ¿å‡½æ•°ï¼Œå¹¶ç»“åˆPPOçš„è£å‰ªæœºåˆ¶ï¼ˆé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼‰å’ŒKLæ•£åº¦æ­£åˆ™åŒ–ï¼ˆçº¦æŸç­–ç•¥ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ï¼‰ï¼Œåœ¨æœ€å¤§åŒ–å¥–åŠ±çš„åŒæ—¶ç¡®ä¿ç­–ç•¥ç¨³å®šæ€§ï¼š

  1. å‡½æ•°ä¸­é€šè¿‡æœ€å¤§åŒ–ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼Œå¢åŠ ç”Ÿæˆä¼˜è´¨ç»“æœçš„æ¦‚ç‡ï¼Œå‡å°‘ç”ŸæˆåŠ£è´¨ç»“æœçš„æ¦‚ç‡ï¼›
  2. é€šè¿‡KLæ•£åº¦æƒ©ç½šé¡¹ï¼Œç¡®ä¿æ¨¡å‹ç­–ç•¥ä¸ä¼šè¿‡åº¦åç¦»å‚è€ƒç­–ç•¥ï¼Œä»è€Œä¿è¯ä¼˜åŒ–è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚

å¼ä¸­å‚æ•°æ³¨é‡Šå¦‚ä¸‹ï¼š  
Î¸ : å½“å‰ç­–ç•¥æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡å‡½æ•°æ›´æ–° q âˆ¼ P ( Q ) : ä»é—®é¢˜åˆ†å¸ƒä¸­é‡‡æ ·çš„è¾“å…¥ï¼ˆå¦‚ç”¨æˆ·æŒ‡ä»¤ï¼‰ G :
æ¯ä¸ªè¾“å…¥ç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆç»„å†…æ ·æœ¬æ•°ï¼‰ o i âˆ¼ Ï€ Î¸ old ( O âˆ£ q ) : æ—§ç­–ç•¥ç”Ÿæˆçš„ç¬¬ i ä¸ªè¾“å‡ºæ ·æœ¬ Ï€ Î¸ old :
æ—§ç­–ç•¥æ¨¡å‹ï¼ˆç”Ÿæˆæ ·æœ¬æ—¶å›ºå®šï¼‰ Ï€ Î¸ : å½“å‰ç­–ç•¥æ¨¡å‹ï¼ˆé€šè¿‡ä¼˜åŒ–æ›´æ–°ï¼‰ O âˆ£ q : è¾“å…¥ q æ¡ä»¶ä¸‹ç­–ç•¥ç”Ÿæˆçš„è¾“å‡ºåˆ†å¸ƒ âˆ£ o i âˆ£ : è¾“å‡ºåºåˆ— o
i çš„é•¿åº¦ï¼ˆtokenæ•°é‡ï¼‰ t : åºåˆ—ç”Ÿæˆçš„æ—¶é—´æ­¥ï¼ˆç¬¬ t ä¸ªtokenä½ç½®ï¼‰ A ^ i , t : ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆåŸºäºç»„å†…å¥–åŠ±å·®å€¼è®¡ç®—ï¼‰ Ïµ :
è£å‰ªèŒƒå›´å‚æ•°ï¼ˆé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼‰ Î² : KLæ•£åº¦æ­£åˆ™åŒ–ç³»æ•°ï¼ˆæ§åˆ¶ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹åç¦»ï¼‰ D KL ( Ï€ Î¸ âˆ£ âˆ£ Ï€ ref ) :
å½“å‰ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹çš„KLæ•£åº¦ Ï€ ref : å‚è€ƒæ¨¡å‹ï¼ˆå¦‚åˆå§‹SFTæ¨¡å‹ï¼‰ \begin{aligned} &\theta &&:
\text{å½“å‰ç­–ç•¥æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡å‡½æ•°æ›´æ–°} \\\ &q \sim P(Q) &&: \text{ä»é—®é¢˜åˆ†å¸ƒä¸­é‡‡æ ·çš„è¾“å…¥ï¼ˆå¦‚ç”¨æˆ·æŒ‡ä»¤ï¼‰}
\\\ &G &&: \text{æ¯ä¸ªè¾“å…¥ç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆç»„å†…æ ·æœ¬æ•°ï¼‰} \\\ &o_i \sim
\pi_{\theta_{\text{old}}}(O|q) &&: \text{æ—§ç­–ç•¥ç”Ÿæˆçš„ç¬¬ }i\text{ ä¸ªè¾“å‡ºæ ·æœ¬} \\\
&\pi_{\theta_{\text{old}}} &&: \text{æ—§ç­–ç•¥æ¨¡å‹ï¼ˆç”Ÿæˆæ ·æœ¬æ—¶å›ºå®šï¼‰} \\\ &\pi_{\theta} &&:
\text{å½“å‰ç­–ç•¥æ¨¡å‹ï¼ˆé€šè¿‡ä¼˜åŒ–æ›´æ–°ï¼‰} \\\ &O|q &&: \text{è¾“å…¥ }q\text{ æ¡ä»¶ä¸‹ç­–ç•¥ç”Ÿæˆçš„è¾“å‡ºåˆ†å¸ƒ} \\\ &|o_i|
&&: \text{è¾“å‡ºåºåˆ— }o_i\text{ çš„é•¿åº¦ï¼ˆtokenæ•°é‡ï¼‰} \\\ &t &&: \text{åºåˆ—ç”Ÿæˆçš„æ—¶é—´æ­¥ï¼ˆç¬¬ }t\text{
ä¸ªtokenä½ç½®ï¼‰} \\\ &\hat{A}_{i,t} &&: \text{ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆåŸºäºç»„å†…å¥–åŠ±å·®å€¼è®¡ç®—ï¼‰} \\\ &\epsilon &&:
\text{è£å‰ªèŒƒå›´å‚æ•°ï¼ˆé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼‰} \\\ &\beta &&: \text{KLæ•£åº¦æ­£åˆ™åŒ–ç³»æ•°ï¼ˆæ§åˆ¶ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹åç¦»ï¼‰} \\\
&D_{\text{KL}}(\pi_{\theta}||\pi_{\text{ref}}) &&: \text{å½“å‰ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹çš„KLæ•£åº¦} \\\
&\pi_{\text{ref}} &&: \text{å‚è€ƒæ¨¡å‹ï¼ˆå¦‚åˆå§‹SFTæ¨¡å‹ï¼‰} \end{aligned}
â€‹Î¸qâˆ¼P(Q)Goiâ€‹âˆ¼Ï€Î¸oldâ€‹â€‹(Oâˆ£q)Ï€Î¸oldâ€‹â€‹Ï€Î¸â€‹Oâˆ£qâˆ£oiâ€‹âˆ£tA^i,tâ€‹ÏµÎ²DKLâ€‹(Ï€Î¸â€‹âˆ£âˆ£Ï€refâ€‹)Ï€refâ€‹â€‹â€‹:å½“å‰ç­–ç•¥æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼Œé€šè¿‡ä¼˜åŒ–ç›®æ ‡å‡½æ•°æ›´æ–°:ä»é—®é¢˜åˆ†å¸ƒä¸­é‡‡æ ·çš„è¾“å…¥ï¼ˆå¦‚ç”¨æˆ·æŒ‡ä»¤ï¼‰:æ¯ä¸ªè¾“å…¥ç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆç»„å†…æ ·æœ¬æ•°ï¼‰:æ—§ç­–ç•¥ç”Ÿæˆçš„ç¬¬
i ä¸ªè¾“å‡ºæ ·æœ¬:æ—§ç­–ç•¥æ¨¡å‹ï¼ˆç”Ÿæˆæ ·æœ¬æ—¶å›ºå®šï¼‰:å½“å‰ç­–ç•¥æ¨¡å‹ï¼ˆé€šè¿‡ä¼˜åŒ–æ›´æ–°ï¼‰:è¾“å…¥ q æ¡ä»¶ä¸‹ç­–ç•¥ç”Ÿæˆçš„è¾“å‡ºåˆ†å¸ƒ:è¾“å‡ºåºåˆ— oiâ€‹
çš„é•¿åº¦ï¼ˆtokenæ•°é‡ï¼‰:åºåˆ—ç”Ÿæˆçš„æ—¶é—´æ­¥ï¼ˆç¬¬ t
ä¸ªtokenä½ç½®ï¼‰:ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆåŸºäºç»„å†…å¥–åŠ±å·®å€¼è®¡ç®—ï¼‰:è£å‰ªèŒƒå›´å‚æ•°ï¼ˆé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼‰:KLæ•£åº¦æ­£åˆ™åŒ–ç³»æ•°ï¼ˆæ§åˆ¶ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹åç¦»ï¼‰:å½“å‰ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹çš„KLæ•£åº¦:å‚è€ƒæ¨¡å‹ï¼ˆå¦‚åˆå§‹SFTæ¨¡å‹ï¼‰â€‹  
å…¶ä¸­â€‹ä¸¤ä¸ªè¶…å‚æ•°ï¼šğœ€æ˜¯PPOæœºåˆ¶ä¸­çš„è£å‰ªèŒƒå›´å‚æ•°ï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼›ğ›½æ˜¯KLæ•£åº¦æ­£åˆ™åŒ–ç³»æ•°ï¼Œæ§åˆ¶å½“å‰ç­–ç•¥ä¸å‚è€ƒæ¨¡å‹ Ï€ r e f Ï€_{ref}
Ï€refâ€‹çš„åç¦»ç¨‹åº¦ã€‚

è¿™é‡Œçš„KLæ•£åº¦ä¸ä¼ ç»Ÿçš„KLæ•£åº¦è®¡ç®—æ–¹å¼ä¸åŒï¼Œä½¿ç”¨Schulmanæå‡ºçš„æ— åä¼°è®¡ï¼Œç¡®ä¿ç»“æœæ˜¯æ­£æ•°ï¼š  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e9c464e2662f42d9971d9d487ee0ce26.png)  
å¼ä¸­ o < t o_{<t} o<tâ€‹è¡¨ç¤ºåœ¨æ—¶é—´æ­¥ t ä¹‹å‰çš„æ‰€æœ‰è§‚æµ‹ï¼ˆæˆ–è¾“å‡ºï¼‰åºåˆ— ã€‚

### GRPOç®—æ³•çš„ä¼ªç è¡¨ç¤º

DeepSeekMathè®ºæ–‡é‡Œç»™å‡ºäº†GRPOè¿­ä»£ä¼˜åŒ–çš„æ­¥éª¤ï¼š  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/8e060d26b3bf41b598f753f55e451bd1.png)

åŸºäºä¸Šè¿°æµç¨‹ï¼ŒGRPOç®—æ³•çš„ä¼ªç å¯è¡¨ç¤ºä¸ºï¼š

    
    
    Input: 
    - initial_policy: Starting model to be trained
    - reward_function: Function that evaluates outputs
    - training_prompts: Set of training examples
    - group_size: Number of outputs per prompt (typically 4-16)
    
    Algorithm GRPO:
    1. For each training iteration:
       a. Set reference_policy = initial_policy (snapshot current policy)
       b. For each prompt in batch:
          i. Generate group_size different outputs using initial_policy
          ii. Compute rewards for each output using reward_function
          iii. Normalize rewards within group:
               normalized_advantage = (reward - mean(rewards)) / std(rewards)
          iv. Update policy by maximizing the clipped ratio:
              min(prob_ratio * normalized_advantage, 
                  clip(prob_ratio, 1-epsilon, 1+epsilon) * normalized_advantage)
              - kl_weight * KL(initial_policy || reference_policy)
              
              where prob_ratio is current_prob / reference_prob
    
    Output: Optimized policy model
    

### GRPOç®—æ³•çš„å±€é™ä¸æŒ‘æˆ˜

GRPOç®—æ³•åœ¨å®è·µä¸­ä¹Ÿé¢ä¸´ä¸€äº›æŒ‘æˆ˜ï¼š

  1. ç”Ÿæˆæˆæœ¬ã€‚ä¸ºæ¯ä¸ªæç¤ºè¯ç”Ÿæˆå¤šä¸ªè¡¥å…¨ï¼ˆ4-16ä¸ªï¼‰ç›¸æ¯”åªç”Ÿæˆä¸€ä¸ªæˆ–ä¸¤ä¸ªè¡¥å…¨çš„æ–¹æ³•ï¼Œæ˜¾è‘—å¢åŠ äº†è®¡ç®—éœ€æ±‚ã€‚
  2. æ‰¹é‡å¤§å°é™åˆ¶ã€‚éœ€è¦ä¸€èµ·å¤„ç†ä¸€ç»„è¡¥å…¨ï¼Œè¿™å¯èƒ½ä¼šé™åˆ¶æœ‰æ•ˆçš„æ‰¹é‡å¤§å°ï¼Œå¢åŠ è®­ç»ƒè¿‡ç¨‹çš„å¤æ‚æ€§ï¼Œå¹¶å¯èƒ½å‡ç¼“è®­ç»ƒé€Ÿåº¦ã€‚
  3. å¥–åŠ±å‡½æ•°çš„è®¾è®¡ã€‚è®­ç»ƒè´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºç²¾å¿ƒè®¾è®¡çš„å¥–åŠ±å‡½æ•°ã€‚è®¾è®¡ä¸ä½³çš„å¥–åŠ±å¯èƒ½å¯¼è‡´æ„å¤–è¡Œä¸ºæˆ–ä¼˜åŒ–é”™è¯¯çš„ç›®æ ‡ã€‚
  4. ç»„å¤§å°çš„æƒè¡¡ã€‚é€‰æ‹©æœ€ä¼˜çš„ç»„å¤§å°éœ€è¦åœ¨è§£å†³æ–¹æ¡ˆçš„å¤šæ ·æ€§ä¸è®¡ç®—æˆæœ¬ä¹‹é—´è¿›è¡Œå¹³è¡¡ã€‚ç»„å†…æ ·æœ¬å¤ªå°‘å¯èƒ½æ— æ³•æä¾›è¶³å¤Ÿçš„å¤šæ ·æ€§ï¼Œè€Œæ ·æœ¬å¤ªå¤šåˆ™ä¼šå¢åŠ è®­ç»ƒæ—¶é—´å’Œèµ„æºéœ€æ±‚ã€‚
  5. KLæ•£åº¦è°ƒå‚ã€‚æ‰¾åˆ°åˆé€‚çš„KLæ•£åº¦æƒ©ç½šå¹³è¡¡éœ€è¦è°¨æ…è°ƒæ•´â€”â€”è¿‡é«˜ä¼šå¯¼è‡´æ¨¡å‹æ— æ³•æœ‰æ•ˆå­¦ä¹ ï¼Œè¿‡ä½åˆ™å¯èƒ½ä½¿å…¶åç¦»åˆå§‹èƒ½åŠ›è¿‡è¿œã€‚

* * *

## ä»£ç å®ç°

### GRPOè®­ç»ƒå‚æ•°é…ç½®

TRLåº“ä¸­å°†GRPOç®—æ³•å°è£…ä¸º`GRPOConfig`å‚æ•°é…ç½®å™¨å’Œ`GRPOTrainer`è®­ç»ƒå™¨ã€‚  
è¿™é‡Œå‚è€ƒTRLåº“çš„æºç ï¼Œç»™å‡ºç®€åŒ–ç‰ˆçš„ä»£ç è§£è¯»ã€‚  
å°†GRPOè®­ç»ƒæ‰€éœ€å‚æ•°å°è£…æˆ GRPOConfigæ•°æ®ç±»ã€‚ç”¨@dataclassè£…é¥°å™¨å°†
GRPOConfigå®šä¹‰ä¸ºä¸€ä¸ªæ•°æ®ç±»ï¼Œå¹¶é€šè¿‡fieldå‡½æ•°ä¸ºç±»ä¸­çš„å­—æ®µæŒ‡å®šé»˜è®¤å€¼ã€é»˜è®¤å·¥å‚å‡½æ•°ã€å…ƒæ•°æ®ç­‰ã€‚  
åŸä»£ç è¿‡é•¿ï¼Œè¿™é‡Œåªè´´å‡ºæ§åˆ¶GRPOè®­ç»ƒçš„å‚æ•°ï¼š

    
    
    @dataclass
    class GRPOConfig(TrainingArguments):
    # Parameters that control the training
        learning_rate: float = field(
            default=1e-6,
            metadata={
                "help": "Initial learning rate for `AdamW` optimizer. The default value replaces that of "
                "`transformers.TrainingArguments`."
            },
        )
        beta: float = field(
            default=0.04,
            metadata={
                "help": "KL coefficient. If `0.0`, the reference model is not loaded, reducing memory usage and improving "
                "training speed, but may be numerically unstable for long training runs."
            },
        ) # KLæ•£åº¦éƒ¨åˆ†çš„è¶…å‚æ•°$\beta$,æ§åˆ¶KLæ•£åº¦æƒ©ç½šé¡¹çš„å¤§å°
        num_iterations: int = field(
            default=1,
            metadata={"help": "Number of iterations per batch (denoted as Î¼ in the algorithm)."},
        )
         epsilon: float = field(
            default=0.2,
            metadata={"help": "Epsilon value for clipping."},
        ) # clipéƒ¨åˆ†è¶…å‚æ•°ï¼Œæ§åˆ¶è£å‰ªçš„èŒƒå›´
    

æ§åˆ¶æ¯ä¸ªå¥–åŠ±å‡½æ•°çš„æƒé‡å æ¯”çš„å‚æ•°ï¼š

    
    
    reward_weights: Optional[list[float]] = field(
            default=None,
            metadata={
                "help": "Weights for each reward function. Must match the number of reward functions. If `None`, all "
                "rewards are weighted equally with weight `1.0`."
            },
        ) # æ¯ä¸ªå¥–åŠ±å‡½æ•°çš„æƒé‡å æ¯”
    

TR-DPOè®ºæ–‡ä¸­æå‡ºçš„**æ§åˆ¶å‚è€ƒæ¨¡å‹åŠ¨æ€æ›´æ–°** çš„ä¸‰ä¸ªå‚æ•°,
å½“å‰ç­–ç•¥æ¨¡å‹ä¸æ—§å‚è€ƒæ¨¡å‹çš„æ··åˆæ¯”ä¾‹ã€æ˜¯å¦å¯ç”¨å‚è€ƒæ¨¡å‹ä¸å½“å‰ç­–ç•¥æ¨¡å‹çš„åŒæ­¥æœºåˆ¶ã€å‚è€ƒæ¨¡å‹ä¸å½“å‰ç­–ç•¥æ¨¡å‹åŒæ­¥çš„é¢‘ç‡:

    
    
    ## >TR-DPOè®ºæ–‡ä¸­æå‡ºçš„æ§åˆ¶å‚è€ƒæ¨¡å‹ä¸å½“å‰ç­–ç•¥æ¨¡å‹åŒæ­¥æœºåˆ¶ä»¥åŠ¨æ€æ›´æ–°çš„ä¸‰ä¸ªå‚æ•°
        # æ˜¯å¦å¯ç”¨å‚è€ƒæ¨¡å‹ä¸å½“å‰ç­–ç•¥æ¨¡å‹çš„åŒæ­¥æœºåˆ¶
        sync_ref_model: bool = field(
            default=False,
            metadata={
                "help": "Whether to synchronize the reference model with the active model every `ref_model_sync_steps` "
                "steps, using the `ref_model_mixup_alpha` parameter."
            },
        ) 
        # æ§åˆ¶å‚è€ƒæ¨¡å‹æ›´æ–°æ—¶ï¼Œå½“å‰ç­–ç•¥æ¨¡å‹ä¸æ—§å‚è€ƒæ¨¡å‹çš„æ··åˆæ¯”ä¾‹
        # åŠ æƒå› å­alphaæ§åˆ¶è½¯æ›´æ–°ï¼ˆsoft updateï¼‰ï¼›aplha=1æ—¶æ›´æ–°æ–¹æ³•å˜ä¸ºç¡¬æ›´æ–°ï¼ˆhard updateï¼‰ï¼Œå³å°†å‚è€ƒç­–ç•¥æ¨¡å‹æ›¿æ¢ä¸ºå½“å‰ç­–ç•¥æ¨¡å‹
        ref_model_mixup_alpha: float = field(
            default=0.6,
            metadata={
                "help": "Î± parameter from the TR-DPO paper, which controls the mix between the current policy and the "
                "previous reference policy during updates. The reference policy is updated according to the equation: "
                "`Ï€_ref = Î± * Ï€_Î¸ + (1 - Î±) * Ï€_ref_prev`. To use this parameter, you must set `sync_ref_model=True`."
            },
        )
        # æ§åˆ¶å‚è€ƒæ¨¡å‹ä¸å½“å‰ç­–ç•¥æ¨¡å‹åŒæ­¥çš„é¢‘ç‡
        # æ¯éš” ref_model_sync_steps æ­¥ï¼Œå‚è€ƒæ¨¡å‹ä¼šæ ¹æ® ref_model_mixup_alpha çš„è§„åˆ™ä¸å½“å‰ç­–ç•¥æ¨¡å‹è¿›è¡ŒåŒæ­¥æ›´æ–°ã€‚
        ref_model_sync_steps: int = field(
            default=512,
            metadata={
                "help": "Ï„ parameter from the TR-DPO paper, which determines how frequently the current policy is "
                "synchronized with the reference policy. To use this parameter, you must set `sync_ref_model=True`."
            },
        )
    

### GRPOè®­ç»ƒå™¨

å°†GRPOè®­ç»ƒè¿‡ç¨‹å°è£…ä¸ºä¸€ä¸ª`Trainer`çš„ä¸€ä¸ªå­ç±»ã€‚

    
    
    class GRPOtrainer(Trainer):
        def __init__(
                self,
            model,
            reward_funcs,
            args,
            train_dataset,
            eval_dataset,
            processing_class,
            reward_processing_classes,
            callbacks,
            optimizers,
            peft_config,
        ):
             # Training arguments
            self.max_prompt_length = args.max_prompt_length
            self.max_completion_length = args.max_completion_length  # = |o_i| in the GRPO paper
            self.num_generations = args.num_generations  # = G in the GRPO paper
    
             # Multi-step
            self.num_iterations = args.num_iterations  # = ğœ‡ in the GRPO paper
            self.epsilon = args.epsilon # $\epsilon$è¶…å‚æ•°ç”¨äºæ¢¯åº¦clip
            # Tracks the number of iterations (forward + backward passes), including those within a gradient accumulation cycle.
            self._step = 0
    
            self._buffered_inputs = [None] * args.gradient_accumulation_steps
             # Initialize the metrics
            self._metrics = {"train": defaultdict(list), "eval": defaultdict(list)}
            self.log_completions = args.log_completions
    
            super().__init__(
                model=model,
                args=args,
                data_collator=data_collator,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                processing_class=processing_class,
                callbacks=callbacks,
                optimizers=optimizers,
            )
    
            self.generation_config = GenerationConfig(
                max_new_tokens=self.max_completion_length,
                do_sample=True,
                pad_token_id=processing_class.pad_token_id,
                temperature=args.temperature,
                top_p=args.top_p,
                top_k=args.top_k,
                min_p=args.min_p,
                repetition_penalty=args.repetition_penalty,
            )
    

#### tokenå¯¹æ•°æ¦‚ç‡è®¡ç®—

è®¡ç®—æ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡,ä»¥æ§åˆ¶æ¨¡å‹åœ¨è®­ç»ƒä¸­çš„ç­–ç•¥æ›´æ–°ï¼š

    
    
    # Get the per-token log probabilities for the completions for the model and the reference model
        @profiling_decorator # æ€§èƒ½åˆ†æ
        def _get_per_token_logps(self, model, input_ids, attention_mask, logits_to_keep):
            # We add 1 to `logits_to_keep` because the last logits of the sequence is later excluded
            logits = model(input_ids=input_ids, attention_mask=attention_mask, logits_to_keep=logits_to_keep + 1).logits
            logits = logits[:, :-1, :]  # (B, L-1, V), æ’é™¤æœ€åä¸€ä¸ª logit: å¯¹åº”ä¸‹ä¸€ä¸ªtokençš„é¢„æµ‹
    
            input_ids = input_ids[:, -logits_to_keep:]
            logits = logits[:, -logits_to_keep:]
            return selective_log_softmax(logits, input_ids)  #  è®¡ç®—æ¯ä¸ªè¾“å…¥tokençš„å¯¹æ•°æ¦‚ç‡
    

#### å¥–åŠ±çŸ©é˜µè®¡ç®—

åˆå§‹åŒ–å¥–åŠ±çŸ©é˜µåï¼Œéå†æ‰€æœ‰é¢„å®šä¹‰çš„å¥–åŠ±å‡½æ•°ï¼ˆå¯çµæ´»å®šä¹‰ä¸ºpytorchæ¨¡å‹æˆ–æ™®é€špythonå‡½æ•°ï¼‰ï¼Œåˆ†åˆ«è®¡ç®—å¥–åŠ±å€¼åæ›´æ–°å¥–åŠ±çŸ©é˜µï¼š

    
    
     rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device) # åˆå§‹åŒ–å¥–åŠ±çŸ©é˜µ
            for i, (reward_func, reward_processing_class) in enumerate(
                zip(self.reward_funcs, self.reward_processing_classes) # éå†æ‰€æœ‰çš„å¥–åŠ±å‡½æ•°
            ):
                if isinstance(reward_func, nn.Module):  # Module instead of PretrainedModel for compat with compiled models
                    reward_func_name = f"reward {reward_func.config._name_or_path.split('/')[-1]}"
                else:
                    reward_func_name = reward_func.__name__
                with profiling_context(self, reward_func_name):
                    if isinstance( # åŸºäºpytorchæ¨¡å‹è®¡ç®—å¥–åŠ±å€¼
                        reward_func, nn.Module 
                    ):  # Module instead of PretrainedModel for compat with compiled models
                        if is_conversational(inputs[0]):
                            messages = [{"messages": p + c} for p, c in zip(prompts, completions)]
                            texts = [apply_chat_template(x, reward_processing_class)["text"] for x in messages]
                        else:
                            texts = [p + c for p, c in zip(prompts, completions)]
                        reward_inputs = reward_processing_class(
                            texts, return_tensors="pt", padding=True, padding_side="right", add_special_tokens=False
                        )
                        reward_inputs = super()._prepare_inputs(reward_inputs)
                        with torch.inference_mode():
                            rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, 0]  # Shape (B*G,)
                    else: # åŸºäºpythonå‡½æ•°è®¡ç®—å¥–åŠ±å€¼
                        # Repeat all input columns (but "prompt" and "completion") to match the number of generations
                        keys = [key for key in inputs[0] if key not in ["prompt", "completion"]]
                        reward_kwargs = {key: [example[key] for example in inputs] for key in keys}
                        output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)
                        rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)
    
    

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/0e8f6705c84447aba6e923efb43e5701.png)

#### ç»„å†…ä¼˜åŠ¿è®¡ç®—

æ ¹æ®ç»„å†…è§£ç­”æ•°(`num_generations`)ï¼Œè®¡ç®—ç»„å†…ä¼˜åŠ¿ï¼š

    
    
    		rewards_per_func = gather(rewards_per_func)
    
            # Apply weights to each reward function's output and sum
            rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).sum(dim=1)
    
            # Compute grouped-wise rewards
            mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
            std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1)
    
            # Normalize the rewards to compute the advantages
            mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
            std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
            advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)
    
    

#### è®¡ç®—æŸå¤±å‡½æ•°

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/5d2653ac5700471e831a5a4969d7ba38.png)

æ ¹æ®deepseek mathè®ºæ–‡ä¸­å…¬å¼(4), è®¡ç®— D K L D_{KL} DKLâ€‹çš„æ— åä¼°è®¡ï¼š

    
    
     # è®¡ç®—å‚è€ƒæ¨¡å‹ä¸å½“å‰æ¨¡å‹ä¹‹é—´çš„KLæ•£åº¦
            if self.beta != 0.0: # å½“KLæ•£åº¦æ­£åˆ™é¡¹çš„å‚æ•°$beta$ä¸ä¸º0æ—¶
                ref_per_token_logps = inputs["ref_per_token_logps"]
                per_token_kl = (
                    torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
                ) # KLæ•£åº¦çš„æ— åä¼°è®¡ï¼Œ,deepseek mathè®ºæ–‡ä¸­å…¬å¼(4)
    
    

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1f1ee8515461400aba7ec26a30d2ce11.png)

æ ¹æ®deepseek math è®ºæ–‡ä¸­çš„å…¬å¼(3),è®¡ç®—æŸå¤±å‡½æ•°ï¼š

    
    
     # Compute the loss
            advantages = inputs["advantages"]
           
            old_per_token_logps = inputs["old_per_token_logps"] if self.num_iterations > 1 else per_token_logps.detach()
            coef_1 = torch.exp(per_token_logps - old_per_token_logps) # æ–°æ—§æ¨¡å‹tokenæ¦‚ç‡çš„æ¯”å€¼(å…ˆå–logå†å–æŒ‡æ•°ä¾¿äºè®¡ç®—)
            coef_2 = torch.clamp(coef_1, 1 - self.epsilon, 1 + self.epsilon) #clipæˆªæ–­éƒ¨åˆ†
            per_token_loss1 = coef_1 * advantages.unsqueeze(1) # æœªæˆªæ–­çš„æ¦‚ç‡æ¯”å€¼è®¡ç®—çš„æŸå¤±
            per_token_loss2 = coef_2 * advantages.unsqueeze(1) # æˆªæ–­çš„æ¦‚ç‡æ¯”å€¼è®¡ç®—çš„æŸå¤±
            per_token_loss = -torch.min(per_token_loss1, per_token_loss2) # æŸå¤±éƒ¨åˆ†è®¡ç®—ï¼Œæœ€å°åŒ–æŸå¤±(æœ€å¤§åŒ–å¥–åŠ±)
            if self.beta != 0.0:
                per_token_loss = per_token_loss + self.beta * per_token_kl # deepseek math è®ºæ–‡ä¸­çš„å…¬å¼(3)ï¼ŒGRPOç›®æ ‡å‡½æ•°
            loss = (per_token_loss * completion_mask).sum() / completion_mask.sum()
    
    

è®°å½•KLæ•£åº¦å¹³å‡å€¼ï¼š

    
    
     # è®°å½•KLæ•£åº¦çš„å¹³å‡å€¼
            if self.beta != 0.0:
                mean_kl = (per_token_kl * completion_mask).sum() / completion_mask.sum()
                self._metrics[mode]["kl"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())
    

è®¡ç®—æˆªæ–­æ¯”ä¾‹(clip ratio)ï¼š

    
    
     # è®¡ç®—æˆªæ–­æ¯”ä¾‹
            is_clipped = (per_token_loss1 < per_token_loss2).float()
            clip_ratio = (is_clipped * completion_mask).sum() / completion_mask.sum() 
            self._metrics[mode]["clip_ratio"].append(self.accelerator.gather_for_metrics(clip_ratio).mean().item())
    

* * *

## æ¡ˆä¾‹å®æˆ˜ï¼šä½¿ç”¨TRLåº“å®ç°GRPOè®­ç»ƒæ–‡æœ¬ç”Ÿæˆæ¨¡å‹

è¿™é‡Œç»™å‡ºä¸€ä¸ªç”¨GRPOç®—æ³•ï¼Œåœ¨smoltldræ•°æ®é›†ä¸Šè®­ç»ƒSmolLM135Mæ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„demoä¾‹å­ã€‚ä½¿ç”¨trlã€peftåº“å®ç°GRPOå’ŒLORAå¾®è°ƒã€‚  
**æ•°æ®é›†ä»‹ç»**  
mlabonne/smoltldr æ˜¯ä¸€ä¸ªåŒ…å«çŸ­ç¯‡å°è¯´åˆ—è¡¨çš„æ•°æ®é›†ï¼Œç”±ç”¨æˆ· mlabonne åœ¨ Hugging Face
ä¸Šåˆ›å»ºå¹¶ç»´æŠ¤ã€‚è¯¥æ•°æ®é›†ä¸»è¦ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œä¾‹å¦‚æ–‡æœ¬ç”Ÿæˆã€æ•…äº‹åˆ›ä½œç­‰ã€‚æ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬é€šå¸¸æ˜¯ä¸€ä¸ªçŸ­ç¯‡æ•…äº‹ï¼Œå†…å®¹å¯èƒ½æ¶µç›–å¤šç§ä¸»é¢˜å’Œé£æ ¼ã€‚è¿™äº›æ•…äº‹ç»è¿‡æ¸…æ´—å’Œæ ¼å¼åŒ–ï¼Œé€‚åˆç”¨äºè®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚SmolLM-135Måˆ™æ˜¯ç”¨äºæ–‡æœ¬ç”Ÿæˆçš„å°æ¨¡å‹ã€‚

**æ•°æ®å’Œæ¨¡å‹åŠ è½½**

    
    
    import torch
    import wandb
    from datasets import load_dataset
    from peft import LoraConfig,get_peft_model
    from transformers import AutoModelForCausalLM,AutoTokenizer
    from trl import GRPOConfig,GRPOTrainer
    
    
    wandb.login() # ç™»å½•wandbï¼Œè¾“å…¥api keyï¼Œä¿å­˜è®­ç»ƒç»“æœåˆ°wandb
    
    # åŠ è½½huggingfaceä¸Šçš„æ•°æ®é›†
    dataset = load_dataset("mlabonne/smoltldr")
    
    # åŠ è½½huggingfaceä¸Šçš„æ¨¡å‹
    model_id = "HuggingFaceTB/SmolLM-135M-Instruct"
    model = AutoModelForCausalLM(
        model_id,
        torch_dtype = "auto",
        device_map = "auto",
        # attn_implementation = "flash_attention_2",
        attn_implementation = "eager",# GPUä¸æ”¯æŒflashattentionæ—¶æ”¹ç”¨æ ‡å‡†æ³¨æ„åŠ›
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    

**LORAå¾®è°ƒé…ç½®**

    
    
    # åŠ è½½PEFTåº“ä¸­çš„loraé…ç½®
    lora_config = LoraConfig(
        task_type = "CAUSAL_LM", # ç”Ÿæˆå¼ä»»åŠ¡
        r=16, # ç§©ä¸º16
        lora_alpha=32, # ä½ç§©çŸ©é˜µæƒé‡è´¡çŒ®çš„ç¼©æ”¾å› å­è®¾ä¸º32
        target_modules = "all-linear",# æ¨¡å‹ä¸­çš„çº¿æ€§å±‚åº”ç”¨lora
    )
    model = get_peft_model(model,lora_config) # å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°ï¼Œæ ¹æ®lora_configåœ¨æ¨¡å‹ä¸­æ·»åŠ ä½ç§©çŸ©é˜µ
    print(model.print_trainable_parameters()) # æ‰“å°æ¨¡å‹ä¸­å¯è®­ç»ƒå‚æ•°çš„æ•°é‡åŠå æ€»å‚æ•°çš„æ¯”ä¾‹
    
    

**å®šä¹‰å¥–åŠ±å‡½æ•°**

    
    
    # å®šä¹‰å¥–åŠ±å‡½æ•°
    def reward_len(completions,**kwargs):
        return [-abs(50-len(completion)) for completion in completions]
    
    

**GRPOè®­ç»ƒå‚æ•°è®¾ç½®**

    
    
    # GRPOè®­ç»ƒå‚æ•°é…ç½®
    training_args = GRPOConfig(
        output_dir="GRPO",
        run_name="GRPO_experi_0308_01",# wandbä¿å­˜çš„å®éªŒåç§°
        learning_rate=2e-5,
        per_device_train_batch_size=4,# æ‰¹é‡å¤§å°è®¾å°ä¸€ç‚¹å‡å°‘æ˜¾å­˜å ç”¨
        gradient_accumulation_steps=2, # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        max_prompt_length=512,
        max_completion_length=96,
        num_generations=4,# GRPOæ¯ç»„ç”Ÿæˆçš„è§£ç­”æ•°
        optim="adamW_8bit",
        num_train_epochs=1,# è®­ç»ƒæ•°æ®é›†è®­ç»ƒçš„æ€»è½®æ•°
        bf16=True,
        report_to=["wandb"],
        remove_unused_columns=False,# ä¸ç§»é™¤æ•°æ®é›†ä¸­æœªä½¿ç”¨çš„åˆ—
        logging_steps=1,# æ¯éš”ä¸€æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    )
    # è®¾ç½®è®­ç»ƒå™¨
    trainer = GRPOTrainer(
        model=model,
        reward_funcs = [reward_len],# è‡ªå®šä¹‰çš„å¥–åŠ±å‡½æ•°
        agrs=training_args,# GRPOçš„è®­ç»ƒé…ç½®
        train_dataset = dataset["train"],
    )
    
    

**è®­ç»ƒæ¨¡å‹**

    
    
    # è®­ç»ƒæ¨¡å‹
    wandb.init(project="GRPO") # åˆå§‹åŒ–wandbæ—¥å¿—ç¯å¢ƒ
    trainer.train() # å¼€å§‹è®­ç»ƒ
    
    

**ä¸Šä¼ è®­ç»ƒå®Œæˆçš„æ¨¡å‹å‚æ•°**

    
    
    # ä¿å­˜æ¨¡å‹å‚æ•°ï¼Œä¸Šä¼ åˆ°huggingface hub
    merged_model = trainer.model.merge_and_unload() # åˆå¹¶loraæƒé‡å’Œé¢„è®­ç»ƒæƒé‡
    merged_model.push_to_hub("<your_username/your_modelname>",private=False) # æ¨¡å‹å…¬å¼€å¯è§
    
    

**ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ**

    
    
    prompt = """
    # A long document about the Cat
    
    The cat (Felis catus), also referred to as the domestic cat or house cat, is a small
    domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.
    Advances in archaeology and genetics have shown that the domestication of the cat occurred
    in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges
    freely as a feral cat avoiding human contact. It is valued by humans for companionship and
    its ability to kill vermin. Its retractable claws are adapted to killing small prey species
    such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,
    and its night vision and sense of smell are well developed. It is a social species,
    but a solitary hunter and a crepuscular predator. Cat communication includes
    vocalizationsâ€”including meowing, purring, trilling, hissing, growling, and gruntingâ€”as
    well as body language. It can hear sounds too faint or too high in frequency for human ears,
    such as those made by small mammals. It secretes and perceives pheromones.
    """
    messages = [
        {"role":"user","content":prompt},
    ]
    
    from transformers import pipeline
    generator = pipeline("text-generation",model="<your_username/your_modelname>")
    generate_kwargs = {
        "max_new_tokens": 256,
        "do_sample":True, # å¯ç”¨é‡‡æ ·ç”Ÿæˆæ¨¡å¼
        "temperature":0.5, 
        "min_p":0.1,
    }
    generated_text = generator(messages,generate_kwargs=generate_kwargs)
    print(generated_text)
    

**è®­ç»ƒç»“æœåˆ†æ**  
éšç€æ¨¡å‹çš„å­¦ä¹ ï¼Œå¥–åŠ±å‡½æ•°çš„å¥–åŠ±å€¼é€æ¸æ¥è¿‘0ã€‚è¿™è¡¨æ˜æ¨¡å‹æ­£åœ¨å­¦ä¹ ç”Ÿæˆæ­£ç¡®é•¿åº¦çš„æ–‡æœ¬ã€‚  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/f306dd6a38eb43a7b866b16a20752da3.png)  
åœ¨GRPOä¸­ï¼ŒæŸå¤±å‡½æ•°çš„åˆå§‹å€¼ä¸ºé›¶ï¼Œç„¶ååœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¢åŠ ã€‚GRPOä¸­çš„æŸå¤±ä¸KLæ•£åº¦ï¼ˆç›¸å¯¹äºåŸå§‹ç­–ç•¥çš„ä¸Šé™ï¼‰æˆæ­£æ¯”ã€‚éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ¨¡å‹å­¦ä¼šäº†ç”Ÿæˆæ›´å¥½åœ°ç¬¦åˆå¥–åŠ±å‡½æ•°çš„æ–‡æœ¬ï¼Œå¯¼è‡´å®ƒä¸åˆå§‹ç­–ç•¥çš„åå·®è¶Šæ¥è¶Šå¤§ã€‚è¿™ç§å¢åŠ çš„åå·®åæ˜ åœ¨ä¸Šå‡çš„æŸå¤±å€¼ä¸­ï¼Œå®é™…ä¸Šè¡¨æ˜æ¨¡å‹æ­£åœ¨æˆåŠŸåœ°é€‚åº”ä»¥ä¼˜åŒ–å¥–åŠ±å‡½æ•°ã€‚  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/0960cfed54f04d9a94c7d9fa5349c73c.png)

* * *

## æ¨èé˜…è¯»

  * æŠ±æŠ±è„¸25å¹´3æœˆæ›´æ–°ä¸­çš„æ·±åº¦æ¨ç†è¯¾ç¨‹ï¼š https://huggingface.co/reasoning-course
  * DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Modelsï¼š https://arxiv.org/pdf/2402.03300

## REF

  1. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Modelsï¼š https://arxiv.org/pdf/2402.03300
  2. https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py#L98
  3. https://github.com/huggingface/open-r1/blob/main/src/open_r1/grpo.py
  4. https://open-r1.com/#:~:text=Open%20R1%20is%20an%20open-source%20reproduction%20of%20DeepSeek-R1%2C,MIT%20license%2C%20though%20original%20training%20data%20remains%20proprietary.
  5. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning https://arxiv.org/abs/2501.12948
  6. https://huggingface.co/learn/nlp-course/en/chapter12/1?fw=pt



