---
layout: post
title: "自监督预训练算法核心原理"
date: 2025-03-09 20:19:16 +0800
description: "本章我们深入探讨了自监督预训练算法的核心原理，详细解析了自监督学习范式，以及 MLM、CLM、PLM、DAE 和对比学习等经典算法。这些算法各有特点，但都巧妙地利用无标签数据自身提供的监督信号，学习到了通用的数据表示，为后续的下游任务奠定了坚实的基础。对比学习 (Contrastive Learning) 是近年来兴起的一种自监督学习方法，在图像、音频、文本等领域都取得了显著的成果。：在许多任务上，使用自监督预训练的模型能够显著提升性能，尤其是在标注数据稀缺的情况下。，然后利用这些伪标签训练模型。"
keywords: "自监督预训练算法核心原理"
categories: ['大模型科普']
tags: ['算法']
artid: "146137883"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146137883
    alt: "自监督预训练算法核心原理"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146137883
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146137883
cover: https://bing.ee123.net/img/rand?artid=146137883
image: https://bing.ee123.net/img/rand?artid=146137883
img: https://bing.ee123.net/img/rand?artid=146137883
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     自监督预训练算法核心原理
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2 style="text-align:justify">
     <strong>
      自监督预训练算法核心原理
     </strong>
    </h2>
    <h3 style="text-align:justify">
     <strong>
      引言
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     在人工智能的浪潮中，预训练算法已经成为自然语言处理 (NLP) 领域不可或缺的基石。它们如同强大的引擎，驱动着机器理解和生成人类语言的能力。在上一章，我们概述了预训练算法的重要性及其发展历程。本章，我们将深入探讨自监督预训练算法的核心原理，揭开其背后的奥秘。
    </p>
    <h3 style="text-align:justify">
     <strong>
      1.
     </strong>
     <strong>
      自监督学习 (Self-Supervised Learning) 范式详解
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     自监督学习 (Self-Supervised Learning, SSL) 是近年来备受瞩目的一种机器学习范式。它巧妙地利用数据自身提供的内在结构作为监督信号，从而在无需人工标注的情况下训练模型。这种范式尤其适用于海量无标签数据的场景，极大地扩展了模型学习的边界。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       核心思想
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     自监督学习的核心思想在于，
     <strong>
      <strong>
       从无标签数据中构建“伪标签”
      </strong>
     </strong>
     ，然后利用这些伪标签训练模型。模型学习的目标不再是预测人工设定的标签，而是
     <strong>
      <strong>
       预测数据自身被遮蔽或破坏的部分，或者学习数据不同部分之间的关联性。
      </strong>
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       范式详解
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     自监督学习通常遵循以下范式：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="283" src="https://i-blog.csdnimg.cn/direct/eae5f17cb3a14682b8f2d290f98c6569.png" width="830"/>
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        预任务 (Pretext Task) 设计
       </strong>
      </strong>
      ：这是自监督学习的关键步骤。预任务的设计需要巧妙地利用数据自身的特性，构建一个容易自动化标注，但又能够捕捉数据内在结构的任务。常见的预任务类型包括：
     </li>
    </ol>
    <ol>
     <li style="text-align:justify">
      上下文预测：例如，预测句子中被遮蔽的词语 (Masked Language Modeling, MLM) 或预测下一个词语 (Causal Language Modeling, CLM)。
     </li>
     <li style="text-align:justify">
      图像块预测：例如，预测图像中被遮蔽区域的内容，或者预测图像块之间的相对位置。
     </li>
     <li style="text-align:justify">
      时序预测：例如，预测视频的下一帧，或者预测音频的下一个时间步。
     </li>
     <li style="text-align:justify">
      对比学习：通过区分相似和不相似的数据对，学习数据的表示。
     </li>
    </ol>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        模型训练
       </strong>
      </strong>
      ： 使用预任务生成的伪标签，训练深度学习模型。模型在完成预任务的过程中，学习到通用的数据表示，这些表示能够捕捉数据中的重要特征和模式。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        下游任务微调 (Fine-tuning)
       </strong>
      </strong>
      ：将预训练好的模型迁移到下游的真实任务 (例如，文本分类、情感分析、目标检测等)。在下游任务中，通常只需要少量标注数据即可对模型进行微调，使其适应特定任务的需求。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       自监督学习的优势
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       无需人工标注
      </strong>
     </strong>
     ：极大地降低了数据标注成本，能够利用海量的无标签数据。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       学习通用表示
      </strong>
     </strong>
     ：预训练模型学习到的表示具有良好的泛化能力，能够迁移到各种下游任务。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       提升模型性能
      </strong>
     </strong>
     ：在许多任务上，使用自监督预训练的模型能够显著提升性能，尤其是在标注数据稀缺的情况下。
    </p>
    <h3 style="text-align:justify">
     <strong>
      2.
     </strong>
     <strong>
      Masked Language Modeling (MLM) 算法深度解析
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Masked Language Modeling (MLM)，中文译为掩码语言模型，是自监督学习中一种非常经典且有效的预训练任务。BERT (Bidirectional Encoder Representations from Transformers) 模型正是基于 MLM 算法而提出的，并取得了巨大的成功。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="451" src="https://i-blog.csdnimg.cn/direct/5cdd8c68b07f482f872ff5f172f5ddf9.png" width="831"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       算法原理
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     MLM 的核心思想非常简洁：随机遮蔽输入文本中的一部分词语，然后让模型预测被遮蔽的词语。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       具体步骤
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      掩码 (Masking)： 对于输入的文本序列，随机选择一部分词语 (通常是 15% 左右)，用特殊的 "[MASK]" 标记替换这些词语。
     </li>
     <li style="text-align:justify">
      模型预测： 将带有 "[MASK]" 标记的文本输入到模型中 (例如，Transformer 编码器)。模型需要根据上下文信息，预测被 "[MASK]" 标记替换的原始词语。
     </li>
     <li style="text-align:justify">
      损失函数： 使用交叉熵损失函数，衡量模型预测结果与真实被掩码词语之间的差异。模型训练的目标是最小化这个损失函数。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       示例
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     原始句子： "The quick brown fox jumps over the lazy dog."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     掩码后的句子： "The quick brown [MASK] jumps over the lazy dog."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     MLM 任务： 模型需要预测 "[MASK]" 位置的词语是 "fox"。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       MLM 的特点
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        双向上下文理解
       </strong>
      </strong>
      ：由于模型需要根据被掩码词语的左右两侧上下文信息进行预测，因此 MLM 能够学习到文本的双向上下文表示。这与传统的单向语言模型 (例如，CLM) 形成鲜明对比。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        适用于编码器结构
       </strong>
      </strong>
      ：MLM 更适合用于训练编码器结构的模型 (例如，Transformer 编码器)，因为编码器可以同时处理整个输入序列。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        BERT 的核心
       </strong>
      </strong>
      ：MLM 是 BERT 模型的核心预训练任务之一，BERT 通过 MLM 任务学习到了丰富的语言知识，并在各种 NLP 任务中取得了state-of-the-art 的结果。
     </li>
    </ol>
    <h3 style="text-align:justify">
     <strong>
      3.
     </strong>
     <strong>
      Causal Language Modeling (CLM) 算法深度解析
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Causal Language Modeling (CLM)，中文译为
     <strong>
      <strong>
       因果语言模型
      </strong>
     </strong>
     ，也称为
     <strong>
      <strong>
       自回归语言模型
      </strong>
     </strong>
     。与 MLM 不同，CLM 是一种
     <strong>
      <strong>
       单向
      </strong>
     </strong>
     的语言模型，它在预测下一个词语时，
     <strong>
      <strong>
       只能利用到当前词语之前的上下文信息
      </strong>
     </strong>
     。GPT (Generative Pre-trained Transformer) 系列模型正是基于 CLM 算法而构建的。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       算法原理
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     CLM 的核心思想是：
     <strong>
      <strong>
       根据上文预测下一个词语
      </strong>
     </strong>
     。 模型按照文本序列的顺序，逐词预测下一个可能出现的词语。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="356" src="https://i-blog.csdnimg.cn/direct/45c334549c9f4b9887ed75ecc98b2856.png" width="830"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       具体步骤
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        序列输入
       </strong>
      </strong>
      ：将文本序列逐词输入到模型中 (例如，Transformer 解码器)。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        单向预测
       </strong>
      </strong>
      ：模型在预测当前位置的词语时，只能利用到之前位置的词语信息，而不能看到之后位置的词语。这通过在模型结构中引入 Mask 机制来实现，防止模型“作弊”看到未来信息。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        损失函数
       </strong>
      </strong>
      ：使用交叉熵损失函数，衡量模型预测的下一个词语概率分布与真实下一个词语之间的差异。模型训练的目标是最大化预测下一个词语的准确率。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       示例
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     原始句子： "The quick brown fox jumps over the lazy dog."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     CLM 任务：
    </p>
    <ol>
     <li style="text-align:justify">
      输入 "The"，预测 "quick"
     </li>
     <li style="text-align:justify">
      输入 "The quick"，预测 "brown"
     </li>
     <li style="text-align:justify">
      输入 "The quick brown"，预测 "fox"
     </li>
     <li style="text-align:justify">
      ...
     </li>
     <li style="text-align:justify">
      输入 "The quick brown fox jumps over the lazy"，预测 "dog"
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       CLM 的特点
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        单向上下文理解
       </strong>
      </strong>
      ： CLM 只能学习到文本的单向上下文表示，即从左到右的依赖关系。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        适用于解码器结构
       </strong>
      </strong>
      ： CLM 更适合用于训练解码器结构的模型 (例如，Transformer 解码器)，因为解码器天然就是按照序列顺序逐词生成的。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        GPT 的核心
       </strong>
      </strong>
      ： CLM 是 GPT 模型的核心预训练任务，GPT 通过 CLM 任务学习到了强大的文本生成能力，并在文本生成、对话系统等领域表现出色。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        生成能力
       </strong>
      </strong>
      ： CLM 由于其自回归的特性，天然适合用于文本生成任务。模型可以从一个起始词语开始，不断预测下一个词语，直到生成完整的文本序列。
     </li>
    </ol>
    <h3 style="text-align:justify">
     <strong>
      4.
     </strong>
     <strong>
      Prefix Language Modeling (PLM) 与 Denoising Autoencoding (DAE) 算法
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     除了 MLM 和 CLM 之外，还有一些其他的自监督预训练算法，例如 Prefix Language Modeling (PLM) 和 Denoising Autoencoding (DAE)。
    </p>
    <h4 style="text-align:justify">
     <strong>
      4.1
     </strong>
     <strong>
      Prefix Language Modeling (PLM)
     </strong>
    </h4>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Prefix Language Modeling (PLM)，中文译为
     <strong>
      <strong>
       前缀语言模型
      </strong>
     </strong>
     ，是一种
     <strong>
      <strong>
       结合了 MLM 和 CLM 优点的预训练任务
      </strong>
     </strong>
     。UniLM (Unified Language Model) 模型采用了 PLM 算法。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       算法原理
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     PLM 的核心思想是：
     <strong>
      <strong>
       将输入文本分为前缀 (prefix) 部分和补全 (completion) 部分
      </strong>
     </strong>
     。 对于前缀部分，模型采用
     <strong>
      <strong>
       双向编码
      </strong>
     </strong>
     的方式进行处理；对于补全部分，模型采用
     <strong>
      <strong>
       单向自回归
      </strong>
     </strong>
     的方式进行生成。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="646" src="https://i-blog.csdnimg.cn/direct/f8dd84f045e543a98eaeaa91363e87b9.png" width="830"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       具体步骤
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        文本分割
       </strong>
      </strong>
      ：将输入文本随机分割为前缀部分和补全部分。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        前缀双向编码
       </strong>
      </strong>
      ：对于前缀部分，使用双向编码器 (例如，Transformer 编码器) 进行编码，学习前缀的双向上下文表示。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        补全单向生成
       </strong>
      </strong>
      ：对于补全部分，使用单向解码器 (例如，Transformer 解码器) 进行生成，解码器在生成每个词语时，可以利用到前缀的双向上下文表示以及补全部分已生成的内容。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        损失函数
       </strong>
      </strong>
      ：模型训练的目标是最大化补全部分文本序列的生成概率。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       示例
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       原始句子
      </strong>
     </strong>
     ： "Summarize the article: Natural language processing is a subfield of artificial intelligence."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       分割
      </strong>
     </strong>
     ： 前缀: "Summarize the article: Natural language processing is a subfield of" 补全: "artificial intelligence."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       PLM 任务
      </strong>
     </strong>
     ： 模型需要根据前缀部分，自回归地生成补全部分 "artificial intelligence."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       PLM 的特点
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        兼顾编码和生成
       </strong>
      </strong>
      ： PLM 既可以学习到文本的双向上下文表示 (通过前缀部分的双向编码)，又具备文本生成能力 (通过补全部分的单向生成)。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        适用于多种任务
       </strong>
      </strong>
      ： PLM 预训练的模型可以应用于多种 NLP 任务，例如，自然语言理解 (NLU) 任务 (通过利用前缀部分的编码表示) 和自然语言生成 (NLG) 任务 (通过利用补全部分的生成能力)。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        UniLM 的核心
       </strong>
      </strong>
      ： PLM 是 UniLM 模型的核心预训练任务，UniLM 通过 PLM 任务实现了在 NLU 和 NLG 任务上的统一建模。
     </li>
    </ol>
    <h4 style="text-align:justify">
     <strong>
      4.2 Denoising Autoencoding (DAE)
     </strong>
    </h4>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Denoising Autoencoding (DAE)，中文译为
     <strong>
      <strong>
       去噪自编码器
      </strong>
     </strong>
     ，是一种
     <strong>
      <strong>
       通用的自监督学习方法
      </strong>
     </strong>
     ，不仅可以应用于文本，也可以应用于图像、音频等多种数据类型。T5 (Text-to-Text Transfer Transformer) 模型采用了 DAE 算法。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       算法原理
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     DAE 的核心思想是：
     <strong>
      <strong>
       对输入数据进行加噪处理，然后让模型学习恢复原始的干净数据
      </strong>
     </strong>
     。 模型在去噪的过程中，学习到数据的鲁棒表示。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="536" src="https://i-blog.csdnimg.cn/direct/baaf8ac0fbc7462e81f73663c00f4b86.png" width="831"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     具体步骤 (以文本为例)：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        数据加噪
       </strong>
      </strong>
      ：对于输入的文本序列，采用各种噪声操作进行破坏，例如：
     </li>
    </ol>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        随机掩码 (Masking)
       </strong>
      </strong>
      ：类似于 MLM，随机遮蔽一部分词语。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        词语删除 (Deletion)
       </strong>
      </strong>
      ：随机删除一部分词语。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        词语替换 (Replacement)
       </strong>
      </strong>
      ：随机用其他词语替换一部分词语。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        文本段乱序 (Permutation)
       </strong>
      </strong>
      ：随机打乱文本段落的顺序。
     </li>
    </ol>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        模型重建
       </strong>
      </strong>
      ：将加噪后的文本输入到模型中 (例如，Transformer 编码器-解码器结构)。模型需要根据加噪后的文本，重建原始的干净文本。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        损失函数
       </strong>
      </strong>
      ：使用交叉熵损失函数，衡量模型重建的文本与原始干净文本之间的差异。模型训练的目标是最小化这个损失函数。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="477" src="https://i-blog.csdnimg.cn/direct/dd7813cfc2214d1d8be1a3dab9becaba.png" width="831"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       示例
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       原始句子
      </strong>
     </strong>
     ："Natural language processing is a subfield of artificial intelligence."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       加噪后的句子 (随机掩码)
      </strong>
     </strong>
     ："Natural language processing is a [MASK] of artificial intelligence."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       DAE 任务
      </strong>
     </strong>
     ：模型需要根据加噪后的句子，重建原始句子 "Natural language processing is a subfield of artificial intelligence."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       DAE 的特点
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        通用性强
       </strong>
      </strong>
      ： DAE 可以应用于多种数据类型，只需要设计合适的噪声操作和重建目标即可。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        鲁棒性学习
       </strong>
      </strong>
      ： DAE 迫使模型学习从噪声数据中恢复原始信息，从而增强模型的鲁棒性和泛化能力。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        T5 的核心
       </strong>
      </strong>
      ： DAE 是 T5 模型的核心预训练任务，T5 将各种 NLP 任务都统一建模为文本到文本的生成任务，并使用 DAE 进行预训练。
     </li>
    </ol>
    <h3 style="background-color:transparent; text-align:justify">
     <strong>
      5 对比学习 (Contrastive Learning) 在文本表示学习中的应用
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     对比学习 (Contrastive Learning) 是近年来兴起的一种自监督学习方法，在图像、音频、文本等领域都取得了显著的成果。对比学习的核心思想是
     <strong>
      <strong>
       通过区分相似和不相似的数据对，学习数据的表示。
      </strong>
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="387" src="https://i-blog.csdnimg.cn/direct/c8cd33c69e3c41b49497666513c54d24.png" width="831"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       算法原理
      </strong>
     </strong>
     ：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     对比学习的目标是将相似的数据样本在表示空间中拉近，将不相似的数据样本在表示空间中推远。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       具体步骤 (以文本表示学习为例)
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        正负样本构建
       </strong>
      </strong>
      ：对于每个样本 (锚点样本, anchor sample)，构建正样本 (positive sample) 和负样本 (negative samples)。
      <ol>
       <li style="text-align:justify">
        <strong>
         <strong>
          正样本
         </strong>
        </strong>
        ：与锚点样本语义相似的样本。在文本表示学习中，通常可以使用数据增强技术 (例如，随机插入、删除、替换词语) 对锚点样本进行变换，生成正样本。
       </li>
       <li style="text-align:justify">
        <strong>
         <strong>
          负样本
         </strong>
        </strong>
        ：与锚点样本语义不相似的样本。通常可以从同一批次 (batch) 的其他样本中选择负样本。
       </li>
      </ol>
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        表示学习
       </strong>
      </strong>
      ：使用编码器 (例如，Transformer 编码器) 将锚点样本、正样本和负样本编码到表示空间中。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        对比损失函数
       </strong>
      </strong>
      ：设计对比损失函数，例如 InfoNCE (Noise Contrastive Estimation) 损失函数。对比损失函数的目标是：
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        拉近正样本对
       </strong>
      </strong>
      ：使得锚点样本和正样本的表示在表示空间中尽可能接近。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        推远负样本对
       </strong>
      </strong>
      ：使得锚点样本和负样本的表示在表示空间中尽可能远离。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       示例 (SimCSE - Simple Contrastive Learning of Sentence Embeddings)：
      </strong>
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     SimCSE 是一种简单的对比学习方法，用于学习句子表示。
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        正样本
       </strong>
      </strong>
      ：对于每个句子，使用 Dropout 机制进行两次编码，得到两个不同的句子表示，将这两个表示视为正样本对。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        负样本
       </strong>
      </strong>
      ：同一批次 (batch) 中的其他句子作为负样本。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        对比损失函数
       </strong>
      </strong>
      ：使用 InfoNCE 损失函数，训练模型拉近同一个句子的两个表示，推远不同句子的表示。
     </li>
    </ol>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      <strong>
       对比学习的特点
      </strong>
     </strong>
     ：
    </p>
    <ol>
     <li style="text-align:justify">
      <strong>
       <strong>
        直接学习表示
       </strong>
      </strong>
      ： 对比学习直接学习数据的表示，而无需像 MLM 或 CLM 那样预测具体的词语。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        有效性高
       </strong>
      </strong>
      ： 对比学习在文本表示学习任务中表现出色，能够学习到高质量的句子和文本表示。
     </li>
     <li style="text-align:justify">
      <strong>
       <strong>
        SimCSE 等模型的基石
       </strong>
      </strong>
      ： 对比学习是 SimCSE 等先进文本表示模型的核心技术。
     </li>
    </ol>
    <h3 style="text-align:justify">
     <strong>
      总结
     </strong>
    </h3>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     本章我们深入探讨了自监督预训练算法的核心原理，详细解析了自监督学习范式，以及 MLM、CLM、PLM、DAE 和对比学习等经典算法。这些算法各有特点，但都巧妙地利用无标签数据自身提供的监督信号，学习到了通用的数据表示，为后续的下游任务奠定了坚实的基础。在下一章，我们将继续深入探讨预训练模型的训练技巧和实践应用。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     欢迎关注我的微信公众号
     <span style="color:#fe2c24">
      <strong>
       智语Bot
      </strong>
     </span>
     ，与我互动交流，共同学习进步！
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     参考资料
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding:
     <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1810&lt;/0&gt;.04805" rel="nofollow" title="https://arxiv.org/abs/1810.04805">
      https://arxiv.org/abs/1810.04805
     </a>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     GPT-3: Language Models are Few-Shot Learners:
     <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2005.14165" rel="nofollow" title="https://arxiv.org/abs/2005.14165">
      https://arxiv.org/abs/2005.14165
     </a>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     UniLM: Unified Language Model Pre-training for Natural Language Understanding and Generation:
     <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1905.03197" rel="nofollow" title="https://arxiv.org/abs/1905.03197">
      https://arxiv.org/abs/1905.03197
     </a>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer:
     <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/1910.10683" rel="nofollow" title="https://arxiv.org/abs/1910.10683">
      https://arxiv.org/abs/1910.10683
     </a>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     SimCSE: Simple Contrastive Learning of Sentence Embeddings:
     <a href="https://www.google.com/url?sa=E&amp;source=gmail&amp;q=https://arxiv.org/abs/2104.08821" rel="nofollow" title="https://arxiv.org/abs/2104.08821">
      https://arxiv.org/abs/2104.08821
     </a>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f5950656e675f47616f2f:61727469636c652f64657461696c732f313436313337383833" class_="artid" style="display:none">
 </p>
</div>


