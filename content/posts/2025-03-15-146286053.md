---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33353831323230352f:61727469636c652f64657461696c732f313436323836303533"
layout: post
title: "RSOneRec快手-生成式推荐模型"
date: 2025-03-15 22:05:58 +08:00
description: "本文提出了一种名为 OneRec 的统一生成式推荐框架，旨在替代传统的多阶段排序策略，通过一个端到端的生成模型直接生成推荐结果。OneRec 的主要贡献包括：编码器-解码器结构：采用稀疏混合专家（MoE）架构扩展模型容量，提升对用户兴趣的建模能力。会话式生成方法：与传统的逐点预测不同，OneRec 提出会话式生成方法，生成整个推荐列表，更好地捕捉上下文信息。迭代偏好对齐模块：结合直接偏好优化（DPO），通过奖励模型（RM）生成偏好数据，优化生成结果。实验表明，OneRec 在大规模工业数据集"
keywords: "【RS】OneRec快手-生成式推荐模型"
categories: ['推荐算法2', '多模态大模型', 'Llm']
tags: ['推荐系统', '大模型', 'Moe']
artid: "146286053"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146286053
    alt: "RSOneRec快手-生成式推荐模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146286053
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146286053
cover: https://bing.ee123.net/img/rand?artid=146286053
image: https://bing.ee123.net/img/rand?artid=146286053
img: https://bing.ee123.net/img/rand?artid=146286053
---

# 【RS】OneRec快手-生成式推荐模型

## note

本文提出了一种名为 OneRec 的统一生成式推荐框架，旨在替代传统的多阶段排序策略，通过一个端到端的生成模型直接生成推荐结果。OneRec 的主要贡献包括：

编码器-解码器结构：采用稀疏混合专家（MoE）架构扩展模型容量，提升对用户兴趣的建模能力。

会话式生成方法：与传统的逐点预测不同，OneRec 提出会话式生成方法，生成整个推荐列表，更好地捕捉上下文信息。

迭代偏好对齐模块：结合直接偏好优化（DPO），通过奖励模型（RM）生成偏好数据，优化生成结果。

实验表明，OneRec 在大规模工业数据集上显著优于传统方法，并在快手平台的在线 A/B 测试中实现了 1.6% 的观看时间提升。

## 一、相关背景

快手OneRec结合Scaling Law+DPO实现端到端推荐
  
快手3月份发表了一篇生成式推荐系统工作，将原来的召回->粗排->精排，替换成了端到端的生成式推荐。在模型的构建上，引入了MoE、DPO等LLM中经典有效的方法。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4c23707197ef41bcb2e895113d9807c8.png)

论文标题：OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment
  
地址：https://arxiv.org/pdf/2502.18965

### Q: 这篇论文试图解决什么问题？

A: 这篇论文试图解决现代推荐系统中传统多阶段排序策略（cascade ranking strategy）的局限性问题。具体而言，它旨在通过一个统一的生成式模型来替代现有的级联学习框架，以实现更高效、更准确的推荐结果。主要关注点包括：

* 多阶段排序的局限性：
  + 现代推荐系统通常采用多阶段排序策略，包括召回（recall）、粗排（pre-ranking）和精排（ranking）三个阶段。每个阶段独立运作，前一阶段的性能上限限制了后续阶段的性能，从而限制了整个系统的性能。
  + 传统的生成式模型在推荐系统中仅作为召回阶段的选择器，其推荐准确性尚未达到多阶段排序系统的水平。
* 生成式推荐系统的潜力：
  + 最近，生成式检索推荐系统（Generative Retrieval-based recommendation systems, GRs）作为一种新兴范式，通过自回归方式直接生成候选视频，显示出巨大的潜力。然而，这些生成式模型目前仅在召回阶段发挥作用，未能充分发挥其潜力。
* 用户偏好对齐问题：
  + 在自然语言处理（NLP）中，偏好学习通常依赖于人类明确标注的偏好数据。然而，在推荐系统中，用户与项目的交互数据稀疏，难以直接获得明确的偏好数据。因此，如何在推荐系统中有效地进行偏好学习是一个独特的挑战。
* 上下文信息的重要性：
  + 传统的逐点预测方法（point-wise prediction）在生成推荐结果时，难以平衡连贯性和多样性。而会话式生成方法（session-wise generation）可以更好地捕捉用户在会话中的兴趣动态，生成更连贯、多样化的推荐列表。

综上所述，论文提出了一种名为 OneRec 的统一生成式推荐框架，旨在克服传统多阶段排序的局限性，通过会话式生成方法和偏好对齐策略，显著提升推荐系统的性能。

### Q: 有哪些相关研究？

A: 这篇论文涉及了多个相关研究领域，主要包括生成式推荐系统、偏好对齐、特征工程、模型架构等方面。以下是详细的相关研究内容：

生成式推荐系统

* GENRE：首次采用Transformer架构进行实体检索，基于上下文条件以自回归方式生成实体名称。
* DSI：提出为文档分配结构化语义ID的概念，并训练编码器-解码器模型进行生成式文档检索。
* TIGER：引入生成式项目检索模型的公式，为推荐系统中的生成式检索奠定了基础。
* Recforest：利用层次化的k-means聚类对项目文本嵌入进行处理，以获得聚类索引作为语义ID。
* EAGER：探索将语义和协同信息整合到标记化过程中，以提升推荐效果。

偏好对齐

* RLHF：在大型语言模型（LLM）的后训练阶段，通过强化学习技术引导奖励模型来对齐人类价值观，但存在不稳定和低效的问题。
* DPO：提出直接偏好优化（DPO），通过偏好数据直接优化，避免了RLHF的不足。此外，还有多种DPO的变体，如IPO、cDPO、rDPO等，它们在不同方面对DPO进行了改进或扩展。

特征工程

* 多模态嵌入：使用多模态嵌入来描述视频，这些嵌入与真实的用户-项目行为分布对齐，以更准确地表示项目。
* 语义ID编码：采用残差量化（RQVAE）等技术将多模态嵌入编码为语义ID，但存在“沙漏现象”（hourglass phenomenon），即代码分布不平衡的问题。

模型架构

* MoE（Mixture-of-Experts）：在Transformer架构中，MoE通过稀疏激活机制扩展模型容量，同时保持计算效率。例如，Deepseek MoE提出了一种新的MoE架构，旨在提高专家的特化程度。
* Transformer架构：广泛应用于自然语言处理任务，通过多头自注意力机制和前馈网络层来处理序列数据。在推荐系统中，Transformer架构也被用于建模用户的历史交互行为。

这些相关研究为OneRec的提出提供了理论基础和技术支持，使其能够在生成式推荐和偏好对齐方面取得显著进展。

## 二、OneRec模型

概括：构建一个Encoder-Decoder模型，Encoder输入用户的历史正向行为序列（使用多模态表征+离散化生成Token序列），Decoder生成一个session内的推荐item。这个Encoder-Decoder模型采用了MoE进行高效的参数量扩展。模型进行了2个阶段的训练，第一阶段采用基础的item预测任务，第二阶段采用DPO进行偏好对齐。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/91b4d1fa77684fd293d9d32b79fb3696.png)

### 输入处理—多模态表征量化

OneRec模型的核心是根据用户带有正反馈行为的历史观看序列，生成下一个session的推荐item，这篇文章的场景对应的item是短视频。文中将短视频的多模态表征，通过量化的方式离散化成token序列，作为模型输入。

具体的离散化方法使用了QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou（2024）这篇文章中的思路（如下图中间Quantitative Code部分）。设定了多级的codebook，每一级codebook对前一级的表征残差进行聚类，第一级就是多模态表征本身。聚类算法采用Balanced K-means，保证每个类簇包含的item数量相同。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7229432415ea430c9ea5cff9578ee920.png)

文中采用了3级Codebook，对应每个视频被映射成3个token。将用户历史行为序列的每个短视频的3个token平铺到一个序列，每个短视频之间的token使用[SEP]进行分割，构成最终的输入token序列。

### T5结构和Session粒度生成

OneRec整体模型结构类似T5，由一个Transformer Encoder和Transformer Decoder组成。将上述token序列输入到多层Transformer Encoder中，生成每个位置的编码结果。

在Decoder部分，采用Self-Attention+Cross-Attention的结构进行建模。其中Cross-Attention和Encoder每个位置输出的表征进行attention计算，融合Encoder表征信息。在Decoder部分，采用了MoE的结构，实现在单样本计算量增加较少的情况下提升模型参数空间，这也是目前LLM中常用的模型参数scaling up方法。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6d1f212f422b45aba993b6491833484a.png)

和之前生成模型中Next Item Prediction任务不同的是，OneRec采用了Session维度的预测，即每条样本需要预测一个Session内的item。由于在推荐阶段就是session粒度推荐的，这样建模能让模型更好的获取到一个session内的上下文信息。此外，由于前面将每个短视频映射成了多级token，这一步也会预测多级token。假设一个session需要预测N个item，有K层级的token，那么Decoder就需要预测N\*K个token。每个item的token之间使用[BOS]特殊符号进行分割。

论文提出了一种名为
**OneRec**
的统一生成式推荐框架，通过以下三个关键方法来解决传统多阶段排序策略的局限性问题：

### 1.编码器-解码器结构与模型扩展

* 模型架构：OneRec 采用编码器-解码器结构，编码器对用户的历史行为序列进行编码，解码器逐步解码出用户可能感兴趣的视频。
* 模型扩展：借鉴大规模语言模型的扩展规律，OneRec 使用稀疏混合专家（Mixture-of-Experts, MoE）架构来扩展模型容量，而不会成比例增加计算量。具体来说，通过在解码器中引入MoE层，每个前馈网络（Feed-Forward Network, FNN）被替换为多个专家网络，并通过稀疏门控机制选择部分专家进行计算。这种设计在保持计算效率的同时，显著提升了模型对用户兴趣的建模能力。

### 2.会话式生成方法

* 传统方法的局限性：传统的逐点预测方法（point-wise prediction）在生成推荐结果时，需要手工策略来确保生成结果的连贯性和多样性，这限制了模型的性能。
* 会话式生成：OneRec 提出会话式生成方法（session-wise generation），将用户的一次请求视为一个会话，目标是生成一个高质量的视频列表，而不是逐个生成下一个视频。这种方法允许模型自主学习最优的会话结构，通过提供偏好数据来训练模型，从而更好地捕捉用户在会话中的兴趣动态。
* 高质量会话的定义：为了训练会话式生成模型，论文定义了高质量会话的标准，例如用户实际观看的视频数量、观看时长以及用户与视频的交互行为（如点赞、收藏、分享等）。这些标准确保模型从真实的用户行为模式中学习，从而生成更符合用户兴趣的推荐列表。

### 3.迭代偏好对齐模块与直接偏好优化（DPO）

* 偏好对齐的挑战：在推荐系统中，用户与项目的交互数据稀疏，难以直接获得明确的偏好数据。因此，传统的偏好学习方法（如NLP中的DPO）难以直接应用于推荐系统。
* 奖励模型（Reward Model, RM）：为了解决这一问题，论文设计了一个奖励模型（RM），该模型根据用户的行为和生成的会话来评估用户的偏好。RM通过自注意力机制融合会话中不同项目的信息，并通过多目标预测来估计用户对会话的偏好。
* 迭代偏好对齐（Iterative Preference Alignment, IPA）：基于预训练的RM和当前的OneRec模型，通过beam search生成多个响应，并根据RM的评分选择最佳和最差的响应，构建偏好对。然后，使用这些偏好对进行DPO训练，以优化模型的生成结果。这种迭代过程允许模型不断自我改进，逐步提高生成结果的质量。

为了进一步让生成式推荐的效果更符合用户需求，同时也引入观看时长等监督指标，OneRec进一步采用LLM中的DPO思路进行偏好对齐（IPA）。DPO的核心思路是构建不同偏好好坏的样本pair对，要求模型对于其中满足偏好的样本的打分序高于不满足偏好的样本打分序。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dc1f362630d24a799026803aed81e03f.png)

首先需要训练一个奖励模型，用来评估每条session序列是否满足用户偏好。这里使用推荐系统的关注目标，如观看时长、点击率等作为目标，训练一个预测模型作为奖励模型。奖励模型可以给出一个session序列的观看时长等指标的预测结果。模型结构主要采用self-attention。

接下来，通过beamsearch的方式，为一个用户生成多个session推荐序列，并使用奖励模型进行打分。从中选出打分最高的和打分最差的，作为pair对，并使用DPO进行训练。基于DPO在上一步训练好的模型上进一步训练，得到最终的模型。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b9d4a309a0804e87b7745ee851c3470e.png)

### 小结

总结：通过上述三个关键方法，OneRec 实现了从传统多阶段排序到统一生成式模型的转变，显著提升了推荐系统的性能。具体来说：

* 编码器-解码器结构与MoE扩展：提高了模型对用户兴趣的建模能力，同时保持了计算效率。
* 会话式生成方法：更好地捕捉了用户在会话中的兴趣动态，生成更连贯、多样化的推荐列表。
* 迭代偏好对齐与DPO：通过奖励模型和偏好对齐，优化了生成结果的质量，使其更符合用户的偏好。

这些方法的结合使得OneRec在大规模工业数据集上取得了显著的性能提升，并在实际应用中实现了可观的业务增长。

## 三、实验部分

Q: 论文做了哪些实验？

A: 论文中进行了以下几类实验来验证 OneRec 的性能和有效性：

### 1. 离线性能比较实验

实验目的：比较 OneRec 与传统的逐点方法、列表方法以及偏好对齐方法在推荐性能上的差异。

实验设置：

数据集：使用大规模工业数据集。

基线方法：包括 SASRec、BERT4Rec、FDSA 和 TIGER 等逐点方法，以及 DPO 及其变体（如 IPO、cDPO、rDPO、CPO、simPO 和 S-DPO）。

评估指标：使用观看时间（swt）、观看概率（vtr）、关注概率（wtr）和点赞概率（ltr）等指标来评估模型性能。

实验结果：

OneRec 在会话观看时间（swt）和点赞概率（ltr）等关键指标上显著优于传统的逐点方法和 TIGER 等生成式方法。

仅使用 1% 的 DPO 训练数据，OneRec-1B+IPA 在最大 swt 和最大 ltr 上分别比基础 OneRec-1B 提高了 4.04% 和 5.43%。

OneRec 提出的 IPA 策略在偏好对齐方面优于现有的 DPO 变体。

### 2. 消融实验

实验目的：验证 OneRec 中各个模块的有效性，特别是 DPO 样本比例和模型规模对性能的影响。

实验设置：

DPO 样本比例消融实验：改变 DPO 训练中的样本比例（从 1% 到 5%），观察模型性能的变化。

模型规模消融实验：评估不同模型规模（从 0.05B 到 1B 参数）对 OneRec 性能的影响。

实验结果：

DPO 样本比例消融实验：发现 1% 的 DPO 样本比例已经能够带来显著的性能提升，而进一步增加样本比例对性能的提升有限，但计算成本显著增加。

模型规模消融实验：随着模型规模的增加，OneRec 的性能持续提升，表明模型扩展对于提升推荐性能是有效的。

### 3. 在线 A/B 测试实验

实验目的：评估 OneRec 在实际工业场景中的在线性能。

实验设置：

平台：在快手（Kuaishou）的短视频推荐平台进行 A/B 测试。

流量分配：使用 1% 的主流量进行实验。

基线系统：与当前的多阶段推荐系统进行比较。

评估指标：使用总观看时间和平均观看时长来衡量用户对推荐结果的接受程度。

实验结果：

OneRec 在总观看时间上实现了 1.68% 的提升，在平均观看时长上实现了 6.56% 的提升，表明 OneRec 能够提供更优质的推荐结果，并为平台带来显著的业务增长。

### 4. 预测动态分析

实验目的：分析 OneRec 在不同层的预测分布，以验证偏好对齐策略的效果。

实验设置：

可视化方法：对每个层的语义 ID 的 softmax 输出进行可视化，标记出具有最高奖励值的项目的语义 ID。
  
实验结果：

OneRec+IPA 在预测分布上显示出显著的置信度变化，表明偏好对齐策略有效地促使基础模型生成更符合用户偏好的结果。

第一层的预测分布显示出更高的不确定性（熵为 6.00），而后续层的分布逐渐集中（第二层平均熵为 3.71，第三层熵为 0.048），这归因于自回归解码机制，后续层受益于累积的上下文信息，从而约束了决策空间。

通过这些实验，论文全面验证了 OneRec 在离线和在线环境中的性能，以及各个模块对模型性能的贡献。

## 四、未来方向

Q: 有什么可以进一步探索的点？

A: 尽管 OneRec 在生成式推荐系统方面取得了显著的进展，但仍有一些可以进一步探索和改进的方向：

### 1. 多目标优化

问题：当前的 OneRec 主要关注观看时间（swt）和交互指标（如点赞概率 ltr），但在实际应用中，推荐系统可能需要同时优化多个目标，例如用户留存率、新用户获取、内容多样性等。

探索方向：

多目标优化框架：设计一个能够同时优化多个目标的框架，例如通过多任务学习（Multi-Task Learning, MTL）或强化学习（Reinforcement Learning, RL）来平衡不同目标之间的关系。

动态权重调整：根据不同的业务需求和用户行为动态调整各个目标的权重，以实现更灵活的推荐策略。

### 2. 用户冷启动问题

问题：对于新用户或历史行为数据较少的用户，OneRec 的性能可能会受到限制，因为模型难以从有限的数据中准确建模用户兴趣。

探索方向：

冷启动策略：探索有效的冷启动策略，例如利用用户的基本属性（如年龄、性别、地理位置等）或引入外部知识图谱来辅助建模。

零样本学习：研究零样本学习（Zero-Shot Learning, ZSL）或少样本学习（Few-Shot Learning, FSL）技术，以提高模型对新用户的适应能力。

### 3. 生成结果的多样性

问题：虽然 OneRec 在会话式生成中考虑了上下文信息，但在某些情况下，生成的推荐结果可能仍然缺乏多样性。

探索方向：

多样性增强技术：引入多样性增强技术，如通过正则化方法、引入噪声或使用不同的解码策略（如 Top-K Sampling、Nucleus Sampling）来增加生成结果的多样性。

多模态信息融合：进一步融合多模态信息（如文本、图像、音频等），以更全面地捕捉用户兴趣，从而生成更多样化的推荐结果。

### 4. 实时反馈机制

问题：在实际应用中，用户的兴趣可能会快速变化，而 OneRec 的当前实现可能无法及时捕捉这些变化。

探索方向：

实时反馈学习：设计实时反馈机制，使模型能够根据用户的即时反馈（如点击、停留时间、滑动行为等）快速调整推荐结果。

在线学习算法：研究在线学习算法，如增量学习（Incremental Learning）或持续学习（Continual Learning），以提高模型对用户兴趣变化的适应能力。

### 5. 模型可解释性

问题：OneRec 作为一个复杂的生成式模型，其推荐结果的可解释性可能有限，这在某些应用场景中是一个重要的问题。

探索方向：

可解释性技术：引入可解释性技术，如特征重要性分析、注意力机制可视化或生成式对抗网络（GANs）中的解释性方法，以提高模型的透明度。

用户反馈解释：设计机制让用户能够理解推荐结果的依据，并通过用户反馈进一步优化模型的可解释性。

### 6. 跨领域推荐

问题：OneRec 目前主要应用于短视频推荐，但在其他领域（如电商、音乐、新闻等）的应用潜力尚未充分挖掘。

探索方向：

跨领域适应性：研究 OneRec 在不同领域的适应性，探索如何调整模型架构和训练策略以适应不同领域的特点。

领域迁移学习：利用迁移学习技术，将在一个领域学到的知识迁移到其他领域，以提高模型的泛化能力。

### 7. 模型压缩与优化

问题：尽管 MoE 架构提高了模型的计算效率，但随着模型规模的进一步扩大，模型的存储和计算成本仍然是一个挑战。

探索方向：

模型压缩技术：研究模型压缩技术，如量化（Quantization）、剪枝（Pruning）和知识蒸馏（Knowledge Distillation），以进一步降低模型的存储和计算需求。

高效推理算法：开发高效的推理算法，如稀疏激活机制的优化和并行计算技术，以提高模型的在线推理速度。

### 8. 用户隐私保护

问题：在处理用户数据时，保护用户隐私是一个重要的问题，尤其是在模型需要大量用户行为数据进行训练的情况下。

探索方向：

隐私保护技术：引入隐私保护技术，如差分隐私（Differential Privacy）或同态加密（Homomorphic Encryption），以确保用户数据的安全性和隐私性。

联邦学习：研究联邦学习（Federated Learning）技术，使模型能够在不直接访问用户数据的情况下进行训练，从而保护用户隐私。

## Reference

[1] https://arxiv.org/pdf/2502.18965
  
[2] QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou（2024）