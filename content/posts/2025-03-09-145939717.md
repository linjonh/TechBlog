---
arturl_encode: "6874747073:3a2f2f626c6f672e6373646e2e6e65742f706173737867782f:61727469636c652f64657461696c732f313435393339373137"
layout: post
title: "7.2-奇异值分解的基与矩阵"
date: 2025-03-09 23:21:24 +08:00
description: "奇异值分解"
keywords: "7.2 奇异值分解的基与矩阵"
categories: ['第7章', '奇异值分解Svd']
tags: ['线性代数', '矩阵']
artid: "145939717"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145939717
    alt: "7.2-奇异值分解的基与矩阵"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145939717
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145939717
cover: https://bing.ee123.net/img/rand?artid=145939717
image: https://bing.ee123.net/img/rand?artid=145939717
img: https://bing.ee123.net/img/rand?artid=145939717
---

# 7.2 奇异值分解的基与矩阵

### 一、奇异值分解

奇异值分解（SVD）是线性代数的高光时刻。

A
A





A
是一个

m
×
n
m\times n





m



×





n
的矩阵，可以是方阵或者长方形矩阵，秩为

r
r





r
。我们要对角化

A
A





A
，但并不是把它化成

X
−
1
A
X
X^{-1}A X






X










−

1

A

X
的形式。这是因为

X
X





X
中的特征向量有三个大问题：它们通常并不正交，并不总是有足够数量的特征向量，并且

A
x
=
λ
x
A\boldsymbol x=\lambda\boldsymbol x





A



x



=





λ



x
要求

A
A





A
是方阵。

A
A





A
的
**奇异值向量**
（
**Singular vectors**
）完美的解决了这些问题。
  
由 SVD 我们可以得到：
**四个基本子空间合适的基**
。下面是按照基向量的
**重要性顺序**
，来求得这些基向量的步骤。
  
我们需要有
**两组奇异值向量**



u
i
\boldsymbol u\_i








u









i

​

和

v
i
\boldsymbol v\_i








v









i

​

，其中

u
i
\boldsymbol u\_i








u









i

​

在

R
m
\pmb {\textrm R}^m








R









m
中，

v
i
\boldsymbol v\_i








v









i

​

在

R
n
\textrm{\pmb R}^n








R









n
中，它们将分别是

m
×
m
m\times m





m



×





m
的矩阵

U
\pmb U






U
和

n
×
n
n\times n





n



×





n
的矩阵

V
\pmb V






V
的列。下面会先根据这些基向量来描述 SVD，然后再根据正交矩阵

U
U





U
和

V
V





V
来描述 SVD。其基本思想是，在行空间中找到一组基标准交向量

v
i
\boldsymbol v\_i








v









i

​

，然后通过

A
v
i
=
σ
i
u
i
A\boldsymbol v\_i=\sigma\_i\boldsymbol u\_i





A




v









i

​




=






σ









i

​





u









i

​

映射到列空间中的

u
i
\boldsymbol u\_i








u









i

​

，我们的目标是

v
i
\boldsymbol v\_i








v









i

​

是特殊的标准正交向量，映射到

u
i
\boldsymbol u\_i








u









i

​

也是标准正交。
  
（
**向量角度**
）

u
i
\boldsymbol u\_i








u









i

​

和

v
j
\boldsymbol v\_j








v









j

​

给出了

A
A





A
的四个基本子空间的基：

> u
> 1
> ,
> u
> 2
> ,
> ⋯
>  
> ,
> u
> r
> 是
> 列空间
> 的标准正交基
> u
> r
> +
> 1
> ,
> u
> r
> +
> 2
> ,
> ⋯
>  
> ,
> u
> m
> 是
> 左零空间
>  
> N
> (
> A
> T
> )
>  
> 的标准正交基
> v
> 1
> ,
> v
> 2
> ,
> ⋯
>  
> ,
> v
> r
> 是
> 行空间
> 的标准正交基
> v
> r
> +
> 1
> ,
> v
> r
> +
> 2
> ,
> ⋯
>  
> ,
> v
> n
> 是
> 零空间
>  
> N
> (
> A
> )
>  
> 的标准正交基
> \begin{array}{ll}\boldsymbol u\_1,\boldsymbol u\_2,\cdots,\boldsymbol u\_r&是\pmb{列空间}的标准正交基\\\boldsymbol u\_{r+1},\boldsymbol u\_{r+2},\cdots,\boldsymbol u\_m&是\pmb{左零空间\,N(A^T)\,}的标准正交基\\\boldsymbol v\_1,\boldsymbol v\_2,\cdots,\boldsymbol v\_r&是\pmb{行空间}的标准正交基\\\boldsymbol v\_{r+1},\boldsymbol v\_{r+2},\cdots,\boldsymbol v\_n&是\pmb{零空间}\,N(A)\,的标准正交基\end{array}
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
> ,
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
> ,
>
>
>
> ⋯
>
>
>
>
>
> ,
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
>
> r
>
> +
>
> 1
>
> ​
>
>
> ,
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
>
> r
>
> +
>
> 2
>
> ​
>
>
> ,
>
>
>
> ⋯
>
>
>
>
>
> ,
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> m
>
> ​
>
>
>
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
> ,
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
> ,
>
>
>
> ⋯
>
>
>
>
>
> ,
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
>
> r
>
> +
>
> 1
>
> ​
>
>
> ,
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
>
> r
>
> +
>
> 2
>
> ​
>
>
> ,
>
>
>
> ⋯
>
>
>
>
>
> ,
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> n
>
> ​
>
>
> ​
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 是
>
>
> 列空间
>
> 的标准正交基
>
>
>
>
>
> 是
>
>
> 左零空间
>
>
>
> N
>
> (
>
>
> A
>
>
>
>
>
>
>
>
>
> T
>
> )
>
>
> 的标准正交基
>
>
>
>
>
> 是
>
>
> 行空间
>
> 的标准正交基
>
>
>
>
>
> 是
>
>
> 零空间
>
>
>
> N
>
> (
>
> A
>
> )
>
>
>
> 的标准正交基
>
> ​

这些基向量不仅正交，而且可以对角化矩阵

A
A





A
：

> **“对角化
>
> A
> \pmb A
>
>
>
>
>
>
> A
> ”**
>
>
>
> \kern 20pt
>
>
>
>
> A
> v
> 1
> =
> σ
> 1
> u
> 1
> ,
> A
> v
> 2
> =
> σ
> 2
> u
> 2
> ,
> ⋯
>  
> ,
> A
> v
> r
> =
> σ
> r
> u
> r
> (
> 7.2.1
> )
> {\color{blue}A\boldsymbol v\_1=\sigma\_1\boldsymbol u\_1,\kern 5ptA\boldsymbol v\_2=\sigma\_2\boldsymbol u\_2,\kern 5pt\cdots,\kern 5ptA\boldsymbol v\_r=\sigma\_r\boldsymbol u\_r}\kern 20pt(7.2.1)
>
>
>
>
>
>
> A
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
> =
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
> ,
>
>
>
>
>
> A
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
> =
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
> ,
>
>
>
>
>
> ⋯
>
>
>
>
>
> ,
>
>
>
>
>
> A
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
> =
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
> (
>
> 7.2.1
>
> )

这些
**奇异值**



σ
1
,
σ
2
,
⋯
 
,
σ
r
\pmb{\sigma\_1,\sigma\_2,\cdots,\sigma\_r}







σ









1

​


,




σ









2

​


,



⋯





,




σ









r

​

都是正数，

σ
i
\sigma\_i






σ









i

​

是

A
v
i
A\boldsymbol v\_i





A




v









i

​

的长度。对角矩阵

Σ
\Sigma





Σ
的对角元素除了奇异值

σ
i
\sigma\_i






σ









i

​

外都是零。
  
（
**矩阵角度**
）由于

u
i
\boldsymbol u\_i








u









i

​

是标准正交的，所以由

r
r





r
个列向量组成的矩阵

U
r
U\_r






U









r

​

有

U
r
T
U
r
=
I
U\_r^TU\_r=I






U









r





T

​



U









r

​




=





I
，而

v
i
\boldsymbol v\_i








v









i

​

也是标准正交的，所以矩阵

V
r
V\_r






V









r

​

有

V
r
T
V
r
=
I
V\_r^TV\_r=I






V









r





T

​



V









r

​




=





I
。则由方程

A
v
i
=
σ
i
u
i
A\boldsymbol v\_i=\sigma\_i\boldsymbol u\_i





A




v









i

​




=






σ









i

​





u









i

​

推出

A
V
r
=
U
r
Σ
r
\pmb{AV\_r=U\_r\Sigma\_r}






A


V









r

​




=




U









r

​



Σ









r

​

的各列成立：

(
m
×
n
)
(
n
×
r
)
A
V
r
=
U
r
Σ
r
(
m
×
r
)
(
r
×
r
)
A
[
v
1
v
2
⋯
v
r
]
=
[
u
1
u
2
⋯
u
r
]
[
σ
1
σ
2
⋱
σ
r
]
(
7.2.2
)
\begin{array}{l}(m\times n)(n\times r)\\\pmb{AV\_r=U\_r\Sigma\_r}\\(m\times r)(r\times r)\end{array}\kern 10ptA\begin{bmatrix}\boldsymbol v\_1&\boldsymbol v\_2&\cdots&\boldsymbol v\_r\end{bmatrix}=\begin{bmatrix}\boldsymbol u\_1&\boldsymbol u\_2&\cdots&\boldsymbol u\_r\end{bmatrix}\begin{bmatrix}\sigma\_1\\&\sigma\_2\\&&\ddots\\&&&\sigma\_r\end{bmatrix}\kern 15pt(7.2.2)

















(

m



×



n

)

(

n



×



r

)






A


V









r

​




=




U









r

​



Σ









r

​






(

m



×



r

)

(

r



×



r

)

​




A





[














v









1

​


​

















v









2

​


​














⋯

​

















v









r

​


​



]



=







[














u









1

​


​

















u









2

​


​














⋯

​

















u









r

​


​



]




















​













σ









1

​


​















σ









2

​


​














⋱


​















σ









r

​


​


















​






(

7.2.2

)
这个是 SVD 的核心，但是 SVD 并不是只有这些内容。这些

v
i
\boldsymbol v\_i








v









i

​

和

u
i
\boldsymbol u\_i








u









i

​

可以生成

A
A





A
的行空间和列空间，还可以从零空间

N
(
A
)
\pmb N(A)






N

(

A

)
和左零空间

N
(
A
T
)
\pmb N(A^T)






N

(


A









T

)
得到

n
−
r
n-r





n



−





r
个

v
j
\boldsymbol v\_j








v









j

​

和

m
−
r
m-r





m



−





r
个

u
i
\boldsymbol u\_i








u









i

​

，它们也和前面的

v
i
\boldsymbol v\_i








v









i

​

和

u
i
\boldsymbol u\_i








u









i

​

正交（这是因为四个基本子空间配对正交）。我们现在在

V
V





V
和

U
U





U
中包含所有的

v
j
\boldsymbol v\_j








v









j

​

和

u
i
\boldsymbol u\_i








u









i

​

，这两个矩阵就变成了方阵，
**仍然有**



A
V
=
U
Σ
\pmb{AV=U\Sigma}






A

V



=



U

Σ
。

(
m
×
n
)
(
n
×
n
)
A
V
=
U
Σ
(
m
×
m
)
(
m
×
n
)
A
[
v
1
⋯
v
r
⋯
v
n
]
=
[
u
1
⋯
u
r
⋯
u
m
]
[
σ
1
σ
2
⋱
σ
r
]
(
7.2.3
)
\begin{array}{l}(m\times n)(n\times n)\\\pmb{AV=U\Sigma}\\(m\times m)(m\times n)\end{array}\kern 10ptA\begin{bmatrix}\boldsymbol v\_1&\cdots&\boldsymbol v\_r&\cdots&\boldsymbol v\_n\end{bmatrix}=\begin{bmatrix}\boldsymbol u\_1&\cdots&\boldsymbol u\_r&\cdots&\boldsymbol u\_m\end{bmatrix}\begin{bmatrix}\sigma\_1\\&\sigma\_2\\&&\ddots\\&&&\sigma\_r\\&\end{bmatrix}\kern 15pt(7.2.3)

















(

m



×



n

)

(

n



×



n

)






A

V



=



U

Σ





(

m



×



m

)

(

m



×



n

)

​




A





[














v









1

​


​














⋯

​

















v









r

​


​














⋯

​

















v









n

​


​



]



=







[














u









1

​


​














⋯

​

















u









r

​


​














⋯

​

















u









m

​


​



]




















​













σ









1

​


​















σ









2

​


​














⋱


​















σ









r

​


​


















​






(

7.2.3

)
新的

Σ
\Sigma





Σ
是

m
×
n
m\times n





m



×





n
的矩阵，它是（7.3.2）中的

r
×
r
r\times r





r



×





r
矩阵下面添加

m
−
r
m-r





m



−





r
个零行，右侧添加

n
−
r
n-r





n



−





r
个零列。真正的变化是

U
U





U
和

V
V





V
的形状，它们变成了方阵，并且有

V
−
1
=
V
T
V^{-1}=V^T






V










−

1



=






V









T
，所以

A
V
=
U
Σ
AV=U\Sigma





A

V



=





U

Σ
就变成了

A
=
U
Σ
V
T
\pmb{A=U\Sigma V^T}






A



=



U

Σ


V









T
，这个就是
**奇异值分解（Singular Value Decompositon）**
，可以用

U
Σ
U\Sigma





U

Σ
中的列

u
i
σ
i
\boldsymbol u\_i\sigma\_i








u









i

​



σ









i

​

乘上

V
T
V^T






V









T
的行：

> **SVD**
>
>
>
> A
> =
> U
> Σ
> V
> T
> =
> u
> 1
> σ
> 1
> v
> 1
> T
> +
> u
> 2
> σ
> 2
> v
> 2
> T
> +
> ⋯
> +
> u
> r
> σ
> r
> v
> r
> T
> (
> 7.2.4
> )
> \kern 30pt{\color{blue}A=U\Sigma V^T=\boldsymbol u\_1\sigma\_1\boldsymbol v\_1^T+\boldsymbol u\_2\sigma\_2\boldsymbol v\_2^T+\cdots+\boldsymbol u\_r\sigma\_r\boldsymbol v\_r^T}\kern 20pt(7.2.4)
>
>
>
>
>
>
>
>
> A
>
>
>
> =
>
>
>
> U
>
> Σ
>
>
> V
>
>
>
>
>
>
>
>
>
> T
>
>
>
> =
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 2
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
> ⋯
>
>
>
> +
>
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> r
>
>
>
>
>
> T
>
> ​
>
>
>
>
> (
>
> 7.2.4
>
> )

式（7.3.2）是 “缩略版的 SVD（reduced SVD）”，它只包含行空间和列空间的基，式（7.3.3）是完整版的 SVD，它将零空间的基也包含了进去。这两个式子都是将

A
A





A
分解成同样的

r
r





r
个秩一矩阵

u
i
σ
r
v
i
T
\boldsymbol u\_i\sigma\_r\boldsymbol v\_i^T








u









i

​



σ









r

​





v









i





T

​

的和。列乘行是矩阵乘法的第四种方式。
  
我们后面能够看到每个

σ
i
2
\sigma\_i^2






σ









i





2

​

既是

A
T
A
A^TA






A









T

A
的特征值，也是

A
A
T
AA^T





A


A









T
的特征值。我们将奇异值按照降序排列：

σ
1
≥
σ
2
≥
⋯
≥
σ
r
>
0
\sigma\_1\geq\sigma\_2\geq\cdots\geq\sigma\_r>0






σ









1

​




≥






σ









2

​




≥





⋯



≥






σ









r

​




>





0
，式（7.2.4）中的分解就是按照
**重要性顺序**
得到的

A
A





A
的

r
r





r
个秩一项，这一点非常重要。

【
**例1**
】什么时候

A
=
U
Σ
V
T
A=U\Sigma V^T





A



=





U

Σ


V









T
（奇异值）和

X
Λ
X
−
1
X\Lambda X^{-1}





X

Λ


X










−

1
相同（特征值）？
  
**解：**



A
A





A
的特征向量需要标准正交，才有

X
=
U
=
V
X=U=V





X



=





U



=





V
，如果

A
=
Σ
A=\Sigma





A



=





Σ
，也需要特征值

λ
≥
0
\lambda\geq0





λ



≥





0
，所以

A
A





A
必须是一个
**半正定（或正定）的对称矩阵**
。只有这样，才会有

A
=
X
Λ
X
−
1
A=X\Lambda X^{-1}





A



=





X

Λ


X










−

1
，就是

Q
Λ
Q
T
Q\Lambda Q^{T}





Q

Λ


Q










T
和

A
=
U
Σ
V
T
A=U\Sigma V^{T}





A



=





U

Σ


V










T
一致。

【
**例2**
】如果

A
=
x
y
T
A=\boldsymbol x \boldsymbol y^T





A



=







x




y









T
（秩一），其中

x
\boldsymbol x







x
和

y
\boldsymbol y







y
都是单位向量，那么

A
A





A
的 SVD 是什么？
  
**解：**
式（7.2.2）中缩略版的 SVD 就是

x
y
T
\boldsymbol x\boldsymbol y^T







x




y









T
，秩为

r
=
1
r=1





r



=





1
，其中

u
1
=
x
\boldsymbol u\_1=\boldsymbol x








u









1

​




=







x
，

v
1
=
y
\boldsymbol v\_1=\boldsymbol y








v









1

​




=







y
且

σ
1
=
1
\sigma\_1=1






σ









1

​




=





1
。完全版的 SVD，要将

u
1
=
x
\boldsymbol u\_1=\boldsymbol x








u









1

​




=







x
扩充为标准正交基

u
i
\boldsymbol u\_i








u









i

​

，然后将

v
1
=
y
\boldsymbol v\_1=\boldsymbol y








v









1

​




=







y
扩充为标准正交基

v
j
\boldsymbol v\_j








v









j

​

，没有新的

σ
\sigma





σ
，只有一个

σ
1
=
1
\sigma\_1=1






σ









1

​




=





1
。

### 二、SVD 的证明

我们需要知道这令人惊叹的

u
i
\boldsymbol u\_i








u









i

​

和

v
j
\boldsymbol v\_j








v









j

​

是如何构造出来的。

v
j
\boldsymbol v\_j








v









j

​

**是**



A
T
A
\pmb{A^TA}







A









T

A
**的特征向量**
，这个一定是正确的，因为我们的目标是：

A
T
A
=
(
U
Σ
V
T
)
T
(
U
Σ
V
T
)
=
V
Σ
T
U
T
U
Σ
V
T
=
V
Σ
T
Σ
V
T
(
7.2.5
)
\pmb{A^TA}=(U\Sigma V^T)^T(U\Sigma V^T)=V\Sigma^TU^TU\Sigma V^T=\pmb{V\Sigma^T\Sigma V^T}\kern 20pt(7.2.5)







A









T

A



=





(

U

Σ


V









T


)









T

(

U

Σ


V









T

)



=





V


Σ









T


U









T

U

Σ


V









T



=






V


Σ









T

Σ


V









T



(

7.2.5

)
右侧包含了对称（半）正定矩阵

A
T
A
A^TA






A









T

A
的特征向量矩阵

V
V





V
，并且

(
Σ
T
Σ
)
(\Sigma^T\Sigma)





(


Σ









T

Σ

)
是

(
A
T
A
)
(A^TA)





(


A









T

A

)
的特征值矩阵：每个

σ
2
\sigma^2






σ









2
就是

(
A
T
A
)
(A^TA)





(


A









T

A

)
特征值

λ
(
A
T
A
)
\lambda(A^TA)





λ

(


A









T

A

)
！
  
现在由

A
v
i
=
σ
i
u
i
A\boldsymbol v\_i=\sigma\_i\boldsymbol u\_i





A




v









i

​




=






σ









i

​





u









i

​

可以得到单位向量

u
1
,
u
2
,
⋯
 
,
u
r
\boldsymbol u\_1,\boldsymbol u\_2,\cdots,\boldsymbol u\_r








u









1

​


,






u









2

​


,



⋯





,






u









r

​

，这就是关键的式（7.2.1），核心点，也就是 SVD 成功的全部原因，是这些单位向量

u
1
,
u
2
,
⋯
 
,
u
r
\boldsymbol u\_1,\boldsymbol u\_2,\cdots,\boldsymbol u\_r








u









1

​


,






u









2

​


,



⋯





,






u









r

​

一定是两两正交（因为

v
i
\boldsymbol v\_i








v









i

​

是正交的）：

关键步骤
i
≠
j
u
i
T
u
j
=
(
A
v
i
σ
i
)
T
(
A
v
j
σ
j
)
=
v
i
T
A
T
A
v
j
σ
i
σ
j
=
σ
j
2
σ
i
σ
j
v
i
T
v
j
=
0
(
7.2.6
)
\begin{array}{}\pmb{关键步骤}\\\pmb{i\neq j}\end{array}\kern 20pt\boldsymbol u\_i^T\boldsymbol u\_j=(\frac{A\boldsymbol v\_i}{\sigma\_i})^T(\frac{A\boldsymbol v\_j}{\sigma\_j})=\frac{\boldsymbol v\_i^TA^TA\boldsymbol v\_j}{\sigma\_i\sigma\_j}=\frac{\sigma\_j^2}{\sigma\_i\sigma\_j}\boldsymbol v\_i^T\boldsymbol v\_j=0\kern 20pt(7.2.6)


















关键步骤






i















=



j

​







u









i





T

​





u









j

​




=





(













σ









i

​












A




v









i

​


​



)









T

(













σ









j

​












A




v









j

​


​


)



=

















σ









i

​



σ









j

​















v









i





T

​



A









T

A




v









j

​


​




=

















σ









i

​



σ









j

​













σ









j





2

​


​





v









i





T

​





v









j

​




=





0



(

7.2.6

)



v
i
\boldsymbol v\_i








v









i

​

是

A
T
A
A^TA






A









T

A
（对称）的特征向量，它们是正交的并且得到

u
i
\boldsymbol u\_i








u









i

​

也是正交的。实际上

u
i
\boldsymbol u\_i








u









i

​

是

A
A
T
AA^T





A


A









T
的特征向量。
  
最终，在

v
j
\boldsymbol v\_j








v









j

​

和

u
i
\boldsymbol u\_i








u









i

​

的基础上分别构造出了零空间

N
(
A
)
\pmb N(A)






N

(

A

)
和 左零空间

N
(
A
T
)
\pmb N(A^T)






N

(


A









T

)
的标准正交基，得到

n
n





n
个

v
j
\boldsymbol v\_j








v









j

​

和

m
m





m
个

u
i
\boldsymbol u\_i








u









i

​

，得到

A
=
U
Σ
V
T
A=U\Sigma V^T





A



=





U

Σ


V









T
中的

V
,
Σ
V,\Sigma





V

,



Σ
和

U
U





U
。

### 三、SVD 的一个例子

下例演示了

A
=
U
Σ
V
T
A=U\Sigma V^T





A



=





U

Σ


V









T
中全部三个矩阵的计算过程。

【
**例3**
】矩阵

A
=
[
3
0
4
5
]
A=\begin{bmatrix}3&0\\4&5\end{bmatrix}





A



=







[











3





4

​














0





5

​



]
，秩为

r
=
2
r=2





r



=





2
，求出对应的矩阵

U
,
Σ
,
V
U,\Sigma,V





U

,



Σ

,



V
。
  
**解：**
由于秩

r
=
2
r=2





r



=





2
，所以

A
A





A
有两个正的奇异值

σ
1
\sigma\_1






σ









1

​

和

σ
2
\sigma\_2






σ









2

​

，下面会看到

σ
1
\sigma\_1






σ









1

​

比特征值

λ
m
a
x
=
5
\lambda\_{max}=5






λ










ma

x

​




=





5
更大，

σ
2
\sigma\_2






σ









2

​

比特征值

λ
m
i
n
=
3
\lambda\_{min}=3






λ










min

​




=





3
要更小，先从

A
T
A
A^TA






A









T

A
和

A
A
T
AA^T





A


A









T
开始：

A
T
A
=
[
25
20
20
25
]
A
A
T
=
[
9
12
12
41
]
A^TA=\begin{bmatrix}25&20\\20&25\end{bmatrix}\kern 20ptAA^T=\begin{bmatrix}\kern 4pt9&12\\12&41\end{bmatrix}






A









T

A



=







[











25





20

​














20





25

​



]





A


A









T



=







[













9





12

​














12





41

​



]
它们有相同的迹

(
50
)
(50)





(

50

)
和相同的特征值

σ
1
2
=
45
\sigma\_1^2=45






σ









1





2

​




=





45
和

σ
2
2
=
5
\sigma\_2^2=5






σ









2





2

​




=





5
，取算术平方根得

σ
1
=
45
\sigma\_1=\pmb{\sqrt{45}}






σ









1

​




=














45


​

和

σ
2
=
5
\sigma\_2=\pmb{\sqrt5}






σ









2

​




=













5


​

，则

σ
1
σ
2
=
15
\sigma\_1\sigma\_2=15






σ









1

​



σ









2

​




=





15
，也就是

A
A





A
的行列式。
  
关键步骤是求

A
T
A
A^TA






A









T

A
的特征向量（分别对应特征值

45
45





45
和

5
5





5
）：

[
25
20
20
25
]
[
1
1
]
=
45
[
1
1
]
[
25
20
20
25
]
[
−
1
1
]
=
5
[
−
1
1
]
\begin{bmatrix}25&20\\20&25\end{bmatrix}\begin{bmatrix}1\\1\end{bmatrix}=\pmb{45}\begin{bmatrix}1\\1\end{bmatrix}\kern 20pt\begin{bmatrix}25&20\\20&25\end{bmatrix}\begin{bmatrix}-1\\\kern 7pt1\end{bmatrix}=\pmb5\begin{bmatrix}-1\\\kern 7pt1\end{bmatrix}







[











25





20

​














20





25

​



]





[











1





1

​



]



=






45





[











1





1

​



]







[











25





20

​














20





25

​



]





[











−

1







1

​



]



=






5





[











−

1







1

​



]
然后将这些正交特征向量单位化，都除以

2
\sqrt2












2


​

，即得到

v
1
\boldsymbol v\_1








v









1

​

和

v
2
\boldsymbol v\_2








v









2

​

。

右奇异向量
v
1
=
1
2
[
1
1
]
v
2
=
1
2
[
−
1
1
]
左奇异向量
u
i
=
A
v
i
σ
i
\pmb{右奇异向量}\kern 10pt\boldsymbol v\_1=\frac{1}{\sqrt2}\begin{bmatrix}1\\1\end{bmatrix}\kern 5pt\boldsymbol v\_2=\frac{1}{\sqrt2}\begin{bmatrix}-1\\\kern 7pt1\end{bmatrix}\kern 10pt\pmb{左奇异向量}\kern 5pt\boldsymbol u\_i=\frac{A\boldsymbol v\_i}{\sigma\_i}






右奇异向量






v









1

​




=























2


​












1

​






[











1





1

​



]








v









2

​




=























2


​












1

​






[











−

1







1

​



]






左奇异向量






u









i

​




=

















σ









i

​












A




v









i

​


​

现在计算

A
v
1
A\boldsymbol v\_1





A




v









1

​

和

A
v
2
A\boldsymbol v\_2





A




v









2

​

，它们分别是

σ
1
u
1
=
45
u
1
\sigma\_1\boldsymbol u\_1=\sqrt{45}\boldsymbol u\_1






σ









1

​





u









1

​




=













45


​





u









1

​

和

σ
2
u
2
=
5
u
2
\sigma\_2\boldsymbol u\_2=\sqrt5\boldsymbol u\_2






σ









2

​





u









2

​




=












5


​





u









2

​

：

A
v
1
=
3
2
[
1
3
]
=
45
1
10
[
1
3
]
=
σ
1
u
1
A
v
2
=
1
2
[
−
3
1
]
=
5
1
10
[
−
3
1
]
=
σ
2
u
2
\begin{array}{l}A\boldsymbol v\_1=\displaystyle\frac{3}{\sqrt2}\begin{bmatrix}1\\3\end{bmatrix}=\sqrt{45}\pmb{\frac{1}{\sqrt{10}}\begin{bmatrix}1\\3\end{bmatrix}}=\sigma\_1\boldsymbol u\_1\\\\A\boldsymbol v\_2=\displaystyle\frac{1}{\sqrt2}\begin{bmatrix}-3\\\kern 7pt1\end{bmatrix}=\sqrt5\pmb{\frac{1}{\sqrt{10}}\begin{bmatrix}-3\\\kern 7pt1\end{bmatrix}}=\sigma\_2\boldsymbol u\_2\end{array}

















A




v









1

​




=





















2


​












3

​






[











1





3

​



]



=











45


​






















10


​












1

​






[











1





3

​



]



=




σ









1

​





u









1

​












A




v









2

​




=





















2


​












1

​






[











−

3







1

​



]



=










5


​






















10


​












1

​






[











−

3







1

​



]



=




σ









2

​





u









2

​


​

除以

10
\sqrt{10}













10


​

使得

u
1
\boldsymbol u\_1








u









1

​

和

u
2
\boldsymbol u\_2








u









2

​

标准正交，然后可以得到预期的

σ
1
=
45
\sigma\_1=\sqrt{45}






σ









1

​




=













45


​

和

σ
2
=
5
\sigma\_2=\sqrt5






σ









2

​




=












5


​

。

A
A





A
的奇异值分解就是

U
Σ
V
T
U\Sigma V^T





U

Σ


V









T
。

> U
> =
> 1
> 10
> [
> 1
> −
> 3
> 3
> 1
> ]
> Σ
> =
> [
> 45
> 5
> ]
> V
> =
> 1
> 2
> [
> 1
> −
> 1
> 1
> 1
> ]
> (
> 7.2.7
> )
> \pmb U=\frac{1}{\sqrt{10}}\begin{bmatrix}1&-3\\3&\kern 7pt1\end{bmatrix}\kern 15pt\pmb\Sigma=\begin{bmatrix}\sqrt{45}\\&\sqrt5\end{bmatrix}\kern 15pt\pmb V=\frac{1}{\sqrt2}\begin{bmatrix}1&-1\\1&\kern 7pt1\end{bmatrix}\kern 20pt(7.2.7)
>
>
>
>
>
>
> U
>
>
>
> =
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 10
>
>
> ​
>
>
>
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
>
> [
>
>
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> 3
>
> ​
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> −
>
> 3
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
> ]
>
>
>
>
>
>
> Σ
>
>
>
> =
>
>
>
>
>
>
>
> [
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 45
>
>
> ​
>
>
> ​
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 5
>
>
> ​
>
>
> ​
>
>
>
> ]
>
>
>
>
>
>
> V
>
>
>
> =
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 2
>
>
> ​
>
>
>
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
>
> [
>
>
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> −
>
> 1
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
> ]
>
>
>
>
>
> (
>
> 7.2.7
>
> )

U
U





U
和

V
V





V
包含列空间和行空间（两个空间都是

R
2
\pmb{\textrm R}^2








R









2
）的标准正交基。真正要做的是使用两组基对角化

A
A





A
：

A
V
=
U
Σ
AV=U\Sigma





A

V



=





U

Σ
。矩阵

A
A





A
分解成了两个秩一矩阵（列乘行）的组合：

σ
1
u
1
v
1
T
+
σ
2
u
2
v
2
T
=
45
20
[
1
1
3
3
]
+
5
20
[
3
−
3
−
1
1
]
=
[
3
0
4
5
]
=
A
\sigma\_1\boldsymbol u\_1\boldsymbol v\_1^T+\sigma\_2\boldsymbol u\_2\boldsymbol v\_2^T=\pmb{\frac{\sqrt{45}}{\sqrt{20}}\begin{bmatrix}1&1\\3&3\end{bmatrix}+\frac{\sqrt5}{\sqrt{20}}\begin{bmatrix}\kern 7pt3&-3\\-1&\kern 7pt1\end{bmatrix}}=\begin{bmatrix}3&0\\4&5\end{bmatrix}=A






σ









1

​





u









1

​





v









1





T

​




+






σ









2

​





u









2

​





v









2





T

​




=

























20


​




















45


​


​






[











1





3

​














1





3

​



]



+






















20


​



















5


​


​






[













3





−

1

​














−

3







1

​



]



=







[











3





4

​














0





5

​



]



=





A

### 四、一个极端的矩阵

下面是一个阶数更大的矩阵的例子，它的

u
i
\boldsymbol u\_i








u









i

​

和

v
i
\boldsymbol v\_i








v









i

​

都是单位矩阵的列，所以计算很简单，但是要注意列的顺序。矩阵

A
A





A
非常的不平衡（它是一个严格的三角形矩阵），它所有的特征值都为零，

A
A
T
AA^T





A


A









T
和

A
T
A
A^TA






A









T

A
并不接近，矩阵

U
U





U
和

V
V





V
是置换矩阵可以弥补这些问题。

A
=
[
0
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
]
特征值
 
λ
=
0
,
0
,
0
,
0
 
全是零！
只有一个特征向量
 
(
1
,
0
,
0
,
0
)
奇异值
 
σ
=
3
,
2
,
1
奇异向量是
 
I
 
的列
A=\begin{bmatrix}0&\pmb1&0&0\\0&0&\pmb2&0\\0&0&0&\pmb3\\0&0&0&0\end{bmatrix}\kern 20pt\begin{array}{l}特征值\,\lambda=0,0,0,0\,全是零！\\只有一个特征向量\,(1,0,0,0)\\奇异值\,\sigma=\pmb3,\pmb2,\pmb1\\奇异向量是\,I\,的列\end{array}





A



=






















​












0





0





0





0

​















1





0





0





0

​














0






2





0





0

​














0





0






3





0

​


















​


















特征值



λ



=



0

,



0

,



0

,



0



全是零！





只有一个特征向量



(

1

,



0

,



0

,



0

)





奇异值



σ



=




3

,




2

,




1





奇异向量是



I



的列

​




A
T
A
A^TA






A









T

A
和

A
A
T
AA^T





A


A









T
都是对角矩阵（有简单的特征向量，但是顺序不同）：

A
T
A
=
[
0
0
0
0
0
1
0
0
0
0
4
0
0
0
0
9
]
A
A
T
=
[
1
0
0
0
0
4
0
0
0
0
9
0
0
0
0
0
]
A^TA=\begin{bmatrix}\pmb0&0&0&0\\0&\pmb1&0&0\\0&0&\pmb4&0\\0&0&0&\pmb9\end{bmatrix}\kern 20ptAA^T=\begin{bmatrix}\pmb1&0&0&0\\0&\pmb4&0&0\\0&0&\pmb9&0\\0&0&0&\pmb0\end{bmatrix}






A









T

A



=






















​













0





0





0





0

​














0






1





0





0

​














0





0






4





0

​














0





0





0






9

​


















​






A


A









T



=






















​













1





0





0





0

​














0






4





0





0

​














0





0






9





0

​














0





0





0






0

​


















​

它们的特征向量（

u
i
\boldsymbol u\_i








u









i

​

对应

A
A
T
AA^T





A


A









T
，

v
i
\boldsymbol v\_i








v









i

​

对应

A
T
A
A^TA






A









T

A
）按照特征值减小的顺序

σ
1
2
>
σ
2
2
>
σ
3
2
\sigma\_1^2>\sigma\_2^2>\sigma\_3^2






σ









1





2

​




>






σ









2





2

​




>






σ









3





2

​

排列，这些特征值是

9
,
4
,
1
9,4,1





9

,



4

,



1
。

U
=
[
0
0
1
0
0
1
0
0
1
0
0
0
0
0
0
1
]
Σ
=
[
3
2
1
0
]
V
=
[
0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0
]
\pmb U=\begin{bmatrix}0&0&\pmb1&0\\0&\pmb1&0&0\\\pmb1&0&0&0\\0&0&0&\pmb1\end{bmatrix}\kern 15pt\Sigma=\begin{bmatrix}\pmb3\\&\pmb2\\&&\pmb1\\&&&0\end{bmatrix}\kern 15pt\pmb V=\begin{bmatrix}0&0&0&\pmb1\\0&0&\pmb1&0\\0&\pmb1&0&0\\\pmb1&0&0&0\end{bmatrix}






U



=






















​












0





0






1





0

​














0






1





0





0

​















1





0





0





0

​














0





0





0






1

​


















​






Σ



=






















​













3


​















2


​















1


​














0

​


















​







V



=






















​












0





0





0






1

​














0





0






1





0

​














0






1





0





0

​















1





0





0





0

​


















​




U
\pmb U






U
和

V
\pmb V






V
的第一列

u
1
\boldsymbol u\_1








u









1

​

和

v
1
\boldsymbol v\_1








v









1

​

的分量

1
1





1
在第

3
3





3
位和第

4
4





4
位，则

u
1
σ
1
v
1
T
\boldsymbol u\_1\sigma\_1\boldsymbol v\_1^T








u









1

​



σ









1

​





v









1





T

​

就表示矩阵

A
A





A
中最大的数

A
34
A\_{34}






A










34

​

，对于这个极端的例子，SVD 的三个秩一矩阵恰好来自于

A
A





A
中的数字

3
,
2
,
1
3,2,1





3

,



2

,



1
。

> A
> =
> U
> Σ
> V
> T
> =
> 3
> u
> 1
> v
> 1
> T
> +
> 2
> u
> 2
> v
> 2
> T
> +
> 1
> u
> 3
> v
> 3
> T
> \pmb{A=U\Sigma V^T=3\boldsymbol u\_1\boldsymbol v\_1^T+2\boldsymbol u\_2\boldsymbol v\_2^T+1\boldsymbol u\_3\boldsymbol v\_3^T}
>
>
>
>
>
>
> A
>
>
>
> =
>
>
>
> U
>
> Σ
>
>
> V
>
>
>
>
>
>
>
>
>
> T
>
>
>
> =
>
>
>
> 3
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
> 2
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 2
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
> 1
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 3
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 3
>
>
>
>
>
> T
>
> ​

注：假设将

A
A





A
的最后一行（全零行）去掉，则

A
A





A
是一个

3
×
4
3\times4





3



×





4
的矩阵，

A
A
T
AA^T





A


A









T
是

3
×
3
3\times3





3



×





3
的矩阵 —— 它的第四行和第四列都会消失，

A
T
A
A^TA






A









T

A
和

A
A
T
AA^T





A


A









T
的特征值仍然是

λ
=
1
,
4
,
9
\lambda=1,4,9





λ



=





1

,



4

,



9
，也会在

Σ
\Sigma





Σ
中得到相同的奇异值

σ
=
3
,
2
,
1
\sigma=3,2,1





σ



=





3

,



2

,



1
。
  



A
A





A
的最后一行的全零行移除后，现在变成了

3
×
4
3\times4





3



×





4
的矩阵，也会移除

Σ
\Sigma





Σ
的最后一行和

U
U





U
的最后一列。则

(
3
×
4
)
=
U
Σ
V
T
=
(
3
×
3
)
(
3
×
4
)
(
4
×
4
)
(3\times4)=U\Sigma V^T=(3\times3)(3\times4)(4\times4)





(

3



×





4

)



=





U

Σ


V









T



=





(

3



×





3

)

(

3



×





4

)

(

4



×





4

)
，SVD 完全适用于长方形矩阵。
  
一件好的事情是，由于矩阵

A
A





A
的行和列经常有完全不同的含义（如电子表格），如果要统计所有课程的成绩，那么可以用各列表示每个学生的情况，而各行表示每个课程的情况：则元素

a
i
j
a\_{ij}






a










ij

​

就表示成绩。那么

σ
1
u
1
v
1
T
\sigma\_1\boldsymbol u\_1\boldsymbol v\_1^T






σ









1

​





u









1

​





v









1





T

​

中，

u
1
=
\boldsymbol u\_1=








u









1

​




=
**课程组合**
，

v
1
=
\boldsymbol v\_1=








v









1

​




=
**学生组合**
，而

σ
1
\sigma\_1






σ









1

​

就是这些组合的成绩：最高成绩。
  
矩阵

A
A





A
也可以对一本杂志中关键词的出现频率计数：

A
A





A
的列是不同的文章，每一行代表不同的词，那么

A
A





A
就是整个杂志的索引，其中最重要的信息就是

σ
1
u
1
v
1
T
\sigma\_1\boldsymbol u\_1\boldsymbol v\_1^T






σ









1

​





u









1

​





v









1





T

​

，

σ
1
\sigma\_1






σ









1

​

就是高频词

u
1
\boldsymbol u\_1








u









1

​

在高频文章

v
1
\boldsymbol v\_1








v









1

​

出现的最高的频率。

### 五、奇异值的稳定性对比特征值的不稳定性

通过

4
×
4
4\times4





4



×





4
的矩阵

A
A





A
这个例子（一个极端情况）我们可以构造出一个特征值不稳定的例子。
**假设

(
4
,
1
)
(4,1)





(

4

,



1

)
元素**
从零变为

1
/
60000
1/60000





1/60000
，这个
**几乎没有变化**
，秩变成了

4
4





4
。

A
=
[
0
1
0
0
0
0
2
0
0
0
0
3
1
60000
0
0
0
]
仅仅是
 
1
/
60000
 
的微小改变，
A
的特征值就发生了很大的变化：
λ
=
0
,
0
,
0
,
0
 
变为了
 
λ
=
1
10
,
i
10
,
−
1
10
,
−
i
10
A=\begin{bmatrix}0&1&0&0\\0&0&2&0\\0&0&0&3\\\displaystyle\frac{1}{60000}&0&0&0\end{bmatrix}\kern 20pt\begin{array}{l}仅仅是\,1/60000\,的微小改变，A\\的特征值就发生了很大的变化：\\\lambda=0,0,0,0\,变为了\,\pmb{\lambda=\displaystyle\frac{1}{10},\frac{i}{10},\frac{-1}{10},\frac{-i}{10}}\end{array}





A



=






















​












0





0





0
















60000











1

​


​














1





0





0





0

​














0





2





0





0

​














0





0





3





0

​


















​


















仅仅是



1/60000



的微小改变，

A





的特征值就发生了很大的变化：





λ



=



0

,



0

,



0

,



0



变为了




λ



=














10











1

​


,














10











i

​


,














10











−

1

​


,














10











−

i

​


​

新元素仅仅变化了

1
/
60000
1/60000





1/60000
，四个特征值就从零变化到了在复平面上以原点为圆心，半径为

1
10
\displaystyle\frac{1}{10}
















10











1

​

的圆上，这表明当

A
T
A
A^TA






A









T

A
远离

A
A
T
AA^T





A


A









T
时特征值严重的不稳定性，在另外的极端情况下，如果

A
T
A
=
A
A
T
A^TA=AA^T






A









T

A



=





A


A









T
（“正规矩阵”），那么

A
A





A
的特征向量正交且

A
A





A
的特征值完全稳定。
  
对比之下，
**任意矩阵的奇异值都是稳定的**
，它们不会超过

A
A





A
的变化。在这个例子中，新的奇异值是

3
,
2
,
1
\pmb{3,2,1}






3

,



2

,



1
和

1
/
60000
\pmb{1/60000}






1/60000
，矩阵

U
U





U
和

V
V





V
保持不变，

A
A





A
的 SVD 中的第四项是

σ
4
u
4
v
4
T
\sigma\_4\boldsymbol u\_4\boldsymbol v\_4^T






σ









4

​





u









4

​





v









4





T

​

，它有十五个零元素和一个小的元素

σ
4
=
1
/
60000
\sigma\_4=1/60000






σ









4

​




=





1/60000
。

### 六、A 的奇异向量和 S = A T A S=A^TA S = A T A 的特征向量

式（7.2.5-7.2.6）同时证明了 SVD，奇异向量

v
i
\boldsymbol v\_i








v









i

​

是

S
=
A
T
A
S=A^TA





S



=






A









T

A
的特征向量

q
i
\boldsymbol q\_i








q









i

​

。

S
S





S
的特征值

λ
i
\lambda\_i






λ









i

​

和

A
A





A
相应的奇异值的平方

σ
i
2
\sigma\_i^2






σ









i





2

​

相等，

S
S





S
的秩

r
r





r
等于

A
A





A
的秩。特征向量和奇异向量的展开式是完美对应的。

> 对称矩阵
>  
> S
> S
> =
> Q
> Λ
> Q
> T
> =
> λ
> 1
> q
> 1
> q
> 1
> T
> +
> λ
> 2
> q
> 2
> q
> 2
> T
> +
> ⋯
> +
> λ
> r
> q
> r
> q
> r
> T
> 任意矩阵
>  
> A
> A
> =
> U
> Σ
> V
> T
> =
> σ
> 1
> u
> 1
> v
> 1
> T
> +
> σ
> 2
> u
> 2
> v
> 2
> T
> +
> ⋯
> +
> σ
> r
> u
> r
> v
> r
> T
> \begin{array}{l}\pmb{对称矩阵\,S}\kern 25pt\color{blue}S=Q\Lambda Q^T=\lambda\_1\boldsymbol q\_1\boldsymbol q\_1^T+\lambda\_2\boldsymbol q\_2\boldsymbol q\_2^T+\cdots+\lambda\_r\boldsymbol q\_r\boldsymbol q\_r^T\\\pmb{任意矩阵\,A}\kern 23pt\color{blue}A=U\Sigma V^T=\sigma\_1\boldsymbol u\_1\boldsymbol v\_1^T+\sigma\_2\boldsymbol u\_2\boldsymbol v\_2^T+\cdots+\sigma\_r\boldsymbol u\_r\boldsymbol v\_r^T\end{array}
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> 对称矩阵
>
>
>
> S
>
>
>
> S
>
>
>
> =
>
>
>
> Q
>
> Λ
>
>
> Q
>
>
>
>
>
>
>
>
>
> T
>
>
>
> =
>
>
>
>
> λ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
>
> λ
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 2
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
> ⋯
>
>
>
> +
>
>
>
>
> λ
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> r
>
>
>
>
>
> T
>
> ​
>
>
>
>
>
>
>
> 任意矩阵
>
>
>
> A
>
>
>
> A
>
>
>
> =
>
>
>
> U
>
> Σ
>
>
> V
>
>
>
>
>
>
>
>
>
> T
>
>
>
> =
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 2
>
>
>
>
>
> T
>
> ​
>
>
>
>
> +
>
>
>
> ⋯
>
>
>
> +
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> r
>
> ​
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> r
>
>
>
>
>
> T
>
> ​
>
>
> ​

各个

q
i
\boldsymbol q\_i








q









i

​

是正交的，各个

u
i
\boldsymbol u\_i








u









i

​

是正交的，各个

v
i
\boldsymbol v\_i








v









i

​

也是正交的，非常漂亮！
  
但是还要更深入理解，有以下两个原因：其一是要弥补特征值部分的短板， 如果

λ
\lambda





λ
是

S
S





S
的二重特征值，我们能够且一定可以找出两个标准正交的向量。另一个原因是要明白 SVD 是如何选出在

σ
2
u
2
v
2
T
\sigma\_2\boldsymbol u\_2\boldsymbol v\_2^T






σ









2

​





u









2

​





v









2





T

​

之前的最大项

σ
1
u
1
v
1
T
\sigma\_1\boldsymbol u\_1\boldsymbol v\_1^T






σ









1

​





u









1

​





v









1





T

​

。需要理解的是

S
S





S
的特征值

λ
\lambda





λ
和

A
A





A
的奇异值

σ
\sigma





σ
，而
**不只是求出它们**
。
  
从

S
S





S
最大的特征值

λ
1
\lambda\_1






λ









1

​

开始，它解决了下面的这个问题：

> λ
> 1
> =
> max
>  
> x
> T
> S
> x
> x
> T
> x
> \pmb{\lambda\_1=\textrm{max}\,\displaystyle\frac{\boldsymbol x^TS\boldsymbol x}{\boldsymbol x^T\boldsymbol x}}
>
>
>
>
>
>
>
> λ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
> =
>
>
>
>
> max
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> T
>
>
>
> x
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> T
>
> S
>
>
>
> x
>
> ​
>
> ，这个向量是
>
> x
> =
> q
> 1
> \boldsymbol x=\boldsymbol q\_1
>
>
>
>
>
>
>
> x
>
>
>
> =
>
>
>
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
> ，且
>
> S
> q
> 1
> =
> λ
> 1
> q
> 1
> S\boldsymbol q\_1=\lambda\_1\boldsymbol q\_1\kern 20pt
>
>
>
>
>
> S
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
> =
>
>
>
>
>
>
> λ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
> (7.2.8)
>   
>
>
>
> \,
>
>   
> 对比
>
> A
> A
>
>
>
>
>
> A
> 最大的奇异值
>
> σ
> 1
> \sigma\_1
>
>
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
> ，它解决的这个问题是：
>   
>
>
>
> σ
> 1
> =
> max
> ⁡
> ∣
> ∣
> A
> x
> ∣
> ∣
> ∣
> ∣
> x
> ∣
> ∣
> \pmb{\sigma\_1=\max\displaystyle\frac{||A\boldsymbol x||}{||\boldsymbol x||}}
>
>
>
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
> =
>
>
>
> max
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> ∣∣
>
>
>
> x
>
> ∣∣
>
>
>
>
>
>
>
>
>
>
>
> ∣∣
>
> A
>
>
>
> x
>
> ∣∣
>
> ​
>
> ，这个向量是
>
> x
> =
> v
> 1
> \boldsymbol x=\boldsymbol v\_1
>
>
>
>
>
>
>
> x
>
>
>
> =
>
>
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
> 且
>
> A
> v
> 1
> =
> σ
> 1
> u
> 1
> A\boldsymbol v\_1=\sigma\_1\boldsymbol u\_1\kern 20pt
>
>
>
>
>
> A
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
> =
>
>
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
>
>
>
>
> u
>
>
>
>
>
>
>
>
>
> 1
>
> ​
>
> （7.2.9）

从式（7.2.8）可以推出（7.2.9），由于

S
=
A
T
A
S=A^TA





S



=






A









T

A
，所以

x
T
S
x
x
T
x
=
x
T
A
T
A
x
x
T
x
=
(
A
x
)
T
(
A
x
)
x
T
x
=
∣
∣
A
x
∣
∣
2
∣
∣
x
∣
∣
2
=
σ
2
\displaystyle\frac{\boldsymbol x^TS\boldsymbol x}{\boldsymbol x^T\boldsymbol x}=\frac{\boldsymbol x^TA^TA\boldsymbol x}{\boldsymbol x^T\boldsymbol x}=\frac{(A\boldsymbol x)^T(A\boldsymbol x)}{\boldsymbol x^T\boldsymbol x}=\frac{||A\boldsymbol x||^2}{||\boldsymbol x||^2}=\sigma^2



















x









T



x














x









T

S



x

​




=



















x









T



x














x









T


A









T

A



x

​




=



















x









T



x











(

A



x


)









T

(

A



x

)

​




=
















∣∣



x

∣


∣









2











∣∣

A



x

∣


∣









2

​




=






σ









2
。
  
这个一次找到一个值的方法也可以适用于

λ
2
\lambda\_2






λ









2

​

和

σ
2
\sigma\_2






σ









2

​

，但是并不是任意的

x
\boldsymbol x







x
都可以：

> λ
> 2
> =
> max
> ⁡
> x
> T
> S
> x
> x
> T
> x
> \pmb{\lambda\_2=\max\displaystyle\frac{\boldsymbol x^TS\boldsymbol x}{\boldsymbol x^T\boldsymbol x}}
>
>
>
>
>
>
>
> λ
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
> =
>
>
>
> max
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> T
>
>
>
> x
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> T
>
> S
>
>
>
> x
>
> ​
>
> ，其中
>
> x
> \boldsymbol x
>
>
>
>
>
>
>
> x
> 满足
>
> q
> 1
> T
> x
> =
> 0
> \boldsymbol q\_1^T\boldsymbol x=0
>
>
>
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> T
>
> ​
>
>
>
>
> x
>
>
>
> =
>
>
>
>
>
> 0
> ，最大的是
>
> x
> =
> q
> 2
> \boldsymbol x=\boldsymbol q\_2\kern 20pt
>
>
>
>
>
>
>
> x
>
>
>
> =
>
>
>
>
>
>
>
>
> q
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
> （7.2.10）
>   
>
>
>
> \,
>
>   
>
>
>
> σ
> 2
> =
> max
> ⁡
> ∣
> ∣
> A
> x
> ∣
> ∣
> ∣
> ∣
> x
> ∣
> ∣
> \pmb{\sigma\_2=\max\displaystyle\frac{||A\boldsymbol x||}{||\boldsymbol x||}}
>
>
>
>
>
>
>
> σ
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
>
>
>
> =
>
>
>
> max
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> ∣∣
>
>
>
> x
>
> ∣∣
>
>
>
>
>
>
>
>
>
>
>
> ∣∣
>
> A
>
>
>
> x
>
> ∣∣
>
> ​
>
> ，其中
>
> x
> \boldsymbol x
>
>
>
>
>
>
>
> x
> 满足
>
> v
> 1
> T
> x
> =
> 0
> \boldsymbol v\_1^T\boldsymbol x=0
>
>
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 1
>
>
>
>
>
> T
>
> ​
>
>
>
>
> x
>
>
>
> =
>
>
>
>
>
> 0
> ，最大的是
>
> x
> =
> v
> 2
> \boldsymbol x=\boldsymbol v\_2\kern 20pt
>
>
>
>
>
>
>
> x
>
>
>
> =
>
>
>
>
>
>
>
>
> v
>
>
>
>
>
>
>
>
>
> 2
>
> ​
>
> （7.2.11）

当

S
=
A
T
A
S=A^TA





S



=






A









T

A
时，可以得到

λ
1
=
σ
1
2
\lambda\_1=\sigma\_1^2






λ









1

​




=






σ









1





2

​

和

λ
2
=
σ
2
2
\lambda\_2=\sigma\_2^2






λ









2

​




=






σ









2





2

​

，为什么这个方法会成功呢？
  
从比值

r
(
x
)
=
x
T
S
x
x
T
x
r(\boldsymbol x)=\displaystyle\frac{\boldsymbol x^TS\boldsymbol x}{\boldsymbol x^T\boldsymbol x}





r

(



x

)



=



















x









T



x














x









T

S



x

​

开始，这个称为瑞利商（Rayleigh quotient），要求

r
(
x
)
r(\boldsymbol x)





r

(



x

)
的最大值，要令其偏导数为零：

∂
r
∂
x
i
=
0
，
i
=
1
,
2
,
⋯
 
,
n
\displaystyle\frac{\partial r}{\partial x\_i}=0，i=1,2,\cdots,n
















∂


x









i

​












∂

r

​




=





0

，

i



=





1

,



2

,



⋯





,



n
。这些导数的求解比较困难，下面是结果：瑞利商最大时的向量

x
\boldsymbol x







x
：

当
 
S
x
=
r
(
x
)
x
 
时，
r
(
x
)
=
x
T
S
x
x
T
x
 
的偏导数全为零
(
7.2.12
)
当\,S\boldsymbol x=r(\boldsymbol x)\boldsymbol x\,时，\pmb{r(\boldsymbol x)=\frac{\boldsymbol x^TS\boldsymbol x}{\boldsymbol x^T\boldsymbol x}\,的偏导数全为零}\kern 20pt(7.2.12)





当



S



x



=





r

(



x

)



x



时，


r

(



x

)



=

















x









T



x














x









T

S



x

​




的偏导数全为零



(

7.2.12

)
所以向量

x
\boldsymbol x







x
是

S
S





S
的特征向量，最大的比值

r
(
x
)
r(\boldsymbol x)





r

(



x

)
就是

S
S





S
最大的特征值

λ
1
\lambda\_1






λ









1

​

。下面转而讨论

A
A





A
—— 注意它和

S
=
A
T
A
S=A^TA





S



=






A









T

A
的联系！

最大化
 
∣
∣
A
x
∣
∣
∣
∣
x
∣
∣
，也会最大化
 
(
∣
∣
A
x
∣
∣
∣
∣
x
∣
∣
)
2
=
x
T
A
T
A
x
x
T
x
=
x
T
S
x
x
x
最大化\,\frac{||A\boldsymbol x||}{||\boldsymbol x||}，也会最大化\,\Big(\frac{||A\boldsymbol x||}{||\boldsymbol x||}\Big)^2=\frac{\boldsymbol x^TA^TA\boldsymbol x}{\boldsymbol x^T\boldsymbol x}=\frac{\boldsymbol x^TS\boldsymbol x}{\boldsymbol x\boldsymbol x}





最大化














∣∣



x

∣∣











∣∣

A



x

∣∣

​


，也会最大化




(












∣∣



x

∣∣











∣∣

A



x

∣∣

​




)









2



=



















x









T



x














x









T


A









T

A



x

​




=


















x



x














x









T

S



x

​

所以式（7.2.9）中

x
=
v
1
\boldsymbol x=\boldsymbol v\_1







x



=








v









1

​

与式（7.2.8）中

S
=
A
T
A
S=A^TA





S



=






A









T

A
最前面的特征向量

q
1
\boldsymbol q\_1








q









1

​

相同。
  
下面解释为什么式（7.2.10）和（7.2.11）中的向量为什么是

q
2
\boldsymbol q\_2








q









2

​

和

v
2
\boldsymbol v\_2








v









2

​

。它们分别与

q
1
\boldsymbol q\_1








q









1

​

和

v
1
\boldsymbol v\_1








v









1

​

正交，因此是在考虑范围内。
  
任意的正交向量

Q
1
Q\_1






Q









1

​

的首列是

q
1
\boldsymbol q\_1








q









1

​

，剩余的

n
−
1
n-1





n



−





1
个标准正交列都与

q
1
\boldsymbol q\_1








q









1

​

正交，使用

S
q
1
=
λ
1
q
1
S\boldsymbol q\_1=\lambda\_1\boldsymbol q\_1





S




q









1

​




=






λ









1

​





q









1

​

：

S
Q
1
=
S
[
q
1
q
2
⋯
q
n
]
=
[
q
1
q
2
⋯
q
n
]
[
λ
1
w
T
0
S
n
−
1
]
=
Q
1
[
λ
1
w
T
0
S
n
−
1
]
(
7.2.13
)
SQ\_1=S\begin{bmatrix}\boldsymbol q\_1&\boldsymbol q\_2&\cdots&\boldsymbol q\_n\end{bmatrix}=\begin{bmatrix}\boldsymbol q\_1&\boldsymbol q\_2&\cdots&\boldsymbol q\_n\end{bmatrix}\begin{bmatrix}\lambda\_1&\boldsymbol w^T\\\boldsymbol 0&S\_{n-1}\end{bmatrix}=Q\_1\begin{bmatrix}\lambda\_1&\boldsymbol w^T\\\boldsymbol 0&S\_{n-1}\end{bmatrix}\kern 15pt(7.2.13)





S


Q









1

​




=





S





[














q









1

​


​

















q









2

​


​














⋯

​

















q









n

​


​



]



=







[














q









1

​


​

















q









2

​


​














⋯

​

















q









n

​


​



]





[












λ









1

​








0

​

















w









T






S










n

−

1

​


​



]



=






Q









1

​






[












λ









1

​








0

​

















w









T






S










n

−

1

​


​



]





(

7.2.13

)
其中

w
T
\boldsymbol w^T








w









T
表示的是

S
S





S
作用于

q
1
\boldsymbol q\_1








q









1

​

，

S
n
−
1
S\_{n-1}






S










n

−

1

​

是降维后的矩阵，利用矩阵的乘法，将它们乘开，第一列即

λ
1
q
1
\lambda\_1\boldsymbol q\_1






λ









1

​





q









1

​

，后面的即为

w
T
q
1
+
S
n
−
1
[
q
2
⋯
q
n
]
\boldsymbol w^T\boldsymbol q\_1+S\_{n-1}\begin{bmatrix}\boldsymbol q\_2&\cdots&\boldsymbol q\_n\end{bmatrix}








w









T




q









1

​




+






S










n

−

1

​






[














q









2

​


​














⋯

​

















q









n

​


​



]
。再用

Q
1
T
Q\_1^T






Q









1





T

​

左乘上式，注意到

Q
1
T
Q
1
=
I
Q\_1^TQ\_1=I






Q









1





T

​



Q









1

​




=





I
，且

Q
1
T
S
Q
1
Q\_1^TSQ\_1






Q









1





T

​


S


Q









1

​

是像

S
S





S
一样的对称矩阵：

对称矩阵
 
Q
1
T
S
Q
1
=
[
λ
1
w
T
0
S
n
−
1
]
将强制
 
w
=
0
 
且
 
S
n
−
1
T
=
S
n
−
1
对称矩阵\,Q\_1^TSQ\_1=\begin{bmatrix}\lambda\_1&\boldsymbol w^T\\\boldsymbol 0&S\_{n-1}\end{bmatrix}将强制\,\boldsymbol w=\boldsymbol 0\,且\,S^T\_{n-1}=S\_{n-1}





对称矩阵




Q









1





T

​


S


Q









1

​




=







[












λ









1

​








0

​

















w









T






S










n

−

1

​


​



]



将强制





w



=







0



且




S










n

−

1





T

​




=






S










n

−

1

​

根据条件

q
1
T
x
=
0
\boldsymbol q\_1^T\boldsymbol x=0








q









1





T

​




x



=





0
将问题（7.2.10）的最大值问题简化成了

n
−
1
n-1





n



−





1
阶的情况，

S
n
−
1
S\_{n-1}






S










n

−

1

​

最大的特征值将是

S
S





S
的第二大特征值，
**就是

λ
2
\lambda\_2






λ









2

​**
，式（7.2.10）中的向量是特征向量

q
2
\boldsymbol q\_2








q









2

​

且

S
q
2
=
λ
2
q
2
S\boldsymbol q\_2=\lambda\_2\boldsymbol q\_2





S




q









2

​




=






λ









2

​





q









2

​

。
  
继续下去，或者使用数学归纳法，就可以得到所有的特征向量

q
1
,
q
2
,
⋯
 
,
q
2
\boldsymbol q\_1,\boldsymbol q\_2,\cdots,\boldsymbol q\_2








q









1

​


,






q









2

​


,



⋯





,






q









2

​

和它们对应的特征值

λ
1
,
λ
2
,
⋯
 
λ
n
\lambda\_1,\lambda\_2,\cdots\,\lambda\_n






λ









1

​


,




λ









2

​


,



⋯






λ









n

​

，即使存在重复的特征值，谱定理

S
=
Q
Λ
Q
T
S=Q\Lambda Q^T





S



=





Q

Λ


Q









T
也可以被证明。所有的对称矩阵都可被对角化。
  
类似的，SVD 可以从式（7.2.9）和（7.2.11）中一步一步的得到。下面有一个很重要的问题：
**实际情况中如何计算

λ
\lambda





λ
和

σ
\sigma





σ
呢？**

### 七、计算 S 的特征值和 A 的奇异值

A
A





A
的奇异值

σ
i
\sigma\_i






σ









i

​

是

S
=
A
T
A
S=A^TA





S



=






A









T

A
特征值

λ
i
\lambda\_i






λ









i

​

的算术平方根，这个将 SVD 和对称矩阵的特征值问题联系了起来，这个很好；但是我们不想使用

A
T
A
A^TA






A









T

A
乘

A
A





A
，因为这个运算非常耗时，这个很不好。
  
首先想到的是
**尽可能将

A
A





A
和

S
S





S
的元素都变成零**
，
**而不改变任何的**



σ
\sigma





σ
和

λ
\lambda





λ
。奇异向量和特征向量会改变 —— 这个没有问题。相似矩阵

Q
−
1
S
Q
Q^{-1}SQ






Q










−

1

SQ
和

S
S





S
的特征值

λ
\lambda





λ
相同，如果

S
S





S
正交，那么这个矩阵就是

Q
T
S
Q
Q^TSQ






Q









T

SQ
也是对称的。
  
对于二阶的对称矩阵可以构造出

2
×
2
2\times2





2



×





2
的旋转矩阵

Q
Q





Q
，使得

Q
T
S
Q
Q^TSQ






Q









T

SQ
是
**对称的三对角矩阵（symmetric and tridiagonal matrix）**
，它含有大量的零元素，但是旋转无法保证总能得到对角矩阵，所以要想求得

S
S





S
所有的特征值，需要新的方法。
  
对于 SVD，和

Q
T
S
Q
Q^TSQ






Q









T

SQ
相对应的是什么？我们不想改变

A
A





A
任何的奇异值，那么就是：在

A
A





A
的两边分别乘上不同的正交矩阵

Q
1
Q\_1






Q









1

​

和

Q
2
Q\_2






Q









2

​

，利用这两个在矩阵

Q
1
T
A
Q
2
Q\_1^TAQ\_2






Q









1





T

​


A


Q









2

​

中生成零元素，而

σ
\sigma





σ
不会改变：

(
Q
1
T
A
Q
2
)
T
(
Q
1
T
A
Q
2
)
=
Q
2
T
A
T
A
Q
2
=
Q
2
T
S
Q
2
得到相同的
 
σ
(
A
)
 
和
 
λ
(
S
)
(Q\_1^TAQ\_2)^T(Q\_1^TAQ\_2)=Q\_2^TA^TAQ\_2=Q\_2^TSQ\_2\kern 10pt得到相同的\,\sigma(A)\,和\,\lambda(S)





(


Q









1





T

​


A


Q









2

​



)









T

(


Q









1





T

​


A


Q









2

​


)



=






Q









2





T

​



A









T

A


Q









2

​




=






Q









2





T

​


S


Q









2

​




得到相同的



σ

(

A

)



和



λ

(

S

)
两个自由选取的

Q
Q





Q
将可以使得

Q
1
T
A
Q
2
Q\_1^TAQ\_2






Q









1





T

​


A


Q









2

​

为
**两对角矩阵（bidiagonal matrix）**
，这个完美对应了

Q
T
S
Q
Q^TSQ






Q









T

SQ
是三对角矩阵，它们之间的联系是：

(
两对角矩阵
)
T
(
两对角矩阵
)
=
三对角矩阵
(两对角矩阵)^T(两对角矩阵)=三对角矩阵





(

两对角矩阵


)









T

(

两对角矩阵

)



=





三对角矩阵
。
  
最后要算出对角矩阵

Λ
\Lambda





Λ
和对角矩阵

Σ
\Sigma





Σ
的步骤需要更多的新思路，这个也不简单，因为有些要解的

det
⁡
(
S
−
λ
I
)
=
0
\det(S-\lambda I)=0





det

(

S



−





λ

I

)



=





0
是

n
=
100
n=100





n



=





100
或

1000
1000





1000
甚至是更多次数的多项式，而我们不可能去求解这些多项式！
  
LAPACK 中求解

λ
\lambda





λ
和

σ
\sigma





σ
是使用简单的正交矩阵来逼近

Q
T
S
Q
=
Λ
Q^TSQ=\Lambda






Q









T

SQ



=





Λ
和

U
T
A
V
=
Σ
U^TAV=\Sigma






U









T

A

V



=





Σ
，
**当很接近

Λ
\Lambda





Λ
和

Σ
\Sigma





Σ
时就停止。**
  
这两步（先是零元素）方法在指令
**eig(S)**
和
**svd(A)**
中内置了。

### 八、主要内容总结

1. SVD 将

   A
   A





   A
   分解成

   U
   Σ
   V
   T
   U\Sigma V^T





   U

   Σ


   V









   T
   ，共有

   r
   r





   r
   个奇异值

   σ
   1
   ≥
   σ
   2
   ≥
   ⋯
   ≥
   σ
   r
   >
   0
   \sigma\_1\geq\sigma\_2\geq\cdots\geq\sigma\_r>0






   σ









   1

   ​




   ≥






   σ









   2

   ​




   ≥





   ⋯



   ≥






   σ









   r

   ​




   >





   0
   .
2. 数值

   σ
   1
   2
   ,
   σ
   2
   2
   ,
   ⋯
    
   ,
   σ
   r
   2
   \sigma\_1^2,\sigma\_2^2,\cdots,\sigma\_r^2






   σ









   1





   2

   ​


   ,




   σ









   2





   2

   ​


   ,



   ⋯





   ,




   σ









   r





   2

   ​

   是

   A
   A
   T
   AA^T





   A


   A









   T
   和

   A
   T
   A
   A^TA






   A









   T

   A
   非零的特征值。
3. U
   U





   U
   和

   V
   V





   V
   的标准正交列是

   A
   A
   T
   AA^T





   A


   A









   T
   和

   A
   T
   A
   A^TA






   A









   T

   A
   的特征向量。
4. 这些列包含了矩阵

   A
   A





   A
   四个基本子空间的标准正交基。
5. 这些基对角化矩阵

   A
   A





   A
   ：

   A
   v
   i
   =
   σ
   i
   u
   i
   ,
    
   i
   ≤
   r
   A\boldsymbol v\_i=\sigma\_i\boldsymbol u\_i,\,i\leq r





   A




   v









   i

   ​




   =






   σ









   i

   ​





   u









   i

   ​


   ,





   i



   ≤





   r
   ，即是

   A
   V
   =
   U
   Σ
   \pmb{AV=U\Sigma}






   A

   V



   =



   U

   Σ
   .
6. A
   =
   σ
   1
   u
   1
   v
   1
   T
   +
   σ
   2
   u
   2
   v
   2
   T
   +
   ⋯
   +
   σ
   r
   u
   r
   v
   r
   T
   A=\sigma\_1\boldsymbol u\_1\boldsymbol v\_1^T+\sigma\_2\boldsymbol u\_2\boldsymbol v\_2^T+\cdots+\sigma\_r\boldsymbol u\_r\boldsymbol v\_r^T





   A



   =






   σ









   1

   ​





   u









   1

   ​





   v









   1





   T

   ​




   +






   σ









   2

   ​





   u









   2

   ​





   v









   2





   T

   ​




   +





   ⋯



   +






   σ









   r

   ​





   u









   r

   ​





   v









   r





   T

   ​

   ，其中

   σ
   1
   \sigma\_1






   σ









   1

   ​

   是比值

   ∣
   ∣
   A
   x
   ∣
   ∣
   ∣
   ∣
   x
   ∣
   ∣
   \displaystyle\frac{||A\boldsymbol x||}{||\boldsymbol x||}
















   ∣∣



   x

   ∣∣











   ∣∣

   A



   x

   ∣∣

   ​

   的最大值。

### 九、例题

【
**例4**
】识别出下列将

A
A





A
分解成列乘行的和的分解方式的类型：

1.
 
正交列
u
1
σ
1
,
u
2
σ
2
,
⋯
 
,
u
r
σ
r
乘
标准正交行
v
1
T
,
v
2
T
,
⋯
 
,
v
r
T
2.
 
标准正交列
q
1
,
q
2
,
⋯
 
,
q
r
乘
三角形矩阵的行
r
1
T
,
r
2
T
,
⋯
 
,
r
r
T
3.
 
三角形矩阵的列
l
1
,
l
2
,
⋯
 
,
l
r
乘
三角形矩阵的行
u
1
T
,
u
2
T
⋯
 
,
u
r
T
\begin{array}{llll}1.\,正交列&\boldsymbol u\_1\sigma\_1,\boldsymbol u\_2\sigma\_2,\cdots,\boldsymbol u\_r\sigma\_r&乘&标准正交行&\boldsymbol v\_1^T,\boldsymbol v\_2^T,\cdots,\boldsymbol v\_r^T\\2.\,标准正交列&\boldsymbol q\_1,\boldsymbol q\_2,\cdots,\boldsymbol q\_r&乘&三角形矩阵的行&\boldsymbol r\_1^T,\boldsymbol r\_2^T,\cdots,\boldsymbol r\_r^T\\3.\,三角形矩阵的列&\boldsymbol l\_1,\boldsymbol l\_2,\cdots,\boldsymbol l\_r&乘&三角形矩阵的行&\boldsymbol u\_1^T,\boldsymbol u\_2^T\cdots,\boldsymbol u\_r^T\end{array}

















1.



正交列





2.



标准正交列





3.



三角形矩阵的列

​

















u









1

​



σ









1

​


,






u









2

​



σ









2

​


,



⋯





,






u









r

​



σ









r

​









q









1

​


,






q









2

​


,



⋯





,






q









r

​









l









1

​


,






l









2

​


,



⋯





,






l









r

​


​














乘





乘





乘

​














标准正交行





三角形矩阵的行





三角形矩阵的行

​

















v









1





T

​


,






v









2





T

​


,



⋯





,






v









r





T

​









r









1





T

​


,






r









2





T

​


,



⋯





,






r









r





T

​









u









1





T

​


,






u









2





T

​




⋯





,






u









r





T

​


​




A
A





A
的秩、主元和奇异值在上述分解中的哪里出现？
  
**解：**
这三种分解不管是对理论数学还是应用数学中的线性代数都是基础：

1. **奇异值分解 Singular Value Decompositon**
   ：

   A
   =
   U
   Σ
   V
   T
   \pmb{A=U\Sigma V^T}






   A



   =



   U

   Σ


   V









   T
2. **格拉姆-施密特正交化 Gram-Schmidt Orthogonalization**
   ：

   A
   =
   Q
   R
   \pmb{A=QR}






   A



   =



   QR
3. **高斯消元法 Gaussian Elimination**
   ：

   A
   =
   L
   U
   \pmb{A=LU}






   A



   =



   LU

可以将奇异值

σ
i
\pmb{\sigma\_i}







σ









i

​

、高度

h
i
\pmb{h\_i}







h









i

​

和主元

d
i
\pmb{d\_i}







d









i

​

单独表示：

1. A
   =
   U
   Σ
   V
   T
   A=U\Sigma V^T





   A



   =





   U

   Σ


   V









   T
   ，其中

   U
   U





   U
   和

   V
   V





   V
   的列都是单位向量，
   **r
   r





   r
   个奇异值

   σ
   i
   \sigma\_i






   σ









   i

   ​

   在

   Σ
   \Sigma





   Σ
   中**
   .
2. A
   =
   Q
   H
   R
   A=QHR





   A



   =





   Q

   H

   R
   ，其中

   Q
   Q





   Q
   的列是单位向量，

   R
   R





   R
   中的对角元素都是

   1
   1





   1
   ，
   **r
   r





   r
   个高度

   h
   i
   h\_i






   h









   i

   ​

   在

   H
   H





   H
   中**
   .
3. A
   =
   L
   D
   U
   A=LDU





   A



   =





   L

   D

   U
   ，其中

   L
   L





   L
   和

   U
   U





   U
   的对角元素都是

   1
   1





   1
   ，
   **r
   r





   r
   个主元在

   D
   D





   D
   中**
   .

每个

h
i
h\_i






h









i

​

表示第

i
i





i
列到由第

1
,
2
,
⋯
 
,
i
−
1
1,2,\cdots,i-1





1

,



2

,



⋯





,



i



−





1
列所形成平面的高度，当

r
=
m
=
n
r=m=n





r



=





m



=





n
时，

n
n





n
维的 “平行多面体”（以

A
A





A
各列为同一顶点处的棱）的体积可以由

A
=
U
Σ
V
T
=
L
D
U
=
Q
H
R
A=U\Sigma V^T=LDU=QHR





A



=





U

Σ


V









T



=





L

D

U



=





Q

H

R
求得：

∣
det
⁡
A
∣
=
∣
σ
i
的乘积
∣
=
∣
d
i
 
的乘积
∣
=
∣
h
i
 
的乘积
∣
\pmb{|\det A|=|\sigma\_i 的乘积|=|d\_i\,的乘积|=|h\_i\,的乘积|}






∣



det



A

∣



=



∣


σ









i

​


的乘积

∣



=



∣


d









i

​




的乘积

∣



=



∣


h









i

​




的乘积

∣
【
**例5**
】
**证明

σ
1
≥
∣
λ
∣
max
\pmb{\sigma\_1\ge|\lambda|\_{\textrm{max}}}







σ









1

​




≥



∣

λ


∣











max

​

，最大的奇异值大于或等于所有的特征值**
。
  
**证明：**
由

A
=
U
Σ
V
T
A=U\Sigma V^T





A



=





U

Σ


V









T
，注意到左乘一个正交矩阵并不改变这个向量的长度：

∣
∣
Q
x
∣
∣
=
∣
∣
x
∣
∣
||Q\boldsymbol x||=||\boldsymbol x||





∣∣

Q



x

∣∣



=





∣∣



x

∣∣
，这是因为

∣
∣
Q
x
∣
∣
2
=
x
T
Q
T
Q
x
=
x
T
x
=
∣
∣
x
∣
∣
2
||Q\boldsymbol x||^2=\boldsymbol x^TQ^TQ\boldsymbol x=\boldsymbol x^T\boldsymbol x=||\boldsymbol x||^2





∣∣

Q



x

∣


∣









2



=








x









T


Q









T

Q



x



=








x









T



x



=





∣∣



x

∣


∣









2
，这个也适用于

Q
=
U
Q=U





Q



=





U
和

Q
=
V
T
Q=V^T





Q



=






V









T
，这两个矩阵的中间是对角矩阵

Σ
\Sigma





Σ
：

∣
∣
A
x
∣
∣
=
∣
∣
U
Σ
V
T
x
∣
∣
=
∣
∣
Σ
V
T
x
∣
∣
≤
σ
1
∣
∣
V
T
x
∣
∣
=
σ
1
∣
∣
x
∣
∣
(
7.2.14
)
||A\boldsymbol x||=||U\Sigma V^T\boldsymbol x||=||\Sigma V^T\boldsymbol x||\le\sigma\_1||V^T\boldsymbol x||=\sigma\_1||\boldsymbol x||\kern 20pt(7.2.14)





∣∣

A



x

∣∣



=





∣∣

U

Σ


V









T



x

∣∣



=





∣∣Σ


V









T



x

∣∣



≤






σ









1

​


∣∣


V









T



x

∣∣



=






σ









1

​


∣∣



x

∣∣



(

7.2.14

)
特征向量满足

∣
∣
A
x
∣
∣
=
∣
λ
∣
∣
∣
x
∣
∣
||A\boldsymbol x||=|\lambda|||\boldsymbol x||





∣∣

A



x

∣∣



=





∣

λ

∣∣∣



x

∣∣
，所以式（7.2.14）表明

∣
λ
∣
∣
∣
x
∣
∣
≤
σ
1
∣
∣
x
∣
∣
|\lambda|||\boldsymbol x||\le\sigma\_1||\boldsymbol x||





∣

λ

∣∣∣



x

∣∣



≤






σ









1

​


∣∣



x

∣∣
，所以

∣
λ
∣
≤
σ
1
\pmb{|\lambda|\le\sigma\_1}






∣

λ

∣



≤




σ









1

​

.
  
取

x
=
(
1
,
0
,
⋯
 
,
0
)
\boldsymbol x=(1,0,\cdots,0)







x



=





(

1

,



0

,



⋯





,



0

)
为单位向量，则

A
x
A\boldsymbol x





A



x
就是

A
A





A
的第一列，然后由不等式（7.2.14）可得，这列的长度小于或等于

σ
1
\sigma\_1






σ









1

​

。所以

A
A





A
的每个元素都有

∣
a
i
j
∣
≤
σ
1
|a\_{ij}|\le\sigma\_1





∣


a










ij

​


∣



≤






σ









1

​

.
  
式（7.2.14）再次证明了
**∣
∣
A
x
∣
∣
∣
∣
x
∣
∣
\displaystyle\frac{||A\boldsymbol x||}{||\boldsymbol x||}
















∣∣



x

∣∣











∣∣

A



x

∣∣

​

的最大值等于

σ
1
\sigma\_1






σ









1

​

.**
  
在求解

A
x
=
b
A\boldsymbol x=\boldsymbol b





A



x



=







b
时，条件数（condition number）

σ
max
σ
min
\displaystyle\frac{\sigma\_{\textrm{max}}}{\sigma\_{\textrm{min}}}

















σ











min

​













σ











max

​


​

控制舍入的误差，如果条件数太大，那么 MATLAB 会警告此时的

x
\boldsymbol x







x
不可靠。