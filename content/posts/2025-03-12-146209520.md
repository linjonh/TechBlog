---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f676f6e676469777564752f:61727469636c652f64657461696c732f313436323039353230"
layout: post
title: "自然语言处理初学者指南"
date: 2025-03-12 18:43:48 +0800
description: "对于初学者来说，自然语言处理的发展历史非常有必要了解，通过梳理历史发展，我们可以清楚技术脉络，从而有助于我们对于这门技术的整体有宏观认知。本文就是要梳理该技术的发展过程，呈现给初学者。"
keywords: "自然语言处理初学者指南"
categories: ['自然语言大模型', '人工智能']
tags: ['自然语言处理', '人工智能']
artid: "146209520"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146209520
    alt: "自然语言处理初学者指南"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146209520
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146209520
cover: https://bing.ee123.net/img/rand?artid=146209520
image: https://bing.ee123.net/img/rand?artid=146209520
img: https://bing.ee123.net/img/rand?artid=146209520
---

# 自然语言处理初学者指南

## 一、说明

对于初学者来说，自然语言处理的发展历史非常有必要了解，通过梳理历史发展，我们可以清楚技术脉络，从而有助于我们对于这门技术的整体有宏观认知。本文就是要梳理该技术的发展过程，呈现给初学者。

## 二、自然语言处理发展史

### 2.1 最早的自然语言处理简介

1954 年，IBM 演示了使用 IBM 701 大型机上的机器翻译将俄语句子翻译成英语的能力。虽然以今天的标准来看这很简单，但这次演示证明了语言翻译的巨大优势。在本文中，我们将研究自然语言处理(NLP) 以及它如何帮助我们更自然地与计算机交谈。

由于各种原因，NLP 是机器学习最重要的子领域之一。自然语言是用户和机器之间最自然的界面。在理想情况下，这涉及语音识别和语音生成。甚至艾伦·图灵也在他的“智能”文章中认识到了这一点，他在文章中将“图灵测试”定义为一种测试机器通过自然语言对话表现出智能行为的能力的方法。

NLP 不是一个单一的实体，而是一个研究领域的范围。图 1 展示了语音助手，这是当今 NLP 的常见产品。NLP 研究领域在语音助手应用程序的基本块上下文中显示。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c3b8b9d417d84838b64b05ea65f66547.png)
  
除了语音助手技术之外，NLP 的主要优势之一是世界上存在大量非结构化文本数据，这些数据是自然语言处理和理解的驱动力。如果机器能够处理、组织和理解这些文本（主要为人类使用而生成），我们可以为未来的机器学习应用解锁大量有用的应用程序，并释放出大量可以投入使用的知识。例如，维基百科包含大量知识，这些知识以多种方式链接在一起，以说明主题之间的关系。维基百科本身就是一个可以应用 NLP 的非结构化数据宝贵来源。

现在让我们探索一下 NLP 的历史和方法。

### 2.2 历史

NLP 与 AI 一样，有着跌宕起伏的历史。IBM 在 1954 年为乔治敦大学演示所做的早期工作强调了机器翻译的巨大优势（将 60 多个俄语句子翻译成英语）。这种早期方法使用六条语法规则来编写 250 个单词的词典，并导致对机器翻译进行大量投资，但基于规则的方法无法扩展到生产系统。

MIT 的 SHRDLU（以英文字母的频率顺序命名）于 20 世纪 60 年代末在 LISP 中开发，使用自然语言让用户操纵和查询积木世界的状态。积木世界是一个充满不同积木的虚拟世界，用户可以使用“拿起一个大红色积木”等命令来操纵它。可以堆叠和查询对象以了解世界的状态（“红色金字塔的右边有什么东西吗？”）。当时，这个演示被认为非常成功，但无法扩展到更复杂和模糊的环境。

在 20 世纪 70 年代和 80 年代初期，开发了许多聊天机器人式的应用程序，这些应用程序可以就有限的话题进行交谈。这些是现在所谓的对话式人工智能的前身，在许多领域得到了广泛而成功的应用。其他应用程序（如 Lehnert 的 Plot Units）实现了叙事摘要。这允许用“情节单元”总结一个简单的故事，例如动机、成功、喜忧参半和其他叙事构建块。

20 世纪 80 年代末，NLP 系统研究从基于规则的方法转向统计模型。随着互联网的出现，大量文本信息变得可供机器访问，NLP 变得更加重要。

### 2.3 NLP 的早期工作

20 世纪 60 年代，人们开始研究如何将意义赋予单词序列。在一种称为标记的过程中，句子可以被分解成词性，以了解它们在句子中的关系。这些标记器依靠人为构建的基于规则的算法，用句子中的上下文来“标记”单词（例如名词、动词、形容词等）。但这种标记相当复杂，因为英语中有多达 150 种不同类型的语音标记。

使用 Python 的自然语言工具包 (NLTK)，您可以看到词性标注的产物。在此示例中，最终的元组集表示标记化的单词及其标签（使用 UPenn 标签集）。此标签集由 36 个标签组成，例如 VBG（动词、动名词或现在分词）、NN（单数名词）、PRP（人称代词）等。

```
>>> quote = "Knowing yourself is the beginning of all wisdom."
>>> tokens = nltk.word_tokenize( quote )
>>> tags = nltk.pos_tag( tokens )
>>> tags
[('Knowing', 'VBG'), ('yourself', 'PRP'), ('is', 'VBZ'),
 ('the', 'DT'), ('beginning', 'NN'), ('of', 'IN'), ('all', 'DT'),
 ('wisdom', 'NN'), ('.', '.')]
>>>

```

标记单词似乎并不复杂，但由于单词的含义可能因使用地点而异，因此这个过程可能很复杂。词性标记是解决其他问题的先决条件，并应用于各种 NLP 任务。

在存在歧义的情况下，基于严格规则的标记方法已让位于统计方法。给定一段文本（或语料库），可以计算一个单词跟在另一个单词后面的概率。在某些情况下，概率非常高，而在其他情况下，概率为零。庞大的单词图及其转换概率是训练机器找出哪些单词更有可能跟在其他单词后面的产物，并且可以以多种方式使用。例如，在语音识别应用程序中，此单词图可用于识别被噪音弄乱的单词（基于该单词之前的单词序列的概率）。这也可以用于自动更正应用程序（为拼写错误的单词推荐一个单词）。这种技术通常使用隐马尔可夫模型 (HMM) 来解决。

HMM 的用处在于，人类不需要构建此图；机器可以根据有效文本语料库构建它。此外，它可以根据单词元组（一个单词跟在另一个单词后面的概率，称为二元组）或基于 n-gram（其中 n=3，一个单词在序列中跟在另外两个单词后面的概率）构建。HMM 不仅应用于 NLP，还应用于各种其他领域（例如蛋白质或 DNA 测序）。

以下示例说明了在 NLTK 中如何从简单句子构造二元词组：

```
>>> sentence = "the man we saw saw a saw"
>>> tokens = nltk.word_tokenize( sentence )
>>> list(nltk.bigrams( tokens ) )
[('the', 'man'), ('man', 'we'), ('we', 'saw'), ('saw', 'saw'),
 ('saw', 'a'), ('a', 'saw')]
>>>

```

让我们探索一些 NLP 任务的现代方法。

## 三、NLP的现代方法

现代 NLP 方法主要侧重于神经网络架构。由于神经网络架构依赖于数值处理，因此需要编码来处理单词。两种常见方法是独热编码和词向量。

### 3.1 单词编码

独热编码将单词转换为独特的向量，然后可以由神经网络进行数字处理。考虑上一个二元组示例中的单词。我们创建独热向量，其维度与要表示的单词数量相同，并在该向量中分配一个位来表示每个单词。这将创建一个独特的映射，可用作神经网络的输入（向量中的每个位作为神经元的输入）（参见图 2）。这种编码比简单地将单词编码为数字（标签编码）更有利，因为网络可以更有效地使用独热向量进行训练。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c6c58f94216840e79527032bcf26c631.png)
  
另一种编码是词向量，它将单词表示为高维向量，其中向量的单位是实数。但不是为每个单位分配一个单词（如独热编码），而是每个单位代表单词的类别（例如单数与复数或名词与动词），并且可以有 100-1,000 个单位宽（维数）。这种编码的有趣之处在于单词现在具有数字相关性，并且编码可以对词向量应用数学运算（例如加、减或取反）。

### 3.2 循环神经网络

循环神经网络 (RNN) 于 20 世纪 80 年代发展起来，在 NLP 中占据着独特的地位。顾名思义，与典型的前馈神经网络相比，RNN 在时间域中运行。RNN 随时间展开并分阶段运行，其中先前的输出为后续阶段的输入提供信息（请参阅图 3 中的展开网络示例）。这种类型的架构非常适用于 NLP，因为网络不仅考虑单词（或其编码），还考虑单词出现的上下文（后面的内容、前面的内容）。在这个设计好的网络示例中，输入神经元接收单词编码，输出通过网络前馈到输出节点（目的是输出用于语言翻译的单词编码）。在实践中，每个单词编码一次只接收一个并传播。在下一个时间步骤中，下一个单词编码被接收（输出仅在最后一个单词被接收后发生）。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1ce61c8b52de4b27b703d3f4c5f04f58.png)
  
传统 RNN 通过反向传播的一种变体（称为时间反向传播 (BPTT)）进行训练。RNN 的一种流行变体是长短期记忆单元 (LSTM)，它具有独特的架构和遗忘信息的能力。

### 3.3 强化学习

强化学习侧重于在环境中选择行动以最大化某些累积奖励（这种奖励不是立即可理解的，而是通过多次行动习得的）。行动的选择基于一种策略，该策略定义给定行动是应该探索新状态（可以进行学习的未知领域）还是旧状态（基于过去的经验）。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2992cf8d48ca4c2ab64e4ef80618bab8.png)
  
在 NLP 和机器翻译的背景下，观察是呈现的单词序列。状态表示部分翻译，而动作表示是否可以提供翻译或是否需要更多信息（更多观察或单词）。随着进一步的观察，状态可能会确定有足够的信息并呈现翻译。这种方法的关键是翻译是逐步完成的，强化学习可以确定何时进行翻译或何时等待更多信息（在主动词出现在句子末尾的语言中很有用）。

强化学习也被用作实现基于文本摘要的 RNN 的训练算法。

### 3.4 深度学习

深度学习/深度神经网络已成功应用于各种问题。您会发现深度学习是问答系统、文档摘要、图像标题生成、文本分类和建模等诸多领域的核心。请注意，这些案例代表了自然语言理解和自然语言生成。

深度学习是指具有多层（深度部分）的神经网络，它将特征作为输入并从这些数据中提取更高级的特征。深度学习网络能够学习表示的层次结构和输入的不同抽象级别。深度学习网络可以使用监督学习或无监督学习，也可以形成其他方法的混合体（例如将循环神经网络与深度学习网络结合起来）。

最常见的深度学习网络方法是卷积神经网络 (CNN)，它主要用于图像处理应用（例如对图像内容进行分类）。图 5 展示了一个用于情绪分析的简单 CNN。它由一个单词编码输入层（来自标记化输入）组成，然后将其馈送到卷积层。卷积层将输入分割成许多输入“窗口”以生成特征图。这些特征图通过最大运算进行池化，从而降低输出的维度并提供输入的最终表示。这被馈送到提供分类（例如正面、中性、负面）的最终神经网络中。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a9471dced17e4d2fb010a38bffc9c1f8.png)
  
虽然 CNN 在图像和语言领域已被证明是有效的，但其他类型的网络也可以使用。长短期记忆是一种新型的 RNN。LSTM 细胞比典型的神经元更复杂，因为它们包括状态和许多内部门，可用于接受输入、输出数据或忘记内部状态信息。LSTM 通常用于自然语言应用。LSTM 最有趣的用途之一是与 CNN 结合使用，其中 CNN 提供处理图像的能力，而 LSTM 经过训练可生成输入图像内容的文本句子。

## 四、更进一步的方法

使用 NLP 的应用程序越来越多，这证明了 NLP 的重要性。NLP 为计算机和在线提供的大量非结构化数据提供了最自然的界面。2011 年，IBM 展示了 Watson™，它与 Jeopardy 的两位最伟大的冠军展开较量，并使用自然语言界面击败了他们。Watson 还使用 2011 版维基百科作为其知识来源 - 这是语言处理和理解道路上的一个重要里程碑，也是未来发展的指标。您也可以在此处了解有关NLP 的更多信息。