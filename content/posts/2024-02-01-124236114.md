---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33343132303031352f:61727469636c652f64657461696c732f313234323336313134"
layout: post
title: "超参数调优HPO网格搜索随机搜索-对半网格搜索贝叶斯优化算法"
date: 2024-02-01 10:55:14 +08:00
description: "禁止转载，谢谢！当代超参数优化算法主要可以分为：基于网格的各类搜索（Grid）基于贝叶斯优化的各类优"
keywords: "hpo62220502000008,谢谢"
categories: ['机器学习']
tags: ['机器学习']
artid: "124236114"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=124236114
    alt: "超参数调优HPO网格搜索随机搜索-对半网格搜索贝叶斯优化算法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=124236114
featuredImagePreview: https://bing.ee123.net/img/rand?artid=124236114
---

# 超参数调优HPO：网格搜索、随机搜索、 对半网格搜索、贝叶斯优化算法

> 禁止转载，谢谢！

当代超参数优化算法主要可以分为：

* 基于网格的各类搜索（Grid）
* 基于贝叶斯优化的各类优化算法（Baysian）
* 基于梯度的各类优化（Gradient-based）
* 基于种群的各类优化（进化算法，遗传算法等）

###### 1、网格搜索gridsearch（简单且广泛）

* 通过查找搜索范围内的所有的点来确定最优值**：指的是将备选的参数一一列出，多个不同参数的不同取值最终将组成一个参数空间（parameter space），在这个参数空间中选取不同的值带入模型进行训练，最终选取一组最优的值作为模型的最终超参数。
* 网格搜索的理论极限与缺点**：若采用较大的搜索范围以及较小的步长，网格搜索有很大的概率找到全局最优值。但是，参数空间越大，网格搜索所需的算力和时间也会越大，当参数维度上升时，网格搜索所需的计算量更是程指数级上升的。

###### 2、随机网格搜索RandomizedSearchCV：

* 随机抽取参数子空间并在子空间中进行搜索的方法（调整搜索空间）：可以控制随机网格搜索的迭代次数，来控制整体被抽出的参数子空间的大小。提升了运算速度，又没有过多地伤害搜索的精度。
* 随机网格搜索的理论极限：

> 为什么缩小参数空间之后，又没有过多地削弱搜索的精度。
>
> * 抽样出的子空间可以一定程度上反馈出全域空间的分布，且子空间相对越大（含有的参数组合数越多），子空间的分布越接近全域空间的分布。
> * 当全域空间本身足够密集时，很小的子空间也能获得与全域空间相似的分布。
> * 如果全域空间包括了理论上的损失函数最小值，那一个与全域空间分布高度相似的子空间很可能也包括损失函数的最小值，或包括非常接近最小值的一系列次小值。

因此，只要子空间足够大，随机网格搜索的效果一定是高度逼近枚举网格搜索的。

###### 3、对半网格搜索HalvingSearchCV

未完