---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34313832363231352f:61727469636c652f64657461696c732f313436313738313032"
layout: post
title: "大数据面试之路-二-hive小文件合并优化方法"
date: 2025-03-11 15:30:57 +08:00
description: "大量小文件容易在文件存储端造成瓶颈，影响处理效率。对此，您可以通过合并Map和Reduce的结果文件来处理。"
keywords: "hive insert overwrite 合并文件"
categories: ['大数据']
tags: ['大数据', 'Hive', 'Hadoop']
artid: "146178102"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146178102
    alt: "大数据面试之路-二-hive小文件合并优化方法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146178102
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146178102
cover: https://bing.ee123.net/img/rand?artid=146178102
image: https://bing.ee123.net/img/rand?artid=146178102
img: https://bing.ee123.net/img/rand?artid=146178102
---

# 大数据面试之路 (二) hive小文件合并优化方法

大量小文件容易在文件存储端造成瓶颈，影响处理效率。对此，您可以通过合并Map和Reduce的结果文件来处理。

## **一、合并小文件的常见场景**

1. **写入时产生小文件**
   ：Reduce任务过多或数据量过小，导致每个任务输出一个小文件。
2. **动态分区插入**
   ：分区字段基数高，每个分区生成少量数据，形成大量小文件。
3. **频繁追加数据**
   ：通过
   `INSERT INTO`
   多次追加数据，导致文件碎片化。

## **二、合并小文件的核心方法**

### **方法1：调整Reduce任务数量**

> -- 1. 设置Reduce任务数（根据数据量调整）
>   
>
> SET hive.exec.reducers.bytes.per.reducer=256000000; -- 每个Reduce处理256MB数据
>   
>
> SET hive.exec.reducers.max=1009; -- Reduce最大数量
>
> -- 2. 执行插入操作（自动合并到指定Reduce数）
>   
>
> INSERT OVERWRITE TABLE target_table
>   
>
> SELECT * FROM source_table;

### **方法2：启用Hive自动合并**

> -- 启用Map端和Reduce端小文件合并
>   
> SET hive.merge.mapfiles = true;          -- Map-only任务结束时合并小文件
>   
> SET hive.merge.mapredfiles = true;       -- Map-Reduce任务结束时合并小文件
>   
> SET hive.merge.size.per.task = 256000000; -- 合并后文件目标大小（256MB）
>   
> SET hive.merge.smallfiles.avgsize = 16000000; -- 平均文件小于16MB时触发合并
>
> -- 执行插入操作（自动合并）
>   
> INSERT OVERWRITE TABLE target_table
>   
> SELECT * FROM source_table;

### **方法3：使用 `ALTER TABLE ... CONCATENATE` （ORC格式专用）**

> -- 合并表或分区的ORC文件
>   
> ALTER TABLE table_name [PARTITION (partition_key='value')] CONCATENATE;

### **方法4：重写数据（通用）**

通过
`INSERT OVERWRITE`
重新写入数据，强制合并文件：

> -- 1. 将数据覆盖写入原表（自动合并）
>   
> INSERT OVERWRITE TABLE target_table
>   
> SELECT * FROM target_table;
>
> -- 2. 写入新表后替换旧表
>   
> CREATE TABLE new_table AS SELECT * FROM old_table;
>   
> DROP TABLE old_table;
>   
> ALTER TABLE new_table RENAME TO old_table;

### **方法5：使用Hadoop命令合并（手动操作）**

合并HDFS上已有的小文件（需谨慎操作）：

> # 1. 合并HDFS文件到本地（合并后需重新加载）
>   
> hadoop fs -getmerge /user/hive/warehouse/table_dir/* merged_file.txt
>   
> hadoop fs -put merged_file.txt /user/hive/warehouse/table_dir/
>
> # 2. 使用Hive的`hadoop jar`命令合并（针对特定格式）
>   
> hadoop jar $HIVE_HOME/lib/hive-exec.jar -Dmapreduce.job.queuename=default \
>   
> -Dmapreduce.map.memory.mb=2048 \
>   
> org.apache.hadoop.hive.ql.io.HiveFileFormatUtils \
>   
> --combine /user/hive/warehouse/table_dir/ /user/hive/warehouse/table_dir_merged/

## **三、动态分区场景下的优化**

若使用动态分区（如按天、按用户ID分区），需额外配置：

> -- 启用动态分区模式
>   
> SET hive.exec.dynamic.partition = true;
>   
> SET hive.exec.dynamic.partition.mode = nonstrict;
>
> -- 设置每个Reduce任务写入的分区数
>   
> SET hive.optimize.sort.dynamic.partition = true;
>   
> SET hive.exec.max.dynamic.partitions = 1000;
>   
> SET hive.exec.max.dynamic.partitions.pernode = 100;

## **四、不同文件格式的注意事项**

| 文件格式 | 合并特点 |
| --- | --- |
| **Text** | 需通过重写数据或Hadoop命令合并。 |
| **ORC** | 支持 `ALTER TABLE ... CONCATENATE` 快速合并，或重写数据。 |
| **Parquet** | 只能通过重写数据合并（如 `INSERT OVERWRITE` ）。 |
| **RCFile** | 类似ORC，但无专用合并命令，需重写数据。 |

## **五、最佳实践**

1. ### **写入时预防** ：

   * 合理设置Reduce任务数，避免过度并行化。
   * 启用
     `hive.merge`
     参数自动合并小文件。
2. ### **事后合并** ：

   * ORC表优先使用
     `ALTER TABLE ... CONCATENATE`
     。
   * 其他格式通过
     `INSERT OVERWRITE`
     重写数据。
3. ### **分区管理** ：

   * 避免过多细粒度分区，定期清理过期数据。

**示例：合并ORC表文件**

> -- 1. 检查表格式
>   
> DESCRIBE FORMATTED table_name;
>
> -- 2. 合并文件（ORC格式）
>   
> ALTER TABLE table_name CONCATENATE;
>
> -- 3. 验证合并后文件大小
>   
> hadoop fs -du -h /user/hive/warehouse/table_dir;

## 如何调优Hive作业

更多内容请参考 案例云帮助文档

[如何调优Hive作业_开源大数据平台 E-MapReduce(EMR)-阿里云帮助中心](https://help.aliyun.com/zh/emr/emr-on-ecs/user-guide/optimize-hive-jobs?spm=5176.21213303.J_v8LsmxMG6alneH-O7TCPa.16.216f2f3dlTQifS&scm=20140722.S_help@@%E6%96%87%E6%A1%A3@@437605._.ID_help@@%E6%96%87%E6%A1%A3@@437605-RL_%E8%B0%83%E4%BC%98-LOC_2024SPHelpResult-OR_ser-PAR1_2150448017394070838114102e7987-V_4-RE_new4-P0_2-P1_0 "如何调优Hive作业_开源大数据平台 E-MapReduce(EMR)-阿里云帮助中心")