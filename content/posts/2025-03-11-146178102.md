---
layout: post
title: "大数据面试之路-二-hive小文件合并优化方法"
date: 2025-03-11 15:30:57 +0800
description: "大量小文件容易在文件存储端造成瓶颈，影响处理效率。对此，您可以通过合并Map和Reduce的结果文件来处理。"
keywords: "大数据面试之路 (二) hive小文件合并优化方法"
categories: ['大数据']
tags: ['大数据', 'Hive', 'Hadoop']
artid: "146178102"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146178102
    alt: "大数据面试之路-二-hive小文件合并优化方法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146178102
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146178102
cover: https://bing.ee123.net/img/rand?artid=146178102
image: https://bing.ee123.net/img/rand?artid=146178102
img: https://bing.ee123.net/img/rand?artid=146178102
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     大数据面试之路 (二) hive小文件合并优化方法
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     大量小文件容易在文件存储端造成瓶颈，影响处理效率。对此，您可以通过合并Map和Reduce的结果文件来处理。
    </p>
    <h2>
     <strong>
      一、合并小文件的常见场景
     </strong>
    </h2>
    <ol>
     <li>
      <p>
       <strong>
        写入时产生小文件
       </strong>
       ：Reduce任务过多或数据量过小，导致每个任务输出一个小文件。
      </p>
     </li>
     <li>
      <p>
       <strong>
        动态分区插入
       </strong>
       ：分区字段基数高，每个分区生成少量数据，形成大量小文件。
      </p>
     </li>
     <li>
      <p>
       <strong>
        频繁追加数据
       </strong>
       ：通过
       <code>
        INSERT INTO
       </code>
       多次追加数据，导致文件碎片化。
      </p>
     </li>
    </ol>
    <h2>
     <strong>
      二、合并小文件的核心方法
     </strong>
    </h2>
    <h3>
     <strong>
      方法1：调整Reduce任务数量
     </strong>
    </h3>
    <blockquote>
     <p>
      <span style="background-color:null">
       -- 1. 设置Reduce任务数（根据数据量调整）
      </span>
      <br/>
      <span style="background-color:null">
       SET hive.exec.reducers.bytes.per.reducer=256000000; -- 每个Reduce处理256MB数据
      </span>
      <br/>
      <span style="background-color:null">
       SET hive.exec.reducers.max=1009; -- Reduce最大数量
      </span>
     </p>
     <p>
      <span style="background-color:null">
       -- 2. 执行插入操作（自动合并到指定Reduce数）
      </span>
      <br/>
      <span style="background-color:null">
       INSERT OVERWRITE TABLE target_table
      </span>
      <br/>
      <span style="background-color:null">
       SELECT * FROM source_table;
      </span>
     </p>
    </blockquote>
    <h3>
     <strong>
      方法2：启用Hive自动合并
     </strong>
    </h3>
    <blockquote>
     <p>
      -- 启用Map端和Reduce端小文件合并
      <br/>
      SET hive.merge.mapfiles = true;          -- Map-only任务结束时合并小文件
      <br/>
      SET hive.merge.mapredfiles = true;       -- Map-Reduce任务结束时合并小文件
      <br/>
      SET hive.merge.size.per.task = 256000000; -- 合并后文件目标大小（256MB）
      <br/>
      SET hive.merge.smallfiles.avgsize = 16000000; -- 平均文件小于16MB时触发合并
     </p>
     <p>
      -- 执行插入操作（自动合并）
      <br/>
      INSERT OVERWRITE TABLE target_table
      <br/>
      SELECT * FROM source_table;
     </p>
    </blockquote>
    <h3>
     <strong>
      方法3：使用
      <code>
       ALTER TABLE ... CONCATENATE
      </code>
      （ORC格式专用）
     </strong>
    </h3>
    <blockquote>
     <p>
      -- 合并表或分区的ORC文件
      <br/>
      ALTER TABLE table_name [PARTITION (partition_key='value')] CONCATENATE;
     </p>
    </blockquote>
    <h3>
     <strong>
      方法4：重写数据（通用）
     </strong>
    </h3>
    <p>
     通过
     <code>
      INSERT OVERWRITE
     </code>
     重新写入数据，强制合并文件：
    </p>
    <blockquote>
     <p>
      -- 1. 将数据覆盖写入原表（自动合并）
      <br/>
      INSERT OVERWRITE TABLE target_table
      <br/>
      SELECT * FROM target_table;
     </p>
     <p>
      -- 2. 写入新表后替换旧表
      <br/>
      CREATE TABLE new_table AS SELECT * FROM old_table;
      <br/>
      DROP TABLE old_table;
      <br/>
      ALTER TABLE new_table RENAME TO old_table;
     </p>
    </blockquote>
    <h3>
     <strong>
      方法5：使用Hadoop命令合并（手动操作）
     </strong>
    </h3>
    <p>
     合并HDFS上已有的小文件（需谨慎操作）：
    </p>
    <p>
    </p>
    <blockquote>
     <p>
      # 1. 合并HDFS文件到本地（合并后需重新加载）
      <br/>
      hadoop fs -getmerge /user/hive/warehouse/table_dir/* merged_file.txt
      <br/>
      hadoop fs -put merged_file.txt /user/hive/warehouse/table_dir/
     </p>
     <p>
      # 2. 使用Hive的`hadoop jar`命令合并（针对特定格式）
      <br/>
      hadoop jar $HIVE_HOME/lib/hive-exec.jar -Dmapreduce.job.queuename=default \
      <br/>
      -Dmapreduce.map.memory.mb=2048 \
      <br/>
      org.apache.hadoop.hive.ql.io.HiveFileFormatUtils \
      <br/>
      --combine /user/hive/warehouse/table_dir/ /user/hive/warehouse/table_dir_merged/
     </p>
    </blockquote>
    <h2>
     <strong>
      三、动态分区场景下的优化
     </strong>
    </h2>
    <p>
     若使用动态分区（如按天、按用户ID分区），需额外配置：
    </p>
    <blockquote>
     <p>
      -- 启用动态分区模式
      <br/>
      SET hive.exec.dynamic.partition = true;
      <br/>
      SET hive.exec.dynamic.partition.mode = nonstrict;
     </p>
     <p>
      -- 设置每个Reduce任务写入的分区数
      <br/>
      SET hive.optimize.sort.dynamic.partition = true;
      <br/>
      SET hive.exec.max.dynamic.partitions = 1000;
      <br/>
      SET hive.exec.max.dynamic.partitions.pernode = 100;
     </p>
    </blockquote>
    <h2>
     <strong>
      四、不同文件格式的注意事项
     </strong>
    </h2>
    <p>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        文件格式
       </th>
       <th>
        合并特点
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         Text
        </strong>
       </td>
       <td>
        需通过重写数据或Hadoop命令合并。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         ORC
        </strong>
       </td>
       <td>
        支持
        <code>
         ALTER TABLE ... CONCATENATE
        </code>
        快速合并，或重写数据。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         Parquet
        </strong>
       </td>
       <td>
        只能通过重写数据合并（如
        <code>
         INSERT OVERWRITE
        </code>
        ）。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         RCFile
        </strong>
       </td>
       <td>
        类似ORC，但无专用合并命令，需重写数据。
       </td>
      </tr>
     </tbody>
    </table>
    <h2>
     <strong>
      五、最佳实践
     </strong>
    </h2>
    <ol>
     <li>
      <h3>
       <strong>
        写入时预防
       </strong>
       ：
      </h3>
      <ul>
       <li>
        <p>
         合理设置Reduce任务数，避免过度并行化。
        </p>
       </li>
       <li>
        <p>
         启用
         <code>
          hive.merge
         </code>
         参数自动合并小文件。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <h3>
       <strong>
        事后合并
       </strong>
       ：
      </h3>
      <ul>
       <li>
        <p>
         ORC表优先使用
         <code>
          ALTER TABLE ... CONCATENATE
         </code>
         。
        </p>
       </li>
       <li>
        <p>
         其他格式通过
         <code>
          INSERT OVERWRITE
         </code>
         重写数据。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <h3>
       <strong>
        分区管理
       </strong>
       ：
      </h3>
      <ul>
       <li>
        <p>
         避免过多细粒度分区，定期清理过期数据。
        </p>
       </li>
      </ul>
     </li>
    </ol>
    <p>
     <strong>
      示例：合并ORC表文件
     </strong>
    </p>
    <blockquote>
     <p>
      -- 1. 检查表格式
      <br/>
      DESCRIBE FORMATTED table_name;
     </p>
     <p>
      -- 2. 合并文件（ORC格式）
      <br/>
      ALTER TABLE table_name CONCATENATE;
     </p>
     <p>
      -- 3. 验证合并后文件大小
      <br/>
      hadoop fs -du -h /user/hive/warehouse/table_dir;
     </p>
    </blockquote>
    <h2>
     如何调优Hive作业
    </h2>
    <p>
     更多内容请参考 案例云帮助文档
    </p>
    <p>
     <a href="https://help.aliyun.com/zh/emr/emr-on-ecs/user-guide/optimize-hive-jobs?spm=5176.21213303.J_v8LsmxMG6alneH-O7TCPa.16.216f2f3dlTQifS&amp;scm=20140722.S_help@@%E6%96%87%E6%A1%A3@@437605._.ID_help@@%E6%96%87%E6%A1%A3@@437605-RL_%E8%B0%83%E4%BC%98-LOC_2024SPHelpResult-OR_ser-PAR1_2150448017394070838114102e7987-V_4-RE_new4-P0_2-P1_0" rel="nofollow" title="如何调优Hive作业_开源大数据平台 E-MapReduce(EMR)-阿里云帮助中心">
      如何调优Hive作业_开源大数据平台 E-MapReduce(EMR)-阿里云帮助中心
     </a>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34313832363231352f:61727469636c652f64657461696c732f313436313738313032" class_="artid" style="display:none">
 </p>
</div>


