---
layout: post
title: "Spark-eventlog"
date: 2025-03-12 15:37:53 +0800
description: "【代码】Spark eventlog。"
keywords: "Spark eventlog"
categories: ['未分类']
tags: ['大数据', '分布式', 'Spark']
artid: "146206728"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146206728
    alt: "Spark-eventlog"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146206728
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146206728
cover: https://bing.ee123.net/img/rand?artid=146206728
image: https://bing.ee123.net/img/rand?artid=146206728
img: https://bing.ee123.net/img/rand?artid=146206728
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Spark eventlog
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-github-gist" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h5>
     <a id="Eventlog__2">
     </a>
     Eventlog 示例
    </h5>
    <pre><code>{
    "Event": "org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart",
    "executionId": 0,
    "rootExecutionId": 0,
    "desc ription": "select round(a, 2), a from double_table",
    "details": "org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMetho    dAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\njava.lang.reflect.Method.invoke(Method.java:498)\norg.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\norg.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain    (SparkSubmit.scala:1029)\norg.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\norg.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\norg.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\norg.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\norg.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\norg.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",
    "physicalPlanDescription": "==   Physical Plan ==\n* Project (3)\n+- * ColumnarToRow (2)\n   +- Scan parquet spark_catalog.default.double_table (1)\n    \n\n(1) Scan parquet spark_catalog.default.double_table\nOutput [1]: [a#0]\nBatched: true\nLocation: InMemoryFileInde    x [file:/home/hadoop/files/double_table]\nReadSchema: struct&lt;a:double&gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [1]: [a#0]\n\n(3) Project [codegen id : 1]\nOutput [2]: [round(a#0, 2) AS round(a, 2)#1, a#0]\nInput [1]: [a#0]\n\n",
    " sparkPlanInfo": {
        "nodeName": "WholeStageCodegen (1)",
        "simpleString": "WholeStageCodegen (1)",
        "children": [
            {
                "nodeName": "Project",
                "simpleString": "Project [round(a#0, 2) AS round(a, 2)#1, a#0]",
                "children": [
                    {
                        "nodeName": "ColumnarToRow",
                        "simple String": "ColumnarToRow",
                        "children": [
                            {
                                "nodeName": "InputAdapter",
                                "simpleString": "InputAdapter",
                                "children": [
                                    {
                                        "nodeName": "Scan parquet spark_catalog.default.double_table",
                                        "simpleString": "FileScan parquet spark_catalog.default.double_table    [a#0] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/files/d    ouble_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;a:double&gt;",
                                        "children": [

                                        ],
                                        "metadata": {
                                            "Locat ion": "InMemoryFileIndex(1 paths)[file:/home/hadoop/files/double_table]",
                                            "ReadSchema": "struct&lt;a:double&gt;",
                                            "Format": "Par    quet",
                                            "Batched": "true",
                                            "PartitionFilters": "[]",
                                            "PushedFilters": "[]",
                                            "DataFilters": "[]"
                                        },
                                        "metrics": [
                                            {
                                                "name": "number of     files read",
                                                "accumulatorId": 5,
                                                "metricType": "sum"
                                            },
                                            {
                                                "name": "scan time",
                                                "accumulatorId": 4,
                                                "metricType": "timing"
                                            },
                                            {
                                                "nam e": "metadata time",
                                                "accumulatorId": 6,
                                                "metricType": "timing"
                                            },
                                            {
                                                "name": "size of files read",
                                                "accumulatorId": 7,
                                                "metricTyp e": "size"
                                            },
                                            {
                                                "name": "number of output rows",
                                                "accumulatorId": 3,
                                                "metricType": "sum"
                                            }
                                        ]
                                    }
                                ],
                                "metadata": {

                                },
                                "metrics": [

                                ]
                            }
                        ],
                        "met adata": {

                        },
                        "metrics": [
                            {
                                "name": "number of output rows",
                                "accumulatorId": 1,
                                "metricType": "sum"
                            },
                            {
                                "name": "number of input b    atches",
                                "accumulatorId": 2,
                                "metricType": "sum"
                            }
                        ]
                    }
                ],
                "metadata": {

                },
                "metrics": [

                ]
            }
        ],
        "metadata": {

        },
        "metrics": [
            {
                "name": "durat    ion",
                "accumulatorId": 0,
                "metricType": "timing"
            }
        ]
    },
    "time": 1741661558528,
    "modifiedConfigs": {

    },
    "jobTags": [

    ]
}

</code></pre>
    <pre><code>== Physical Plan ==
* Project (3)
+- * ColumnarToRow (2)
   +- Scan parquet spark_catalog.default.double_table (1)
</code></pre>
    <p>
     对应于
    </p>
    <pre><code>==     Physical Plan ==\n* Project (3)\n+- * ColumnarToRow (2)\n   +- Scan parquet spark_catalog.default.double_table (1)\n    \n\n
</code></pre>
    <pre><code>(1) Scan parquet spark_catalog.default.double_table
Output [1]: [a#0]
Batched: true
Location: InMemoryFileIndex [file:/home/hadoop/files/double_table]
ReadSchema: struct&lt;a:double&gt;

(2) ColumnarToRow [codegen id : 1]
Input [1]: [a#0]

(3) Project [codegen id : 1]
Output [2]: [round(a#0, 2) AS round(a, 2)#1, a#0]
Input [1]: [a#0]

</code></pre>
    <p>
     对应于
    </p>
    <pre><code> "physicalPlanDescription": "==     Physical Plan ==\n* Project (3)\n+- * ColumnarToRow (2)\n   +- Scan parquet spark_catalog.default.double_table (1)\n    \n\n(1) Scan parquet spark_catalog.default.double_table\nOutput [1]: [a#0]\nBatched: true\nLocation: InMemoryFileIndex [file:/home/hadoop/files/double_table]\nReadSchema: struct&lt;a:double&gt;\n\n(2) ColumnarToRow [codegen id : 1]\nInput [1]: [a#0]\n\n(3) Project [codegen id : 1]\nOutput [2]: [round(a#0, 2) AS round(a, 2)#1, a#0]\nInput [1]: [a#0]\n\n",
</code></pre>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e63:73646e2e6e65742f7a686978696e67686579695f7469616e2f:61727469636c652f64657461696c732f313436323036373238" class_="artid" style="display:none">
 </p>
</div>


