---
layout: post
title: "CVPR-2025-长程视觉语言导航平台与数据集迈向复杂环境中的智能机器人"
date: 2025-03-15 19:59:39 +0800
description: "近日，中山大学HCP-Lab团队提出复杂长程视觉语言导航（LH-VLN）任务，并配套开发了自动化数据生成平台NavGen、复杂长程导航基准测试LHPR-VLN，以及创新模型MGDM，为智能机器人在动态复杂环境中的自主导航开辟了新路径。目前该论文已被CVPR2025接收。"
keywords: "CVPR-2025 | 长程视觉语言导航平台与数据集：迈向复杂环境中的智能机器人"
categories: ['Vln']
tags: ['深度学习', '具身智能', '人工智能']
artid: "146284195"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146284195
    alt: "CVPR-2025-长程视觉语言导航平台与数据集迈向复杂环境中的智能机器人"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146284195
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146284195
cover: https://bing.ee123.net/img/rand?artid=146284195
image: https://bing.ee123.net/img/rand?artid=146284195
img: https://bing.ee123.net/img/rand?artid=146284195
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     CVPR-2025 | 长程视觉语言导航平台与数据集：迈向复杂环境中的智能机器人
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="57" src="https://i-blog.csdnimg.cn/img_convert/91b1f8d45169c261a40d124a611dc6ee.jpeg" width="359"/>
    </p>
    <ul>
     <li>
      <p>
       作者：Xinshuai Song, Weixing Chen, Yang Liu, Weikai Chen, Guanbin Li, Liang Lin
      </p>
     </li>
     <li>
      <p>
       单位：中山大学，Independent Researcher，鹏城实验室
      </p>
     </li>
     <li>
      <p>
       项目主页：https://hcplab-sysu.github.io/LH-VLN
      </p>
     </li>
     <li>
      <p>
       论文地址：https://arxiv.org/pdf/2412.09082
      </p>
     </li>
    </ul>
    <p>
     从“拿毛巾到厨房岛台，再取茶壶放到茶几”到“找到客厅的遥控器后去卧室关灯”，现实中的机器人需要完成的往往是包含多个步骤的长链条任务。然而，现有的视觉语言导航（Vision-Language Navigation, VLN）技术大多局限于单一目标、短路径的简单场景，难以应对复杂环境中的多阶段挑战。
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="137" src="https://i-blog.csdnimg.cn/img_convert/88a9db3b63d1a5708bcdd618566cd460.png" width="514"/>
    </p>
    <p>
     近日，中山大学HCP-Lab团队提出
     <strong>
      复杂长程视觉语言导航（LH-VLN）任务，并配套开发了自动化数据生成平台NavGen
     </strong>
     、复杂长程导航基准测试
     <strong>
      LHPR-VLN
     </strong>
     ，以及创新模型
     <strong>
      MGDM
     </strong>
     ，为智能机器人在动态复杂环境中的自主导航开辟了新路径。目前该论文已被CVPR2025接收。
    </p>
    <h3>
     困境：单阶段导航的“玻璃天花板”
    </h3>
    <p>
     传统VLN任务通常要求机器人根据指令完成单一目标的导航，例如“走到客厅的沙发旁”。这类任务在实验室中表现优异，但面对现实场景时却捉襟见肘——真正的挑战往往需要
     <strong>
      连续决策
     </strong>
     和
     <strong>
      动态调整
     </strong>
     。例如，家政机器人可能需要先找到浴室中的毛巾，将其送至厨房岛台，再取出茶壶放置在茶几上。这类任务不仅涉及多个子目标，还需要在过程中保持上下文连贯性，避免因环境变化或路径阻塞导致任务中断。
    </p>
    <p>
     现有研究的短板显而易见：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        数据局限
       </strong>
       ：主流数据集（如R2R、VLN-CE）任务步骤短（平均&lt;10步），缺乏多阶段交互设计；
      </p>
     </li>
     <li>
      <p>
       <strong>
        评估粗放
       </strong>
       ：仅用整体成功率（SR）衡量性能，无法反映子任务执行质量；
      </p>
     </li>
     <li>
      <p>
       <strong>
        模型僵化
       </strong>
       ：依赖静态路径规划，缺乏长期记忆和动态调整能力。
      </p>
     </li>
    </ul>
    <p>
     <strong>
      “要让机器人真正走进家庭，必须突破单阶段任务的思维定式。”
     </strong>
     论文作者在引言中直指问题核心。
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="365" src="https://i-blog.csdnimg.cn/img_convert/0cac0dcfd3d9a877a58d6c09697e17e9.png" width="865"/>
    </p>
    <p style="text-align:center">
     <span style="color:#1c7331">
      图1. 框架总览以及与现有单阶段导航的对比
     </span>
    </p>
    <h3>
     破局：NavGen——复杂任务数据的“全自动工厂”
    </h3>
    <p>
     为解决数据瓶颈，研究团队开发了
     <strong>
      NavGen平台
     </strong>
     ，这是一个支持
     <strong>
      多阶段、多粒度
     </strong>
     任务生成的自动化系统。其核心创新在于
     <strong>
      双向生成机制
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        前向生成
       </strong>
       ：基于GPT-4构建复杂任务指令。例如，输入浴室和厨房的场景信息后，自动生成“将浴室毛巾送至厨房岛台，再取茶壶放到客厅茶几”的多步骤任务；
      </p>
     </li>
     <li>
      <p>
       <strong>
        后向分解
       </strong>
       ：通过轨迹分割算法，将长路径拆解为“左转绕过沙发”“直行至餐桌”等原子动作，并反向生成对应的分步指令。
      </p>
     </li>
    </ul>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="265" src="https://i-blog.csdnimg.cn/img_convert/1217a6e4967605a8f71b248a8c69cc1f.png" width="865"/>
    </p>
    <p style="text-align:center">
     <span style="color:#1c7331">
      图2. NavGen通过前向生成复杂任务，后向分解为原子动作，形成完整数据闭环
     </span>
    </p>
    <p>
     NavGen的三大优势使其成为VLN领域的“数据引擎”：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        场景多样性
       </strong>
       ：整合HM3D数据集中的216个3D室内场景，涵盖卧室、厨房、办公室等多种环境；
      </p>
     </li>
     <li>
      <p>
       <strong>
        机器人适配
       </strong>
       ：支持波士顿动力Spot（四足机器人）和Hello Robot Stretch（轮式机械臂）等不同形态的任务设置；
      </p>
     </li>
     <li>
      <p>
       <strong>
        任务复杂度
       </strong>
       ：单个任务可包含4-6个子步骤，平均指令长度达18.17词，远超传统数据集。
      </p>
     </li>
    </ul>
    <p>
     “
     <strong>
      这相当于为模型提供了‘任务炼狱’级别的训练场
     </strong>
     。” 研究者如此评价NavGen的生成能力。
    </p>
    <h3>
     试金石：LHPR-VLN基准——让模型“原形毕露”
    </h3>
    <p>
     基于NavGen，团队构建了复杂长程VLN基准LHPR-VLN，包含3260个任务，平均每个复杂任务需执行150个动作步骤。与传统基准相比，LHPR-VLN有两大革新：
    </p>
    <h4>
     1. 任务设计：从“线性执行”到“逻辑串联”
    </h4>
    <p>
     每个任务要求机器人按顺序完成
     <strong>
      对象定位-抓取-转移
     </strong>
     的链条操作。例如： “在卧室找到台灯，将其搬到书房书桌，再取出桌上的文件放到文件柜。” 这种设计迫使模型必须理解任务间的逻辑依赖——若未能正确放置台灯，后续寻找文件的子任务将直接失败。
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="171" src="https://i-blog.csdnimg.cn/img_convert/b64ce1867a757f26e74b0f5aa0ad5611.png" width="865"/>
    </p>
    <p style="text-align:center">
     <span style="color:#1c7331">
      表1. 与现有VLN基准的对比
     </span>
    </p>
    <h4>
     2. 评估体系：从“笼统打分”到“显微镜式诊断”
    </h4>
    <p>
     传统指标如成功率（SR）已无法满足需求，LHPR-VLN引入三大新指标：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        独立成功率（ISR）
       </strong>
       ：衡量每个子任务的单独完成度；
      </p>
     </li>
     <li>
      <p>
       <strong>
        条件成功率（CSR）
       </strong>
       ：评估任务链条的整体连贯性；
      </p>
     </li>
     <li>
      <p>
       <strong>
        基于真实路径加权的CGT
       </strong>
       ：考虑实际路径难度，避免“取巧式”成功。
      </p>
     </li>
    </ul>
    <p>
     是任务的数量， 是子任务的数量。
    </p>
    <p>
     是第个子任务的成功情况。
    </p>
    <p>
     CSR通过加权计算任务链的连贯性，CGT进一步引入真实路径长度修正偏差
    </p>
    <p>
     实验显示，传统模型在LHPR-VLN上表现惨淡：在2-3个子任务场景中，所有基线模型的整体成功率（SR）均为0%，凸显现有技术的局限性。
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="142" src="https://i-blog.csdnimg.cn/img_convert/b1f7a2e4309b5a4ad65977cf4d566c77.png" width="865"/>
    </p>
    <p style="text-align:center">
     <span style="color:#1c7331">
      表2. 在LHPR-VLN基准上的性能对比
     </span>
    </p>
    <h3>
     智慧大脑：MGDM模型——记忆与推理的“双螺旋”
    </h3>
    <p>
     为攻克复杂长程导航难题，团队提出
     <strong>
      多粒度动态记忆模型（MGDM）
     </strong>
     ，其核心架构如同“生物神经系统”：
    </p>
    <ul>
     <li>
      <strong>
       记忆分层：短期模糊与长期强化
      </strong>
      <ul>
       <li>
        <p>
         <strong>
          短期记忆
         </strong>
         ：通过滑动窗口池化动态“遗忘”次要信息；
        </p>
       </li>
       <li>
        <p>
         <strong>
          长期记忆
         </strong>
         ：从数据集中检索历史成功案例，为当前决策提供参考。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <strong>
       链式思维（CoT）反馈：让AI“说出推理过程”
      </strong>
      <ul>
       <li>
        <p>
         模型在一定行动步，会通过GPT-4生成
         <strong>
          推理链条
         </strong>
         ：“当前位于走廊，需先左转进入浴室；浴室门可能位于左侧视野，需向前移动2步确认...” 这种显式推理机制大幅降低了传统LLM模型的“幻觉”风险，使决策过程可解释、可调整。
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="418" src="https://i-blog.csdnimg.cn/img_convert/c6ebfd3dd60fe84ae0472920081cb06d.png" width="865"/>
    </p>
    <p style="text-align:center">
     <span style="color:#1c7331">
      图3. MGDM通过CoT模块生成推理链条，结合短/长期记忆动态调整决策
     </span>
    </p>
    <p>
     实验结果表2验证了MGDM的优越性：在4个子任务场景中，其CGT指标达到5.83，全部模型中最佳。
    </p>
    <h3>
     未来：从虚拟场景到现实世界的“惊险一跃”
    </h3>
    <p>
     尽管LH-VLN框架取得突破，研究者坦言
     <strong>
      现实落地仍面临三重挑战
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        跨场景泛化
       </strong>
       ：实验室训练的模型能否适应真实家庭的布局变异？
      </p>
     </li>
     <li>
      <p>
       <strong>
        多模态融合
       </strong>
       ：如何整合语音指令、触觉反馈等更丰富的信息源？
      </p>
     </li>
     <li>
      <p>
       <strong>
        实时性瓶颈
       </strong>
       ：150步任务的平均决策耗时需从分钟级压缩至秒级。
      </p>
     </li>
    </ul>
    <p>
     对此，论文提出两条演进路径：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        仿真-现实迁移学习
       </strong>
       ：利用Holodeck等工具生成高保真虚拟环境，缩小仿真与现实差距；
      </p>
     </li>
     <li>
      <p>
       <strong>
        具身大模型
       </strong>
       ：将VLM（视觉语言模型）与机器人运动控制模块深度耦合，实现端到端优化。
      </p>
     </li>
    </ul>
    <h3>
     结语：推开智能机器人的“第二扇门”
    </h3>
    <p>
     当实验室的机器人能流畅完成“泡茶-清洁-整理”的连贯操作时，我们离真正的家庭服务机器人便不再遥远。这项研究的意义不仅在于技术指标的提升，更在于
     <strong>
      重构了VLN任务的范式
     </strong>
     ——从孤立动作到连续决策，从静态环境到动态交互，从人工规则到自主推理。
    </p>
    <p>
     正如论文结尾的展望：“LH-VLN是一把钥匙，它将打开智能体在复杂物理世界中长期生存的大门。”在这条通向未来的道路上，每一步导航的突破，都是对人类生活方式的重新定义。
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="53" src="https://i-blog.csdnimg.cn/img_convert/9850486b286791b84ea1f7aa85b4e88a.jpeg" width="334"/>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f33373939303138362f:61727469636c652f64657461696c732f313436323834313935" class_="artid" style="display:none">
 </p>
</div>


