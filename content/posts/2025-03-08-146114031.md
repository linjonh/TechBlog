---
layout: post
title: "模型微调-基于LLaMA-Factory进行微调的一个简单案例"
date: 2025-03-08 12:30:18 +0800
description: "接下来就可以通过 LLaMa-Factory 进行微调了，这里先做一个简单的演示，为了不让这篇文章篇幅过长，先不具体讲解各种微调参数的含义和作用，数据集也只使用 LLaMa-Factory 自带的示例数据集，演示一下对模型认知设定的微调。需要注意的是，加载本地模型的时候，需要修改填写模型本地路径，这里的路径是模型快照的唯一哈希值，而不是模型文件夹的路径。之后就可通过和模型进行对话，测试下载下来的模型是否正常了，也可以看下对话中模型输出的风格，和我们微调之后的做下对比。"
keywords: "模型微调-基于LLaMA-Factory进行微调的一个简单案例"
categories: ['Ai']
tags: ['Llama']
artid: "146114031"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146114031
    alt: "模型微调-基于LLaMA-Factory进行微调的一个简单案例"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146114031
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146114031
cover: https://bing.ee123.net/img/rand?artid=146114031
image: https://bing.ee123.net/img/rand?artid=146114031
img: https://bing.ee123.net/img/rand?artid=146114031
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     模型微调-基于LLaMA-Factory进行微调的一个简单案例
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <p>
    </p>
    <h2>
     <a id="1__2">
     </a>
     1. 租用云计算资源
    </h2>
    <p>
     以下示例基于
     <a href="https://www.autodl.com/console" rel="nofollow">
      AutoDL
     </a>
     云计算资源。
    </p>
    <p>
     在云计算平台选择可用的云计算资源实例，如果有4090实例可用，推荐选择4090实例。同时注意镜像的选择，所以镜像会包含特定的环境，省去一些基础环境的安装步骤，不过这里镜像在实例启动之后也可以进行切换。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/21562705453241be922e2de19215b54b.png">
      <br/>
      创建实例之后，通过SSH连接远程服务器
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/60475923357646e2865d5ee239b54a22.png">
       <br/>
       这里我使用 VS Code的 Remote-SSH 插件进行连接，连接进去之后可以看到实例中有两个盘，其中/root/autodl-tmp是数据盘，推荐运行环境、模型文件都放在数据盘，避免后续因为实例关机回收导致数据文件丢失。
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c5d6f4e8faad43fbb69e5a3842d6eb88.png"/>
      </img>
     </img>
    </p>
    <h2>
     <a id="2__LLaMaFactory_13">
     </a>
     2. 拉取 LLaMa-Factory
    </h2>
    <p>
     LLaMa-Factory 的 git 地址如下，通过 git 命令拉取
    </p>
    <pre><code class="prism language-bash"><span class="token function">git</span> clone https://github.com/hiyouga/LLaMA-Factory.git
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7571ded26c4c413e9ccb4efcef5ae4e9.png"/>
    </p>
    <h2>
     <a id="3__21">
     </a>
     3. 安装依赖环境
    </h2>
    <p>
     LLaMa-Factory 依赖 Python 特定版本，这里使用 Conda 来进行 Python 虚拟环境管理，大语言模型相关的框架对运行环境的依赖比较严重，推荐通过虚拟环境进行隔离。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/2d39c4b4a25747ab8be6a08c83e6bede.png">
      <br/>
      而在创建虚拟环境之前，推荐设置一下 Conda 虚拟环境和 Python 包的保存路径，还是那个原因，避免因为云计算资源回收导致数据丢失。
     </img>
    </p>
    <pre><code class="prism language-bash"><span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /root/autodl-tmp/conda/pkgs
conda config <span class="token parameter variable">--add</span> pkgs_dirs /root/autodl-tmp/conda/pkgs
<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /root/autodl-tmp/conda/envs
conda config <span class="token parameter variable">--add</span> envs_dirs /root/autodl-tmp/conda/envs/
</code></pre>
    <p>
     之后创建虚拟环境：
    </p>
    <pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span> llama-factory <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.10</span>
</code></pre>
    <p>
     虚拟环境创建完成之后，通过以下命令初始化以下Conda，并刷新一下命令行环境变量，再激活环境：
    </p>
    <pre><code class="prism language-bash">conda init
<span class="token builtin class-name">source</span> ~/.bashrc
conda activate llama-factory
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/1d43dcb69b1c46208b9873bd8d7032ea.png">
      <br/>
      之后进入 LLaMa-Factory 文件夹，通过以下命令进行 LLaMa-Factory 相关依赖包的安装
     </img>
    </p>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">".[torch,metrics]"</span>
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fc66d59bf4cb4fe88f54146aade0ac23.png">
      <br/>
      安装完成之后，通过以下命令测试一下 LLaMa-Factory 是否正常安装：
     </img>
    </p>
    <pre><code class="prism language-bash">llamafactory-cli version
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9f8ef2d2f40949fb84cdbb6e6fe54f67.png"/>
    </p>
    <h2>
     <a id="4__LLaMaFactory__60">
     </a>
     4. 启动 LLaMa-Factory 界面
    </h2>
    <p>
     通过以下命令启动 LLaMa-Factory 可视化微调界面：
    </p>
    <pre><code class="prism language-bash">llamafactory-cli webui
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f9bab1bffcc14345b880411933eb8706.png"/>
     <br/>
     通过 VS Code 中的 Remote-SSH 插件连接云服务器的情况，启动可视化界面之后，Remote-SSH 会自动进行端口转发，从而自动在本地浏览器打开相应的页面。如果是其他工具的话，可能需要在云平台配置一下相应的端口，之后通过云平台暴漏出来的域名打开。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9763d22f416b4e34827599120692e009.png"/>
    </p>
    <h2>
     <a id="5__Huggingface__72">
     </a>
     5. 从 Huggingface 下载模型
    </h2>
    <p>
     首先创建一个文件夹用于存放模型文件：
    </p>
    <pre><code class="prism language-bash"><span class="token function">mkdir</span> hugging-face
</code></pre>
    <p>
     增加环境变量，修改 HuggingFace 镜像源为国内镜像网站：
    </p>
    <pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_ENDPOINT</span><span class="token operator">=</span>https://hf-mirror.com
</code></pre>
    <p>
     修改模型默认存储路径：
    </p>
    <pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">HF_HOME</span><span class="token operator">=</span>/root/autodl-tmp/hugging-face
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/2662bbddd133426da0f54b8fe4b1a691.png"/>
     <br/>
     之后还是切换到 llama-factory 虚拟环境，安装 HuggingFace官方下载工具：
    </p>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-U</span> huggingface_hub
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7dfa13604e2f49cbbc58b53bd5d0a848.png"/>
     <br/>
     安装完成之后，通过以下命令下载模型：
    </p>
    <pre><code class="prism language-bash">huggingface-cli download --resume-download Qwen/Qwen2.5-0.5B-Instruct
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d19f10d59f19429a8bf95a9c68e82460.png"/>
     <br/>
     这里为了下载和后面的微调演示快点就下载0.5B的模型了，具体的模型大家可以根据实际情况去选择自己需要的模型，在huggingface上搜索模型名称，之后进入模型主页，复制名称即可：
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7cc8a4e2ef504158b9fcdb98a6e02139.png"/>
     <br/>
     模型文件都比较大，在线下载的话需要等待一段时间，下载完成之后，可以看到模型文件就在 hugging-face 文件夹下了。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/50737ebaea20419c823f2f29bb478163.png"/>
     <br/>
     在线下载比较占用时间，而云服务器开机每一分钟都需要花钱，你可以在本地下载模型文件之后传到云服务器上。这时候可以通过 AutoDL 提供的 Jupyter 工具进行文件上传，或者通过其他带有SFTP功能的工具连接云服务器上传。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f70c10d8eeb44da5b8ec0bf4dd5c20fb.png"/>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d99abfcd19954890afb211d270c59089.png"/>
    </p>
    <h2>
     <a id="6__115">
     </a>
     6. 模型验证
    </h2>
    <p>
     下载完成模型之后，我们需要验证模型文件是否可以正常加载、运行，可以通过 LLaMa-Factory 的可视乎界面加载运行模型：
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/1ed2b46fb3634e3ab1bda8a8568f4762.png"/>
     <br/>
     需要注意的是，加载本地模型的时候，需要修改填写模型本地路径，这里的路径是模型快照的唯一哈希值，而不是模型文件夹的路径。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4447eb49d19d414a8d4f42b445472a7f.png"/>
     <br/>
     之后就可通过和模型进行对话，测试下载下来的模型是否正常了，也可以看下对话中模型输出的风格，和我们微调之后的做下对比。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a3e04f17a83743c4a433d2011d8ae135.png"/>
    </p>
    <h2>
     <a id="7__126">
     </a>
     7. 模型微调
    </h2>
    <p>
     接下来就可以通过 LLaMa-Factory 进行微调了，这里先做一个简单的演示，为了不让这篇文章篇幅过长，先不具体讲解各种微调参数的含义和作用，数据集也只使用 LLaMa-Factory 自带的示例数据集，演示一下对模型认知设定的微调。
    </p>
    <p>
     修改一下默认的 identity.json 数据集，将其中的{
     <!-- -->
     {name}}、{
     <!-- -->
     {author}}替换为我们自己的设定，并保存文件。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7e053b6f28d04c2d95162112962ea5a7.png"/>
     <br/>
     之后在 LLaMa-Factory Web界面中加载预览数据集，并且稍微调整一下超参，主要是学习率先保存不变，主要是训练轮次，以及验证集比例。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fc413d9dd3834850871549144de87c01.png"/>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/12a722edab7d41e99e0adb4dd99fea47.png"/>
     <br/>
     后续如果需要使用我们自定义的数据集的话，也只要将数据集文件放到 LLaMa-Factory 的 data 文件夹，再在 dataset_info.json 中进行配置，就可以在 Web 界面进行加载使用。这里就先不细说了。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9361c70394b34d409ea686e05df50a90.png"/>
     <br/>
     之后点击开始，可以看到微调任务的执行进度，以及损失函数的变化情况。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/193a800336c1473e8305a87bb04682c9.png"/>
     <br/>
     0.5B 的模型，再加上数据集数据量不多，只有不到100条，所以微调过程还是很快的，可以看到最终的损失函数降到了 0.5 左右。不过这是因为训练数据太少，而且一些超参设置比较不合理，才有这样的较低损失函数，实际微调用于生产环境的模型时，要注意防止过拟合的情况。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d40b4528b26147b2901075f5231695b9.png"/>
     <br/>
     之后，还是用 LLaMa-Factory 加载微调之后的模型文件进行测试，这里通过检查点路径添加刚刚训练完成的模型文件：
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/70a776a871524fb7ba804a3851b28110.png"/>
     <br/>
     之后就可以和我们微调之后的模型进行对话了
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/16b5b5427bc64d2e9d4cd02b03f8ac3d.png"/>
     <br/>
     可以看到，微调之后的模型已经学习了数据集中的信息，可以按照我们预设的人设回答问题。
    </p>
    <p>
     参考文档：
     <br/>
     <a href="https://llamafactory.readthedocs.io/en/latest/" rel="nofollow">
      LLama-Factory 官方文档
     </a>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f33373634383532352f:61727469636c652f64657461696c732f313436313134303331" class_="artid" style="display:none">
 </p>
</div>


