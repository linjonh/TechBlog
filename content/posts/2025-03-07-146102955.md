---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f4465736f6c6174654749532f:61727469636c652f64657461696c732f313436313032393535"
layout: post
title: "数学建模MATLAB强化学习"
date: 2025-03-07 20:31:45 +08:00
description: "的强化学习算法，用于解决具有高维状态空间的问题。基于当前观察到的状态，智能体选择一定策略决定采取的动作，根据每个动作的价值分配一个选择的概率，价值越高的动作被选中的概率越大，所有动作都有可能会被选择。基于新获得的状态和奖励，智能体采用一定的价值函数更新其对当前策略的价值评估：时序差分学习、蒙特卡洛方法、动态规划方法。智能体执行动作后，观察环境对其动作的响应，接收新的状态和相应的奖励，奖励是标量值，用于评估所选动作的好坏。智能体通过与环境的交互，使得其作出的动作所得到的决策得到的总的奖励达到最大。"
keywords: "matlab rltraining"
categories: ['数学建模']
tags: ['开发语言', 'Matlab']
artid: "146102955"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146102955
    alt: "数学建模MATLAB强化学习"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146102955
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146102955
cover: https://bing.ee123.net/img/rand?artid=146102955
image: https://bing.ee123.net/img/rand?artid=146102955
img: https://bing.ee123.net/img/rand?artid=146102955
---

# 数学建模：MATLAB强化学习

### 一、强化学习简述

强化学习是一种通过与环境交互，学习状态到行为的映射关系，以获得最大积累期望回报的方法。包含环境，动作和奖励三部分，本质是
智能体通过与环境的交互，使得其作出的动作所得到的决策得到的总的奖励达到最大
。强化学习主要是智能体与环境的交互过程。

步骤：

智能体先观察环境的状态。状态是智能体周围的位置布局或者智能体与周围物体的距离，通常用向量表示。
  
基于当前观察到的状态，智能体选择一定策略决定采取的动作，根据每个动作的价值分配一个选择的概率，价值越高的动作被选中的概率越大，所有动作都有可能会被选择。
  
智能体执行动作后，观察环境对其动作的响应，接收新的状态和相应的奖励，奖励是标量值，用于评估所选动作的好坏。
  
基于新获得的状态和奖励，智能体采用一定的价值函数更新其对当前策略的价值评估：时序差分学习、蒙特卡洛方法、动态规划方法。
  
重复上面步骤改变策略直至达到停止条件。

### 二、用DQN方法创建智能体并实现智能体训练与环境的交互

#### 1.简述

深度Q网络（DQN）是一种结合了
**Q学习**
和
**深度神经网络**
的强化学习算法，用于解决具有高维状态空间的问题。DQN通过使用一个神经网络来近似动作价值函数（Q值），从而能够处理复杂的状态输入，并为每个可能的动作估计其预期收益。使得智能体能够在未知环境中通过试错学习到最优策略。

![](https://i-blog.csdnimg.cn/direct/2e995e6bb0f54e78808562ab82ff1afb.png)

![](https://i-blog.csdnimg.cn/direct/26e90fbecc3b43b3b7e8a2f61a8e15f5.png)

#### 2.代码

```Matlab
clear
clc

%%
%创建强化学习环境

%使用预定义环境直接创建一个离散动作空间的倒立摆环境
env = rlPredefinedEnv('CartPole-Discrete');  

%查看环境的状态信息
%getObservationInfo函数返回环境中状态观测值的相关信息，包括每个状态变量的名称、描述、低值、高值以及维度
obsInfo = getObservationInfo(env);            

%查看环境的动作信息
%getActionInfo函数返回环境中所有可能动作的相关信息
actInfo = getActionInfo(env);                 

%rng函数控制随机数生成器的状态
%设置为0，表示每次运行时都会使用相同的随机数序列，确保了结果的一致性和可重复性
rng(0)                                                 


%%
%创建智能体

% 创建神经网络结构
%定义神经网络的超参数
layers =[                                      
    %特征输入层，大小为状态观测值的维度。
    %使用featureInputLayer函数创建一个输入层
    %obsInfo.Dimension(1)参数表示环境状态空间的维度
    featureInputLayer(obsInfo.Dimension(1))   

    %fullyConnectedLayer函数创建一个全连接层，传入参数为神经元数量
    %创建两个全连接层有助于在保持重要信息的同时降低模型复杂度
    fullyConnectedLayer(200)                  
    fullyConnectedLayer(50)                   

    %用reluLayer函数创建一个ReLU激活层，引入了非线性，对输入数据进行非线性处理
    reluLayer                                  

    %创建全连接层为输出层，神经元数量等于环境中可执行的动作数，通过这个层预测每个动作的价值
    %length(actInfo.Elements)返回环境中所有可能动作的数量。
    fullyConnectedLayer(length(actInfo.Elements))
]; 

%dlnetwork函数将之前定义的layers数组转换为一个可被MATLAB中的深度学习工具箱所识别和使用深度学习网络对象，进而进行训练操作
net = dlnetwork(layers);                      

%用rlVectorQValueFunction函数创建一个Q值函数作为评估器
%传入参数net表示已创建好的神经网络；obsInfo和actInfo分别表示状态信息和动作信息
%该Q值函数用于评估在给定状态下采取每个可能动作的价值
critic = rlVectorQValueFunction(net, obsInfo, actInfo);

%用rlOptimizerOptions函数设置Q值函数的优化参数
%传入参数LearnRate表示学习率；GradientThreshold表示梯度阈值
%学习率控制模型参数更新速度；梯度阈值有助于稳定训练过程
crtic_Opts = rlOptimizerOptions(LearnRate=1e-3, GradientThreshold=1);

%用rlDQNAgentOptions函数设置DQN智能体的训练选项
%SampleTime0表示每个时间步的时间间隔
%MiniBatchSize表示每次抽取训练样本的数量
%UseDoubleDQN表示不使用双重DQN
agentoption = rlDQNAgentOptions( ...
    SampleTime = 0.1, ...                       
    Critic = crtic_Opts, ...     
    MiniBatchSize = 256, ...                   
    UseDoubleDQN = false);                      

%用rlDQNAgent函数创建DQN智能体
agent = rlDQNAgent(critic, agentoption);       

%%
%配置训练选项

%MaxEpisodes表示智能体与环境交互的最大次数
%MaxStepsPerEpisode表示每次与环境交互智能体执行的最大操作次数
%"StopTrainingCriteria","AverageReward"表示停止训练的标准是基于平均奖励
%"SaveAgentCriteria","AverageReward"表示当达到某个平均奖励时保存智能体
trainOpts = rlTrainingOptions( ...
    "MaxEpisodes", 200, ...                     
    "MaxStepsPerEpisode", 400, ...               
    "StopTrainingCriteria", "AverageReward", ... 
    "StopTrainingValue", 400, ...                                                    
    "SaveAgentCriteria", "AverageReward", ...   
    "SaveAgentValue", 100);                      

%%
%绘图，使推杆系统可视化
plot(env)                                       

%%
%训练智能体
%用train函数实现智能体与环境之间的交互训练过程
%在训练过程中，智能体学习如何通过最大化累积奖励来选择最佳动作
training_Stats = train(agent, env, trainOpts);     

```

#### 3.运行结果

浅蓝色折线表示每轮智能体与环境交互的奖励

深蓝色折线表示当前所有交互轮次的平均奖励

![](https://i-blog.csdnimg.cn/direct/e1af7c9eb0524fc9b0e3f6c1b85bc4c8.png)

![](https://i-blog.csdnimg.cn/direct/d8406cdfb9c44f51acbddb8689c85e5b.png)

### 三、用simulink模型创建环境

Simulink是一个基于MATLAB的图形化编程环境，用于建模、仿真和分析多域动态系统，由一系列模块组成，这些模块代表了系统的各个组成部分。

用simulink模型创建强化学习环境，接收智能体的动作作为输入，输出相应的观察值、奖励和完成信号。

![](https://i-blog.csdnimg.cn/direct/0f61170332f7461e8a2ece2705fc04ee.png)