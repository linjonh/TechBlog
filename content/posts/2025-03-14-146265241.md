---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33373637393633392f:61727469636c652f64657461696c732f313436323635323431"
layout: post
title: "地基Prompt提示常用方式"
date: 2025-03-14 19:05:47 +08:00
description: "Prompt提示常用方式 ： 思维链（Chain of Thought, CoT）​ 、LTM 提示方法（Long-Term Memory）​、​思维树（Tree of Thoughts, ToT）​。"
keywords: "地基Prompt提示常用方式"
categories: ['未分类']
tags: ['提示词', '人工智能', 'Prompt']
artid: "146265241"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146265241
    alt: "地基Prompt提示常用方式"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146265241
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146265241
cover: https://bing.ee123.net/img/rand?artid=146265241
image: https://bing.ee123.net/img/rand?artid=146265241
img: https://bing.ee123.net/img/rand?artid=146265241
---

# 地基Prompt提示常用方式

## **思维链（Chain of Thought, CoT）​**

**思维链（Chain of Thought, CoT）​**
是指让 AI 在回答问题时，像人类一样展示出一步步的推理过程，而不仅仅是直接给出最终答案。这种方法的目的是让答案更清晰、可信，尤其是解决复杂问题时，用户能看懂 AI 的逻辑。

### 简单理解：

就像做数学题时，老师要求你「写出计算过程」，而不是只写答案。CoT 就是让 AI 把「脑子里的思考过程」说出来。

---

#### ​ **例子对比：​**

##### 1. ​ **没有思维链（直接回答）：​**

**问题**
  
小明买了 5 个苹果和 5 根香蕉，苹果 3 元一个，香蕉 2 元一根，他一共花了多少钱？

**回答**
  
一共花了 25 元。

*缺点*
：虽然答案正确，但不知道 AI 是蒙对的还是真的会算。

---

##### 2. ​ **有思维链（CoT）：​**

**问题**
  
小明买了 5 个苹果和 5 根香蕉，苹果 3 元一个，香蕉 2 元一根，他一共花了多少钱？请一步步思考。

**回答**

1. 先算苹果的总价：5 个苹果 × 3 元/个 = 15 元；
2. 再算香蕉的总价：5 根香蕉 × 2 元/根 = 10 元；
3. 最后把两者相加：15 元 + 10 元 = 25 元。
     
   所以，小明一共花了 25 元。

*优点*
：过程清晰，用户能验证每一步是否正确。

---

#### ​ **另一个例子（非数学问题）：​**

**问题**
  
今天阴天，但天气预报说不会下雨，我要不要带伞？

**CoT 回答**

1. 虽然阴天，但天气预报说不下雨，说明气象数据支持不带伞；
2. 但阴天可能有突发降雨，如果没带伞可能被淋湿；
3. 如果包里空间足够，带伞可以避免风险；
4. 如果不想麻烦，也可以不带。
     
   建议：如果出门时间长，可以带伞；如果短时间出门，可以不带。

---

### ​ **总结：​**

CoT 就像让 AI「把作业本上的草稿纸展示给你看」，帮助用户理解它的逻辑。尤其适用于数学题、逻辑推理、复杂决策等问题。你可以通过在提问时加上「请一步步思考」或「解释你的推理过程」来触发 CoT 效果。

## **LTM 提示方法（Long-Term Memory）​**

**简单来说**
：LTM 是让 AI 在回答问题时，能够主动调用「长期记忆」（比如之前的对话内容、外部知识库或用户提供的历史信息），从而生成更连贯、个性化的回答。
  
**核心目的**
：让 AI 像人类一样记住上下文或背景知识，避免重复提问或回答脱节。

---

### ​ **通俗理解**

假设你和一个朋友聊天：

* ​
  **没有 LTM**
  ：每次对话他都忘记之前聊过什么，每次都要你重复信息。
* ​
  **有 LTM**
  ：他能记住你们之前聊的话题、你的喜好或背景知识，回答更符合你的需求。
    
  LTM 提示就是通过设计问题或指令，让 AI 主动利用这些「记忆」。

---

### ​ **LTM 的两种常见场景**

#### 1. ​ **对话中的上下文记忆（短期长期记忆）​**

**例子**
：

* ​
  **用户第一次提问**
  ：
    
  “我想买一台笔记本电脑，预算 5000 元左右，主要用来办公。”
* ​
  **后续提问**
  ​（依赖 LTM 的效果）：
    
  “刚刚提到的预算内，有哪些推荐的型号？”

**好的 LTM 回答**
：
  
“根据您 5000 元的预算和办公需求，推荐联想小新系列或华为 MateBook D，它们续航强、轻薄便携。”
  
*（AI 记住了「预算」和「用途」）*

**差的回答（无 LTM）​**
：
  
“请告诉我您的预算和用途。”
  
*（重复提问，没有记忆）*

---

#### 2. ​ **结合外部知识库（长期知识记忆）​**

**例子**
：
  
**问题**
：
  
“用 LTM 方法写一个关于登月计划的科幻故事，要求提到阿波罗 11 号的真实历史细节。”

**LTM 提示的回答**
：
  
“2060 年，人类在月球建立了永久基地。基地的中央广场立着一块金属牌，上面刻着：​\*\*‘1969 年 7 月 20 日，阿波罗 11 号宇航员首次踏上月球’\*\*。此刻，新一代探险家正站在牌子前，准备向火星出发……”
  
*（AI 调用了阿波罗 11 号的真实历史知识）*

**无 LTM 的回答**
：
  
“未来人类在月球上建立了基地，他们探索宇宙……”
  
*（缺乏真实历史细节）*

---

#### ​ **如何触发 LTM 效果？**

在提问时明确要求 AI 调用记忆，例如：

1. ​
   **关联上下文**
   ：
     
   “根据我们之前的讨论，你推荐的笔记本电脑是否支持扩展内存？”
2. ​
   **指定知识范围**
   ：
     
   “参考《三国演义》中赤壁之战的过程，分析孙刘联盟胜利的原因。”
3. ​
   **提供背景信息**
   ：
     
   “我之前喜欢科幻和悬疑小说，请推荐类似的新书。”

---

### ​ **总结**

LTM 的核心是让 AI ​
**记住并利用已有信息**
，而不是每次都从零开始。适用于：

* 多轮对话（如客服、个性化推荐）
* 需要结合专业知识的问题（如历史、科学）
* 创作类任务（如故事、文案需引用特定资料）

通过清晰的提示词（例如“参考之前的信息”“结合某某知识”），可以更好地激活 AI 的 LTM 能力。

## ​ **思维树（Tree of Thoughts, ToT）​**

**简单来说**
：ToT 是一种让 AI 像人类一样 ​
**多角度、分步骤探索问题**
的提示方法。它要求 AI 将复杂问题拆解成多个可能的解决路径（像树枝分叉），逐一分析后选择最优解。
  
**核心目的**
：解决单一思维链（CoT）可能遗漏潜在答案的问题，提升答案的全面性和可靠性。

---

### ​ **通俗理解**

假设你要解决一道迷宫题：

* ​
  **用思维链（CoT）​**
  ：你会沿着一条路走到底，如果走不通就失败。
* ​
  **用思维树（ToT）​**
  ：你会先在脑海中画出迷宫地图，标记所有可能的岔路，再选择最合理的路径。
    
  ToT 就是让 AI 在脑海中生成多个“解题路线图”，并选出最佳方案。

---

### ​ **例子对比（ToT vs CoT）​**

##### ​ **问题**

“小明有 15 元，需要买 3 支笔和 2 个笔记本。笔单价 2 元，笔记本单价 4 元。他钱够吗？如果不够，怎么调整数量？”

---

#### ​ **1. 用思维链（CoT）回答**

1. 计算总花费：3×2 + 2×4 = 6 + 8 = 14 元；
2. 比较金额：15 元 > 14 元，所以够用。
     
   **结果**
   ：直接给出答案，但忽略了问题中“如果不够，怎么调整”的可能性分析。

---

#### ​ **2. 用思维树（ToT）回答**

**步骤 1：拆解问题分支**

* 分支 1：当前购买方案是否超支？
* 分支 2：如果超支，如何减少数量？
  + 子分支 2.1：减少 1 支笔 → 总价 2×2 + 8 = 12 元
  + 子分支 2.2：减少 1 个笔记本 → 总价 6 + 4 = 10 元
* 分支 3：如果钱有剩余，能否多买？
  + 子分支 3.1：多买 1 支笔 → 剩余 15 - (14+2) = -1 元（不可行）
  + 子分支 3.2：多买 1 个笔记本 → 剩余 15 - (14+4) = -3 元（不可行）

**步骤 2：评估每个分支**

* 分支 1：当前方案花费 14 元，15 元足够，剩余 1 元。
* 分支 2：无需调整，但可建议优化（例如用剩余钱买橡皮）。
* 分支 3：剩余钱不足以多买。

**最终回答**
  
当前方案够用，剩余 1 元。如果想调整，可减少 1 支笔（总价 12 元）并多买一块橡皮（1 元），或保持原计划。

---

### ​ **另一个例子（非数学问题）​**

**问题**
  
“我想周末去郊游，但可能下雨，有什么建议？”

**ToT 回答**

1. ​
   **分支 1：晴天方案**
   * 爬山、野餐、拍照，需准备防晒用品。
2. ​
   **分支 2：雨天方案**
   * 改为室内博物馆或咖啡馆，带伞和防水鞋。
3. ​
   **分支 3：折中方案**
   * 选择有遮阳棚的露营地，下雨可避雨，晴天可活动。
       
     **最终建议**
     ：提前看天气预报，若不确定，优先选择分支 3 的露营地。

---

### ​ **如何触发 ToT 效果？**

在提问时明确要求 AI ​
**分步骤探索可能性**
，例如：

* “请列出所有解决方案并分析优缺点。”
* “分步骤思考：1. 问题拆解，2. 可能性分析，3. 最优选择。”
* “如果遇到 X 情况，可能有哪些应对方式？请逐一评估。”

---

### ​ **ToT 的适用场景**

1. ​
   **开放式问题**
   ​（如策划、决策、创意）。
2. ​
   **存在多种解法的问题**
   ​（如数学题、逻辑推理）。
3. ​
   **需要考虑风险和备选方案的问题**
   ​（如旅行计划、投资建议）。

---

### ​ **总结**

* ​
  **ToT 像解题时的“头脑风暴”​**
  ，先穷举可能性，再筛选最优解。
* ​
  **对比 CoT（直线思维）​**
  ：ToT 更适合复杂、多路径问题，避免遗漏潜在答案。
* ​
  **缺点**
  ：效率较低，简单问题直接用 CoT 更高效。

**（望各位潘安、各位子健/各位彦祖、于晏不吝赐教！多多指正！🙏）**