---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37333733363639352f:61727469636c652f64657461696c732f313436313832393537"
layout: post
title: "第85期-GPTSecurity周报"
date: 2025-03-11 16:39:26 +08:00
description: "结构化数据包含的信息往往有限，这让注释过程更加复杂，极具挑战性。为了验证 DarkMind 的效果，研究者在八个数据集上，使用五个最先进的 LLM 以及五种不同的触发器实现，对 DarkMind 进行了评估，这些数据集涵盖了算术、常识和符号推理领域。在本文中，研究者提出了 CASEY，这是一种借助大语言模型（以 GPT 模型为例）的全新方法，它能自动识别安全漏洞的常见弱点枚举（CWE）并评估其严重性。最后，研究者探讨了潜在的防御机制，以降低 DarkMind 带来的风险，强调了实施更有力安全措施的必要性。"
keywords: "第85期 | GPTSecurity周报"
categories: ['Gptsecurity']
tags: ['网络安全', '人工智能', 'Gpt']
artid: "146182957"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146182957
    alt: "第85期-GPTSecurity周报"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146182957
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146182957
cover: https://bing.ee123.net/img/rand?artid=146182957
image: https://bing.ee123.net/img/rand?artid=146182957
img: https://bing.ee123.net/img/rand?artid=146182957
---

# 第85期 | GPTSecurity周报

![图片](https://i-blog.csdnimg.cn/img_convert/5972117733ca3e905b53a039f305446d.jpeg)

GPTSecurity是一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练Transformer（GPT）、人工智能生成内容（AIGC）以及大语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。现为了更好地知悉近一周的贡献内容，现总结如下。

### **Security Papers**

1. 探索检索增强代码生成中知识库中毒的安全威胁

简介：大语言模型（LLM）的集成显著改变了软件开发领域，尤其是通过检索增强代码生成（RACG）系统，这些系统利用外部知识库的信息来增强代码生成过程。然而，RACG系统的安全隐患，尤其是知识库中易受攻击的代码示例所带来的风险，尚未得到充分探索。这一风险尤为严重，因为RACG系统中的知识库通常来源于公共代码存储库，而这些存储库对社区中的任何人都是开放的。恶意攻击者可能利用这种开放性，将易受攻击的代码注入知识库，从而污染知识库。一旦这些被污染的代码样本被检索并整合到生成的代码中，它们就可能将安全漏洞传播到最终产品中。

本文首次全面研究了RACG系统相关的安全风险，重点探讨了知识库中的易受攻击代码如何危及生成代码的安全性。研究者通过使用四个主要LLM、两个检索器和两种中毒场景进行大量实验，分析了不同设置下LLM生成代码的安全性。研究结果表明，知识库中毒的威胁极为严重，即使仅有一个中毒代码示例，也可能危及高达48%的生成代码。这些发现为RACG系统中漏洞的引入提供了重要见解，并提出了切实可行的缓解建议，有助于在未来提高LLM生成代码的安全性。

链接：

*https://arxiv.org/abs/2502.03233*

2. LLMSecConfig：一种基于 LLM 的修复软件容器错误配置的方法

简介：容器编排器（CO）中的安全配置错误对软件系统构成了严重威胁。尽管静态分析工具（SAT）能够有效检测这些安全漏洞，但业界目前缺乏自动化解决方案来修复这些错误配置。大语言模型（LLM）的出现及其在代码理解和生成方面的成熟能力，为解决这一限制提供了新的可能性。本研究提出了LLMSecConfig，这是一个创新框架，通过将SAT与LLM相结合来弥补这一缺陷。该框架利用高级提示技术和检索增强生成（RAG）来自动修复安全配置错误，同时确保操作功能的完整性。通过对1,000个真实Kubernetes配置的评估，该框架实现了94%的成功率，并且在修复过程中引入新错误配置的概率保持在较低水平。这一研究为自动化修复容器编排器中的安全配置错误提供了重要进展。

链接：

*https://arxiv.org/abs/2502.02009*

3. 在可变上下文窗口下评估大语言模型在漏洞检测中的作用

简介：本研究探讨了标记化Java代码长度对十大主流大语言模型（LLM）在漏洞检测任务中的准确性和明确性的影响。通过卡方检验并结合已知的真实数据，研究者发现不同模型之间存在显著差异：部分模型（如GPT-4、Mistral和Mixtral）表现出较强的稳健性，而其他模型则显示出标记化长度与性能之间的显著相关性。基于这些发现，研究者建议未来LLM的开发应着重减少输入长度对性能的影响，以提升漏洞检测的效果。此外，采用能够在减少标记数量的同时保留代码结构的预处理技术，有望进一步提高LLM在这些任务中的准确性和明确性。

链接：

*https://arxiv.org/abs/2502.00064*

4. 使用 LLM 合成输入生成器进行低成本、全面的非文本输入模糊测试

简介：现代软件通常需要处理具有高度复杂语法的输入。尽管大语言模型（LLM）的最新进展表明其在生成符合特定输入格式语法的高质量自然语言文本和代码方面表现出色，但LLM通常难以生成非文本输出（如图像、视频和PDF文件），或者生成成本过高。这一限制阻碍了LLM在语法感知模糊测试中的应用。

为了解决这一问题，研究者提出了一种新方法，用于对非文本输入进行语法感知模糊测试。该方法利用LLM以Python脚本的形式合成并变异输入生成器，从而生成符合给定输入格式语法的数据。随后，这些输入生成器产生的非文本数据由传统模糊测试工具（如AFL++）进一步变异，以有效探索软件输入空间。研究者将这种方法命名为G2FUZZ，它采用了一种混合策略，结合了LLM驱动的全局搜索和工业级模糊器驱动的局部搜索。G2FUZZ的两个主要优势在于：（1）LLM擅长合成和变异输入生成器，并能够跳出局部最优解，从而在与基于变异的模糊器结合时产生协同效应；（2）除非必要，否则LLM的调用频率较低，这显著降低了LLM的使用成本。

研究者在多种输入格式（如TIFF图像、MP4音频和PDF文件）上对G2FUZZ进行了评估。结果表明，在UNIFUZZ、FuzzBench和MAGMA三个平台上测试的大多数程序中，G2FUZZ在代码覆盖率和错误检测方面均优于AFL++、Fuzztruction和FormatFuzzer等当前最先进的工具。这一研究为非文本输入的语法感知模糊测试提供了新的解决方案，并展示了LLM与传统模糊测试工具结合的巨大潜力。

链接：

*https://arxiv.org/abs/2501.19282*

5. Rule-ATT&CK Mapper (RAM)：使用 LLM 将 SIEM 规则映射到 TTP

简介：随着网络攻击频率不断攀升，准确、高效的威胁检测系统愈发重要。SIEM 平台借助基于规则的查询（即 SIEM 规则）分析日志数据、检测对抗活动，意义重大。威胁分析过程的效率，在很大程度上依赖于将这些 SIEM 规则映射到 MITRE ATT&CK 框架里的相关攻击技术。倘若 SIEM 规则注释不准确，就可能造成对攻击的误解，进而增加威胁被忽视的风险。

当前，使用 MITRE ATT&CK 技术标签注释 SIEM 规则的方案存在明显不足。手动注释 SIEM 规则不仅耗时，还容易出错；基于 ML 的方法主要聚焦于注释非结构化的自由文本源，而非 SIEM 规则这类结构化数据。结构化数据包含的信息往往有限，这让注释过程更加复杂，极具挑战性。

为了攻克这些难题，研究者推出了 Rule-ATT&CK Mapper（RAM），这是一个全新框架，它能利用 LLM 将结构化 SIEM 规则自动映射到 MITRE ATT&CK 技术。RAM 的多阶段管道受提示链技术启发，无需对 LLM 进行预训练或微调，就能提升映射准确性。研究者运用 Splunk Security Content 数据集，借助多个 LLM（例如 GPT-4-Turbo、Qwen、IBM Granite 和 Mistral ）对 RAM 的性能展开评估。评估结果显示，GPT-4-Turbo 表现卓越，这得益于其丰富的知识库；一项消融研究则强调了外部背景知识的重要性，它能克服 LLM 隐性知识在特定领域任务中的局限性。这些研究成果表明，RAM 在自动化网络安全工作流程上颇具潜力，也为该领域未来发展提供了有价值的见解。

链接：

*https://arxiv.org/abs/2502.02337*

6. 利用大语言模型简化安全漏洞分类

简介：安全漏洞的 Bug 分类是软件维护的重要环节，它能确保最紧迫的漏洞得到及时修复，从而维护系统完整性和保护用户数据。然而，这一过程资源消耗大，还面临诸多挑战，例如对软件漏洞进行分类、评估其严重性，以及处理大量的 Bug 报告。

在本文中，研究者提出了 CASEY，这是一种借助大语言模型（以 GPT 模型为例）的全新方法，它能自动识别安全漏洞的常见弱点枚举（CWE）并评估其严重性。CASEY 运用了提示工程技术，同时整合不同粒度级别的上下文信息，以此辅助 Bug 分类过程。

研究者使用增强版的国家漏洞数据库（NVD）对 CASEY 进行评估，采用定量和定性指标，从 CWE 识别、严重性评估及其综合分析等方面来衡量其表现。结果显示，CASEY 的 CWE 识别准确率达到 68%，严重性识别准确率为 73.6%，两者的综合识别准确率为 51.2%。

这些结果表明，大语言模型在识别 CWE 和严重性级别、简化软件漏洞管理，以及提升安全漏洞分类工作流程效率等方面具有很大潜力。

链接：

*https://arxiv.org/abs/2501.18908*

7. DarkMind：定制LLM中潜在的思维链后门

简介：随着对个性化 AI 解决方案的需求日益增长，定制的大语言模型（LLM）成为企业和个人的优先选择。这一趋势推动了数百万个 AI 代理在各类平台上的部署，以 GPT Store 为例，其上托管了超过 300 万个定制的 GPT。定制 LLM 的流行，部分归因于诸如思维链（Chain-of-Thought）等高级推理能力，这些能力提升了它们处理复杂任务的水平。

然而，定制 LLM 的迅速普及也带来了新的安全隐患，尤其是在很大程度上尚未被充分探索的推理过程中。研究者提出了一种新型后门攻击 ——DarkMind，它利用了定制 LLM 的推理能力。DarkMind 的设计目标是保持潜伏状态，在推理链中激活，秘密改变最终结果。与现有攻击方式不同，它无需向用户查询注入触发器即可运行，这使其成为更具威胁性的安全风险。

为了验证 DarkMind 的效果，研究者在八个数据集上，使用五个最先进的 LLM 以及五种不同的触发器实现，对 DarkMind 进行了评估，这些数据集涵盖了算术、常识和符号推理领域。研究结果证实了 DarkMind 在所有场景中的有效性，凸显了其影响力。

最后，研究者探讨了潜在的防御机制，以降低 DarkMind 带来的风险，强调了实施更有力安全措施的必要性。

链接：

*https://arxiv.org/abs/2501.18617*