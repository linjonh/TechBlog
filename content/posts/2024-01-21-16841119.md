---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f6368656e796938383838:2f61727469636c652f64657461696c732f3136383431313139"
layout: post
title: "hbase测试压缩效果报告"
date: 2024-01-21 03:36:36 +08:00
description: "测试环境：Linux master 2.6.18-348.12.1.el5 #1 SMP Wed J"
keywords: "hbase压测结果"
categories: ['Hbase']
tags: ['无标签']
artid: "16841119"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=16841119
    alt: "hbase测试压缩效果报告"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=16841119
featuredImagePreview: https://bing.ee123.net/img/rand?artid=16841119
---

# hbase测试压缩效果报告

测试环境：

Linux master 2.6.18-348.12.1.el5 #1 SMP Wed Jul 10 05:28:41 EDT 2013 x86_64 x86_64 x86_64 GNU/Linux

hadoop-1.0.3

hbase-0.94.2

Oracle JRockit(R) (build R28.1.5-20-146757-1.6.0_29-20111004-1750-linux-x86_64, compiled mode)

测试需求，就是将hbase里的表，按照不同的压缩方式（因不支持bz，所以没有bz的测试结果），进行保存，以下是对比结果：

| 原始数据大小 | gz | snappy | lzo |
| --- | --- | --- | --- |
| 938.15MB | 174.37MB | 253.35MB | 563.09MB |

压缩率：

| gz | snappy | lzo |
| --- | --- | --- |
| 81.41% | 72.99% | 39.98% |

总得来说，gz效果最好。snappy和lzo都不太适合在hbase里用压缩

后面进行了一个比较特殊的测试。就是原始数据有43个columns，如果了解其存储原理的话，那么占用的空间是很大的。

我采用了合并这个43个column变成一个（注：这里考虑合并是因为有业务的需要）

根据以上的测试结果，我将测试两种场景：

1、原始数据大小与合并column后数据量的大小

2、原始数据大小与合并column并增加压缩的大小（采用gz压缩方式）

| 原始数据大小 | 合并字段后数据大小 | 合并字段并压缩后数据大小 |
| --- | --- | --- |
| 938.15MB | 203.3MB | 65.6MB |

压缩率：

| 合并字段 | 合并字段并压缩 |
| --- | --- |
| 78.33% | 93.01% |

通过一系列的测试发现，采用合并字段并压缩，这样达到的压缩效率是非常高的。而且也非常适合我们的业务使用场景。

所以最终的方案我们也采用了合并字段并压缩的方式，来对hbase进行相关优化处理。

此测试主要集中在如何节约磁盘空间考虑，并没有对读/写进行测试。