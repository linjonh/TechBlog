---
layout: post
title: "大模型理论篇-Mixture-of-Experts架构"
date: 2025-03-06 17:32:20 +0800
description: "概括来看，在MoE模型中，用混合专家层替换 Transformer 模型的每个前馈网络层，混合专家层由一个门控网络和一定数量的专家组成。在MoE模型的核心是“专家”子网络。这些专家是更大神经网络中的独立模块，每个模块都能够处理输入数据。其概念是不同的专家专注于输入数据的不同方面，从而使模型能够有效利用专门的知识。门控机制是一个关键组件，负责将输入引导至适当的专家网络。它根据一组门控值来决定每个专家的参与程度。门控机制可以实现为稠密或稀疏结构，其中稀疏结构由于选择性地激活部分专家，因而在计算上更加高效。"
keywords: "【大模型理论篇】--Mixture of Experts架构"
categories: ['大模型']
tags: ['架构']
artid: "146072460"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146072460
    alt: "大模型理论篇-Mixture-of-Experts架构"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146072460
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146072460
cover: https://bing.ee123.net/img/rand?artid=146072460
image: https://bing.ee123.net/img/rand?artid=146072460
img: https://bing.ee123.net/img/rand?artid=146072460
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【大模型理论篇】--Mixture of Experts架构
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     Mixture of Experts（MoE，专家混合）【1】架构是一种神经网络架构，旨在通过有效分配计算负载来扩展模型规模。MoE架构通过在
     <span style="color:#fe2c24">
      推理和训练过程中仅使用部分“专家”（子模型），优化了资源利用率，从而能够处理复杂任务
     </span>
     。
    </p>
    <p>
     在具体介绍MoE之前，先抛出MoE的一些表现【2】：
    </p>
    <blockquote>
     <ul>
      <li>
       与密集模型相比，预训练速度快得多。
      </li>
      <li>
       与具有相同参数数量的模型相比，推理速度更快。
      </li>
      <li>
       由于所有Experts都加载在内存中，所以需要大量的显存。
      </li>
      <li>
       在微调方面面临许多挑战，但最近关于 MoE 指令微调的工作很有前景。
      </li>
     </ul>
    </blockquote>
    <hr id="hr-toc" name="tableOfContents"/>
    <p>
    </p>
    <h2 id="1.MOE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F" name="1.MOE%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F">
     1.MOE是什么？
    </h2>
    <p>
     MoE 是一种包含多个被称为“专家”子网的架构，每个子网专注于
     <a href="https://so.csdn.net/so/search?q=%E4%B8%8D%E5%90%8C%E7%9A%84&amp;spm=1001.2101.3001.7020" title="不同的">
      不同的
     </a>
     数据任务或方面。其优势显著，在保持甚至提升模型质量的同时，能够以比相同或更大规模的传统模型更少的计算量进行预训练。例如，Mixtral 8x7B 在众多评估数据集上就超越了 LLaMA 2 70B。
    </p>
    <blockquote>
     <p>
      这个概念可以类比为劳动分工，每个专家专注于某个大问题中的特定一小部分任务，从而生成更快、更准确的结果。
     </p>
     <p>
     </p>
    </blockquote>
    <p>
     混合专家模型由三个关键组件组成：
    </p>
    <ul>
     <li>
      专家（Experts）：专门针对特定任务的子模型。
     </li>
     <li>
      门控网络（Gating Network）：一个选择器，它将输入数据路由到相关的专家。
     </li>
     <li>
      稀疏激活（Sparse Activation）：只有少数专家针对每个输入被激活的方法，优化了计算效率。
     </li>
    </ul>
    <p>
     传统transform架构：
    </p>
    <p class="img-center">
     <img alt="" height="331" src="https://i-blog.csdnimg.cn/direct/96c0906651ca43bdb197b8f69850b3ea.png" width="240"/>
    </p>
    <p>
     MOE架构：
    </p>
    <p class="img-center">
     <img alt="" height="295" src="https://i-blog.csdnimg.cn/direct/b1a1d12b469f4c34977a354904cf1eed.png" width="599"/>
    </p>
    <h3 id="1.1.%E4%B8%93%E5%AE%B6" name="1.1.%E4%B8%93%E5%AE%B6">
     1.1.专家
    </h3>
    <p>
     在 MoE 架构中，专家是指训练好的子网络（神经网络或层），它们专门处理特定的数据或任务。例如，在图像分类任务中，一个专家可能专门识别纹理，而另一个专家可能识别边缘或形状。这种分工有助于整个模型更高效地处理问题，因为每个专家只处理它最适合的数据类型。
    </p>
    <h3 id="1.2.%E9%97%A8%E6%8E%A7%E7%BD%91%E7%BB%9C%EF%BC%88%E8%B7%AF%E7%94%B1%E5%99%A8%EF%BC%89" name="1.2.%E9%97%A8%E6%8E%A7%E7%BD%91%E7%BB%9C%EF%BC%88%E8%B7%AF%E7%94%B1%E5%99%A8%EF%BC%89">
     1.2.门控网络（路由器）
    </h3>
    <p>
     门控网络充当一个选择器，它决定将哪些输入数据发送给哪些专家。不是所有专家都同时工作，而是门控网络将数据路由到最相关的专家那里。类似于 token 选择路由策略，路由算法为每个 token 选择最佳的一个或两个专家。例如，在下图中，输入令牌 1，“我们”，被发送到第二个专家，而输入令牌 2，“喜欢”，被发送到第一个网络。
    </p>
    <p class="img-center">
     <img alt="" height="280" src="https://i-blog.csdnimg.cn/direct/1514c3fea7c44d9694ba3131cf9a5731.png" width="550"/>
    </p>
    <p>
     以下是一些主流的Token 路由技术：
    </p>
    <ul>
     <li>
      Top-k 路由：这是最简单的方法。门控网络选择亲和力得分（affinity score）最高的 top -k 个专家，并将输入数据发送给它们。
     </li>
    </ul>
    <p class="img-center">
     <img alt="" height="297" src="https://i-blog.csdnimg.cn/direct/22f099ab2d3b4dc49227f25c11a306b5.png" width="504"/>
    </p>
    <ul>
     <li>
      专家选择路由：与根据数据选择专家不同，这种方法由专家决定它们最能处理哪些数据。这种策略旨在实现最佳的负载均衡，并支持以多种方式将数据映射到专家。
     </li>
    </ul>
    <h3 id="1.3.%E7%A8%80%E7%96%8F%E6%BF%80%E6%B4%BB" name="1.3.%E7%A8%80%E7%96%8F%E6%BF%80%E6%B4%BB">
     1.3.稀疏激活
    </h3>
    <p>
     稀疏激活是 MoE 模型的关键部分和优势之一。与所有专家或参数对输入都活跃的密集模型不同，稀疏激活确保只有一小部分专家根据输入数据被激活。这种方法在保持性能的同时减少了计算需求，因为任何时候只有最相关的专家是活跃的。
    </p>
    <ul>
     <li>
      稀疏路由：稀疏激活的一种特定技术，其中门控网络针对每个输入只激活少数专家。
     </li>
    </ul>
    <h3 id="1.4.%E6%80%BB%E7%BB%93" name="1.4.%E6%80%BB%E7%BB%93">
     1.4.总结
    </h3>
    <p>
     概括来看，在MoE模型中，
     <span style="color:#fe2c24">
      用混合专家层替换 Transformer 模型的每个前馈网络层，混合专家层由一个门控网络和一定数量的专家组成
     </span>
     。在MoE模型的核心是“专家”子网络。这些专家是更大神经网络中的独立模块，每个模块都能够处理输入数据。其概念是不同的专家专注于输入数据的不同方面，从而使模型能够有效利用专门的知识。
    </p>
    <p>
     门控机制是一个关键组件，负责将输入引导至适当的专家网络。它根据一组门控值来决定每个专家的参与程度。门控机制可以实现为稠密或稀疏结构，其中稀疏结构由于选择性地激活部分专家，因而在计算上更加高效。
    </p>
    <p>
     在稀疏MoE模型中，路由算法在决定哪些专家被激活以处理特定输入时起着重要作用。这些算法的复杂性各不相同，需考虑平衡模型的准确性和计算效率。路由算法的选择对模型的性能和推理速度有明显的影响。
    </p>
    <hr/>
    <h2 id="2.MOE%E8%BF%90%E4%BD%9C%E6%96%B9%E5%BC%8F" name="2.MOE%E8%BF%90%E4%BD%9C%E6%96%B9%E5%BC%8F">
     2.MOE运作方式
    </h2>
    <p>
     首先引入混合专家的想法，其实很直接，就是希望不同的专家角色，关注不同的领域。例如 某部分的专家网络专注于数学能力的提升。另一部分专注编程能力的加强。这由此引出一个关键概念：稀疏性【2】。稀疏性利用了条件计算的理念。
    </p>
    <p>
     在Dense模型中，所有输入都会使用所有参数，而稀疏性则允许仅运行系统的一部分。条件计算的思想（网络的一部分在每个示例的基础上被激活）使得可以在不增加计算量的情况下扩大模型的规模，因此，每个 MoE 层中使用了成千上万的专家。不过更多的专家可以提高样本效率并加快速度，但这些收益逐渐递减（尤其是在超过256或512个专家之后），而且推理时需要更多的显存。
    </p>
    <p class="img-center">
     <img alt="" height="295" src="https://i-blog.csdnimg.cn/direct/926acda26f6147d1ae1851c60b512bd8.png" width="898"/>
    </p>
    <p>
     在语言建模和 Transformer 的架构背景下，引入混合专家模型，需要注意专家们一次帮助预测一个token。路由器决定在每个token处哪个专家处于活动状态。专家网络通过门控线性组合来组合以预测下一个token。
    </p>
    <p>
     <span style="background-color:#ffd900">
      输出：
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="53" src="https://i-blog.csdnimg.cn/direct/a79d4bb170aa49fdb35776171bc52a6f.png" width="257"/>
    </p>
    <blockquote>
     <p>
      <img alt="" height="153" src="https://i-blog.csdnimg.cn/direct/dcedc951ec3a4932aa19329d1d42e783.png" width="693"/>
     </p>
    </blockquote>
    <p>
     <img alt="" height="137" src="https://i-blog.csdnimg.cn/direct/352b2e8e26684a5ea1da90735d9a17de.png" width="846"/>
    </p>
    <blockquote>
     <p>
      <strong>
       Softmax 函数
      </strong>
      是一种常用于多分类问题中的激活函数，它能够将一个向量转换成概率分布的形式。具体来说，Softmax 函数可以将任意实数向量映射到（0,1）区间内的概率分布，并且向量中的所有元素之和为1。
     </p>
     <p class="img-center">
      <img alt="" height="80" src="https://i-blog.csdnimg.cn/direct/7933e62dc25b4c4189b870863ab6bf53.png" width="233"/>
     </p>
    </blockquote>
    <p>
     <img alt="" height="104" src="https://i-blog.csdnimg.cn/direct/f4c14e62e1df451c98cbc74e30a3f14c.png" width="696"/>
    </p>
    <p>
     由于专家网络在一个 Transformer 模块内，路由是逐个token进行的。当token X 进入时，Transformer 模块的工作是预测下一个token Y。Transformer 模块通过自注意力机制可以访问所有先前的上下文。然后自注意力机制的输出作为条件，路由器判断输入到哪k个专家网络。
    </p>
    <p>
     那么路由是如何工作的？实际上，路由只是一个位于注意力头和 K 个专家集合之间的 softmax 层。这个层学习哪种专家最适合哪种类型的token。
    </p>
    <p>
     <span style="background-color:#ffd900">
      门控网络：
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="54" src="https://i-blog.csdnimg.cn/direct/e996309a861040dfa11678a8e730a167.png" width="289"/>
    </p>
    <p>
     举个例子：
    </p>
    <p>
     例如，序列[“I”、“love”、“LLM”、“#s”]中的每个标记都会通过路由函数，形成一个专家概率分布。 然后，我们为每个单个标记选择前 K 名专家–同一序列中的标记并不总是发送给相同的专家。
    </p>
    <hr/>
    <h2 id="3.MOE%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0" name="3.MOE%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">
     3.MOE代码实现
    </h2>
    <p>
     <img alt="" height="296" src="https://i-blog.csdnimg.cn/direct/83fdcfb68ba64dd3aa1a5bc499303953.png" width="905"/>
    </p>
    <pre><code class="hljs">class Expert(nn.Module):
  def  __init__(self, input_size, output_size): 
    super(Expert, self).__init__()
    self.linear = nn.Linear(input_size, output_size)
    
  def forward(self, data):
    x = self.linear(data)
    return x
 
 
class GatingNetwork(nn.Module):
  def __init__(self, input_size, num_experts):
    super(GatingNetwork, self).__init__()
    self.linear1 = nn.Linear(input_size, 4)
    self.relu = nn.ReLU()
    self.linear2 = nn.Linear(4, num_experts)
    self.softmax = nn.Softmax(dim=-1)
  
  def forward(self, data): 
    x = self.linear1(data)
    x = self.relu(x)
    x = self.linear2(x)
    x = self.softmax(x)
    return x
 
 
class MixtureOfExperts(nn.Module):
  def __init__(self, num_experts=2):
    super(MixtureOfExperts, self).__init__()  
    self.expert1 = Expert(2,1)
    self.expert2 = Expert(2,1)
    self.gating =  GatingNetwork(2, num_experts)
    self.sigmoid = nn.Sigmoid()
      
  def forward(self, data):
    expert1_output = self.expert1(data)
    expert2_output = self.expert2(data)  
    
    gating_output =  self.gating(data)
 
    s = (expert1_output*gating_output[:,0][:,None] + 
         expert2_output*gating_output[:,1][:,None])
    
    a = self.sigmoid(s)
    
    return a
 
  def backward(self, y_hat, labels, criterion, optimizer): 
    optimizer.zero_grad()
    loss = criterion(y_hat, labels)    
    loss.backward()
    optimizer.step()
    return loss.item()
 
 
# Define the model, loss, and optimizer
moe = MixtureOfExperts()
criterion = nn.MSELoss() 
optimizer = torch.optim.Adam(moe.parameters(),lr=0.01)
 
# Define the learning rate scheduler
scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer) 
 
# Convert data and labels to float tensors
data_tensor = data.float()
labels_tensor = labels.view(-1, 1).float()
 
# Training loop
num_epochs = 500 
for epoch in tqdm(range(num_epochs)):
    # Forward pass
    y_hat = moe.forward(data)
 
    # Backward pass and optimization
    loss_value = moe.backward(y_hat, labels_tensor, criterion, optimizer)
 
    # Decay the learning rate
    scheduler.step()
 
 
expert1_weights = moe.expert1.linear.weight.detach()[0,0]
expert2_weights = moe.expert2.linear.weight.detach()[0,0]
 
expert1_bias = moe.expert1.linear.bias.detach()
expert2_bias = moe.expert2.linear.bias.detach()
 
gating_weights = moe.gating.linear2.weight.detach().flatten()
 
x_line = np.linspace(min(data[:, 0]), max(data[:, 0]), 100)
 
y_line1 = expert1_weights * x_line + 5
y_line2 = expert2_weights * x_line + 5
 
class_0 = data[labels == 0]
class_1 = data[labels == 1]
plt.scatter(class_0[:, 0], class_0[:, 1], label='Class 0', alpha=0.5)
plt.scatter(class_1[:, 0], class_1[:, 1], label='Class 1', alpha=0.5)
 
plt.plot(x_line, y_line1, label='Expert 1', alpha = 1)
plt.plot(x_line, y_line2, label='Expert 2', alpha = 1)
 
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid(True, c='gray')
plt.legend()
plt.autoscale()
plt.show()</code></pre>
    <h2 id="4.%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE" name="4.%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">
     4.参考文献
    </h2>
    <p>
     <br/>
     【1】Fedus, William, Barret Zoph, and Noam Shazeer. "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity." Journal of Machine Learning Research 23.120 (2022): 1-39.
    </p>
    <p>
     【2】
     <a href="https://huggingface.co/blog/moe" rel="nofollow" title="Mixture of Experts Explained">
      Mixture of Experts Explained
     </a>
    </p>
    <p>
     【3】
     <a href="https://medium.com/@marketing_novita.ai/deep-dive-into-mixture-of-experts-for-llm-models-4bf609bc82a0" rel="nofollow" title="Deep Dive into Mixture of Experts for LLM Models">
      Deep Dive into Mixture of Experts for LLM Models
     </a>
    </p>
    <p>
     【4】
     <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/" rel="nofollow" title="Our next-generation model: Gemini 1.5">
      Our next-generation model: Gemini 1.5
     </a>
    </p>
    <p>
     【5】
     <a href="https://arxiv.org/abs/2403.05530" rel="nofollow" title="Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context">
      Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
     </a>
    </p>
    <p>
     【6】
     <a href="https://www.baeldung.com/cs/gemini-google-deepmind" rel="nofollow" title="An Introduction to Gemini by Google DeepMind">
      An Introduction to Gemini by Google DeepMind
     </a>
    </p>
    <p>
     【7】
     <a href="https://medium.com/@oxenai/arxiv-dives-how-mixture-of-experts-works-with-mixtral-8x7b-4acbbc64010d" rel="nofollow" title="Arxiv Dives — How Mixture of Experts works with Mixtral 8x7B">
      Arxiv Dives — How Mixture of Experts works with Mixtral 8x7B
     </a>
    </p>
    <p>
     【8】
     <a href="https://arxiv.org/pdf/2401.04088" rel="nofollow" title="Mixtral of Experts">
      Mixtral of Experts
     </a>
    </p>
    <p>
     【9】
     <a href="https://arxiv.org/pdf/1701.06538" rel="nofollow" title="OUTRAGEOUSLY LARGE NEURAL NETWORKS">
      OUTRAGEOUSLY LARGE NEURAL NETWORKS
     </a>
     : THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER
    </p>
    <p>
     【10】
     <a href="https://arxiv.org/pdf/2209.01667" rel="nofollow" title="A REVIEW OF SPARSE EXPERT MODELS IN DEEP LEARNING">
      A REVIEW OF SPARSE EXPERT MODELS IN DEEP LEARNING
     </a>
    </p>
    <p>
     【11】
     <a href="https://github.com/ozyurtf/mixture-of-experts?tab=readme-ov-file" title="Mixture of Experts Model">
      Mixture of Experts Model
     </a>
    </p>
    <p>
     【12】
     <a href="https://www.kaggle.com/code/aliabdin1/how-does-mixture-of-experts-moe-work" rel="nofollow" title="How does Mixture-of-Experts (MoE) work">
      How does Mixture-of-Experts (MoE) work
     </a>
    </p>
    <p>
     【13】
     <a href="https://towardsdatascience.com/moe-moa-for-large-language-models-c1cafeffd6a5" rel="nofollow" title="MOE &amp; MOA for Large Language Models">
      MOE &amp; MOA for Large Language Models
     </a>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35383630323535322f:61727469636c652f64657461696c732f313436303732343630" class_="artid" style="display:none">
 </p>
</div>


