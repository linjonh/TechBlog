---
layout: post
title: "XGBoost介绍"
date: 2025-03-10 17:44:58 +0800
description: "XGBoost介绍"
keywords: "XGBoost介绍"
categories: ['Learning', 'Deep']
tags: ['Xgboost']
artid: "146159691"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146159691
    alt: "XGBoost介绍"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146159691
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146159691
cover: https://bing.ee123.net/img/rand?artid=146159691
image: https://bing.ee123.net/img/rand?artid=146159691
img: https://bing.ee123.net/img/rand?artid=146159691
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     XGBoost介绍
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     XGBoost：
     <strong>
      是eXtreme Gradient Boosting(极端梯度提升)的缩写，是一种强大的集成学习(ensemble learning)算法
     </strong>
     ，旨在提高效率、速度和高性能。XGBoost是梯度提升(Gradient Boosting)的优化实现。集成学习将多个弱模型组合起来，形成一个更强大的模型。可用于回归、分类、排序。
    </p>
    <p>
     源码地址：
     <a class="link-info" href="https://github.com/dmlc/xgboost" title="https://github.com/dmlc/xgboost">
      https://github.com/dmlc/xgboost
     </a>
     ，它是经过优化的，可扩展、可移植和分布式梯度提升(GBDT, GBRT or GBM)库，适用于Python、C++等，可在单机、Hadoop、Spark等上运行，支持在Linux、Mac、Windows上的安装，license为Apache-2.0，最新发布版本为2.1.4。
    </p>
    <p>
     <strong>
      XGBoost使用决策树(decision trees)作为基础学习器(learners)
     </strong>
     ，按顺序组合它们以提高模型的性能。每棵新树都经过训练以纠正前一棵树的错误，这个过程称为提升(boosting)。该过程可以分解如下：
    </p>
    <p>
     (1).从基础学习器开始：第一个模型决策树在数据上进行训练。在回归任务中，这个基础模型只是预测目标变量的平均值。
    </p>
    <p>
     (2).计算误差：训练第一棵树后，计算预测值和实际值之间的误差。
    </p>
    <p>
     (3).训练下一棵树：下一棵树在前一棵树的错误上进行训练。此步骤尝试纠正第一棵树所犯的错误。
    </p>
    <p>
     (4).重复该过程：这个过程继续进行，每棵新树都试图纠正前一棵树的错误，直到满足停止标准。
    </p>
    <p>
     (5).合并预测：最终预测是所有树的预测总和。
    </p>
    <p>
     XGBoost模型主要保存为二进制文件UBJSON、也可选择JSON或文本格式。
    </p>
    <p>
     <strong>
      XGBoost优势
     </strong>
     ：
    </p>
    <p>
     (1).具有高度可扩展性和高效率，适合处理大型数据集。
    </p>
    <p>
     (2).实现并行处理技术并利用硬件优化来加快训练过程，在训练期间使用所有CPU内核并行构建树。
    </p>
    <p>
     (3).提供了广泛的可自定义参数和正则化技术，允许用户根据自己的特定需求对模型进行微调。
    </p>
    <p>
     (4).分布式计算，可使用一组机器训练非常大的模型。
    </p>
    <p>
     XGBoost缺点：
    </p>
    <p>
     (1).计算量非常大，尤其是在训练复杂模型时，因此不太适合资源受限的系统。
    </p>
    <p>
     (2).对噪声数据或异常值很敏感，因此需要数据预处理才能获得最佳性能。
    </p>
    <p>
     (3).在小数据集上或在模型中使用过多树时容易过拟合。
    </p>
    <p>
     <strong>
      注意
     </strong>
     ：
    </p>
    <p>
     (1).只有Linux平台支持使用多个GPU进行训练。
    </p>
    <p>
     (2).本地安装时，Linux上需要glibc 2.28+；Windows上需要安装Visual C++ Redistributable。pip的版本需要21.3+。
    </p>
    <p>
     (3).使用pip的默认安装("pip install xgboost")将安装完整的XGBoost包，包括对GPU算法和联合学习(federated learning)的支持。可安装仅cpu版的，执行"pip install xgboost-cpu"，此版本将减少安装包的大小并节省磁盘空间，但不提供某些功能，如GPU算法和联合学习。
    </p>
    <p>
     (4).通过Conda安装，执行"conda install -c conda-forge py-xgboost"，Conda应该能够检测到您的机器上是否存在GPU，并安装正确的XGBoost变体。也可指定仅安装cpu版本，执行"conda install -c conda-forge py-xgboost-cpu"。
    </p>
    <p>
     注：以上整理的内容主要来自：
    </p>
    <p>
     1.
     <a class="link-info" href="https://xgboost.readthedocs.io/en/latest/" rel="nofollow" title="https://xgboost.readthedocs.io/en/latest/">
      https://xgboost.readthedocs.io/en/latest/
     </a>
    </p>
    <p>
     2.
     <a class="link-info" href="http://ttps://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/" rel="nofollow" title="ttps://machinelearningmastery.com">
      ttps://machinelearningmastery.com
     </a>
    </p>
    <p>
     3.
     <a class="link-info" href="https://www.geeksforgeeks.org/xgboost/" rel="nofollow" title="https://www.geeksforgeeks.org/xgboost/">
      https://www.geeksforgeeks.org/xgboost/
     </a>
    </p>
    <p>
     以下Python测试代码用于回归，使用波士顿房价数据集，共包含506个样本，每个样本有13个特征和1个目标变量：
    </p>
    <pre><code class="language-python">import colorama
import argparse
import pandas as pd
import xgboost as xgb

def parse_args():
	parser = argparse.ArgumentParser(description="test XGBoost")
	parser.add_argument("--task", required=True, type=str, choices=["regress", "classify", "rank"], help="specify what kind of task")
	parser.add_argument("--csv", required=True, type=str, help="source csv file")
	parser.add_argument("--model", required=True, type=str, help="model file, save or load")

	args = parser.parse_args()
	return args

def split_train_test(X, y):
	X = X.sample(frac=1, random_state=42).reset_index(drop=True) # random_state=42: make the results consistent each time
	y = y.sample(frac=1, random_state=42).reset_index(drop=True)

	index = int(len(X) * 0.8)
	X_train, X_test = X[:index], X[index:]
	y_train, y_test = y[:index], y[index:]
	return X_train, X_test, y_train, y_test

def calculate_rmse(input, target): # Root Mean Squared Error
	return (sum((input - target) ** 2) / len(input)) ** 0.5

def regress(csv_file, model_file):
	# 1. load data
	data = pd.read_csv(csv_file)

	# 2. split into training set and test se
	X = data.drop('MEDV', axis=1)
	y = data['MEDV']
	print(f"X: type: {type(X)}, shape: {X.shape}; y: type: {type(X)}, shape: {y.shape}")

	X_train, X_test, y_train, y_test = split_train_test(X, y)

	train_dmatrix = xgb.DMatrix(X_train, label=y_train)
	test_dmatrix = xgb.DMatrix(X_test, label=y_test)
	print(f"train_dmatrix type: {type(train_dmatrix)}, shape(h,w): {train_dmatrix.num_row()}, {train_dmatrix.num_col()}")

	# 3. set XGBoost params
	params = {
		'objective': 'reg:squarederror', # specify the learning task: classify: binary:logistic or multi:softmax or multi:softprob; rank: rank:ndcg
		'max_depth': 5, # maximum tree depth
		'eta': 0.1, # learning rate
		'subsample': 0.8, # subsample ratio of the training instance
		'colsample_bytree': 0.8, # subsample ratio of columns when constructing each tree
		'seed': 42, # random number seed
		'eval_metric': 'rmse' # metric used for monitoring the training result and early stopping
	}

	# 4. train model
	best = xgb.train(params, train_dmatrix, num_boost_round=1000) # num_boost_round: epochs

	# 5. predict
	y_pred = best.predict(test_dmatrix)
	# print(f"y_pred: {y_pred}")

	# 6. evaluate the model
	rmse = calculate_rmse(y_test, y_pred)
	print(f"rmse: {rmse}")

	# 7. save model
	best.save_model(model_file)

	# 8. load mode and predict
	model = xgb.Booster()
	model.load_model(model_file)
	result = model.predict(test_dmatrix)

	test_label = test_dmatrix.get_label()
	for idx in range(len(result)):
		print(f"ground truth: {test_label[idx]:.1f}, \tpredict: {result[idx]:.1f}")

if __name__ == "__main__":
	print("xgboost version:", xgb.__version__)
	colorama.init(autoreset=True)
	args = parse_args()

	if args.task == "regress":
		regress(args.csv, args.model)

	print(colorama.Fore.GREEN + "====== execution completed ======")</code></pre>
    <p>
     执行结果如下图所示：
    </p>
    <p style="text-align:center">
     <img alt="" src="https://i-blog.csdnimg.cn/direct/b98e2c8259dd4547a6d7d28fb50d1aba.png"/>
    </p>
    <p>
     <strong>
      GitHub
     </strong>
     ：
     <a class="link-info" href="https://github.com/fengbingchun/NN_Test" title="https://github.com/fengbingchun/NN_Test">
      https://github.com/fengbingchun/NN_Test
     </a>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c:6f672e6373646e2e6e65742f66656e6762696e676368756e2f:61727469636c652f64657461696c732f313436313539363931" class_="artid" style="display:none">
 </p>
</div>


