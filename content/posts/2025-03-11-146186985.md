---
layout: post
title: "DeepSeek模型本地化部署方案及Python实现"
date: 2025-03-11 19:26:37 +0800
description: "DeepSeek实在是太火了，虽然经过扩容和调整，但反应依旧不稳定，甚至小圆圈转半天最后却提示“服务器繁忙，请稍后再试。” 故此，本文通过讲解在本地部署 DeepSeek并配合python代码实现，让你零成本搭建自己的AI助理，无惧任务提交失败的压力。"
keywords: "DeepSeek模型本地化部署方案及Python实现"
categories: ['Ai']
tags: ['语言模型', '算法', '深度学习', '本地化部署', '人工智能', 'Deepseek', 'Ai']
artid: "146186985"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146186985
    alt: "DeepSeek模型本地化部署方案及Python实现"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146186985
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146186985
cover: https://bing.ee123.net/img/rand?artid=146186985
image: https://bing.ee123.net/img/rand?artid=146186985
img: https://bing.ee123.net/img/rand?artid=146186985
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     DeepSeek模型本地化部署方案及Python实现
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fba44171a8c1433fba488c0149dceda5.png"/>
    </p>
    <p>
     DeepSeek实在是太火了，虽然经过扩容和调整，但反应依旧不稳定，甚至小圆圈转半天最后却提示“服务器繁忙，请稍后再试。” 故此，本文通过讲解在本地部署 DeepSeek并配合python代码实现，让你零成本搭建自己的AI助理，无惧任务提交失败的压力。
    </p>
    <h4>
     <a id="_5">
     </a>
     一、环境准备
    </h4>
    <h5>
     <a id="1__6">
     </a>
     1. 安装依赖库
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 创建虚拟环境（可选但推荐）</span>
python <span class="token parameter variable">-m</span> venv deepseek_env
<span class="token builtin class-name">source</span> deepseek_env/bin/activate  <span class="token comment"># Linux/Mac</span>
deepseek_env<span class="token punctuation">\</span>Scripts<span class="token punctuation">\</span>activate.bat  <span class="token comment"># Windows</span>

<span class="token comment"># 安装核心依赖</span>
pip <span class="token function">install</span> transformers torch flask accelerate sentencepiece
</code></pre>
    <h5>
     <a id="2__17">
     </a>
     2. 验证安装
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"PyTorch version:"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"CUDA available:"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_26">
     </a>
     二、模型下载与加载
    </h4>
    <h5>
     <a id="1_DeepSeek7BChat_27">
     </a>
     1. 下载模型（以DeepSeek-7B-Chat为例）
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> huggingface_hub <span class="token keyword">import</span> snapshot_download

snapshot_download<span class="token punctuation">(</span>repo_id<span class="token operator">=</span><span class="token string">"deepseek-ai/deepseek-llm-7b-chat"</span><span class="token punctuation">,</span>
                  local_dir<span class="token operator">=</span><span class="token string">"./deepseek-7b-chat"</span><span class="token punctuation">,</span>
                  local_dir_use_symlinks<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="2__36">
     </a>
     2. 模型加载代码
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer

model_path <span class="token operator">=</span> <span class="token string">"./deepseek-7b-chat"</span>  <span class="token comment"># 或在线模型ID</span>

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_path<span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_path<span class="token punctuation">,</span>
    trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="APIFlask_52">
     </a>
     三、API服务部署（使用Flask）
    </h4>
    <h5>
     <a id="1_APIapppy_53">
     </a>
     1. 创建API服务文件（app.py）
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> flask <span class="token keyword">import</span> Flask<span class="token punctuation">,</span> request<span class="token punctuation">,</span> jsonify
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoModelForCausalLM<span class="token punctuation">,</span> AutoTokenizer
<span class="token keyword">import</span> torch

app <span class="token operator">=</span> Flask<span class="token punctuation">(</span>__name__<span class="token punctuation">)</span>

<span class="token comment"># 初始化模型</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"./deepseek-7b-chat"</span><span class="token punctuation">,</span> trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    <span class="token string">"./deepseek-7b-chat"</span><span class="token punctuation">,</span>
    trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span>
<span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token decorator annotation punctuation">@app<span class="token punctuation">.</span>route</span><span class="token punctuation">(</span><span class="token string">'/generate'</span><span class="token punctuation">,</span> methods<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'POST'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">generate_text</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    data <span class="token operator">=</span> request<span class="token punctuation">.</span>json
    inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'prompt'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>model<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>
            <span class="token operator">**</span>inputs<span class="token punctuation">,</span>
            max_new_tokens<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
            temperature<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">,</span>
            top_p<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">,</span>
            repetition_penalty<span class="token operator">=</span><span class="token number">1.1</span>
        <span class="token punctuation">)</span>
    
    response <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> jsonify<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"response"</span><span class="token punctuation">:</span> response<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    app<span class="token punctuation">.</span>run<span class="token punctuation">(</span>host<span class="token operator">=</span><span class="token string">'0.0.0.0'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">,</span> threaded<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="2__92">
     </a>
     2. 启动服务
    </h5>
    <pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable">FLASK_APP</span><span class="token operator">=</span>app.py
flask run <span class="token parameter variable">--port</span><span class="token operator">=</span><span class="token number">5000</span>
</code></pre>
    <h4>
     <a id="_98">
     </a>
     四、效果验证与测试
    </h4>
    <h5>
     <a id="1__99">
     </a>
     1. 基础功能测试
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">import</span> requests

url <span class="token operator">=</span> <span class="token string">"http://localhost:5000/generate"</span>
headers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"Content-Type"</span><span class="token punctuation">:</span> <span class="token string">"application/json"</span><span class="token punctuation">}</span>

data <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"prompt"</span><span class="token punctuation">:</span> <span class="token string">"如何制作美味的法式洋葱汤？"</span><span class="token punctuation">,</span>
    <span class="token string">"max_tokens"</span><span class="token punctuation">:</span> <span class="token number">300</span>
<span class="token punctuation">}</span>

response <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> json<span class="token operator">=</span>data<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="2_locust_115">
     </a>
     2. 压力测试（使用locust）
    </h5>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> locust
</code></pre>
    <p>
     创建locustfile.py：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> locust <span class="token keyword">import</span> HttpUser<span class="token punctuation">,</span> task<span class="token punctuation">,</span> between

<span class="token keyword">class</span> <span class="token class-name">ModelUser</span><span class="token punctuation">(</span>HttpUser<span class="token punctuation">)</span><span class="token punctuation">:</span>
    wait_time <span class="token operator">=</span> between<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@task</span>
    <span class="token keyword">def</span> <span class="token function">generate_request</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        payload <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"prompt"</span><span class="token punctuation">:</span> <span class="token string">"解释量子力学的基本原理"</span><span class="token punctuation">,</span>
            <span class="token string">"max_tokens"</span><span class="token punctuation">:</span> <span class="token number">200</span>
        <span class="token punctuation">}</span>
        self<span class="token punctuation">.</span>client<span class="token punctuation">.</span>post<span class="token punctuation">(</span><span class="token string">"/generate"</span><span class="token punctuation">,</span> json<span class="token operator">=</span>payload<span class="token punctuation">)</span>
</code></pre>
    <p>
     启动压力测试：
    </p>
    <pre><code class="prism language-bash">locust <span class="token parameter variable">-f</span> locustfile.py
</code></pre>
    <h5>
     <a id="3__141">
     </a>
     3. 效果验证指标
    </h5>
    <ul>
     <li>
      响应时间：平均响应时间应 &lt; 5秒（根据硬件配置）
     </li>
     <li>
      错误率：HTTP 500错误率应 &lt; 1%
     </li>
     <li>
      内容质量：人工评估返回结果的逻辑性和相关性
     </li>
     <li>
      吞吐量：单卡应能处理 5-10 req/s（取决于GPU型号）
     </li>
    </ul>
    <h4>
     <a id="_147">
     </a>
     五、生产部署建议
    </h4>
    <ol>
     <li>
      性能优化：
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token comment"># 在模型加载时添加优化参数</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_path<span class="token punctuation">,</span>
    trust_remote_code<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
    attn_implementation<span class="token operator">=</span><span class="token string">"flash_attention_2"</span><span class="token punctuation">,</span>  <span class="token comment"># 使用Flash Attention</span>
<span class="token punctuation">)</span>
</code></pre>
    <ol start="2">
     <li>
      使用生产级服务器：
     </li>
    </ol>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> gunicorn
gunicorn <span class="token parameter variable">-w</span> <span class="token number">4</span> <span class="token parameter variable">-b</span> <span class="token number">0.0</span>.0.0:5000 app:app
</code></pre>
    <ol start="3">
     <li>
      容器化部署（Dockerfile示例）：
     </li>
    </ol>
    <pre><code class="prism language-dockerfile">FROM python:3.9-slim

WORKDIR /app
COPY . .

RUN pip install --no-cache-dir transformers torch flask accelerate sentencepiece

EXPOSE 5000
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
</code></pre>
    <h4>
     <a id="_179">
     </a>
     六、常见问题排查
    </h4>
    <ol>
     <li>
      <p>
       CUDA内存不足：
      </p>
      <ul>
       <li>
        减小max_new_tokens参数
       </li>
       <li>
        使用量化加载：
        <pre><code class="prism language-python">model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_path<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
</code></pre>
       </li>
      </ul>
     </li>
     <li>
      <p>
       响应速度慢：
      </p>
      <ul>
       <li>
        启用缓存（在generate参数中添加
        <code>
         use_cache=True
        </code>
        ）
       </li>
       <li>
        使用批处理（需要修改API设计）
       </li>
      </ul>
     </li>
     <li>
      <p>
       中文支持问题：
      </p>
      <ul>
       <li>
        确保使用正确的分词器
       </li>
       <li>
        在prompt中添加中文指令前缀：
        <pre><code class="prism language-python">prompt <span class="token operator">=</span> <span class="token string">"&lt;|im_start|&gt;user\n请用中文回答：{你的问题}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"</span>
</code></pre>
       </li>
      </ul>
     </li>
    </ol>
    <p>
     以上部署方案在NVIDIA T4 GPU（16GB显存）上实测可用，如需部署更大模型（如67B版本），建议使用A100（80GB）级别GPU并调整device_map策略。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c:6f672e6373646e2e6e65742f616769746f5f636865756e672f:61727469636c652f64657461696c732f313436313836393835" class_="artid" style="display:none">
 </p>
</div>


