---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f636979756e646174612f:61727469636c652f64657461696c732f313436303335353431"
layout: post
title: "本地部署类似-ChatGPT-的大模型基于-Ollama-Open-WebUI"
date: 2025-03-05 14:30:00 +08:00
description: "是一款自托管的 Web 界面，支持 Ollama、OpenAI 兼容 API，完全离线运行。是一个大模型容器管理框架，可帮助用户快速在本地运行大模型，类似于 Docker。如果出现版本号，则安装成功。部署类似 ChatGPT 的网页版大模型，实现流畅的自然语言交互。提供的云服务器，支持高性能计算任务，适合 AI 模型训练与推理。完成上述步骤后，你的本地 ChatGPT 即可投入使用！云服务器，提供稳定的计算资源和高可用性支持。默认界面为英文，若不习惯，可以调整为中文。，会自动根据硬件选择最佳运行方式。"
keywords: "类似于chatgpt web的js框架"
categories: ['博客', 'Linux', 'Ai']
tags: ['服务器', '人工智能', 'Gpt', 'Chatgpt']
artid: "146035541"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146035541
    alt: "本地部署类似-ChatGPT-的大模型基于-Ollama-Open-WebUI"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146035541
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146035541
cover: https://bing.ee123.net/img/rand?artid=146035541
image: https://bing.ee123.net/img/rand?artid=146035541
img: https://bing.ee123.net/img/rand?artid=146035541
---

# 本地部署类似 ChatGPT 的大模型：基于 Ollama + Open-WebUI

### **一、效果预览**

本教程介绍如何在本地使用
**Ollama**
和
**Open-WebUI**
部署类似 ChatGPT 的网页版大模型，实现流畅的自然语言交互。

![](https://i-blog.csdnimg.cn/direct/57d7735c6b874eba92d075cc56514f42.png)

默认界面为英文，若不习惯，可以调整为中文。

---

### **二、部署 Ollama**

![](https://i-blog.csdnimg.cn/direct/2d9415a7f55149c0ad48448a0c0d8fa2.png)

#### **1. Ollama 说明**

**Ollama**
是一个大模型容器管理框架，可帮助用户快速在本地运行大模型，类似于 Docker。它支持
**GPU 和 CPU**
，会自动根据硬件选择最佳运行方式。

* 官网：
  [https://www.ollama.com](https://www.ollama.com/ "https://www.ollama.com")
* GitHub 项目地址：
  [GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.](https://github.com/ollama/ollama "GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.")

> **硬件要求：**
>
> * Windows 10 及以上
> * **Nvidia**
>   显卡要求
>   **计算能力 5.0+**
> * 兼容
>   **AMD GPU**
>   （详见
>   [官方文档](https://github.com/ollama/ollama/blob/main/docs/gpu.md "官方文档")
>   ）
> * 支持
>   **量化模型**
>   ，例如 LLaMA3-8B 量化后仅
>   **4.7G**
>   （原始大小约
>   **15G**
>   ）

---

#### **2. 服务器选择**

对于
**云端运行**
或
**本地部署**
，推荐使用
**慈云数据**
提供的云服务器，支持高性能计算任务，适合 AI 模型训练与推理。

**推荐配置**
（基于
**慈云数据**
云服务器）：

* **2 核 4G**
  ：适用于轻量级本地推理测试
* **4 核 8G**
  ：适用于中型模型推理
* **8 核 16G**
  及以上：适用于大型模型的并行计算

可前往
**[慈云数据官网](https://www.zovps.com/ "慈云数据官网")**
选择合适的服务器方案。

---

#### **3. 安装 Ollama**

##### **Windows 系统**

###### **3.1 下载安装包**

直接下载并安装
[Ollama Windows 版](https://ollama.com/download "Ollama Windows 版")
，安装路径默认为：

```
C:\Users\Administrator\AppData\Local\Programs\Ollama

```

###### **3.2 验证安装**

安装完成后，在
**CMD 命令行**
输入：

```
ollama -v

```

如果出现版本号，则安装成功。若 Ollama 进程意外关闭，可用以下命令重启：

```
ollama serve

```

###### **3.3 设置模型文件存储位置**

避免模型文件占用系统盘，可设置环境变量：

```
OLLAMA_MODELS=D:\ollama_models

```

如果不设置，默认存储在
`C:\Users\用户名ollama\models`
。

###### **3.4 下载 LLaMA3-8B**

```
ollama pull llama3:8b

```

运行模型：

```
ollama run llama3:8b

```

如遇
**下载速度变慢**
，可
`Ctrl+C`
终止后
**重新下载**
，通常速度会恢复。

---

##### **Linux 系统**

###### **3.5 安装 Ollama**

```
curl -fsSL https://ollama.com/install.sh | sh

```

###### **3.6 设置环境变量**

```
echo 'export OLLAMA_HOST="0.0.0.0:11434"' >> ~/.bashrc
echo 'export OLLAMA_MODELS=/root/ollama/models' >> ~/.bashrc
source ~/.bashrc

```

###### **3.7 运行 Ollama**

```
ollama serve

```

###### **3.8 下载并运行 LLaMA3-8B**

```
ollama run llama3:8b

```

---

### **三、部署 Open-WebUI**

#### **1. Open-WebUI 介绍**

**Open-WebUI**
是一款自托管的 Web 界面，支持 Ollama、OpenAI 兼容 API，完全离线运行。

* GitHub 项目地址：
  [GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)](https://github.com/open-webui/open-webui "GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)")

---

#### **2. 安装 Open-WebUI**

![](https://i-blog.csdnimg.cn/direct/d5703ad70b444707824c7c88ce6467da.png)

##### **2.1 Windows**

###### **2.1.1 下载源码**

从
**GitHub**
或
**Gitee**
获取
`open-webui`
源码：

```
git clone https://github.com/open-webui/open-webui.git

```

复制
`.env`
文件：

```
cp .env.example .env

```

###### **2.1.2 配置 Python 虚拟环境**

使用
**Pycharm**
或手动创建 Python 虚拟环境：

```
python -m venv venv
source venv/bin/activate

```

###### **2.1.3 安装 Node.js 依赖**

```
npm config set registry https://mirrors.huaweicloud.com/repository/npm/
npm i
npm run build

```

###### **2.1.4 安装 Python 依赖**

```
cd backend
pip install -r requirements.txt

```

###### **2.1.5 启动 WebUI**

```
start_windows.bat

```

此时，访问
**[http://localhost:3000](http://localhost:3000/ "http://localhost:3000")**
即可使用 WebUI。

---

##### **2.2 Linux**

###### **2.2.1 安装 Node.js**

```
curl -sL https://deb.nodesource.com/setup_20.x | bash -
apt install nodejs -y

```

###### **2.2.2 安装 Miniconda**

```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
bash miniconda.sh -b -u -p ~/miniconda3
source ~/.bashrc

```

###### **2.2.3 创建 Python 虚拟环境**

```
conda create -n open-webui python=3.8
conda activate open-webui

```

###### **2.2.4 下载源码**

```
git clone https://gitee.com/pandaworker/open-webui.git
cd open-webui
cp -RPp .env.example .env

```

###### **2.2.5 安装前端依赖**

```
npm i
npm run build

```

###### **2.2.6 安装后端依赖**

```
cd backend
pip install -r requirements.txt

```

###### **2.2.7 启动 Open-WebUI**

```
bash start.sh

```

访问
**[http://localhost:3000](http://localhost:3000/ "http://localhost:3000")**
即可使用 WebUI。

---

### **四、注意事项**

1. **管理员账户**
   ：注册的
   **第一个用户**
   为管理员账户，其他用户需手动分配权限。
2. **修改默认用户角色**
   ：可在
   `backend/config.py`
   中更改：

   ```
   DEFAULT_USER_ROLE = "admin"  # 修改为 "user" 或 "admin"

   ```
3. **常见问题**
   ：
   * **Ollama 运行失败 (
     `Error: llama runner process no longer running: 3221225785`
     )**
       
     可能是
     **Ollama 版本过高**
     ，建议降级到
     `0.1.31`
     ，可在
     [博客首页](https://chatgpt.com/c/67c6fa5d-027c-8003-8734-4da8e7723cef# "博客首页")
     获取旧版本下载链接。

---

### **总结**

本教程介绍了如何在
**本地服务器或云服务器**
上部署
**Ollama + Open-WebUI**
以实现类似
**ChatGPT**
的大模型服务。对于
**云端部署**
，建议使用
**[慈云数据](https://www.zovps.com/ "慈云数据")**
云服务器，提供稳定的计算资源和高可用性支持。

完成上述步骤后，你的本地 ChatGPT 即可投入使用！🚀