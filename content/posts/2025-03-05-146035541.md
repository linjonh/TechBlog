---
layout: post
title: "本地部署类似-ChatGPT-的大模型基于-Ollama-Open-WebUI"
date: 2025-03-05 14:30:00 +0800
description: "是一款自托管的 Web 界面，支持 Ollama、OpenAI 兼容 API，完全离线运行。是一个大模型容器管理框架，可帮助用户快速在本地运行大模型，类似于 Docker。如果出现版本号，则安装成功。部署类似 ChatGPT 的网页版大模型，实现流畅的自然语言交互。提供的云服务器，支持高性能计算任务，适合 AI 模型训练与推理。完成上述步骤后，你的本地 ChatGPT 即可投入使用！云服务器，提供稳定的计算资源和高可用性支持。默认界面为英文，若不习惯，可以调整为中文。，会自动根据硬件选择最佳运行方式。"
keywords: "类似于chatgpt web的js框架"
categories: ['博客', 'Linux', 'Ai']
tags: ['服务器', '人工智能', 'Gpt', 'Chatgpt']
artid: "146035541"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146035541
    alt: "本地部署类似-ChatGPT-的大模型基于-Ollama-Open-WebUI"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146035541
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146035541
cover: https://bing.ee123.net/img/rand?artid=146035541
image: https://bing.ee123.net/img/rand?artid=146035541
img: https://bing.ee123.net/img/rand?artid=146035541
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     本地部署类似 ChatGPT 的大模型：基于 Ollama + Open-WebUI
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
    </p>
    <h3>
     <strong>
      一、效果预览
     </strong>
    </h3>
    <p>
     本教程介绍如何在本地使用
     <strong>
      Ollama
     </strong>
     和
     <strong>
      Open-WebUI
     </strong>
     部署类似 ChatGPT 的网页版大模型，实现流畅的自然语言交互。
    </p>
    <p>
     <img alt="" height="159" src="https://i-blog.csdnimg.cn/direct/57d7735c6b874eba92d075cc56514f42.png" width="318"/>
    </p>
    <p>
     默认界面为英文，若不习惯，可以调整为中文。
    </p>
    <hr/>
    <h3>
     <strong>
      二、部署 Ollama
     </strong>
    </h3>
    <p>
     <img alt="" height="172" src="https://i-blog.csdnimg.cn/direct/2d9415a7f55149c0ad48448a0c0d8fa2.png" width="293"/>
    </p>
    <h4>
     <strong>
      1. Ollama 说明
     </strong>
    </h4>
    <p>
     <strong>
      Ollama
     </strong>
     是一个大模型容器管理框架，可帮助用户快速在本地运行大模型，类似于 Docker。它支持
     <strong>
      GPU 和 CPU
     </strong>
     ，会自动根据硬件选择最佳运行方式。
    </p>
    <ul>
     <li>
      官网：
      <a href="https://www.ollama.com/" rel="nofollow" title="https://www.ollama.com">
       https://www.ollama.com
      </a>
     </li>
     <li>
      GitHub 项目地址：
      <a href="https://github.com/ollama/ollama" title="GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.">
       GitHub - ollama/ollama: Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 2, and other large language models.
      </a>
     </li>
    </ul>
    <blockquote>
     <p>
      <strong>
       硬件要求：
      </strong>
     </p>
     <ul>
      <li>
       Windows 10 及以上
      </li>
      <li>
       <strong>
        Nvidia
       </strong>
       显卡要求
       <strong>
        计算能力 5.0+
       </strong>
      </li>
      <li>
       兼容
       <strong>
        AMD GPU
       </strong>
       （详见
       <a href="https://github.com/ollama/ollama/blob/main/docs/gpu.md" title="官方文档">
        官方文档
       </a>
       ）
      </li>
      <li>
       支持
       <strong>
        量化模型
       </strong>
       ，例如 LLaMA3-8B 量化后仅
       <strong>
        4.7G
       </strong>
       （原始大小约
       <strong>
        15G
       </strong>
       ）
      </li>
     </ul>
    </blockquote>
    <hr/>
    <h4>
     <strong>
      2. 服务器选择
     </strong>
    </h4>
    <p>
     对于
     <strong>
      云端运行
     </strong>
     或
     <strong>
      本地部署
     </strong>
     ，推荐使用
     <strong>
      慈云数据
     </strong>
     提供的云服务器，支持高性能计算任务，适合 AI 模型训练与推理。
    </p>
    <p>
     <strong>
      推荐配置
     </strong>
     （基于
     <strong>
      慈云数据
     </strong>
     云服务器）：
    </p>
    <ul>
     <li>
      <strong>
       2 核 4G
      </strong>
      ：适用于轻量级本地推理测试
     </li>
     <li>
      <strong>
       4 核 8G
      </strong>
      ：适用于中型模型推理
     </li>
     <li>
      <strong>
       8 核 16G
      </strong>
      及以上：适用于大型模型的并行计算
     </li>
    </ul>
    <p>
     可前往
     <strong>
      <a href="https://www.zovps.com/" rel="nofollow" title="慈云数据官网">
       慈云数据官网
      </a>
     </strong>
     选择合适的服务器方案。
    </p>
    <hr/>
    <h4>
     <strong>
      3. 安装 Ollama
     </strong>
    </h4>
    <h5>
     <strong>
      Windows 系统
     </strong>
    </h5>
    <h6>
     <strong>
      3.1 下载安装包
     </strong>
    </h6>
    <p>
     直接下载并安装
     <a href="https://ollama.com/download" rel="nofollow" title="Ollama Windows 版">
      Ollama Windows 版
     </a>
     ，安装路径默认为：
    </p>
    <pre><code>C:\Users\Administrator\AppData\Local\Programs\Ollama
</code></pre>
    <h6>
     <strong>
      3.2 验证安装
     </strong>
    </h6>
    <p>
     安装完成后，在
     <strong>
      CMD 命令行
     </strong>
     输入：
    </p>
    <pre><code>ollama -v
</code></pre>
    <p>
     如果出现版本号，则安装成功。若 Ollama 进程意外关闭，可用以下命令重启：
    </p>
    <pre><code>ollama serve
</code></pre>
    <h6>
     <strong>
      3.3 设置模型文件存储位置
     </strong>
    </h6>
    <p>
     避免模型文件占用系统盘，可设置环境变量：
    </p>
    <pre><code>OLLAMA_MODELS=D:\ollama_models
</code></pre>
    <p>
     如果不设置，默认存储在
     <code>
      C:\Users\用户名\.ollama\models
     </code>
     。
    </p>
    <h6>
     <strong>
      3.4 下载 LLaMA3-8B
     </strong>
    </h6>
    <pre><code>ollama pull llama3:8b
</code></pre>
    <p>
     运行模型：
    </p>
    <pre><code>ollama run llama3:8b
</code></pre>
    <p>
     如遇
     <strong>
      下载速度变慢
     </strong>
     ，可
     <code>
      Ctrl+C
     </code>
     终止后
     <strong>
      重新下载
     </strong>
     ，通常速度会恢复。
    </p>
    <hr/>
    <h5>
     <strong>
      Linux 系统
     </strong>
    </h5>
    <h6>
     <strong>
      3.5 安装 Ollama
     </strong>
    </h6>
    <pre><code>curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
    <h6>
     <strong>
      3.6 设置环境变量
     </strong>
    </h6>
    <pre><code>echo 'export OLLAMA_HOST="0.0.0.0:11434"' &gt;&gt; ~/.bashrc
echo 'export OLLAMA_MODELS=/root/ollama/models' &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
    <h6>
     <strong>
      3.7 运行 Ollama
     </strong>
    </h6>
    <pre><code>ollama serve
</code></pre>
    <h6>
     <strong>
      3.8 下载并运行 LLaMA3-8B
     </strong>
    </h6>
    <pre><code>ollama run llama3:8b
</code></pre>
    <hr/>
    <h3>
     <strong>
      三、部署 Open-WebUI
     </strong>
    </h3>
    <h4>
     <strong>
      1. Open-WebUI 介绍
     </strong>
    </h4>
    <p>
     <strong>
      Open-WebUI
     </strong>
     是一款自托管的 Web 界面，支持 Ollama、OpenAI 兼容 API，完全离线运行。
    </p>
    <ul>
     <li>
      GitHub 项目地址：
      <a href="https://github.com/open-webui/open-webui" title="GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)">
       GitHub - open-webui/open-webui: User-friendly AI Interface (Supports Ollama, OpenAI API, ...)
      </a>
     </li>
    </ul>
    <hr/>
    <h4>
     <strong>
      2. 安装 Open-WebUI
     </strong>
    </h4>
    <p>
     <img alt="" height="176" src="https://i-blog.csdnimg.cn/direct/d5703ad70b444707824c7c88ce6467da.png" width="287"/>
    </p>
    <h5>
     <strong>
      2.1 Windows
     </strong>
    </h5>
    <h6>
     <strong>
      2.1.1 下载源码
     </strong>
    </h6>
    <p>
     从
     <strong>
      GitHub
     </strong>
     或
     <strong>
      Gitee
     </strong>
     获取
     <code>
      open-webui
     </code>
     源码：
    </p>
    <pre><code>git clone https://github.com/open-webui/open-webui.git
</code></pre>
    <p>
     复制
     <code>
      .env
     </code>
     文件：
    </p>
    <pre><code>cp .env.example .env
</code></pre>
    <h6>
     <strong>
      2.1.2 配置 Python 虚拟环境
     </strong>
    </h6>
    <p>
     使用
     <strong>
      Pycharm
     </strong>
     或手动创建 Python 虚拟环境：
    </p>
    <pre><code>python -m venv venv
source venv/bin/activate
</code></pre>
    <h6>
     <strong>
      2.1.3 安装 Node.js 依赖
     </strong>
    </h6>
    <pre><code>npm config set registry https://mirrors.huaweicloud.com/repository/npm/
npm i
npm run build
</code></pre>
    <h6>
     <strong>
      2.1.4 安装 Python 依赖
     </strong>
    </h6>
    <pre><code>cd backend
pip install -r requirements.txt
</code></pre>
    <h6>
     <strong>
      2.1.5 启动 WebUI
     </strong>
    </h6>
    <pre><code>start_windows.bat
</code></pre>
    <p>
     此时，访问
     <strong>
      <a href="http://localhost:3000/" rel="nofollow" title="http://localhost:3000">
       http://localhost:3000
      </a>
     </strong>
     即可使用 WebUI。
    </p>
    <hr/>
    <h5>
     <strong>
      2.2 Linux
     </strong>
    </h5>
    <h6>
     <strong>
      2.2.1 安装 Node.js
     </strong>
    </h6>
    <pre><code>curl -sL https://deb.nodesource.com/setup_20.x | bash -
apt install nodejs -y
</code></pre>
    <h6>
     <strong>
      2.2.2 安装 Miniconda
     </strong>
    </h6>
    <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
bash miniconda.sh -b -u -p ~/miniconda3
source ~/.bashrc
</code></pre>
    <h6>
     <strong>
      2.2.3 创建 Python 虚拟环境
     </strong>
    </h6>
    <pre><code>conda create -n open-webui python=3.8
conda activate open-webui
</code></pre>
    <h6>
     <strong>
      2.2.4 下载源码
     </strong>
    </h6>
    <pre><code>git clone https://gitee.com/pandaworker/open-webui.git
cd open-webui
cp -RPp .env.example .env
</code></pre>
    <h6>
     <strong>
      2.2.5 安装前端依赖
     </strong>
    </h6>
    <pre><code>npm i
npm run build
</code></pre>
    <h6>
     <strong>
      2.2.6 安装后端依赖
     </strong>
    </h6>
    <pre><code>cd backend
pip install -r requirements.txt
</code></pre>
    <h6>
     <strong>
      2.2.7 启动 Open-WebUI
     </strong>
    </h6>
    <pre><code>bash start.sh
</code></pre>
    <p>
     访问
     <strong>
      <a href="http://localhost:3000/" rel="nofollow" title="http://localhost:3000">
       http://localhost:3000
      </a>
     </strong>
     即可使用 WebUI。
    </p>
    <hr/>
    <h3>
     <strong>
      四、注意事项
     </strong>
    </h3>
    <ol>
     <li>
      <strong>
       管理员账户
      </strong>
      ：注册的
      <strong>
       第一个用户
      </strong>
      为管理员账户，其他用户需手动分配权限。
     </li>
     <li>
      <strong>
       修改默认用户角色
      </strong>
      ：可在
      <code>
       backend/config.py
      </code>
      中更改：
      <pre><code>DEFAULT_USER_ROLE = "admin"  # 修改为 "user" 或 "admin"
</code></pre>
     </li>
     <li>
      <strong>
       常见问题
      </strong>
      ：
      <ul>
       <li>
        <strong>
         Ollama 运行失败 (
         <code>
          Error: llama runner process no longer running: 3221225785
         </code>
         )
        </strong>
        <br/>
        可能是
        <strong>
         Ollama 版本过高
        </strong>
        ，建议降级到
        <code>
         0.1.31
        </code>
        ，可在
        <a href="https://chatgpt.com/c/67c6fa5d-027c-8003-8734-4da8e7723cef#" rel="nofollow" title="博客首页">
         博客首页
        </a>
        获取旧版本下载链接。
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h3>
     <strong>
      总结
     </strong>
    </h3>
    <p>
     本教程介绍了如何在
     <strong>
      本地服务器或云服务器
     </strong>
     上部署
     <strong>
      Ollama + Open-WebUI
     </strong>
     以实现类似
     <strong>
      ChatGPT
     </strong>
     的大模型服务。对于
     <strong>
      云端部署
     </strong>
     ，建议使用
     <strong>
      <a href="https://www.zovps.com/" rel="nofollow" title="慈云数据">
       慈云数据
      </a>
     </strong>
     云服务器，提供稳定的计算资源和高可用性支持。
    </p>
    <p>
     完成上述步骤后，你的本地 ChatGPT 即可投入使用！🚀
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f636979756e646174612f:61727469636c652f64657461696c732f313436303335353431" class_="artid" style="display:none">
 </p>
</div>


