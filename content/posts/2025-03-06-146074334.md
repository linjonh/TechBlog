---
layout: post
title: "DeepSeekUbuntu快速部署DeepSeekOllama方式"
date: 2025-03-06 16:47:51 +0800
description: "DeepSeek 作为一款先进的人工智能工具，具备强大的推理能力和广泛的应用场景，能够帮助用户高效解决复杂问题。它支持文本生成、代码编写、数据分析、情感分析等多种任务，适用于教育、医疗、金融、创意等各行各业。无论是提升工作效率、辅助学习，还是解决生活中的问题，DeepSeek 都能提供智能化支持。"
keywords: "部署deepseek ubuntu操作系统"
categories: ['人工智能']
tags: ['深度学习', 'Deepseek']
artid: "146074334"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146074334
    alt: "DeepSeekUbuntu快速部署DeepSeekOllama方式"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146074334
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146074334
cover: https://bing.ee123.net/img/rand?artid=146074334
image: https://bing.ee123.net/img/rand?artid=146074334
img: https://bing.ee123.net/img/rand?artid=146074334
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【DeepSeek】Ubuntu快速部署DeepSeek（Ollama方式）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-github-gist" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <br/>
    <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/724d5768dc8c409f860152e46eaaa6ca.png#pic_center">
     <p>
     </p>
     <h2>
      <a id="DeepSeek_2">
      </a>
      人人都该学习的DeepSeek
     </h2>
     <p>
      DeepSeek 作为一款先进的人工智能工具，具备强大的推理能力和广泛的应用场景，能够帮助用户高效解决复杂问题。它支持文本生成、代码编写、数据分析、情感分析等多种任务，适用于教育、医疗、金融、创意等各行各业。它的开源特性使得个人和企业能够低成本地利用其功能，推动AI技术的普及。无论是提升工作效率、辅助学习，还是解决生活中的问题，DeepSeek 都能提供智能化支持。
     </p>
     <h2>
      <a id="DeepSeek_4">
      </a>
      DeepSeek不同版本功能差异
     </h2>
     <table>
      <thead>
       <tr>
        <th>
         版本
        </th>
        <th>
         特点
        </th>
        <th>
         适用场景
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         ​
         <strong>
          1.5B
         </strong>
        </td>
        <td>
         轻量级模型，参数量少，推理速度快，适合低资源环境。
        </td>
        <td>
         短文本生成、基础问答、移动端应用（如简单智能助手）。
        </td>
       </tr>
       <tr>
        <td>
         ​
         <strong>
          7B
         </strong>
        </td>
        <td>
         平衡型模型，性能与资源需求适中，支持中等复杂度任务。
        </td>
        <td>
         文案撰写、表格处理、统计分析、简单代码生成。
        </td>
       </tr>
       <tr>
        <td>
         ​
         <strong>
          8B
         </strong>
        </td>
        <td>
         性能略强于7B，优化逻辑推理和代码生成。
        </td>
        <td>
         代码生成、逻辑推理（如数学题解决）、中等复杂度文本生成。
        </td>
       </tr>
       <tr>
        <td>
         ​
         <strong>
          14B
         </strong>
        </td>
        <td>
         高性能模型，擅长复杂任务（如数学推理、长文本生成）。
        </td>
        <td>
         数据分析、长文本生成（如研究报告）、多模态任务预处理。
        </td>
       </tr>
       <tr>
        <td>
         ​
         <strong>
          32B
         </strong>
        </td>
        <td>
         专业级模型，支持高精度任务和大规模数据处理。
        </td>
        <td>
         语言建模、金融预测、复杂病例分析（医疗场景）。
        </td>
       </tr>
       <tr>
        <td>
         ​
         <strong>
          70B
         </strong>
        </td>
        <td>
         顶级模型，多模态任务支持，科研级分析能力。
        </td>
        <td>
         高精度临床决策（医疗）、多模态数据分析、前沿科学研究。
        </td>
       </tr>
       <tr>
        <td>
         <strong>
          <mark>
           671B
          </mark>
         </strong>
        </td>
        <td>
         超大规模基础模型，最高准确性和推理速度，支持国家级研究。
        </td>
        <td>
         气候建模、基因组分析、通用人工智能探索。
        </td>
       </tr>
      </tbody>
     </table>
     <p>
      注：671B是我们常说的满血版deepseek。
     </p>
     <p>
      <strong>
       ​关键点
      </strong>
     </p>
     <p>
      ​1. 输入输出
     </p>
     <ul>
      <li>
       ​短文本处理​（1.5B-7B）：最大支持16k tokens，适合对话和短文生成。
      </li>
      <li>
       ​长文本处理​（32B+）：32k-10M tokens，可处理整本书籍或科研论文。
      </li>
      <li>
       ​多模态支持：32B及以上版本实验性支持图文混合输入，671B版本实现视频流解析。
      </li>
     </ul>
     <p>
      ​2. 推理计算
     </p>
     <ul>
      <li>
       ​数学能力：7B版本仅支持四则运算，32B版本可解微积分方程（准确率92%）。
      </li>
      <li>
       ​代码生成：7B生成单文件脚本，14B支持全栈项目架构设计（含单元测试）。
      </li>
     </ul>
     <p>
      ​3. 部署
     </p>
     <ul>
      <li>
       ​量化支持：1.5B支持8-bit量化（体积压缩至400MB），70B需保留FP16精度。
      </li>
      <li>
       ​分布式训练：70B版本支持千卡并行训练（吞吐量1.2 exaFLOPs），671B版本兼容量子计算节点。
      </li>
     </ul>
     <p>
      ​
     </p>
     <h2>
      <a id="DeepSeek_38">
      </a>
      DeepSeek与硬件直接的关系
     </h2>
     <table>
      <thead>
       <tr>
        <th>
         参数
        </th>
        <th>
         推荐显卡型号
        </th>
        <th>
         显存要求
        </th>
        <th>
         内存
        </th>
        <th>
         存储
        </th>
        <th>
         适用场景
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         1.5B
        </td>
        <td>
         NVIDIA RTX 3060
        </td>
        <td>
         4-8GB
        </td>
        <td>
         8GB+
        </td>
        <td>
         3GB+ SSD
        </td>
        <td>
         低资源设备部署、简单对话
        </td>
       </tr>
       <tr>
        <td>
         7B
        </td>
        <td>
         NVIDIA RTX 3070/4060
        </td>
        <td>
         8GB+
        </td>
        <td>
         16GB+
        </td>
        <td>
         8GB+ NVMe SSD
        </td>
        <td>
         本地开发测试、中小型企业任务
        </td>
       </tr>
       <tr>
        <td>
         8B
        </td>
        <td>
         NVIDIA RTX 3090
        </td>
        <td>
         8GB+
        </td>
        <td>
         16GB+
        </td>
        <td>
         8GB+ NVMe SSD
        </td>
        <td>
         高精度轻量级任务
        </td>
       </tr>
       <tr>
        <td>
         14B
        </td>
        <td>
         NVIDIA RTX 3090
        </td>
        <td>
         16GB+
        </td>
        <td>
         32GB+
        </td>
        <td>
         15GB+ NVMe SSD
        </td>
        <td>
         企业级复杂任务、专业咨询
        </td>
       </tr>
       <tr>
        <td>
         <strong>
          32B
         </strong>
        </td>
        <td>
         NVIDIA A100 40GB
        </td>
        <td>
         24GB+
        </td>
        <td>
         64GB+
        </td>
        <td>
         30GB+ NVMe SSD
        </td>
        <td>
         <strong>
          高精度专业领域任务
         </strong>
        </td>
       </tr>
       <tr>
        <td>
         70B
        </td>
        <td>
         NVIDIA A100 80GB 多卡
        </td>
        <td>
         ≥40GB（多卡）
        </td>
        <td>
         128GB+
        </td>
        <td>
         70GB+ NVMe SSD
        </td>
        <td>
         企业级复杂任务处理、科研
        </td>
       </tr>
       <tr>
        <td>
         671B
        </td>
        <td>
         NVIDIA H100/HGX 集群
        </td>
        <td>
         640GB（8卡并行）
        </td>
        <td>
         512GB+
        </td>
        <td>
         400GB+ NVMe SSD
        </td>
        <td>
         超大规模科研计算、国家级项目
        </td>
       </tr>
      </tbody>
     </table>
     <p>
      注：32B是一个分水岭，从该版本开始对硬件要求开始急速升高。
     </p>
     <h2>
      <a id="DeepSeek_55">
      </a>
      DeepSeek系统兼容性
     </h2>
     <table>
      <thead>
       <tr>
        <th>
         操作系统
        </th>
        <th>
         兼容性与性能
        </th>
        <th>
         问题与风险
        </th>
        <th>
         工具与部署建议
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         Windows
        </td>
        <td>
         支持轻量级至中型模型（如7B-32B量化版）
        </td>
        <td>
         底层架构限制可能导致闪退或延迟，需关闭后台程序、更新显卡驱动
        </td>
        <td>
         推荐使用Ollama进行一键部署，结合任务管理器监控资源占用，性能较Linux低10%-15%
        </td>
       </tr>
       <tr>
        <td>
         <strong>
          Linux
         </strong>
        </td>
        <td>
         适配全版本模型（含70B+超算级部署）
        </td>
        <td>
         需注意安全防护（88.9%未防护服务器存在漏洞风险）
        </td>
        <td>
         通过LMDeploy优化推理速度，SGLang实现多模型协同，建议Ubuntu系统，
         <strong>
          <mark>
           性能最优
          </mark>
         </strong>
         且支持分布式计算
        </td>
       </tr>
       <tr>
        <td>
         Mac
        </td>
        <td>
         仅支持1.5B-8B轻量模型，依赖M系列芯片NPU加速（如M2 Ultra）
        </td>
        <td>
         模型选择受限，复杂任务响应延迟显著（生成速度约2-3 tokens/秒）
        </td>
        <td>
         必须通过Ollama进行4-bit量化压缩，优先使用Metal框架加速
        </td>
       </tr>
      </tbody>
     </table>
     <p>
      注：部署时Linux系统最优。
     </p>
     <h2>
      <a id="_64">
      </a>
      部署方式选择
     </h2>
     <ol>
      <li>
       优先选 Ollama 的场景
       <ul>
        <li>
         快速原型开发、个人项目测试
        </li>
        <li>
         硬件资源有限（如无高端 GPU）
        </li>
        <li>
         无需复杂参数调优
        </li>
       </ul>
      </li>
      <li>
       优先选直接部署的场景
       <ul>
        <li>
         企业级服务需高并发、低延迟响应
        </li>
        <li>
         需定制模型或优化底层计算（如 FP8 加速、MOE 负载均衡）
        </li>
        <li>
         对数据隐私和合规性要求极高
        </li>
       </ul>
      </li>
     </ol>
     <h2>
      <a id="Ollama_74">
      </a>
      部署步骤（Ollama方式）
     </h2>
     <h3>
      <a id="1deepseek_75">
      </a>
      1.选定适合的deepseek版本
     </h3>
     <p>
      按照自己的需求选取合适的deepseek版本，可参照上文的表格内容。
      <br/>
      选择的依据主要是：
     </p>
     <ul>
      <li>
       使用场景
      </li>
      <li>
       功能需要
      </li>
      <li>
       硬件限制
      </li>
      <li>
       成本要求
      </li>
     </ul>
     <h3>
      <a id="2_82">
      </a>
      2.环境准备
     </h3>
     <p>
      准备好Ubuntu系统，deepseek推荐使用Ubuntu20.04及以上版本。当前示例使用的是Ubuntu18.04版本。
     </p>
     <p>
      当前配置情况：
     </p>
     <ul>
      <li>
       CPU：16核心
      </li>
      <li>
       内存：64Gb
      </li>
      <li>
       硬盘：128Gb
      </li>
      <li>
       GPU：RTX 4090
      </li>
     </ul>
     <p>
      <strong>
       显卡驱动准备
      </strong>
      <br/>
      准备好裸机后首先更新系统：
     </p>
     <pre><code class="prism language-bash"><span class="token function">sudo</span> add-apt-repository ppa:graphics-drivers/ppa <span class="token comment">#18.04版本较旧，需要加上新的驱动</span>
<span class="token function">sudo</span> <span class="token function">apt</span> update <span class="token operator">&amp;&amp;</span> <span class="token function">sudo</span> <span class="token function">apt</span> upgrade <span class="token parameter variable">-y</span>  <span class="token comment"># 更新系统包</span>
<span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> nvidia-driver-535  <span class="token comment"># 安装NVIDIA驱动</span>
</code></pre>
     <p>
      安装好显卡驱动后，确认显卡运行情况：
     </p>
     <pre><code class="prism language-bash">nvidia-smi
</code></pre>
     <p>
      如图所示是驱动完成。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4ca2d6976d944edd9308e1d4f1c6b993.png"/>
     </p>
     <p>
      <strong>
       CUDA环境准备
      </strong>
     </p>
     <pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> update
<span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> nvidia-cuda-toolkit
</code></pre>
     <h3>
      <a id="3Ollama_114">
      </a>
      3.安装Ollama
     </h3>
     <p>
      安装Ollama：
     </p>
     <pre><code class="prism language-bash"><span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://ollama.ai/install.sh <span class="token operator">|</span> <span class="token function">sh</span>  <span class="token comment"># 执行官方安装脚本</span>
</code></pre>
     <p>
      启用Ollama：
     </p>
     <pre><code class="prism language-bash"><span class="token function">sudo</span> systemctl start ollama  <span class="token comment"># 启动服务</span>
ollama <span class="token parameter variable">--version</span>  <span class="token comment"># 输出版本号即成功</span>
</code></pre>
     <p>
      可能的问题：
     </p>
     <p>
      1.如果下载Ollama网络慢导致异常中断，可能如下所示：
     </p>
     <pre><code class="prism language-bash">curl: <span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">)</span> Error <span class="token keyword">in</span> the HTTP2 framing layer
gzip: stdin: unexpected end of <span class="token function">file</span>
tar: Child returned status <span class="token number">1</span>
tar: Error is not recoverable: exiting now
</code></pre>
     <p>
      解决方案：
     </p>
     <pre><code class="prism language-bash"><span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://ollama.com/install.sh <span class="token parameter variable">-o</span> ollama_install.sh
<span class="token function">sed</span> <span class="token parameter variable">-i</span> <span class="token string">'s|https://ollama.com/download/ollama-linux|https://gh.llkk.cc/https://github.com/ollama/ollama/releases/download/v0.5.7/ollama-linux|g'</span> ollama_install.sh
<span class="token function">chmod</span> +x ollama_install.sh
<span class="token function">sudo</span> ./ollama_install.sh
</code></pre>
     <h3>
      <a id="4deepseek_143">
      </a>
      4.部署deepseek
     </h3>
     <pre><code class="prism language-bash">ollama pull deepseek-r1:14b  <span class="token comment"># 下载14B参数版本</span>
</code></pre>
     <p>
      整个过程需要一些时间：
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a89f398e3c8840e9858b3b068f34bce2.png">
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/8d06a49f8f594bd5abc648b100174226.png"/>
      </img>
     </p>
     <h3>
      <a id="5_151">
      </a>
      5.测试使用
     </h3>
     <p>
      测试deepseek运行情况：
     </p>
     <pre><code class="prism language-bash">ollama run deepseek-r1:14b
</code></pre>
     <p>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/21dc8d7e6888467888faa39bffaeb148.png"/>
     </p>
    </img>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f7975656775616e677a68697975616e2f:61727469636c652f64657461696c732f313436303734333334" class_="artid" style="display:none">
 </p>
</div>


