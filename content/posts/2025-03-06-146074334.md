---
layout: post
title: "DeepSeekUbuntu快速部署DeepSeekOllama方式"
date: 2025-03-06 16:47:51 +0800
description: "DeepSeek 作为一款先进的人工智能工具，具备强大的推理能力和广泛的应用场景，能够帮助用户高效解决复杂问题。它支持文本生成、代码编写、数据分析、情感分析等多种任务，适用于教育、医疗、金融、创意等各行各业。无论是提升工作效率、辅助学习，还是解决生活中的问题，DeepSeek 都能提供智能化支持。"
keywords: "【DeepSeek】Ubuntu快速部署DeepSeek（Ollama方式）"
categories: ['人工智能']
tags: ['深度学习', 'Deepseek']
artid: "146074334"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146074334
    alt: "DeepSeekUbuntu快速部署DeepSeekOllama方式"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146074334
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146074334
cover: https://bing.ee123.net/img/rand?artid=146074334
image: https://bing.ee123.net/img/rand?artid=146074334
img: https://bing.ee123.net/img/rand?artid=146074334
---

# 【DeepSeek】Ubuntu快速部署DeepSeek（Ollama方式）

  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/724d5768dc8c409f860152e46eaaa6ca.png#pic_center)

## 人人都该学习的DeepSeek

DeepSeek
作为一款先进的人工智能工具，具备强大的推理能力和广泛的应用场景，能够帮助用户高效解决复杂问题。它支持文本生成、代码编写、数据分析、情感分析等多种任务，适用于教育、医疗、金融、创意等各行各业。它的开源特性使得个人和企业能够低成本地利用其功能，推动AI技术的普及。无论是提升工作效率、辅助学习，还是解决生活中的问题，DeepSeek
都能提供智能化支持。

## DeepSeek不同版本功能差异

版本| 特点| 适用场景  
---|---|---  
​**1.5B**|  轻量级模型，参数量少，推理速度快，适合低资源环境。| 短文本生成、基础问答、移动端应用（如简单智能助手）。  
​**7B**|  平衡型模型，性能与资源需求适中，支持中等复杂度任务。| 文案撰写、表格处理、统计分析、简单代码生成。  
​**8B**|  性能略强于7B，优化逻辑推理和代码生成。| 代码生成、逻辑推理（如数学题解决）、中等复杂度文本生成。  
​**14B**|  高性能模型，擅长复杂任务（如数学推理、长文本生成）。| 数据分析、长文本生成（如研究报告）、多模态任务预处理。  
​**32B**|  专业级模型，支持高精度任务和大规模数据处理。| 语言建模、金融预测、复杂病例分析（医疗场景）。  
​**70B**|  顶级模型，多模态任务支持，科研级分析能力。| 高精度临床决策（医疗）、多模态数据分析、前沿科学研究。  
**671B**|  超大规模基础模型，最高准确性和推理速度，支持国家级研究。| 气候建模、基因组分析、通用人工智能探索。  
  
注：671B是我们常说的满血版deepseek。

**​关键点**

​1. 输入输出

  * ​短文本处理​（1.5B-7B）：最大支持16k tokens，适合对话和短文生成。
  * ​长文本处理​（32B+）：32k-10M tokens，可处理整本书籍或科研论文。
  * ​多模态支持：32B及以上版本实验性支持图文混合输入，671B版本实现视频流解析。

​2. 推理计算

  * ​数学能力：7B版本仅支持四则运算，32B版本可解微积分方程（准确率92%）。
  * ​代码生成：7B生成单文件脚本，14B支持全栈项目架构设计（含单元测试）。

​3. 部署

  * ​量化支持：1.5B支持8-bit量化（体积压缩至400MB），70B需保留FP16精度。
  * ​分布式训练：70B版本支持千卡并行训练（吞吐量1.2 exaFLOPs），671B版本兼容量子计算节点。

​

## DeepSeek与硬件直接的关系

参数| 推荐显卡型号| 显存要求| 内存| 存储| 适用场景  
---|---|---|---|---|---  
1.5B| NVIDIA RTX 3060| 4-8GB| 8GB+| 3GB+ SSD| 低资源设备部署、简单对话  
7B| NVIDIA RTX 3070/4060| 8GB+| 16GB+| 8GB+ NVMe SSD| 本地开发测试、中小型企业任务  
8B| NVIDIA RTX 3090| 8GB+| 16GB+| 8GB+ NVMe SSD| 高精度轻量级任务  
14B| NVIDIA RTX 3090| 16GB+| 32GB+| 15GB+ NVMe SSD| 企业级复杂任务、专业咨询  
**32B**|  NVIDIA A100 40GB| 24GB+| 64GB+| 30GB+ NVMe SSD| **高精度专业领域任务**  
70B| NVIDIA A100 80GB 多卡| ≥40GB（多卡）| 128GB+| 70GB+ NVMe SSD| 企业级复杂任务处理、科研  
671B| NVIDIA H100/HGX 集群| 640GB（8卡并行）| 512GB+| 400GB+ NVMe SSD| 超大规模科研计算、国家级项目  
  
注：32B是一个分水岭，从该版本开始对硬件要求开始急速升高。

## DeepSeek系统兼容性

操作系统| 兼容性与性能| 问题与风险| 工具与部署建议  
---|---|---|---  
Windows| 支持轻量级至中型模型（如7B-32B量化版）| 底层架构限制可能导致闪退或延迟，需关闭后台程序、更新显卡驱动|
推荐使用Ollama进行一键部署，结合任务管理器监控资源占用，性能较Linux低10%-15%  
**Linux**|  适配全版本模型（含70B+超算级部署）| 需注意安全防护（88.9%未防护服务器存在漏洞风险）|
通过LMDeploy优化推理速度，SGLang实现多模型协同，建议Ubuntu系统，**性能最优** 且支持分布式计算  
Mac| 仅支持1.5B-8B轻量模型，依赖M系列芯片NPU加速（如M2 Ultra）| 模型选择受限，复杂任务响应延迟显著（生成速度约2-3
tokens/秒）| 必须通过Ollama进行4-bit量化压缩，优先使用Metal框架加速  
  
注：部署时Linux系统最优。

## 部署方式选择

  1. 优先选 Ollama 的场景 
     * 快速原型开发、个人项目测试
     * 硬件资源有限（如无高端 GPU）
     * 无需复杂参数调优
  2. 优先选直接部署的场景 
     * 企业级服务需高并发、低延迟响应
     * 需定制模型或优化底层计算（如 FP8 加速、MOE 负载均衡）
     * 对数据隐私和合规性要求极高

## 部署步骤（Ollama方式）

### 1.选定适合的deepseek版本

按照自己的需求选取合适的deepseek版本，可参照上文的表格内容。  
选择的依据主要是：

  * 使用场景
  * 功能需要
  * 硬件限制
  * 成本要求

### 2.环境准备

准备好Ubuntu系统，deepseek推荐使用Ubuntu20.04及以上版本。当前示例使用的是Ubuntu18.04版本。

当前配置情况：

  * CPU：16核心
  * 内存：64Gb
  * 硬盘：128Gb
  * GPU：RTX 4090

**显卡驱动准备**  
准备好裸机后首先更新系统：

    
    
    sudo add-apt-repository ppa:graphics-drivers/ppa #18.04版本较旧，需要加上新的驱动
    sudo apt update && sudo apt upgrade -y  # 更新系统包
    sudo apt install nvidia-driver-535  # 安装NVIDIA驱动
    

安装好显卡驱动后，确认显卡运行情况：

    
    
    nvidia-smi
    

如图所示是驱动完成。  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4ca2d6976d944edd9308e1d4f1c6b993.png)

**CUDA环境准备**

    
    
    sudo apt update
    sudo apt install nvidia-cuda-toolkit
    

### 3.安装Ollama

安装Ollama：

    
    
    curl -fsSL https://ollama.ai/install.sh | sh  # 执行官方安装脚本
    

启用Ollama：

    
    
    sudo systemctl start ollama  # 启动服务
    ollama --version  # 输出版本号即成功
    

可能的问题：

1.如果下载Ollama网络慢导致异常中断，可能如下所示：

    
    
    curl: (16) Error in the HTTP2 framing layer
    gzip: stdin: unexpected end of file
    tar: Child returned status 1
    tar: Error is not recoverable: exiting now
    

解决方案：

    
    
    curl -fsSL https://ollama.com/install.sh -o ollama_install.sh
    sed -i 's|https://ollama.com/download/ollama-linux|https://gh.llkk.cc/https://github.com/ollama/ollama/releases/download/v0.5.7/ollama-linux|g' ollama_install.sh
    chmod +x ollama_install.sh
    sudo ./ollama_install.sh
    

### 4.部署deepseek

    
    
    ollama pull deepseek-r1:14b  # 下载14B参数版本
    

整个过程需要一些时间：  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a89f398e3c8840e9858b3b068f34bce2.png)  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8d06a49f8f594bd5abc648b100174226.png)

### 5.测试使用

测试deepseek运行情况：

    
    
    ollama run deepseek-r1:14b
    

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/21dc8d7e6888467888faa39bffaeb148.png)



