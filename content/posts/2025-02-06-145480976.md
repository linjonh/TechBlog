---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34363434353039302f:61727469636c652f64657461696c732f313435343830393736"
layout: post
title: "Mac-部署Ollama-OpenWebUI完全指南"
date: 2025-02-06 22:08:13 +08:00
description: "通过本教程的配置，你可以拥有一个完全本地化的AI助手！_mac"
keywords: "mac openwebui"
categories: ['未分类']
tags: ['人工智能', 'Macos']
artid: "145480976"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=145480976
  alt: "Mac-部署Ollama-OpenWebUI完全指南"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145480976
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145480976
---

# Mac 部署Ollama + OpenWebUI完全指南

#### 文章目录

* + [💻 环境说明](#__4)
  + [🛠️ Ollama安装配置](#_Ollama_12)
  + - [1. 安装[Ollama](https://github.com/ollama/ollama)](#1_Ollamahttpsgithubcomollamaollama_21)
    - [2. 启动Ollama](#2_Ollama_37)
    - [3. 模型存储位置](#3__61)
    - [4. 配置 Ollama 环境变量](#4___Ollama__74)
    - [5. Ollama 常用命令](#5__Ollama__109)
    - [6. Ollama 终端指令](#6__Ollama__121)
  + [🌐 OpenWebUI部署](#_OpenWebUI_130)
  + - [1. 安装Docker](#1_Docker_137)
    - [2. 部署[OpenWebUI](https://www.openwebui.com/)（可视化大模型对话界面）](#2_OpenWebUIhttpswwwopenwebuicom_146)
  + [🔒 离线部署方案](#__179)
  + - [1. 准备工作](#1__180)
    - * [下载必要的 Docker 镜像](#_Docker__181)
      * [下载所需的模型文件](#_197)
    - [2. 离线环境部署](#2__207)
    - * [加载 Docker 镜像](#_Docker__209)
      * [创建 Docker 网络](#_Docker__217)
      * [启动 Ollama 容器](#_Ollama__223)
      * [启动 OpenWebUI 容器](#_OpenWebUI__234)
    - [3. 验证部署](#3__245)
    - * [检查容器状态](#_247)
      * [访问 WebUI](#_WebUI_255)
    - [4. 故障排除](#4__259)
    - [5. 注意事项](#5__282)
  + [⚡ 性能优化建议](#__293)
  + [❓ 常见问题](#__310)
  + [🎉 结语](#__325)

> 想拥有一个完全属于自己的AI助手，还不依赖互联网？本教程带你从零开始搭建本地AI环境！（Apple Silicon架构）

### 💻 环境说明

| 配置项 | Mac | Windows |
| --- | --- | --- |
| 操作系统 | macOS Sonoma | Windows 10/11 |
| CPU | M4 | 12核或以上 |
| 内存 | 16GB | 32GB或以上 |
| 硬盘空间 | 20GB | 20GB或以上 |

### 🛠️ Ollama安装配置

```python
官网：https://ollama.com/
模型：https://ollama.com/library
Github：https://github.com/ollama/ollama
Docker：https://hub.docker.com/r/ollama/ollama/tags

```

#### 1. 安装 [Ollama](https://github.com/ollama/ollama)

```bash
# 使用Homebrew安装
brew install ollama

# 或直接下载安装包
curl https://ollama.ai/download/Ollama-darwin.zip -o Ollama.zip
unzip Ollama.zip

# 输入`ollama`或 `ollama -v`验证安装
ollama


```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f619dbb9fe3f402b8c4b995b0fbd629a.png)

#### 2. 启动 Ollama

```bash
# 启动 Ollama 服务
ollama serve

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/15a1652352224034a6935907dbef0f1d.png)

```python
# 或点击浏览器访问：http://localhost:11434

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/bd79f95cd7ad4d2d9900a7fa85c29487.png)

显示
`Ollama is running`
代表已经运行起来了！

```python
 # 下载 Llama3 8B 模型
ollama run llama3:8b # 建议先尝试小模型

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8eb258a56179470881bb40867d387d52.png)
  
💡 小贴士：你应该至少有 8 GB 的 RAM 来运行 7B 模型，16 GB 的 RAM 来运行 13B 模型，以及 32 GB 的 RAM 来运行 33B 模型。

#### 3. 模型存储位置

Mac 下，Ollama 的默认存储位置：

* 模型文件：
  `~/.ollama/models`
* 配置文件：
  `~/Library/Application Support/Ollama`

Windows 下，Ollama 的默认存储位置：

* 程序目录：
  `C:\Users\<用户名>\AppData\Local\Programs\Ollama`
* 模型目录：
  `C:\Users\<用户名>\.ollamamodels`
* 配置文件：
  `C:\Users\<用户名>\AppData\Local\Ollama`

💡 小贴士：建议通过环境变量
`OLLAMA_MODELS`
自定义模型存储路径，避免占用系统盘空间。

#### 4. 配置 Ollama 环境变量

Ollama 提供了多种环境变量以供配置：

* OLLAMA\_DEBUG：是否开启调试模式，默认为 false。
* OLLAMA\_FLASH\_ATTENTION：是否闪烁注意力，默认为 true。
* OLLAMA\_HOST：Ollama 服务器的主机地址，默认为空。
* OLLAMA\_KEEP\_ALIVE：保持连接的时间，默认为 5m。
* OLLAMA\_LLM\_LIBRARY：LLM 库，默认为空。
* OLLAMA\_MAX\_LOADED\_MODELS：最大加载模型数，默认为 1。
* OLLAMA\_MAX\_QUEUE：最大队列数，默认为空。
* OLLAMA\_MAX\_VRAM：最大虚拟内存，默认为空。
* OLLAMA\_MODELS：模型目录，默认为空。
* OLLAMA\_NOHISTORY：是否保存历史记录，默认为 false。
* OLLAMA\_NOPRUNE：是否启用剪枝，默认为 false。
* OLLAMA\_NUM\_PARALLEL：并行数，默认为 1。
* OLLAMA\_ORIGINS：允许的来源，默认为空。
* OLLAMA\_RUNNERS\_DIR：运行器目录，默认为空。
* OLLAMA\_SCHED\_SPREAD：调度分布，默认为空。
* OLLAMA\_TMPDIR：临时文件目录，默认为空。
* OLLAMA\_DEBUG：是否开启调试模式，默认为 false。
* OLLAMA\_FLASH\_ATTENTION：是否闪烁注意力，默认为 true。
* OLLAMA\_HOST：Ollama 服务器的主机地址，默认为空。
* OLLAMA\_KEEP\_ALIVE：保持连接的时间，默认为 5m。
* OLLAMA\_LLM\_LIBRARY：LLM 库，默认为空。
* OLLAMA\_MAX\_LOADED\_MODELS：最大加载模型数，默认为 1。
* OLLAMA\_MAX\_QUEUE：最大队列数，默认为空。
* OLLAMA\_MAX\_VRAM：最大虚拟内存，默认为空。
* OLLAMA\_MODELS：模型目录，默认为空。
* OLLAMA\_NOHISTORY：是否保存历史记录，默认为 false。
* OLLAMA\_NOPRUNE：是否启用剪枝，默认为 false。
* OLLAMA\_NUM\_PARALLEL：并行数，默认为 1。
* OLLAMA\_ORIGINS：允许的来源，默认为空。
* OLLAMA\_RUNNERS\_DIR：运行器目录，默认为空。
* OLLAMA\_SCHED\_SPREAD：调度分布，默认为空。
* OLLAMA\_TMPDIR：临时文件目录，默认为空。

#### 5. Ollama 常用命令

1. 启动 Ollama 服务： ollama serve
2. 从模型文件创建模型： ollama create
3. 显示模型信息： ollama show
4. 运行模型： ollama run 模型名称
5. 从注册表中拉去模型： ollama pull 模型名称
6. 将模型推送到注册表： ollama push
7. 列出模型： ollama list
8. 复制模型： ollama cp
9. 删除模型： ollama rm 模型名称
10. 获取有关 Ollama 任何命令的帮助信息： ollama help

#### 6. Ollama 终端指令

1. 查看支持的指令：使用命令 /?
2. 退出对话模型：使用命令 /bye
3. 显示模型信息：使用命令 /show
4. 设置对话参数：使用命令 /set 参数名 参数值，例如设置温度（temperature）或 top\_k 值
5. 清理上下文：使用命令 /clear
6. 动态切换模型：使用命令 /load 模型名称
7. 存储模型：使用命令 /save 模型名称
8. 查看快捷键：使用命令 /?shortcuts

### 🌐 OpenWebUI 部署

```python
Github：https://github.com/open-webui/open-webui
文档：https://docs.openwebui.com/

```

#### 1. 安装 Docker

1. 访问
   [Docker 官网](https://www.docker.com/products/docker-desktop)
   下载 Mac 版本（Apple Silicon）
2. 安装并启动 Docker Desktop
3. 配置国内镜像源加速下载（我这里
   `科学上网`
   不需要）
     
   ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/46b5f0a06cdc42d68e021ba16f7b4f36.png)

💡 小贴士：Windows 安装 Docker 需要开启 Hyper-V（Windows 专业版必需）

#### 2. 部署 [OpenWebUI](https://www.openwebui.com/) （可视化大模型对话界面）

```bash
# 拉取镜像 (直接 run 默认会拉取 latest 标签的镜像)
docker pull ghcr.io/open-webui/open-webui:main

#（官方文档）以上是从 GitHub Container Registry (GHCR) 上拉取镜像，而不是从 Docker Hub。
# 也可以 docker hub 拉取 open-webui 镜像
docker pull dyrnq/open-webui:main

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/361e58474834446cb840fdd80b23d184.png)

```bash
# 启动容器
docker run -d -p 3000:8080 \
--add-host=host.docker.internal:host-gateway \
-v open-webui:/app/backend/data \
--name open-webui \
--restart always \
ghcr.io/open-webui/open-webui:main

```

访问
`http://localhost:3000`
即可使用 Web 界面。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c0c9cf51ff6f4c3eb67d85f730ebaea0.png)
创建账号，这个是本地账号，随便添加账号信息即可
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9c2e50d44be84a0ba1c6dbe3a77dc5ac.png)

* 注册时邮箱可以随便填写，设置密码后注意保存！
* ollama 后台一定要运行着模型，如：ollama run phi-4

### 🔒 离线部署方案

#### 1. 准备工作

##### 下载必要的 Docker 镜像

在有网络的环境中，需要下载以下镜像:

```bash
# 下载 Ollama 镜像
docker pull ollama/ollama:latest

# 下载 OpenWebUI 镜像
docker pull ghcr.io/open-webui/open-webui:main

# 保存镜像到文件
docker save ollama/ollama:latest -o ollama.tar
docker save ghcr.io/open-webui/open-webui:main -o openwebui.tar

```

##### 下载所需的模型文件

在有网络的环境中下载模型文件:

```bash
# 使用 Ollama 下载模型
ollama pull llama3
# 模型文件位于 ~/.ollama/models 目录

```

#### 2. 离线环境部署

##### 加载 Docker 镜像

```bash
# 加载保存的镜像文件
docker load -i ollama.tar
docker load -i openwebui.tar

```

##### 创建 Docker 网络

```bash
docker network create ollama-network

```

##### 启动 Ollama 容器

```bash
docker run -d \
--name ollama \
--network ollama-network \
-v ~/.ollama:/root/.ollama \
-p 11434:11434 \
ollama/ollama

```

##### 启动 OpenWebUI 容器

```bash
docker run -d \
--name open-webui \
--network ollama-network \
-p 3000:8080 \
-e OLLAMA_API_BASE_URL=http://ollama:11434/api \
ghcr.io/open-webui/open-webui:main

```

#### 3. 验证部署

##### 检查容器状态

```bash
docker ps
docker logs ollama
docker logs open-webui

```

##### 访问 WebUI

在浏览器中访问
`http://localhost:3000`

#### 4. 故障排除

1. 如果容器无法启动，检查日志:

```bash
docker logs ollama
docker logs open-webui

```

2. 检查网络连接:

```bash
# 确保容器间可以通信
docker network inspect ollama-network

```

3. 检查模型文件:

```bash
# 进入 Ollama 容器检查模型文件
docker exec -it ollama ls /root/.ollama/models

```

#### 5. 注意事项

1. 确保模型文件已正确复制到 Ollama 容器的对应目录
2. 检查磁盘空间是否充足，容器间的网络通信是关键，需要确保正确配置
3. 确保端口 3000 和 11434 未被占用
4. 容器的启动顺序很重要，必须先启动 Ollama 再启动 OpenWebUI
5. 目前也有捆绑安装的方式，将 Open WebUI 和 Ollama 打包在一个 Docker 容器中，通过一条命令就能同时安装和启动两者。

   ```python
   docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama

   ```

### ⚡ 性能优化建议

1. **内存管理**

   * 关闭不必要的后台应用
   * 使用 Activity Monitor 监控内存使用
2. **模型选择**

   * 建议从小模型开始测试
   * 推荐模型大小顺序：
     + qwen2:0.5b (最轻量)
     + llama2:7b (平衡型)
     + codellama:7b (代码专用)
3. **温度控制**

   * 保持 Mac Mini 通风良好
   * 可使用监控工具观察 CPU 温度

### ❓ 常见问题

1. Q: M4 芯片能跑多大的模型？
     
   A: 16GB 内存的 M4 可以流畅运行 8B 参数的模型，更大的模型可能会影响性能。
2. Q: Llama 中文支持不好怎么办？
     
   A: 可以使用
   [Llama-Chinese](https://github.com/LlamaFamily/Llama-Chinese)
   等经过中文优化的模型。
3. Q: OpenWebUI 打不开怎么办？
     
   A: 检查 Docker 状态：

   ```bash
   docker ps # 查看容器状态
   docker logs open-webui # 查看日志

   ```

### 🎉 结语

通过本教程的配置，你已经拥有了一个完全本地化的 AI 助手！有任何问题欢迎在评论区讨论，让我们一起探索 AI 的无限可能！

---

如果觉得这篇文章对你有帮助，别忘了点赞转发哦~ 👍

你用 Mac Mini 跑过哪些 AI 模型？欢迎分享你的使用体验！💭