---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f71797138383838382f:61727469636c652f64657461696c732f313436303739323831"
layout: post
title: "ClickHouse-中出现-DBException-Too-many-parts-错误"
date: 2025-03-06 20:15:18 +0800
description: "**`background_pool_size`** 控制后台线程池的大小，用于处理合并（Merge）、物化视图刷新、数据插入（Insert）等后台任务。- **`number_of_free_entries_in_pool_to_execute_mutation`**：控制突变任务触发阈值。- **任务积压减少**：检查 `system.merges` 和 `system.metrics`。- **`max_background_merges`**：控制合并任务并发数（默认 16）。"
keywords: "ClickHouse 中出现 DB::Exception: Too many parts 错误"
categories: ['未分类']
tags: ['数据库', '前端', 'Java']
artid: "146079281"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146079281
    alt: "ClickHouse-中出现-DBException-Too-many-parts-错误"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146079281
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146079281
cover: https://bing.ee123.net/img/rand?artid=146079281
image: https://bing.ee123.net/img/rand?artid=146079281
img: https://bing.ee123.net/img/rand?artid=146079281
---

# ClickHouse 中出现 DB::Exception: Too many parts 错误

在 ClickHouse 中出现
`DB::Exception: Too many parts`
错误，通常是由于表中数据分片（parts）数量超过系统限制，导致合并（merge）操作无法及时处理。以下是逐步解决方案：

---

#### **1. 理解问题原因**

* **MergeTree 表引擎特性**
  ：ClickHouse 的 MergeTree 引擎表会将数据划分为多个 parts，后台线程定期合并小 parts 成大 part。如果写入速度远快于合并速度，parts 数量会累积。
* **直接原因**
  ：当前表有 600 个 parts（平均大小 10.82 MiB），超过默认阈值（通常为 300）。

---

#### **2. 临时应急措施**

##### **手动触发合并**

```
OPTIMIZE TABLE your_table FINAL;

```

* **作用**
  ：强制合并所有 parts，但可能耗时较长，生产环境需谨慎。
* **注意**
  ：
  `FINAL`
  关键字会强制合并，即使数据已经合并过。

---

#### **3. 优化写入策略**

##### **减少小批量写入频率**

* **推荐批量大小**
  ：单次插入数据量建议在
  **100MB~1GB**
  之间（根据硬件调整）。
* **示例**
  ：将每秒写入 100 次 1MB 的数据，改为每 10 秒写入 1 次 100MB 的数据。

##### **使用 Buffer 表缓冲写入**

```
CREATE TABLE your_table_buffer AS your_table
ENGINE = Buffer(default, your_table, 16, 10, 100, 10000, 1000000, 10000000, 100000000);

```

* **作用**
  ：通过内存缓冲表累积小批量写入，批量刷入目标表。

---

#### **4. 调整合并参数**

##### **修改 `merge_tree` 配置（在 `config.xml` 或 `users.xml` ）**

```
<merge_tree>
    <max_suspicious_broken_parts>5</max_suspicious_broken_parts>
    <max_parts_in_total>1000</max_parts_in_total>  <!-- 调高阈值 -->
    <parts_to_delay_insert>500</parts_to_delay_insert>  <!-- 插入延迟阈值 -->
    <parts_to_throw_insert>600</parts_to_throw_insert>  <!-- 插入报错阈值 -->
</merge_tree>

```

* **关键参数**
  ：
  + `max_parts_in_total`
    : 允许的最大 parts 总数。
  + `parts_to_delay_insert`
    : 达到此数量后，新插入会延迟。
  + `parts_to_throw_insert`
    : 达到此数量后，新插入会报错。

##### **增加后台合并线程数**

```
<background_pool_size>16</background_pool_size>       <!-- 默认 16 -->
<background_schedule_pool_size>16</background_schedule_pool_size>

```

* **注意**
  ：根据 CPU 核心数调整，避免过度占用资源。

---

#### **5. 优化表结构**

##### **调整分区粒度**

```
CREATE TABLE your_table (
    ...
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(date_column)  -- 按月度分区，而非按天
ORDER BY (timestamp, id);

```

* **作用**
  ：减少分区数量，降低每个分区的 parts 数量。

##### **使用 TTL 自动清理旧数据**

```
ALTER TABLE your_table MODIFY TTL date_column + INTERVAL 30 DAY;

```

* **作用**
  ：自动删除过期数据，减少 parts 总数。

---

#### **6. 监控与维护**

##### **查询当前 parts 状态**

```
SELECT
    table,
    count() AS parts_count,
    formatReadableSize(sum(bytes)) AS total_size
FROM system.parts
WHERE active
GROUP BY table;

```

##### **定期检查合并状态**

```
SELECT
    database,
    table,
    elapsed,
    progress,
    is_mutation
FROM system.merges;

```

---

#### **7. 高级方案（可选）**

##### **使用 Kafka 引擎表 + Materialized View**

```
CREATE TABLE your_kafka_source
ENGINE = Kafka()
SETTINGS ...;

CREATE MATERIALIZED VIEW your_mv TO your_table AS
SELECT * FROM your_kafka_source;

```

* **作用**
  ：通过 Kafka 引擎表批量消费数据，减少写入频率。

---

#### **总结**

1. **立即执行**
   ：手动
   `OPTIMIZE TABLE`
   缓解问题。
2. **短期调整**
   ：调高
   `max_parts_in_total`
   阈值。
3. **长期根治**
   ：优化写入批量大小、使用 Buffer 表或 Kafka 管道。
4. **预防措施**
   ：监控 parts 数量，调整分区策略和 TTL。

通过组合使用这些方法，可有效避免
`Too many parts`
错误，确保 ClickHouse 稳定运行。

优化 ClickHouse 的 `background\_pool\_size` 需要结合硬件资源、工作负载类型和监控指标逐步调整。以下是分步指南：

---

### \*\*1. 理解参数作用\*\*
  
- \*\*`background\_pool\_size`\*\* 控制后台线程池的大小，用于处理合并（Merge）、物化视图刷新、数据插入（Insert）等后台任务。
  
- 线程不足会导致任务积压（如合并延迟、写入卡顿）；过多可能导致资源争用（CPU/IO）或上下文切换开销。

---

### \*\*2. 查看当前状态\*\*
  
#### \*\*检查后台任务积压\*\*
  
```sql
  
SELECT
  
database,
  
table,
  
elapsed,
  
progress
  
FROM system.merges;  -- 查看合并任务进度和耗时

SELECT \* FROM system.metrics
  
WHERE metric IN ('BackgroundPoolTask', 'BackgroundSchedulePoolTask');  -- 等待执行的任务数
  
```
  
- 如果 `elapsed` 值高或任务堆积，可能是线程不足。

#### \*\*监控系统指标\*\*
  
```sql
  
SELECT
  
value AS threads\_num,
  
'background\_pool\_size' AS param
  
FROM system.settings
  
WHERE name = 'background\_pool\_size';
  
```
  
- 对比当前线程数与实际负载。

---

### \*\*3. 设置建议值\*\*
  
#### \*\*初始建议值\*\*
  
- \*\*CPU 核心数\*\*：通常设置为物理 CPU 核心数的 \*\*50%~100%\*\*。
  
- 例如：16 核 CPU → 初始值设为 `8~16`。
  
- \*\*存储类型\*\*：
  
- \*\*HDD\*\*：保守设置（避免 IO 争用）。
  
- \*\*SSD/NVMe\*\*：可适当调高（IO 吞吐更高）。

#### \*\*写入/合并密集型场景\*\*
  
- 高频写入或大分区合并时，可逐步增加线程数（例如从 `16` 调整到 `24`），但需观察资源瓶颈。

---

### \*\*4. 调整并验证\*\*
  
#### \*\*修改配置\*\*
  
在 `config.xml` 或 `users.xml` 中调整：
  
```xml
  
<background\_pool\_size>24</background\_pool\_size>
  
```
  
重启 ClickHouse 服务生效。

#### \*\*验证效果\*\*
  
- \*\*任务积压减少\*\*：检查 `system.merges` 和 `system.metrics`。
  
- \*\*资源利用率\*\*：监控 CPU、IO 使用率（避免长期超过 80%）。
  
- \*\*查询性能\*\*：确保前台查询未因资源争用而变慢。

---

### \*\*5. 高级优化\*\*
  
#### \*\*区分任务优先级\*\*
  
- 使用 `background\_processing\_pool\_size`（社区版需手动调整）分离合并和插入任务。
  
- 通过 `SET max\_threads = ...` 限制单个查询资源，避免后台任务被阻塞。

#### \*\*结合其他参数\*\*
  
- \*\*`max\_background\_merges`\*\*：控制合并任务并发数（默认 16）。
  
- \*\*`number\_of\_free\_entries\_in\_pool\_to\_execute\_mutation`\*\*：控制突变任务触发阈值。

---

### \*\*6. 监控工具\*\*
  
- \*\*Prometheus + Grafana\*\*：集成 `ClickHouse Exporter` 监控后台任务队列、CPU/IO。
  
- \*\*内置表\*\*：定期检查 `system.asynchronous\_metrics` 和 `system.events`。

---

### \*\*示例配置\*\*
  
```xml
  
<!-- 针对 32 核 SSD 服务器，高写入场景 -->
  
<background\_pool\_size>24</background\_pool\_size>
  
<max\_background\_merges>16</max\_background\_merges>
  
<number\_of\_free\_entries\_in\_pool\_to\_execute\_mutation>8</number\_of\_free\_entries\_in\_pool\_to\_execute\_mutation>
  
```

---

### \*\*总结\*\*
  
- 从默认值开始，逐步调整并观察监控指标。
  
- 平衡后台任务和前台查询的资源占用。
  
- 磁盘 IO 或 CPU 瓶颈时，优先优化硬件或数据分布（如分区键设计）。

通过以上步骤，可有效优化 `background\_pool\_size` 提升 ClickHouse 后台任务处理效率。