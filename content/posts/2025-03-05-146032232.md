---
layout: post
title: "机器学习4-PCA降维"
date: 2025-03-05 07:57:17 +0800
description: "在数据处理过程中，会碰到维度爆炸，维度灾难的情况，为了得到更精简更有价值的信息，我们需要进一步处理，用的方法就是降维。降维有两种方式：特征抽取、特征选择过滤式（打分机制）：过滤，指的是通过某个阈值进行过滤，比如经常会看到但可能并不会去用的，根据方差、信息增益、互信息、相关系数、卡方检验、F检验来选择特征。（什么是互信息？在某个特定类别出现频率高，但其他类别出现频率比较低的词条与该类的互信息比较大。通常互信息作为特征词和类别之间的测度，如果特征词属于该类的话，他们的互信息就大）"
keywords: "机器学习4-PCA降维"
categories: ['机器学习']
tags: ['深度学习', '机器学习', '人工智能']
artid: "146032232"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146032232
    alt: "机器学习4-PCA降维"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146032232
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146032232
cover: https://bing.ee123.net/img/rand?artid=146032232
image: https://bing.ee123.net/img/rand?artid=146032232
img: https://bing.ee123.net/img/rand?artid=146032232
---

# 机器学习4-PCA降维

## 1 降维

在数据处理过程中，会碰到维度爆炸，维度灾难的情况，为了得到更精简更有价值的信息，我们需要进一步处理，用的方法就是降维。

降维有两种方式：特征抽取、特征选择

  * 特征抽取：就是特征映射，它的思想是把高纬空间的数据映射到低维空间，比如PCA降维、基于神经网络的降维
  * 特征选择： 
    * 过滤式（打分机制）：过滤，指的是通过某个阈值进行过滤，比如经常会看到但可能并不会去用的，根据方差、信息增益、互信息、相关系数、卡方检验、F检验来选择特征。

（什么是互信息？在某个特定类别出现频率高，但其他类别出现频率比较低的词条与该类的互信息比较大。通常互信息作为特征词和类别之间的测度，如果特征词属于该类的话，他们的互信息就大）

![image.png](https://i-blog.csdnimg.cn/img_convert/fd664d0cd5529275a5d13f55f44be5b6.png)

    * 包裹式：每次迭代产生一个特征子集，评分

    * 嵌入式：先通过机器学习模型训练来对每个特征得到一个权重值，接下来和过滤式相似，通过设定某个阈值来筛选特征。**区别在于，嵌入式采用机器学习训练，过滤式采用统计特征**

    * 过滤式方法运用统计指标来为每个特征打分并筛选特征，其聚焦于数据本身的特点。其优点是计算快，不依赖与具体的模型，缺点是选择的统计指标不是为特定模型定制的，因而最后的准确率可能不高，而且因为进行的是单变量统计检验，没有考虑特征间的相互关系

    * 包裹式方法使用模型来筛选特征，通过不断地增加或删除特征，在验证集上测试模型准确率，寻找最优的特征子集。包裹式方法因为有模型的直接参与，因而通常准确性较高，但是因为每变动一个特征都要重新训练模型，因而计算开销大，其另一个缺点是容易过拟合

    * 嵌入式方法利用了模型本身的特性，将特征选择嵌入到模型的构建过程中。典型的如Lasso和决策树模型等。准确率较高，计算复杂度介于过滤式和包裹式方法之间，但缺点是只有部分模型有这个功能

Principal Components Analysis 主成分分析在压缩消除冗余和数据噪音消除等有广泛应用

## 2 特征值和特征向量

A ξ = λ ξ A\xi=\lambda\xi Aξ=λξ

首先要理解矩阵线性变换，即矩阵乘法：矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在变换过程中，原向量主要发生旋转、伸缩变化。**如果矩阵对某一向量或某些向量只发生伸缩变换，不对这些向量产生旋转效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值**

在数据挖掘中，就会直接用特征值来描述对应特征向量方向上包含的信息量，而某一特征值除以所有特征值的和得到的值：该特征向量的方差贡献率。（在该维度下蕴含的信息的比例）

经过特征向量变换下的数据称为变量的主成分，当前m各主成分累计的方差贡献率达到85%以上就保留这m个的主成分数据，实现对数据进行降维的目的。

## 3 PCA的目标

PCA的目标是实现最小投影距离，最大投影方差。降维后不同维度的相关性为0。（也就是向量之间正交）

![image.png](https://i-blog.csdnimg.cn/img_convert/f88c17c8efb9007e5f262e3f0397a1f0.png)

显然数据离散性最大，代表数据在所投影的维度有越高的区分度，这个区分度就是信息量。

应该考虑新的坐标轴，将坐标轴进行旋转就能正确降维，这个旋转的操作就要用到线性变换—奇异值分解

![image.png](https://i-blog.csdnimg.cn/img_convert/136bc846cda825f1345de83a5f7529fc.png)

通过矩阵A对坐标系X进行旋转，经过一些数学推导，其实就可以得知，特征值对应的特征向量就是理想中想取得的正确坐标轴，而特征值就等于数据在旋转后的坐标上对应维度的方差（沿对应的特征向量的数据的方差）。而A其实即为我们想求得的那个降维特征空间，Y则使我们想要的降维后的数据。

## 4 PCA过程

### 4.1 中心化

采用了中心化，均值为0，如果不进行中心化，可能第一主成分的方向有误

![image.png](https://i-blog.csdnimg.cn/img_convert/0a70a2e292ebc6d74f8d533d12e341bd.png)

这里可以看出主成分分析的目的是最小化投影距离，最大化投影方差。如果不中心化就达不到上述目的。

![image.png](https://i-blog.csdnimg.cn/img_convert/95746c45f61edea03c206969c14d4b9a.png)

### 4.2 标准化数据

为什么要标准化，因为等下要算特征值和特征向量，特征值对应的特征向量就是理想中想取得的正确坐标轴，而特征值就等于数据在变换后的坐标上对应维度的方差（沿对应的特征向量的数据的方差）。

考虑这样一个例子，一个特征表示对象的长度（米为单位），而第二个特征表示对象的宽度（厘米为单位）。如果数据没有被标准化，那么最大方差及最大特征向量将隐式地由第一个特征定义。

### 4.3 算协方差矩阵

为什么要算协方差矩阵？我们之前的目的是在降维后的每一维度上，方差最大。而方差最大，则容易想到的就是协方差矩阵，去中心化后，协方差矩阵的对角线上的值正好就是各个数据维度的方差。原始数据的协方差矩阵X【n
*
n】，对应的就是降维后的数据的方差。而我们的目的，这是使方差最大，这就又想到另一个概念，迹（trace），因为迹是对角线上所有元素之和，则协方差矩阵的迹，就是方差之和，这样我们就可以构建损失函数，即argmax(协方差矩阵X的迹)

### 4.4 过程

  1. 首先设新的坐标系为W【n×n】

[ w 1 , w 2 , w 3 , . . . , w n ] [w_1,w_2,w_3, ...,w_n] [w1​,w2​,w3​,...,wn​]

    
    
    	显然w为标准正交基
    

∣ ∣ w ∣ ∣ 2 = 1 , w i T w j = 0 ||w||_2 =1, w_i^Tw_j=0 ∣∣w∣∣2​=1,wiT​wj​=0

  2. 在新的坐标系的投影为

Z = W T X Z=W^TX Z=WTX

其中Z为{z1,z2,z3,…,zn}

  3. 向量Xi在w上的投影坐标可以表示为

( x i , w ) = x i T w (x_i,w)=x_i^Tw (xi​,w)=xiT​w



