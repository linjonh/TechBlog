---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36333239343530342f:61727469636c652f64657461696c732f313436313337393535"
layout: post
title: "2020CVPR-SiamBAN用于视觉跟踪的Siamese框自适应网络"
date: 2025-03-09 20:26:09 +0800
description: "在本文中，作者利用全卷积网络的表达能力，提出了一种简单而有效的视觉跟踪框架，名为 SiamBAN，它不需要多尺度搜索模式和预定义的候选框。SiamBAN 直接在统一网络中对目标进行分类并回归边界框。因此，视觉跟踪问题变成了分类-回归问题。对六个视觉跟踪基准的广泛实验表明 SiamBAN 实现了最先进的性能并以 40 FPS 运行，证实了其有效性和效率。"
keywords: "2020CVPR-SiamBAN：用于视觉跟踪的Siamese框自适应网络"
categories: ['目标检测跟踪论文精读专栏']
tags: ['计算机视觉', '视觉跟踪', '目标检测', 'Siamese']
artid: "146137955"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146137955
    alt: "2020CVPR-SiamBAN用于视觉跟踪的Siamese框自适应网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146137955
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146137955
cover: https://bing.ee123.net/img/rand?artid=146137955
image: https://bing.ee123.net/img/rand?artid=146137955
img: https://bing.ee123.net/img/rand?artid=146137955
---

# 2020CVPR-SiamBAN：用于视觉跟踪的Siamese框自适应网络

**原文标题：Siamese Box Adaptive Network for Visual Tracking**

**中文标题：用于视觉跟踪的Siamese框自适应网络**

**代码地址：**
[GitHub - hqucv/siamban: Siamese Box Adaptive Network for Visual Tracking](https://github.com/hqucv/siamban "GitHub - hqucv/siamban: Siamese Box Adaptive Network for Visual Tracking")

## **Abstract**

大多数现有的跟踪器通常依靠多尺度搜索方案或预定义的锚框来准确估计目标的尺度和纵横比。不幸的是，它们通常需要乏味且启发式的配置。 为了解决这个问题，我们通过利用全卷积网络（FCN）的表达能力，提出了一个简单而有效的视觉跟踪框架（名为Siamese Box Adaptive Network，SiamBAN）。SiamBAN 将视觉跟踪问题视为并行的分类和回归问题，从而直接在统一的 FCN 中对目标进行分类并回归其边界框。无先验框设计避免了与候选框相关的超参数，使 SiamBAN 更加灵活和通用。 在 VOT2018、VOT2019、OTB100、NFS、UAV123 和 LaSOT 等视觉跟踪基准上进行的大量实验表明，SiamBAN 实现了最先进的性能并以 40 FPS 的速度运行，证实了其有效性和效率。

## **1. Introduction**

视觉跟踪是计算机视觉中一项基本但具有挑战性的任务。给定序列初始帧中的目标状态，跟踪器需要预测每个后续帧中的目标状态。尽管近年来取得了巨大进步，但视觉跟踪仍然面临着由于遮挡、尺度变化、背景杂乱、快速运动、照明变化和外观变化而带来的挑战。在现实视频中，目标比例和纵横比也会由于目标或摄像机的移动以及目标外观的变化而发生变化。准确估计目标的尺度和纵横比成为视觉跟踪领域的挑战。然而，许多现有的跟踪器忽略了这个问题，并依靠多尺度搜索来估计目标大小。最近，基于Siamese 网络的视觉跟踪器引入了区域提议网络（RPN）来获得准确的目标边界框。然而为了处理不同的尺度和长宽比，他们需要基于启发式知识仔细设计锚框，这引入了许多超参数和计算复杂性。

相比之下，神经科学家表明，生物视觉的初级视觉皮层可以快速有效地从复杂环境中提取观察对象的轮廓或边界。也就是说，人类可以在没有候选框的情况下识别物体的位置和边界。那么我们能否设计一个准确且鲁棒的视觉跟踪框架而不依赖候选框呢？受到无锚检测器的启发，答案是肯定的。通过利用全卷积网络（FCN）的表达能力，我们提出了一种简单而有效的视觉跟踪框架，名为Siamese 框自适应网络（SiamBAN），以解决准确估计目标的尺度和纵横比的挑战。
**该框架由一个Siamese网络骨干和多个框自适应头组成，不需要预先定义候选框，可以在训练过程中进行端到端的优化。SiamBAN对目标进行分类，并将边界框直接回归到统一的FCN中，将跟踪问题转化为分类回归问题。**
具体来说，它直接预测相关特征图上每个空间位置的前景-背景类别得分和 4D 向量。4D向量描述了从边界框的四个边到与搜索区域相对应的特征位置的中心点的相对偏移。在推理过程中，我们使用以目标的先前位置为中心的搜索图像。通过最佳得分位置对应的边界框，我们可以得到目标在帧之间的位移和尺寸变化。

这项工作的主要贡献有三个方面。

（1）我们设计了一个 Siamese 框自适应网络，它可以在注释良好的数据集上
**使用深度卷积神经网络**
执行
**端到端离线训练**
。

（2）SiamBAN 中的
**无先验框设计**
避免了与候选框相关的超参数，使我们的跟踪器更加灵活和通用。

（3）SiamBAN 不仅取得了最先进的结果，而且在跟踪基准测试中以 40 FPS 的速度运行，包括 VOT2018、VOT2019、OTB100、NFS、UAV123 和 LaSOT。

![](https://i-blog.csdnimg.cn/direct/5a493f8ec51c4d8ea75616c44e9dc6a8.png)

**(b)SiamBAN跟踪器和两个最先进的跟踪器的一些代表性实验结果。从可视化结果来看，我们的跟踪器在尺度和纵横比方面优于其他两个跟踪器。**

## **2. Related Works**

视觉跟踪是近几十年来计算机视觉领域最活跃的研究课题之一。 对相关跟踪器的全面调查超出了本文的范围，因此我们仅简要回顾与我们的工作最相关的两个方面：基于Siamese网络的视觉跟踪器和无锚目标检测器。

### **2.1. Siamese Network Based Visual Trackers**

近年来，基于Siamese网络的跟踪器因其端到端训练能力和高效率而受到视觉跟踪界的广泛关注。SiamFC采用Siamese网络作为特征提取器，并首次引入相关层对特征图进行组合。由于它的轻量结构和不需要模型更新，SiamFC以 86 FPS的速度高效运行。DSiam学习特征变换来处理目标的外观变化并抑制背景。RASNet在Siamese网络中嵌入了多种注意机制，使跟踪模型适应当前目标。

然而，这些方法需要多尺度测试来应对尺度变化（远近、大小），无法处理由于目标外观变化而引起的纵横比变化。为了获得更准确的目标边界框，SiamRPN将RPN引入SiamFC中。SPM-Tracker提出了一种串并联匹配框架来增强SiamRPN的鲁棒性和判别能力。
**SiamRPN++**
、
**SiamMask**
和
**SiamDW**
以不同的方式去除padding等影响因素，并将ResNet、ResNeXt和MobileNet等现代深度神经网络引入Siamese中 基于网络的视觉跟踪器。虽然基于锚点的跟踪器可以处理尺度和纵横比的变化，但需要仔细设计和固定锚框的参数。设计参数通常需要启发式调整，并涉及许多技巧才能获得良好的性能。与基于锚点的跟踪器相比，我们的跟踪器避免了与锚框相关的超参数，并且更加灵活和通用。

![](https://i-blog.csdnimg.cn/direct/deca1a7ba4924ab69659fea81224b7f4.png)

### **2.2. Anchor-free Object Detectors**

最近，无锚目标检测引起了目标检测界的关注。然而，无锚检测并不是一个新概念。DenseBox 首先引入了FCN框架来联合执行人脸检测和地标定位。UnitBox 通过仔细设计优化损失，提供了另一种性能改进的选择。YOLOv1 提出将输入图像划分为网格，然后预测每个网格单元上的边界框和类别概率。最近，出现了许多新的无锚检测器。这些检测方法可以大致分为基于关键点的目标检测和密集检测 具体来说，CornerNet 提出将目标边界框检测为一对关键点。ExtremeNet 提出使用标准关键点估计网络来检测目标的四个极值点和一个中心点。RepPoints 引入了代表点，这是一种新的目标表示形式，用于对细粒度定位信息进行建模并识别对目标分类重要的局部区域。FSAF 提出了特征选择性无锚模块，以解决具有特征金字塔的基于锚的单次检测器的启发式特征选择所带来的限制。FCOS 提出直接预测目标存在的可能性和边界框坐标，无需锚点参考。

与目标检测相比，视觉跟踪任务存在两个关键挑战，即未知类别和不同目标之间的区分。无锚检测器通常假设要检测的目标类别是预先定义的。然而，在跟踪之前目标的类别是未知的。同时，anchor-free检测器通常专注于检测不同类别的目标，而在跟踪时，需要判定当前帧中的某个物体是否与初始帧中的目标相同。为了解决这些问题，需要
**在视觉跟踪框架中引入一个模板分支（template branch），用于编码初始帧中目标的外观信息，从而在后续帧中帮助识别目标与背景的区别**
。

## **3. SiamBAN Framework**

在本节中，我们将描述 SiamBAN 框架。如图2所示，SiamBAN由Siamese网络主干和多个框自适应头组成。
**Siamese 网络主干负责计算模板补丁和搜索补丁的卷积特征图**
，它使用现成的卷积网络。
**框自适应头包括分类模块和回归模块**
。具体来说，
**分类模块对相关层的每个点进行前景-背景分类，回归模块对相应位置进行边界框预测**
。

![](https://i-blog.csdnimg.cn/direct/fb03afdbbd2f46b2b727ad5cdda5f348.png)

**图2。Siamese 框自适应网络的框架。左图为其主要结构，其中C3、C4、C5为骨干网络的特征图，Cls**
**\_**
**Map和Reg**
**\_**
**Map为SiamBAN头部输出的特征图。右图显示了每个SiamBAN头，其中DW-Corr表示深度交叉相关操作。**

### **3.1. Siamese Network Backbone**

现代深度神经网络已被证明在基于Siamese网络的跟踪器中是有效的，现在我们可以在基于Siamese网络的跟踪器中使用ResNet，ResNeXt和MobileNet。在我们的跟踪器中，我们采用ResNet-50作为骨干网。虽然采用连续卷积步进的ResNet-50可以学习到越来越多的抽象特征表示，但它降低了特征分辨率。 然而，基于Siamese网络的跟踪器需要详细的空间信息来执行密集预测。为了解决这个问题，我们从最后两个卷积块中移除下采样操作。同时，为了改善感受野，我们使用了空洞卷积，这被证明对于视觉跟踪是有效的。此外，受多网格方法的启发，我们在模型中采用了不同的空洞率。具体来说，我们将 conv4 和 conv5 块中的步长设置为 1，将 conv4 块中的空洞率设置为 2，将 conv5 块中的空洞率设置为 4。

Siamese 网络主干由两个相同的分支组成。一种称为模板分支，它接收模板补丁作为输入（表示为 z）。另一个称为搜索分支，它接收搜索补丁作为输入（表示为 x）。 这两个分支共享卷积神经网络中的参数，以确保对两个补丁应用相同的变换。为了减少计算量，我们增加一个1×1的卷积将输出特征通道减少到256个，并且只使用模板分支中心7×7区域的特征，仍然可以捕获整个目标区域。为方便起见，Siamese网络的输出特征表示为φ(z)和φ(x)。

### **3.2. Box Adaptive Head**

如图2（右）所示，框自适应头部由分类模块和回归模块组成。两个模块都从模板分支和搜索分支接收特征。因此，我们调整并复制φ(z)和φ(x)到[φ(z)]cls, [φ(z)]reg和[φ(x)]cls, [φ(x)]reg到相应的模块。根据我们的设计，
**分类模块相关层的每个像素点需要输出2个通道进行前景-背景分类，回归模块相关层的每个像素点需要输出4个通道进行边界框的预测**
。每个模块使用深度可分离相关层组合特征图：

![](https://i-blog.csdnimg.cn/direct/f562fcf381cc4a66b0d54d95bb13adf7.png)

**式中**
**⋆**
**表示以[φ(z)]cls或[φ(z)]reg为卷积核的卷积操作，P**
**cls**
**w×h×2表示分类图，P**
**reg**
**w×h×4表示回归图。**
值得注意的是，我们的跟踪器输出的变量比具有5个先验框的anchor-based跟踪器少5倍。

对于分类特征图Pcls(w×h×2)或回归特征图Preg(w×h×4)上的每个位置，都将其映射到输入搜索补丁。
**对于特征图上的点 (i, j)，其对应在搜索图像中感受野的中心位置 (pi**
**,**
**pj)为：**

![](https://i-blog.csdnimg.cn/direct/39bd098069724458b9c7b9bbf630eb6c.png)

其中 w 和 h 是分类或回归特征图的宽度和高度。wim​和 him是搜索图像的宽度和高度。s是网络的总步幅（stride）。

基于锚点的回归方法将特征图上的每个点 (i , j) 映射到搜索图像后，把映射得到的位置 (pi , pj) 作为锚框（anchor box）的中心，并
**预测该锚框的位置 (pi**
**,**
**pj)、宽度 aw**
**​**
**、高度 ah**
。基于锚点的方法在回归中可以调整宽度、高度和位移值，但分类仍然是在特征图上固定的点上进行，这可能导致分类和回归的不一致性。为了解决分类与回归不一致的问题，
**本文方法不调整 (pi**
**,**
**pj)**
**的位置，而是直接计算它到目标边界框四条边的偏移值**
。这样可以避免锚点框设计带来的问题，并保持分类与回归的一致性。由于回归目标是正实数，因此在回归模块的最后一层对预测值应用指数函数exp(x)，将任意实数映射到正区间 (0,+∞)，确保回归预测值（如宽度、高度）始终为正。

### **3.3. Multi-level Prediction**

利用带有空洞卷积的 ResNet-50 后，我们可以使用多级特征进行预测。 虽然我们骨干网络的 conv3、conv4 和 conv5 块的空间分辨率相同，但它们具有不同扩展率的空洞卷积，因此它们的感受野差异很大，捕获的信息自然不同。正如 CF2 所指出的，早期层的特征可以捕获细粒度的信息，这对于精确定位很有用；而后面层的特征可以编码抽象语义信息，这对目标外观变化具有鲁棒性。为了充分利用多级特征的不同特性，我们
**使用多个框自适应头进行预测**
。每个检测头获得的分类图和回归图自适应地融合：

![](https://i-blog.csdnimg.cn/direct/4e836d26beb04152a0ffea92fd7d3e33.png)

**其中αl和βl是每个特征图对应的权重，与网络一起优化。通过独立组合分类图和回归图，分类模块和回归模块可以专注于它们需要的领域。**

### **3.4. Ground-truth and Loss**

**Classification Labels and Regression Targets.**
如图 3 所示，每个搜索块上的目标都标有真实边界框。真实边界框的宽度、高度、左上角、中心点和右下角分别由gw、gh、(gx1，gy1)、(gxc，gyc)和(gx2，gy2)表示。

![](https://i-blog.csdnimg.cn/direct/32ad2c785d5e42f3ba06f0889226ffbd.png)

**图 3.分类标签和回归目标的图示。预测值和监督信号如图所示，其中E1代表椭圆E1，E2代表椭圆E2。 我们分别使用交叉熵和 IoU 损失进行分类和框回归。**

以(gxc , gyc)为圆心，gw/2 , gh/2 为轴长，可得椭圆E1：

![](https://i-blog.csdnimg.cn/direct/501b6077c2a74a29aa518efd86faf907.png)

以 (gxc , gyc) 为圆心， gw/4 , gh/4 为轴长，可得椭圆 E2：

![](https://i-blog.csdnimg.cn/direct/585b54ac4f084709878809c6dfb91dd2.png)

**如果位置（pi，pj）落在椭圆E2内，则为其分配正标签，如果其落在椭圆E1之外，则为其分配负标签，如果其落在椭圆E2和E1之间，忽略它。带有正标签的位置（pi，pj）用于对边界框进行回归，回归目标可以表示为：**

![](https://i-blog.csdnimg.cn/direct/1c6e299af779466183f3c530c122dc03.png)

**其中 dl、dt、dr、db 是该位置（pi，pj）到边界框四个边的距离，如图 3 所示。**

**Classification Loss and Regression Loss.**
**多任务损失函数定义如下：**

![](https://i-blog.csdnimg.cn/direct/fca73601019949f0afa935db61fe48b6.png)

其中Lcls是交叉熵损失，Lreg是IoU（交集/并集）损失。不搜索Eq.6的超参数，简单地设 λ1 = λ2 = 1。与GIoU类似，将IoU损失定义为：

![](https://i-blog.csdnimg.cn/direct/dc8036ad5a9b47a9a8e2fd941ba897e3.png)

其中 IoU 表示预测边界框和真实边界框的交集与并集的面积比。标签为正的位置(pi,pj)在椭圆E2内且回归值大于0，所以0 < IoU ≤ 1，则0 ≤ LIoU < 1。IoU损失可以使dl, dt, dr, db 联合回归。

### **3.5. Training and Inference**

**Training.**
整个网络可以在大规模数据集上进行端到端训练。使用视频或静态图像上采样的图像对来训练 SiamBAN。训练集包括 ImageNet VID 、YouTube-BoundingBoxes、COCO、ImageNet DET、GOT10k 和 LaSOT。模板补丁的大小为 127×127 像素，而搜索补丁的大小为 255×255 像素。另外，虽然我们的负样本比基于锚的跟踪器少得多，但负样本仍然比正样本多得多。因此，我们
**从一对图像中最多收集 16 个正样本和 48 个负样本**
。

**Inference.**
在推理过程中，我们从第一帧中裁剪模板补丁并将其输入特征提取网络。
**提取的模板特征被缓存，因此我们不必在后续跟踪中再次计算它们**
。对于后续帧，根据前一帧的目标位置裁剪搜索块并提取特征，然后在搜索区域中进行预测，得到
**总分类图 P**
**cls−all**
**w×h×2**
和
**总回归图 P**
**reg−all**
**w×h×**
**4**
。之后通过以下方程得到预测框：

![](https://i-blog.csdnimg.cn/direct/3e0558391aef4937b8ae5e27aa5e5262.png)

**其中dreg**
**l**
**、dreg t、dreg r、dreg b为回归图的预测值，（px1, py1）、（px2, py2）为预测框的左上角和右下角。**

在生成预测框后，我们使用余弦窗口和尺度变化惩罚来平滑目标的运动和改变，然后选择得分最好的预测框，并通过与前一帧的状态进行线性插值来更新其大小 。

## **4. Experiments**

### **4.1. Implementation Details**

我们使用在 ImageNet 上预训练的权重来初始化主干网络，并且前两层的参数被冻结。网络采用随机梯度下降 (SGD) 进行训练，小批量为 28 对。共训练 20 个 epoch，前 5 个 epoch 的预热学习率为 0.001 到 0.005，最后 15 个 epoch 的学习率从 0.005 指数衰减到 0.00005。在前 10 个 epoch 中，仅训练框自适应头，并在最后 10 个 epoch 中以当前学习率的十分之一对主干网络进行微调。权重衰减和动量设置为 0.0001 和 0.9。我们的方法是在配备 Intel Xeon(R) 4108 1.8GHz CPU、64G RAM、Nvidia GTX 1080Ti 的 PC 上使用 PyTorch 在 Python 中实现的。

### **4.2. Comparison with State-of-the-art Trackers**

我们在六个跟踪基准上将 SiamBAN 跟踪器与最先进的跟踪器进行比较。实现了最先进的结果并以 40 FPS 的速度运行。

**VOT2018.**
我们在 2018 年视觉目标跟踪挑战赛 (VOT2018) 上评估了我们的跟踪器，该挑战赛由 60 个序列组成。跟踪器的整体性能使用 EAO（预期平均重叠）进行评估，它结合了准确性（成功跟踪期间的平均重叠）和鲁棒性（失败率）。

![](https://i-blog.csdnimg.cn/direct/1ea58534a1e543fda07b7494ae1eacca.png)

**VOT2019.**
我们在 2019 年视觉目标跟踪挑战赛 (VOT2019) 实时实验中评估我们的跟踪器。与 VOT2018 相比，VOT2019 序列被替换了 20%。表 2 显示了 EAO、稳健性和准确性方面的结果。SiamMargin 通过在线更新实现了较低的失败率，但我们的准确性比它更高。尽管 SiamRPN++ 达到了与我们的跟踪器相似的精度，但我们的失败率比它低 17.8%，并且在 EAO 中实现了 14.7% 的相对增益。在这些跟踪器中，我们的跟踪器具有最高的精度和 EAO。这表明我们的方法可以准确地估计目标边界框。

![](https://i-blog.csdnimg.cn/direct/dc118a99a6c3472391fe5f3e4214ca07.png)

**OTB100**
**.**
OTB100 是一种广泛使用的公共跟踪基准，由 100 个序列组成。我们的 SiamBAN 跟踪器与众多最先进的跟踪器进行了比较，包括 SiamRPN++、ECO、DiMP、C-COT、ATOM、DaSiamRPN、TADT、C-RPN、GradNet。图 7 显示了比较跟踪器的成功率和精度图。在 SiamRPN++ 之前，由于浅层网络的表示能力有限，基于 Siamese 网络的跟踪器在 OTB100 上实现了次优性能。使用ResNet50作为特征提取网络后，SiamRPN++取得了领先的结果。 与 SiamRPN++ 相比，通过更简单的设计实现了类似的结果。

![](https://i-blog.csdnimg.cn/direct/7b5b1d8a0d5b43b38120f261e0925f1a.png)

**UAV123.**
UAV123 是一个新的航空视频基准和数据集，其中包含从低空航空视角捕获的 123 个序列。 这些基准可用于评估跟踪器是否适合在实时场景中部署到无人机。我们将我们的跟踪器与其他 9 个最先进的实时跟踪器进行比较，包括 DiMP、ATOM、SiamRPN++、DaSiamRPN、SiamRPN、ECO、ECO- HC，SRDCF，SAM。图 8 显示了成功率和精度图。我们的追踪器取得了最先进的分数。

![](https://i-blog.csdnimg.cn/direct/80b21b7b4c614ac09319957cd6389f60.png)

**LaSOT.**
LaSOT是一个高质量、大规模的数据集，共有1,400个序列。与之前的数据集相比，LaSOT 的序列更长，平均序列长度超过 2,500 帧。每个序列都有各种来自野外的挑战，目标可能会消失并重新出现在视图中，这考验了跟踪器重新跟踪目标的能力。我们在由 280 个视频组成的测试集上评估我们的跟踪器，结果包括成功图和归一化精度图，如图 9 所示。我们的跟踪器在 AUC 方面排名第三，在归一化精度方面排名第二，比 SiamRPN++ 高 5.1%。

![](https://i-blog.csdnimg.cn/direct/b5c2b8a47c5e43e5a1d4d4cdf0f91d3f.png)

## **5. Conclusions**

在本文中，我们利用全卷积网络的表达能力，提出了一种简单而有效的视觉跟踪框架，名为 SiamBAN，它不需要多尺度搜索模式和预定义的候选框。SiamBAN 直接在统一网络中对目标进行分类并回归边界框。因此，视觉跟踪问题变成了分类-回归问题。对六个视觉跟踪基准的广泛实验表明 SiamBAN 实现了最先进的性能并以 40 FPS 运行，证实了其有效性和效率。