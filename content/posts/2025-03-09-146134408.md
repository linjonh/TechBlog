---
layout: post
title: "Scaled_dot_product_attentionSDPA使用详解"
date: 2025-03-09 16:43:48 +0800
description: "在学习huggingFace的Transformer库时，我们不可避免会遇到scaled_dot_product_attention(SDPA)这个函数，它被用来加速大模型的Attention计算，本文就详细介绍一下它的使用方法，核心内容主要参考了torch.nn.functional中该函数的注释。"
keywords: "Scaled_dot_product_attention(SDPA)使用详解"
categories: ['Llm']
tags: ['深度学习', '大模型推理', '人工智能', 'Llm']
artid: "146134408"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146134408
    alt: "Scaled_dot_product_attentionSDPA使用详解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146134408
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146134408
cover: https://bing.ee123.net/img/rand?artid=146134408
image: https://bing.ee123.net/img/rand?artid=146134408
img: https://bing.ee123.net/img/rand?artid=146134408
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Scaled_dot_product_attention(SDPA)使用详解
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     在学习huggingFace的Transformer库时，我们不可避免会遇到scaled_dot_product_attention(SDPA)这个函数，它被用来加速大模型的Attention计算，本文就详细介绍一下它的使用方法，核心内容主要参考了torch.nn.functional中该函数的注释。
    </p>
    <h2>
     <a id="1_Attention_1">
     </a>
     1. Attention计算公式
    </h2>
    <p>
     Attention的计算主要涉及三个矩阵：Q、K、V。我们先不考虑multi-head attention，只考虑one head的self attention。在大模型的prefill阶段，这三个矩阵的维度均为N x d，N即为上下文的长度；在decode阶段，Q的维度为1 x d， KV还是N x d。然后通过下面的公式计算attention矩阵：
     <br/>
     <span class="katex--display">
      <span class="katex-display">
       <span class="katex">
        <span class="katex-mathml">
         O 
         
        
          = 
         
        
          A 
         
        
          t 
         
        
          t 
         
        
          e 
         
        
          n 
         
        
          t 
         
        
          i 
         
        
          o 
         
        
          n 
         
        
          ( 
         
        
          Q 
         
        
          , 
         
        
          K 
         
        
          , 
         
        
          V 
         
        
          ) 
         
        
          = 
         
        
          s 
         
        
          o 
         
        
          f 
         
        
          t 
         
        
          m 
         
        
          a 
         
        
          x 
         
        
          ( 
         
         
          
          
            Q 
           
           
           
             K 
            
           
             T 
            
           
          
          
          
            d 
           
          
         
        
          ) 
         
        
          V 
         
        
       
         O=Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt d})V
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.6833em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.0278em;">
           O
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
          <span class="mrel">
           =
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 1em; vertical-align: -0.25em;">
          </span>
          <span class="mord mathnormal">
           A
          </span>
          <span class="mord mathnormal">
           tt
          </span>
          <span class="mord mathnormal">
           e
          </span>
          <span class="mord mathnormal">
           n
          </span>
          <span class="mord mathnormal">
           t
          </span>
          <span class="mord mathnormal">
           i
          </span>
          <span class="mord mathnormal">
           o
          </span>
          <span class="mord mathnormal">
           n
          </span>
          <span class="mopen">
           (
          </span>
          <span class="mord mathnormal">
           Q
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.0715em;">
           K
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.2222em;">
           V
          </span>
          <span class="mclose">
           )
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
          <span class="mrel">
           =
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 2.4483em; vertical-align: -0.93em;">
          </span>
          <span class="mord mathnormal">
           so
          </span>
          <span class="mord mathnormal" style="margin-right: 0.1076em;">
           f
          </span>
          <span class="mord mathnormal">
           t
          </span>
          <span class="mord mathnormal">
           ma
          </span>
          <span class="mord mathnormal">
           x
          </span>
          <span class="mopen">
           (
          </span>
          <span class="mord">
           <span class="mopen nulldelimiter">
           </span>
           <span class="mfrac">
            <span class="vlist-t vlist-t2">
             <span class="vlist-r">
              <span class="vlist" style="height: 1.5183em;">
               <span class="" style="top: -2.1778em;">
                <span class="pstrut" style="height: 3em;">
                </span>
                <span class="mord">
                 <span class="mord sqrt">
                  <span class="vlist-t vlist-t2">
                   <span class="vlist-r">
                    <span class="vlist" style="height: 0.9322em;">
                     <span class="svg-align" style="top: -3em;">
                      <span class="pstrut" style="height: 3em;">
                      </span>
                      <span class="mord mathnormal" style="padding-left: 0.833em;">
                       d
                      </span>
                     </span>
                     <span class="" style="top: -2.8922em;">
                      <span class="pstrut" style="height: 3em;">
                      </span>
                      <span class="hide-tail" style="min-width: 0.853em; height: 1.08em;">
                       <svg height="1.08em" preserveaspectratio="xMinYMin slice" viewbox="0 0 400000 1080" width="400em">
                        <path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z">
                        </path>
                       </svg>
                      </span>
                     </span>
                    </span>
                    <span class="vlist-s">
                     ​
                    </span>
                   </span>
                   <span class="vlist-r">
                    <span class="vlist" style="height: 0.1078em;">
                     <span class="">
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               <span class="" style="top: -3.23em;">
                <span class="pstrut" style="height: 3em;">
                </span>
                <span class="frac-line" style="border-bottom-width: 0.04em;">
                </span>
               </span>
               <span class="" style="top: -3.677em;">
                <span class="pstrut" style="height: 3em;">
                </span>
                <span class="mord">
                 <span class="mord mathnormal">
                  Q
                 </span>
                 <span class="mord">
                  <span class="mord mathnormal" style="margin-right: 0.0715em;">
                   K
                  </span>
                  <span class="msupsub">
                   <span class="vlist-t">
                    <span class="vlist-r">
                     <span class="vlist" style="height: 0.8413em;">
                      <span class="" style="top: -3.063em; margin-right: 0.05em;">
                       <span class="pstrut" style="height: 2.7em;">
                       </span>
                       <span class="sizing reset-size6 size3 mtight">
                        <span class="mord mathnormal mtight" style="margin-right: 0.1389em;">
                         T
                        </span>
                       </span>
                      </span>
                     </span>
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
               </span>
              </span>
              <span class="vlist-s">
               ​
              </span>
             </span>
             <span class="vlist-r">
              <span class="vlist" style="height: 0.93em;">
               <span class="">
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mclose nulldelimiter">
           </span>
          </span>
          <span class="mclose">
           )
          </span>
          <span class="mord mathnormal" style="margin-right: 0.2222em;">
           V
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     <br/>
     在真正使用attention的时候，我们往往采用multi-head attention(MHA)。MHA的计算公式和one head attention基本一致，它改变了Q、K、V每一行的定义：将维度d的向量分成h组变成一个h x dk的矩阵，Q、K、V此时成为了
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        N 
        
       
         ∗ 
        
       
         h 
        
       
         ∗ 
        
        
        
          d 
         
        
          k 
         
        
       
      
        N * h * d_k
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.6833em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.109em;">
          N
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ∗
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6944em;">
         </span>
         <span class="mord mathnormal">
          h
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ∗
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.8444em; vertical-align: -0.15em;">
         </span>
         <span class="mord">
          <span class="mord mathnormal">
           d
          </span>
          <span class="msupsub">
           <span class="vlist-t vlist-t2">
            <span class="vlist-r">
             <span class="vlist" style="height: 0.3361em;">
              <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
               <span class="pstrut" style="height: 2.7em;">
               </span>
               <span class="sizing reset-size6 size3 mtight">
                <span class="mord mathnormal mtight" style="margin-right: 0.0315em;">
                 k
                </span>
               </span>
              </span>
             </span>
             <span class="vlist-s">
              ​
             </span>
            </span>
            <span class="vlist-r">
             <span class="vlist" style="height: 0.15em;">
              <span class="">
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     的三维矩阵（不考虑batch维）。分别将Q、K、V的第一和第二维进行转置得到三个维度为
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        h 
        
       
         ∗ 
        
       
         N 
        
       
         ∗ 
        
        
        
          d 
         
        
          k 
         
        
       
      
        h * N * d_k
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.6944em;">
         </span>
         <span class="mord mathnormal">
          h
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ∗
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6833em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.109em;">
          N
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ∗
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.8444em; vertical-align: -0.15em;">
         </span>
         <span class="mord">
          <span class="mord mathnormal">
           d
          </span>
          <span class="msupsub">
           <span class="vlist-t vlist-t2">
            <span class="vlist-r">
             <span class="vlist" style="height: 0.3361em;">
              <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
               <span class="pstrut" style="height: 2.7em;">
               </span>
               <span class="sizing reset-size6 size3 mtight">
                <span class="mord mathnormal mtight" style="margin-right: 0.0315em;">
                 k
                </span>
               </span>
              </span>
             </span>
             <span class="vlist-s">
              ​
             </span>
            </span>
            <span class="vlist-r">
             <span class="vlist" style="height: 0.15em;">
              <span class="">
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     的三维矩阵。此时的三个矩阵就是具有h个头的Q、K、V，我们就可以按照self attention的定义计算h个头的attention值。
    </p>
    <p>
     不过，在真正进行大模型推理的时候就会发现KV Cache是非常占显存的，所以大家尝试各种手段压缩KV Cache，具体可以参考《
     <a href="https://blog.csdn.net/yutianzuijin/article/details/145812216?spm=1001.2014.3001.5501">
      大模型推理–KV Cache压缩
     </a>
     》。一种手段就是将MHA替换成group query attention(GQA)，这块在torch2.5以上的SDPA中也已经得到了支持。
    </p>
    <h2>
     <a id="2_SDPA_9">
     </a>
     2. SDPA伪代码
    </h2>
    <p>
     在SDPA的注释中，给出了伪代码：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout_p<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
                is_causal<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> scale<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> enable_gqa<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
	L<span class="token punctuation">,</span> S <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>
	scale_factor <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">if</span> scale <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> scale
	
    attn_bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>L<span class="token punctuation">,</span> S<span class="token punctuation">,</span> dtype<span class="token operator">=</span>query<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>
	<span class="token keyword">if</span> is_causal<span class="token punctuation">:</span>
    	 <span class="token keyword">assert</span> attn_mask <span class="token keyword">is</span> <span class="token boolean">None</span>
         temp_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>L<span class="token punctuation">,</span> S<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tril<span class="token punctuation">(</span>diagonal<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
         attn_bias<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>temp_mask<span class="token punctuation">.</span>logical_not<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         attn_bias<span class="token punctuation">.</span>to<span class="token punctuation">(</span>query<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>

    <span class="token keyword">if</span> attn_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    	 <span class="token keyword">if</span> attn_mask<span class="token punctuation">.</span>dtype <span class="token operator">==</span> torch<span class="token punctuation">.</span><span class="token builtin">bool</span><span class="token punctuation">:</span>
         	 attn_bias<span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>attn_mask<span class="token punctuation">.</span>logical_not<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         <span class="token keyword">else</span><span class="token punctuation">:</span>
             attn_bias <span class="token operator">+=</span> attn_mask

    <span class="token keyword">if</span> enable_gqa<span class="token punctuation">:</span>
    	 key <span class="token operator">=</span> key<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">//</span>key<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>
         value <span class="token operator">=</span> value<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token operator">//</span>value<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">)</span>

    attn_weight <span class="token operator">=</span> query @ key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale_factor
    attn_weight <span class="token operator">+=</span> attn_bias
    attn_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_weight<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    attn_weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>attn_weight<span class="token punctuation">,</span> dropout_p<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
	<span class="token keyword">return</span> attn_weight @ value
</code></pre>
    <p>
     可以看出，我们实际在使用SDPA时除了query、key和value之外，还有另外几个参数：attn_mask、dropout_p、is_causal、scale和enable_gqa。scale就是计算Attention时的缩放因子，一般无需传递。dropout_p表示Dropout概率，在推理阶段也不需要传递，不过官方建议如下输入：dropout_p=(self.p if self.training else 0.0)。我们着重看一下另外三个参数在使用时该如何设置。
    </p>
    <p>
     先看enable_gqa。前面提到GQA是一种KV Cache压缩方法，MHA的KV和Q一样，也会有h个头，GQA则将KV的h个头进行压缩来减小KV Cache的大小。比如Qwen2-7B-Instruct这个模型，Q的h等于28，KV的h等于4，相当于把KV Cache压缩到之前的七分之一。GQA虽然压缩了KV Cache，但是真正要计算Attention的时候还是需要对齐KV与Q的head数，所以我们可以看到HF Transformer库中的qwen2.py在Attention计算时会有一个repeat_kv的操作，目的就是将QKV的head数统一。在torch2.5以后的版本中，我们无需再手动去执行repeat_kv，直接将SDPA的enable_gqa设置为True即可自动完成repeat_kv，而且速度比自己去做repaet_kv还要更快。
    </p>
    <p>
     attn_mask和is_causal两个参数的作用相同，目的都是要给softmax之前的QKT矩阵添加mask。只不过attn_mask是自己在外面构造mask矩阵，is_causal则是根据大模型推理的阶段属于prefill还是decode来进行设置。通过看伪代码可以看出，SDPA会首先构造一个L x S的零矩阵attn_bias，L表示Q的上下文长度，S表示KV Cache的长度。在prefill阶段，L和S相等，在decode阶段，L为1，S还是N。所以在prefill阶段，attn_bias就是一个N x N的矩阵，将is_causal设置为True时就会构造一个下三角为0，上三角为负无穷的矩阵作为attn_bias，然后将其加到QKT矩阵上，这样就实现了因果关系的Attention计算。在decode阶段，attn_bias就是一个1 x N的向量，此时可以将is_causal设置为False，attn_bias始终为0就不会对
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        Q 
        
        
        
          K 
         
        
          T 
         
        
       
      
        QK^T
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1.0358em; vertical-align: -0.1944em;">
         </span>
         <span class="mord mathnormal">
          Q
         </span>
         <span class="mord">
          <span class="mord mathnormal" style="margin-right: 0.0715em;">
           K
          </span>
          <span class="msupsub">
           <span class="vlist-t">
            <span class="vlist-r">
             <span class="vlist" style="height: 0.8413em;">
              <span class="" style="top: -3.063em; margin-right: 0.05em;">
               <span class="pstrut" style="height: 2.7em;">
               </span>
               <span class="sizing reset-size6 size3 mtight">
                <span class="mord mathnormal mtight" style="margin-right: 0.1389em;">
                 T
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     行向量产生影响，表示KV Cache所有的行都参与计算，因果关系保持正确。
    </p>
    <p>
     attn_mask作用和is_causal一样，但是需要我们自行构造，如果你对如何构造不了解建议就使用is_causal选项，prefill阶段设置为True，decode阶段设置为False，attn_mask设置为None。不过，如果prefill按照chunk来执行也即chunk_prefill阶段，我们会发现is_causal设置为True时的attn_bias设置的不正确，我们不是从左上角开始构造下三角矩阵，而是要从右下角开始构造下三角矩阵，这种情况下我们可以从外面自行构造attn_mask矩阵代替SDPA的构造。attn_mask有两种构造方式，一种是bool类型，True的位置会保持不变，False的位置会置为负无穷；一种是float类型，会直接将attn_mask加到SDPA内部的attn_bias上，和bool类型一样，我们一般是构造一个下三角为0上三角为负无穷的矩阵。总结来说，绝大多数情况下我们只需要设置is_causal选项，prefill阶段设置为True，decode阶段设置为False，attn_mask设置为None即可。如果推理阶段引入了chunk_prefill，则我们需要自行构造attn_mask，但是要注意构造的attn_mask矩阵是从右下角开始的下三角矩阵。
    </p>
    <h2>
     <a id="3_SDPASDPA_49">
     </a>
     3. SDPA实现(翻译自SDPA注释)
    </h2>
    <p>
     目前SDPA有三种实现：
    </p>
    <ol>
     <li>
      基于FlashAttention-2的实现；
     </li>
     <li>
      Memory-Efficient Attention(facebook xformers)；
     </li>
     <li>
      Pytorch版本对上述伪代码的c++实现(对应MATH后端)。
     </li>
    </ol>
    <p>
     针对CUDA后端，SDPA可能会调用经过优化的内核以提高性能。对于所有其他后端，将使用PyTorch实现。所有实现方式默认都是启用的，SDPA会尝试根据输入自动选择最优的实现方式。为了对使用哪种实现方式提供更细粒度的控制，torch提供了以下函数来启用和禁用各种实现方式：
    </p>
    <ol>
     <li>
      torch.nn.attention.sdpa_kernel：一个上下文管理器，用于启用或禁用任何一种实现方式；
     </li>
     <li>
      torch.backends.cuda.enable_flash_sdp：全局启用或禁用FlashAttention
     </li>
     <li>
      torch.backends.cuda.enable_mem_efficient_sdp：全局启用或禁用memory efficient attention
     </li>
     <li>
      torch.backends.cuda.enable_math_sdp：全局启用或禁用PyTorch的C++实现。
     </li>
    </ol>
    <p>
     每个融合内核都有特定的输入限制。如果用户需要使用特定的融合实现方式，请使用torch.nn.attention.sdpa_kernel 禁用PyTorch 的C++实现。如果某个融合实现方式不可用，将会发出警告，说明该融合实现方式无法运行的原因。由于融合浮点运算的特性，此函数的输出可能会因所选择的后端内核而异。C++ 实现支持torch.float64，当需要更高精度时可以使用。对于math后端，如果输入是torch.half或torch.bfloat16类型，那么所有中间计算结果都会保持为torch.float类型。
    </p>
    <h2>
     <a id="4_SDPA_64">
     </a>
     4. SDPA使用示例
    </h2>
    <p>
     首先强调一点，灌入SDPA的QKV都是做过转置的，也即维度为batch x head x N x d，在老版本的torch中还需要QKV都是contiguous的，新版本下无此要求。SDPA注释中还给了两个示例，我们在此也给出：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Optionally use the context manager to ensure one of the fused kernels is run</span>
 query <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
 key <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
 value <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
 <span class="token keyword">with</span> sdpa_kernel<span class="token punctuation">(</span>backends<span class="token operator">=</span><span class="token punctuation">[</span>SDPBackend<span class="token punctuation">.</span>FLASH_ATTENTION<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
 	F<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span>
</code></pre>
    <p>
     上述示例中，给定的输入为batch等于32，head等于8，上下文长度128，embedding维度64，然后通过sdpa_kernel选择使用FlashAttention。
     <br/>
     示例二：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Sample for GQA for llama3</span>
query <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
key <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
value <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> sdpa_kernel<span class="token punctuation">(</span>backends<span class="token operator">=</span><span class="token punctuation">[</span>SDPBackend<span class="token punctuation">.</span>MATH<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
	F<span class="token punctuation">.</span>scaled_dot_product_attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span>key<span class="token punctuation">,</span>value<span class="token punctuation">,</span>enable_gqa<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     示例二演示了GQA的用法，给定的query head数为32，key和value均为8，此时我们可以通过enable_gqa选项来实现对GQA的支持，此外代码还通过sdpa_kernel选项使用了MATH后端。
    </p>
    <h2>
     <a id="5__87">
     </a>
     5. 参考
    </h2>
    <ol>
     <li>
      <a href="https://arxiv.org/abs/2307.08691" rel="nofollow">
       FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
      </a>
     </li>
     <li>
      <a href="https://github.com/facebookresearch/xformers">
       Memory-Efficient Attention
      </a>
     </li>
     <li>
      <a href="https://arxiv.org/pdf/2305.13245" rel="nofollow">
       Grouped-Query Attention
      </a>
     </li>
     <li>
      <a href="https://arxiv.org/pdf/1706.03762" rel="nofollow">
       Attention Is All You Need
      </a>
     </li>
    </ol>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c:6f672e6373646e2e6e65742f79757469616e7a75696a696e2f:61727469636c652f64657461696c732f313436313334343038" class_="artid" style="display:none">
 </p>
</div>


