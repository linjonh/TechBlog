---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37333335393036382f:61727469636c652f64657461696c732f313436303738323736"
layout: post
title: "视觉-语言模型-出发点CLIP-精读论文"
date: 2025-03-06 20:43:00 +0800
description: "clip的论文笔记"
keywords: "视觉-语言模型-出发点CLIP--(精读论文)"
categories: ['基于Prompt视觉语言模型的长视频行文理解分析']
tags: ['语言模型', '计算机视觉', '人工智能']
artid: "146078276"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146078276
    alt: "视觉-语言模型-出发点CLIP-精读论文"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146078276
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146078276
cover: https://bing.ee123.net/img/rand?artid=146078276
image: https://bing.ee123.net/img/rand?artid=146078276
img: https://bing.ee123.net/img/rand?artid=146078276
---

# 视觉-语言模型-出发点CLIP--(精读论文)

**阅读建议**
：配合这个
[源码分析](https://blog.csdn.net/m0_73359068/article/details/146085584 "源码分析")
阅读效果更加

## ****研究背景和目的****

介绍当前计算机视觉系统依赖固定类别标签训练的局限性，以及自然语言监督作为一种有潜力替代方式的研究现状。强调论文旨在探索从自然语言监督中学习可迁移视觉模型，实现零样本学习，提升模型通用性和灵活性。

## ****研究方法****

### ****数据集****

现有常用数据集，像 MS - COCO、Visual Genome 规模较小，YFCC100M 虽规模大但元数据质量参差不齐。为充分利用自然语言监督数据的优势，作者构建了 WIT 数据集。具体做法是，从互联网上的多种公开渠道收集数据，基于 50 万个查询来搜索（图像，文本）对。这些查询包括在英文维基百科中出现至少 100 次的单词、高频双词组合、特定搜索量以上的维基百科文章名称以及所有 WordNet 同义词集 。在收集时，对每个查询最多收集 20,000 对数据，以此平衡结果，


最终得到约 4 亿对（图像，文本）数据的 WIT 数据集，其总词数与训练 GPT - 2 的 WebText 数据集相近。

### ****模型训练策略****

#### ****对比学习方法** ：**

CLIP


模型的训练核心是对比学习。给定一批 N 个（图像，文本）对，模型要学习预测批次中 N×N 种可能配对里实际出现的配对。通过联合训练图像编码器和文本编码器，最大化 N 个真实配对的图像和文本嵌入的余弦相似度，同时最小化 N² - N 个错误配对的相似度，优化对称交叉熵损失。这种方法源于深度度量学习中的多类 N - 对损失，经 InfoNCE 损失推广，在对比（文本，图像）表示学习中发挥重要作用。

解释：

对于这 N 个（图像，文本）对，两两计算它们嵌入的余弦相似度，会得到一个 N×N 的相似度矩阵。矩阵对角线上的元素对应着真实配对的相似度，而其他元素则是错误配对的相似度。在


训练时，希望对角线上元素（真实配对相似度）尽可能大，非对角线上元素（错误配对相似度）尽可能小。

对称交叉熵损失综合考虑了图像到文本和文本到图像两个方向的预测误差。具体计算时，


对于每个图像，计算它与所有文本配对的预测概率（基于余弦相似度经 softmax 得到），与真实标签（只有真实配对的标签为 1，其余为 0）之间的交叉熵损失；同样，对于每个文本，也计算它与所有图像配对的预测概率和真实标签之间的交叉熵损失。将这两部分损失相加并求平均


，得到最终的对称交叉熵损失

### ****模型架构选择与调整(vit方案)****

![](https://i-blog.csdnimg.cn/direct/d48ed6e750b745dab3dc910d1104ac08.png)

#### **视频编码器( [vit](https://blog.csdn.net/m0_73359068/article/details/145993165?spm=1001.2014.3001.5502 "vit") )**

ViT


将输入图像分割成一系列固定大小的图像块（patch）。例如，把常见的 224×224 像素的图像，切成 14×14 个 16×16 像素的 patch。这些 patch 被线性映射为具有固定维度的向量，使其能够适配 Transformer 的输入要求。同时，为了保留图像中各部分的位置信息，会给这些向量添加位置编码，位置编码会随着训练一同优化，帮助模型理解图像中各部分的相对位置关系。

#### ****与文本编码器( [transformer](https://blog.csdn.net/m0_73359068/article/details/145966836?spm=1001.2014.3001.5502 "transformer") )****

采用 Transformer 架构，并对其进行了特定修改。基础模型是 6300 万参数、12 层、512 维宽度、8 个注意力头的模型，对文本进行小写字节对编码（BPE），词汇表大小为 49152，最大序列长度限制为 76。将文本序列用 [SOS] 和 [EOS] 标记括起来，取 Transformer 最高层在 [EOS] 标记处的激活作为文本特征表示，经层归一化后线性投影到多模态嵌入空间

#### ****与文本编码器协同训练****

在 CLIP 模型里，ViT 作为图像编码器与文本编码器（Transformer 架构）协同工作。将图像和文本分别输入到各自的编码器中，得到对应的嵌入向量。通过对比学习的方式，最大化匹配的图像 - 文本对之间的嵌入向量相似度，最小化不匹配对的相似度。例如，一张猫的图片和 “一只猫” 的文本描述，模型会学习让它们的嵌入向量在共享的特征空间中距离更近，而与 “一只狗” 这样不匹配的文本描述的嵌入向量距离更远，以此来学习图像和文本之间的关联。

#### **prompt工程**

prompt


工程是提升零样本性能的重要手段。由于标准图像分类数据集在标注时，对基于自然语言的零样本转移信息重视不足，导致 CLIP 在处理一些数据集时面临挑战。例如，许多数据集的标签选择较为随意，存在多义词问题，缺乏上下文信息，使得 CLIP 的文本编码器难以准确区分单词的含义。像 ImageNet 数据集中，“crane” 既表示 “建筑起重机”，也表示 “鹤”；在 Oxford-IIIT Pet 数据集中，“boxer” 既可以指 “拳击手”，也可以指 “拳师犬” 。

**prompt**
**模板的使用**


：为解决这些问题，研究人员发现使用特定的 prompt 模板能有效提升 CLIP 的性能。默认的 “A photo of a {label}.” 模板，明确了文本是关于图像内容的描述，弥补了数据集中标签信息的不足，在 ImageNet 数据集上，仅使用该模板就使准确率提高了 1.3%。

**prompt**
**工程与模型性能提升**


：通过 prompt 工程，CLIP 在多个数据集上的零样本分类性能得到了显著提升。在 ImageNet 数据集上，通过定制 prompt 和集成多个零样本分类器，模型的准确率提高了近 5%。这表明 prompt 工程不仅能解决 CLIP 在处理自然语言标签时的一些固有问题，还能充分发挥其在零样本学习中的潜力，为模型在不同任务中的应用提供了更强大的支持。

## ****研究结论与意义****

通过上述研究，验证了从自然语言监督学习可迁移视觉模型的可行性，CLIP 模型在多任务和数据集上展现出良好的零样本学习能力和泛化性能。ViT 架构和 prompt 工程在提升模型性能方面发挥了重要作用，为计算机视觉领域的研究开辟了新方向，为后续研究奠定了基础，具有重要的理论和实践意义。