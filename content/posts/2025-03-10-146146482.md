---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34303733373739382f:61727469636c652f64657461696c732f313436313436343832"
layout: post
title: "文本对抗样本系列的论文阅读笔记整理合订"
date: 2025-03-10 10:41:47 +08:00
description: "文本对抗样本论文阅读笔记整理"
keywords: "文本对抗样本系列的论文阅读笔记（整理合订）"
categories: ['Reading', 'Paper', 'Nlp', 'Learning']
tags: ['论文阅读', '笔记']
artid: "146146482"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146146482
    alt: "文本对抗样本系列的论文阅读笔记整理合订"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146146482
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146146482
cover: https://bing.ee123.net/img/rand?artid=146146482
image: https://bing.ee123.net/img/rand?artid=146146482
img: https://bing.ee123.net/img/rand?artid=146146482
---

# 文本对抗样本系列的论文阅读笔记（整理合订）

## 文本对抗样本系列的论文阅读笔记

以前调研文本对抗样本时的论文笔记梳理，论文都很经典，有现成的框架（TextAttack）可以直接用，论文中部分内容直接是截取自论文，所以存在中英混合笔记的情况。

### BERT-Attack

> 作者：Linyang Li, Ruotian Ma, et al.
>
> 单位：复旦大学
>
> 来源：EMNLP 2020

#### Introduction

对抗样本：imperceptible to human judges while they can mislead the neural networks to incorrect predictions.

文本对抗样本与图像对抗样本的区别：

1. imperceptible to human judges & misleading to models
2. fluent in grammar and semantically consistent with original inputs

先前的工作特点：

1. based on specific rules
2. difficult to guarantee the fluency and semantically preservation in the generated adversarial samples at the same time.
3. rather complicated

核心思想：将BERT当作对抗样本的生成器，生成对抗样本

BERT-Attack的优势：

1. Training -> Semantic-preserving
2. Context around -> fluent & reasonable
3. inference the language model once as a perturbation generator rather than repeatedly using language models to score the generated adversarial samples in a trial and error process

实验效果：successfully fooled the downstream models

#### Related Work

character-level heuristic rules: Jin et al. 2019

substituting words with synonyms: Ren et.al 2019, Li et al. 2018

score perturbations by searching for close meaning words in the embedding space: Alzantot et al. 2018

semantically enhanced embedding but context unaware: Jin et al. 2019

replace words manually to break the language inference system: Glockner et al. 2018

replacement strategies using embedding transition: Lei et al. 2019

#### Method

两个步骤：

1. finding the vulnerable words of target model
2. replacing the vulnerable words with semantically similar and grammatically correct words until a successful attack

##### Finding Vulnerable Words

输入序列：

S
=
[
w
0
,
⋯
 
,
w
i
,
⋯
 
]
S=[w\_0,\cdots,w\_i,\cdots]





S



=





[


w









0

​


,



⋯





,




w









i

​


,



⋯



]

o
y
(
S
)
o\_y(S)






o









y

​


(

S

)
：目标模型正确标签的logit输出

重要性分数：

I
w
i
=
o
y
(
S
)
−
o
y
(
S
/
w
i
)
I\_{w\_i}=o\_y(S)-o\_y(S\_{/w\_i})






I











w









i

​


​




=






o









y

​


(

S

)



−






o









y

​


(


S










/


w









i

​


​


)
，其中

S
/
w
i
=
[
w
0
,
⋯
 
,
w
i
−
1
,
[
M
A
S
K
]
,
w
i
+
1
,
⋯
 
]
S\_{/w\_i}=[w\_0,\cdots,w\_{i-1},[MASK],w\_{i+1},\cdots]






S










/


w









i

​


​




=





[


w









0

​


,



⋯





,




w










i

−

1

​


,



[

M

A

S

K

]

,




w










i

+

1

​


,



⋯



]

将评分排序，创造单词列表

L
L





L
，取前

ϵ
\epsilon





ϵ
分数的单词作为攻击单词目标

##### Word Replacement via BERT

Previous approaches: 多个人工规则，例如Synonym dictionary(Ren et al. 2019)、POS Checker(Jin et al. 2019)，Semantic Similarity Checker(Jin et al. 2019)

这些替代策略缺陷：

1. unaware of the context between the substitution position
2. insufficient in fluency control and semantic consistency

使用BERT进行替代策略解决fluency control与semantic preservation的问题：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7a829c60c59040cfb0fc15bcfa3c2c0b.png)

算法流程：

![外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传](https://i-blog.csdnimg.cn/direct/ffd78380c4bb4fad94de512a880fdf8e.png)

BPE算法将

S
S





S
进一步分词为

H
=
[
h
0
,
h
1
,
⋯
 
]
H=[h\_0,h\_1,\cdots]





H



=





[


h









0

​


,




h









1

​


,



⋯



]
，因此需要进行分词对齐

令

M
M





M
表示为BERT模型，则输出结果为

P
=
M
(
H
)
P=M(H)





P



=





M

(

H

)
。在每个位置上使用最可能的

K
K





K
词预测，

K
K





K
是超参数。遍历预测的

K
K





K
个词，得到对抗样本

**单一词**
：对于单词

w
j
w\_j






w









j

​

，其top-

K
K





K
个预测候选为

P
j
P^j






P









j
。通过NLTK过滤停止词，使用synonym dictionaries过滤反义词。最终构建干扰序列

H
′
=
[
h
0
,
⋯
 
,
h
j
−
1
,
c
k
,
h
j
+
1
,
⋯
 
]
H'=[h\_0,\cdots, h\_{j-1},c\_k,h\_{j+1},\cdots]






H










′



=





[


h









0

​


,



⋯





,




h










j

−

1

​


,




c









k

​


,




h










j

+

1

​


,



⋯



]
，如果能成功逆转结果，则该序列为对抗样本

H
a
d
v
H^{adv}






H










a

d

v
。否则，在

L
L





L
中查找下一个单词继续挑选最佳干扰项

**分词**
：perplexity指标寻找合适的单词替代。给定单词

w
w





w
的分词串

[
h
0
,
h
1
,
⋯
 
,
h
t
]
[h\_0,h\_1,\cdots,h\_t]





[


h









0

​


,




h









1

​


,



⋯





,




h









t

​


]
，根据

M
M





M
列出来自预测

P
t
×
K
P^{t\times K}






P










t

×

K
的所有可能组合，从而通过逆转BERT分词过程以转换这些二分词到正常单词之中

#### 实验

**数据集**
：

* Text Classification

  + Yelp: review classification dataset
  + IMDB: document-level movie review dataset
  + AG’s News: sentence-level news-type classification dataset
  + FAKE: fake news classification dataset
* Natural Language Inference

  + SNLIL: Stanford language INFERENCE TASK
  + MNLI: language inference dataset on multi-genre texts

**Baseline**
: TextFooler, GA

**Evaluation**
:

1. Attacked Accuracy
2. Perturb Percentage
3. Query Number
4. Average Length
5. Semantic Similarity (Universal Sentence Encoder)

同其他baseline比较：

![](https://i-blog.csdnimg.cn/direct/53bb64e70ffb4c19a6c907ae74f53919.png)

样本展示：

![](https://i-blog.csdnimg.cn/direct/61147661f4224b39bb108703437a2466.png)

#### 笔记中提及的论文参考

1. Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2019. Is BERT really robust? natural language attack on text classification and entailment. CoRR
2. Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics
3. Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. Textbugger: Generating adversarial text against real-world applications
4. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei Chang. 2018. Generating natural language adversarial examples. CoRR
5. Max Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking nli systems with sentences that require simple lexical inferences

### Semantically Equivalent Adversarial Rules for Debugging NLP Models

> 作者：Marco Tulio Ribeiro, Sameer Singh et al.
>
> 单位：University of Washington, University of California (Irvine)
>
> 来源：ACL 2018

#### Introduction

Challenges:

1. different ways of phrasing the same sentence can often cause the model to output different predictions.
2. oversensitivity

提出的对抗样本方法：SEA
  
优势：

1. model-agnostic
2. generate semantically equivalent rules for optimal rule sets: semantic equivalence, high adversary count, non-redundancy.

#### Semantically Equivalent Adversaries

给定黑盒模型

f
f





f
，句子

x
x





x
，预测结果为

f
(
x
)
f(x)





f

(

x

)

基本思想：通过调整

x
x





x
，以改变

f
(
x
)
f(x)





f

(

x

)

指示函数：

S
e
m
E
q
(
x
,
x
′
)
=
I
[
S
e
m
E
q
(
x
,
x
′
)
∧
f
(
x
)
≠
f
(
x
′
)
]
SemEq(x,x')=\mathbb{I}[SemEq(x,x')\wedge f(x)\not=f(x')]





S

e

m

Eq

(

x

,




x










′

)



=





I

[

S

e

m

Eq

(

x

,




x










′

)



∧





f

(

x

)

















=





f

(


x










′

)]

语义分数：

S
(
x
,
x
′
)
=
min
⁡
(
1
,
P
(
x
′
∣
x
)
P
(
x
∣
x
)
)
S(x,x')=\min(1,\frac{P(x'|x)}{P(x|x)})





S

(

x

,




x










′

)



=





min

(

1

,















P

(

x

∣

x

)












P

(


x










′

∣

x

)

​


)
，其中

P
(
x
′
∣
x
)
P(x'|x)





P

(


x










′

∣

x

)
代表的是重新调整句子

x
x





x
后的

x
′
x'






x










′
概率

进一步有：

S
e
m
E
q
(
x
,
x
′
)
=
I
[
S
(
x
,
x
′
)
≥
τ
]
SemEq(x,x')=\mathbb{I}[S(x,x')\geq \tau]





S

e

m

Eq

(

x

,




x










′

)



=





I

[

S

(

x

,




x










′

)



≥





τ

]

paraphrase set via beam search:

Π
x
\Pi\_x






Π









x

​

挑选最佳的对抗样本：

arg max
⁡
x
′
∈
Π
x
S
(
x
,
x
′
)
S
E
A
x
(
x
′
)
\argmax\limits\_{x'\in\Pi\_x}S(x,x')SEA\_x(x')















x










′

∈


Π









x

​







arg



max

​




S

(

x

,




x










′

)

SE


A









x

​


(


x










′

)

#### Semantically Equivalent Adversarial Rules (SEARs)

假设：人的时间受限，愿意看

B
B





B
条规则

SEARs：给定一个参考数据集

X
X





X
，根据

X
X





X
选择规则集

B
B





B

规则形式：

r
=
(
a
→
c
)
r=(a\rightarrow c)





r



=





(

a



→





c

)
，

a
a





a
为原始单词，

c
c





c
为替代词

**构建规则集**
：提取匹配词对，挑选最小连续序列使得

x
→
x
′
x\rightarrow x'





x



→






x










′
。同时包含中间上下文， e.g. What color

→
\rightarrow





→
Which color。通过粗粒度和细粒度的Part-of-Speech tags乘积泛化，如果tags能匹配上前项，则允许这些tags出现在结果之中，e.g. What NOUN

→
\rightarrow





→
Which NOUN

**选择规则集**
：给定候选规则，想要挑选规则集

R
R





R
使得

∣
R
∣
≤
B
|R|\leq B





∣

R

∣



≤





B

* 语义相等：在集合中规则的应用应该产生语义相等的实例，即：

  E
  [
  S
  e
  m
  E
  q
  (
  x
  ,
  r
  (
  x
  )
  )
  ]
  ≥
  1
  −
  δ
  E[SemEq(x,r(x))]\geq 1-\delta





  E

  [

  S

  e

  m

  Eq

  (

  x

  ,



  r

  (

  x

  ))]



  ≥





  1



  −





  δ
  （Filter操作）
* 高对抗样本数量：能在验证集中诱导尽可能多的SEAs，并且语义相似分数高
* 不重复：不同的规则可能造成相同的SEAs，或者诱导不同的SEAs到相同的实例上，即目标函数：

  max
  ⁡
  R
  ,
  ∣
  R
  ∣
  <
  B
  ∑
  x
  ∈
  X
  max
  ⁡
  r
  ∈
  R
  S
  (
  x
  ,
  r
  (
  x
  )
  )
  S
  E
  A
  (
  x
  ,
  r
  (
  x
  )
  )
  \max\limits\_{R,|R|<B}\sum\limits\_{x\in X}\max\limits\_{r\in R}S(x,r(x))SEA(x,r(x))














  R

  ,

  ∣

  R

  ∣

  <

  B





  max

  ​













  x

  ∈

  X





  ∑

  ​













  r

  ∈

  R





  max

  ​




  S

  (

  x

  ,



  r

  (

  x

  ))

  SE

  A

  (

  x

  ,



  r

  (

  x

  ))
  ，这是一个贪心算法、SubMod过程。

![](https://i-blog.csdnimg.cn/direct/dda58681b2324796bd9f45a70a5cf190.png)

样本展示：

![外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传](https://i-blog.csdnimg.cn/direct/26402adc14244fd18d9dfb545ecc291d.png)

#### User Studies

neural machine translation models parameters: default OpeNMT-py parameters

POS tagging: spacy library

SEAR generation:

δ
=
0.1
\delta=0.1





δ



=





0.1
,

τ
=
0.0008
\tau=0.0008





τ



=





0.0008

VQA:
*telling*
system, questions include “What”, “Where”, “When”, “Who”, “Why”, and “How”.

**Condition Study**
： Human, SEA, HSEA (human & SEA collaboration)

**Condition Result**
:

![](https://i-blog.csdnimg.cn/direct/a822436298424659b08ed2f6fa2dc45a.png)

![](https://i-blog.csdnimg.cn/direct/370ccdf8bcc0417e87c6731d0e5cf49c.png)

**专家 VS SEA**
：

![](https://i-blog.csdnimg.cn/direct/7fe12a4e75a342de9e2113f49e808bab.png)

### TextBugger: Generating Adversarial Text Against Real-world Applications

> 作者：Jingfeng Li, ShouJing Ji et al.
>
> 单位：浙江大学计算机学院
>
> 来源：NDSS 2019

#### Introduction

对抗攻击分类

* causative attacks: manipulate the training data to mislead the classifiers
* exploratory attacks: craft malicious testing instances to evade a given classifiers

文本对抗样本的挑战：

1. discrete property, hard to optimize
2. small perturbations are usually clearly perceptible
3. replacement of a single word may drastically alter the semantics of the sentence

已有工作(2019年前)缺陷：

1. not computationally efficient
2. under the white-box setting
3. manual intervention
4. against a particular NLP model, not comprehensively evaluated

本文提出的TextBugger，分为白盒跟黑盒情景：

* 白盒：通过计算分类器的Jacobian矩阵找到关键词，然后通过生成五种扰动选择最优扰动放进去
* 黑盒：首先寻找重要句子，之后选择打分函数寻找句子中的重要单词污染语料

#### Attack Design

##### Problem Formulation

受害模型

F
:
X
→
Y
F: X\rightarrow Y





F



:





X



→





Y

语义相似指标：

S
:
X
×
X
→
R
+
S:X\times X\rightarrow \mathbb{R}\_+





S



:





X



×





X



→






R









+

​

对抗文档：

F
(
x
)
=
y
F(x)=y





F

(

x

)



=





y
,

x
a
d
v
x\_{adv}






x










a

d

v

​

s.t.

F
(
x
a
d
v
)
=
t
(
t
≠
y
)
F(x\_{adv})=t(t\not= y)





F

(


x










a

d

v

​


)



=





t

(

t

















=





y

)
,

S
(
x
,
x
a
d
v
)
≥
ϵ
(
ϵ
∈
R
)
S(x,x\_{adv})\geq \epsilon (\epsilon \in \mathbb{R})





S

(

x

,




x










a

d

v

​


)



≥





ϵ

(

ϵ



∈





R

)

##### Threat Model

白盒设置：complete knowledge about the targeted model architecture parameters (worst-case attack)

黑盒设置：users can only access the model via an API (not aware of the model architecture)

##### TextBugger

白盒攻击：

1. 寻找重要单词：给定

   x
   =
   (
   x
   1
   ,
   x
   2
   ,
   ⋯
    
   ,
   x
   N
   )
   x=(x\_1,x\_2,\cdots,x\_N)





   x



   =





   (


   x









   1

   ​


   ,




   x









   2

   ​


   ,



   ⋯





   ,




   x









   N

   ​


   )
   ，

   x
   i
   x\_i






   x









   i

   ​

   为第i个单词，目标模型为

   F
   F





   F
   ，则矩阵为：

   J
   F
   (
   x
   )
   =
   ∂
   F
   (
   x
   )
   ∂
   x
   =
   [
   ∂
   F
   j
   (
   x
   )
   ∂
   x
   i
   ]
   i
   ∈
   {
   1
   ,
   ⋯
    
   ,
   N
   }
   ,
   j
   ∈
   {
   1
   ,
   ⋯
    
   ,
   K
   }
   J\_F(x)=\frac{\partial F(x)}{\partial x}=[\frac{\partial F\_j(x)}{\partial x\_i}]\_{i\in\{1,\cdots,N\},j\in \{1,\cdots,K\}}






   J









   F

   ​


   (

   x

   )



   =

















   ∂

   x












   ∂

   F

   (

   x

   )

   ​




   =





   [













   ∂


   x









   i

   ​













   ∂


   F









   j

   ​


   (

   x

   )

   ​



   ]










   i

   ∈

   {

   1

   ,

   ⋯



   ,

   N

   }

   ,

   j

   ∈

   {

   1

   ,

   ⋯



   ,

   K

   }

   ​

   ，其中

   K
   K





   K
   表示为标签类别数量，

   F
   j
   (
   ⋅
   )
   F\_j(\cdot)






   F









   j

   ​


   (

   ⋅

   )
   表示为

   j
   t
   h
   j^{th}






   j










   t

   h
   类别的confidence value，则单词

   x
   i
   x\_i






   x









   i

   ​

   的重要性为：

   C
   x
   i
   =
   J
   F
   (
   i
   ,
   y
   )
   =
   ∂
   F
   y
   (
   x
   )
   ∂
   x
   i
   C\_{x\_i}=J\_{F(i,y)}=\frac{\partial F\_y(x)}{\partial x\_i}






   C











   x









   i

   ​


   ​




   =






   J










   F

   (

   i

   ,

   y

   )

   ​




   =

















   ∂


   x









   i

   ​













   ∂


   F









   y

   ​


   (

   x

   )

   ​
2. Bugs生成：考虑字符级扰动和单词级扰动。

   * 字符级：将重要单词转化为未知单词
   * 单词级：插入、删除、交换、替代字符、替代单词

白盒攻击下的算法：

![](https://i-blog.csdnimg.cn/direct/d6ff0dc77d8c4093aac3de378a2cc94d.png)

![](https://i-blog.csdnimg.cn/direct/64c048891eb04cfba8bcc7123fd523f0.png)

黑盒攻击：

1. 找到重要句子：假定文档

   x
   =
   (
   s
   1
   ,
   ⋯
    
   ,
   s
   n
   )
   x=(s\_1,\cdots,s\_n)





   x



   =





   (


   s









   1

   ​


   ,



   ⋯





   ,




   s









   n

   ​


   )
   ，其中

   s
   i
   s\_i






   s









   i

   ​

   表示第

   i
   i





   i
   个句子。先使用spaCy切片每个文档到句子之中。之后通过模型查看是否与不同标签一致

   (
   F
   l
   (
   s
   i
   )
   ≠
   y
   )
   (F\_l(s\_i)\not=y)





   (


   F









   l

   ​


   (


   s









   i

   ​


   )











   





   =





   y

   )
   ，逆序排列句子重要性分数，其句子的重要性分数表示为：

   C
   s
   i
   =
   F
   y
   (
   s
   i
   )
   C\_{s\_i}=F\_y(s\_i)






   C











   s









   i

   ​


   ​




   =






   F









   y

   ​


   (


   s









   i

   ​


   )
2. 找到重要单词：找到最重要单词，并通过控制语义相似进行修改。设计了一个新的打分函数：

   C
   w
   j
   =
   F
   y
   (
   w
   1
   ,
   w
   2
   ,
   ⋯
    
   ,
   w
   m
   )
   −
   F
   y
   (
   w
   1
   ,
   ⋯
    
   ,
   w
   j
   −
   1
   ,
   w
   j
   +
   1
   ,
   ⋯
    
   ,
   w
   m
   )
   C\_{w\_j}=F\_y(w\_1,w\_2,\cdots,w\_m)-F\_y(w\_1,\cdots,w\_{j-1},w\_{j+1},\cdots,w\_m)






   C











   w









   j

   ​


   ​




   =






   F









   y

   ​


   (


   w









   1

   ​


   ,




   w









   2

   ​


   ,



   ⋯





   ,




   w









   m

   ​


   )



   −






   F









   y

   ​


   (


   w









   1

   ​


   ,



   ⋯





   ,




   w










   j

   −

   1

   ​


   ,




   w










   j

   +

   1

   ​


   ,



   ⋯





   ,




   w









   m

   ​


   )
3. Bugs生成

#### Attack Evaluation

##### Sentiment Analysis

数据集：IMDB，Rotten Tomatoes Movie Reviews (MR)

受害模型：LR，CNN，LSTM

baseline: Random，FGSM+Nearest Neighbor Search (NNS)，DeepFool+NNS

评估指标：

* Edit Distance
* Jaccard Similarity Coefficient:

  J
  (
  A
  ,
  B
  )
  =
  ∣
  A
  ∩
  B
  ∣
  ∣
  A
  ∪
  B
  ∣
  =
  ∣
  A
  ∩
  B
  ∣
  ∣
  A
  ∣
  +
  ∣
  B
  ∣
  −
  ∣
  A
  ∩
  B
  ∣
  J(A,B)=\frac{|A\cap B|}{|A\cup B|}=\frac{|A\cap B|}{|A|+|B|-|A\cap B|}





  J

  (

  A

  ,



  B

  )



  =

















  ∣

  A

  ∪

  B

  ∣












  ∣

  A

  ∩

  B

  ∣

  ​




  =

















  ∣

  A

  ∣

  +

  ∣

  B

  ∣

  −

  ∣

  A

  ∩

  B

  ∣












  ∣

  A

  ∩

  B

  ∣

  ​
* Euclidean Distance:

  d
  (
  p
  ,
  q
  )
  =
  (
  p
  1
  −
  q
  1
  )
  2
  +
  (
  p
  2
  −
  q
  2
  )
  2
  +
  ⋯
  +
  (
  p
  n
  −
  q
  n
  )
  2
  d(\bold{p},\bold{q})=\sqrt{(p\_1-q\_1)^2+(p\_2-q\_2)^2+\cdots+(p\_n-q\_n)^2}





  d

  (

  p

  ,



  q

  )



  =













  (


  p









  1

  ​




  −




  q









  1

  ​



  )









  2



  +



  (


  p









  2

  ​




  −




  q









  2

  ​



  )









  2



  +



  ⋯



  +



  (


  p









  n

  ​




  −




  q









  n

  ​



  )









  2


  ​
* Semantic Similarity：

  S
  (
  p
  ,
  q
  )
  =
  p
  ⋅
  q
  ∣
  ∣
  p
  ∣
  ∣
  ⋅
  ∣
  ∣
  q
  ∣
  ∣
  =
  ∑
  i
  =
  1
  n
  p
  i
  ×
  q
  i
  ∑
  i
  n
  (
  p
  i
  )
  2
  ×
  ∑
  i
  =
  1
  n
  (
  q
  i
  )
  2
  S(\bold{p},\bold{q})=\frac{\bold{p}\cdot \bold{q}}{||\bold{p}||\cdot||\bold{q}||}=\frac{\sum^n\_{i=1}p\_i\times q\_i}{\sqrt{\sum^n\_i(p\_i)^2}\times \sqrt{\sum^n\_{i=1}(q\_i)^2}}





  S

  (

  p

  ,



  q

  )



  =

















  ∣∣

  p

  ∣∣

  ⋅

  ∣∣

  q

  ∣∣












  p

  ⋅

  q

  ​




  =


























  ∑









  i





  n

  ​


  (


  p









  i

  ​



  )









  2


  ​


  ×










  ∑










  i

  =

  1





  n

  ​


  (


  q









  i

  ​



  )









  2


  ​














  ∑










  i

  =

  1





  n

  ​





  p









  i

  ​


  ×


  q









  i

  ​


  ​

  with USE

实验发现：

1. 这个模型效果比较好，速度也快
2. 长文本的攻击效果弱于短文本
3. 评分上：逆转负面评价到正面评价会部分失败
4. 数据集上负面词多于正面词
5. 扰动类型的影响：字符级替代最难被发现造成词表之外的现象

![](https://i-blog.csdnimg.cn/direct/2f75a577e3c240eab6bc1d3c719912ed.png)

##### Toxic Content Detection

数据集：Kaggle Toxic Comment Classification competation

受害模型：LR, CNN, LSTM

实验结果：

![](https://i-blog.csdnimg.cn/direct/4581d277f48941949a208bcb91a83057.png)

#### Potential Defenses

1. Spelling Check
2. Adversarial Training

### Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency

> 作者：Shuhuai Ren, Yihe Deng et al.
>
> 单位：杭州科技大学、加利福尼亚大学、哈工大

#### Introduction

NLP对抗样本难的问题：

1. words in sentences are discrete tokens
2. hard in human’s perception to make sense of the texts with perturbations

本文的出发点：could guarantee the lexical correctness with little grammatical error and semantic shifting.

提出的方法：Probability Weighted Word Saliency (PWWS)

#### Text Classification Attack

特征空间

X
X





X
，输出空间

Y
=
{
y
1
,
⋯
 
,
y
K
}
Y=\{y\_1,\cdots, y\_K\}





Y



=





{


y









1

​


,



⋯





,




y









K

​


}
,，目标模型

f
:
X
→
Y
f:X\rightarrow Y





f



:





X



→





Y
， 正确的标签

y
t
r
u
e
∈
Y
y\_{true}\in Y






y










t

r

u

e

​




∈





Y

##### Text Adversarial Example

模型分类：

arg max
⁡
y
i
∈
Y
(
y
i
∣
x
)
=
y
t
r
u
e
\argmax\limits\_{y\_i\in Y}(y\_i|x)=y\_{true}















y









i

​


∈

Y






arg



max

​


(


y









i

​


∣

x

)



=






y










t

r

u

e

​

扰动

△
x
\triangle x





△

x
，

x
∗
=
x
+
△
x
x^\*=x+\triangle x






x









∗



=





x



+





△

x
，s.t.

arg max
⁡
y
i
∈
Y
P
(
y
i
∣
x
∗
)
≠
y
t
r
u
e
\argmax\limits\_{y\_i\in Y}P(y\_i|x^\*)\not= y\_{true}















y









i

​


∈

Y






arg



max

​




P

(


y









i

​


∣


x









∗

)

















=






y










t

r

u

e

​

定义的对抗样本为:

x
∗
=
x
+
△
x
,
∣
∣
△
x
∣
∣
<
ϵ
arg max
⁡
y
i
∈
Y
P
(
y
i
∣
x
∗
)
≠
arg max
⁡
y
i
∈
Y
P
(
y
i
∣
x
)
x^\*=x+\triangle x, ||\triangle x||< \epsilon \\\argmax\limits\_{y\_i\in Y} P(y\_i|x^\*)\not=\argmax\limits\_{y\_i\in Y}P(y\_i|x)






x









∗



=





x



+





△

x

,



∣∣△

x

∣∣



<





ϵ
















y









i

​


∈

Y






arg



max

​




P

(


y









i

​


∣


x









∗

)

















=















y









i

​


∈

Y






arg



max

​




P

(


y









i

​


∣

x

)

其

p
p





p
范数为：

∣
∣
△
x
∣
∣
p
=
(
∑
i
=
1
n
∣
w
i
∗
−
w
i
∣
p
)
1
p
||\triangle x||\_p=(\sum^n\limits\_{i=1}|w^\*\_i-w\_i|^p)^{\frac{1}{p}}





∣∣△

x

∣


∣









p

​




=





(










i

=

1





∑





n

​




∣


w









i





∗

​




−






w









i

​



∣









p


)






















p












1

​

该论文中，通过替代输入单词的同义词（来自WordNet）并通过取代相似的命名实体(Name Entries, NE)以生成对抗样本

假定属于类别

y
t
r
u
e
y\_{true}






y










t

r

u

e

​

的输入样本和字典

D
y
t
r
u
e
⊆
D
\mathbb{D}\_{y\_{true}}\subseteq \mathbb{D}






D











y










t

r

u

e

​


​




⊆





D
包含了所有出现在文中的NE，而最频繁的

N
E
a
d
v
NE\_{adv}





N


E










a

d

v

​

存在于

D
−
D
y
t
r
u
e
\mathbb{D}-\mathbb{D}\_{y\_{true}}





D



−






D











y










t

r

u

e

​


​

中作为替代词。

##### PWWWS

PWWS属于贪心算法

**单词替代策略**
：

1. 对于单词

   w
   i
   ∈
   x
   w\_i\in x






   w









   i

   ​




   ∈





   x
   ，首先用WordNet构建同义词集

   L
   i
   ⊆
   D
   \mathbb{L}\_i\subseteq \mathbb{D}






   L









   i

   ​




   ⊆





   D
   ，若

   w
   i
   w\_i






   w









   i

   ​

   是个命名实体，则寻找对应同类型的词放到

   L
   i
   \mathbb{L}\_i






   L









   i

   ​

   中。当

   w
   i
   ′
   w'\_i






   w









   i






   ′

   ​

   影响最大时，从

   L
   i
   \mathbb{L}\_i






   L









   i

   ​

   中选择

   w
   i
   ′
   w'\_i






   w









   i






   ′

   ​

   作为

   w
   i
   ∗
   w^\*\_i






   w









   i





   ∗

   ​
2. 替代词选择策略：

   * w
     i
     ∗
     =
     R
     (
     w
     i
     ,
     L
     i
     )
     =
     arg max
     ⁡
     w
     i
     ′
     ∈
     L
     i
     P
     (
     y
     t
     r
     u
     e
     ∣
     x
     i
     ′
     )
     w^\*\_i=R(w\_i,\mathbb{L}\_i)=\argmax\_{w'\_i\in \mathbb{L}\_i}{P(y\_{true}|x'\_i)}






     w









     i





     ∗

     ​




     =





     R

     (


     w









     i

     ​


     ,




     L









     i

     ​


     )



     =







     arg



     max











     w









     i






     ′

     ​


     ∈


     L









     i

     ​


     ​





     P

     (


     y










     t

     r

     u

     e

     ​


     ∣


     x









     i






     ′

     ​


     )
     ，其中

     x
     =
     w
     1
     w
     2
     ⋯
     w
     i
     ⋯
     w
     n
     x=w\_1 w\_2\cdots w\_i\cdots w\_n





     x



     =






     w









     1

     ​



     w









     2

     ​




     ⋯




     w









     i

     ​




     ⋯




     w









     n

     ​

     ，

     x
     i
     ′
     =
     w
     1
     w
     2
     ⋯
     w
     i
     ′
     ⋯
     w
     n
     x'\_i=w\_1 w\_2\cdots w'\_i\cdots w\_n






     x









     i






     ′

     ​




     =






     w









     1

     ​



     w









     2

     ​




     ⋯




     w









     i






     ′

     ​




     ⋯




     w









     n

     ​
   * x
     i
     ∗
     =
     w
     1
     w
     2
     ⋯
     w
     i
     ∗
     ⋯
     w
     n
     x^\*\_i=w\_1 w\_2 \cdots w^\*\_i \cdots w\_n






     x









     i





     ∗

     ​




     =






     w









     1

     ​



     w









     2

     ​




     ⋯




     w









     i





     ∗

     ​




     ⋯




     w









     n

     ​

     ，有：

     △
     P
     i
     ∗
     =
     P
     (
     y
     t
     r
     u
     e
     ∣
     x
     )
     −
     P
     (
     y
     t
     r
     u
     e
     ∣
     x
     i
     ∗
     )
     \triangle P^\*\_i=P(y\_{true}|x)-P(y\_{true}|x^\*\_i)





     △


     P









     i





     ∗

     ​




     =





     P

     (


     y










     t

     r

     u

     e

     ​


     ∣

     x

     )



     −





     P

     (


     y










     t

     r

     u

     e

     ​


     ∣


     x









     i





     ∗

     ​


     )

**替换顺序策略**
：

进行切片打分，切片打分函数为：

S
(
x
,
w
i
)
=
P
(
y
t
r
u
e
∣
x
)
−
P
(
y
t
r
u
e
∣
x
i
^
)
S(x,w\_i)=P(y\_{true}|x)-P(y\_{true}|\hat{x\_i})





S

(

x

,




w









i

​


)



=





P

(


y










t

r

u

e

​


∣

x

)



−





P

(


y










t

r

u

e

​


∣









x









i

​






^

​


)
，其中

x
=
w
1
w
2
⋯
w
i
⋯
w
d
x=w\_1 w\_2 \cdots w\_i \cdots w\_d





x



=






w









1

​



w









2

​




⋯




w









i

​




⋯




w









d

​

，

x
^
i
=
w
1
w
2
⋯
u
n
k
n
o
w
n
⋯
w
d
\hat{x}\_i=w\_1 w\_2 \cdots unknown \cdots w\_d













x





^









i

​




=






w









1

​



w









2

​




⋯



u

nkn

o

w

n



⋯




w









d

​

对所有

w
i
∈
x
w\_i\in x






w









i

​




∈





x
计算切片分数，获得最佳切片向量

S
(
x
)
S(x)





S

(

x

)

单词替换优先级的评分函数：

H
(
x
,
x
i
∗
,
w
i
)
=
ϕ
(
S
(
x
)
)
i
⋅
△
P
i
∗
H(x,x^\*\_i,w\_i)=\phi(S(x))\_i\cdot \triangle P^\*\_i





H

(

x

,




x









i





∗

​


,




w









i

​


)



=





ϕ

(

S

(

x

)


)









i

​




⋅





△


P









i





∗

​

，其中

ϕ
(
⋅
)
\phi(\cdot)





ϕ

(

⋅

)
是Softmax函数。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/81ff975f14a345d294aa855782695033.png)

### Word-level Textual Adversarial Attacking as Combinatorial Optimization

> 作者：Yuan Zang, Fanchao Qi et al.
>
> 单位：清华大学
>
> 来源：ACL 2020

#### Introduction

基本点：把对抗那个样本攻击当作是组合优化问题

方法：基于义元(sememe)的单词替代方法 + 基于粒子群优化的搜索算法

#### Background

**Sememes**
: 义元是单词的语义标签，相关工作有HowNet

**PSO**
: 连续空间

S
∈
R
D
S\in \mathbb{R}^D





S



∈






R









D
，有

N
N





N
个粒子，每个粒子的位置、速度能被表示为

x
n
∈
S
x^n\in S






x









n



∈





S
，

x
n
∈
R
D
x^n\in \mathbb{R}^D






x









n



∈






R









D
，

n
∈
{
1
,
⋯
 
,
N
}
n\in \{1,\cdots, N\}





n



∈





{

1

,



⋯





,



N

}

1. 初始化，随机初始化每个粒子的位置和速度，初始速度的维度为

   v
   d
   n
   ∈
   [
   −
   V
   m
   a
   x
   ,
   V
   m
   a
   x
   ]
   v^n\_d\in [-V\_{max}, V\_{max}]






   v









   d





   n

   ​




   ∈





   [

   −


   V










   ma

   x

   ​


   ,




   V










   ma

   x

   ​


   ]
2. 记录，搜索空间的每个位置对应于一个优化分数，最高优化分数记录为个体最佳位置。个体最佳位置中的最高分数为全局最佳位置
3. 终止，如果全局最佳位置已经达到期待的最佳分数，则算法终止
4. 更新，未终止则更新速度与位置，更新的公式为：

   v
   d
   n
   =
   w
   v
   d
   n
   +
   c
   1
   ×
   r
   1
   ×
   (
   p
   d
   n
   −
   x
   d
   n
   )
   +
   c
   2
   ×
   r
   2
   ×
   (
   p
   d
   g
   −
   x
   d
   n
   )
   v^n\_d=wv^n\_d+c\_1\times r\_1\times (p^n\_d - x^n\_d) + c\_2 \times r\_2 \times (p^g\_d-x^n\_d)






   v









   d





   n

   ​




   =





   w


   v









   d





   n

   ​




   +






   c









   1

   ​




   ×






   r









   1

   ​




   ×





   (


   p









   d





   n

   ​




   −






   x









   d





   n

   ​


   )



   +






   c









   2

   ​




   ×






   r









   2

   ​




   ×





   (


   p









   d





   g

   ​




   −






   x









   d





   n

   ​


   )
   ，

   x
   d
   n
   =
   x
   d
   n
   +
   v
   d
   n
   x^n\_d=x^n\_d+ v^n\_d






   x









   d





   n

   ​




   =






   x









   d





   n

   ​




   +






   v









   d





   n

   ​

   ，其中

   w
   w





   w
   是惯性权重，

   p
   d
   n
   p^n\_d






   p









   d





   n

   ​

   和

   p
   d
   g
   p^g\_d






   p









   d





   g

   ​

   是d维第

   n
   n





   n
   个粒子的个体最佳位置与全局最佳位置，

   c
   1
   c\_1






   c









   1

   ​

   和

   c
   2
   c\_2






   c









   2

   ​

   是加速度系数，

   r
   1
   r\_1






   r









   1

   ​

   和

   r
   2
   r\_2






   r









   2

   ​

   是随机系数。更新之后，算法返回记录步骤。

#### Methodology

two parts: sememe-based word substitution & PSO-based adversarial example search

##### Sememe-based Word Substitution

1. 只替代content words (words that carry meanings and consist mostly of nouns, verbs, adjectives and adverbs)，并限制替代词跟原单词part-of-speech tag相同
2. w
   ∗
   w^\*






   w









   ∗
   替代

   w
   w





   w
   时，当且仅当

   w
   w





   w
   的意义跟

   w
   ∗
   w\_\*






   w









   ∗

   ​

   的意义有相同的义元

##### PSO-based adversarial example search

一个位置对应于一个句子，每个位置的维度对应于句子的每个单词

x
n
=
w
1
n
⋯
w
d
n
⋯
w
D
n
,
w
d
n
∈
V
(
w
d
o
)
x^n=w^n\_1 \cdots w^n\_d \cdots w^n\_D, w^n\_d\in \mathbb{V}(w^o\_d)






x









n



=






w









1





n

​




⋯




w









d





n

​




⋯




w









D





n

​


,




w









d





n

​




∈





V

(


w









d





o

​


)
，

V
(
w
d
o
)
\mathbb{V}(w^o\_d)





V

(


w









d





o

​


)
包含了

w
d
o
w^o\_d






w









d





o

​

与其替代词，

D
D





D
为原始输入的长度

**初始化**
：随机替换原始输入的一个词以决定粒子的初始化位置

**记录**
：与原PSO算法相同

**终止**
：受害模型预测到攻击者期待得到的结果标签

**更新**
：

1. 考虑搜索空间的离散性，

   v
   d
   n
   =
   w
   v
   d
   n
   +
   (
   1
   −
   w
   )
   ×
   [
   I
   (
   p
   d
   n
   ,
   x
   d
   n
   )
   +
   I
   (
   p
   d
   g
   ,
   x
   d
   n
   )
   ]
   v^n\_d=w v^n\_d + (1-w)\times [I(p^n\_d,x^n\_d) + I(p^g\_d,x^n\_d)]






   v









   d





   n

   ​




   =





   w


   v









   d





   n

   ​




   +





   (

   1



   −





   w

   )



   ×





   [

   I

   (


   p









   d





   n

   ​


   ,




   x









   d





   n

   ​


   )



   +





   I

   (


   p









   d





   g

   ​


   ,




   x









   d





   n

   ​


   )]
2. w
   w





   w
   是惯性系数，

   I
   (
   a
   ,
   b
   )
   I(a,b)





   I

   (

   a

   ,



   b

   )
   定义为：

   I
   (
   a
   ,
   b
   )
   =
   {
   1
   ,
   a
   =
   b
   −
   1
   ,
   a
   ≠
   b
   I(a,b)=\begin{cases}1,&a=b\\ -1,&a\not=b\end{cases}





   I

   (

   a

   ,



   b

   )



   =







   {











   1

   ,





   −

   1

   ,

   ​












   a



   =



   b





   a











   


   =



   b

   ​
3. w
   w





   w
   的更新公式为：

   w
   =
   (
   w
   m
   a
   x
   −
   w
   m
   i
   n
   )
   ×
   T
   −
   t
   T
   +
   w
   m
   i
   n
   w=(w\_{max}-w\_{min})\times \frac{T-t}{T} + w\_{min}





   w



   =





   (


   w










   ma

   x

   ​




   −






   w










   min

   ​


   )



   ×

















   T












   T

   −

   t

   ​




   +






   w










   min

   ​

   ，其中参数范围：

   0
   <
   w
   m
   i
   n
   <
   w
   m
   a
   x
   <
   1
   0<w\_{min}<w\_{max}<1





   0



   <






   w










   min

   ​




   <






   w










   ma

   x

   ​




   <





   1
   ，

   T
   T





   T
   和

   t
   t





   t
   分别为最大迭代次数值和最近迭代次数值
4. 调整离散的搜索空间：

   * 第一步，新的移动概率

     P
     i
     P\_i






     P









     i

     ​

     随粒子被引入到个体最佳位置，当粒子决定移动时，位置的每个维度由相同维度的速度决定，通过

     s
     i
     g
     m
     o
     i
     d
     (
     ⋅
     )
     sigmoid(\cdot)





     s

     i

     g

     m

     o

     i

     d

     (

     ⋅

     )
     函数进行概率评判。其中

     P
     i
     P\_i






     P









     i

     ​

     为：

     P
     i
     =
     P
     m
     a
     x
     −
     t
     T
     ×
     (
     P
     m
     a
     x
     −
     P
     m
     i
     n
     )
     P\_i=P\_{max}-\frac{t}{T}\times (P\_{max}-P\_{min})






     P









     i

     ​




     =






     P










     ma

     x

     ​




     −

















     T












     t

     ​




     ×





     (


     P










     ma

     x

     ​




     −






     P










     min

     ​


     )
     ，其中参数范围：

     0
     <
     P
     m
     i
     n
     <
     P
     m
     a
     x
     <
     1
     0<P\_{min}<P\_{max}<1





     0



     <






     P










     min

     ​




     <






     P










     ma

     x

     ​




     <





     1
   * 第二步，通过移动概率

     P
     g
     P\_g






     P









     g

     ​

     决定全局最佳概率：

     P
     g
     =
     P
     m
     i
     n
     +
     t
     T
     ×
     (
     P
     m
     a
     x
     −
     P
     m
     i
     n
     )
     P\_g=P\_{min}+\frac{t}{T}\times (P\_{max}-P\_{min})






     P









     g

     ​




     =






     P










     min

     ​




     +

















     T












     t

     ​




     ×





     (


     P










     ma

     x

     ​




     −






     P










     min

     ​


     )
5. 更新后应用突变：

   P
   m
   (
   x
   n
   )
   =
   m
   i
   n
   (
   0
   ,
   1
   −
   k
   ϵ
   (
   x
   n
   ,
   x
   o
   )
   D
   )
   P\_m(x^n)=min(0, 1-k\frac{\epsilon(x^n,x^o)}{D})






   P









   m

   ​


   (


   x









   n

   )



   =





   min

   (

   0

   ,



   1



   −





   k













   D












   ϵ

   (


   x









   n

   ,


   x









   o

   )

   ​


   )
   ，

   ϵ
   (
   ⋅
   )
   \epsilon(\cdot)





   ϵ

   (

   ⋅

   )
   为编辑距离。之后，返回记录步骤

#### Experiments

数据集：IMDB、SST-2、NLI、SNLI

baseline：Embedding/LM + Genetic、SYNONYM + Greedy

Evaluation Metrics:

1. Attack Success Rate (ASR)
2. Attack Validity
3. Quality of adversarial examples (modification rate, grammatical error increase rate, language model perplexity)

### Contextualized Perturbation for Textual Adversarial Attack

> 作者：Dianqi Li, Yizhe Zhang et al.
>
> 单位：华盛顿大学、微软研究院、杜克大学
>
> 来源：NAACL 2021

#### Introduction

Problem: rule-based methods are agnostic to context, limiting their ability to produce natural, fluent, and grammatical outputs

ContextuaLized AdversaRial Example: CLARE, a mask-then-infill procedure

CLARE features three contextualized perturbations: Replace, Insert and Merge

#### CLARE

##### Background

victim model:

f
(
⋅
)
f(\cdot)





f

(

⋅

)

similarity function:

s
i
m
(
x
′
,
x
)
sim(x',x)





s

im

(


x










′

,



x

)

adversarial example:

x
′
x'






x










′
for

x
x





x
, s.t.

f
(
x
′
)
≠
f
(
x
)
f(x')\not=f(x)





f

(


x










′

)

















=





f

(

x

)
,

s
i
m
(
x
′
,
x
)
>
l
sim(x',x)>l





s

im

(


x










′

,



x

)



>





l

##### Masking and Contextualized Infilling

**Replace**
:

对于给定的第

i
i





i
个位置，首先给

x
i
x\_i






x









i

​

进行Mask然后从候选词集

Z
Z





Z
中选出token

z
z





z
来填充：

x
~
=
x
1
⋯
x
i
−
1
[
M
A
S
K
]
x
i
+
1
⋯
x
n
\tilde{x}=x\_1\cdots x\_{i-1} [MASK] x\_{i+1} \cdots x\_n












x





~



=






x









1

​




⋯




x










i

−

1

​


[

M

A

S

K

]


x










i

+

1

​




⋯




x









n

​

x
~
z
=
r
e
p
l
a
c
e
(
x
,
i
)
=
x
1
⋯
x
i
−
1
z
x
i
+
1
⋯
x
n
\tilde{x}\_z = replace(x,i)=x\_1\cdots x\_{i-1}z x\_{i+1}\cdots x\_n













x





~









z

​




=





re

pl

a

ce

(

x

,



i

)



=






x









1

​




⋯




x










i

−

1

​


z


x










i

+

1

​




⋯




x









n

​

要求：

1. z
   z





   z
   应该适应于未mask的上下文
2. x
   ~
   z
   \tilde{x}\_z













   x





   ~









   z

   ​

   应该与

   x
   x





   x
   相似
3. x
   ~
   z
   \tilde{x}\_z













   x





   ~









   z

   ​

   应该能在

   f
   f





   f
   中触发错误

p
M
L
M
p\_{MLM}






p










M

L

M

​

: 预训练好的语言建模模型

根据要求约束可以用数学公式描述为：

* 对应于第1、2点：

  {
  z
  ′
  ∈
  V
  ∣
  p
  M
  L
  M
  (
  z
  ′
  ∣
  x
  ~
  )
  >
  k
  ,
  s
  i
  m
  (
  x
  ,
  x
  ~
  z
  ′
  )
  >
  l
  }
  \{z'\in V| p\_{MLM}(z'|\tilde{x})>k, sim(x,\tilde{x}\_{z'})>l\}





  {


  z










  ′



  ∈





  V

  ∣


  p










  M

  L

  M

  ​


  (


  z










  ′

  ∣








  x





  ~

  )



  >





  k

  ,



  s

  im

  (

  x

  ,











  x





  ~











  z










  ′

  ​


  )



  >





  l

  }
  ，

  V
  V





  V
  为语言建模模型的单词表，从

  Z
  Z





  Z
  中挑选token填充
* 对应于第3点：

  z
  =
  arg min
  ⁡
  z
  ′
  ∈
  Z
  p
  f
  (
  y
  ∣
  x
  z
  ′
  ~
  )
  z=\argmin\limits\_{z'\in Z}p\_f(y|\tilde{x\_{z'}})





  z



  =















  z










  ′

  ∈

  Z






  arg



  min

  ​





  p









  f

  ​


  (

  y

  ∣









  x











  z










  ′

  ​






  ~

  ​


  )

**Insert**
:

x
~
=
x
1
⋯
x
i
[
M
A
S
K
]
x
i
+
1
⋯
x
n
\tilde{x} = x\_1\cdots x\_i [MASK] x\_{i+1} \cdots x\_n












x





~



=






x









1

​




⋯




x









i

​


[

M

A

S

K

]


x










i

+

1

​




⋯




x









n

​

i
n
s
e
r
t
(
x
,
i
)
=
x
1
⋯
x
i
z
x
i
+
1
⋯
x
n
insert(x,i)=x\_1\cdots x\_i z x\_{i+1} \cdots x\_n





in

ser

t

(

x

,



i

)



=






x









1

​




⋯




x









i

​


z


x










i

+

1

​




⋯




x









n

​

**Merge**
: 就是二元词组换成一元词

x
~
=
x
1
⋯
x
i
−
1
[
M
A
S
K
]
x
i
+
2
⋯
x
n
\tilde{x}=x\_1\cdots x\_{i-1} [MASK] x\_{i+2} \cdots x\_n












x





~



=






x









1

​




⋯




x










i

−

1

​


[

M

A

S

K

]


x










i

+

2

​




⋯




x









n

​

m
e
r
g
e
(
x
,
i
)
=
x
1
⋯
x
i
−
1
z
x
i
+
2
⋯
x
n
merge(x,i)=x\_1\cdots x\_{i-1}z x\_{i+2}\cdots x\_n





m

er

g

e

(

x

,



i

)



=






x









1

​




⋯




x










i

−

1

​


z


x










i

+

2

​




⋯




x









n

​

对于输入序列每个位置，CLARE进行替换或插入或合并，之后通过语言建模模型和文本相似度函数构建候选令牌集，最小化正确标签的概率的令牌当作替代令牌。

#### Sequentially Applying the PErturbations

输入对：

(
x
,
y
)
(x,y)





(

x

,



y

)

x
x





x
的长度为

n
n





n
，若候选集不为空，共进行

3
n
3n





3

n
个操作，操作为那三种，所有操作的应用操作表示为

a
(
x
)
a(x)





a

(

x

)
。

每一步都计算一个评分：

s
(
x
,
y
)
(
a
)
=
−
p
f
(
y
∣
a
(
x
)
)
s\_{(x,y)}(a)=-p\_f(y|a(x))






s










(

x

,

y

)

​


(

a

)



=





−


p









f

​


(

y

∣

a

(

x

))

每个位置只有一种操作被应用到。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/993e2b9c22184533bd2a0069e8b8b578.png)

### Frequency-Guided Word Substitutions for Detecting Textual Adversarial Examples

> 作者：Xinghao Yang, Yongshun Gong et al.
>
> 单位：IEEE
>
> 来源：Trans on Cybernetics

#### Algorithm

##### Black Box settings

提供的：输入文本

x
∈
X
x\in X





x



∈





X
，DNN模型

F
F





F
，正确标签

y
t
r
u
e
∈
Y
y\_{true}\in Y






y










t

r

u

e

​




∈





Y
，i.e.,

F
(
x
)
=
y
t
r
u
e
F(x)=y\_{true}





F

(

x

)



=






y










t

r

u

e

​

，由该目标函数优化得到：

arg max
⁡
y
i
∈
Y
P
(
y
i
∣
x
)
=
y
t
r
u
e
\argmax\limits\_{y\_i\in Y}P(y\_i|x)=y\_{true}















y









i

​


∈

Y






arg



max

​




P

(


y









i

​


∣

x

)



=






y










t

r

u

e

​

，或者用户特定目标标签：

arg max
⁡
y
i
∈
Y
P
(
y
i
∣
x
∗
)
=
y
t
a
r
g
e
t
\argmax\limits\_{y\_i\in Y}P(y\_i|x^\*)=y\_{target}















y









i

​


∈

Y






arg



max

​




P

(


y









i

​


∣


x









∗

)



=






y










t

a

r

g

e

t

​

##### Sementic Similarity

E
n
c
o
d
e
r
Encoder





E

n

co

d

er
为

U
S
E
USE





U

SE
的编码器

U
S
E
s
c
o
r
e
=
C
o
s
i
n
e
(
E
n
c
o
d
e
r
(
x
)
,
E
n
c
o
d
e
r
(
x
a
d
v
)
)
USE\_{score}=Cosine(Encoder(x),Encoder(x\_{adv}))





U

S


E










score

​




=





C

os

in

e

(

E

n

co

d

er

(

x

)

,



E

n

co

d

er

(


x










a

d

v

​


))

##### Bigram & Unigram Candidate Selection

使用WordNet (Synonym来源，假定WordNet的同义词空间为

W
\mathbb{W}





W
) 跟HowNet (sememes，假定义元空间为

H
\mathbb{H}





H
)。

1. 创建候选集，

   * 给定输入句子

     X
     =
     {
     w
     1
     ,
     ⋯
      
     ,
     w
     n
     }
     X=\{w\_1,\cdots, w\_n\}





     X



     =





     {


     w









     1

     ​


     ,



     ⋯





     ,




     w









     n

     ​


     }
     ，用WordNet判断

     (
     w
     i
     ,
     w
     i
     +
     1
     )
     (w\_i, w\_{i+1})





     (


     w









     i

     ​


     ,




     w










     i

     +

     1

     ​


     )
     是否有

     w
     s
     y
     n
     ∗
     ∈
     W
     w^\*\_{syn}\in \mathbb{W}






     w










     sy

     n





     ∗

     ​




     ∈





     W
     ，没有则根据

     w
     i
     w\_i






     w









     i

     ​

     从

     W
     \mathbb{W}





     W
     中选同义词以及从

     H
     \mathbb{H}





     H
     中选候选义元，构成候选词集

     S
     i
     ⊂
     W
     ∪
     H
     S\_i\subset \mathbb{W}\cup \mathbb{H}






     S









     i

     ​




     ⊂





     W



     ∪





     H
     。同时通过候选过滤器，选择相同POS tag的单词
   * 若

     w
     i
     w\_i






     w









     i

     ​

     为命名实体，则通过加入更多NE候选词以拓展候选集
2. 选择最佳候选：

   * 对于候选集

     S
     i
     S\_i






     S









     i

     ​

     ，候选重要性分数为：

     I
     w
     i
     ′
     =
     P
     (
     y
     t
     r
     u
     e
     ∣
     x
     )
     −
     P
     (
     y
     t
     r
     u
     e
     ∣
     x
     i
     ′
     )
     ,
     ∀
     w
     i
     ′
     ∈
     S
     i
     I\_{w'\_i}=P(y\_{true}|x)-P(y\_{true}|x'\_i), \forall w'\_i\in \mathbb{S}\_i






     I











     w









     i






     ′

     ​


     ​




     =





     P

     (


     y










     t

     r

     u

     e

     ​


     ∣

     x

     )



     −





     P

     (


     y










     t

     r

     u

     e

     ​


     ∣


     x









     i






     ′

     ​


     )

     ,



     ∀


     w









     i






     ′

     ​




     ∈






     S









     i

     ​

     ，其中

     x
     =
     [
     w
     1
     ,
     ⋯
      
     ,
     w
     i
     ,
     ⋯
      
     ,
     w
     n
     ]
     x=[w\_1,\cdots, w\_i, \cdots, w\_n]





     x



     =





     [


     w









     1

     ​


     ,



     ⋯





     ,




     w









     i

     ​


     ,



     ⋯





     ,




     w









     n

     ​


     ]
     ，

     x
     i
     ′
     =
     [
     w
     1
     ,
     ⋯
      
     ,
     w
     i
     ′
     ,
     ⋯
      
     ,
     w
     n
     ]
     x'\_i=[w\_1,\cdots,w'\_i,\cdots, w\_n]






     x









     i






     ′

     ​




     =





     [


     w









     1

     ​


     ,



     ⋯





     ,




     w









     i






     ′

     ​


     ,



     ⋯





     ,




     w









     n

     ​


     ]
   * 最佳候选：

     w
     i
     ∗
     =
     R
     (
     w
     i
     ,
     S
     i
     )
     =
     arg max
     ⁡
     w
     i
     ′
     ∈
     S
     i
     I
     w
     i
     ′
     w^\*\_i=R(w\_i,\mathbb{S}\_i)=\argmax\limits\_{w'\_i\in \mathbb{S}\_i} I\_{
     {w}'\_i}






     w









     i





     ∗

     ​




     =





     R

     (


     w









     i

     ​


     ,




     S









     i

     ​


     )



     =















     w









     i






     ′

     ​


     ∈


     S









     i

     ​







     arg



     max

     ​





     I












     w









     i






     ′

     ​


     ​

##### Semantic Preservation Optimization

SPO用于优化单词替代顺序优先级，通过三个目标：

* 成功攻击
* 最小替代
* 语义不变

获得的

n
n





n
个对抗语句：

{
x
1
∗
,
⋯
 
,
x
n
∗
}
\{x^\*\_1,\cdots, x^\*\_n\}





{


x









1





∗

​


,



⋯





,




x









n





∗

​


}
，从

X
X





X
到

X
i
∗
X^\*\_i






X









i





∗

​

的差为最大攻击效果：

△
P
i
∗
=
P
(
y
t
r
u
e
∣
x
)
−
P
(
y
t
r
u
e
∣
x
i
∗
)
\triangle P^\*\_i=P(y\_{true}|x)-P(y\_{true}|x^\*\_i)





△


P









i





∗

​




=





P

(


y










t

r

u

e

​


∣

x

)



−





P

(


y










t

r

u

e

​


∣


x









i





∗

​


)
，直接使用将可能导致替换陷入局部最优而非全局最优。

初始的迭代输入：

G
0
\mathbb{G}^0






G









0

阈值：

M
M





M
，限制单词被修改的数量

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/136cc4ed206546d2bdbaebcfc933f5b4.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e608cfd9b24a4da69dc0962089f00df6.png)

##### SPO with Semantic Filter (SPOF)

收集可能的对抗样本的空集：

S
u
c
A
d
v
SucAdv





S

u

c

A

d

v

![](https://i-blog.csdnimg.cn/direct/77328852db8849ae929167f8e62b6705.png)

##### Targeted Attack Strategy

考虑目标攻击时：

1. 算法1 行18跟算法3 行19将修改为：

   F
   (
   x
   a
   d
   v
   )
   =
   y
   t
   a
   r
   g
   e
   t
   F(x\_{adv})=y\_{target}





   F

   (


   x










   a

   d

   v

   ​


   )



   =






   y










   t

   a

   r

   g

   e

   t

   ​
2. I
   w
   i
   ′
   =
   P
   (
   y
   t
   a
   r
   g
   e
   t
   ∣
   x
   i
   ′
   )
   −
   P
   (
   y
   t
   a
   r
   g
   e
   t
   ∣
   x
   )
   ,
   ∀
   w
   i
   ′
   ∈
   S
   i
   I\_{w'\_i}=P(y\_{target}|x'\_i)-P(y\_{target}|x), \forall w'\_i\in \mathbb{S}\_i






   I











   w









   i






   ′

   ​


   ​




   =





   P

   (


   y










   t

   a

   r

   g

   e

   t

   ​


   ∣


   x









   i






   ′

   ​


   )



   −





   P

   (


   y










   t

   a

   r

   g

   e

   t

   ​


   ∣

   x

   )

   ,



   ∀


   w









   i






   ′

   ​




   ∈






   S









   i

   ​
3. △
   P
   i
   ∗
   =
   P
   (
   y
   t
   a
   r
   g
   e
   t
   ∣
   x
   i
   ∗
   )
   −
   P
   (
   y
   t
   a
   r
   g
   e
   t
   ∣
   x
   )
   \triangle P^\*\_i=P(y\_{target}|x^\*\_i) - P(y\_{target}|x)





   △


   P









   i





   ∗

   ​




   =





   P

   (


   y










   t

   a

   r

   g

   e

   t

   ​


   ∣


   x









   i





   ∗

   ​


   )



   −





   P

   (


   y










   t

   a

   r

   g

   e

   t

   ​


   ∣

   x

   )
4. △
   P
   a
   d
   v
   =
   P
   (
   y
   t
   a
   r
   g
   e
   t
   ∣
   x
   a
   d
   v
   )
   −
   P
   (
   y
   t
   a
   r
   g
   e
   t
   ∣
   x
   )
   \triangle P\_{adv} = P(y\_{target}|x\_{adv}) - P(y\_{target}|x)





   △


   P










   a

   d

   v

   ​




   =





   P

   (


   y










   t

   a

   r

   g

   e

   t

   ​


   ∣


   x










   a

   d

   v

   ​


   )



   −





   P

   (


   y










   t

   a

   r

   g

   e

   t

   ​


   ∣

   x

   )

#### Experiment

数据集：IMDB, AG’s News, Yahoo! Answers

受害模型：CNN、Ch-CNN、LSTM、Bi-LSTM

评估指标：

A
S
R
=
∑
x
∈
X
{
F
(
x
)
=
y
t
r
u
e
∧
F
(
x
+
△
x
)
=
y
∗
}
∑
x
∈
X
{
F
(
x
)
=
y
t
r
u
e
}
ASR=\frac{\sum\_{x\_\in X}\{F(x)=y\_{true}\wedge F(x+\triangle x)=y^\*\}}{\sum\_{x\in X}\{F(x)=y\_{true}\}}





A

SR



=


















∑










x

∈

X

​


{

F

(

x

)

=


y










t

r

u

e

​


}













∑











x








∈

​


X

​


{

F

(

x

)

=


y










t

r

u

e

​


∧

F

(

x

+

△

x

)

=


y









∗

}

​

### Universal Adversarial Triggers for Attacking and Analyzing NLP

> 作者：Eric Wallace, Shi Feng et al.
>
> 单位：Allen Institute for AI et al.
>
> 来源：ACL 2021

#### Abstract & Intro

**universal adversarial triggers**
: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset.

**contribution**
: gradient-guided search over tokens which finds short trigger sequences that successfully trigger the target prediction

**constraint**
: white-box attack to specific model (however, can transfer to other models)

**triggers**
: a new form of universal adversarial perturbation adapted to discrete textual inputs.

**finding**
:

1. short sequences can trigger successfully.
2. trigger can be used in transfer learning.
3. identify heuristics learned by SQuAD models

#### Universal Adversarial Triggers

##### Setting and Motivation

universal adversarial attack:

1. using the exact same attack for any input (Moosavi-Dezfooli 2017, Brown 2017)
2. advantageous: no access to the target model at test time, trigger sequences can be widely distributed for anyone to fool machine learning models.
3. transfer across models and don’t need white-box access to the target model (Moosavi-Dezfooli 2017)

##### Attack Model and Objective

model:

f
f





f

a text input of tokens:

t
t





t

target label:

y
~
\tilde{y}












y





~

​

aim:

f
(
t
a
d
v
;
t
)
=
y
~
f(t\_{adv};t)=\tilde{y}





f

(


t










a

d

v

​


;



t

)



=












y





~

​

Universal Objective:

arg
⁡
t
a
d
v
min
⁡
E
t
∼
T
[
L
(
y
~
,
f
(
t
a
d
v
;
t
)
)
]
\arg\limits\_{t\_{adv}}\min \mathbb{E}\_{t\sim \Tau}[L(\tilde{y},f(t\_{adv};t))]















t










a

d

v

​






ar
g

​




min




E










t

∼

T

​


[

L

(








y





~

​


,



f

(


t










a

d

v

​


;



t

))]

trigger token:

e
a
d
v
i
e\_{adv\_i}






e










a

d


v









i

​


​

##### Trigger Search Algorithm

Token Replacement Strategy: based on a linear approximation of the task loss.

update the embedding for

e
a
d
v
i
e\_{adv\_i}






e










a

d


v









i

​


​

to minimize the loss:

arg
⁡
e
i
′
∈
V
min
⁡
[
e
i
′
−
e
a
d
v
i
]
⊤
∇
e
a
d
v
i
L
\arg\limits\_{e'\_i\in V} \min [e'\_i-e\_{adv\_i}]^\top \nabla\_{e\_{adv\_i}} L















e









i






′

​


∈

V





ar
g

​




min

[


e









i






′

​




−






e










a

d


v









i

​


​



]









⊤


∇











e










a

d


v








i

​


​


​


L

set of all token embeddings:

V
V





V

average gradient of the task loss:

∇
e
a
d
v
i
L
\nabla\_{e\_{adv\_i}}L






∇











e










a

d


v








i

​


​


​


L

e
i
′
e'\_i






e









i






′

​

: computed in brute-force with |V|d dimensional dot products, d is the dimensionality of the token embedding.

Process Pic: 先算任务梯度 -> 遍历所有的token取极小 -> 获得极小的token -> 作为trigger结合语句计算概率分布 -> 继续重复以上步骤 -> 目标函数极小得到结果

augment: beam search, top-k token considered

![外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传](https://i-blog.csdnimg.cn/direct/af2bcb1a077b4dba811b5785c6dcfd4d.jpeg)

##### Tasks and Associated Loss Functions

Classification: bypass fake news detection by trigger.

Reading Comprehension: modify a web page in order to trigger malicious or vulgar answers, focus on why, who, when and where questions.

Conditional Text Generation: create triggers that are prepended before t to let model generate similar content to a set of targets Y. Maximize the likelihood of racist outputs by minimizing the following loss:
  



E
∑
i
=
1
∣
y
∣
log
⁡
(
1
−
p
(
y
i
∣
t
∗
a
d
v
,
t
,
y
1
,
⋯
 
,
y
i
−
1
)
)
,
y
∼
Y
,
t
∼
T
\mathbb{E}\sum^{|y|}\limits\_{i=1}\log(1-p(y\_i|t^\*{adv},t,y\_1,\cdots,y\_{i-1})), y\sim Y, t\sim \Tau





E












i

=

1





∑






∣

y

∣

​




lo
g

(

1



−





p

(


y









i

​


∣


t









∗


a

d

v

,



t

,




y









1

​


,



⋯





,




y










i

−

1

​


))

,



y



∼





Y

,



t



∼





T

#### Attacking Text Classification

two text classification datasets.

Sentiment Analysis: Stanford Sentiment Treebank, Bi-LSTM model, word2vec / ELMo embeddings.

Natural Language Inference: SNLI dataset, ESIM, DA-GloVe, DA-ELMo.

##### Breaking Sentiment Analysis

pre-avoid: use a lexicon to blacklist sentiment words. “zoning tapping fiennes” is a trigger.

ELMo-based Model: “uˆ{b”, “m&s~” are triggers, droping accuracy.

##### Breaking Natural Language Inference

motivation: threat the accuracy.

attack SNLI models, result is here: these trigger can degrade the three model’s accuracy to nearly 0.

the attack also readily transfer.

![](https://i-blog.csdnimg.cn/direct/28c1fe7360334ccfa22db25ab2bf29eb.jpeg)

#### Attacking Reading Comprehension

motivation: answer the specific answer just like a backdoor to trigger

triggers for SQuAD: use an simple baseline and test the trigger’s transferability to more advanced models

embedding: GloVe

target answer: ‘to kill anmerican people’、‘donald trump’、‘january 2014’、‘new york’

question type: why, who, when, where.

Results:
![](https://i-blog.csdnimg.cn/direct/bab51261c959467b970ab3a7a866917f.jpeg)

transferability:

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/27f251985bf845cd8237a58ad34d4ee4.jpeg)

#### Analyzing The Trigger

Triggers Align With SNLI Artifacts:

* dataset artifacts are successful triggers，‘no’、‘tv’、‘naked’ can drop accuracy.
* entailment overlap bias

explain the triggers:

* PMI Analysis: question-correlation answer triggers have high PMI values,

  P
  M
  I
  (
  w
  o
  r
  d
  ,
  c
  l
  a
  s
  s
  )
  =
  log
  ⁡
  p
  (
  w
  o
  r
  d
  ,
  c
  l
  a
  s
  s
  )
  p
  (
  w
  o
  r
  d
  )
  p
  (
  c
  l
  a
  s
  s
  )
  PMI(word, class)=\log \frac{p(word, class)}{p(word)p(class)}





  PM

  I

  (

  w

  or

  d

  ,



  c

  l

  a

  ss

  )



  =





  lo
  g















  p

  (

  w

  or

  d

  )

  p

  (

  c

  l

  a

  ss

  )












  p

  (

  w

  or

  d

  ,

  c

  l

  a

  ss

  )

  ​
* Question Type Matching
* token order, Placement, and Removal: model is sensitive to token order, trigger is not very correlated with replacement, remove tokens can increase the success rate when transferring the triggers to black-box models.