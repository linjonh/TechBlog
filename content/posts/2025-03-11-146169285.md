---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34343638313830392f:61727469636c652f64657461696c732f313436313639323835"
layout: post
title: "2025ICLR厦大华为,LoSA,基于表示互信息的动态层级稀疏率,基于重构误差的秩分配LLM-的动态低秩稀疏自适应"
date: 2025-03-11 10:28:40 +08:00
description: "本文提出动态低秩稀疏适配，实现稀疏 LLM 与低秩适配的无缝集成。它在微调过程中动态调整稀疏率和秩，以提高稀疏 LLM 的性能而不增加推理延迟。它通过基于表示互信息的动态层级稀疏率和基于重构误差的秩分配策略实现了稀疏与低秩适配的有效融合"
keywords: "（2025|ICLR|厦大&华为，LoSA，基于表示互信息的动态层级稀疏率，基于重构误差的秩分配）LLM 的动态低秩稀疏自适应"
categories: ['论文笔记']
tags: ['语言模型', '算法', '深度学习']
artid: "146169285"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146169285
    alt: "2025ICLR厦大华为,LoSA,基于表示互信息的动态层级稀疏率,基于重构误差的秩分配LLM-的动态低秩稀疏自适应"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146169285
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146169285
cover: https://bing.ee123.net/img/rand?artid=146169285
image: https://bing.ee123.net/img/rand?artid=146169285
img: https://bing.ee123.net/img/rand?artid=146169285
---

# （2025|ICLR|厦大&华为，LoSA，基于表示互信息的动态层级稀疏率，基于重构误差的秩分配）LLM 的动态低秩稀疏自适应

## Dynamic Low-Rank Sparse Adaptation for Large Language Models

![](https://i-blog.csdnimg.cn/direct/961894009b8240d8979f7516c418e588.png)

---

## **1. 引言**

随着大语言模型（Large Language Models, LLMs）的发展，模型规模显著增加，极大提升了在多领域理解和生成内容的能力。然而，
**模型规模的指数增长也带来了部署和推理困难，主要表现为计算需求和延迟的剧增**
。针对这些问题，模型压缩方法，如稀疏化（Sparsity）、量化和知识蒸馏被广泛研究，其中稀疏方法在降低模型大小和推理延迟方面表现突出。

现有的稀疏方法，如 SparseGPT 和 Wanda，在高稀疏度情况下性能会严重退化，通常需要额外的微调以恢复性能。低秩适配（Low-Rank Adaptation, LoRA）是一种高效的参数微调方法，但存在两个核心问题：

* LoRA 无法在微调后融入稀疏的 LLM 权重中，导致推理延迟增加。
* 均匀稀疏率和静态秩设置未能充分利用不同层之间的差异，限制了稀疏模型性能的恢复。

本文提出了一种
**动态低秩稀疏适配方法（Dynamic Low-rank Sparse Adaptation, LoSA）**
，实现了稀疏 LLM 与低秩适配的无缝集成。
**LoSA 在微调过程中动态调整稀疏率和秩**
，以提高稀疏 LLM 的性能而不增加推理延迟。具体而言，LoSA 通过
**基于表示互信息（Representation Mutual Information, RMI）的动态层级稀疏率**
和
**基于重构误差的秩分配策略**
实现了稀疏与低秩适配的有效融合。

### 1.1 关键词

大语言模型（LLMs），稀疏化（Sparsity），低秩适配（LoRA），表示互信息（Representation Mutual Information, RMI），稀疏感知秩分配（Sparsity-Aware Rank Allocation），动态稀疏微调（Dynamic Sparse Fine-tuning）

## 2. 方法

### 2.1 预备知识

本文基于 SparseGPT 的思想，将 LLM 的稀疏化视作逐层的重构问题。具体而言，将密集模型每一层的权重与输入特征图进行稀疏化掩码处理，并引入低秩适配模块以减小稀疏引入的重构误差。通过统一优化问题同时确定稀疏掩码、层级稀疏率和层级秩分配。

优化目标是尽量减少稀疏 LLM 的每一层与其对应的密集层之间的输出差异：

![](https://i-blog.csdnimg.cn/direct/12cafd2c090d45a2935e581fc2192719.png)

其中，i：层，s：稀疏率，r：秩，X：输入，M：掩码，W：权重，BA：低秩矩阵

![](https://i-blog.csdnimg.cn/direct/53e275d2e24c41469921d28f6901c7dc.png)

**图 1：将传统的稀疏 LLM 与 LoRA 相结合与 LoSA 方法进行比较**
：

* (a) 传统的 LLM 稀疏方法采用均匀的稀疏率，而 LoRA 也使用均匀的秩。此外，LoRA 权重不能合并（merge）到稀疏 LLM 权重中。
* (b) LoSA 对 LLM 执行动态稀疏低秩自适应，同时将稀疏性应用于 LLM 和低秩自适应。此外，LoSA 根据表示互信息动态确定逐层稀疏率，并根据稀疏 LLM 的重建误差分配低秩自适应的秩。

### 2.2 层级稀疏率确定

现有稀疏方法采用均匀稀疏率，忽视了层间重要性的差异。本文基于 RMI 提出了一种快速计算层级重要性的方法。RMI 用于衡量不同层之间的冗余性，进而确定每一层的稀疏率。

* 公式 2：信息瓶颈（Information Bottleneck，IB），用于压缩冗余
* 公式 3：扩展的 IB，附加了最小化层间信息冗余。

![](https://i-blog.csdnimg.cn/direct/60c49ea902eb43c892f309c9e893fb34.png)

![](https://i-blog.csdnimg.cn/direct/4715726551e04360a7342302ec64218e.png)

上式意味着与其他层高度相关的层不太重要。 层 i 的重要性计算为：

![](https://i-blog.csdnimg.cn/direct/d5442e5735b34c7ba75f9c0ec61faad3.png)

层稀疏率 s 计算为：

![](https://i-blog.csdnimg.cn/direct/6ad57a9920504abc8ac050c8745f650d.png)

上述方法难以计算，可利用归一化 Hilbert-Schmidt 独立性准则（Hilbert-Schmidt Independence Criterion，HSIC）实现快速高效计算，仅需通过模型的特征图即可完成，极大降低计算复杂度。

![](https://i-blog.csdnimg.cn/direct/0c74a74502d84a8d8b3616d4cdf9ea81.png)

其中，X_i 表示第 i 层的输入，
**X**
表示第 i 层的特征图。

**【注：直观理解，类似于余弦相似度，两层特征图相似度越高，这两层的相关度越高】**

### 2.3 稀疏感知的秩分配

使用低秩自适应可以有效恢复稀疏 LLM 的性能。为合理分配有限的低秩适配参数预算，本文提出了基于层级重构误差的秩分配方法，更高的重构误差意味着该层需要更多的微调参数。秩分配公式为：

![](https://i-blog.csdnimg.cn/direct/ab57435db1cd43c6aefd3805e237847a.png)

其中，L_i 表示第 i 层的重构误差，L_avg 表示所有 n 层重构误差的均值， Ω 表示所有 n 层的平均秩，⌊x⌉ 将 x 四舍五入为最接近的整数。

### 2.4 动态稀疏与适配

本文进一步提出了一种动态稀疏与适配策略，逐步增加稀疏率，同时动态调整低秩适配模块的秩，以有效整合稀疏和微调。

执行 T 步稀疏性和微调，并使用立方稀疏性调度（cubic sparsity schedule）确定渐进稀疏率，如下所述：

![](https://i-blog.csdnimg.cn/direct/99d371771c5c4aa6bbdcc442823dc530.png)

其中 Θ^f 是最终稀疏率，Θ^t 表示第 t 步中 n 层的平均稀疏率。

此外，由于重构误差会随着稀疏率的上升而增加，因此在每个步骤中线性增加平均秩 Ω^t，即

![](https://i-blog.csdnimg.cn/direct/f268fe1d77864774962f0d46243488c7.png)

在计算步骤 t 的平均稀疏率 Θ^t 后，

* 首先使用第 2.2 节中概述的方法建立分层稀疏率 s^t。
* 随后，通过应用稀疏掩码 M^t 同时稀疏化 LLM 和低秩自适应的权重，该掩码是使用 SparseGPT或 Wanda 得出的。这种协调的方法确保了 LLM 权重和低秩自适应之间的兼容性，有助于在微调后将低秩自适应集成到 LLM 的稀疏权重中。
* 一旦建立了稀疏 LLM，就会采用第 2.3 节中描述的秩分配方法确定低秩自适应的分层秩 r^t。
* 算法的全部细节在算法 1 中概述。

![](https://i-blog.csdnimg.cn/direct/49519625b73645aba47335fe2b4ce207.png)

## **3. 实验**

### **3.1 实验设置**

实验模型包括 LLaMA-1、LLaMA-2、LLaMA-3、Vicuna 和 OPT，参数规模覆盖 7B 到 70B。使用 WikiText-2 数据集评估语言建模能力，使用多个零样本任务评估模型泛化能力。微调数据集为从 Alpaca-GPT4 中随机抽取的 10K 样本，优化器为 Paged AdamW，学习率为 2×10⁻⁴，初始秩为 6。

### 3.2 语言建模

![](https://i-blog.csdnimg.cn/direct/ebd3d8f51aac433f879466395c87ab77.png)

LoSA 方法在 WikiText-2 上显著降低了不同稀疏水平（50%-70%）下稀疏 LLM 的困惑度（Perplexity），表现显著优于 LoRA。如在 70% 稀疏率下，LLaMA-2-7B 使用 LoSA 微调，困惑度较 Wanda 降低 68.73，展现了显著的性能提升。

### 3.3 零样本任务

![](https://i-blog.csdnimg.cn/direct/81ca87f0735645989ba7e2c179817cc4.png)

LoSA 在 HellaSwag、Winogrande 等七个下游零样本任务上明显提高了稀疏 LLM 的准确性。尤其在 70% 稀疏率下，LoSA 对 LLaMA-2-7B 的平均零样本准确率提升达 16.32%，明显优于 LoRA，证明了其强大的泛化性能。

### 3.4 N:M 稀疏性

![](https://i-blog.csdnimg.cn/direct/44df04ef9bb74e01ba23a57372119bab.png)

LoSA 同样适用于混合 N:M 稀疏设置（如混合 2:8 稀疏，N 表示非零权重），通过给更重要层更低的 N 值，进一步提升了稀疏模型的性能，实验结果表明 LoSA 显著优于传统方法和 LoRA。

### 3.5 消融实验

![](https://i-blog.csdnimg.cn/direct/beb4a8ff982641c79500e924a80ee8c7.png)

![](https://i-blog.csdnimg.cn/direct/b475cabcd4064e8abaa4f3ecba608b27.png)

消融实验验证了层级稀疏率（LSR）、稀疏感知秩分配（SRA）和动态稀疏适配（DSA）三个核心策略的有效性。实验表明，移除任一策略都会导致性能下降，表明三者均对LoSA的性能提升具有贡献，其中动态稀疏适配策略的贡献最大。

![](https://i-blog.csdnimg.cn/direct/cb1367752a664804a329d3a0d131f04d.png)

![](https://i-blog.csdnimg.cn/direct/fab4f6b70a1d4a8fa69cce7f96eae7af.png)

步 T 决定了稀疏率增加的速度。T 越大，稀疏率增加越慢，每次删除的参数越少。

### 3.6 分析

![](https://i-blog.csdnimg.cn/direct/3e486e0e17b144e4b3f4213eb5e5d497.png)

![](https://i-blog.csdnimg.cn/direct/664ab444daf945efb4f1343f7fb1db39.png)

LoSA 微调效率高，参数量仅为 LoRA 的 1 − s%，GPU 内存占用与 LoRA 相近，但由于额外步骤耗时较长，LoSA 微调需要更多时间（约 45 分钟）。但其相比 LoRA 在推理速度和模型精度方面均有明显优势，如 CPU 加速最高达到 2.60 倍。

## 4. 结论

本文提出了动态低秩稀疏适配方法 LoSA，实现了 LLM 稀疏和低秩适配的统一优化，有效提升了稀疏 LLM 的性能，无额外推理延迟，并通过广泛实验验证了方法的高效性和实用性。

---

**论文地址：https://arxiv.org/abs/2502.14816**

**项目页面：https://github.com/wzhuang-xmu/LoSA**