---
arturl_encode: "6874747073:3a2f2f626c6f672e6373646e2e6e65742f79757562656b612f:61727469636c652f64657461696c732f313238393435323431"
layout: post
title: "使用dperf测试dpvs性能"
date: 2025-01-19 09:51:06 +08:00
description: "文章介绍了如何使用dperf这款基于DPDK的高性能网络压测工具来测试DPVS的性能，包括单向和双向"
keywords: "dperf"
categories: ['未分类']
tags: ['Tcp']
artid: "128945241"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=128945241
  alt: "使用dperf测试dpvs性能"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=128945241
featuredImagePreview: https://bing.ee123.net/img/rand?artid=128945241
---

# 使用dperf测试dpvs性能

## 为什么选择dperf

[dpvs](https://github.com/iqiyi/dpvs)
是一个基于dpdk实现的高性能四层负载均衡，最近在学习怎么测试四层lb各种性能指标。简单罗列了下dpvs需要关注的性能指标：

| 指标 | 说明 |
| --- | --- |
| 单向pps | 指入向每秒可以处理的数据包个数极限值，测试单向pps性能一般使用udp小包（因为udp是无状态的，比较简单），当dpvs出现imiss的时候则达到极限值 |
| 双向pps | 即入向+出向pps极限值，需要在rs上部署udp server |
| bps | 指dpvs每秒可以处理的数据包长度的极限值，一半可以使用udp大包来测试 |
| cps | 指每秒新建连接的极限值，新建连接一般是指tcp三次握手后处于的establish状态 |
| cc | 最大并发连接数 |
| 大象流 | 单条流即单核最大bps/pps |

用测试仪其实很容易测到以上指标，但是一是测试仪比较昂贵，二是需要直连测试仪到测试环境，没有这些条件，只能使用其他性能测试工具。目前比较常用且简单的测试工具有两个：pktgen和wrk。

* pktgen是一个基于linux内核的发包工具，它有一个升级版pktgen-dpdk，可以单机达到更高pps的压力，不过对于dpvs来说pktgen就足够了，一台mlx 25G网卡的客户端可以轻松发送16M的pps。pktgen可以调整发送udp包的大小，适用于测试单向pps、bps。pktgen还可以控制源端口的范围和目的端口的范围，也适用于测试cc和大象流。
* wrk是一个七层的qps性能测试工具，对于短链接来说，我们可以近似的把qps看作cps，所以可以使用wrk来测试cps。

上述两个工具很简单好用，但也有一些不足，比如pktgen没法测试双向pps，要测试双向pps，需要在rs上部署高性能的udp\_server；wrk性能不够，一般只能测试1核或者2核的dpvs性能。。。
  
[dperf](https://github.com/baidu/dperf/blob/main/README-CN.md)
是一个基于dpdk的高性能网络压测工具。dperf可用作client，也可用作server，所以可以测试双向pps；dperf的性能表现也比wrk强太多，所以我希望dperf可以解决上述工具的问题，并且可以独自测试dpvs所有的性能指标。

## 部署dperf

1. 编译dpdk
     
   由于dperf是基于dpdk的，所以首先要编译dpdk。编译dpdk又需要安装网卡驱动、配置大页内存等操作，可以参考我的以下操作：

```bash
# 由于我是mlx网卡，所以需要安装官方的ofed网卡驱动，需要注意的是cx6网卡需要安装ofed 5.4版本，
# 可以登陆这个页面自行下载安装包https://www.mellanox.com/page/mlnx_ofed_eula?mtag=linux_sw_drivers&mrequest=downloads&mtype=ofed&mver=MLNX_OFED-5.4-3.0.3.0&mname=MLNX_OFED_LINUX-5.4-3.0.3.0-debian9.13-x86_64.tgz
# 解压，安装ofed
tar xvf MLNX_OFED_LINUX-5.0-2.1.8.0-debian9.11-x86_64.tgz
cd MLNX_OFED_LINUX-5.0-2.1.8.0-debian9.11-x86_64/
./uninstall.sh --force
./mlnxofedinstall --dpdk --upstream-libs --skip-distro-check --add-kernel-support

# 配置大页内存
# 修改 grub 文件
vim /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT="quiet default_hugepagesz=1G hugepagesz=1G hugepages=49 iommu=pt intel_iommu=on"
# 生成启动脚本
update-grub
# 重启机器
reboot -f

# 下载 dpdk-19.11.10 安装包
# 解压后，修改 config/common_base，打开编译开关：CONFIG_RTE_LIBRTE_MLX5_PMD=y
# X86_64 环境下编译：
make install T=x86_64-native-linuxapp-gcc -j16

# 如需使用 kni 的话，加载 kni：
insmod x86_64-native-linuxapp-gcc/kmod/rte_kni.ko

```

2. 编译 dperf

```bash
git clone https://github.com/baidu/dperf
cd /root/dperf
make -j8 RTE_SDK=/root/dpdk-stable-19.11.10 RTE_TARGET=x86_64-native-linuxapp-gcc

```

之后就可以执行./build/dperf -c test/http/server-cps.conf 来启动 dperf server，在客户端上修改 test/http/client-cps.conf 中的配置后，执行./build/dperf -c test/http/client-cps.conf 即可启动 dperf client，就可以进行 cps 压测了，具体配置详情请见：https://github.com/baidu/dperf/blob/main/docs/configuration-CN.md

## 测试单向 pps

单向 pps 只需要起一个 udp client，配置如下：

```bash
mode client
cpu 0-15
duration 600s
cps 50k
cc 500k
rss l3l4
flood
protocol udp
packet_size 64
keepalive 100ms
port 0000:aa:00.0 10.10.10.10 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8050 50
lport_range 1000 65535
kni

```

pps 的数量等于并发数除以 keepalive 的值，所以上述配置的 pps 可以达到 5M。相对于 pktgen 来说，dperf 并发数启动较慢，即使配置一个较大的 cps 值如 500k，等待数秒之后才可以达到 50w 并发；pktgen 基本可以瞬间达到 50w 并发。但是 dperf 有一个很大的优势就是可以较为精确的发出的 pps 值，用做自动化测试调参会比较方便。

## 测试双向 pps

双向 pps 需要起两个 dperf，一个作为 udp client，一个作为 udp server，配置如下：

```bash
# client
mode client
cpu 0-15
duration 600s
cps 50k
cc 500k
rss l3l4
flood
protocol udp
packet_size 64
keepalive 100ms
port 0000:aa:00.0 10.10.10.10 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8050 50
lport_range 1000 65535
kni

# server
mode server
protocol udp

keepalive 100ms
payload_size 64

cpu 0-15
rss l3l4

duration 100m
port 0000:aa:00.0 10.10.10.11 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8050 50
kni

```

上述配置可以达到入向 5M+出向 5M pps 流量。

## 测试 bps

测试 bps 用 udp client 发入向的 udp 大包即可，配置如下：

```bash
mode client
cpu 0-15
duration 600s
cps 50k
cc 500k
rss l3l4
flood
protocol udp
packet_size 500
keepalive 100ms
port 0000:aa:00.0 10.10.10.10 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8050 50
lport_range 1000 65535
kni


```

这个和单向 pps 配置一样，只有包长不一样，通过包长和 pps 可以算出带宽，上述配置大概是 20G 左右

## 测试 cps

dperf 新建非常快，比 dpvs 释放连接的速度快很多，所以必须要配置非常多的四元组，才可以保证客户端的新建连接与 dpvs 上的 cps 一致。
  
需要注意的是 dperf 显示的 skOpen 并不是 dpvs 真实的 cps，建议把 dpvs 的 timeout 先调小，然后使用以下命令监控 dpvs 上的 cps。ipvsadm --stats 只统计 dpvs 的新建连接，不会统计复用的连接。

```bash
#!/bin/bash
while true
do
pre=$(/bin/ipvsadm -ln --stats | grep -E 'UDP|TCP' | awk '{sum += $3};END {print sum}')
	sleep 1
	post=$(/bin/ipvsadm -ln --stats | grep -E 'UDP|TCP' | awk '{sum += $3};END {print sum}')
	res=$((${post}-${pre}))
echo ${res}
done

```

dperf 配置参考如下：

```bash
mode client
cpu 0-15
duration 600s
cps 100k
rss l3l4
protocol tcp
port 0000:aa:00.0 10.10.10.10 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8000 99
lport_range 1000 65535
kni

```

如果只有一个 server ip，listen 监听端口一定要配多一点。

## 测试 cc

dperf 测试 cc 非常方便，配置足够多的四元组即可。

```bash
mode client
cpu 0-15
duration 600s
cps 500k
cc 5m
rss l3l4
flood
protocol udp
packet_size 64
keepalive 1s
port 0000:aa:00.0 10.10.10.10 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8000 99
lport_range 1000 65535
kni

```

上述配置可以轻松达到 5m 的并发数。

## 测试大象流

由于 dperf 配置中的 keepalive 最少只能配置 10us，也就是单条流最大 pps 只有 100k，而 dpvs 单核的 pps 性能约等于 1M，所以 pps 大象流用 dperf 是无法达到极限值的；通过调大 packet\_size 包长，最大是 1500 字节，所以单流 bps 最大 1.2Gb，也没有办法达到 dpvs 的单核 bps 极限值，所以 dperf 无法测试 dpvs 的大象流场景，配置参考如下：

```bash
mode client
cpu 0-15
duration 600s
cps 1
cc 1
rss l3l4
flood
protocol udp
packet_size 1500
keepalive 100ms
port 0000:aa:00.0 10.10.10.10 10.10.10.1
client 10.10.10.10 1
server 10.10.10.11 1
listen 8050 1
lport_range 1000 65535
kni

```

## 结论

总而言之，dperf 是一个非常好用的软件性能测试工具，可以很方便的测试出了大象流以外的其他场景，部署简单，配置简单，同时可以用做高性能的 client 和 server。也希望 dperf 后续能开发出更多的功能，适用更多的性能测试场景。