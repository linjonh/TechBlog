---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f68626b79626b7a772f:61727469636c652f64657461696c732f313436323639313730"
layout: post
title: "ollama-API-本地调用"
date: 2025-03-14 23:09:28 +08:00
description: "前提条件，ollama 已经启动了模型，查看 ollama 中的 model 名称。使用 openai 调用。"
keywords: "ollama API 本地调用"
categories: ['大模型平台', 'Llm']
tags: ['开发语言', 'Python']
artid: "146269170"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146269170
    alt: "ollama-API-本地调用"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146269170
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146269170
cover: https://bing.ee123.net/img/rand?artid=146269170
image: https://bing.ee123.net/img/rand?artid=146269170
img: https://bing.ee123.net/img/rand?artid=146269170
---

# ollama API 本地调用

### ollama API 本地调用

* 前提条件，ollama 已经启动了模型，查看 ollama 中的 model 名称

  ```python
  ollama list

  ```

  ![image-20250314230724978](https://i-blog.csdnimg.cn/img_convert/ae3a01cbb90d01d0a16007b07cd8e38c.png)
* 使用 openai 调用

  ```python
  from openai import OpenAI
  import openai

  openai.api_key = 'ollama'
  openai.base_url = 'http://localhost:11434/v1/'

  def get_completion(prompt, model="qwq"):
      client = OpenAI(api_key=openai.api_key,
                      base_url=openai.base_url
                      )
      messages = [{"role": "user", "content": prompt}]
      response = client.chat.completions.create(
          model=model,
          messages=messages,
          stream=False
      )
      return response.choices[0].message.content
    
  prompt = '你好，请介绍下你自己'
  model = "qwq"
  get_completion(prompt, model=model)

  ```

  ![image-20250314230516157](https://i-blog.csdnimg.cn/img_convert/6d59e646d8dfe07d7e7f0d4b6f8ec421.png)

  ### 可以使用 fastapi将后端服务暴露出来