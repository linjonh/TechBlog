---
layout: post
title: "深度学习-bert流程"
date: 2025-03-13 16:41:36 +0800
description: "在自然语言处理任务中，特别是使用预训练模型如BERT时，文本首先通过一个分词器（例如）转换为一系列的token IDs。这些ID是每个词或子词单元在词汇表（包含汉字、英文单词、标点符号）中的索引位置。如果输入句子是，经过分词器处理后，得到的token IDs可能是[1, 2]，这里1和2分别对应词汇表中的'hello'和'world'。"
keywords: "深度学习 bert流程"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Bert']
artid: "146233612"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146233612
    alt: "深度学习-bert流程"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146233612
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146233612
cover: https://bing.ee123.net/img/rand?artid=146233612
image: https://bing.ee123.net/img/rand?artid=146233612
img: https://bing.ee123.net/img/rand?artid=146233612
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     深度学习 bert流程
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="Token_IDs_0">
     </a>
     Token IDs
    </h2>
    <p>
     在自然语言处理任务中，特别是使用预训练模型如BERT时，文本首先通过一个分词器（例如
     <code>
      BertTokenizer
     </code>
     ）转换为一系列的token IDs。这些ID是每个词或子词单元在词汇表（包含汉字、英文单词、标点符号）中的索引位置。例如，假设有一个简化的词汇表如下：
    </p>
    <pre><code>{
 0: '[PAD]',
 1: 'hello',
 2: 'world',
 3: '[UNK]',
 ...
}
</code></pre>
    <p>
     如果输入句子是
     <code>
      "hello world"
     </code>
     ，经过分词器处理后，得到的token IDs可能是
     <code>
      [1, 2]
     </code>
     ，这里
     <code>
      1
     </code>
     和
     <code>
      2
     </code>
     分别对应词汇表中的
     <code>
      'hello'
     </code>
     和
     <code>
      'world'
     </code>
     。
    </p>
    <h4>
     <a id="BERT_14">
     </a>
     BERT中的应用
    </h4>
    <p>
     在BERT模型中，输入首先是被转换成token IDs的形式，然后通过嵌入层（Embedding Layer）将这些token IDs映射到一个高维（768维）的向量空间中。这个过程允许模型基于上下文学习更丰富的表示形式，而不是简单地依赖于稀疏的独热编码表示。因此，在您的代码中：
    </p>
    <pre><code class="prism language-python">input_text <span class="token operator">=</span> self<span class="token punctuation">.</span>bert_tokenizer<span class="token punctuation">(</span>data<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">)</span>
input_ids <span class="token operator">=</span> input_text<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
</code></pre>
    <p>
     这里的
     <code>
      input_ids
     </code>
     就是包含了一系列token IDs的张量，而不是独热编码的表示形式。BERT模型随后会使用这些token IDs来查找对应的词嵌入（word embeddings），作为其输入的一部分进行进一步的处理。这种方法不仅节省了内存和计算资源，还使得模型能够学习更加紧凑和有效的特征表示。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f68755f6d696e677765692f:61727469636c652f64657461696c732f313436323333363132" class_="artid" style="display:none">
 </p>
</div>


