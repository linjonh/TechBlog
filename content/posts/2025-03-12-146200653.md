---
layout: post
title: "本地部署DeepSeek-R1模型详细流程"
date: 2025-03-12 11:36:27 +0800
description: "本地部署就是自己部署DeepSeek-R1模型，使用本地的算力。"
keywords: "本地部署DeepSeek-R1模型详细流程"
categories: ['Ai']
tags: ['人工智能']
artid: "146200653"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146200653
    alt: "本地部署DeepSeek-R1模型详细流程"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146200653
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146200653
cover: https://bing.ee123.net/img/rand?artid=146200653
image: https://bing.ee123.net/img/rand?artid=146200653
img: https://bing.ee123.net/img/rand?artid=146200653
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     本地部署DeepSeek-R1模型详细流程
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="DeepSeekR1_0">
     </a>
     本地部署DeepSeek-R1模型详细流程
    </h2>
    <hr/>
    <h4>
     <a id="_4">
     </a>
     一、版本选择
    </h4>
    <p>
     本地部署就是自己部署DeepSeek-R1模型，使用本地的算力。
    </p>
    <p>
     <strong>
      主要瓶颈
     </strong>
     ：内存+显存的大小。
    </p>
    <p>
     <strong>
      特点
     </strong>
     ：此方案不用联网。
    </p>
    <p>
     <strong>
      适合
     </strong>
     ：有数据隐私方面担忧的或者保密单位根本就不能上网的。
    </p>
    <p>
     <strong>
      满血版
     </strong>
     ：DeepSeek R1671B 全量模型的文件体积高达 720GB，对于绝大部分人而言，本地资源有限，很难达到这个配置。
    </p>
    <p>
     <strong>
      蒸馏版
     </strong>
     ：
    </p>
    <ul>
     <li>
      开源2+6个模型。
     </li>
     <li>
      R1预览版和正式版的参数高达660B，非一般公司能用。
     </li>
     <li>
      为进一步平权，于是他们就蒸馏出了6个小模型，并开源给社区。
     </li>
     <li>
      最小的为1.5B参数，10G显存可跑。
     </li>
    </ul>
    <p>
     如果你要在个人电脑上部署，一般选择其他架构的蒸馏模型，本质是微调后的Llama或Qwen模型，基本32B以下，并不能完全发挥出DeepSeek R1的实力。
    </p>
    <hr/>
    <h4>
     <a id="_27">
     </a>
     二、部署过程
    </h4>
    <p>
     比较流行的是使用
     <a href="https://ollama.com/" rel="nofollow">
      ollama
     </a>
     。
    </p>
    <p>
     <strong>
      步骤1：下载Ollama
     </strong>
    </p>
    <p>
     <strong>
      步骤2：安装Ollama 傻瓜式安装
     </strong>
    </p>
    <ol>
     <li>
      <p>
       打开命令行（Windows用户可以通过
       <code>
        win+r
       </code>
       输入
       <code>
        cmd
       </code>
       ）。
      </p>
     </li>
     <li>
      <p>
       在命令行中输入以下命令来验证是否安装成功：
      </p>
      <pre><code class="prism language-sh">ollama <span class="token parameter variable">-v</span>
</code></pre>
     </li>
    </ol>
    <p>
     <strong>
      步骤3：选择r1模型
     </strong>
    </p>
    <p>
     <strong>
      步骤4：选择版本
     </strong>
    </p>
    <ul>
     <li>
      b代表10亿参数量，7b就是70亿参数量。
     </li>
     <li>
      这里的671B是 HuggingFace经过4-bit 标准量化的，所以大小是404GB。
     </li>
     <li>
      Ollama 支持 CPU 与 GPU 混合推理。将内存与显存之和大致视为系统的“总内存空间”。
     </li>
    </ul>
    <p>
     <strong>
      建议
     </strong>
     ：如果你想运行404GB的671B，建议你的内存+显存能达到500GB以上。
    </p>
    <p>
     <strong>
      步骤5：本地运行DeepSeek模型
     </strong>
    </p>
    <p>
     在命令行中，输入如下命令：
    </p>
    <pre><code class="prism language-sh">ollama run deepseek-r1:7b
</code></pre>
    <p>
     首次运行会下载对应模型文件。下载支持断点续传，如果下载中速度变慢，可以鼠标点击命令行窗口，然后
     <code>
      ctrl+c
     </code>
     取消，取消后按方向键“上”，可以找到上一条命令，即”ollama run deepseek-r1:7b“，按下回车会重新链接，按照之前进度接着下载。
    </p>
    <p>
     <strong>
      步骤6：查看已有模型
     </strong>
    </p>
    <p>
     查询已有模型：
    </p>
    <pre><code class="prism language-sh">ollama list
</code></pre>
    <p>
     后续要运行模型，仍然使用之前的命令：
    </p>
    <pre><code class="prism language-sh">ollama run deepseek-r1:7b
</code></pre>
    <hr/>
    <h4>
     <a id="_79">
     </a>
     三、使用客户端工具
    </h4>
    <p>
     本地部署好模型之后，在命令行操作还是不太方便，我们继续使用一些客户端工具来使用。
    </p>
    <p>
     <strong>
      Cherry Studio的下载
     </strong>
     ：
     <br/>
     <a href="https://cherry-ai.com/" rel="nofollow">
      Cherry Studio下载地址
     </a>
    </p>
    <p>
     <strong>
      以Cherry Studio为例访问7b的蒸馏模型
     </strong>
     ：
    </p>
    <ol>
     <li>
      下载并安装Cherry Studio。
     </li>
     <li>
      启动Cherry Studio，访问7b的蒸馏模型。
     </li>
     <li>
      如果列表没有r1模型，则是之前没有安装好。
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="models_94">
     </a>
     四、修改models文件夹路径（可选）
    </h4>
    <p>
     模型默认会下载到：
     <br/>
     <code>
      C:\Users\你的用户名\.ollama\目录下的 models文件夹
     </code>
    </p>
    <p>
     如果想修改模型的存放位置，做如下配置：
    </p>
    <ol>
     <li>
      <p>
       拷贝
       <code>
        models
       </code>
       文件夹到你指定的目录，比如我剪切到
       <code>
        E:\ollama
       </code>
       下。
      </p>
     </li>
     <li>
      <p>
       添加环境变量：
      </p>
      <ul>
       <li>
        <p>
         右键“我的电脑”，选择“属性”。
        </p>
       </li>
       <li>
        <p>
         按如下方式配置：
        </p>
        <pre><code class="prism language-plaintext">C:\Users\你的用户名\.ollama\models
</code></pre>
       </li>
      </ul>
     </li>
     <li>
      <p>
       重启Ollama客户端生效。
      </p>
     </li>
    </ol>
    <p>
     注意：修改完之后，需要重启Ollama客户端，右键图标，选择退出，重新运行Ollama。
     <br/>
     验证是否生效：重新运行Ollama之后，重新打开命令行，输入命令
     <code>
      ollama list
     </code>
     查看。
    </p>
    <hr/>
    <h4>
     <a id="_120">
     </a>
     五、其它方式：服务器部署
    </h4>
    <p>
     在企业中，想要私有化部署满血版DeepSeek-R1，即671B版本，需要有更好的硬件配置。
    </p>
    <p>
     <strong>
      服务器可以是物理机，也可以是云服务器。
     </strong>
    </p>
    <p>
     使用Ollama提供的经过量化压缩的671B模型的大小是404GB，建议内存+显存≥500 GB，举例几种性价比配置如下：
    </p>
    <ul>
     <li>
      Mac Studio：配备大容量高带宽的统一内存（比如 X 上的 @awnihannun 使用了两台192 GB 内存的 Mac Studio 运行3-bit 量化的版本）
     </li>
     <li>
      高内存带宽的服务器：比如 HuggingFace 上的 alain401使用了配备了24×16 GB DDR54800内存的服务器）
     </li>
     <li>
      云 GPU 服务器：配备2张或更多的80GB 显存 GPU（如英伟达的 H100，租赁价格约2美元/小时/卡）
     </li>
    </ul>
    <p>
     在这些硬件上的运行速度可达到10+ token /秒。
    </p>
    <p>
     <strong>
      部署流程与个人电脑部署7B的流程没有太大区别，都是以下几个步骤：
     </strong>
    </p>
    <ol>
     <li>
      根据服务器的操作系统，下载对应版本的Ollama客户端；
     </li>
     <li>
      运行Ollama，执行
      <code>
       Ollama
      </code>
      命令运行671B版本模型；首次执行自动下载模型；
     </li>
     <li>
      使用客户端工具/自己开发页面/代码调用，对接Ollama的R1模型；
     </li>
    </ol>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f33373630303339372f:61727469636c652f64657461696c732f313436323030363533" class_="artid" style="display:none">
 </p>
</div>


