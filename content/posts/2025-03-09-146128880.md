---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f79756b616930383030382f:61727469636c652f64657461696c732f313436313238383830"
layout: post
title: "最后203篇系列011-Mongo异步代理开发回顾"
date: 2025-03-09 17:43:34 +08:00
description: "出于我自身的使用特性，我较早就确定了以Mongo为主库，然后其他业务形态根据实际场景进行细分的框架。最初的时候做了一版同步版的Mongo Agent，用于代理数据库操作，后来发现在大量使用的时候数据库会承受很大压力，浪费CPU。这大概是与我最初是做数据分析、建模有关系的，在这种场景下的确是批量操作大量数据；而在应用的场景下，往往是大批量的小数据操作-- 这会占据太多数据库连接不释放，网络情况越糟，这种情况越严重。也就是说在分布式的场景下，Mongo Agent不适用，这是由于同步和异步的基本特性产生的。"
keywords: "【最后203篇系列】011 Mongo异步代理开发回顾"
categories: ['未分类']
tags: ['数据库', 'Oracle']
artid: "146128880"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146128880
    alt: "最后203篇系列011-Mongo异步代理开发回顾"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146128880
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146128880
cover: https://bing.ee123.net/img/rand?artid=146128880
image: https://bing.ee123.net/img/rand?artid=146128880
img: https://bing.ee123.net/img/rand?artid=146128880
---

# 【最后203篇系列】011 Mongo异步代理开发回顾

## 说明

出于我自身的使用特性，我较早就确定了以Mongo为主库，然后其他业务形态根据实际场景进行细分的框架。

最初的时候做了一版同步版的Mongo Agent，用于代理数据库操作，后来发现在大量使用的时候数据库会承受很大压力，浪费CPU。这大概是与我最初是做数据分析、建模有关系的，在这种场景下的确是批量操作大量数据；而在应用的场景下，往往是大批量的小数据操作-- 这会占据太多数据库连接不释放，网络情况越糟，这种情况越严重。

也就是说在分布式的场景下，Mongo Agent不适用，这是由于同步和异步的基本特性产生的。所以我又决定做一个异步的数据库代理 ，目前的状态是做了一个基础版，能用，但是还有些问题没有厘清。然后由于有其他工作上的任务压力，没有时间细细研究，所以在这里做一个系统性的回顾，然后一边使用，一边做针对性的实验，过一阵子再完善。

## 内容

### Q1: 为什么要做数据库agent?

不使用pymongo，而是采用代理 ，有几方面原因。

首先，因为数据库太多了，每种数据库的语法都不一样，对接很麻烦。 mongo的操作语法和sql类的完全不同，还有neo4j, clickhouse等很多数据库。数据库作为基础支撑，不应该这么麻烦，所以需要有agent来将同类型的操作进行规整，接口化，这样才能保持多种数据库并并存和高效使用。

其次，agent可以做到环境隔离与智能优化的功能。原则上，所有的客户端，都可以很快发起操作，只需要httpx请求即可，这个是很容易做的，这是环境隔离的一部分。另外，agent除了帮助用户完成数据库操作，还能够做权限统计，限制，以及统计，以进行后续优化。特别是接口是参数化的，这个更容易提取规律。（相比原生的语句和工具）

此外，agent在同步状态下，将线程限制为1，还可以扮演task manager的角色，保证任务数据的获取按顺序，不重不漏。

当然，用了异步的agent,则可以最大化减轻数据库的cpu占用，应对网络延时造成的cpu浪费。

### Q2：为什么需要维持多种数据库？

我自己的一个观察结论：数据库都是为了应对特定的问题而研发的，这些问题不存在统一解决的可能，所以有多种数据库。

不同数据库在擅长的领域与其他数据库相比，可能有1到5个量级的水平提升，差距太大了，所以必须要维持多种数据库。

比如，表格类是一种常见的数据格式，clickhouse在存储空间、统计上有巨大的优势(几乎相当于内存计算了）。

文档类又是另一种格式，频繁的变换字段是结构化库很难应对的，像mongo就无所谓这些。还有层层叠叠的json结构，这种深度的数据结构是表格化库不擅长的。当然，mongo更加全能一些，性能也够用。比如，当我们临时固定一些扁平化字段，mongo看起来就像是一个表格化库；mongo甚至也搞了时间序列数据库(mongo8特别优化了这一块），那么又可以像influxdb那样快速存储和分析时间序列。甚至在某些方面更加灵活。当然，在性能上可能比influxdb低1-3个量级。mongo还有capped colletion，很适合存日志，可以自动剪断，这个又很像kafka。另外mongo的副本集搭建起来是最轻松的，所以分布式也不错。对于我来说，mongo的能力对我来已经足够，所以这是选择它作为主库的原因。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9cdaf3a3d8464bb18a369716b3f7c3ca.png)
  
类似地，有特定领域，且性能差异巨大的应用还有图库(neo4j)、kv存储(redis)、向量数据库(milvus)等。

### 当前的任务

上面的两个问题讨论了mongo agent存在的必要性以及未来发展潜力。回到当前的点，async mongo agent。

这个问题最初引起我关注的是我的计算机(5950x)上cpu资源耗用比较高，大约在50%左右，虽然有不少分钟级的周期性任务会与mongo交互，但是交互的数据量都很少，没有很繁重的计算，所以这么高的cpu占比是不对的。所以问题应该在于交互的方式：同步的mongo agent是有比较大的问题的，100个程序要交互，就使用了100个连接。理论上，这样的交互，使用一个异步连接可能就够了。如果我按计划展开后续的任务，那么很可能这个cpu就会耗费在无意义的IO等待上，所以才决定要做异步的MongoAgent。

我的计划是使用FastAPI + Pydantic + AsyncIOMotorClient(motor)来打造一个异步的agent，这样可以在达到节约cpu资源的同时，对数据的格式和规范进行限制。

就目前的实施进展开来，现在已经达到了部分目的。比如大量的程序都使用Mongo作为程序的持久缓存(Persistent Cache),服务器的cpu好用(m7)已经保持在一个极低的水平(~5%)。当然之前许多旧版本的任务已经拆除，这个可以等进一步的任务上量再观察。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d359be665c3b49c780b96dc102d026b7.png)

> 另外最近我也在看企业级服务器(霄龙处理器)，其实高核心数和内存还是比较吸引人的，不过就我的小项目而言，9950x可能是更好的选择：核心数不低，主频非常高，可挂载的内存也够(192G),PCIE5的固态速度也不错10GB+，扩展性也可以了，5个m2,还有pcie插槽可继续扩展固态，唯一美中不足的就是散热，我还是不太相信水冷。简而言之，消费类的顶配主机在小规模计算上更快，存储也可以堆到20T固态以上，成本大约只有企业级服务器的一半。有4台主机，构成的集群，稳定性就完全没有问题了，所以还是应当考虑9950x选项。

所以，应对【大量零散的需求】，mongo通过异步方式解决。这也算是另一种redis的替代方案，redis可以用于更高频的kv缓存，不需要考虑持久化。

然后说问题：

* 1 低效处理。
* 2 格式不兼容。

当前版本在进行格式的转换时，用了一些很蠢的规范方法，效率肯定是低的，比如for循环。

```python
        for item in ma_stand_s.lod:
            if item.meta.pid in insert_pids:
                # 插入数据
                insert_data.append({
                    **filter_dict_by_keys(item.meta.dict(exclude_none=True) ,['pid','create_time','update_time']) ,   # 包含 pid、create_time、update_time、data_time
                    **item.data 
                })

```

低效处理。我这次迭代时看到这个都愣了一会，后来想起来一方面考虑的场景就是零散需求，所以这点开销也还好；另一方面我也做了测试，这种情况仍然能保证每秒万条左右的IO，性能上也过得去。但这个肯定是后面要迭代掉的，当时的限制是 mongo的并发操作格式上有要求，然后另一边pydantic也有自己的要求，我没匹配好。

格式不兼容。这块是我在引入mongo的时间序列代理时发生的。原来设计的meta和后来引入时间序列的metadata不太兼容，在自动对时间字段进行转换时存在效率上的浪费，对于有点强迫症的我来说，感觉真的很不能接受。这个要修好就要重构了，所以只能放在下一版进行。

> mongo的时间格式转换不能自动的，要转成datetime格式，然而这个格式也不能直接json传输。所以需要在服务端进行处理。

所以下一版需要：

* 重新设计数据结构，关于元数据的结构，以及自动填充转换的部分。

这版为了使用时间序列，强行对服务做了调整，在不影响现有服务的情况下，增加了部分时间序列的支持。比如insert 和 insert if not exists。然后发生了一个很搞笑的事：即使在我不管效率的情况下转换时间，但是在update类的接口上对时间序列报了错
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1c41b4fd7fe94f4c93a67a87dd5b0f22.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/18cbc4a7992d4593aaa4515e41223aaa.png)
  
数据库的错误信息的确像deepseek说的那样，说是因为没有进行元数据字段的筛选而不能进行，但是我在客户端（Studio3T)直接执行删除全部的命令又是可以的，所以有点奇怪。

这部分就需要后续我先在ipython之类的终端再进行多一些的实验，才确定这件事。
`【实验时间序列的更改和删除】`

然后这种感觉就和之前用influxdb有点像了，时间序列确实有点点特殊。但是按照line protocal的方式整理数据，我相信是对的。

这次的改动，最后在时间序列方面：

* 1 增加了集合的创建接口。特别是对于统计粒度和过期时间的限制。
* 2 增加了数据的插入 insert。插入相对比较简单，在修改了pydantic对象后，可以进行批量的时间转换，速度还是比较快的。
* 3 增加了数据的不存在插入 insert if not exists。这个是之前希望的一种基础模式，在时间序列上会特别有用。例如我们获取时间序列接口，然后会有部分重叠的（重复的），我们可以跳过重复，只插入新增的，这样对于数据的压力会更小，我们也能知道数据是否有真正的写入。另外，还可以避免某些程序错误产生的全量数据插入，这种错误可能直接撑爆磁盘。
* 4 增加了统计。在过去，我一般不在mongo里统计，很多时候是直接获取全部相关数据统计，或者写入clickhouse统计。但现在有了时间序列集合，很多时间相关统计在mongo里做反而更好。

所以本次改动带来的实际变化是：可以用mongo时间序列在承载一些新的需求，但控制上还达不到那么灵活。

这种插入会先判断主键，也就是比insert的确要多一步查询，这是额外开销。要存入时则要进行时间转换(string to datetime)。

总的看起来，速度还行，换成成每秒的话大约在8000左右，反正目前应该够了。不行可以加workers，
`在逻辑复杂性和效率上，有时候不得不做一些平衡。`
我的基本理念是，只要不是存在量级差，那么就ok,让硬件的升级来carry这一切。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dfb251013dab4620994c80a5c2f0bbb6.png)
  
再次执行插入时，不会重复插入记录，但是时间开销是有的。所以，目前更合适小批量插入的情况。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b25fe8d6b258475f98f1a2aab7dbadd8.png)
  
然后就是汇聚了,如果说插入数据的时候比较让人失望，在汇聚的时候就比较惊喜了（6ms)。所以时间序列或多或少像是
`为汇聚服务`
的，这个倒是不错。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/414e91c1b26a49618ef0fa107b5ef201.png)

当然，直接使用微服务也不是一个很好的体验，所以还需要包一层对象。在脚本中使用是类似这样的：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e2f9e5f4b24640b09c903211c8cc10d6.png)
  
这个对象的形式和原理很像SQLAlchemy，所以我就把这个对象叫做AMalchmey(Async Mongo Alchemy)。当然，不同于SQLAlchemy, 我的对象内部嵌入了更友好的数据库连接方式，而且本身不需要在本地维持连接池，不会出现类似SQLAlchmey pool过期的问题。

当然，最重要的是在微服务和使用者之间又架了一个桥梁。这样从数据- Agent - AMalchemy - User就完全连起来了，在使用者的感知上就是一个函数。

我估算了一下，如果要再重构一次 Async Mongo Agent 大约会花一整周的时间，我短期内很难有这样的空闲。但是现在功能可以使用，我也不是很着急，所以可以合理的把这个开发时间拉长：

* 1 phase 1 功能再验证：对于需要验证的单点功能，比如时间序列的删改问题，先进行离线测试，用本地包的方式看怎么做合适
* 2 phase 2 数据对象再设计：这里一个比较核心的问题是：数据应该是怎样的结构？哪些是操作元数据，哪些是分析元数据，哪些是数据？以及数据应该有哪些标量类型？ 比如操作元数据，除了现有的create_time 和update_time，本身还有crc_block(循环冗余分块码)
* 3 phase 3 批量处理优化： 虽然异步服务主要面向大量零散的需求，但是我还是希望看看能否在批量处理上有一个更理想的方式。否则对于批量处理，就还需要另一个同步服务了。(在块处理时，同步服务是可行的,甚至比异步的效率更高一些)
* 4 phase 4 服务组装测试：在以上都ready了之后，进行功能的重新组装和测试。

这样phase1 可以边做边想，把重要的功能点都列出来，在1个月左右完成都行；phase2 应该同步重叠的零星思考，然后在phase1结束后2周内完成。phase3 看缘分，2周能搞定就搞定，实在不行就算了。然后phase4应该抽两个周末，集中调试完成。

大约2个月之后，会再进行一次组装和测试，那时候应该就不错了。