---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f35383730313939352f:61727469636c652f64657461696c732f313436303531313236"
layout: post
title: "论文阅读多模态PointCLIP"
date: 2025-03-05 19:58:30 +08:00
description: "本文提出PointCLIP，将CLIP的2D视觉-语言预训练能力迁移至3D点云理解。通过多视角投影将点云转化为伪2D图像，结合可学习的视图间适配器进行特征融合，在ModelNet40数据集上仅用10%数据实现87.2%的分类准确率，逼近全监督方法。实验表明，模型融合策略能利用2D/3D特征互补性提升性能，但Zero-Shot效果仍有局限。该工作为低资源3D识别提供了新思路，验证了跨模态预训练模型在三维领域的扩展潜力。"
keywords: "【论文阅读】多模态——PointCLIP"
categories: ['论文阅读']
tags: ['计算机视觉', '深度学习', '机器学习']
artid: "146051126"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146051126
    alt: "论文阅读多模态PointCLIP"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146051126
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146051126
cover: https://bing.ee123.net/img/rand?artid=146051126
image: https://bing.ee123.net/img/rand?artid=146051126
img: https://bing.ee123.net/img/rand?artid=146051126
---

# 【论文阅读】多模态——PointCLIP

## 文献基本信息

* **标题：**
  PointCLIP: Point Cloud Understanding by CLIP
* **作者：**
  Renrui Zhang、Ziyu Guo、Wei Zhang、Kunchang Li、Xupeng Miao、Bin Cui、Yu Qiao、Peng Gao、Hongsheng Li
* **单位：**
  上海人工智能实验室、北京大学、香港中文大学
* **会议/期刊：**
  CVPR
* **发表时间：**
  2021年12月4日
* **代码：**
  [https://github.com/ZrrSkywalker/PointCLIP](https://github.com/ZrrSkywalker/PointCLIP "https://github.com/ZrrSkywalker/PointCLIP")

## 背景与意义

* 最近，通过
  **对比视觉-语言预训练（CLIP）**
  进行的
  **zero-shot**
  和
  **few-shot**
  学习在
  **2D视觉识别**
  方面表现出鼓舞人心的表现，该方法学习在开放词汇设置中将图像与其对应的文本进行匹配。
* 然而，通过2D中的大规模图像-文本对预训练的CLIP是否可以
  **推广到3D识别**
  ，仍有待探索。

## 研究方法与创新点&研究结论

### 回顾CLIP

![](https://i-blog.csdnimg.cn/direct/180bd72469eb4ff1a7b4782b354c4403.png)

* **CLIP**
  的全称是
  **Constrative Vison-Language Pre-training**
  ，如上图所示，使用图像和文本对训练，用了
  **对比学习**
  的方式。
* 首先是对
  ![$n$](https://latex.csdn.net/eq?%24n%24)
  对图像文本分别提特征，计算特征
  **余弦相似性**
  ，构成一个
  ![$n \times n$](https://latex.csdn.net/eq?%24n%20%5Ctimes%20n%24)
  的矩阵，分别计算
  **图像的分类损失**
  和
  **文本的分类损失**
  。
* 分类的时候，以ImageNet为例，构造句子例如
  **“A photo of a {}”**
  ，把ImageNet中的类别，以
  **填空**
  的形式填入句子，然后计算每个句子和图像的相似度，找出最高的为最终的类别。

### 基于CLIP的点云理解

![](https://i-blog.csdnimg.cn/direct/5d52170bf57d417b8dfbbabb40f71780.png)

* **PointCLIP**
  的框架和CLIP非常像，核心修改在于
  **如何用视觉编码器给点云图像提取特征**
  。

#### 点云特征抽取

* 采用了
  **投影**
  的方式，把3维的点，朝几个平面投影，变成2维的图像。
* 点云的坐标可以表示为
  ![$\left( {x,y,z} \right)$](https://latex.csdn.net/eq?%24%5Cleft%28%20%7Bx%2Cy%2Cz%7D%20%5Cright%29%24)
  , 对
  ![$z$](https://latex.csdn.net/eq?%24z%24)
  方向做
  **透视投影（perceptive project）**
  可以把这个点变换为
  ![$\left( {\lceil {x/z} \rceil],\lceil {y/z} \rceil} \right)$](https://latex.csdn.net/eq?%24%5Cleft%28%20%7B%5Clceil%20%7Bx/z%7D%20%5Crceil%5D%2C%5Clceil%20%7By/z%7D%20%5Crceil%7D%20%5Cright%29%24)
  ,这种投影的好处是可以让图片比较接近于
  **自然图像**
  。
* 然后把投影得到的图像复制两次，变成
  **三通道图像**
  ，这样
  **CLIP预训练得到的知识**
  就可以应用在点云上了。

#### zero-shot分类

* 对象做
  ![$M$](https://latex.csdn.net/eq?%24M%24)
  个视角的投影，通过诗句编码器抽取特征
  ![${f_i}$](https://latex.csdn.net/eq?%24%7Bf_i%7D%24)
* 但是这种方式的结果和
  **有监督**
  相差甚远，毕竟点云投影和真实图像还是有一些差距的，在ModelNet40数据集上的准确率只有20.18%，
  **基本不太可用**
  。

#### 视图间adapter

![](https://i-blog.csdnimg.cn/direct/66e4abc9d1284d88bfa6b620afc7ee7c.png)

* **zero-shot**
  的方式虽然有一定的效果，但是
  **和有监督方法比起来差太多**
  ，于是考虑加个小网络，进行
  **few-shot微调**
  。
* 用一个小网络作为
  **adapter**
  ，结构如上图所示。
* 首先把多视角的特征
  **concat**
  成一维，通过两个
  **全连接**
  得到全局特征
  ![${f_{​{\rm{global}}}}$](https://latex.csdn.net/eq?%24%7Bf_%7B%7B%5Crm%7Bglobal%7D%7D%7D%7D%24)
  。
* 然后如下式，每个全局特征乘一个矩阵，再和原始的
  ![$f_i$](https://latex.csdn.net/eq?%24f_i%24)
  做一个残差连接，
  **训练的时候把其余部分固定住，只训练这个adapter**
  ，做一个few-shot学习，就得到了最终的adapted特征，之后用这个特征代替
  ![$f_i$](https://latex.csdn.net/eq?%24f_i%24)
  去和文本特征算相似度。

![f_i^a = {f_i} + {\rm{RELU}}\left( {​{f_{​{\rm{global}}}}W_{3i}^t} \right)](https://latex.csdn.net/eq?f_i%5Ea%20%3D%20%7Bf_i%7D%20&plus;%20%7B%5Crm%7BRELU%7D%7D%5Cleft%28%20%7B%7Bf_%7B%7B%5Crm%7Bglobal%7D%7D%7D%7DW_%7B3i%7D%5Et%7D%20%5Cright%29)

* 靠着视图间adapter，在ModelNet40上的结果从20.18%提升到了87.20%，
  **基本达到了有监督方法的效果，并且只用了全部数据的1/10**
  。

![](https://i-blog.csdnimg.cn/direct/a1eae87da8b44882b2fb672214162b02.png)

### 模型融合

![](https://i-blog.csdnimg.cn/direct/17677d0cc9ac4f6daeec4288d7183e7b.png)

* 大概是因为few-shot的效果，还是比PointNet差一点，本文考虑是否可以用
  **模型融合**
  的方式，得到更好的模型。
* 融合方式比较简单，
  **把不同模型预测的各类别的分数加起来得到最终的分数**
  。
* 实验结果表明，
  **用PointCLIP和别的模型融合，结果有所提升，可以得到新的SOTA，用传统有监督方法得到的结果做融合，却出现了下降**
  。
* 这大概是因为用
  **few-shot**
  的方式，
  **学到的知识和有监督学到的差异比较大**
  ，所以做模型融合才会有提升。
* 具体结果如下图所示。

![](https://i-blog.csdnimg.cn/direct/5825e1aeae7249ee9f5dcb69a0436ba1.png)

![](https://i-blog.csdnimg.cn/direct/3405bdf2bdda47d6a4e173052f449b60.png)

![](https://i-blog.csdnimg.cn/direct/825b861dde6d402898a5d66458b83d56.png)

## 存在的问题

1. **zero-shot的效果低**
   ，准确度仅30%（ModelNet10）、20%（ModelNet40）和15%（ScanObejctNN），能否将zero-shot的效果继续提升？有可能
   **3D点云与预训练好的2D编码器并不是一个好结合**
   。
2. 模型融合之后一般效果都有所提升，但是本文通过实验说明并不是两两模型融合都有提升，而是和PointCLIP融合之后能提升，说明
   **PointCLIP能够用2D信息与3D模型互补**
   ，这点存疑。
3. 本文利用CLIP只做了3D分类任务，
   **其它任务还未探索**
   。

## 启发与思考

1. 本文是CLIP的又一后续应用，
   **从2D问题拓展到了3D问题**
   ，说明这是对现有模型改进工作的思路之一。
2. 提供了在
   **低资源成本和数据机制**
   下利用CLIP的有效方案。
3. **模型融合**
   可能是一个实用且便捷的提升性能的方法，有助于弥补模型的内在缺陷。