---
layout: post
title: "论文阅读多模态PointCLIP"
date: 2025-03-05 19:58:30 +0800
description: "本文提出PointCLIP，将CLIP的2D视觉-语言预训练能力迁移至3D点云理解。通过多视角投影将点云转化为伪2D图像，结合可学习的视图间适配器进行特征融合，在ModelNet40数据集上仅用10%数据实现87.2%的分类准确率，逼近全监督方法。实验表明，模型融合策略能利用2D/3D特征互补性提升性能，但Zero-Shot效果仍有局限。该工作为低资源3D识别提供了新思路，验证了跨模态预训练模型在三维领域的扩展潜力。"
keywords: "【论文阅读】多模态——PointCLIP"
categories: ['论文阅读']
tags: ['计算机视觉', '深度学习', '机器学习']
artid: "146051126"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146051126
    alt: "论文阅读多模态PointCLIP"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146051126
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146051126
cover: https://bing.ee123.net/img/rand?artid=146051126
image: https://bing.ee123.net/img/rand?artid=146051126
img: https://bing.ee123.net/img/rand?artid=146051126
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【论文阅读】多模态——PointCLIP
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     文献基本信息
    </h2>
    <ul>
     <li>
      <strong>
       标题：
      </strong>
      PointCLIP: Point Cloud Understanding by CLIP
     </li>
     <li>
      <strong>
       作者：
      </strong>
      Renrui Zhang、Ziyu Guo、Wei Zhang、Kunchang Li、Xupeng Miao、Bin Cui、Yu Qiao、Peng Gao、Hongsheng Li
     </li>
     <li>
      <strong>
       单位：
      </strong>
      上海人工智能实验室、北京大学、香港中文大学
     </li>
     <li>
      <strong>
       会议/期刊：
      </strong>
      CVPR
     </li>
     <li>
      <strong>
       发表时间：
      </strong>
      2021年12月4日
     </li>
     <li>
      <strong>
       代码：
      </strong>
      <a class="link-info" href="https://github.com/ZrrSkywalker/PointCLIP" title="https://github.com/ZrrSkywalker/PointCLIP">
       https://github.com/ZrrSkywalker/PointCLIP
      </a>
     </li>
    </ul>
    <h2>
     背景与意义
    </h2>
    <ul>
     <li>
      最近，通过
      <strong>
       对比视觉-语言预训练（CLIP）
      </strong>
      进行的
      <strong>
       zero-shot
      </strong>
      和
      <strong>
       few-shot
      </strong>
      学习在
      <strong>
       2D视觉识别
      </strong>
      方面表现出鼓舞人心的表现，该方法学习在开放词汇设置中将图像与其对应的文本进行匹配。
     </li>
     <li>
      然而，通过2D中的大规模图像-文本对预训练的CLIP是否可以
      <strong>
       推广到3D识别
      </strong>
      ，仍有待探索。
     </li>
    </ul>
    <h2>
     研究方法与创新点&amp;研究结论
    </h2>
    <h3>
     回顾CLIP
    </h3>
    <p style="text-align:center">
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/180bd72469eb4ff1a7b4782b354c4403.png" width="843"/>
    </p>
    <ul>
     <li>
      <strong>
       CLIP
      </strong>
      的全称是
      <strong>
       Constrative Vison-Language Pre-training
      </strong>
      ，如上图所示，使用图像和文本对训练，用了
      <strong>
       对比学习
      </strong>
      的方式。
     </li>
     <li>
      首先是对
      <img alt="$n$" class="mathcode" src="https://latex.csdn.net/eq?%24n%24">
       对图像文本分别提特征，计算特征
       <strong>
        余弦相似性
       </strong>
       ，构成一个
       <img alt="$n \times n$" class="mathcode" src="https://latex.csdn.net/eq?%24n%20%5Ctimes%20n%24">
        的矩阵，分别计算
        <strong>
         图像的分类损失
        </strong>
        和
        <strong>
         文本的分类损失
        </strong>
        。
       </img>
      </img>
     </li>
     <li>
      分类的时候，以ImageNet为例，构造句子例如
      <strong>
       “A photo of a {}”
      </strong>
      ，把ImageNet中的类别，以
      <strong>
       填空
      </strong>
      的形式填入句子，然后计算每个句子和图像的相似度，找出最高的为最终的类别。
     </li>
    </ul>
    <h3>
     基于CLIP的点云理解
    </h3>
    <p style="text-align:center">
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/5d52170bf57d417b8dfbbabb40f71780.png" width="745"/>
    </p>
    <ul>
     <li>
      <strong>
       PointCLIP
      </strong>
      的框架和CLIP非常像，核心修改在于
      <strong>
       如何用视觉编码器给点云图像提取特征
      </strong>
      。
     </li>
    </ul>
    <h4>
     点云特征抽取
    </h4>
    <ul>
     <li>
      采用了
      <strong>
       投影
      </strong>
      的方式，把3维的点，朝几个平面投影，变成2维的图像。
     </li>
     <li>
      点云的坐标可以表示为
      <img alt="$\left( {x,y,z} \right)$" class="mathcode" src="https://latex.csdn.net/eq?%24%5Cleft%28%20%7Bx%2Cy%2Cz%7D%20%5Cright%29%24">
       , 对
       <img alt="$z$" class="mathcode" src="https://latex.csdn.net/eq?%24z%24">
        方向做
        <strong>
         透视投影（perceptive project）
        </strong>
        可以把这个点变换为
        <img alt="$\left( {\lceil {x/z} \rceil],\lceil {y/z} \rceil} \right)$" class="mathcode" src="https://latex.csdn.net/eq?%24%5Cleft%28%20%7B%5Clceil%20%7Bx/z%7D%20%5Crceil%5D%2C%5Clceil%20%7By/z%7D%20%5Crceil%7D%20%5Cright%29%24">
         ,这种投影的好处是可以让图片比较接近于
         <strong>
          自然图像
         </strong>
         。
        </img>
       </img>
      </img>
     </li>
     <li>
      然后把投影得到的图像复制两次，变成
      <strong>
       三通道图像
      </strong>
      ，这样
      <strong>
       CLIP预训练得到的知识
      </strong>
      就可以应用在点云上了。
     </li>
    </ul>
    <h4>
     zero-shot分类
    </h4>
    <ul>
     <li>
      对象做
      <img alt="$M$" class="mathcode" src="https://latex.csdn.net/eq?%24M%24">
       个视角的投影，通过诗句编码器抽取特征
       <img alt="${f_i}$" class="mathcode" src="https://latex.csdn.net/eq?%24%7Bf_i%7D%24"/>
       ，通过预设的类别和模板“point cloud depth map of a [CLASS].”，抽取文本特征
       <img alt="${W_t} \in {\mathbb{R}^{K \times C}}$" class="mathcode" src="https://latex.csdn.net/eq?%24%7BW_t%7D%20%5Cin%20%7B%5Cmathbb%7BR%7D%5E%7BK%20%5Ctimes%20C%7D%7D%24"/>
       ，再对每个视角的特征分别计算分数，然后加权得到最终的分数。
      </img>
     </li>
     <li>
      但是这种方式的结果和
      <strong>
       有监督
      </strong>
      相差甚远，毕竟点云投影和真实图像还是有一些差距的，在ModelNet40数据集上的准确率只有20.18%，
      <strong>
       基本不太可用
      </strong>
      。
     </li>
    </ul>
    <h4>
     视图间adapter
    </h4>
    <p style="text-align:center">
     <img alt="" height="250" src="https://i-blog.csdnimg.cn/direct/66e4abc9d1284d88bfa6b620afc7ee7c.png" width="561"/>
    </p>
    <ul>
     <li>
      <strong>
       zero-shot
      </strong>
      的方式虽然有一定的效果，但是
      <strong>
       和有监督方法比起来差太多
      </strong>
      ，于是考虑加个小网络，进行
      <strong>
       few-shot微调
      </strong>
      。
     </li>
     <li>
      用一个小网络作为
      <strong>
       adapter
      </strong>
      ，结构如上图所示。
     </li>
     <li>
      首先把多视角的特征
      <strong>
       concat
      </strong>
      成一维，通过两个
      <strong>
       全连接
      </strong>
      得到全局特征
      <img alt="${f_{​{\rm{global}}}}$" class="mathcode" src="https://latex.csdn.net/eq?%24%7Bf_%7B%7B%5Crm%7Bglobal%7D%7D%7D%7D%24"/>
      。
     </li>
     <li>
      然后如下式，每个全局特征乘一个矩阵，再和原始的
      <img alt="$f_i$" class="mathcode" src="https://latex.csdn.net/eq?%24f_i%24"/>
      做一个残差连接，
      <strong>
       训练的时候把其余部分固定住，只训练这个adapter
      </strong>
      ，做一个few-shot学习，就得到了最终的adapted特征，之后用这个特征代替
      <img alt="$f_i$" class="mathcode" src="https://latex.csdn.net/eq?%24f_i%24"/>
      去和文本特征算相似度。
     </li>
    </ul>
    <p style="text-align:center">
     <img alt="f_i^a = {f_i} + {\rm{RELU}}\left( {​{f_{​{\rm{global}}}}W_{3i}^t} \right)" class="mathcode" src="https://latex.csdn.net/eq?f_i%5Ea%20%3D%20%7Bf_i%7D%20&amp;plus;%20%7B%5Crm%7BRELU%7D%7D%5Cleft%28%20%7B%7Bf_%7B%7B%5Crm%7Bglobal%7D%7D%7D%7DW_%7B3i%7D%5Et%7D%20%5Cright%29"/>
    </p>
    <ul>
     <li>
      靠着视图间adapter，在ModelNet40上的结果从20.18%提升到了87.20%，
      <strong>
       基本达到了有监督方法的效果，并且只用了全部数据的1/10
      </strong>
      。
     </li>
    </ul>
    <p style="text-align:center">
     <img alt="" height="200" src="https://i-blog.csdnimg.cn/direct/a1eae87da8b44882b2fb672214162b02.png" width="713"/>
    </p>
    <h3>
     模型融合
    </h3>
    <p style="text-align:center">
     <img alt="" height="250" src="https://i-blog.csdnimg.cn/direct/17677d0cc9ac4f6daeec4288d7183e7b.png" width="323"/>
    </p>
    <ul>
     <li>
      大概是因为few-shot的效果，还是比PointNet差一点，本文考虑是否可以用
      <strong>
       模型融合
      </strong>
      的方式，得到更好的模型。
     </li>
     <li>
      融合方式比较简单，
      <strong>
       把不同模型预测的各类别的分数加起来得到最终的分数
      </strong>
      。
     </li>
     <li>
      实验结果表明，
      <strong>
       用PointCLIP和别的模型融合，结果有所提升，可以得到新的SOTA，用传统有监督方法得到的结果做融合，却出现了下降
      </strong>
      。
     </li>
     <li>
      这大概是因为用
      <strong>
       few-shot
      </strong>
      的方式，
      <strong>
       学到的知识和有监督学到的差异比较大
      </strong>
      ，所以做模型融合才会有提升。
     </li>
     <li>
      具体结果如下图所示。
     </li>
    </ul>
    <p style="text-align:center">
     <img alt="" height="150" src="https://i-blog.csdnimg.cn/direct/5825e1aeae7249ee9f5dcb69a0436ba1.png" width="390"/>
    </p>
    <p style="text-align:center">
     <img alt="" height="100" src="https://i-blog.csdnimg.cn/direct/3405bdf2bdda47d6a4e173052f449b60.png" width="365"/>
    </p>
    <p style="text-align:center">
     <img alt="" height="100" src="https://i-blog.csdnimg.cn/direct/825b861dde6d402898a5d66458b83d56.png" width="364"/>
    </p>
    <h2>
     存在的问题
    </h2>
    <ol>
     <li>
      <strong>
       zero-shot的效果低
      </strong>
      ，准确度仅30%（ModelNet10）、20%（ModelNet40）和15%（ScanObejctNN），能否将zero-shot的效果继续提升？有可能
      <strong>
       3D点云与预训练好的2D编码器并不是一个好结合
      </strong>
      。
     </li>
     <li>
      模型融合之后一般效果都有所提升，但是本文通过实验说明并不是两两模型融合都有提升，而是和PointCLIP融合之后能提升，说明
      <strong>
       PointCLIP能够用2D信息与3D模型互补
      </strong>
      ，这点存疑。
     </li>
     <li>
      本文利用CLIP只做了3D分类任务，
      <strong>
       其它任务还未探索
      </strong>
      。
     </li>
    </ol>
    <h2>
     启发与思考
    </h2>
    <ol>
     <li>
      本文是CLIP的又一后续应用，
      <strong>
       从2D问题拓展到了3D问题
      </strong>
      ，说明这是对现有模型改进工作的思路之一。
     </li>
     <li>
      提供了在
      <strong>
       低资源成本和数据机制
      </strong>
      下利用CLIP的有效方案。
     </li>
     <li>
      <strong>
       模型融合
      </strong>
      可能是一个实用且便捷的提升性能的方法，有助于弥补模型的内在缺陷。
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f35383730313939352f:61727469636c652f64657461696c732f313436303531313236" class_="artid" style="display:none">
 </p>
</div>


