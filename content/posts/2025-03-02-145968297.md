---
layout: post
title: "初学者如何用-Python-写第一个爬虫"
date: 2025-03-02 19:18:04 +0800
description: "爬虫（Web Crawler）是一种自动化脚本或程序，它会模拟用户访问网页的行为，从而提取网页中的特定内容。"
keywords: "初学者如何用 Python 写第一个爬虫？"
categories: ['面试', '阿里巴巴', '学习路线']
tags: ['爬虫', '开发语言', 'Python']
artid: "145968297"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145968297
    alt: "初学者如何用-Python-写第一个爬虫"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145968297
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145968297
cover: https://bing.ee123.net/img/rand?artid=145968297
image: https://bing.ee123.net/img/rand?artid=145968297
img: https://bing.ee123.net/img/rand?artid=145968297
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     初学者如何用 Python 写第一个爬虫？
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <blockquote>
     <p>
      ??
      <strong>
       欢迎来到我的博客！
      </strong>
      非常高兴能在这里与您相遇。在这里，您不仅能获得有趣的技术分享，还能感受到轻松愉快的氛围。无论您是编程新手，还是资深开发者，都能在这里找到属于您的知识宝藏，学习和成长。
     </p>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/direct/53fc66a83b5e454d9b94c9c87c72543b.gif"/>
     </p>
     <p>
      ??
      <strong>
       博客内容包括：
      </strong>
     </p>
     <ul>
      <li>
       <strong>
        Java核心技术与微服务
       </strong>
       ：涵盖Java基础、JVM、并发编程、Redis、Kafka、Spring等，帮助您全面掌握企业级开发技术。
      </li>
      <li>
       <strong>
        大数据技术
       </strong>
       ：涵盖Hadoop（HDFS）、Hive、Spark、Flink、Kafka、Redis、ECharts、Zookeeper等相关技术。
      </li>
      <li>
       <strong>
        开发工具
       </strong>
       ：分享常用开发工具（IDEA、Git、Mac、Alfred、Typora等）的使用技巧，提升开发效率。
      </li>
      <li>
       <strong>
        数据库与优化
       </strong>
       ：总结MySQL及其他常用数据库技术，解决实际工作中的数据库问题。
      </li>
      <li>
       <strong>
        Python与大数据
       </strong>
       ：专注于Python编程语言的深度学习，数据分析工具（如Pandas、NumPy）和大数据处理技术，帮助您掌握数据分析、数据挖掘、机器学习等技术。
      </li>
      <li>
       <strong>
        数据结构与算法
       </strong>
       ：总结数据结构与算法的核心知识，提升编程思维，帮助您应对大厂面试挑战。
      </li>
     </ul>
     <p>
      ??
      <strong>
       我的目标
      </strong>
      ：持续学习与总结，分享技术心得与解决方案，和您一起探索技术的无限可能！在这里，我希望能与您共同进步，互相激励，成为更好的自己。
     </p>
     <p>
      ??
      <strong>
       欢迎订阅本专栏
      </strong>
      ，与我一起在这个知识的海洋中不断学习、分享和成长！???
     </p>
     <hr/>
     <p>
      ??
      <strong>
       版权声明
      </strong>
      ：本博客所有内容均为原创，遵循CC 4.0 BY-SA协议，转载请注明出处。
     </p>
    </blockquote>
    <p>
     <strong>
      目录
     </strong>
    </p>
    <p>
     <a href="#%E4%B8%80%E3%80%81%E7%88%AC%E8%99%AB%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5" rel="nofollow">
      一、爬虫的基本概念
     </a>
    </p>
    <p>
     <a href="#1.%20%E7%88%AC%E8%99%AB%E7%9A%84%E5%AE%9A%E4%B9%89" rel="nofollow">
      1. 爬虫的定义
     </a>
    </p>
    <p>
     <a href="#2.%20%E7%88%AC%E8%99%AB%E7%9A%84%E4%B8%BB%E8%A6%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B" rel="nofollow">
      2. 爬虫的主要工作流程
     </a>
    </p>
    <p>
     <a href="#3.%20%E5%B8%B8%E7%94%A8%20Python%20%E5%B7%A5%E5%85%B7" rel="nofollow">
      3. 常用 Python 工具
     </a>
    </p>
    <p>
     <a href="#%E4%BA%8C%E3%80%81%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87" rel="nofollow">
      二、环境准备
     </a>
    </p>
    <p>
     <a href="#1.%20%E5%AE%89%E8%A3%85%20Python" rel="nofollow">
      1. 安装 Python
     </a>
    </p>
    <p>
     <a href="#2.%20%E5%AE%89%E8%A3%85%E5%BF%85%E8%A6%81%E5%BA%93" rel="nofollow">
      2. 安装必要库
     </a>
    </p>
    <p>
     <a href="#%E4%B8%89%E3%80%81%E5%86%99%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%88%AC%E8%99%AB" rel="nofollow">
      三、写第一个简单的爬虫
     </a>
    </p>
    <p>
     <a href="#1.%20%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B" rel="nofollow">
      1. 完整代码示例
     </a>
    </p>
    <p>
     <a href="#2.%20%E4%BB%A3%E7%A0%81%E9%80%90%E6%AD%A5%E8%A7%A3%E6%9E%90" rel="nofollow">
      2. 代码逐步解析
     </a>
    </p>
    <p>
     <a href="#1%EF%BC%89%E5%8F%91%E9%80%81%20HTTP%20%E8%AF%B7%E6%B1%82" rel="nofollow">
      1）发送 HTTP 请求
     </a>
    </p>
    <p>
     <a href="#2%EF%BC%89%E6%A3%80%E6%9F%A5%E8%AF%B7%E6%B1%82%E7%8A%B6%E6%80%81" rel="nofollow">
      2）检查请求状态
     </a>
    </p>
    <p>
     <a href="#3%EF%BC%89%E8%A7%A3%E6%9E%90%20HTML%20%E6%95%B0%E6%8D%AE" rel="nofollow">
      3）解析 HTML 数据
     </a>
    </p>
    <p>
     <a href="#4%EF%BC%89%E6%8F%90%E5%8F%96%E7%BD%91%E9%A1%B5%E5%86%85%E5%AE%B9" rel="nofollow">
      4）提取网页内容
     </a>
    </p>
    <p>
     <a href="#5%EF%BC%89%E6%89%93%E5%8D%B0%E7%BB%93%E6%9E%9C" rel="nofollow">
      5）打印结果
     </a>
    </p>
    <p>
     <a href="#%E5%9B%9B%E3%80%81%E6%94%B9%E8%BF%9B%E7%88%AC%E8%99%AB%E5%8A%9F%E8%83%BD" rel="nofollow">
      四、改进爬虫功能
     </a>
    </p>
    <p>
     <a href="#1.%20%E6%B7%BB%E5%8A%A0%E8%AF%B7%E6%B1%82%E5%A4%B4" rel="nofollow">
      1. 添加请求头
     </a>
    </p>
    <p>
     <a href="#2.%20%E6%8E%A7%E5%88%B6%E7%88%AC%E5%8F%96%E9%A2%91%E7%8E%87" rel="nofollow">
      2. 控制爬取频率
     </a>
    </p>
    <p>
     <a href="#3.%20%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE" rel="nofollow">
      3. 保存数据
     </a>
    </p>
    <p>
     <a href="#%E4%BA%94%E3%80%81%E5%BA%94%E5%AF%B9%E5%A4%8D%E6%9D%82%E7%BD%91%E9%A1%B5" rel="nofollow">
      五、应对复杂网页
     </a>
    </p>
    <p>
     <a href="#1.%20%E5%8A%A8%E6%80%81%E5%8A%A0%E8%BD%BD%E7%BD%91%E9%A1%B5" rel="nofollow">
      1. 动态加载网页
     </a>
    </p>
    <p>
     <a href="#2.%20%E7%88%AC%E5%8F%96%E5%9B%BE%E7%89%87%E6%88%96%E6%96%87%E4%BB%B6" rel="nofollow">
      2. 爬取图片或文件
     </a>
    </p>
    <p>
     <a href="#%E5%85%AD%E3%80%81%E7%88%AC%E8%99%AB%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9" rel="nofollow">
      六、爬虫的注意事项
     </a>
    </p>
    <p>
     <a href="#1.%20%E9%81%B5%E5%AE%88%E6%B3%95%E5%BE%8B%E5%92%8C%E9%81%93%E5%BE%B7" rel="nofollow">
      1. 遵守法律和道德
     </a>
    </p>
    <p>
     <a href="#2.%20%E5%A4%84%E7%90%86%E5%BC%82%E5%B8%B8" rel="nofollow">
      2. 处理异常
     </a>
    </p>
    <p>
     <a href="#3.%20%E9%81%BF%E5%85%8D%E8%BF%87%E4%BA%8E%E9%A2%91%E7%B9%81%E7%9A%84%E8%AF%B7%E6%B1%82" rel="nofollow">
      3. 避免过于频繁的请求
     </a>
    </p>
    <hr/>
    <p>
     网页爬虫是一种通过程序自动抓取网页数据的技术。对于初学者来说，使用 Python 写一个简单的爬虫是一个很好的入门项目。Python 提供了许多强大的工具和库，如
     <code>
      requests
     </code>
     和
     <code>
      BeautifulSoup
     </code>
     ，可以帮助快速实现网页数据的爬取。
    </p>
    <p>
     在本文中，我们将从爬虫的基本概念开始，逐步实现一个可以抓取网页内容的简单爬虫，并探讨如何改进爬虫以应对复杂场景。我们将从以下几个方面展开：
    </p>
    <hr/>
    <h5>
     <a id="_87">
     </a>
     一、爬虫的基本概念
    </h5>
    <h6>
     <a id="1__89">
     </a>
     1. 爬虫的定义
    </h6>
    <p>
     爬虫（Web Crawler）是一种自动化脚本或程序，它会模拟用户访问网页的行为，从而提取网页中的特定内容。
    </p>
    <h6>
     <a id="2__93">
     </a>
     2. 爬虫的主要工作流程
    </h6>
    <p>
     一个典型的爬虫任务通常包括以下步骤：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        发送请求
       </strong>
       ：通过 HTTP 协议访问目标网页，获取其 HTML 内容。
      </p>
     </li>
     <li>
      <p>
       <strong>
        解析数据
       </strong>
       ：对获取到的 HTML 进行解析，提取我们需要的数据。
      </p>
     </li>
     <li>
      <p>
       <strong>
        存储数据
       </strong>
       ：将提取到的数据保存到文件或数据库中，便于后续处理。
      </p>
     </li>
    </ul>
    <h6>
     <a id="3__Python__104">
     </a>
     3. 常用 Python 工具
    </h6>
    <ul>
     <li>
      <p>
       <code>
        **requests**
       </code>
       ：发送 HTTP 请求，获取网页内容。
      </p>
     </li>
     <li>
      <p>
       <code>
        **BeautifulSoup**
       </code>
       ：解析 HTML 或 XML 数据，提取特定内容。
      </p>
     </li>
     <li>
      <p>
       <code>
        **re**
       </code>
       <strong>
        （正则表达式）
       </strong>
       ：对复杂文本模式进行匹配和提取。
      </p>
     </li>
     <li>
      <p>
       <code>
        **pandas**
       </code>
       ：对数据进行清洗和分析。
      </p>
     </li>
    </ul>
    <hr/>
    <h5>
     <a id="_117">
     </a>
     二、环境准备
    </h5>
    <h6>
     <a id="1__Python_119">
     </a>
     1. 安装 Python
    </h6>
    <p>
     确保你的计算机上已经安装了 Python（推荐使用 3.7 及以上版本）。如果尚未安装，可以从
     <a href="https://www.python.org/" rel="nofollow" title="Python 官方网站">
      Python 官方网站
     </a>
     下载并安装。
    </p>
    <h6>
     <a id="2__123">
     </a>
     2. 安装必要库
    </h6>
    <p>
     打开命令行或终端，运行以下命令安装我们需要的 Python 库：
    </p>
    <pre><code>pip install requests beautifulsoup4
</code></pre>
    <ul>
     <li>
      <p>
       <code>
        **requests**
       </code>
       ：用于发送 HTTP 请求。
      </p>
     </li>
     <li>
      <p>
       <code>
        **beautifulsoup4**
       </code>
       ：用于解析 HTML 数据。
      </p>
     </li>
    </ul>
    <hr/>
    <h5>
     <a id="_136">
     </a>
     三、写第一个简单的爬虫
    </h5>
    <p>
     我们来实现一个简单的爬虫，它将抓取某个网页的标题和正文内容。
    </p>
    <h6>
     <a id="1__140">
     </a>
     1. 完整代码示例
    </h6>
    <p>
     以下代码实现了一个基本的爬虫：
    </p>
    <pre><code>import requests
from bs4 import BeautifulSoup

def simple_crawler(url):
    try:
        # 1. 发送请求
        response = requests.get(url)
        response.raise_for_status()  # 检查请求是否成功

        # 2. 解析网页内容
        soup = BeautifulSoup(response.text, 'html.parser')

        # 3. 提取标题和段落内容
        title = soup.find('title').text  # 获取网页标题
        paragraphs = soup.find_all('p')  # 获取所有段落内容

        print(f"网页标题: {title}
")
        print("网页内容:")
        for p in paragraphs:
            print(p.text)

    except requests.exceptions.RequestException as e:
        print(f"请求失败: {e}")

# 示例网址
url = "https://example.com"  # 替换为你想爬取的网页地址
simple_crawler(url)
</code></pre>
    <h6>
     <a id="2__173">
     </a>
     2. 代码逐步解析
    </h6>
    <h6>
     <a id="1_HTTP__175">
     </a>
     1）发送 HTTP 请求
    </h6>
    <pre><code>response = requests.get(url)
</code></pre>
    <ul>
     <li>
      <p>
       使用
       <code>
        requests.get()
       </code>
       方法向目标网址发送 GET 请求。
      </p>
     </li>
     <li>
      <p>
       返回的
       <code>
        response
       </code>
       对象包含网页的所有内容，包括 HTML 源代码。
      </p>
     </li>
    </ul>
    <h6>
     <a id="2_184">
     </a>
     2）检查请求状态
    </h6>
    <pre><code>response.raise_for_status()
</code></pre>
    <ul>
     <li>
      通过
      <code>
       raise_for_status()
      </code>
      检查请求是否成功。如果返回的 HTTP 状态码表示错误（如 404 或 500），会抛出异常。
     </li>
    </ul>
    <h6>
     <a id="3_HTML__191">
     </a>
     3）解析 HTML 数据
    </h6>
    <pre><code>soup = BeautifulSoup(response.text, 'html.parser')
</code></pre>
    <ul>
     <li>
      <p>
       <code>
        BeautifulSoup
       </code>
       用于解析 HTML 内容，并将其转化为 Python 对象，方便后续操作。
      </p>
     </li>
     <li>
      <p>
       第二个参数
       <code>
        'html.parser'
       </code>
       指定使用 Python 内置的 HTML 解析器。
      </p>
     </li>
    </ul>
    <h6>
     <a id="4_200">
     </a>
     4）提取网页内容
    </h6>
    <pre><code>title = soup.find('title').text
paragraphs = soup.find_all('p')
</code></pre>
    <ul>
     <li>
      <p>
       <code>
        find('title')
       </code>
       方法返回
       <code>
        &lt;title&gt;
       </code>
       标签的内容。
      </p>
     </li>
     <li>
      <p>
       <code>
        find_all('p')
       </code>
       方法返回所有段落标签
       <code>
        &lt;p&gt;
       </code>
       ，并以列表形式存储。
      </p>
     </li>
    </ul>
    <h6>
     <a id="5_210">
     </a>
     5）打印结果
    </h6>
    <pre><code>for p in paragraphs:
    print(p.text)
</code></pre>
    <ul>
     <li>
      遍历提取到的段落内容，并打印每个段落的文本。
     </li>
    </ul>
    <hr/>
    <h5>
     <a id="_220">
     </a>
     四、改进爬虫功能
    </h5>
    <h6>
     <a id="1__222">
     </a>
     1. 添加请求头
    </h6>
    <p>
     一些网站会检测爬虫程序并阻止访问。可以通过添加请求头来模拟浏览器访问。
    </p>
    <pre><code>headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
response = requests.get(url, headers=headers)
</code></pre>
    <h6>
     <a id="2__231">
     </a>
     2. 控制爬取频率
    </h6>
    <p>
     为了避免对目标网站造成过高的负载，可以在每次请求后添加延时。
    </p>
    <pre><code>import time

def delay_request(url):
    response = requests.get(url)
    time.sleep(2)  # 等待 2 秒
    return response
</code></pre>
    <h6>
     <a id="3__242">
     </a>
     3. 保存数据
    </h6>
    <p>
     将爬取的数据保存为文件或数据库。
    </p>
    <p>
     <strong>
      保存到文件：
     </strong>
    </p>
    <pre><code>with open("output.txt", "w", encoding="utf-8") as f:
    f.write(f"标题: {title}
")
    for p in paragraphs:
        f.write(p.text + "
")
</code></pre>
    <p>
     <strong>
      保存到 CSV 文件：
     </strong>
    </p>
    <pre><code>import csv

with open("output.csv", "w", newline="", encoding="utf-8") as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(["段落内容"])
    for p in paragraphs:
        writer.writerow([p.text])
</code></pre>
    <hr/>
    <h5>
     <a id="_267">
     </a>
     五、应对复杂网页
    </h5>
    <h6>
     <a id="1__269">
     </a>
     1. 动态加载网页
    </h6>
    <p>
     对于 JavaScript 渲染的网页，
     <code>
      requests
     </code>
     无法获取完整内容，可以使用
     <code>
      selenium
     </code>
     或
     <code>
      playwright
     </code>
     。
    </p>
    <p>
     <strong>
      示例（使用 selenium）：
     </strong>
    </p>
    <pre><code>from selenium import webdriver

url = "https://example.com"

# 配置 WebDriver
driver = webdriver.Chrome()
driver.get(url)

# 获取动态加载的内容
html = driver.page_source
print(html)

# 关闭浏览器
driver.quit()
</code></pre>
    <h6>
     <a id="2__290">
     </a>
     2. 爬取图片或文件
    </h6>
    <pre><code>import os

# 下载图片
img_url = "https://example.com/image.jpg"
response = requests.get(img_url)

# 保存图片
with open("image.jpg", "wb") as f:
    f.write(response.content)
</code></pre>
    <hr/>
    <h5>
     <a id="_304">
     </a>
     六、爬虫的注意事项
    </h5>
    <h6>
     <a id="1__306">
     </a>
     1. 遵守法律和道德
    </h6>
    <ul>
     <li>
      <p>
       <strong>
        避免违反法律
       </strong>
       ：确保爬取行为符合目标网站的使用条款。
      </p>
     </li>
     <li>
      <p>
       <strong>
        尊重 robots.txt 文件
       </strong>
       ：通过
       <code>
        robots.txt
       </code>
       查看目标网站的爬取限制。
      </p>
     </li>
    </ul>
    <h6>
     <a id="2__313">
     </a>
     2. 处理异常
    </h6>
    <p>
     对于网络请求失败、数据缺失等情况，添加异常处理逻辑：
    </p>
    <pre><code>try:
    response = requests.get(url)
    response.raise_for_status()
except requests.exceptions.RequestException as e:
    print(f"请求失败: {e}")
</code></pre>
    <h6>
     <a id="3__323">
     </a>
     3. 避免过于频繁的请求
    </h6>
    <p>
     可以设置延时或使用代理 IP：
    </p>
    <pre><code>proxies = {
    "http": "http://123.45.67.89:8080",
    "https": "http://123.45.67.89:8080"
}
response = requests.get(url, proxies=proxies)
</code></pre>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470:733a2f2f626c6f672e6373646e2e6e65742f41444656424d2f:61727469636c652f64657461696c732f313435393638323937" class_="artid" style="display:none">
 </p>
</div>


