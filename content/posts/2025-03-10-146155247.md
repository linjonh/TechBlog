---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f79757269353135312f:61727469636c652f64657461696c732f313436313535323437"
layout: post
title: "动手学习深度学习13.丢弃法-Dropout"
date: 2025-03-10 15:19:32 +08:00
description: "丢弃法，控制模型复杂度方法，多用在多层感知机的隐藏层输出上"
keywords: "[动手学习深度学习]13.丢弃法 Dropout"
categories: ['深度学习']
tags: ['深度学习', '学习', '人工智能']
artid: "146155247"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146155247
    alt: "动手学习深度学习13.丢弃法-Dropout"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146155247
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146155247
cover: https://bing.ee123.net/img/rand?artid=146155247
image: https://bing.ee123.net/img/rand?artid=146155247
img: https://bing.ee123.net/img/rand?artid=146155247
---

# [动手学习深度学习]13.丢弃法 Dropout

权重衰退是常见处理过拟合的方法
  
丢弃法比权重衰退效果要好

## 动机

* 一个好的模型 需要第输入数据的扰动具有鲁棒性
  + 使用有噪音的数据等价于Tikhonov正则
  + 丢弃法：在层之间加入噪音

（所以丢弃法其实是一个正则）

## 无偏差的加入噪音

* 对x加入噪音得到x’，我们希望
    




  E
  [
  x
  ′
  ]
  =
  x
  E[x'] = x





  E

  [


  x










  ′

  ]



  =





  x
* 丢弃法对每个元素进行如下扰动
    
  ![](https://i-blog.csdnimg.cn/direct/8439e27d5aef4141afd9ae768490bc84.png)

## 使用

通常将丢弃法作用在隐藏全连接层的输出上
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f30c75aa8cd34650ac4252289ca322e8.png)
  
对隐藏层的每一个神经元做dropout，使每一个都有p概率变为0
  
即去掉一些权重（每次可能去掉的不一样）

> 在训练中使用

## 推理中的丢弃法

* 正则项只在训练中使用：他们影响模型参数的更新
* 在推理过程中，dropout直接返回输入

  ```python
  h=dropout(h)

  ```

  这样也能保证确定性的输出
    
  每次随机的采样一些子神经网络

## 总结

* 丢弃法将一些输出项随机置0来控制模型复杂度
* 常作用在多层感知机的隐藏层输出上
* 丢弃概率使控制模型复杂度的超参数