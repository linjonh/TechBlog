---
layout: post
title: "机器学习-决策树"
date: 2025-03-12 15:34:35 +0800
description: "决策树通过递归地选择最优特征对数据集进行分割，最终生成一棵树状模型。每个节点代表一个特征的分裂规则，每个分支代表一个可能的特征值，叶节点则代表最终的预测结果（分类或回归值）。"
keywords: "机器学习-----决策树"
categories: ['机器学习']
tags: ['机器学习', '决策树', '人工智能']
artid: "146199729"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146199729
    alt: "机器学习-决策树"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146199729
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146199729
cover: https://bing.ee123.net/img/rand?artid=146199729
image: https://bing.ee123.net/img/rand?artid=146199729
img: https://bing.ee123.net/img/rand?artid=146199729
---

# 机器学习-----决策树

* * *

* * *

### 1、概念

1.1决策树是什么  
决策树是通过对样本的训练，建立出分类规则，并对新样本进行预测，属于有监督学习。  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f61e25c6e50c4d9e93f46d2c7a0e1dd5.png)

> 根节点：最上面的节点。  
>  叶子节点：能直接看到结果的节点。  
>  非叶子节点：位于中间的节点。

1.2决策树的类型

> 分类树：用于分类任务，叶节点代表类别标签  
>  回归树：用于回归任务，叶节点代表连续值。

### 2\. 决策树的构建过程

#### 2.1 特征选择

特征选择是决策树构建过程中的关键步骤。常用的特征选择方法有：

>   * 信息增益：基于信息熵的减少量来选择特征。
>   * 信息增益比：信息增益的归一化版本，用于解决信息增益偏向于取值较多的特征的问题。
>   * 基尼指数：用于CART算法，表示数据的不纯度。
>

#### 2.2 树的生成

决策树的生成过程是一个递归的过程，具体步骤如下：

> 1.选择最佳特征：根据特征选择方法，选择当前数据集的最佳特征。  
>  2\. 划分数据集：根据最佳特征的取值，将数据集划分为若干子集。  
>  3.递归生成子树：对每个子集递归地应用上述步骤，直到满足停止条件（如所有样本属于同一类别，或没有更多特征可供选择）。

#### 2.3 树的剪枝

为了防止过拟合，决策树通常需要进行剪枝。剪枝分为预剪枝和后剪枝：

>   * 预剪枝：在树的生成过程中，提前停止树的生长。
>   * 后剪枝：先生成完整的树，然后自底向上地剪去一些子树。
>

### 3\. 决策树的优缺点

优点

>   * 易于理解和解释：决策树的结构直观，易于理解和解释。
>   * 处理多种数据类型：可以处理数值型和类别型数据。
>   * 不需要数据标准化：决策树不需要对数据进行标准化或归一化处理。
>

缺点

>   * 容易过拟合：决策树容易生成过于复杂的树，导致过拟合。
>   * 对噪声敏感：决策树对噪声数据较为敏感，可能导致错误的决策路径。
>   * 不稳定性：数据的微小变化可能导致生成完全不同的树。
>

### 4\. 决策树的应用

#### 4.1 分类任务

> 决策树广泛应用于分类任务，如垃圾邮件过滤、疾病诊断等。

#### 4.2 回归任务

> 决策树也可以用于回归任务，如房价预测、股票价格预测等。

#### 4.3 集成学习

> 决策树是许多集成学习方法的基础，如随机森林、梯度提升树（GBDT）等。

### 代码示例

需要用到sk-learn库

    
    
    import pandas as pd
    from sklearn.metrics import confusion_matrix
    import matplotlib.pyplot as plt
    def cm_plot(y, yp):
        cm = confusion_matrix(y, yp)
        plt.matshow(cm, cmap=plt.cm.Blues)
        plt.colorbar()
        for x in range(len(cm)):
            for y in range(len(cm)):
                plt.annotate(cm[x, y], xy=(x, y), horizontalalignment='center', verticalalignment='center')
        plt.ylabel('True label')
        plt.xlabel('Predicted label')
        return plt
    
    #导入数据
    data = pd.read_excel('电信客户流失数据.xlsx')
    
    #将变量与结果划分开
    datas = data.iloc[ : , :-1]
    target = data.iloc[ : ,-1]
    
    
    from sklearn.model_selection import train_test_split
    
    datas_train, datas_test, target_train, target_test = \
            train_test_split(datas,target,test_size=0.25,random_state=45)
    
    #定义决策树
    from sklearn import tree
    tr = tree.DecisionTreeClassifier(criterion='gini', max_depth=7,min_samples_split=7,min_samples_leaf=10,random_state=45)
    tr.fit(datas_train,target_train)
    
    '''
    训练集混淆矩阵
    '''
    #训练集预测值
    train_predicted = tr.predict(datas_train)
    
    from sklearn import metrics
    print(metrics.classification_report(target_train,train_predicted))
    
    cm_plot(target_train, train_predicted).show()
    
    
    '''
    测试集混淆矩阵
    '''
    
    test_predicted = tr.predict(datas_test)
    print(metrics.classification_report(target_test,test_predicted))
    cm_plot(target_test, test_predicted).show()
    tr.score(datas_test,target_test)
    
    
    from sklearn.tree import plot_tree
    fig, ax = plt.subplots(figsize= (32,32))
    plot_tree(tr,filled=True,ax=ax)
    plt.show()
    

* * *

## 总结

决策树通过递归地选择最优特征对数据集进行分割，最终生成一棵树状模型。每个节点代表一个特征的分裂规则，每个分支代表一个可能的特征值，叶节点则代表最终的预测结果（分类或回归值）。



