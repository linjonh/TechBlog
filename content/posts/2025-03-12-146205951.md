---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34353637303430372f:61727469636c652f64657461696c732f313436323035393531"
layout: post
title: "什么是张量不是卖麻辣烫的那个张亮"
date: 2025-03-12 15:11:51 +0800
description: "说人话：张量（Tensor）是啥 。"
keywords: "什么是张量（不是卖麻辣烫的那个张亮）"
categories: ['乱七八糟']
tags: ['计算机视觉', '算法', '神经网络', '深度学习', '人工智能']
artid: "146205951"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146205951
    alt: "什么是张量不是卖麻辣烫的那个张亮"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146205951
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146205951
cover: https://bing.ee123.net/img/rand?artid=146205951
image: https://bing.ee123.net/img/rand?artid=146205951
img: https://bing.ee123.net/img/rand?artid=146205951
---

# 什么是张量（不是卖麻辣烫的那个张亮）

## 张量小记

张量是在数学、物理、计算机等领域的一个重要的学术名词，看起来很高大上（实际有时候也雀食酱紫），在这里煮啵简要通俗的说说这玩意是个啥，补药一听到张量就想起麻辣烫了大馋猫（虽然煮啵曾经也是）。

### **通俗易懂的张量（Tensor）讲解**

### **1. 什么是张量？**

说人话：张量（Tensor）是一种
**“多维数据表”**
，可以用来存储和处理多维信息。

#### **标量、向量、矩阵与张量的关系**

| 维度 | 名称 | 示例 | 直观理解 |
| --- | --- | --- | --- |
| 0 维 | 标量（Scalar） | 3.14 3.14      3.14 ,  42 42      42 | 一个普通的数 |
| 1 维 | 向量（Vector） | [ 1 , 2 , 3 ] [1, 2, 3]      [  1  ,    2  ,    3  ] | 一行数，类似一个箭头 |
| 2 维 | 矩阵（Matrix） | [ 1 2 3 4 ] \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}        [            1      3  ​               2      4  ​    ] | 一个表格，像 Excel 数据表 |
| 3 维 | 3 阶张量 | 彩色图像（  28 × 28 × 3 28\times28\times3      28    ×      28    ×      3 ） | 一本书，每一页是一个矩阵 |
| 4 维 | 4 阶张量 | 视频（  100 × 28 × 28 × 3 100\times28\times28\times3      100    ×      28    ×      28    ×      3 ） | 一部电影，每一帧是一个 3 维数据 |

### **2. 现实中的张量**

张量在现实世界无处不在：

* **灰度图片**
  （2 阶张量）：黑白图片是一个

  28
  ×
  28
  28 \times 28





  28



  ×





  28
  的矩阵。
* **彩色图片**
  （3 阶张量）：一张你的英俊潇洒的自拍

  1960
  ×
  1280
  1960\times1280





  1960



  ×





  1280
  的彩色图片有 R、G、B 三个颜色通道，因此是

  28
  ×
  28
  ×
  3
  28 \times 28 \times 3





  28



  ×





  28



  ×





  3
  的数据。
* **视频**
  （4 阶张量）：如果小电影有 100 帧，每一帧是一个彩色图片，那整个视频是一个

  100
  ×
  1920
  ×
  1280
  ×
  3
  100 \times 1920 \times 1280 \times 3





  100



  ×





  1920



  ×





  1280



  ×





  3
  的数据。

简单来说，
**张量就是更高维的数据表！**

### **3. 张量的坐标变换**

张量的一个重要特点是：
**无论坐标系怎么变，它的本质不变**
。举个例子：

* **标量（如温度）**
  ：无论在哪个国家，

  3
  0
  ∘
  C
  30^{\circ}C





  3


  0










  ∘

  C
  还是

  3
  0
  ∘
  C
  30^{\circ}C





  3


  0










  ∘

  C
  。
* **向量（如风速）**
  ：风速

  5
  m/s
  5 \text{m/s}





  5


  m/s
  向东，如果换个坐标系，它的

  x
  x





  x
  、

  y
  y





  y
  分量会变，但风速本身不变。
* **矩阵（如应力）**
  ：比如房子受到的压力，在不同角度看可能不同，但压力本身是不变的。

这就是
**张量的变换规则**
，确保它可以在不同坐标系下正确描述物理现象。

### **4. 张量的计算**

#### **(1) 张量加法**

如果两个张量的形状相同，比如：
  




A
=
[
1
2
3
4
]
,
B
=
[
5
6
7
8
]
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}, \quad B = \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix}





A



=







[











1





3

​














2





4

​



]



,





B



=







[











5





7

​














6





8

​



]
  
那么：
  




A
+
B
=
[
1
+
5
2
+
6
3
+
7
4
+
8
]
=
[
6
8
10
12
]
A + B = \begin{bmatrix} 1+5 & 2+6 \\ 3+7 & 4+8 \end{bmatrix} = \begin{bmatrix} 6 & 8 \\ 10 & 12 \end{bmatrix}





A



+





B



=







[











1



+



5





3



+



7

​














2



+



6





4



+



8

​



]



=







[











6





10

​














8





12

​



]

#### **(2) 张量乘法**

如果是矩阵乘法：
  




A
×
B
=
矩阵相乘
A \times B = \text{矩阵相乘}





A



×





B



=






矩阵相乘
  
如果是更高阶张量，就会用更复杂的计算方式，比如
**张量积（Tensor Product）**
。

### **5. 张量的应用**

#### **(1) 物理学（广义相对论）**

爱因斯坦的
**广义相对论**
用张量描述引力：
  




R
μ
ν
−
1
2
g
μ
ν
R
+
Λ
g
μ
ν
=
8
π
G
c
4
T
μ
ν
R\_{\mu\nu} - \frac{1}{2} g\_{\mu\nu} R + \Lambda g\_{\mu\nu} = \frac{8\pi G}{c^4} T\_{\mu\nu}






R










μν

​




−
















2











1

​



g










μν

​


R



+





Λ


g










μν

​




=

















c









4











8

π

G

​



T










μν

​

  
这里的
**$ g\_{\mu\nu} $**
就是
**度量张量**
，它决定了空间的形状。

#### **(2) 机器学习**

在
**深度学习**
（如 TensorFlow、PyTorch）中，所有数据（图片、音频、文本）都存储成张量。例如：

* 一张图片是一个 3 维张量。
* 一个批次（Batch）图片是一个 4 维张量。

### （3）在计算机视觉中的应用 （煮啵夹带私活嘿嘿）

张量（Tensor）在\*\*计算机视觉（Computer Vision, CV）\*\*中至关重要，因为几乎所有的视觉数据都可以用张量表示和处理。下面是张量在计算机视觉中的主要应用，以及相关的详细讲解。

---

### **1. 计算机视觉中的张量表示**

在计算机视觉任务中，图片、视频等数据都可以表示为张量。例如：

#### **(1) 图片（Image）**

一张灰度图片可以用
**二维张量（矩阵）**
表示：

* 假设有一张

  28
  ×
  28
  28 \times 28





  28



  ×





  28
  的黑白图片，每个像素点存储一个灰度值（

  0
  ∼
  255
  0 \sim 255





  0



  ∼





  255
  ）。
* 这就是一个
  **(
  H
  ,
  W
  )
  (H, W)





  (

  H

  ,



  W

  )
  形状的 2D 张量**
  ，即

  28
  ×
  28
  28 \times 28





  28



  ×





  28
  的矩阵。

一张彩色图片是一个
**三维张量**
：

* 彩色图片有
  **RGB 通道**
  （红、绿、蓝），每个通道都是一个

  H
  ×
  W
  H \times W





  H



  ×





  W
  的矩阵。
* 这构成了一个
  **(
  H
  ,
  W
  ,
  C
  )
  (H, W, C)





  (

  H

  ,



  W

  ,



  C

  )
  形状的 3D 张量**
  ，例如

  28
  ×
  28
  ×
  3
  28 \times 28 \times 3





  28



  ×





  28



  ×





  3
  。

#### **(2) 视频（Video）**

* 视频是由\*\*多张图片（帧）\*\*组成的，所以它是一个
  **四维张量**
  ：
    




  (
  N
  ,
  H
  ,
  W
  ,
  C
  )
  (N, H, W, C)





  (

  N

  ,



  H

  ,



  W

  ,



  C

  )
    
  其中：

  + N
    N





    N
    表示
    **帧数**
    （Frame number）。
  + H
    ,
    W
    H, W





    H

    ,



    W
    是
    **图片高度和宽度**
    。
  + C
    C





    C
    是
    **通道数（RGB = 3）**
    。

例如，一个

60
60





60
帧的视频，分辨率

1920
×
1080
1920 \times 1080





1920



×





1080
，RGB 通道，就可以用：
  




(
60
,
1080
,
1920
,
3
)
(60, 1080, 1920, 3)





(

60

,



1080

,



1920

,



3

)
  
的 4D 张量来表示。

#### **(3) 批量处理（Batch Processing）**

在深度学习中，我们通常一次处理多个样本，例如：

* 一个批次（Batch）有

  32
  32





  32
  张图片，每张是

  28
  ×
  28
  ×
  3
  28 \times 28 \times 3





  28



  ×





  28



  ×





  3
  ，则数据是：
    




  (
  B
  ,
  H
  ,
  W
  ,
  C
  )
  =
  (
  32
  ,
  28
  ,
  28
  ,
  3
  )
  (B, H, W, C) = (32, 28, 28, 3)





  (

  B

  ,



  H

  ,



  W

  ,



  C

  )



  =





  (

  32

  ,



  28

  ,



  28

  ,



  3

  )
    
  这就是
  **5 维张量（5D Tensor）**
  ，用于批量训练神经网络。

---

### **2. 张量在计算机视觉任务中的应用**

#### **(1) 图像分类（Image Classification）**

任务：给定一张图片，判断其类别，比如
**猫 / 狗 分类**
。

* **输入：**



  (
  H
  ,
  W
  ,
  C
  )
  (H, W, C)





  (

  H

  ,



  W

  ,



  C

  )
  形状的张量（如

  224
  ×
  224
  ×
  3
  224 \times 224 \times 3





  224



  ×





  224



  ×





  3
  ）。
* **模型（CNN）：**
  使用卷积神经网络（CNN）处理。
* **输出：**
  一个
  **类别概率向量**
  （如

  [
  0.9
  ,
  0.1
  ]
  [0.9, 0.1]





  [

  0.9

  ,



  0.1

  ]
  代表 90% 是猫，10% 是狗）。

示例网络：
  




输入
→
CNN 层
→
全连接层
→
Softmax 输出
\text{输入} \rightarrow \text{CNN 层} \rightarrow \text{全连接层} \rightarrow \text{Softmax 输出}






输入



→






CNN

层



→






全连接层



→






Softmax

输出

* CNN 提取特征，如
  **边缘、纹理、形状**
  等。
* 最终输出一个
  **张量**
  ，代表每个类别的概率。

#### **(2) 目标检测（Object Detection）**

任务：检测图片中的多个目标，输出它们的
**位置**
和
**类别**
。

* **输入：**



  (
  H
  ,
  W
  ,
  C
  )
  (H, W, C)





  (

  H

  ,



  W

  ,



  C

  )
  形状的张量（如

  416
  ×
  416
  ×
  3
  416 \times 416 \times 3





  416



  ×





  416



  ×





  3
  ）。
* **模型：**
  使用 YOLO / Faster R-CNN 等目标检测网络。
* **输出：**
  一个
  **多维张量**
  ，表示多个目标的
  **类别和边界框**
  。

示例输出：
  




(
N
,
5
)
=
(
3
,
[
x
,
y
,
w
,
h
,
c
l
a
s
s
]
)
(N, 5) = (3, [x, y, w, h, class])





(

N

,



5

)



=





(

3

,



[

x

,



y

,



w

,



h

,



c

l

a

ss

])

* 这里的

  N
  N





  N
  是检测到的目标数（比如 3 个）。
* 每个目标由 5 个数值表示：
  + x
    ,
    y
    ,
    w
    ,
    h
    x, y, w, h





    x

    ,



    y

    ,



    w

    ,



    h
    表示目标框的位置（中心点坐标 + 宽高）。
  + c
    l
    a
    s
    s
    class





    c

    l

    a

    ss
    表示类别（如 1 = 狗，2 = 猫）。

#### **(3) 语义分割（Semantic Segmentation）**

任务：给图片的每个像素分类，比如区分“人”和“背景”。

* **输入：**



  (
  H
  ,
  W
  ,
  C
  )
  (H, W, C)





  (

  H

  ,



  W

  ,



  C

  )
  形状的图片张量。
* **模型：**
  U-Net、DeepLabV3+ 等语义分割网络。
* **输出：**



  (
  H
  ,
  W
  ,
  K
  )
  (H, W, K)





  (

  H

  ,



  W

  ,



  K

  )
  形状的张量，其中

  K
  K





  K
  是类别数。

示例：

* 对于

  512
  ×
  512
  512 \times 512





  512



  ×





  512
  的图片，输出是

  (
  512
  ,
  512
  ,
  2
  )
  (512, 512, 2)





  (

  512

  ,



  512

  ,



  2

  )
  ，每个像素都有两个数值，分别表示“前景”和“背景”的概率。

#### **(4) 实例分割（Instance Segmentation）**

任务：不仅要分割出物体，还要区分不同个体，比如图中有 3 只狗，要给它们不同编号。

* **输入：**



  (
  H
  ,
  W
  ,
  C
  )
  (H, W, C)





  (

  H

  ,



  W

  ,



  C

  )
  的图片张量。
* **输出：**



  (
  H
  ,
  W
  ,
  N
  )
  (H, W, N)





  (

  H

  ,



  W

  ,



  N

  )
  ，其中

  N
  N





  N
  是检测到的实例个数。

Mask R-CNN 是一个典型的实例分割模型，它的输出是一个
**多通道张量**
：

* 一个通道是目标类别。
* 另一个通道是目标的边界框坐标。
* 还有一个通道是
  **掩码（Mask）**
  ，表示像素级区域。

#### **(5) 生成对抗网络（GANs）**

任务：用深度学习
**生成**
新图像，比如生成逼真的人脸。

* **输入：**
  一个随机向量张量（如

  [
  100
  ]
  [100]





  [

  100

  ]
  ）。
* **生成器（Generator）：**
  把 1D 张量转换成 3D 图片张量。
* **输出：**



  (
  H
  ,
  W
  ,
  C
  )
  (H, W, C)





  (

  H

  ,



  W

  ,



  C

  )
  形状的张量，表示合成图像。

GANs 结构：
  




随机噪声
→
G
(
z
)
→
生成假图片
→
D
(
x
)
→
判别真假
\text{随机噪声} \rightarrow G(z) \rightarrow \text{生成假图片} \rightarrow D(x) \rightarrow \text{判别真假}






随机噪声



→





G

(

z

)



→






生成假图片



→





D

(

x

)



→






判别真假

* 生成器
  **G**
  生成图片张量。
* 判别器
  **D**
  评估真假。

---

### **3. 计算机视觉中张量的操作**

在 PyTorch / TensorFlow 里，我们常用以下操作：

1. **转换形状（Reshape）**

   ```python
   x = torch.randn(1, 3, 224, 224)  # (Batch, Channel, Height, Width)
   x = x.view(3, 224, 224)  # 去掉 batch 维度

   ```

​
**2.张量归一化（Normalization）**

```
x = x / 255.0  # 将像素值缩放到 [0, 1]

```

​ 3.
**维度变换（Permute / Transpose）**

```
x = x.permute(2, 0, 1)  # 从 (H, W, C) 变为 (C, H, W)

```

​ 4.
**卷积运算（Convolution）**

```
pythonCopyEditconv = torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
output = conv(x)  # 经过 3x3 卷积，得到新的张量

```

### **6. 总结**

* **张量是一种“多维数组”**
  ，可以是 0 维（标量）、1 维（向量）、2 维（矩阵）或更高维。
* **张量的核心是“坐标无关”**
  ，无论你换什么坐标系，它的数学性质都不变。
* **张量在现实世界中无处不在**
  ，在物理学、计算机科学、深度学习等领域都非常重要。

---