---
layout: post
title: "大模型最新面试题系列微调篇之微调框架一"
date: 2025-03-15 19:39:34 +0800
description: "克隆仓库创建虚拟环境安装依赖pip install modelscope -U # 国内用户推荐命令行训练（示例）YAML配置文件（以为例）### model### methodstage: sft### train。"
keywords: "大模型最新面试题系列：微调篇之微调框架（一）"
categories: ['大模型最新面试题集锦大全']
tags: ['面试', '人工智能', 'Llm', 'Ai']
artid: "146280221"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146280221
    alt: "大模型最新面试题系列微调篇之微调框架一"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146280221
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146280221
cover: https://bing.ee123.net/img/rand?artid=146280221
image: https://bing.ee123.net/img/rand?artid=146280221
img: https://bing.ee123.net/img/rand?artid=146280221
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     大模型最新面试题系列：微调篇之微调框架（一）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h4>
     <a id="_DeepSpeedZeRO_1">
     </a>
     一. 在DeepSpeed中配置零冗余优化（ZeRO）实现显存优化的步骤
    </h4>
    <p>
     <strong>
      核心原理
     </strong>
    </p>
    <p>
     ZeRO通过分片（Sharding）技术将模型参数、梯度和优化器状态分布到多卡，消除冗余存储。三个阶段逐步减少显存占用：
    </p>
    <ul>
     <li>
      <strong>
       Stage 1
      </strong>
      ：分片优化器状态（如Adam的动量、方差）
     </li>
     <li>
      <strong>
       Stage 2
      </strong>
      ：分片优化器状态+梯度
     </li>
     <li>
      <strong>
       Stage 3
      </strong>
      ：分片优化器状态+梯度+模型参数
     </li>
    </ul>
    <p>
     <strong>
      配置步骤
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       创建DeepSpeed配置文件
      </strong>
      （以Stage 2为例）：
     </li>
    </ol>
    <pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"zero_optimization"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"stage"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>          <span class="token comment">// 启用Stage 2</span>
    <span class="token string-property property">"overlap_comm"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span><span class="token comment">// 计算与通信重叠加速</span>
    <span class="token string-property property">"contiguous_gradients"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span> <span class="token comment">// 减少梯度碎片</span>
    <span class="token string-property property">"reduce_bucket_size"</span><span class="token operator">:</span> <span class="token string">"auto"</span>  <span class="token comment">// 自动优化通信块大小</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token string-property property">"bf16"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span><span class="token string-property property">"enabled"</span><span class="token operator">:</span> <span class="token string">"auto"</span><span class="token punctuation">}</span> <span class="token comment">// 自动启用混合精度</span>
<span class="token punctuation">}</span>
</code></pre>
    <ol start="2">
     <li>
      <strong>
       启动脚本配置
      </strong>
      ：
     </li>
    </ol>
    <pre><code class="prism language-bash">accelerate launch <span class="token punctuation">\</span>
  <span class="token parameter variable">--use_deepspeed</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--deepspeed_config_file</span> stage2.conf <span class="token punctuation">\</span>
  <span class="token parameter variable">--deepspeed_multinode_launcher</span> standard <span class="token punctuation">\</span>
  train.py <span class="token punctuation">\</span>
  <span class="token parameter variable">--model_name_or_path</span> google/gemma-2-2b <span class="token punctuation">\</span>
  <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">1</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">2</span>
</code></pre>
    <p>
     <strong>
      实战对比
     </strong>
    </p>
    <ul>
     <li>
      1.5B参数模型：Stage1将单卡显存从18GB降至2.25GB
     </li>
     <li>
      10B参数模型：Stage2配合32张V100可完成训练
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_DeepSpeedDPMPPP_41">
     </a>
     二. DeepSpeed三种并行策略（DP/MP/PP）微调配置对比
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        策略
       </th>
       <th>
        分片对象
       </th>
       <th>
        显存占用
       </th>
       <th>
        通信开销
       </th>
       <th>
        适用场景
       </th>
       <th>
        配置重点
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         数据并行(DP)
        </strong>
       </td>
       <td>
        优化器状态
       </td>
       <td>
        高
       </td>
       <td>
        低
       </td>
       <td>
        中小模型
       </td>
       <td>
        <code>
         --deepspeed --zero_optimization
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         模型并行(MP)
        </strong>
       </td>
       <td>
        模型参数
       </td>
       <td>
        中
       </td>
       <td>
        中
       </td>
       <td>
        单卡无法容纳的模型
       </td>
       <td>
        <code>
         --model_parallel_size
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         流水线并行(PP)
        </strong>
       </td>
       <td>
        计算阶段
       </td>
       <td>
        低
       </td>
       <td>
        高
       </td>
       <td>
        超深网络
       </td>
       <td>
        <code>
         --num_layers
        </code>
        + 阶段划分
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     <strong>
      关键差异
     </strong>
    </p>
    <ul>
     <li>
      DP通过分片优化器状态减少冗余
     </li>
     <li>
      MP通过纵向切分模型层实现并行
     </li>
     <li>
      PP通过时间流水化隐藏通信延迟
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_vllmPagedAttention_56">
     </a>
     三. vllm的PagedAttention机制解析
    </h4>
    <p>
     <strong>
      背景与挑战
     </strong>
     <br/>
     在LLM推理中，
     <strong>
      KV缓存
     </strong>
     （Key-Value Cache）用于存储历史Token的注意力键值对，以避免重复计算。传统方法采用
     <strong>
      连续内存分配
     </strong>
     ，但存在以下问题：
    </p>
    <ol>
     <li>
      <strong>
       内存碎片
      </strong>
      ：不同请求的序列长度动态变化，导致内存空洞。
     </li>
     <li>
      <strong>
       预分配浪费
      </strong>
      ：需预留大块连续内存，利用率低（如LLaMA-2 7B在序列长度44K时，内存占用超100GB）。
     </li>
     <li>
      <strong>
       算术强度低
      </strong>
      ：Attention计算为矩阵乘向量，Memory Bound特性显著。
     </li>
    </ol>
    <p>
     <strong>
      PagedAttention核心机制
     </strong>
     <br/>
     受操作系统
     <strong>
      虚拟内存分页技术
     </strong>
     启发，PagedAttention通过以下创新优化内存管理：
    </p>
    <ol>
     <li>
      <strong>
       分块存储（Paged Memory）
      </strong>
     </li>
    </ol>
    <ul>
     <li>
      将KV缓存分割为固定大小的
      <strong>
       页块
      </strong>
      （如256KB），允许非连续存储。
     </li>
     <li>
      每个页块通过**页表（Lookup Table）**映射物理地址，支持动态分配与回收。
     </li>
    </ul>
    <ol start="2">
     <li>
      <strong>
       动态分页管理
      </strong>
     </li>
    </ol>
    <ul>
     <li>
      <strong>
       按需分配
      </strong>
      ：仅为当前生成的Token分配页块，无需预分配连续内存。
     </li>
     <li>
      <strong>
       碎片回收
      </strong>
      ：请求结束后，自动释放页块并加入空闲池。
     </li>
    </ul>
    <ol start="3">
     <li>
      <strong>
       跨请求共享
      </strong>
     </li>
    </ol>
    <ul>
     <li>
      不同请求可共享相同的KV页块（如相同前缀的上下文），减少冗余存储。
     </li>
     <li>
      对并行采样（如Beam Search）友好，共享基础KV数据。
     </li>
    </ul>
    <p>
     <strong>
      技术优势
     </strong>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         指标
        </strong>
       </th>
       <th>
        <strong>
         传统方法
        </strong>
       </th>
       <th>
        <strong>
         PagedAttention
        </strong>
       </th>
       <th>
        <strong>
         提升效果
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        内存利用率
       </td>
       <td>
        仅4%~20%有效使用
       </td>
       <td>
        96%以上
       </td>
       <td>
        降低内存浪费96%
       </td>
      </tr>
      <tr>
       <td>
        批处理能力
       </td>
       <td>
        受限于连续内存分配
       </td>
       <td>
        支持更大Batch Size
       </td>
       <td>
        吞吐量提升30倍（Vicuna案例）
       </td>
      </tr>
      <tr>
       <td>
        KV缓存大小（GQA场景）
       </td>
       <td>
        全量存储
       </td>
       <td>
        按需分配
       </td>
       <td>
        减少75%（如8 KV Head场景）
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     <strong>
      实现细节
     </strong>
    </p>
    <ol>
     <li>
      <p>
       <strong>
        CUDA内核优化
       </strong>
      </p>
      <ul>
       <li>
        采用定制化CUDA内核处理分页逻辑，降低页表访问开销。
       </li>
       <li>
        结合
        <strong>
         Tensor Core
        </strong>
        优化矩阵运算（需匹配页块大小，如16x16对齐）。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        与GQA的协同优化
       </strong>
      </p>
      <ul>
       <li>
        GQA通过减少KV Head数量（如8个KV Head）降低缓存体积。
       </li>
       <li>
        PagedAttention进一步动态管理GQA的KV块，避免连续存储限制。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        页表与内存布局
       </strong>
      </p>
      <pre><code class="prism language-python"><span class="token comment"># 伪代码示例：页表映射</span>
<span class="token keyword">class</span> <span class="token class-name">PageTable</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>page_map <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>  <span class="token comment"># 逻辑页号 → 物理页地址</span>
        self<span class="token punctuation">.</span>free_pages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment"># 空闲页池</span>

    <span class="token keyword">def</span> <span class="token function">allocate_page</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>free_pages<span class="token punctuation">:</span>
            <span class="token keyword">return</span> self<span class="token punctuation">.</span>free_pages<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> new_physical_page<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">free_page</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> page_id<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>free_pages<span class="token punctuation">.</span>append<span class="token punctuation">(</span>page_id<span class="token punctuation">)</span>
</code></pre>
     </li>
    </ol>
    <p>
     <strong>
      实际应用与性能验证
     </strong>
    </p>
    <ul>
     <li>
      <strong>
       vLLM框架集成
      </strong>
      ：通过
      <code>
       PagedAttention
      </code>
      类实现，支持Hugging Face模型（如Llama-2、Mistral）。
     </li>
     <li>
      <strong>
       吞吐量对比
      </strong>
      ：
      <ul>
       <li>
        单A100 GPU处理Llama-2 7B，传统方法吞吐量15 tokens/s，PagedAttention提升至450 tokens/s。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       内存节省
      </strong>
      ：在序列长度16K时，内存占用从120GB降至25GB。
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_llamafactory_124">
     </a>
     四. llama-factory自定义微调流水线构建步骤
    </h4>
    <h5>
     <a id="1_126">
     </a>
     1、环境准备与安装
    </h5>
    <p>
     <strong>
      克隆仓库
     </strong>
    </p>
    <pre><code class="prism language-bash"><span class="token function">git</span> clone <span class="token parameter variable">--depth</span> <span class="token number">1</span> https://github.com/hiyouga/LLaMA-Factory.git
<span class="token builtin class-name">cd</span> LLaMA-Factory
</code></pre>
    <p>
     <strong>
      创建虚拟环境
     </strong>
    </p>
    <pre><code class="prism language-bash">conda create <span class="token parameter variable">-n</span> llama_factory <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.10</span>
conda activate llama_factory
</code></pre>
    <p>
     <strong>
      安装依赖
     </strong>
    </p>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> <span class="token parameter variable">-e</span> <span class="token string">".(torch,metrics)"</span>
pip <span class="token function">install</span> modelscope <span class="token parameter variable">-U</span>  <span class="token comment"># 国内用户推荐</span>
</code></pre>
    <h5>
     <a id="2_146">
     </a>
     2、数据集准备
    </h5>
    <p>
     <strong>
      数据格式规范
     </strong>
     <br/>
     需遵循
     <code>
      alpaca_zh_demo.json
     </code>
     格式，示例如下：
    </p>
    <pre><code class="prism language-json"><span class="token punctuation">[</span>
  <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"instruction"</span><span class="token operator">:</span> <span class="token string">"写一个Python函数计算斐波那契数列"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"input"</span><span class="token operator">:</span> <span class="token string">""</span><span class="token punctuation">,</span>
    <span class="token string-property property">"output"</span><span class="token operator">:</span> <span class="token string">"def fibonacci(n):\n    if n &lt;= 0:\n        return []\n    elif n == 1:\n        return [0]\n    ..."</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">]</span>
</code></pre>
    <p>
     <strong>
      注册数据集
     </strong>
     <br/>
     在
     <code>
      dataset_info.json
     </code>
     中添加自定义数据集信息：
    </p>
    <pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
  <span class="token string-property property">"guihua_ner"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"path"</span><span class="token operator">:</span> <span class="token string">"data/guihua_ner.json"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"instruction_column"</span><span class="token operator">:</span> <span class="token string">"instruction"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"input_column"</span><span class="token operator">:</span> <span class="token string">"input"</span><span class="token punctuation">,</span>
    <span class="token string-property property">"output_column"</span><span class="token operator">:</span> <span class="token string">"output"</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
    <h5>
     <a id="3_173">
     </a>
     3、模型下载与校验
    </h5>
    <p>
     <strong>
      国内镜像下载
     </strong>
     （推荐）
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> modelscope <span class="token keyword">import</span> snapshot_download
model_dir <span class="token operator">=</span> snapshot_download<span class="token punctuation">(</span><span class="token string">"LLM-Research/Meta-Llama-3-8B-Instruct"</span><span class="token punctuation">,</span> cache_dir<span class="token operator">=</span><span class="token string">"models/"</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      模型可用性验证
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">)</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_dir<span class="token punctuation">,</span> device_map<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">)</span>
text <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"你好！"</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
output <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>text<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="4_191">
     </a>
     4、自定义训练配置
    </h5>
    <p>
     <strong>
      命令行训练（示例）
     </strong>
    </p>
    <pre><code class="prism language-bash">llamafactory-cli train <span class="token punctuation">\</span>
  <span class="token parameter variable">--stage</span> sft <span class="token punctuation">\</span>
  <span class="token parameter variable">--model_name_or_path</span> qwen/Qwen2.5-7B-Instruct <span class="token punctuation">\</span>
  <span class="token parameter variable">--dataset</span> alpaca_zh_demo <span class="token punctuation">\</span>
  <span class="token parameter variable">--finetuning_type</span> lora <span class="token punctuation">\</span>
  <span class="token parameter variable">--lora_rank</span> <span class="token number">8</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--learning_rate</span> 5e-5 <span class="token punctuation">\</span>
  <span class="token parameter variable">--per_device_train_batch_size</span> <span class="token number">2</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--gradient_accumulation_steps</span> <span class="token number">8</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--output_dir</span> saves/custom_model <span class="token punctuation">\</span>
  <span class="token parameter variable">--bf16</span> True
</code></pre>
    <p>
     <strong>
      YAML配置文件
     </strong>
     （以
     <code>
      qwen2.5-7B-ner.yaml
     </code>
     为例）
    </p>
    <pre><code class="prism language-yaml"><span class="token comment">### model</span>
<span class="token key atrule">model_name_or_path</span><span class="token punctuation">:</span> qwen/Qwen2.5<span class="token punctuation">-</span>7B<span class="token punctuation">-</span>Instruct

<span class="token comment">### method</span>
<span class="token key atrule">stage</span><span class="token punctuation">:</span> sft
<span class="token key atrule">do_train</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
<span class="token key atrule">finetuning_type</span><span class="token punctuation">:</span> lora
<span class="token key atrule">lora_target</span><span class="token punctuation">:</span> query_key_value

<span class="token comment">### dataset</span>
<span class="token key atrule">dataset</span><span class="token punctuation">:</span> guihua_ner
<span class="token key atrule">template</span><span class="token punctuation">:</span> qwen
<span class="token key atrule">cutoff_len</span><span class="token punctuation">:</span> <span class="token number">2048</span>

<span class="token comment">### train</span>
<span class="token key atrule">per_device_train_batch_size</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">gradient_accumulation_steps</span><span class="token punctuation">:</span> <span class="token number">16</span>
</code></pre>
    <h5>
     <a id="5_229">
     </a>
     5、训练与监控
    </h5>
    <p>
     <strong>
      启动Web UI
     </strong>
    </p>
    <pre><code class="prism language-bash">llamafactory-cli webui
</code></pre>
    <ul>
     <li>
      在浏览器中访问
      <code>
       http://localhost:7860
      </code>
      ，通过可视化界面管理数据集、配置训练参数并监控进度。
     </li>
    </ul>
    <p>
     <strong>
      关键参数说明
     </strong>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        参数
       </th>
       <th>
        说明
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <code>
         lora_rank
        </code>
       </td>
       <td>
        LoRA低秩矩阵的秩，控制参数量（推荐8-16）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         learning_rate
        </code>
       </td>
       <td>
        学习率（通常5e-5~1e-4）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         cutoff_len
        </code>
       </td>
       <td>
        截断序列长度，避免内存溢出（建议2048~4096）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         gradient_checkpointing
        </code>
       </td>
       <td>
        启用梯度检查点，节省显存（推荐开启）
       </td>
      </tr>
     </tbody>
    </table>
    <h5>
     <a id="6_245">
     </a>
     6、模型推理与合并
    </h5>
    <p>
     <strong>
      加载LoRA模型
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> PeftModel
model <span class="token operator">=</span> PeftModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>base_model<span class="token punctuation">,</span> <span class="token string">"saves/custom_model/lora"</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      动态合并权重
     </strong>
     （可选）
    </p>
    <pre><code class="prism language-bash">llamafactory-cli merge_lora <span class="token punctuation">\</span>
  <span class="token parameter variable">--base_model_path</span> qwen/Qwen2.5-7B-Instruct <span class="token punctuation">\</span>
  <span class="token parameter variable">--lora_path</span> saves/custom_model/lora <span class="token punctuation">\</span>
  <span class="token parameter variable">--output_path</span> saves/merged_model
</code></pre>
    <h5>
     <a id="7_261">
     </a>
     7、部署与测试
    </h5>
    <p>
     <strong>
      启动API服务
     </strong>
    </p>
    <pre><code class="prism language-bash">llamafactory-cli serve <span class="token punctuation">\</span>
  <span class="token parameter variable">--model_name_or_path</span> saves/merged_model <span class="token punctuation">\</span>
  <span class="token parameter variable">--device</span> cuda:0 <span class="token punctuation">\</span>
  <span class="token parameter variable">--port</span> <span class="token number">8080</span>
</code></pre>
    <p>
     <strong>
      性能评估
     </strong>
    </p>
    <pre><code class="prism language-bash">llamafactory-cli evaluate <span class="token punctuation">\</span>
  <span class="token parameter variable">--model_name_or_path</span> saves/merged_model <span class="token punctuation">\</span>
  <span class="token parameter variable">--benchmark</span> datasets/gsm8k <span class="token punctuation">\</span>
  <span class="token parameter variable">--output_file</span> results.json
</code></pre>
    <hr/>
    <h4>
     <a id="_UnslothTextGenerationInference_280">
     </a>
     五. Unsloth与Text-Generation-Inference部署微调模型的性能差异
    </h4>
    <p>
     <strong>
      核心差异对比
     </strong>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        维度
       </th>
       <th>
        Unsloth
       </th>
       <th>
        Text-Generation-Inference
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         训练速度
        </strong>
       </td>
       <td>
        提升2-5倍（如Llama-3/Qwen2等模型）
       </td>
       <td>
        传统微调速度，无显著加速优化
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         显存占用
        </strong>
       </td>
       <td>
        减少70%，支持4位预量化模型
       </td>
       <td>
        依赖标准FP16/FP32，显存占用较高
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         硬件兼容性
        </strong>
       </td>
       <td>
        支持Nvidia/H100、AMD、Intel GPU
       </td>
       <td>
        主要适配Nvidia GPU
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         内存优化
        </strong>
       </td>
       <td>
        智能权重上投技术（减少QLoRA冗余）
       </td>
       <td>
        常规内存管理策略
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     <strong>
      实战场景
     </strong>
     <br/>
     在H100集群上微调13B模型时，Unsloth通过动态内存分配和4位量化，可在显存限制下同时训练2个实例，而Text-Generation-Inference因显存不足只能单实例运行。
    </p>
    <hr/>
    <h4>
     <a id="_DeepSpeed_294">
     </a>
     六. DeepSpeed断点续训实现方法
    </h4>
    <p>
     <strong>
      流程图
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d92af5e60e784bac9777c917e6e4f9e2.png">
      <br/>
      <strong>
       核心原理
      </strong>
      <br/>
      DeepSpeed 通过保存检查点（Checkpoint）实现断点续训，包含以下关键组件：
      <br/>
      模型参数：所有可训练参数的状态
      <br/>
      优化器状态：梯度累积、动量等信息
      <br/>
      训练元数据：当前轮次、学习率、迭代次数等
     </img>
    </p>
    <p>
     <strong>
      实现过程：
     </strong>
    </p>
    <p>
     <strong>
      配置检查点保存
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       基本参数设置
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-bash">deepspeed <span class="token parameter variable">--deepspeed_checkpoint_interval</span><span class="token operator">=</span><span class="token number">100</span> <span class="token punctuation">\</span>
          <span class="token parameter variable">--deepspeed_checkpoint_path</span><span class="token operator">=</span>./checkpoints <span class="token punctuation">\</span>
          <span class="token parameter variable">--deepspeed</span> <span class="token punctuation">\</span>
          train.py
</code></pre>
    <ol start="2">
     <li>
      <strong>
       高级配置（示例）
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-yaml"><span class="token comment"># deepspeed_config.json</span>
<span class="token punctuation">{<!-- --></span>
  <span class="token key atrule">"checkpoint"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token key atrule">"enable"</span><span class="token punctuation">:</span> <span class="token boolean important">true</span><span class="token punctuation">,</span>
    <span class="token key atrule">"saving_interval"</span><span class="token punctuation">:</span> <span class="token number">100</span><span class="token punctuation">,</span>
    <span class="token key atrule">"path"</span><span class="token punctuation">:</span> <span class="token string">"./checkpoints"</span><span class="token punctuation">,</span>
    <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"all"</span><span class="token punctuation">,</span>
    <span class="token key atrule">"max_to_keep"</span><span class="token punctuation">:</span> <span class="token number">5</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token key atrule">"gradient_clipping"</span><span class="token punctuation">:</span> <span class="token number">1.0</span>
<span class="token punctuation">}</span>
</code></pre>
    <p>
     <strong>
      恢复训练流程
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       标准恢复命令
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-bash">deepspeed <span class="token parameter variable">--deepspeed_restore</span><span class="token operator">=</span>./checkpoints/global_step1000/ <span class="token punctuation">\</span>
          <span class="token parameter variable">--deepspeed</span> <span class="token punctuation">\</span>
          train.py
</code></pre>
    <ol start="2">
     <li>
      <strong>
       动态恢复逻辑
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token comment"># 训练脚本检测</span>
<span class="token keyword">if</span> args<span class="token punctuation">.</span>deepspeed_restore<span class="token punctuation">:</span>
    model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>
        args<span class="token operator">=</span>args<span class="token punctuation">,</span>
        model<span class="token operator">=</span>model<span class="token punctuation">,</span>
        optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
        model_parameters<span class="token operator">=</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        restore<span class="token operator">=</span>args<span class="token punctuation">.</span>deepspeed_restore
    <span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>
        args<span class="token operator">=</span>args<span class="token punctuation">,</span>
        model<span class="token operator">=</span>model<span class="token punctuation">,</span>
        optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
        model_parameters<span class="token operator">=</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      关键验证方法
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       检查点完整性验证
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token comment"># 加载验证脚本</span>
<span class="token keyword">from</span> deepspeed<span class="token punctuation">.</span>checkpoint_management <span class="token keyword">import</span> CheckpointLoader

checkpoint <span class="token operator">=</span> CheckpointLoader<span class="token punctuation">.</span>load_checkpoint<span class="token punctuation">(</span>
    ckpt_dir<span class="token operator">=</span>args<span class="token punctuation">.</span>deepspeed_restore<span class="token punctuation">,</span>
    client_state<span class="token operator">=</span><span class="token punctuation">{<!-- --></span><span class="token string">'epoch'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span>
</code></pre>
    <ol start="2">
     <li>
      <strong>
       状态参数对比表
      </strong>
      |
     </li>
    </ol>
    <table>
     <thead>
      <tr>
       <th>
        检查项
       </th>
       <th>
        验证方法
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        模型参数一致性
       </td>
       <td>
        <code>
         torch.allclose(model.state_dict(), checkpoint['module'])
        </code>
       </td>
      </tr>
      <tr>
       <td>
        优化器状态连续性
       </td>
       <td>
        <code>
         torch.allclose(optimizer.state_dict()['state'], checkpoint['optimizer'])
        </code>
       </td>
      </tr>
      <tr>
       <td>
        训练进度匹配
       </td>
       <td>
        <code>
         checkpoint['epoch'] == last_epoch
        </code>
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     <strong>
      分布式训练注意事项
     </strong>
     <br/>
     4.
     <strong>
      多节点同步策略
     </strong>
    </p>
    <pre><code class="prism language-bash"><span class="token comment"># 使用NFS共享存储</span>
deepspeed <span class="token parameter variable">--checkpoint_path</span><span class="token operator">=</span>/nfs/checkpoints <span class="token punctuation">\</span>
          <span class="token parameter variable">--hostfile</span> hosts.txt <span class="token punctuation">\</span>
          train.py
</code></pre>
    <ol start="2">
     <li>
      <strong>
       混合精度训练兼容
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-bash">deepspeed <span class="token parameter variable">--bf16</span> <span class="token punctuation">\</span>
          <span class="token parameter variable">--deepspeed_checkpoint_path</span><span class="token operator">=</span>./bf16_checkpoints <span class="token punctuation">\</span>
          train.py
</code></pre>
    <p>
     <strong>
      常见问题解决方案
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       检查点损坏修复
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-bash"><span class="token comment"># 单卡修复模式</span>
deepspeed <span class="token parameter variable">--deepspeed_restore</span><span class="token operator">=</span>./corrupted_ckpt/ <span class="token punctuation">\</span>
          <span class="token parameter variable">--repair</span> <span class="token punctuation">\</span>
          train.py
</code></pre>
    <ol start="2">
     <li>
      <strong>
       版本兼容性处理
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token comment"># 向下兼容配置</span>
model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> deepspeed<span class="token punctuation">.</span>initialize<span class="token punctuation">(</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    optimizer<span class="token operator">=</span>optimizer<span class="token punctuation">,</span>
    load_optimizer_states<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    load_lr_scheduler_states<span class="token operator">=</span><span class="token boolean">False</span>
<span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      性能优化建议
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       存储优化策略
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-yaml"><span class="token comment"># 使用SSD存储</span>
<span class="token key atrule">"checkpoint"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
  <span class="token key atrule">"io_config"</span><span class="token punctuation">:</span> <span class="token punctuation">{<!-- --></span>
    <span class="token key atrule">"type"</span><span class="token punctuation">:</span> <span class="token string">"LocalIO"</span><span class="token punctuation">,</span>
    <span class="token key atrule">"path"</span><span class="token punctuation">:</span> <span class="token string">"/ssd/checkpoints"</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
    <ol start="2">
     <li>
      <strong>
       增量保存配置
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-bash">deepspeed <span class="token parameter variable">--deepspeed_checkpoint_type</span><span class="token operator">=</span>incremental <span class="token punctuation">\</span>
          <span class="token parameter variable">--deepspeed_checkpoint_interval</span><span class="token operator">=</span><span class="token number">50</span> <span class="token punctuation">\</span>
          train.py
</code></pre>
    <hr/>
    <h4>
     <a id="_VLLM_434">
     </a>
     七. VLLM异步推理架构解析
    </h4>
    <p>
     <strong>
      架构示意图
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fa5d823853a34e448afb494b93e41502.png"/>
    </p>
    <h4>
     <a id="VLLM__442">
     </a>
     VLLM 架构解析解读
    </h4>
    <p>
     VLLM（High-Throughput Large Language Model Serving）架构旨在优化大语言模型推理服务的性能，其核心组件及交互逻辑如下：
    </p>
    <p>
     <strong>
      Scheduler（调度器）
     </strong>
    </p>
    <ul>
     <li>
      作为架构的“总控中心”，负责协调任务分配，将推理任务分发给多个 Worker 节点，实现任务的高效调度与资源分配，确保系统整体运行的有序性。
     </li>
    </ul>
    <p>
     <strong>
      KV Cache Manager（键值缓存管理器）
     </strong>
    </p>
    <ul>
     <li>
      管理大语言模型推理关键的 KV 缓存（用于存储注意力机制中的键值对），通过
      <strong>
       Block tables（块表）
      </strong>
      组织缓存块，记录缓存块的使用状态。
     </li>
     <li>
      对接
      <strong>
       CPU Block Allocator
      </strong>
      和
      <strong>
       GPU Block Allocator
      </strong>
      ，根据内存资源情况，灵活分配 CPU/GPU 内存块，优化缓存的存储与访问效率，减少重复计算。
     </li>
    </ul>
    <p>
     <strong>
      Worker 节点
     </strong>
    </p>
    <ul>
     <li>
      包含多个 Worker（如 Worker 0、Worker 1 至 Worker N-1），每个 Worker 负责模型分片（
      <strong>
       Model Shard
      </strong>
      ）的推理计算，实现模型并行。
     </li>
     <li>
      内置
      <strong>
       Cache Engine
      </strong>
      ，用于处理 KV 缓存的读写，配合 KV Cache Manager 完成缓存管理，提升推理过程中数据访问的速度。
     </li>
    </ul>
    <p>
     <strong>
      CPU/GPU Block Allocator（内存块分配器）
     </strong>
    </p>
    <ul>
     <li>
      分别负责 CPU 和 GPU 内存块的分配与管理，根据 KV Cache Manager 的指令，动态分配内存资源，确保缓存块合理存储，提升内存利用率。
     </li>
    </ul>
    <p>
     <strong>
      架构核心优势
     </strong>
    </p>
    <ul>
     <li>
      通过 Scheduler 的任务调度、KV Cache Manager 的缓存优化，结合 Worker 节点的模型分片并行计算，VLLM 实现了推理任务的高效调度、内存资源的精细化管理，以及模型推理的并行加速，最终提升大语言模型服务的吞吐量与性能。
     </li>
    </ul>
    <p>
     <strong>
      吞吐量提升原理
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       请求批处理
      </strong>
      ：将多个请求合并推理（类似批处理Batching）
     </li>
     <li>
      <strong>
       动态显存复用
      </strong>
      ：利用K/V Cache共享机制减少内存占用
     </li>
     <li>
      <strong>
       非阻塞IO
      </strong>
      ：请求处理与数据传输异步进行
     </li>
     <li>
      <strong>
       优先级调度
      </strong>
      ：高优先级请求优先分配资源
     </li>
    </ol>
    <p>
     <strong>
      性能对比
     </strong>
     <br/>
     在A100上部署Llama2-70B时，VLLM异步推理吞吐量达320tokens/s，比同步推理提升4.2倍。
    </p>
    <hr/>
    <h4>
     <a id="_LoRALLaMA2_472">
     </a>
     八. LoRA增量训练LLaMA2实践指南
    </h4>
    <p>
     <strong>
      实施步骤
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       参数冻结
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    param<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre>
    <ol start="2">
     <li>
      <strong>
       插入LoRA模块
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig<span class="token punctuation">,</span> get_peft_model

config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    r<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    lora_alpha<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>
    target_modules<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"q_proj"</span><span class="token punctuation">,</span> <span class="token string">"v_proj"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    lora_dropout<span class="token operator">=</span><span class="token number">0.1</span>
<span class="token punctuation">)</span>
model <span class="token operator">=</span> get_peft_model<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token punctuation">)</span>
</code></pre>
    <ol start="3">
     <li>
      <strong>
       训练配置
      </strong>
     </li>
    </ol>
    <pre><code class="prism language-python">training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">"./lora_llama2"</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-4</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      关键优势
     </strong>
    </p>
    <ul>
     <li>
      显存占用减少90%（70B模型仅需32GB显存）
     </li>
     <li>
      训练速度提升3倍（因参数更新量减少）
     </li>
     <li>
      支持动态Rank调整（
      <code>
       adaptive_rank=True
      </code>
      ）
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_API_511">
     </a>
     九. 微调框架API设计对比
    </h4>
    <p>
     <strong>
      核心特性对比
     </strong>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        特性
       </th>
       <th>
        Hugging Face Trainer
       </th>
       <th>
        DeepSpeed
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         易用性
        </strong>
       </td>
       <td>
        高度封装，适合快速实验
       </td>
       <td>
        灵活配置，适合深度优化
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         分布式支持
        </strong>
       </td>
       <td>
        自动数据并行（DDP）
       </td>
       <td>
        混合并行（ZeRO/TP/PP）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         内存优化
        </strong>
       </td>
       <td>
        基础梯度累积
       </td>
       <td>
        优化内存优化（ZeRO Stage3）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         Checkpoint机制
        </strong>
       </td>
       <td>
        保存完整模型/优化器状态
       </td>
       <td>
        增量保存差异参数
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         回调扩展
        </strong>
       </td>
       <td>
        插件式设计（CallbackHandler）
       </td>
       <td>
        自定义钩子（Hook System）
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     <strong>
      实战建议
     </strong>
    </p>
    <ul>
     <li>
      快速验证场景：优先使用Hugging Face Trainer
     </li>
     <li>
      资源受限场景：选择DeepSpeed ZeRO-3
     </li>
     <li>
      混合精度需求：两者均支持，但DeepSpeed提供更细粒度控制
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="_unslothINT8_528">
     </a>
     十. 如何在unsloth中配置模型量化（如INT8）来加速推理，具体配置方法是什么？
    </h4>
    <h5>
     <a id="_530">
     </a>
     量化核心优势
    </h5>
    <ul>
     <li>
      <strong>
       速度提升
      </strong>
      ：INT8推理速度比FP16快2-3倍（实测A100加速2.8倍）
     </li>
     <li>
      <strong>
       显存节省
      </strong>
      ：模型大小减少50%（如70B模型从140GB→70GB）
     </li>
     <li>
      <strong>
       精度保持
      </strong>
      ：通过混合量化策略（INT8+FP16），精度损失&lt;1%
     </li>
    </ul>
    <h5>
     <a id="_535">
     </a>
     动态量化配置（推荐）
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> unsloth <span class="token keyword">import</span> QuantizedModel

<span class="token comment"># 加载原始模型（如LLaMA-2-13B）</span>
model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">"llama-2-13b"</span><span class="token punctuation">)</span>

<span class="token comment"># 配置动态INT8量化</span>
quant_config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"bits"</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
    <span class="token string">"group_size"</span><span class="token punctuation">:</span> <span class="token number">128</span><span class="token punctuation">,</span>  <span class="token comment"># 分组大小影响精度，越大越接近原始精度</span>
    <span class="token string">"dtype"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>int8<span class="token punctuation">,</span>
    <span class="token string">"exclude_modules"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"lm_head"</span><span class="token punctuation">]</span>  <span class="token comment"># 输出层通常保留FP32</span>
<span class="token punctuation">}</span>

<span class="token comment"># 生成量化模型</span>
quant_model <span class="token operator">=</span> QuantizedModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token operator">=</span>quant_config<span class="token punctuation">)</span>

<span class="token comment"># 推理验证</span>
input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token string">"你好，世界"</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">"pt"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>input_ids
output <span class="token operator">=</span> quant_model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>input_ids<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_558">
     </a>
     静态量化配置（高精度需求）
    </h5>
    <pre><code class="prism language-python"><span class="token comment"># 准备校准数据集（500-1000条代表性样本）</span>
calibration_dataset <span class="token operator">=</span> load_calibration_data<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 静态量化配置</span>
quant_config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"method"</span><span class="token punctuation">:</span> <span class="token string">"static"</span><span class="token punctuation">,</span>
    <span class="token string">"calibration_samples"</span><span class="token punctuation">:</span> calibration_dataset<span class="token punctuation">,</span>
    <span class="token string">"calibration_batch_size"</span><span class="token punctuation">:</span> <span class="token number">8</span><span class="token punctuation">,</span>
    <span class="token string">"per_channel"</span><span class="token punctuation">:</span> <span class="token boolean">True</span>  <span class="token comment"># 通道级量化更精细</span>
<span class="token punctuation">}</span>

<span class="token comment"># 生成静态量化模型</span>
quant_model <span class="token operator">=</span> QuantizedModel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> config<span class="token operator">=</span>quant_config<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_575">
     </a>
     关键参数解析
    </h5>
    <table>
     <thead>
      <tr>
       <th>
        参数名称
       </th>
       <th>
        作用说明
       </th>
       <th>
        推荐值范围
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <code>
         bits
        </code>
       </td>
       <td>
        量化位数（支持4/8）
       </td>
       <td>
        8（平衡速度与精度）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         group_size
        </code>
       </td>
       <td>
        量化分组大小（越大精度损失越小）
       </td>
       <td>
        64-256
       </td>
      </tr>
      <tr>
       <td>
        <code>
         exclude_modules
        </code>
       </td>
       <td>
        需跳过量化的模块（如输出层、嵌入层）
       </td>
       <td>
        [“lm_head”, “embed”]
       </td>
      </tr>
      <tr>
       <td>
        <code>
         calibration_samples
        </code>
       </td>
       <td>
        静态量化所需校准样本数量
       </td>
       <td>
        500-1000
       </td>
      </tr>
     </tbody>
    </table>
    <h5>
     <a id="_583">
     </a>
     性能优化技巧
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        混合精度量化
       </strong>
      </p>
      <pre><code class="prism language-python">quant_config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"modules_to_keep_fp16"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"value"</span><span class="token punctuation">]</span>  <span class="token comment"># 保留关键层FP16</span>
<span class="token punctuation">}</span>
</code></pre>
     </li>
     <li>
      <p>
       <strong>
        推理加速配置
       </strong>
      </p>
      <pre><code class="prism language-python">inference_config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token string">"max_batch_size"</span><span class="token punctuation">:</span> <span class="token number">32</span><span class="token punctuation">,</span>
    <span class="token string">"max_tokens"</span><span class="token punctuation">:</span> <span class="token number">2048</span><span class="token punctuation">,</span>
    <span class="token string">"use_cuda_graph"</span><span class="token punctuation">:</span> <span class="token boolean">True</span>  <span class="token comment"># 加速重复推理模式</span>
<span class="token punctuation">}</span>
</code></pre>
     </li>
    </ol>
    <h5>
     <a id="_600">
     </a>
     验证与调优
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        速度测试
       </strong>
      </p>
      <pre><code class="prism language-python"><span class="token keyword">from</span> unsloth <span class="token keyword">import</span> benchmark

<span class="token comment"># 测量吞吐量（tokens/s）</span>
throughput <span class="token operator">=</span> benchmark<span class="token punctuation">(</span>quant_model<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> max_tokens<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"INT8 Throughput: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>throughput<span class="token punctuation">}</span></span><span class="token string"> tokens/s"</span></span><span class="token punctuation">)</span>
</code></pre>
     </li>
     <li>
      <p>
       <strong>
        精度对比
       </strong>
      </p>
      <pre><code class="prism language-python"><span class="token keyword">from</span> evaluate <span class="token keyword">import</span> load

metric <span class="token operator">=</span> load<span class="token punctuation">(</span><span class="token string">"accuracy"</span><span class="token punctuation">)</span>
results <span class="token operator">=</span> metric<span class="token punctuation">.</span>compute<span class="token punctuation">(</span>
    predictions<span class="token operator">=</span>quant_model_outputs<span class="token punctuation">,</span>
    references<span class="token operator">=</span>ground_truth
<span class="token punctuation">)</span>
</code></pre>
     </li>
    </ol>
    <h5>
     <a id="_621">
     </a>
     典型案例对比
    </h5>
    <table>
     <thead>
      <tr>
       <th>
        模型
       </th>
       <th>
        量化方式
       </th>
       <th>
        显存占用（A100）
       </th>
       <th>
        推理延迟（ms/样本）
       </th>
       <th>
        BLEU得分
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        LLaMA2-7B
       </td>
       <td>
        FP16
       </td>
       <td>
        28GB
       </td>
       <td>
        12.3
       </td>
       <td>
        32.1
       </td>
      </tr>
      <tr>
       <td>
        LLaMA2-7B
       </td>
       <td>
        INT8
       </td>
       <td>
        14GB
       </td>
       <td>
        5.8
       </td>
       <td>
        31.9
       </td>
      </tr>
      <tr>
       <td>
        Qwen-14B
       </td>
       <td>
        FP16
       </td>
       <td>
        56GB
       </td>
       <td>
        25.7
       </td>
       <td>
        34.5
       </td>
      </tr>
      <tr>
       <td>
        Qwen-14B
       </td>
       <td>
        INT8
       </td>
       <td>
        28GB
       </td>
       <td>
        10.2
       </td>
       <td>
        34.2
       </td>
      </tr>
     </tbody>
    </table>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f67:2e6373646e2e6e65742f7a68616e677a68656e74697965732f:61727469636c652f64657461696c732f313436323830323231" class_="artid" style="display:none">
 </p>
</div>


