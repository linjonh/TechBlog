---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36323839343637372f:61727469636c652f64657461696c732f313436323133303131"
layout: post
title: "自然语言处理预训练模型的研究综述"
date: 2025-03-12 20:51:09 +08:00
description: "第一个任务是Mask LM（MLM），为了解决GPT完全舍弃下文的问题，不再进行整个句子的预测而是对某个词去做预测，首先屏蔽一定百分比的词，然后通过模型实现对屏蔽词的预测，来进行训练。二是预测的是屏蔽掉的是词而非句子，会使整个句子预训练的收敛速度更慢。N-gram是自然语言处理领域中具有显著历史意义的特征处理模型，基本思想是将文本内容按照字节大小为N的滑动窗口进行操作，形成长度是N的字节片段序列，然后对所有的序列的出现频度进行统计，并且按照实现设定好的阈值进行过滤，形成了这个文本的特征向量空间。"
keywords: "自然语言处理预训练模型的研究综述"
categories: ['超级瓦利']
tags: ['自然语言处理', '人工智能']
artid: "146213011"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146213011
    alt: "自然语言处理预训练模型的研究综述"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146213011
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146213011
cover: https://bing.ee123.net/img/rand?artid=146213011
image: https://bing.ee123.net/img/rand?artid=146213011
img: https://bing.ee123.net/img/rand?artid=146213011
---

# 自然语言处理预训练模型的研究综述

📕参考：：2020-11-02,https://kns.cnki.net/kcms/detail/11.2127.tp.20201030.1952.017.html

主要是这篇文章的自己摘了点笔记。

---

预训练模型的深度学目标是如何使预训练好的模型处于良好的初始状态，在下游任务中达到更好的性能表现。对预训练技术及其发展历史进行介绍，并按照模型特点划分为基于概率统计的传统模型和基于习的新式模型进行综述。

## 1.预训练技术的概述

预训练技术是指预先设计网络结构，并对编码后的数据输入到网络结构中进行训练，增加模型的泛化能力。

预训练后的模型可以直接根据下游任务需求进行微调，免去了从零开始的过程。

语言模型可以看作是一串词序列的概率分布
，因此在早期，研究人员提出了N-gram模型，它是
**基于统计语言模型的算法**
，但是这种做法
**只能通过概率统计**
进行判断，会使数据出现严重的稀疏性，
**无法考虑词内部的关联**
。

随着深度学习技术的迅速发展，
**词嵌入**
正式登上历史的舞台，Bengio等人在2003年提出NNLM模型[4]，随后出现了一
**系列词向量技术（如Word2Vec、Glove、FastTest等**
）为文本提供了一种数值化的表示方法，但是
**无法解决一词多义的问题**
。

于是ELMo应运而生，它采用双向的长短期记忆网络（Long Short-Term Memory，
  
LSTM）进行进行预训练，
**将词向量由静态转化为动态，使其可以结合上下文来赋予词义**
。

GPT首次提出了无监督的预训练和有监督的微调，使得训练好的模型能够更好地适应下游任务。BERT首次将双向Transformer用于语言模型，使得该模型相对GPT对语境的理解会更加深刻。

## 2.预训练技术模型简介

在NLP领域上，其发展趋势可以概括为三阶段：规则-统计-深度学习。

**基于规则的**

一开始，
**研究人员的研究的重点放在如何设定语言规则上面**
，但是这个阶段不能处理复杂
  
的语言问题，因此没有太大的应用价值。

**统计语言模型**

统计语言模型是基于语料库对语句进行预处理，然后对下游任务进行基于概率的判别。

N-gram是自然语言处理领域中具有显著历史意义的特征处理模型，基本思想是将文本内容按照字节大小为N的滑动窗口进行操作，形成长度是N的字节片段序列，然后对所有的序列的出现频度进行统计，并且按照实现设定好的阈值进行过滤，形成了这个文本的特征向量空间。然后用训练后的模型对语句进行概率评估，来判断组成是否合理。

N-gram模型是对文本特征进行预处理，它是将相邻的n个词作为一组进行编码，这就导致它过于依赖训练语料的丰富程度，否则就很容易出现数据稀疏问题，并且计算复杂度会随着n的增加而产生指数级的增长。

**传统预训练模型**

Word2Vec模型的计算复杂度和选取的窗口大小无关，而是由词典大小和词向量维度来决定。但是静态的词向量对一词多义等问题仍然无法解决，仍然属于浅层语言模型。

尽管Word2Vec极大改善了对文本的处理任务，但是难以捕获上下文的高级概念，如多义词消除、句法结构等问题。

Word2Vec提出了两个新的模型体系结构：Skip-gram和CBOW，其中Skip-gram模型是通过输入特定词向量，输出上下文对应的词向量。CBOW是通过输入某一特征词的上下文对应词向量来输出特定向量。
两个模型基本思想相似，都是通过训练学习得到权重矩阵，根据矩阵得到文本中词所对应的词向量
，节省了再次训练的时间和资源。

**基于深度学习的预训练模型**

ELMo

ELMo等动态预训练模型的提出很好地解决了这些问题，比如ELMo中的词向量不再是简单的向量对应关系，而是通过前后语境对多义词进行理解，用其内部函数来表达。

ELMo是基于特征的语言模型，可以结合上下文语境对词进行建模。ELMo中词向量表示的是内部网络状态函数，
**对于相同的词它所展现出来的词向量是动态变化的**
。

![](https://i-blog.csdnimg.cn/direct/d587c3faed2f41e39a8cb1f97a7498e2.png)

它首先采用双向LSTM进行预训练，这个模型包括前向LSTM模型和后向LSTM模型
![](https://i-blog.csdnimg.cn/direct/ca393fa7546c4e309a88c906e0e15616.png)
![](https://i-blog.csdnimg.cn/direct/284b6a143c2d4a14b987a456eb1d48b2.png)
、

> 前向公式：前k-1个词预测第k个词。
>
> 后向公式：已知第k个词后面的词，预测第k个词。

![](https://i-blog.csdnimg.cn/direct/8f6ae4ec2e184b5a89a33b50ebcb9936.png)

![](https://i-blog.csdnimg.cn/direct/13fc0c4be33549a6952df00b612f2875.png)

GPT模型

GPT首次无监督的预训练和有监督的微调相结合，使得模型更加符合下游任务的需求。，GPT针对NLP下游任务采用统一框架，直接在Transformer[11]上的最后一层接上softmax作为任务输出层，减少了计算复杂度。

![](https://i-blog.csdnimg.cn/direct/d6f95ab239cb4d15ba2e0d492faeb834.png)

GPT的训练过程也包括两个阶段，第一阶段是在大型文本语料库上对模型进行预训练，第二阶段是微调阶段，让模型更好地适应下游任务。

![](https://i-blog.csdnimg.cn/direct/5add7040e1bf41bb93147a157f9ded02.png)

> 无监督的预训练阶段：自回归，已知几个词预测下一个词的概率。
>
> 有监督训练：对标签，学习已知 x1 x2 x3....预测标签为y的概率。并极大化似然函数来调参。

![](https://i-blog.csdnimg.cn/direct/3a6d974d4f024dd8839049e42bea2d97.png)

![](https://i-blog.csdnimg.cn/direct/de383b5ae7104852ac018a5e23789f31.png)
![](https://i-blog.csdnimg.cn/direct/b327b84dce784db5b82316f06111693f.png)

BERT

BERT证明了使用双向预训练效果更好，解决了GPT模型为了防止泄密，在进行预测时舍弃了下文信息的局限性。，它使用的是Transformer编码器，由于self-attention机制，所以模型上下层直接全部互相连接的。
  
![](https://i-blog.csdnimg.cn/direct/bf8c36b1a1e84e6eafc9143bdd4df413.png)

在模型输入方面，BERT输入的编码向量是词向量、位置向量、句子切分向量这三个嵌入特征的单位和。
  
在模型的预训练上，BERT利用两个监督任务进行预训练。
  
第一个任务是Mask LM（MLM），为了解决GPT完全舍弃下文的问题，不再进行整个句子的预测而是对某个词去做预测，首先屏蔽一定百分比的词，然后通过模型实现对屏蔽词的预测，来进行训练。但是会存在两点不足：一是由于屏蔽的词在微调期间并不会出现，在进行微调时会出现与预训练不匹配的问题。二是预测的是屏蔽掉的是词而非句子，会使整个句子预训练的收敛速度更慢。

针对
**第一个不足的解决办法是在80%时间保持屏蔽的状态，10%的时间里进行随机词替换，10%
  
的时间使用词本身**
。对于第二个问题，作者认为收敛速度算是对模型效果提升的妥协。
  
第二个任务是Next Sentence Prediction（NSP），主要是为了实现基于上个句子对下个句子的预测，首先在50%的时间是拼接原始的上下句子，标签设为正例。50%的时间里拼接原始句子与随机的下一句，标签设为负例，这样做的目的是提取句子间的关系。在随后发布的XLNet取消了这个任务，并且RoBERTa[38]和SpanBERT[39]通过测试发现，没有NSP的话模型的效果会更好。

目前的研究表明，
**在大型无标注语料库进行预训练，可以在NLP任务上显著提高模型性能。**

目前主要有两种常见的迁移学习方式：
**特征提取和微调**
，两者的区别就是
**以ELMo等为代
  
表的模型使用的特征提取方法冻结了预训练参数**
，而
**以BERT等为代表的模型采用的微调则是动态地改变参数，根据下游任务进行参数上的微调。**
特征提取需要更复杂的任务架构，并且就综合性能来看，微调的方法更适合下游任务。

---

NLP常见的下游任务可以分为四大类：

第一类任务是序列标注，比如分词、命名实体识别、语义标注等；

第二类任务是分类任务，比如文本分类、情感分析等；

第三类任务是句子关系判断，比如句法分析、问答QA、自然语言推理等；

第四类是生成式任务，比如机器翻译、文本摘要、阅读理解、对话系统等。