---
layout: post
title: "LLaMA开放且高效的基础语言模型"
date: 2025-03-13 08:51:46 +0800
description: "我们介绍了LLaMA，这是一系列参数规模从70亿到650亿不等的基础语言模型。我们在数万亿的标记上训练我们的模型，并展示了仅使用公开可用的数据集训练出最先进模型的可能性，而无需依赖专有和难以获取的数据集。特别是，LLaMA-130亿在大多数基准测试中超越了GPT-3（1750亿），而LLaMA-650亿则与最佳模型Chinchilla-700亿和PaLM-5400亿相媲美。我们将所有模型向研究社区公开。"
keywords: "LLaMA：开放且高效的基础语言模型"
categories: ['文本模型Paper阅读']
tags: ['语言模型', '人工智能', 'Llama']
artid: "146221606"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146221606
    alt: "LLaMA开放且高效的基础语言模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146221606
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146221606
cover: https://bing.ee123.net/img/rand?artid=146221606
image: https://bing.ee123.net/img/rand?artid=146221606
img: https://bing.ee123.net/img/rand?artid=146221606
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     LLaMA：开放且高效的基础语言模型
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-dracula" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="_0">
     </a>
     摘要
    </h2>
    <p>
     我们介绍了LLaMA，这是一系列参数规模从70亿到650亿不等的基础语言模型。我们在数万亿的标记上训练我们的模型，并展示了仅使用公开可用的数据集训练出最先进模型的可能性，而无需依赖专有和难以获取的数据集。特别是，LLaMA-130亿在大多数基准测试中超越了GPT-3（1750亿），而LLaMA-650亿则与最佳模型Chinchilla-700亿和PaLM-5400亿相媲美。我们将所有模型向研究社区公开。
    </p>
    <h2>
     <a id="1__4">
     </a>
     1 引言
    </h2>
    <p>
     在大量文本语料库上训练的大型语言模型（LLMs）已经展示了它们从文本指令或少量示例中执行新任务的能力（Brown等人，2020）。这些少样本特性首先在模型规模达到一定大小时出现（Kaplan等人，2020），导致了一系列专注于进一步扩展这些模型的工作（Chowdhery等人，2022；Rae等人，2021）。这些努力基于一个假设，即更多的参数将带来更好的性能。然而，Hoffmann等人（2022）的最新研究表明，在给定的计算预算下，最佳性能不是由最大的模型实现的，而是由在更多数据上训练的较小模型实现的。
    </p>
    <p>
     Hoffmann等人（2022）的扩展法则的目标是确定如何为特定的训练计算预算最佳地扩展数据集和模型规模。然而，这一目标忽略了推理预算，这在规模化服务语言模型时变得至关重要。在这种情况下，给定一个目标性能水平，首选模型不是训练最快的，而是推理最快的，尽管训练一个大型模型以达到某个性能水平可能更便宜，但一个训练时间更长的小型模型最终在推理时会更便宜。例如，尽管Hoffmann等人（2022）建议在2000亿标记上训练一个100亿模型，但我们发现70亿模型的性能即使在1万亿标记后仍在继续提高。
    </p>
    <p>
     这项工作的重点是训练一系列语言模型，通过训练比通常使用的更多的标记，在各种推理预算下实现最佳性能。由此产生的模型称为LLaMA，其参数规模从70亿到650亿不等，与现有的最佳LLMs相比具有竞争力。例如，LLaMA-130亿在大多数基准测试中超越了GPT-3，尽管它小了10倍。我们相信这个模型将有助于民主化LLMs的访问和研究，因为它可以在单个GPU上运行。在规模的高端，我们的650亿参数模型也与Chinchilla或PaLM-5400亿等最佳大型语言模型相媲美。
    </p>
    <p>
     与Chinchilla、PaLM或GPT-3不同，我们仅使用公开可用的数据，使我们的工作与开源兼容，而大多数现有模型依赖于不可公开获取或未记录的数据（例如，“Books – 2TB”或“社交媒体对话”）。存在一些例外，特别是OPT（Zhang等人，2022）、GPT-NeoX（Black等人，2022）、BLOOM（Scao等人，2022）和GLM（Zeng等人，2022），但没有一个能与PaLM-620亿或Chinchilla相媲美。
    </p>
    <p>
     在本文的其余部分，我们概述了我们对变压器架构（Vaswani等人，2017）所做的修改，以及我们的训练方法。然后，我们报告了我们模型的性能，并与其他LLMs在一组标准基准测试上进行了比较。最后，我们使用负责任AI社区的一些最新基准测试，揭示了我们的模型中编码的一些偏见和毒性。
    </p>
    <h2>
     <a id="2__16">
     </a>
     2 方法
    </h2>
    <p>
     我们的训练方法与之前工作中描述的方法相似（Brown等人，2020；Chowdhery等人，2022），并受到Chinchilla扩展法则（Hoffmann等人，2022）的启发。我们使用标准优化器在大量文本数据上训练大型变压器模型。
    </p>
    <h3>
     <a id="21__20">
     </a>
     2.1 预训练数据
    </h3>
    <p>
     我们的训练数据集是多个来源的混合体，如表1所示，涵盖了多样化的领域。在大多数情况下，我们重用了用于训练其他LLMs的数据源，但仅限于使用公开可用且与开源兼容的数据。这导致了以下数据混合及其在训练集中所占的百分比：
    </p>
    <p>
     英语CommonCrawl [67%]。我们使用CCNet管道（Wenzek等人，2020）预处理了从2017年到2020年的五个CommonCrawl转储。这个过程在行级别去重，使用fastText线性分类器进行语言识别以移除非英语页面，并使用n-gram语言模型过滤低质量内容。此外，我们训练了一个线性模型来分类用作维基百科参考的页面与随机采样的页面，并丢弃未被分类为参考的页面。
    </p>
    <p>
     C4 [15%]。在探索性实验中，我们观察到使用多样化的预处理CommonCrawl数据集可以提高性能。因此，我们将公开可用的C4数据集（Raffel等人，2020）纳入我们的数据中。C4的预处理也包含去重和语言识别步骤：与CCNet的主要区别在于质量过滤，这主要依赖于启发式方法，如标点符号的存在或网页中的单词和句子数量。
    </p>
    <p>
     Github [4.5%]。我们使用Google BigQuery上可用的公共GitHub数据集。我们只保留了根据Apache、BSD和MIT许可证分发的项目。此外，我们基于行长度或字母数字字符比例的启发式方法过滤了低质量文件，并使用正则表达式删除了样板文件，如头部信息。最后，我们在文件级别对结果数据集进行去重，要求完全匹配。
    </p>
    <p>
     Wikipedia [4.5%]。我们添加了2022年6月至8月期间的维基百科转储，涵盖20种语言，这些语言使用拉丁或西里尔字母：bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk。我们处理数据以移除超链接、评论和其他格式化样板。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/2b1a605d7b53411fa4d973c868de4e1a.png"/>
    </p>
    <p>
     古腾堡计划与Books3 [4.5%]。我们在训练数据集中包含了两个书籍语料库：古腾堡计划，其中包含公共领域的书籍，以及ThePile（Gao等人，2020）的Books3部分，这是一个用于训练大型语言模型的公开可用数据集。我们在书籍级别进行去重，移除内容重叠超过90%的书籍。
    </p>
    <p>
     ArXiv [2.5%]。我们处理arXiv的Latex文件，以将科学数据添加到我们的数据集中。遵循Lewkowycz等人（2022）的方法，我们移除了第一部分之前的所有内容，以及参考文献。我们还从.tex文件中移除了注释，并内联扩展了用户编写的定义和宏，以提高论文之间的一致性。
    </p>
    <p>
     Stack Exchange [2%]。我们包含了Stack Exchange的转储，这是一个涵盖从计算机科学到化学等多个领域的高质量问答网站。我们保留了28个最大网站的数据，从文本中移除了HTML标签，并按分数（从高到低）对答案进行了排序。
    </p>
    <p>
     分词器。我们使用字节对编码（BPE）算法（Sennrich等人，2015）对数据进行分词，使用了SentencePiece（Kudo和Richardson，2018）的实现。值得注意的是，我们将所有数字拆分为单个数字，并回退到字节以分解未知的UTF-8字符。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/17732f6602fd4858938855327059ec37.png"/>
    </p>
    <p>
     总的来说，我们的整个训练数据集在分词后大约包含1.4万亿个标记。对于大多数训练数据，每个标记在训练期间仅使用一次，除了维基百科和书籍领域，我们对这些领域进行了大约两个周期的训练。
    </p>
    <h3>
     <a id="22__45">
     </a>
     2.2 架构
    </h3>
    <p>
     遵循最近关于大型语言模型的研究，我们的网络基于变压器架构（Vaswani等人，2017）。我们利用了随后提出的各种改进，并在不同的模型中使用，如PaLM。以下是原始架构的主要差异，以及我们找到这些变化的灵感来源（括号内）：
    </p>
    <p>
     预归一化 [GPT3]。为了提高训练稳定性，我们对每个变压器子层的输入进行归一化，而不是归一化输出。我们使用了Zhang和Sennrich（2019）引入的RMSNorm归一化函数。
    </p>
    <p>
     SwiGLU激活函数 [PaLM]。我们将ReLU非线性替换为SwiGLU激活函数，这是Shazeer（2020）引入的，以提高性能。我们使用了
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        2 
        
       
         / 
        
       
         3 
        
       
         ∗ 
        
       
         4 
        
       
         d 
        
       
      
        2/3 * 4d
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1em; vertical-align: -0.25em;">
         </span>
         <span class="mord">
          2/3
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ∗
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6944em;">
         </span>
         <span class="mord">
          4
         </span>
         <span class="mord mathnormal">
          d
         </span>
        </span>
       </span>
      </span>
     </span>
     的维度，而不是PaLM中的4d。
    </p>
    <p>
     旋转嵌入 [GPTNeo]。我们移除了绝对位置嵌入，并在网络的每一层添加了Su等人（2021）引入的旋转位置嵌入（RoPE）。我们不同模型的超参数详细信息见表2。
    </p>
    <h3>
     <a id="23__55">
     </a>
     2.3 优化器
    </h3>
    <p>
     我们的模型使用AdamW优化器（Loshchilov和Hutter，2017）进行训练，具有以下超参数：
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        β 
        
       
         1 
        
       
         = 
        
       
         0.9 
        
       
         ； 
        
       
         β 
        
       
         2 
        
       
         = 
        
       
         0.95 
        
       
      
        β1 = 0.9；β2 = 0.95
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0528em;">
          β
         </span>
         <span class="mord">
          1
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
         <span class="mrel">
          =
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;">
         </span>
         <span class="mord">
          0.9
         </span>
         <span class="mord cjk_fallback">
          ；
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0528em;">
          β
         </span>
         <span class="mord">
          2
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
         <span class="mrel">
          =
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          0.95
         </span>
        </span>
       </span>
      </span>
     </span>
     。我们使用余弦学习率调度，使得最终学习率等于最大学习率的10%。我们使用0.1的权重衰减和1.0的梯度裁剪。我们使用了2,000个预热步骤，并根据模型的大小调整学习率和批量大小（详情见表2）。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fe13bd58b7164730a52760dbfe0c2007.png"/>
    </p>
    <h3>
     <a id="24__61">
     </a>
     2.4 高效实现
    </h3>
    <p>
     我们进行了多项优化以提高模型的训练速度。首先，我们使用了因果多头注意力机制的高效实现，以减少内存占用和运行时间。该实现可在xformers库中找到，其灵感来自Rabe和Staats（2021），并采用了Dao等人（2022）的反向传播方法。这是通过不存储注意力权重以及不计算因语言建模任务的因果性质而被掩码的键/查询分数来实现的。
    </p>
    <p>
     为了进一步提高训练效率，我们通过检查点机制减少了反向传播期间需要重新计算的计算量。更具体地说，我们保存了计算成本较高的激活值，例如线性层的输出。这是通过手动实现变压器层的反向传播函数来实现的，而不是依赖PyTorch的自动求导功能。为了充分利用这一优化，我们需要通过使用模型并行和序列并行来减少模型的内存占用，如Korthikanti等人（2022）所述。此外，我们还尽可能重叠激活值的计算和GPU之间通过网络进行的通信（由于all_reduce操作）。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/e3b7e81b8e96499e99d46b355b274d7a.png"/>
    </p>
    <p>
     在训练一个650亿参数的模型时，我们的代码在2048个80GB内存的A100 GPU上每秒处理约380个标记/GPU。这意味着在包含1.4万亿标记的数据集上进行训练大约需要21天。
    </p>
    <h2>
     <a id="3__70">
     </a>
     3 主要结果
    </h2>
    <p>
     遵循之前的工作（Brown等人，2020），我们考虑了零样本和少样本任务，并在总共20个基准测试中报告了结果：
    </p>
    <ul>
     <li>
      <strong>
       零样本
      </strong>
      ：我们提供任务的文本描述和一个测试示例。模型要么通过开放式生成提供答案，要么对提出的答案进行排序。
     </li>
     <li>
      <strong>
       少样本
      </strong>
      ：我们提供任务的几个示例（1到64个）和一个测试示例。模型将此文本作为输入并生成答案或对不同的选项进行排序。
     </li>
    </ul>
    <p>
     我们将LLaMA与其他基础模型进行了比较，包括未公开的语言模型GPT-3（Brown等人，2020）、Gopher（Rae等人，2021）、Chinchilla（Hoffmann等人，2022）和PaLM（Chowdhery等人，2022），以及开源的OPT模型（Zhang等人，2022）、GPT-J（Wang和Komatsuzaki，2021）和GPT-Neo（Black等人，2022）。在第4节中，我们还简要比较了LLaMA与指令调优模型，如OPT-IML（Iyer等人，2022）和Flan-PaLM（Chung等人，2022）。
    </p>
    <p>
     我们在自由生成任务和多项选择任务上评估了LLaMA。在多项选择任务中，目标是根据提供的上下文从一组给定选项中选择最合适的补全内容。我们选择在给定上下文中具有最高似然的补全内容。我们遵循Gao等人（2021）的方法，使用按补全内容字符数归一化的似然值，但对于某些数据集（如OpenBookQA、BoolQ），我们遵循Brown等人（2020）的方法，基于以“Answer:”为上下文的补全内容似然值进行选择：
     <br/>
     <span class="katex--display">
      <span class="katex-display">
       <span class="katex">
        <span class="katex-mathml">
         P 
         
        
          ( 
         
        
          completion 
         
        
          ∣ 
         
        
          context 
         
        
          ) 
         
        
          / 
         
        
          P 
         
        
          ( 
         
        
          completion 
         
        
          ∣ 
         
        
          “Answer:” 
         
        
          ) 
         
        
       
         P(\text{completion}|\text{context}) / P(\text{completion}|\text{``Answer:''})
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 1em; vertical-align: -0.25em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.1389em;">
           P
          </span>
          <span class="mopen">
           (
          </span>
          <span class="mord text">
           <span class="mord">
            completion
           </span>
          </span>
          <span class="mord">
           ∣
          </span>
          <span class="mord text">
           <span class="mord">
            context
           </span>
          </span>
          <span class="mclose">
           )
          </span>
          <span class="mord">
           /
          </span>
          <span class="mord mathnormal" style="margin-right: 0.1389em;">
           P
          </span>
          <span class="mopen">
           (
          </span>
          <span class="mord text">
           <span class="mord">
            completion
           </span>
          </span>
          <span class="mord">
           ∣
          </span>
          <span class="mord text">
           <span class="mord">
            “Answer:”
           </span>
          </span>
          <span class="mclose">
           )
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/913649b9bdc74b65a50c9895a6a15fe9.png"/>
    </p>
    <h3>
     <a id="31__84">
     </a>
     3.1 常识推理
    </h3>
    <p>
     我们考虑了八个标准的常识推理基准测试：BoolQ（Clark等人，2019）、PIQA（Bisk等人，2020）、SIQA（Sap等人，2019）、HellaSwag（Zellers等人，2019）、WinoGrande（Sakaguchi等人，2021）、ARC easy和challenge（Clark等人，2018）以及OpenBookQA（Mihaylov等人，2018）。这些数据集包括完形填空和Winograd风格的任务，以及多项选择题回答。我们在零样本设置下进行评估，这与语言建模社区的做法一致。
    </p>
    <p>
     在表3中，我们与不同规模的现有模型进行了比较，并报告了相应论文中的结果。首先，LLaMA-65B在所有报告的基准测试中均优于Chinchilla-70B，除了BoolQ。同样，该模型在除BoolQ和WinoGrande之外的所有任务上都超过了PaLM-540B。LLaMA-13B模型在大多数基准测试中也优于GPT-3，尽管其规模小了10倍。
    </p>
    <h3>
     <a id="32__90">
     </a>
     3.2 闭卷问答
    </h3>
    <p>
     我们在两个闭卷问答基准测试上将LLaMA与现有的大型语言模型进行了比较：Natural Questions（Kwiatkowski等人，2019）和TriviaQA（Joshi等人，2017）。对于这两个基准测试，我们报告了在闭卷设置下的精确匹配性能，即模型无法访问包含回答问题证据的文档。在表4中，我们报告了NaturalQuestions的性能，在表5中，我们报告了TriviaQA的性能。在这两个基准测试中，LLaMA-65B在零样本和少样本设置下均达到了最先进的性能。更重要的是，LLaMA-13B在这些基准测试中也与GPT-3和Chinchilla竞争，尽管其规模小了5到10倍。该模型在推理期间可以在单个V100 GPU上运行。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/76505948461d4a1e9949785d1fdc3abc.png"/>
    </p>
    <h3>
     <a id="33__95">
     </a>
     3.3 阅读理解
    </h3>
    <p>
     我们在RACE阅读理解基准测试（Lai等人，2017）上评估了我们的模型。该数据集是从为中国中学生设计的英语阅读理解考试中收集的。我们遵循Brown等人（2020）的评估设置，并在表6中报告了结果。在这些基准测试中，LLaMA-65B与PaLM-540B竞争，而LLaMA-13B比GPT-3高出几个百分点。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c7633b7af9d6485c8b52ad0984b02544.png"/>
    </p>
    <h3>
     <a id="34__100">
     </a>
     3.4 数学推理
    </h3>
    <p>
     我们在两个数学推理基准测试上评估了我们的模型：MATH（Hendrycks等人，2021）和GSM8k（Cobbe等人，2021）。MATH是一个包含12K个中学和高中数学问题的数据集，问题以LaTeX格式编写。GSM8k是一组中学数学问题。
    </p>
    <p>
     在表7中，我们与PaLM和Minerva（Lewkowycz等人，2022）进行了比较。Minerva是一系列在从ArXiv和数学网页中提取的38.5B标记上进行微调的PaLM模型，而PaLM和LLaMA均未在数学数据上进行微调。PaLM和Minerva的数据来自Lewkowycz等人（2022），我们比较了是否使用maj1@k的结果。maj1@k表示我们对每个问题生成k个样本并进行多数投票（Wang等人，2022）的评估方法。在GSM8k上，我们观察到LLaMA-65B优于Minerva-62B，尽管它未在数学数据上进行微调。
    </p>
    <h3>
     <a id="35__106">
     </a>
     3.5 代码生成
    </h3>
    <p>
     我们评估了我们的模型从自然语言描述生成代码的能力，使用了两个基准测试：HumanEval（Chen等人，2021）和MBPP（Austin等人，2021）。对于这两项任务，模型会收到几行文字描述的程序说明以及一些输入输出示例。在HumanEval中，模型还会收到一个函数签名，提示格式为自然代码，文本描述和测试用例以文档字符串形式呈现。模型需要生成一个符合描述并通过测试用例的Python程序。
    </p>
    <p>
     在表8中，我们比较了我们的模型与未在代码上进行微调的现有语言模型（即PaLM和LaMDA（Thoppilan等人，2022））的pass@1分数。PaLM和LLaMA在包含相似数量代码标记的数据集上进行了训练。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/88b4a67d206544c49cdfd6be0bad7709.png"/>
    </p>
    <p>
     如表8所示，在参数数量相似的情况下，LLaMA优于其他通用模型，如LaMDA和PaLM，这些模型并未专门针对代码进行训练或微调。具有130亿参数及以上的LLaMA在HumanEval和MBPP上均优于LaMDA 1370亿。LLaMA 650亿也优于PaLM 620亿，即使PaLM训练时间更长。表中报告的pass@1结果是通过温度为0.1的采样获得的，而pass@100和pass@80指标是通过温度为0.8的采样获得的。我们使用与Chen等人（2021）相同的方法来获得pass@k的无偏估计。
    </p>
    <p>
     通过对特定代码标记进行微调，可以进一步提高代码生成性能。例如，PaLM-Coder（Chowdhery等人，2022）将PaLM在HumanEval上的pass@1分数从26.2%提高到36%。其他专门针对代码训练的模型在这些任务上也优于通用模型（Chen等人，2021；Nijkamp等人，2022；Fried等人，2022）。对代码标记进行微调超出了本文的范围。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/319fa55f8f8c4594a9d7e07b211112ac.png"/>
    </p>
    <h3>
     <a id="36__120">
     </a>
     3.6 大规模多任务语言理解
    </h3>
    <p>
     Hendrycks等人（2020）引入的大规模多任务语言理解基准测试（MMLU）包含涵盖多个知识领域的选择题，包括人文、STEM和社会科学。我们在5-shot设置下评估我们的模型，使用基准测试提供的示例，并在表9中报告结果。在这个基准测试中，我们观察到LLaMA-65B在平均水平和大多数领域中略低于Chinchilla-70B和PaLM-540B。一个可能的解释是，我们在预训练数据中使用的书籍和学术论文数量有限，即ArXiv、Gutenberg和Books3，总计仅为177GB，而这些模型使用了高达2TB的书籍进行训练。Gopher、Chinchilla和PaLM使用的大量书籍可能也解释了为什么Gopher在这个基准测试上优于GPT-3，而在其他基准测试上表现相当。
    </p>
    <h3>
     <a id="37__124">
     </a>
     3.7 训练期间性能的演变
    </h3>
    <p>
     在训练期间，我们跟踪了模型在几个问答和常识推理基准测试上的性能，并在图2中报告了结果。在大多数基准测试中，性能稳步提升，并与模型的训练困惑度相关（见图1）。例外的是SIQA和WinoGrande。最值得注意的是，在SIQA上，我们观察到性能存在较大波动，这可能表明该基准测试不可靠。在WinoGrande上，性能与训练困惑度的相关性较弱：LLaMA-33B和LLaMA-65B在训练期间表现相似。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/896f7c1ad8d043c9b1bbf7a2c053f31f.png"/>
    </p>
    <h2>
     <a id="4__130">
     </a>
     4 指令微调
    </h2>
    <p>
     在本节中，我们展示了在指令数据上进行短暂微调可以快速提升MMLU的性能。尽管未微调的LLaMA-65B版本已经能够遵循基本指令，但我们观察到，即使是非常少量的微调也能提高MMLU的性能，并进一步增强模型遵循指令的能力。由于这不是本文的重点，我们仅进行了一项实验，遵循Chung等人（2022）的相同协议，训练了一个指令模型LLaMA-I。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fb2f3a33100a405ca9cb94cb4ab1c31c.png"/>
    </p>
    <p>
     在表10中，我们报告了指令模型LLaMA-I在MMLU上的结果，并与现有中等规模的指令微调模型进行了比较，即OPT-IML（Iyer等人，2022）和Flan-PaLM系列（Chung等人，2022）。所有报告的数据均来自相应的论文。尽管这里使用的指令微调方法较为简单，但我们在MMLU上达到了68.9%的准确率。LLaMA-I（65B）在MMLU上优于现有的中等规模指令微调模型，但仍远未达到最先进的水平，即GPT code-davinci-002在MMLU上的77.4%（数据来自Iyer等人，2022）。关于MMLU 57项任务的具体性能细节可以在附录的表16中找到。
    </p>
    <h2>
     <a id="5__138">
     </a>
     5 偏见、毒性和错误信息
    </h2>
    <p>
     大型语言模型已被证明会复制和放大训练数据中存在的偏见（Sheng等人，2019；Kurita等人，2019），并生成有毒或冒犯性内容（Gehman等人，2020）。由于我们的训练数据集中包含大量来自网络的数据，我们认为评估模型生成此类内容的潜在风险至关重要。
    </p>
    <p>
     为了了解LLaMA-65B的潜在危害，我们在不同的基准测试上进行了评估，这些基准测试用于衡量有毒内容生成和刻板印象检测。虽然我们选择了一些语言模型社区使用的标准基准测试来揭示这些模型的一些问题，但这些评估并不足以完全理解与这些模型相关的风险。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/397ab351781e4ffa8d1dfa6d3a537a5f.png"/>
    </p>
    <h3>
     <a id="51_RealToxicityPrompts_146">
     </a>
     5.1 RealToxicityPrompts
    </h3>
    <p>
     语言模型可能生成有毒语言，例如侮辱、仇恨言论或威胁。模型可以生成的有毒内容范围非常广泛，这使得全面评估具有挑战性。最近的一些研究（Zhang等人，2022；Hoffmann等人，2022）将RealToxicityPrompts基准测试（Gehman等人，2020）作为衡量模型毒性程度的指标。RealToxicityPrompts包含约10万个提示，模型需要完成这些提示；然后通过向PerspectiveAPI 3发送请求自动评估毒性分数。我们无法控制第三方PerspectiveAPI使用的流程，因此与之前模型的比较较为困难。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a38cb6809d664b44ae99bce29a574aa8.png"/>
    </p>
    <p>
     对于每个10万个提示，我们使用模型贪婪生成内容，并测量其毒性分数。每个提示的分数范围从0（无毒）到1（有毒）。在表11中，我们报告了在RealToxicityPrompts的基本和尊重提示类别上的平均分数。这些分数与文献中观察到的结果“相当”（例如，Chinchilla的0.087），但这些研究的方法与我们的方法有所不同（在采样策略、提示数量和API时间方面）。我们观察到，毒性随着模型规模的增加而增加，尤其是在尊重提示类别中。这一现象在之前的研究中也得到了观察（Zhang等人，2022），但Hoffmann等人（2022）的研究是一个显著的例外，他们发现Chinchilla和Gopher之间没有差异，尽管它们的规模不同。这可能是因为较大的模型Gopher表现不如Chinchilla，这表明毒性与模型规模之间的关系可能仅适用于同一模型家族内。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/bd395f17526042779ef6ba626d314f5a.png"/>
    </p>
    <h3>
     <a id="52_CrowSPairs_154">
     </a>
     5.2 CrowS-Pairs
    </h3>
    <p>
     我们在CrowS-Pairs（Nangia等人，2020）上评估了模型的偏见。该数据集允许衡量9个类别的偏见：性别、宗教、种族/肤色、性取向、年龄、国籍、残疾、外貌和社会经济地位。每个示例由一个刻板印象和一个反刻板印象组成，我们在零样本设置下通过计算两个句子的困惑度来衡量模型对刻板印象句子的偏好。因此，较高的分数表示较高的偏见。我们在表12中与GPT-3和OPT-175B进行了比较。
     <br/>
     LLaMA在平均水平上略优于这两个模型。我们的模型在宗教类别中表现出较高的偏见（比OPT-175B高10%），其次是年龄和性别。我们预计这些偏见来自CommonCrawl，尽管经过了多次过滤步骤。
    </p>
    <h3>
     <a id="53_WinoGender_159">
     </a>
     5.3 WinoGender
    </h3>
    <p>
     为了进一步研究模型在性别类别上的偏见，我们查看了WinoGender基准测试（Rudinger等人，2018），这是一个共指消解数据集。WinoGender由Winograd模式组成，通过确定模型的共指消解性能是否受代词性别的影响来评估偏见。
     <br/>
     更具体地说，每个句子有三个提及：一个“职业”、一个“参与者”和一个“代词”，其中代词与职业或参与者共指。我们提示模型确定共指关系，并根据句子的上下文测量其是否正确。目标是揭示模型是否捕捉到了与职业相关的社会偏见。例如，WinoGender数据集中的一个句子是“护士通知病人，他的轮班将在一小时后结束。”，随后是“他的”指的是。然后我们比较“护士”和“病人”的困惑度，以使用模型进行共指消解。我们评估了使用3种代词时的性能：“她/她的”、“他/他的”和“他们/某人”（不同的选择对应于代词的语法功能）。
     <br/>
     在表13中，我们报告了数据集中包含的三种不同代词的共指分数。我们观察到，我们的模型在使用“他们/某人”代词时比使用“她/她的”和“他/他的”代词时表现显著更好。之前的研究中也观察到了类似的现象（Rae等人，2021；Hoffmann等人，2022），这很可能表明存在性别偏见。事实上，在使用“她/她的”和“他/他的”代词时，模型可能使用职业的多数性别来进行共指消解，而不是使用句子的证据。
     <br/>
     为了进一步研究这一假设，我们查看了WinoGender数据集中“她/她的”和“他/他的”代词的“陷阱”案例。这些案例对应于代词与职业的多数性别不匹配且职业是正确答案的句子。在表13中，我们观察到我们的模型LLaMA-65B在陷阱示例上犯更多错误，清楚地表明它捕捉到了与性别和职业相关的社会偏见。性能下降存在于“她/她的”和“他/他的”代词中，这表明无论性别如何都存在偏见。
    </p>
    <h3>
     <a id="54_TruthfulQA_166">
     </a>
     5.4 TruthfulQA
    </h3>
    <p>
     TruthfulQA（Lin等人，2021）旨在衡量模型的真实性，即其识别声明是否为真的能力。Lin等人（2021）将“真实”定义为“关于现实世界的字面真理”，而不是仅在信仰系统或传统背景下为真的声明。该基准测试可以评估模型生成错误信息或虚假声明的风险。问题以多样化的风格编写，涵盖38个类别，并且设计为对抗性的。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/eb1df503978f47828d58d5bba7cb1865.png"/>
    </p>
    <p>
     在表14中，我们报告了模型在衡量真实性以及真实性与信息性交集问题上的表现。与GPT-3相比，我们的模型在这两个类别中得分更高，但正确答案的比例仍然较低，这表明我们的模型可能会产生不正确的答案。
    </p>
    <h2>
     <a id="6__173">
     </a>
     6 碳足迹
    </h2>
    <p>
     我们模型的训练消耗了大量能源，导致了二氧化碳的排放。我们遵循该主题的最新文献，并在表15中详细列出了总能耗和由此产生的碳足迹。我们采用Wu等人（2022）的公式来估计训练模型所需的瓦时（Wh）以及碳排放量（tCO2eq）。对于Wh，我们使用以下公式：
     <br/>
     [ \text{Wh} = \text{GPU-h} \times (\text{GPU功耗}) \times \text{PUE} ]
     <br/>
     其中，我们将电源使用效率（PUE）设为1.1。碳排放量取决于用于训练网络的数据中心的位置。例如，BLOOM使用的电网排放强度为0.057 kg CO2eq/KWh，导致27 tCO2eq；而OPT使用的电网排放强度为0.231 kg CO2eq/KWh，导致82 tCO2eq。在本研究中，我们感兴趣的是比较这些模型在同一数据中心训练时的碳排放成本。因此，我们不考虑数据中心的位置，而是使用美国国家平均碳排放强度因子0.385 kg CO2eq/KWh。这导致了以下碳排放量的计算公式：
     <br/>
     <span class="katex--display">
      <span class="katex-display">
       <span class="katex">
        <span class="katex-mathml">
         t 
         
        
          C 
         
         
         
           O 
          
         
           2 
          
         
        
          e 
         
        
          q 
         
        
          = 
         
        
          M 
         
        
          W 
         
        
          h 
         
        
          × 
         
        
          0.385. 
         
        
       
         \mathrm { t C O _ { 2 } e q = M W h \times 0 . 3 8 5 . }
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;">
          </span>
          <span class="mord">
           <span class="mord mathrm">
            tC
           </span>
           <span class="mord">
            <span class="mord mathrm">
             O
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   <span class="mord mathrm mtight">
                    2
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mord mathrm">
            eq
           </span>
           <span class="mspace" style="margin-right: 0.2778em;">
           </span>
           <span class="mrel">
            =
           </span>
           <span class="mspace" style="margin-right: 0.2778em;">
           </span>
           <span class="mord mathrm">
            MWh
           </span>
           <span class="mspace" style="margin-right: 0.2222em;">
           </span>
           <span class="mbin">
            ×
           </span>
           <span class="mspace" style="margin-right: 0.2222em;">
           </span>
           <span class="mord mathrm">
            0.385.
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     <br/>
     为了公平比较，我们对OPT和BLOOM应用了相同的公式。对于OPT，我们假设训练需要34天，使用992个A100-80GB GPU（参见他们的日志4）。最后，我们估计我们使用了2048个A100-80GB GPU，耗时约5个月来开发我们的模型。这意味着，根据我们的假设，开发这些模型的能耗约为2,638兆瓦时（MWh），总碳排放量为1,015吨二氧化碳当量（tCO2eq）。
    </p>
    <p>
     我们希望发布这些模型能够帮助减少未来的碳排放，因为训练已经完成，而且其中一些模型相对较小，可以在单个GPU上运行。
    </p>
    <h2>
     <a id="7__185">
     </a>
     7 相关工作
    </h2>
    <p>
     语言模型是对单词、标记或字符序列的概率分布（Shannon，1948，1951）。这项任务通常被定义为下一个标记预测，长期以来一直被认为是自然语言处理中的核心问题（Bahl等人，1983；Brown等人，1990）。由于图灵（1950）提出通过“模仿游戏”使用语言来衡量机器智能，语言建模被提议作为衡量人工智能进展的基准（Mahoney，1999）。
    </p>
    <p>
     <strong>
      架构
     </strong>
     。传统上，语言模型基于n-gram计数统计（Bahl等人，1983），并提出了各种平滑技术来改进对罕见事件的估计（Katz，1987；Kneser和Ney，1995）。在过去的二十年中，神经网络已成功应用于语言建模任务，从前馈模型（Bengio等人，2000）、循环神经网络（Elman，1990；Mikolov等人，2010）到LSTM（Hochreiter和Schmidhuber，1997；Graves，2013）。最近，基于自注意力机制的变压器网络带来了重要改进，尤其是在捕捉长距离依赖关系方面（Vaswani等人，2017；Radford等人，2018；Dai等人，2019）。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/809e45c14a11421fb5fce2edad4fe053.png"/>
    </p>
    <p>
     <strong>
      扩展
     </strong>
     。语言模型的扩展有着悠久的历史，包括模型和数据集的规模扩展。Brants等人（2007）展示了使用在2万亿标记上训练的语言模型（生成3000亿n-gram）对机器翻译质量的提升。虽然这项工作依赖于一种称为“Stupid Backoff”的简单平滑技术，但Heafield等人（2013）后来展示了如何将Kneser-Ney平滑技术扩展到网络规模的数据。这使得可以在CommonCrawl的9750亿标记上训练一个5-gram模型，生成一个包含5000亿n-gram的模型（Buck等人，2014）。Chelba等人（2013）引入了“十亿词基准”（One Billion Word benchmark），这是一个用于衡量语言模型进展的大规模训练数据集。
    </p>
    <p>
     在神经语言模型的背景下，Jozefowicz等人（2016）通过将LSTM扩展到10亿参数，在“十亿词基准”上获得了最先进的结果。随后，扩展变压器模型在许多自然语言处理任务上取得了改进。著名的模型包括BERT（Devlin等人，2018）、GPT-2（Radford等人，2019）、Megatron-LM（Shoeybi等人，2019）和T5（Raffel等人，2020）。GPT-3（Brown等人，2020）的推出是一个重大突破，这是一个拥有1750亿参数的模型。这引发了一系列大型语言模型的研究，例如Jurassic-1（Lieber等人，2021）、Megatron-Turing NLG（Smith等人，2022）、Gopher（Rae等人，2021）、Chinchilla（Hoffmann等人，2022）、PaLM（Chowdhery等人，2022）、OPT（Zhang等人，2022）和GLM（Zeng等人，2022）。Hestness等人（2017）和Rosenfeld等人（2019）研究了扩展对深度学习模型性能的影响，揭示了模型和数据集规模与系统性能之间的幂律关系。Kaplan等人（2020）推导了基于变压器的语言模型的幂律关系，随后Hoffmann等人（2022）通过调整学习率调度进一步优化了这一关系。最后，Wei等人（2022）研究了扩展对大型语言模型能力的影响。
    </p>
    <h2>
     <a id="8__196">
     </a>
     8 结论
    </h2>
    <p>
     在本文中，我们介绍了一系列公开发布的语言模型，这些模型与最先进的基础模型具有竞争力。最值得注意的是，LLaMA-13B在比GPT-3小10倍以上的情况下表现优于GPT-3，而LLaMA-65B与Chinchilla-70B和PaLM-540B竞争。与之前的研究不同，我们展示了仅通过使用公开可用的数据进行训练，无需依赖专有数据集，也可以实现最先进的性能。我们希望将这些模型发布给研究社区，能够加速大型语言模型的发展，并帮助改进其鲁棒性，缓解已知问题，如毒性和偏见。此外，我们与Chung等人（2022）一样观察到，对这些模型进行指令微调会带来有希望的结果，我们计划在未来的工作中进一步研究这一点。最后，我们计划在未来发布在更大预训练语料库上训练的更大模型，因为我们看到了随着扩展性能的持续提升。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33393639383938352f:61727469636c652f64657461696c732f313436323231363036" class_="artid" style="display:none">
 </p>
</div>


