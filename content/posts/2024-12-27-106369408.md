---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f6c697570656e6731393937303131392f:61727469636c652f64657461696c732f313036333639343038"
layout: post
title: 基于音视频特征融合的暴力镜头识别方法研究
date: 2024-12-27 09:31:34 +0800
description: "目录前言摘要1 绪论工作安排2 基于视觉认知机理的视频镜头分割方法研究3基于"
keywords: 音频特征融合
categories: ['Research']
tags: ['音视频融合', '音视频特征融合', '计算机视觉', '暴力镜头识别', '暴力检测']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=106369408
    alt: 基于音视频特征融合的暴力镜头识别方法研究
artid: 106369408
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=106369408
featuredImagePreview: https://bing.ee123.net/img/rand?artid=106369408
---

# 基于音视频特征融合的暴力镜头识别方法研究

#### 目录

* [前言](#_3)
* [摘要](#_6)
* [1 绪论](#1__14)
* + [工作安排](#_27)
* [2 基于视觉认知机理的视频镜头分割方法研究](#2__32)
* [3基于视觉通道的暴力行为检测方法研究](#3_38)
* + [经典算法](#_41)
  + [DL算法](#DL_45)
* [4 基于听觉通道的暴力音频检测方法研究](#4__63)
* [5 基于多通道特征融合的暴力镜头检测方法研究](#5___73)
* [6 总结创新点和展望](#6__81)
* + [创新点](#_82)
  + [展望](#_87)
* [读后感](#_91)

## 前言

最近恶补了音视频融合的方法。国内的论文最后扫19年的硕士毕业论文“基于音视频特征融合的暴力镜头识别方法研究”，国外的还要继续读，差不多再读两篇经典的工作就可以写个小点的综述，然后继续修改自己的论文了。需要注意自己有时候做一个方向不自然的就偏了，或者说是强行把工作范围变大，我前期还是以练好写论文的功底为主，别搞这么杂，对自己不好。

## 摘要

暴力镜头检测是多媒体视频领域一项极其重要的任务，具有较高的研究价值和现实意义。目前多媒体视频的数量与日俱增，这给暴力镜头检测的速度带来了更高的要求。而且暴力镜头涉及的语义类型众多，包括打斗、尖叫、爆炸等，这也给暴力镜头检测任务带来了极大的挑战。当下绝大多数研究只涉及到某一种暴力类型，检测的种类相对单一，而且准确率较低，因而亟需面向多种语义类型的暴力镜头快速检测技术。
  
首先，本文基于暴力的出现一般以镜头为最基本单位的原则，对多媒体视频进行了镜头分割，然后对单个镜头进行暴力识别。视频序列的镜头分割是视频检索中的关键技术之一。
**针对传统镜头分割方法在单一场景下分割效果差、对于渐变镜头检测准确率低等问题，本文出了一种基于视觉认知机理的视频镜头分割方法。该方法利用分块颜色直方图强化视觉显著区域，突出前后帧之间的差异特征，进一步高在单一场景下检测镜头切换的准确率。**
此外，基于人类对于视频图像亮度的视觉感知规律，利用滑动窗内相邻多帧之间的差异来捕捉镜头渐变时亮度的变化规律。与传统方法相比，本文所出的算法取得了较好的分割效果，具有较高的查准率和查全率。
  
其次，\*\*本文分别从视觉通道、听觉通道、视听双通道对于单个镜头的暴力成分进行了深入分析。在视觉通道上，本文比较了视频行为分析领域效果最好的密集轨迹特征方法和目前业界使用较广泛的深度学习方法。\*\*在深度学习方法中，本文将相邻两帧图像的帧间差分图作为卷积神经网络（Convolutional Neural Network，CNN）的输入，之后将 CNN 学习到的每个帧间差分图的特征送入长短时记忆（Long Short-Term Memory，LSTM）网络中，对时序信号进行建模。本文在 LSTM 结构中，使用卷积操作进行了改进，改进后的 ConvLSTM 网络 取到了更高层的空间特征。
**在听觉通道上，本文针对目前暴力音频数据集稀缺问题，基于 MediaEval 电影数据构建了一个 VioAudio 数据集，然后比较了传统的声学特征方法和分别用原始音频波形图和音频语谱图作为网络输入的深度学习方法。最后，本文基于视觉通道和听觉通道上结果最好的深度学习模型进行了融合实验。我们将视频中相邻图像帧的帧间差分图及其对应的音频波形图分别送入两个 CNN 网络中进行特征的取，之后对特征进行融合送入 LSTM 网络中，利用长短时记忆网络对时序信息进行建模与分类。实验表明了该音视频融合方法的有效性。**
  
本文的研究工作为目前的镜头分割任务和多媒体视频中暴力镜头检测供了有效的解决方案，在多个数据集上的实验表明，本文出的方法具有一定的可行性和现实意义。同时音视频融合方案也为目前多模态信息融合供了新的思路和方向。

## 1 绪论

绪论中介绍了暴力检测和镜头分割方法的国内外研究现状，之后提出了这两个方向存在的问题。
  
如何从视频流中找出一个暴力镜头，作者介绍了多种镜头分割方法。此外作者介绍了当前镜头捕捉的一些问题有以下几个，主要还是没有公开评价标准吧，
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/0accafdc6f2337e2ea3994592650f759.png)

**在暴力检测方法中，作者提到了多模态融合方法单一，特征提取缺乏理论指导（希望能够参照大脑如何提取高级特征的方法），音频大都基础特征工程，采用dl的工作研究较少（这里说的不是翻译和文本处理）**

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c3754282bc83c6ccb82c51d03dd595b5.png)

### 工作安排

本文将围绕多媒体镜头分割和暴力镜头检测两个方面进行相关技术的深入研究。首先将研究多媒体视频的镜头分割方法，主要解决突变镜头和渐变镜头检测的准确率低等问题。然后分别从视觉、听觉和视听双通道三个层面对暴力成分进行有效检测。最后，为了验证音视频特征融合的有效性，分别将视觉单通道、听觉单通道、视听双通道上的实验结果进行了对比分析。
  
本论文的主要框架和结构安排如图 1-1 所示，各章节的具体内容和结构组织安排如下。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/99e26f29255b340d55eec06d76007399.png)

## 2 基于视觉认知机理的视频镜头分割方法研究

**镜头是指一个摄像头在时间和空间上均连续拍摄的一组连续帧，因此镜头分割任务也就是将一部完整的视频划分成以镜头为单位的片段。**
  
这一节对现有的镜头分割方法的不足进行深入分析， 指出以视觉认知机理为指导的视频镜头分割方法。对于突变镜头的检测，本文将采用颜色分块直方图的策略解决同一场景下镜头切换检测准确率低等问题；对于渐变镜头的检测，本文将采用长时差分方法来捕捉镜头渐变时亮度信息的变化规律。
  
这部分对我参照意义不大，直接过了。

## 3基于视觉通道的暴力行为检测方法研究

通过上章出的镜头分割方法可以将视频以镜头为单位切分成孤立的视频片段，之后我们的任务就变成了判断这些视频片段是否存在暴力成分，最后可以通过预测出的视频片段的标记信息（暴力/非暴力） 。本文从视觉通道进行暴力行为的检测，对深度学习技术和在视频行为分析领域使用效果最好的密集轨迹特征DT方法进行深入研究和对比。

### 经典算法

**DT算法：**

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9a7a3249d248d657ee0ecd0836def628.png)

### DL算法

**CNN特征提取**
  
本文进行了将原始的视频帧作为网络的输入，以及将视频中相邻两帧之差作为网络输入的对比实验，结果发现后者的效果更好，这说明了相邻两帧之差更容易让网络学习到更多的运动模式信息。基于 AlexNet 的暴力特征取框图如图所示：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/fda41e3af5650b8fd0cfb2da488061bf.png)

**CNN+convLSTM算法**

具体步骤：对连续两帧图像作差，来获得视频中瞬时的运动目标，然后
**再用 CNN 网络对这个瞬时运动目标图进行特征取**
，将每个时刻的特征送入ConvLSTM 网络中，进而对一个视频序列建模。我们使用在 ImageNet 数据集上训练好的AlexNet 网络进行特征取，直接调用 pytorch 包中的AlexNet 的网络参数，免去了 CNN 网络训练的过程。而对于 CNN 获取的特征，本文利用了 ConvLSTM 网络进行时序信息建模，只需对于该 LSTM 网络进行单独的训练即可，因此 CNN-ConvLSTM 融合网络的训练方式即只对于 ConvLSTM 网络进行了训练。框架图如下;

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/2d9633d09ba88937cdb33656dbb66a67.png)

**实验对比结果（包括之前的参考文献方法）**

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/6608057f6c27fc47ce4af0f1d3bf491a.png)

## 4 基于听觉通道的暴力音频检测方法研究

从听觉通道角度进行了暴力音频检测方法的研究。听觉通道的分析由于其相较于视觉通道复杂度更低、检测速度更快，因此在算法时间复杂度要求严格的情况下，音频分析是暴力视频检测的一种有效分析方法。\*\*本章从传统声学特征方法和深度学习方法两个角度对暴力音频进行了深入研究。\*\*首先，\*\*我们抽取了音频样本的声学特征，并且为了将不等长的样本规正到同等维度，使用了线性规整法进行了特征的抽取或拓展。
**此外，为了刻画音频在长时范围内的包络特征，我们还取了统计学特征。基于声学特征和长时统计学特征的暴力音频检测方法取得了较好的检测效果。在**
深度学习方面， 本文采用了原始波形作为网络输入和音频语谱图作为网络输入两种策略，经过对比，我们发现基于音频原始波形的端到端检测方法更有利于暴力音频检测。\*\*这也为后期音视频特征融合方法提供了新的思路。

**实验**

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/8255d6c0e82e49041d5f9d82de724b8f.png)
  
传统方法用svm分类特征，深度学习用上图和不加语谱图的方法。

## 5 基于多通道特征融合的暴力镜头检测方法研究

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/98b5920018f9b53ea4fbd88131cbddf1.png)
  
这部分设计的结构如上，结果如下，比比赛中最好的方法高了1.9%
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/8add7c3c7b3e5f5fca94a17f578681c3.png)

## 6 总结创新点和展望

### 创新点

1. 对于突变镜头的检测，本文利用颜色分块直方图机制强化了视觉显著性区域，突出了前后视频帧之间的差异，进一步提高了在单一场景下检测镜头切换的准确率；对于渐变镜头的检测，本文出了基于亮度的长时差分方法，通过比较滑动窗内相邻多帧之间的差异来捕捉镜头渐变时亮度的变化规律， 并给出了临界阈值算法，提升了目前多媒体镜头分割的效果。
2. 在视觉单独实验中使用convlstm代替lstm
3. 提出了 vioaudio数据集，但是其实还是根据别的数据集做的。使用cnn-lstm发现原始声音更好。
4. 同时刻视觉通道的帧间差分图的特征与原始音频波形图的特征融合，并送至 LSTM 网络中，进行时序信息的建模。该特征融合方法参考了人脑在多媒体视频播放过程中的视听信息加工过程，有效地提升了多媒体暴力镜头检测的效果。同时该特征融合方法也为多模态信息融合领域提供一点新的思路。

### 展望

1. 在音视频特征融合中，可
   **能存在音频和视频信息不同步的问题**
   ，针对该问题，可考虑先将音频特征和视频特征进行对齐，然后将对齐后的特征进行建模，进而构建暴力镜头检测系统
2. 扩大检测数据集，从中提取更多的正样本数据。

## 读后感

**收获：**

1. 每个小节前部分增加一些承上启下的问题会大大增加文章的阅读体验，提高逻辑清晰度。
2. 视频可以分为四种级别：帧，镜头，场景，视频流，自己目前的工作好像没有镜头捕捉的这一工作。
3. **这篇文章中的视觉听觉实验部分做的很详细，必须仔细看看，很值得学习，（对比实验观看原文）**
   。
4. 为什么视觉要用convLSTM是因为效果conv对视觉更好，但是音频就不需要了。

**疑惑**

1. 为什么听觉部分语谱图没有原始音频好呢？一般不是语谱图好点。
2. 论文结束时候说到信息不同步，这是自己造成的可能不同步，还是音视频本身不同步呢？因为相比较动作，声音更先发出来。（反过来想想如果都是这样延迟规则，好像dl也能学习到）

**思考**
  
论文中很多方法可以参考，后面肯定会重新读一下细节。公式很多一次性看不完。