---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f5445555445552f:61727469636c652f64657461696c732f313436313737323233"
layout: post
title: "人工智能直通车系列15机器学习基础决策树算法原理ID3C4.5CART决策树模型实现"
date: 2025-03-11 14:11:34 +0800
description: "人工智能直通车系列正在连载更新中，更新的内容包括Python 基础与数学基础、机器学习基础、深度学习基础、完整学习资料以及详细解释代码示例，关注我，持续学习。"
keywords: "人工智能直通车系列15【机器学习基础】（决策树算法原理（ID3、C4.5、CART）决策树模型实现）"
categories: ['人工智能直通车']
tags: ['自然语言处理', '神经网络', '深度学习', '机器学习', '开发语言', '人工智能', 'Python']
artid: "146177223"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146177223
    alt: "人工智能直通车系列15机器学习基础决策树算法原理ID3C4.5CART决策树模型实现"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146177223
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146177223
cover: https://bing.ee123.net/img/rand?artid=146177223
image: https://bing.ee123.net/img/rand?artid=146177223
img: https://bing.ee123.net/img/rand?artid=146177223
---

# 人工智能直通车系列15【机器学习基础】（决策树算法原理（ID3、C4.5、CART）决策树模型实现）

---

### 决策树算法原理

##### 1. 决策树概述

决策树是一种基本的分类与回归方法，它通过对特征进行递归划分，将数据集分割成不同的子集，直到每个子集都尽可能属于同一类别（分类树）或具有相似的数值输出（回归树）。决策树由节点和边组成，内部节点表示一个特征上的测试，分支表示测试输出，叶节点表示类别或值。

##### 2. ID3 算法原理

* **核心思想**
  ：ID3 算法以信息增益作为特征选择的准则，倾向于选择信息增益大的特征进行划分。
* **信息增益**
  ：信息增益是基于信息熵的概念。信息熵是衡量数据不确定性的指标，对于一个分类问题，设数据集
  ![D](https://latex.csdn.net/eq?D)
  中第
  ![k](https://latex.csdn.net/eq?k)
  类样本所占的比例为
  ![p_k](https://latex.csdn.net/eq?p_k)
  ![(k = 1,2,\cdots,K)](https://latex.csdn.net/eq?%28k%20%3D%201%2C2%2C%5Ccdots%2CK%29)
  ，则数据集
  ![D](https://latex.csdn.net/eq?D)
  的信息熵为：
    
  ![H(D)=-\sum_{k = 1}^{K}p_k\log_2p_k](https://latex.csdn.net/eq?H%28D%29%3D-%5Csum_%7Bk%20%3D%201%7D%5E%7BK%7Dp_k%5Clog_2p_k)
    
  假设使用特征
  ![A](https://latex.csdn.net/eq?A)
  对数据集
  ![D](https://latex.csdn.net/eq?D)
  进行划分，划分后得到
  ![V](https://latex.csdn.net/eq?V)
* **算法步骤**
  ：
  1. 计算数据集
     ![D](https://latex.csdn.net/eq?D)
     的信息熵
     ![H(D)](https://latex.csdn.net/eq?H%28D%29)
     。
  2. 对每个特征
     ![A](https://latex.csdn.net/eq?A)
     ，计算其对数据集
     ![D](https://latex.csdn.net/eq?D)
     的信息增益
     ![g(D,A)](https://latex.csdn.net/eq?g%28D%2CA%29)
     。
  3. 选择信息增益最大的特征作为当前节点的划分特征。
  4. 根据该特征的不同取值将数据集
     ![D](https://latex.csdn.net/eq?D)
     划分为不同的子集，对每个子集递归地重复上述步骤，直到子集的样本都属于同一类别或没有可用的特征为止。

##### 3. C4.5 算法原理

* **核心思想**
  ：C4.5 算法是对 ID3 算法的改进，它使用信息增益比作为特征选择的准则，克服了 ID3 算法倾向于选择取值较多的特征的缺点。
* **信息增益比**
  ：信息增益比是信息增益与特征
  ![A](https://latex.csdn.net/eq?A)
  的固有值
  ![IV(A)](https://latex.csdn.net/eq?IV%28A%29)
  的比值，特征
  ![A](https://latex.csdn.net/eq?A)
  的固有值定义为：
    
  ![IV(A)=-\sum_{v = 1}^{V}\frac{|D_v|}{|D|}\log_2\frac{|D_v|}{|D|}](https://latex.csdn.net/eq?IV%28A%29%3D-%5Csum_%7Bv%20%3D%201%7D%5E%7BV%7D%5Cfrac%7B%7CD_v%7C%7D%7B%7CD%7C%7D%5Clog_2%5Cfrac%7B%7CD_v%7C%7D%7B%7CD%7C%7D)
    
  特征
  ![A](https://latex.csdn.net/eq?A)
  对数据集
  ![D](https://latex.csdn.net/eq?D)
  的信息增益比为：
    
  ![g_R(D,A)=\frac{g(D,A)}{IV(A)}](https://latex.csdn.net/eq?g_R%28D%2CA%29%3D%5Cfrac%7Bg%28D%2CA%29%7D%7BIV%28A%29%7D)
* **算法步骤**
  ：与 ID3 算法类似，只是在选择划分特征时使用信息增益比代替信息增益。

##### 4. CART 算法原理

* **核心思想**
  ：CART（Classification and Regression Trees）算法既可以用于分类问题，也可以用于回归问题。对于分类问题，使用基尼指数作为特征选择的准则；对于回归问题，使用均方误差作为划分准则。
* **基尼指数**
  ：对于一个分类问题，数据集 D 的基尼指数定义为：
    
  ![Gini(D)=1-\sum_{k = 1}^{K}p_k^2](https://latex.csdn.net/eq?Gini%28D%29%3D1-%5Csum_%7Bk%20%3D%201%7D%5E%7BK%7Dp_k%5E2)
    
  假设使用特征
  ![A](https://latex.csdn.net/eq?A)
  及其取值
  ![a](https://latex.csdn.net/eq?a)
  对数据集
  ![D](https://latex.csdn.net/eq?D)
  进行划分，得到两个子集
  ![D_1](https://latex.csdn.net/eq?D_1)
  和
  ![D_2](https://latex.csdn.net/eq?D_2)
  ，则特征
  ![A](https://latex.csdn.net/eq?A)
  在取值
  ![a](https://latex.csdn.net/eq?a)
  处的基尼指数为：
    
  ![Gini(D,A,a)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)](https://latex.csdn.net/eq?Gini%28D%2CA%2Ca%29%3D%5Cfrac%7B%7CD_1%7C%7D%7B%7CD%7C%7DGini%28D_1%29&plus;%5Cfrac%7B%7CD_2%7C%7D%7B%7CD%7C%7DGini%28D_2%29)
* **算法步骤**
  ：
  + **分类树**
    ：
    1. 对于每个特征
       ![A](https://latex.csdn.net/eq?A)
       及其每个可能的取值
       ![a](https://latex.csdn.net/eq?a)
       ，计算基尼指数
       ![Gini(D,A,a)](https://latex.csdn.net/eq?Gini%28D%2CA%2Ca%29)
       。
    2. 选择基尼指数最小的特征和取值作为当前节点的划分条件。
    3. 根据划分条件将数据集
       ![D](https://latex.csdn.net/eq?D)
       划分为两个子集，对每个子集递归地重复上述步骤，直到满足停止条件（如子集样本数小于某个阈值）。
  + **回归树**
    ：选择使得划分后两个子集的均方误差之和最小的特征和划分点进行划分。

#### 决策树模型实现（scikit - learn）

##### 代码实现步骤

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

# 1. 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 2. 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. 创建决策树分类器
# 使用 CART 算法（默认）
clf = DecisionTreeClassifier(random_state=42)

# 4. 训练模型
clf.fit(X_train, y_train)

# 5. 进行预测
y_pred = clf.predict(X_test)

# 6. 评估模型性能
accuracy = accuracy_score(y_test, y_pred)
print(f"模型准确率: {accuracy}")

# 7. 可视化决策树
plt.figure(figsize=(12, 8))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()

```

##### 代码解释

1. **数据加载**
   ：使用
   `load_iris`
   函数加载鸢尾花数据集，该数据集包含 4 个特征和 3 个类别。
2. **数据划分**
   ：使用
   `train_test_split`
   函数将数据集划分为训练集和测试集，测试集占比 20%。
3. **模型创建**
   ：创建
   `DecisionTreeClassifier`
   类的实例
   `clf`
   ，使用默认的 CART 算法。
4. **模型训练**
   ：调用
   `fit`
   方法，使用训练集数据
   `X_train`
   和对应的标签
   `y_train`
   对模型进行训练。
5. **模型预测**
   ：使用训练好的模型对测试集数据
   `X_test`
   进行预测，得到预测结果
   `y_pred`
   。
6. **模型评估**
   ：使用
   `accuracy_score`
   函数计算模型的准确率。
7. **模型可视化**
   ：使用
   `plot_tree`
   函数将决策树可视化，展示决策树的结构和划分规则。

#### 场景示例

##### 贷款审批

银行在进行贷款审批时，可以收集客户的各种信息，如年龄、收入、信用记录等作为特征，客户是否违约作为目标变量。使用决策树模型，银行可以根据客户的信息判断客户是否有违约风险，从而决定是否批准贷款。

##### 疾病诊断

在医疗领域，医生可以收集患者的症状、检查结果等信息作为特征，患者是否患有某种疾病作为目标变量。通过决策树模型，医生可以根据患者的信息进行初步的疾病诊断，辅助制定治疗方案。