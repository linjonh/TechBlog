---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f6e65776561737473756e2f:61727469636c652f64657461696c732f313436313139363838"
layout: post
title: "PyTorch系列教程Tensor.view-方法详解"
date: 2025-03-08 17:47:44 +08:00
description: "这篇简明扼要的文章是关于PyTorch中的tensor.view()方法的介绍与应用，与reshape()方法的区别，同时给出示例进行详细解释。"
keywords: "pytorch tensor.view"
categories: ['人工智能', 'Python']
tags: ['人工智能', 'Pytorch', 'Python']
artid: "146119688"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146119688
    alt: "PyTorch系列教程Tensor.view-方法详解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146119688
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146119688
cover: https://bing.ee123.net/img/rand?artid=146119688
image: https://bing.ee123.net/img/rand?artid=146119688
img: https://bing.ee123.net/img/rand?artid=146119688
---

# PyTorch系列教程：Tensor.view() 方法详解

> 这篇简明扼要的文章是关于PyTorch中的tensor.view()方法的介绍与应用，与reshape()方法的区别，同时给出示例进行详细解释。

### Tensor基础

Tensor(张量)的视图是一个新的Tensor，它与原始Tensor共享相同的底层数据，但具有不同的形状或大小。view()方法用于在不改变其数据的情况下将张量重塑为新的形状。它返回原始张量的新视图。这意味着修改新的张量会影响原来的张量，反之亦然。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a3aa45e442e74b2b944f8a37f9d620a7.png)

语法:

```python
Tensor.view(*shape) -> Tensor

```

* self：想要重塑的输入Tensor。
* \*shape：一个
  `torch.Size`
  对象或指定输出张量所需形状的整数序列。还可以使用-1从其他维度推断一个维度的大小。

然而，Tensor.view()仅适用于连续张量，即存储在连续内存中的张量。如果输入张量不是连续的，则需要在调用tensor .view()之前调用tensor.consecuous()。你可以通过调用tensor.is\_consecuous()来检查张量是否是连续的。

### 代码示例

下面演示如何在实践中使用tensor.view()方法：

```python
>>> import torch
>>> torch.manual_seed(2023)
>>> # Create a tensor with the shape of 4x4
>>> x = torch.randn(4, 4)
>>> print(x)
tensor([[ 0.4305, -0.3499,  0.4749,  0.9041],
        [-0.7021,  1.5963,  0.4228, -0.6940],
        [ 0.9672,  1.5569, -2.3860,  0.6994],
        [-1.0325, -2.6043,  0.9337, -0.1050]])
>>> y = x.view(16)
>>> print(y)
tensor([ 0.4305, -0.3499,  0.4749,  0.9041, -0.7021,  1.5963,  0.4228, -0.6940,
         0.9672,  1.5569, -2.3860,  0.6994, -1.0325, -2.6043,  0.9337, -0.1050])
>>> z=x.view(2,8)
>>> print(z)
tensor([[ 0.4305, -0.3499,  0.4749,  0.9041, -0.7021,  1.5963,  0.4228, -0.6940],
        [ 0.9672,  1.5569, -2.3860,  0.6994, -1.0325, -2.6043,  0.9337, -0.1050]])
>>> w=x.view(-1,2)
>>> print(w)
tensor([[ 0.4305, -0.3499],
        [ 0.4749,  0.9041],
        [-0.7021,  1.5963],
        [ 0.4228, -0.6940],
        [ 0.9672,  1.5569],
        [-2.3860,  0.6994],
        [-1.0325, -2.6043],
        [ 0.9337, -0.1050]])
>>> 

```

你可以看到y和x有相同的数据，但形状不同，是16x1。Z和x有相同的数据，但形状不同，是2x8。W和x有相同的数据，但形状不同，是8x2。w的最后一个维度是从其他维度和x中的元素数量推断出来的。

### Torch.view()和torch.reshape()的区别

torch.reshape()函数和tensor .view()方法之间的区别在于torch.reshape()可以返回原始张量的视图或副本，具体取决于新形状是否与原始形状和步幅兼容，而tensor .view()总是返回原始张量的视图，但仅适用于连续张量。

当你想重塑一个张量而不担心它的连续性或复制行为时，你应该使用torch.reshape()，当你想重塑一个连续张量并确保它与原始张量共享相同的数据时，你应该使用tensor. view()。

#### **示例对比**

##### **场景 1：数据连续时，两者行为一致**

```python
import torch

# 创建一个连续存储的张量 (2x3)
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
print("原始形状:", x.shape)      # (2, 3)
print("是否连续:", x.is_contiguous())  # True

# 使用 view 改变形状 (3x2)
y = x.view(3, 2)
print("view 结果:", y)           # [[1, 2], [3, 4], [5, 6]]

# 使用 reshape 改变形状 (3x2)
z = x.reshape(3, 2)
print("reshape 结果:", z)        # [[1, 2], [3, 4], [5, 6]]

```

**结果一致**
，且未复制数据（
`is_contiguous()`
返回
`True`
）。

##### **场景 2：数据不连续时， `view` 失败， `reshape` 成功**

```python
# 转置后数据不再连续（原按行存储，转置后按列逻辑访问）
x_transposed = x.t()
print("转置后形状:", x_transposed.shape)      # (3, 2)
print("是否连续:", x_transposed.is_contiguous())  # False

# 尝试用 view 改变形状 → 报错！
try:
    y = x_transposed.view(2, 3)
except RuntimeError as e:
    print("view 错误:", str(e))  # "invalid argument: cannot view a non-contiguous tensor as ..."

# 使用 reshape 成功（自动复制数据）
z = x_transposed.reshape(2, 3)
print("reshape 结果:", z)         # [[1, 4], [2, 5], [3, 6]]
print("是否连续:", z.is_contiguous())  # True

```

##### **场景 3： `reshape` 的灵活性（使用 `-1` ）**

```python
# 不确定某一维度的大小时，仅用 reshape
x_2d = torch.randn(4, 6)  # 形状 (4, 6)
# 将最后一维压缩为总和，剩余维度自动推断
y = x_2d.reshape(-1, 3)   # 结果形状: (8, 3)
print(y.shape)              # torch.Size([8, 3])

```

因此，我们应
**优先用
`view`**
：当数据连续且形状明确时，
`view`
更高效（零复制）；
**优先用
`reshape`**
：需处理非连续数据、动态推断维度或不确定形状时。