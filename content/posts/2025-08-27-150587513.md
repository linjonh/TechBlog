---
layout: post
title: "线性回归原理推导与应用十一多重共线性"
date: 2025-08-27T16:02:03+0800
description: "多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。根据定义和影响程度，可以将多重共线性分为极端共线性和一般共线性。极端共线性一般是指变量间有准确的相关关系，例如x1​x2​1x3​3x2​等，一般共线性则是指自变量高度相关。"
keywords: "线性回归原理推导与应用（十一）：多重共线性"
categories: ['机器学习', 'Python']
tags: ['线性回归', '算法', '回归']
artid: "150587513"
arturl: "https://blog.csdn.net/qq_42692386/article/details/150587513"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150587513
    alt: "线性回归原理推导与应用十一多重共线性"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150587513
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150587513
cover: https://bing.ee123.net/img/rand?artid=150587513
image: https://bing.ee123.net/img/rand?artid=150587513
img: https://bing.ee123.net/img/rand?artid=150587513
---



# 线性回归原理推导与应用（十一）：多重共线性



## 多重共线性的定义与影响

多重共线性（Multicollinearity）是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。

根据定义和影响程度，可以将多重共线性分为极端共线性和一般共线性。极端共线性一般是指变量间有准确的相关关系，例如
x
1
=
x
2
+
1
,
x
3
=
3
x
2
x_1= x_2+ 1,x_3= 3x_2
x1​=x2​+1,x3​=3x2​等，一般共线性则是指自变量高度相关。

## 多重共线性对模型的影响

光看定义比较抽象，所以下面我们结合多元线性回归的原理具体说明一下什么是多重共线性以及其对线性模型的影响。

在多元线性回归中,我们使用多个自变量来预测因变量,其方程可以表示为:  
 
y
=
w
0
+
w
1
x
1
+
w
2
x
2
+
⋯
+
w
n
x
n
y =w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n
y=w0​+w1​x1​+w2​x2​+⋯+wn​xn​  
 其中:  
 
x
1
,
x
2
,
…
,
x
n
x_1, x_2, …, x_n
x1​,x2​,…,xn​ 表示不同的自变量，
w
1
,
w
2
,
…
,
w
n
w_1, w_2, …, w_n
w1​,w2​,…,wn​ 表示各个自变量对应的回归系数。

### 对模型参数的影响

现在假设有这样一个多元线性回归方程模型：  
 
y
=
10
+
2
x
1
+
5
x
2
y = 10 + 2x_1 + 5x_2
y=10+2x1​+5x2​  
 假设 
x
1
x_1
x1​和 
x
2
x_2
x2​ 之间存在强相关性,我们可以将它们的关系表示为:  
 
x
1
=
x
2
+
1
x_1= x_2+ 1
x1​=x2​+1  
 那么,原始方程可以转化为以下几种形式：  
 
y
=
12
+
7
x
2
y
=
5
+
7
x
1
y
=
7.5
+
4.5
x
1
+
2.5
x
2
⋯
y = 12 + 7x_2 \\ y = 5 + 7x_1\\ y = 7.5 + 4.5x_1 + 2.5x_2 \\ \cdots
y=12+7x2​y=5+7x1​y=7.5+4.5x1​+2.5x2​⋯  
 可以发现如果存在多重共线性，可以得到一系列不同回归系数下自变量的组合，这些回归模型的回归系数不同，但得到的预测值
y
y
y完全相同(自变量精确相关，极端共线性)或相差不大（自变量高度相关，一般共线性），此时不同模型下的误差平方和相同或相差不大，都可能是恰当的模型，所以如果存在多重共线性，会导致模型的截距项(intercept)和系数(coefficients)无法确定,回归系数可能有很大波动，例如可能由正变负，还可能变为0等等。

### 对求解过程的影响

在之前的多元回归原理的推导中我们讲解并推导了最小二乘法求解的过程，如果对这一方面不熟悉的可以看一下这篇文章：<https://smilecoc.blog.csdn.net/article/details/138210463>，在这里我们推导出求解参数的矩阵形式：  
 
w
=
(
X
T
X
)
−
1
X
T
Y
w=(X^TX)^{−1}X^TY
w=(XTX)−1XTY  
 可以看到计算公式中需要计算逆矩阵 
w
=
(
X
T
X
)
−
1
w=(X^TX)^{−1}
w=(XTX)−1,且
(
X
T
X
)
−
1
=
(
X
T
X
)
∗
∣
X
T
X
∣
(X^{T}X)^{-1}=\frac{(X^{T}X)^*}{|X^{T}X|}
(XTX)−1=∣XTX∣(XTX)∗​，当存在共线性时，若  
 (1)自变量存在精确相关关系，则行列式 
∣
X
T
X
∣
|X^{T}X|
∣XTX∣ 为0,矩阵不可逆,不存在逆矩阵。即此时求不出  
 的最小二乘估计。  
 (2)自变量存在高度相关关系，比如 
x
1
≈
2
x
2
x_1\approx 2x_2
x1​≈2x2​,则行列式 
∣
X
T
X
∣
|X^{T}X|
∣XTX∣ 近似为0,由于处在计算公式的分母上，此时计算得到的回归系数的偏差会很大。

## 多重共线性的判断

那么如何判断变量之间是否有多重共线性呢？一般可以通过方差膨胀因子(Variance inflation factor，VIF)和容忍度(tolerance,T)来诊断多重共线性,VIF和容忍度两者互为倒数，两者的计算公式分别为：  
 
T
=
1
−
R
i
2
V
I
F
=
1
1
−
R
i
2
T={1-R_i^2}\\[15pt]VIF=\frac{1}{1-R_i^2}
T=1−Ri2​VIF=1−Ri2​1​

这两个指标中均涉及R方，先来复习一下这个概念：R方是回归分析中的一个关键概念，也称为决定系数(coefficient of determination),记作
R
2
R^2
R2。
R
2
R^2
R2用于评估回归模型对数据的拟合优度。例如,
R
2
=
0.9
R^2= 0.9
R2=0.9 意味着目标变量
y
y
y中90%的变化可以由模型中的自变量解释。
R
2
R^2
R2的具体计算原理和公式在之前的线性回归系列文章中也有说明过，感兴趣的可以查看。

VIF如何判断出多重共线性呢？VIF首先为每个自变量拟合一个线性回归模型,使用其余的自变量作为预测变量。  
 
x
1
=
α
1
x
2
+
α
2
x
3
+
…
+
α
n
−
1
x
n
x
2
=
θ
1
x
1
+
θ
2
x
3
+
…
+
θ
n
−
1
x
n
⋯
x
n
=
δ
1
x
1
+
δ
2
x
2
+
…
+
δ
n
−
1
x
n
−
1
x_1 = \alpha_1 x_2 + \alpha_2x_3 + … + \alpha_{n-1}x_n\\ x_2 = \theta_1 x_1 + \theta_2x_3 + … + \theta_{n-1}x_n \\ \cdots \\ x_n = \delta_1 x_1 + \delta_2x_2 + … + \delta_{n-1}x_{n-1}
x1​=α1​x2​+α2​x3​+…+αn−1​xn​x2​=θ1​x1​+θ2​x3​+…+θn−1​xn​⋯xn​=δ1​x1​+δ2​x2​+…+δn−1​xn−1​  
 之后对于每个线性回归模型计算决定系数
R
2
R^2
R2。每个自变量，每个回归方程都可以计算出对应的
R
2
R^2
R2值(记为
R
i
2
R_i^2
Ri2​),表示其他自变量能够解释该自变量变变化的程度。使用上面的VIF计算公式即可计算出每个自变量对应的VIF值。

一般地，当VIF的最大值>10（
R
i
2
>
0.9
R_i^2 > 0.9
Ri2​>0.9）时,则认为有严重的共线性，建议处理，如果
5
≤
V
I
F
<
10
(
0.8
≤
R
i
2
<
0.9
)
5 ≤ VIF < 10(0.8 ≤R_i^2 < 0.9)
5≤VIF<10(0.8≤Ri2​<0.9)时，则认为有轻度共线性，需关注。

## Python计算VIF

Python计算VIF的方法有：使用statsmodels库、使用pandas和numpy库手动计算,这里使用statsmodels库，它提供了一个名为variance_inflation_factor()的函数来计算VIF

```py
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor

# 示例数据集
data = {'X1': [1, 2, 3, 4, 5],
'X2': [2, 4, 5, 8, 9],
'X3': [29, 3, 7, 18, 6],
'Y': [1, 2, 3, 4, 5]
}

df = pd.DataFrame(data)

#在计算VIF之前，我们需要移除目标变量，只保留自变量
X = df[['X1', 'X2', 'X3']]
vif_data = pd.DataFrame()
vif_data['Feature'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print(vif_data)

```

运行代码后即可得到每一个自变量的VIF值。

```
  Feature         VIF
0      X1  256.918238
1      X2  266.626208
2      X3    1.835335

```

从结果可以看出前两个自变量存在很强的共线性。

## 如何消除多重共线性

最后来说一下如何消除多重共线性。

进行建模时，首先需要计算两两相关系数，将相关系数较高的变量去除。这一步相当于对变量进行一次初筛，注意：这一步只是降维，不保证消除多重共线性，因为 3 个以上变量仍可能“抱团”。

其次在建模中，可以使用岭回归，Lasso回归等正则化收缩回归系数来减轻多重共线性的影响，并可以删除系数趋近于0的变量。

最后可以计算VIF值，当VIF值过大的时候就需要对变量进行处理。

除了直接删除变量外，如果有的时候不好删除变量，可以使用PCA等降维方法保留大部分的变量信息。

参考文章：  
 https://zhuanlan.zhihu.com/p/355241680  
 https://avoid.overfit.cn/post/512ff1c71eb14f758ff82a94baf06f4c  
 ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/989f75d9628063aaf5ba27bc1fc8a34b.png)



