---
layout: post
title: "Spark-中创建-DataFrame-的2种方式对比"
date: 2025-03-15 16:13:38 +0800
description: "适合简单场景，自动推断模式。：适合复杂场景，显式定义模式。根据数据结构和需求选择合适的方式即可！如果还有其他问题，欢迎继续提问。"
keywords: "Spark 中创建 DataFrame 的2种方式对比"
categories: ['未分类']
tags: ['大数据', '分布式', 'Spark', 'Scala']
artid: "146280949"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146280949
    alt: "Spark-中创建-DataFrame-的2种方式对比"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146280949
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146280949
cover: https://bing.ee123.net/img/rand?artid=146280949
image: https://bing.ee123.net/img/rand?artid=146280949
img: https://bing.ee123.net/img/rand?artid=146280949
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Spark 中创建 DataFrame 的2种方式对比
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     spark.createDataFrame(data).toDF("name", "age")
    </p>
    <p>
     和
    </p>
    <p>
     spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
    </p>
    <p>
     创建df的方式有什么区别？
    </p>
    <p>
    </p>
    <p>
     在 Spark 中，创建 DataFrame 的方式有多种，其中两种常见的方式是：
    </p>
    <ol>
     <li>
      <p>
       <strong>
        <code>
         spark.createDataFrame(data).toDF("name", "age")
        </code>
       </strong>
      </p>
     </li>
     <li>
      <p>
       <strong>
        <code>
         spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
        </code>
       </strong>
      </p>
     </li>
    </ol>
    <p>
     这两种方式的主要区别在于
     <strong>
      数据来源
     </strong>
     和
     <strong>
      模式（Schema）的定义方式
     </strong>
     。下面详细分析它们的区别和适用场景。
    </p>
    <hr/>
    <h4>
     1.
     <strong>
      <code>
       spark.createDataFrame(data).toDF("name", "age")
      </code>
     </strong>
    </h4>
    <h5>
     特点
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        数据来源
       </strong>
       ：
       <code>
        data
       </code>
       是一个本地集合（如
       <code>
        Seq
       </code>
       或
       <code>
        List
       </code>
       ），Spark 会将其并行化为分布式数据集（RDD）。
      </p>
     </li>
     <li>
      <p>
       <strong>
        模式推断
       </strong>
       ：Spark 会自动推断数据的模式（Schema），并根据列的顺序为列命名。
      </p>
     </li>
     <li>
      <p>
       <strong>
        列名指定
       </strong>
       ：通过
       <code>
        toDF("name", "age")
       </code>
       显式指定列名。
      </p>
     </li>
    </ul>
    <h5>
     示例
    </h5>
    <pre><code class="language-Scala">import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("DataFrame Example")
  .master("local[*]")
  .getOrCreate()

// 数据是一个本地集合
val data = Seq(("Alice", 25), ("Bob", 30), ("Charlie", 35))

// 创建 DataFrame，并指定列名
val df = spark.createDataFrame(data).toDF("name", "age")

df.show()</code></pre>
    <p>
     <strong>
      输出：
     </strong>
    </p>
    <pre><code class="language-Scala">+-------+---+
|   name|age|
+-------+---+
|  Alice| 25|
|    Bob| 30|
|Charlie| 35|
+-------+---+</code></pre>
    <h5>
     适用场景
    </h5>
    <ul>
     <li>
      <p>
       数据量较小，可以直接在本地集合中定义。
      </p>
     </li>
     <li>
      <p>
       不需要显式定义复杂的模式（Schema）。
      </p>
     </li>
     <li>
      <p>
       列名可以通过
       <code>
        toDF
       </code>
       简单指定。
      </p>
     </li>
    </ul>
    <hr/>
    <h4>
     2.
     <strong>
      <code>
       spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
      </code>
     </strong>
    </h4>
    <h5>
     特点
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        数据来源
       </strong>
       ：
       <code>
        data
       </code>
       是一个本地集合，通过
       <code>
        spark.sparkContext.parallelize(data)
       </code>
       将其显式转换为 RDD。
      </p>
     </li>
     <li>
      <p>
       <strong>
        模式定义
       </strong>
       ：需要显式定义一个模式（
       <code>
        StructType
       </code>
       ），指定每列的名称和数据类型。
      </p>
     </li>
     <li>
      <p>
       <strong>
        灵活性
       </strong>
       ：适合处理复杂的数据结构（如嵌套结构体）。
      </p>
     </li>
    </ul>
    <h5>
     示例
    </h5>
    <pre><code class="language-Scala">import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("DataFrame Example")
  .master("local[*]")
  .getOrCreate()

// 数据是一个本地集合，每个元素是一个 Row 对象
val data = Seq(
  Row("Alice", 25),
  Row("Bob", 30),
  Row("Charlie", 35)
)

// 定义模式
val schema = new StructType()
  .add(StructField("name", StringType, nullable = false))
  .add(StructField("age", IntegerType, nullable = false))

// 创建 DataFrame
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)

df.show()</code></pre>
    <p>
     <strong>
      输出：
     </strong>
    </p>
    <pre><code class="language-Scala">+-------+---+
|   name|age|
+-------+---+
|  Alice| 25|
|    Bob| 30|
|Charlie| 35|
+-------+---+</code></pre>
    <h5>
     适用场景
    </h5>
    <ul>
     <li>
      <p>
       数据量较大，需要显式并行化为 RDD。
      </p>
     </li>
     <li>
      <p>
       数据结构复杂，需要显式定义模式（Schema）。
      </p>
     </li>
     <li>
      <p>
       需要更精确地控制列的数据类型和是否允许为空。
      </p>
     </li>
    </ul>
    <hr/>
    <h4>
     3.
     <strong>
      主要区别
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        特性
       </th>
       <th>
        <code>
         spark.createDataFrame(data).toDF("name", "age")
        </code>
       </th>
       <th>
        <code>
         spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
        </code>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         数据来源
        </strong>
       </td>
       <td>
        本地集合（自动并行化为 RDD）
       </td>
       <td>
        本地集合（显式并行化为 RDD）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         模式定义
        </strong>
       </td>
       <td>
        自动推断模式
       </td>
       <td>
        需要显式定义模式（
        <code>
         StructType
        </code>
        ）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         列名指定
        </strong>
       </td>
       <td>
        通过
        <code>
         toDF
        </code>
        指定列名
       </td>
       <td>
        在模式中定义列名
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         数据类型控制
        </strong>
       </td>
       <td>
        自动推断数据类型
       </td>
       <td>
        可以显式指定每列的数据类型
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         是否允许为空
        </strong>
       </td>
       <td>
        默认允许为空
       </td>
       <td>
        可以显式指定是否允许为空
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         适用场景
        </strong>
       </td>
       <td>
        简单数据结构，数据量较小
       </td>
       <td>
        复杂数据结构，数据量较大
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h4>
     4.
     <strong>
      选择哪种方式？
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        使用
        <code>
         spark.createDataFrame(data).toDF("name", "age")
        </code>
        的情况
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         数据量较小，可以直接在本地集合中定义。
        </p>
       </li>
       <li>
        <p>
         数据结构简单，不需要显式定义模式。
        </p>
       </li>
       <li>
        <p>
         列名可以通过
         <code>
          toDF
         </code>
         简单指定。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        使用
        <code>
         spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
        </code>
        的情况
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         数据量较大，需要显式并行化为 RDD。
        </p>
       </li>
       <li>
        <p>
         数据结构复杂，需要显式定义模式。
        </p>
       </li>
       <li>
        <p>
         需要精确控制列的数据类型和是否允许为空。
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4>
     5.
     <strong>
      复杂数据结构示例
     </strong>
    </h4>
    <p>
     如果需要处理嵌套结构体（如数组或结构体），推荐使用显式定义模式的方式。
    </p>
    <h5>
     示例：嵌套结构体
    </h5>
    <pre><code class="language-Scala">import org.apache.spark.sql.{SparkSession, Row}
import org.apache.spark.sql.types._

val spark = SparkSession.builder()
  .appName("Nested DataFrame Example")
  .master("local[*]")
  .getOrCreate()

// 数据是一个本地集合，包含嵌套结构
val data = Seq(
  Row("Alice", Row("Java", 5)),
  Row("Bob", Row("Python", 4))
)

// 定义模式
val schema = new StructType()
  .add(StructField("name", StringType, nullable = false))
  .add(StructField("skill", new StructType()
    .add(StructField("name", StringType, nullable = false))
    .add(StructField("level", IntegerType, nullable = false))
  ))

// 创建 DataFrame
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)

df.printSchema()
df.show()</code></pre>
    <p>
     <strong>
      输出：
     </strong>
    </p>
    <pre><code class="language-Scala">root
 |-- name: string (nullable = false)
 |-- skill: struct (nullable = false)
 |    |-- name: string (nullable = false)
 |    |-- level: integer (nullable = false)

+-----+----------+
| name|     skill|
+-----+----------+
|Alice| [Java, 5]|
|  Bob|[Python, 4]|
+-----+----------+</code></pre>
    <hr/>
    <h4>
     总结
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        <code>
         spark.createDataFrame(data).toDF("name", "age")
        </code>
       </strong>
       ：适合简单场景，自动推断模式。
      </p>
     </li>
     <li>
      <p>
       <strong>
        <code>
         spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
        </code>
       </strong>
       ：适合复杂场景，显式定义模式。
      </p>
     </li>
    </ul>
    <p>
     根据数据结构和需求选择合适的方式即可！如果还有其他问题，欢迎继续提问。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36333332323132322f:61727469636c652f64657461696c732f313436323830393439" class_="artid" style="display:none">
 </p>
</div>


