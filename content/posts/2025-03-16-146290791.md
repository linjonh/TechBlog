---
layout: post
title: "卷积神经网络知识点"
date: 2025-03-16 23:52:33 +0800
description: "这主要是通过池化层（如最大池化）或步长大于1的卷积实现的。：随着特征图尺寸的减小，较高层的神经元具有更大的感受野，从而能够覆盖原始输入图像的更大区域。由两种方法：1.增大步长：卷积的时候不是一次一步，而是一次多步，类似一张图片，在原来的像素基础上，每隔一个取一个像素点。：所有组的输出会被拼接在一起，再经过另一个1x1的卷积层来恢复通道数，最后加上原始输入（残差连接），得到最终的输出。是在ResNet基础上进一步发展的模型，它引入了“分组卷积”的概念，旨在以更少的参数实现更高的准确率。"
keywords: "卷积神经网络（知识点）"
categories: ['未分类']
tags: ['神经网络', '人工智能', 'Cnn']
artid: "146290791"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146290791
    alt: "卷积神经网络知识点"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146290791
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146290791
cover: https://bing.ee123.net/img/rand?artid=146290791
image: https://bing.ee123.net/img/rand?artid=146290791
img: https://bing.ee123.net/img/rand?artid=146290791
---

# 卷积神经网络（知识点）

### 一、为了使特征图变小：

由两种方法：1.增大步长：卷积的时候不是一次一步，而是一次多步，类似一张图片，在原来的像素基础上，每隔一个取一个像素点。

![](https://i-blog.csdnimg.cn/direct/c2858bcd76f24ff6adc85913afb9cac2.png)

其中S就是步长

注意：扩大步长不经常用，因为会丢失信息，而且还会引入计算

2.池化：1)平均池化

2)最大池化

### 二、卷积的过程中，希望通道数增加，而特征图大小减小

  1. **增强特征表达能力** ：通过增加通道数，每个卷积层能够学习更多种类的特征。初始层通常识别基本的边缘、颜色和纹理等低级特征，而随着层数加深，网络能够捕捉更加复杂的模式和高层次语义信息。更多的通道意味着网络可以同时处理并整合更丰富的特征表示。

  2. **减少计算复杂度和参数数量** ：虽然增加通道数增加了模型的容量，但减小特征图的大小有助于控制计算成本和内存使用。这主要是通过池化层（如最大池化）或步长大于1的卷积实现的。减小特征图尺寸可以显著降低后续层的计算负担，因为它们需要处理的数据点变少了。

  3. **空间不变性** ：通过下采样（例如，利用池化操作），网络可以获得一定程度的空间不变性，这意味着它对输入图像中的微小平移变得更加鲁棒。这对于许多视觉任务来说是非常重要的，因为它允许模型专注于最重要的特征，而不是被不相关的小变化所干扰。

  4. **有效捕获全局信息** ：随着特征图尺寸的减小，较高层的神经元具有更大的感受野，从而能够覆盖原始输入图像的更大区域。这意味着这些层可以捕获到更全局的信息，有助于理解场景的整体结构和上下文关系。

### 三、归一化和Dropout

归一化就是不同的样本，同一属性才会相对比较，不会受量纲的影响

![](https://i-blog.csdnimg.cn/direct/6698379d51a547f58b717aab74ab840f.png)

![](https://i-blog.csdnimg.cn/direct/c68160af5f6348ccaa7ac56d29d70508.png)

**Dropout**
是一种用于防止神经网络过拟合的技术。在在每一轮训练过程中，Dropout通过随机“丢弃”一部分神经元（即设置这些神经元的输出为零），从而强制网络学习更鲁棒的特征表示。这样做可以避免模型过于依赖某些特定的神经元，促使模型学习到更加泛化的特征。

### 三个基本卷积神经网络模型

#### AlexNet

**AlexNet** 是由Alex Krizhevsky等人提出的，在2012年的ImageNet图像识别挑战赛中大放异彩，标志着深度学习时代的开始。

  * **创新点** ： 
    * 使用ReLU(Rectified Linear Unit)激活函数代替传统的tanh或sigmoid函数，加快了训练速度。
    * 引入了Dropout技术来减少过拟合。
    * 利用数据增强技术提高模型的泛化能力。
    * 使用GPU进行加速训练，解决了大规模数据和复杂模型带来的计算问题。
    * 采用了局部响应归一化(Local Response Normalization, LRN)，虽然后续研究表明这一步并非必不可少。

#### VGG

**VGG** 是由牛津大学视觉几何组（Visual Geometry Group）开发的一系列深度卷积神经网络架构。

  * **创新点** ： 
    * 简洁的网络结构：所有卷积层都使用3x3的小卷积核，并且步长为1，填充为1；池化层则采用2x2窗口，步长为2。这种设计让网络可以更深，同时保持参数量相对较小。
    * 深度增加：VGG通过堆叠多个这样的层，构建出了比以往任何网络都要深的架构（如VGG-16和VGG-19），证明了网络深度对于性能的重要性。
    * 标准化配置：由于其简单而一致的架构，VGG成为了许多研究的基础模型，便于比较不同方法的效果。

#### ResNeXt

**ResNeXt** 是在ResNet基础上进一步发展的模型，它引入了“分组卷积”的概念，旨在以更少的参数实现更高的准确率。

  * **创新点** ： 
    * 分组卷积（Cardinality）：这是ResNeXt的关键创新之一，通过将输入通道分成若干组，每组独立进行卷积操作，然后再合并结果。这种方式可以在不显著增加计算成本的情况下增加网络的表现力。
    * **合并与残差连接** ：所有组的输出会被拼接在一起，再经过另一个1x1的卷积层来恢复通道数，最后加上原始输入（残差连接），得到最终的输出。

    * 统一的设计原则：ResNeXt提出了一个统一的架构设计理念，即每个残差块内的变换可以用三个参数描述：宽度（滤波器的数量）、深度（网络的层数）和基数（分组卷积中的组数）。这种设计简化了超参数的选择过程。
    * 增强的表达能力：与相同参数量的传统网络相比，ResNeXt能够提供更强的特征表示能力，尤其适合处理复杂的视觉任务



