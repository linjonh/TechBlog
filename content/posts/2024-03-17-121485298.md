---
layout: post
title: "TVM系列二TVM介绍"
date: 2024-03-17 23:15:10 +0800
description: "本文详细介绍了TVM的工作流程，包括前端导入、编译转化、目标代码转换和运行过程。TVM由编译器和ru"
keywords: "tvm"
categories: ['机器学习', 'Tvm', 'Tensorflow']
tags: ['深度学习', '机器学习', '人工智能', 'Tensorflow']
artid: "121485298"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=121485298
    alt: "TVM系列二TVM介绍"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=121485298
featuredImagePreview: https://bing.ee123.net/img/rand?artid=121485298
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【TVM系列二】TVM介绍
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <blockquote>
     <p>
      文章同步更新在公众号 AIPlayer，欢迎扫码关注，共同进步
     </p>
     <p>
      <img alt="" height="132" src="https://i-blog.csdnimg.cn/blog_migrate/16a8a7501b02d8d06a1ec16296a8d91c.jpeg" width="132"/>
     </p>
    </blockquote>
    <p id="main-toc">
     <strong>
      目录
     </strong>
    </p>
    <p id="%E4%B8%80%E3%80%81TVM%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B-toc" style="margin-left:0px;">
     <a href="#%E4%B8%80%E3%80%81TVM%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B" rel="nofollow" title="一、TVM的工作流程">
      一、TVM的工作流程
     </a>
    </p>
    <p id="1%E3%80%81%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B-toc" style="margin-left:40px;">
     <a href="#1%E3%80%81%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B" rel="nofollow" title="1、整体流程">
      1、整体流程
     </a>
    </p>
    <p id="2%E3%80%81%E5%85%B3%E9%94%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;">
     <a href="#2%E3%80%81%E5%85%B3%E9%94%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84" rel="nofollow" title="2、关键数据结构">
      2、关键数据结构
     </a>
    </p>
    <p id="3%E3%80%81Transformations-toc" style="margin-left:40px;">
     <a href="#3%E3%80%81Transformations" rel="nofollow" title="3、Transformations">
      3、Transformations
     </a>
    </p>
    <p id="4%E3%80%81%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BD%AC%E6%8D%A2-toc" style="margin-left:40px;">
     <a href="#4%E3%80%81%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BD%AC%E6%8D%A2" rel="nofollow" title="4、搜索空间和基于机器学习的转换">
      4、搜索空间和基于机器学习的转换
     </a>
    </p>
    <p id="5%E3%80%81%E7%9B%AE%E6%A0%87%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96-toc" style="margin-left:40px;">
     <a href="#5%E3%80%81%E7%9B%AE%E6%A0%87%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96" rel="nofollow" title="5、目标代码转化">
      5、目标代码转化
     </a>
    </p>
    <p id="%E4%BA%8C%E3%80%81%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6-toc" style="margin-left:0px;">
     <a href="#%E4%BA%8C%E3%80%81%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6" rel="nofollow" title="二、逻辑架构组件">
      二、逻辑架构组件
     </a>
    </p>
    <p id="%E4%B8%89%E3%80%81%E8%BF%90%E8%A1%8CTVM%E5%AE%9E%E4%BE%8B-toc" style="margin-left:0px;">
     <a href="#%E4%B8%89%E3%80%81%E8%BF%90%E8%A1%8CTVM%E5%AE%9E%E4%BE%8B" rel="nofollow" title="三、运行TVM实例">
      三、运行TVM实例
     </a>
    </p>
    <p id="1%E3%80%81%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91runtime-toc" style="margin-left:40px;">
     <a href="#1%E3%80%81%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91runtime" rel="nofollow" title="1、交叉编译runtime">
      1、交叉编译runtime
     </a>
    </p>
    <p id="2%E3%80%81%E7%BC%96%E8%AF%91%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;">
     <a href="#2%E3%80%81%E7%BC%96%E8%AF%91%E6%A8%A1%E5%9E%8B" rel="nofollow" title="2、编译模型">
      2、编译模型
     </a>
    </p>
    <p id="3%E3%80%81%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B-toc" style="margin-left:40px;">
     <a href="#3%E3%80%81%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B" rel="nofollow" title="3、运行模型">
      3、运行模型
     </a>
    </p>
    <p id="%E5%9B%9B%E3%80%81%E6%80%BB%E7%BB%93-toc" style="margin-left:0px;">
     <a href="#%E5%9B%9B%E3%80%81%E6%80%BB%E7%BB%93" rel="nofollow" title="四、总结">
      四、总结
     </a>
    </p>
    <hr id="hr-toc"/>
    <h2>
     一、TVM的工作流程
    </h2>
    <p>
     TVM主要由两个部分组成：
    </p>
    <p>
     （1）TVM编译器：负责编译和优化模型
    </p>
    <p>
     （2）TVM runtime：提供目标设备上运行模型的API
    </p>
    <h3 id="1%E3%80%81%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B">
     1、整体流程
    </h3>
    <p>
    </p>
    <p style="text-align:center;">
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/4d4832e29d7bd222c5fe8216f8aaa504.png"/>
    </p>
    <p>
     如图所示，TVM的工作流程包括4个主要部分：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        前端导入（Import）
       </strong>
       ：前端部件将不同神经网络框架所训练得到的模型文件转化为IRModule，IRModule是TVM核心的数据结构之一，它包含了可以表述模型的函数集合。
      </p>
     </li>
     <li>
      <p>
       <strong>
        编译转化（Transformation）
       </strong>
       ：编译器对IRModule通过各种Relay Passes的优化规则进行优化，比如对模型进行量化等。
      </p>
     </li>
     <li>
      <p>
       <strong>
        目标代码转换（Target Translation）
       </strong>
       ：既然IRModule是一个函数集合，那编译器就可以将IRModule交叉编译为目标设备可运行的格式，并提供了导出，加载和执行的API给目标设备调用。
      </p>
     </li>
     <li>
      <p>
       <strong>
        运行（Runtime Execution）
       </strong>
       ：用户将交叉编译得到的Module加载到设备上并执行其中的函数集合。
      </p>
     </li>
    </ul>
    <h3 id="2%E3%80%81%E5%85%B3%E9%94%AE%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">
     2、关键数据结构
    </h3>
    <p>
     IRModule (intermediate representation module)是贯穿整个TVM的数据结构，重要性不言而喻。它是一系列Function的集合，用于表述一个神经网络模型，目前TVM支持两种主要的变体函数：
    </p>
    <ul>
     <li>
      <p>
       relay::Function 是高层级的函数编程表示，一个relay.Function通常对应一个端到端的模型。可以将它理解为一个支持控制流、递归和复杂数据结构的计算图。
      </p>
     </li>
     <li>
      <p>
       tir::PrimFunc 是低层级的函数编程表示，它包括循环嵌套、多维加载与存储，线程处理以及向量和张量操作指令。它通常用以定义一个算子的操作，对应一个模型中的某一层。
      </p>
     </li>
    </ul>
    <p>
     在整个编译过程中，一个relay function可能会被优化为多个tir::PrimFunc。
    </p>
    <h3 id="3%E3%80%81Transformations">
     3、Transformations
    </h3>
    <p>
     transformation的作用有两个：
    </p>
    <p>
     （1）优化（optimization）：将程序转换为等效的、或者更优化的版本；
    </p>
    <p>
     （2）底层表示（lowering）：将程序转换为更接近目标设备的低层级表示。
    </p>
    <p>
     relay/transform 包含一组优化模型的passes。优化包括constant folding和dead-code消除，以及针对张量计算的优化，如layout转换和scaling factor folding。
    </p>
    <p>
     在relay优化的pipeline的最后，会运行一个 FuseOps的pass，将一个完整的Function（对应一个端到端的模型如 MobileNet）分解为多个子Funcions（例如 conv2d-relu）段。这样做的好处是将问题分成了两个子问题：
    </p>
    <ul>
     <li>
      <p>
       编译和优化可以针对每个子Function。TVM使用低层级的 tir 来编译和优化每个子功能。对于特定目标设备，也可以直接使用外部代码生成器进行目标代码转换。
      </p>
     </li>
     <li>
      <p>
       整体运行时需要调用所有的子Function。TVM支持几种运行方式，所有运行模式都封装在一个统一的 runtime.Module 接口中：
      </p>
      <ul>
       <li>
        <p>
         对于形状已知且没有控制流的简单模型，我们可以降级为将执行结构存储在图中的图执行器；
        </p>
       </li>
       <li>
        <p>
         支持用于动态执行的虚拟机后端；
        </p>
       </li>
       <li>
        <p>
         后续计划支持提前编译，将高层级的运行结构编译为可运行的原始functions。
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <p>
     tir/transform 包含 TIR 层级functions的转换passes。例如，将multi-dimensional access flatten到一维访问，将内在函数扩展为特定于目标的函数，以及修饰函数入口以满足运行时调用约束。除此之外，也有优化passes，如access index简化和dead-code消除。
    </p>
    <h3 id="4%E3%80%81%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BD%AC%E6%8D%A2">
     4、搜索空间和基于机器学习的转换
    </h3>
    <p>
     前面描述的转换都是基于规则和确定的，而TVM的设计目标之一是支持对于不同的硬件平台都可以进行高性能的代码优化。因此，需要对尽可能多的优化进行选择，每个优化又需要选择最优的参数，从这个角度来看，这个的工作量无疑是巨大的。TVM采用了基于空间搜索和机器学习的方法来解决这个优化选择与调参的问题。
    </p>
    <p>
     顾名思义，空间搜索需要在特定的空间，所以首先需要定义一系列的转换操作，比如循环转换、内联、矢量化等。这些操作称为调度原语（scheduling primitives）。调度原语的集合定义了可以对程序进行优化的搜索空间，然后TVM搜索不同的调度顺序以挑选最佳调度组合。这个搜索过程通常由机器学习算法完成，TVM使用的是xgboost算法。在搜索完成后，记录下每个算子最优的调度顺序，然后编译器就可以将此调度序列应用到程序中。TVM使用基于搜索的优化方法来处理初始 tir function生成问题。这部分模块称为 AutoTVM(auto_scheduler)。
    </p>
    <h3 id="5%E3%80%81%E7%9B%AE%E6%A0%87%E4%BB%A3%E7%A0%81%E8%BD%AC%E5%8C%96">
     5、目标代码转化
    </h3>
    <p>
     目标代码转换阶段主要是将 IRModule 转换为可以相应的目标设备上运行的格式：
    </p>
    <ul>
     <li>
      <p>
       对于 x86 和 ARM 等后端，使用 LLVM IRBuilder 来构建 LLVM IR；
      </p>
     </li>
     <li>
      <p>
       支持生成例如 CUDA C 和 OpenCL等源码级的代码；
      </p>
     </li>
     <li>
      <p>
       支持通过外部代码生成器将Relay function（子图）直接转换为特定目标代码。
      </p>
     </li>
    </ul>
    <p>
     代码生成阶段需要尽可能地轻量化，所以绝大多数的转换和降层级都应该放在目标代码转换之前执行。
    </p>
    <h2 id="%E4%BA%8C%E3%80%81%E9%80%BB%E8%BE%91%E6%9E%B6%E6%9E%84%E7%BB%84%E4%BB%B6">
     二、逻辑架构组件
    </h2>
    <p style="text-align:center;">
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/5033035e3f9882e7db9afdea30175429.png"/>
    </p>
    <p>
     上图显示了TVM中的主要逻辑组件：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/support
       </strong>
      </p>
     </li>
    </ul>
    <p>
     包含tvm最常用的实用工具函数，例如通用的 arena 分配器、套接字和日志记录等。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/runtime
       </strong>
      </p>
     </li>
    </ul>
    <p>
     runtime作为tvm的基础组件，提供了加载和执行编译的机制，它定义了一组标准的C API与前端高级语言如Python和Rust进行交互。在runtime中，runtime::Object是主要的数据结构之一，它是一个带有类型索引的引用计数基类，用于支持运行时类型检查和向下类型转换。通过它可以向runtime引入新的数据结构，如 Array、Map 和新的 IR 数据结构。
    </p>
    <p>
     编译器本身也大量使用了 TVM 的runtime机制。所有 IR 数据结构都是runtime::Object 的子类，因此，它们可以直接通过 Python 前端进行操作，tvm使用 PackedFunc 机制向前端公开各种 API。
    </p>
    <p>
     runtime/rpc实现了对 PackedFunc 的 RPC 支持，由此可以将交叉编译的库发送到远程设备并测试性能。因为rpc架构支持从各种远程硬件后端收集数据，所以它是基于机器学习优化方法的基础。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/node
       </strong>
      </p>
     </li>
    </ul>
    <p>
     node 模块在runtime::Object之上为 IR 数据结构添加了附加功能。主要功能包括反射、序列化、结构等价和散列。还可以将任意 IR 节点序列化为 JSON 格式，然后将它们加载回来。保存/存储和检查 IR 节点的能力为使编译器更易于访问奠定了基础。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/ir
       </strong>
      </p>
     </li>
    </ul>
    <p>
     在tvm/ir文件夹中包含跨所有IR功能变异体的统一的数据结构和接口。tvm/ir中的组件由tvm/relay和tvm/tir共享，包括IRModule、Type、PassContext 和 Pass、OP。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/target
       </strong>
      </p>
     </li>
    </ul>
    <p>
     target模块包含将 IRModule 转换为目标 runtime.Module 的所有代码生成器。它还提供了一个通用的Target类来描述目标。通过查询target中的属性信息和注册到每个target id(cuda, opencl)的内置信息，可以根据target定制编译流水线。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/tir
       </strong>
      </p>
     </li>
    </ul>
    <p>
     tir 包含低层级程序表示的定义，使用tir::PrimFunc来表示可以通过 tir 通道转换的函数。除了 IR 数据结构之外，tir 模块还通过公共 Op 注册表定义了一组内置函数及属性，以及tir/transform 中的转换passes。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/arith
       </strong>
      </p>
     </li>
    </ul>
    <p>
     该模块与 tir 密切相关。低层级代码生成中的关键问题之一是对索引算术属性的分析。arith 模块提供了一组（主要是整数）分析工具。tir 通过可以使用这些分析来简化和优化代码。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/te
       </strong>
      </p>
     </li>
    </ul>
    <p>
     te 代表“张量表达式”，通过编写张量表达式可以快速构建tir::PrimFunc变体。te/schedule提供了一组调度原语来控制正在生成的函数。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/topi
       </strong>
      </p>
     </li>
    </ul>
    <p>
     虽然可以为每个用例直接通过 TIR 或张量表达式 (TE) 构造运算符，但这样做很乏味。topi（张量运算符清单）提供了一组由 numpy 定义并在常见深度学习工作负载中找到的预定义运算符（在 TE 或 TIR 中）。我们还提供了一组通用计划模板，以获得跨不同目标平台的高性能实现。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/relay
       </strong>
      </p>
     </li>
    </ul>
    <p>
     relay 是用于表示完整模型的高级功能 IR。在relay.transform中定义了各种优化。relay 编译器定义了多种优化策略，每种策略都旨在支持特定的优化方式。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        tvm/autotvm
       </strong>
      </p>
     </li>
    </ul>
    <p>
     AutoTVM和AutoScheduler是自动搜索优化所必须的两个组件。主要包括：cost models和特征提取；用于存储运行cost models性能结果的格式以及一组变换搜索策略。
    </p>
    <h2 id="%E4%B8%89%E3%80%81%E8%BF%90%E8%A1%8CTVM%E5%AE%9E%E4%BE%8B">
     三、运行TVM实例
    </h2>
    <h3 id="1%E3%80%81%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91runtime">
     1、交叉编译runtime
    </h3>
    <p>
     想要在目标设备上运行模型的前提是交叉编译模型和runtime库。以Raspberry Pi为例，首先需要在主机安装Raspberry Pi的编译工具链：
    </p>
    <pre><code class="language-bash">sudo apt-get update
sudo apt-get install gcc-aarch64-linux-gnu g++-aarch64-linux-gnu
sudo apt-get install gcc-multilib-arm-linux-gnueabihf g++-multilib-arm-linux-gnueabihf</code></pre>
    <pre>然后交叉编译TVM runtime库：</pre>
    <pre><code class="language-bash">cmake .. \
    -DCMAKE_SYSTEM_NAME=Linux \
    -DCMAKE_SYSTEM_VERSION=1 \
    -DCMAKE_C_COMPILER=/usr/bin/aarch64-linux-gnu-gcc \
    -DCMAKE_CXX_COMPILER=/usr/bin/aarch64-linux-gnu-g++ \
    -DCMAKE_FIND_ROOT_PATH=/usr/aarch64-linux-gnu \
    -DCMAKE_FIND_ROOT_PATH_MODE_PROGRAM=NEVER \
    -DCMAKE_FIND_ROOT_PATH_MODE_LIBRARY=ONLY \
    -DMACHINE_NAME=aarch64-linux-gnu
make -j2 runtime</code></pre>
    <pre>编译完成后使用file命令查看编译出来的runtime库是否OK：</pre>
    <p style="text-align:center;">
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/a1c156dcfc9ffa8460077e4ed058d333.png"/>
    </p>
    <h3 id="2%E3%80%81%E7%BC%96%E8%AF%91%E6%A8%A1%E5%9E%8B">
     2、编译模型
    </h3>
    <p>
     在主机上构造一个简单的kernel，并在主机上编译，示例代码如下：
    </p>
    <p>
     ​​​​​
    </p>
    <pre><code class="language-python">import numpy as np
import tvm
from tvm import te
from tvm import rpc
from tvm.contrib import utils
# 构造计算核
n = tvm.runtime.convert(1024)
A = te.placeholder((n,), name="A")
B = te.compute((n,), lambda i: A[i] + 1.0, name="B")
s = te.create_schedule(B.op)
# 编译并保存结果：local_demo为True表示编译target为主机端运行，否则为raspbarry pi
local_demo = True
if local_demo:
    target = "llvm"
else:
    target = "llvm -mtriple=armv7l-linux-gnueabihf"
func = tvm.build(s, [A, B], target=target, name="add_one") # 为目标设备生成代码
print(func)
path = "./tvm_test_lib.tar"
func.export_library(path)</code></pre>
    <p>
     运行代码后会得到tvm_test_lib.tar的编译结果，func的打印输出为：
    </p>
    <pre><code>Module(llvm, 56334d7e8738)</code></pre>
    <p>
     它是一个 tvm.runtime.PackedFunc 类型，TVM使用Function开作为前后端的黏合，一个编译后的module返回Function，TVM后端同样也以Functions的方式注册和暴露API。
    </p>
    <h3 id="3%E3%80%81%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B">
     3、运行模型
    </h3>
    <p>
     将它用rpc的方式运行在设备上，需要将编译的lib上传到设备，然后使用设备端的编译器重新链接之后，func就是一个设备端的模型对象了。
    </p>
    <pre><code class="language-python">if local_demo:
    remote = rpc.LocalSession()
else:
    host = "192.168.1.111"
    port = 9090
    remote = rpc.connect(host, port)
remote.upload(path)
func = remote.load_module("tvm_test_lib.tar")
print(func)
dev = remote.cpu()
a = tvm.nd.array(np.random.uniform(size=1024).astype(A.dtype), dev)
b = tvm.nd.array(np.zeros(1024, dtype=A.dtype), dev)
func(a, b)
np.testing.assert_equal(b.numpy(), a.numpy() + 1)
time_f = func.time_evaluator(func.entry_name, dev, number=10)
cost = time_f(a, b).mean
print("%g secs/op" % cost)</code></pre>
    <pre>此时的func打印输出是：
</pre>
    <pre><code>Module(rpc, 56334d6d7148)</code></pre>
    <h2 id="%E5%9B%9B%E3%80%81%E6%80%BB%E7%BB%93">
     四、总结
    </h2>
    <p>
     本文介绍了TVM的工作流程和内部的逻辑框架组件，通过运行TVM的一个实例了解和熟悉TVM的Python API使用。
    </p>
    <blockquote>
     <p>
      往期推荐
     </p>
     <p>
      <a href="http://mp.weixin.qq.com/s?__biz=MzAwNzI2NDU4NA==&amp;mid=2247484019&amp;idx=1&amp;sn=82739000e37590a75146cabd58b511c0&amp;chksm=9b018254ac760b4219ceb604eae17a1d94e2eab8cec06cfa6a919ea7ae591cd17ad7e9c6dba1&amp;scene=21#wechat_redirect" rel="nofollow" title="【TVM系列一】开发环境搭建">
       【TVM系列一】开发环境搭建
      </a>
     </p>
    </blockquote>
   </div>
  </div>
 </article>
 <p alt="6874747073:3a2f2f626c6f672e6373646e2e6e65742f4f72656f7a67732f:61727469636c652f64657461696c732f313231343835323938" class_="artid" style="display:none">
 </p>
</div>


