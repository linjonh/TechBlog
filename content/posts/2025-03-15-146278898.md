---
layout: post
title: "Mac-上编译-Ragflow"
date: 2025-03-15 14:26:07 +0800
description: "一开始尝试按源码启动的方式（https://ragflow.io/docs/dev/launch_ragflow_from_source），直接运行 Ragflow，但是在安装 Python 依赖的时候就报错了。ragflow 提供的 docker compose 使用的是 ragflow 的对应的网桥就是 docker_ragflow，也可以通过如下命令查看。会下载一些基础依赖和模型，可以根据自己的实际情况调整模型的下载，比如是否下载模型，模型的下载地址等（未经完全验证是否可以屏蔽模型）。"
keywords: "Mac 上编译 Ragflow"
categories: ['Ai']
tags: ['人工智能', 'Ragflow']
artid: "146278898"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146278898
    alt: "Mac-上编译-Ragflow"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146278898
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146278898
cover: https://bing.ee123.net/img/rand?artid=146278898
image: https://bing.ee123.net/img/rand?artid=146278898
img: https://bing.ee123.net/img/rand?artid=146278898
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Mac 上编译 Ragflow
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h3>
     <a id="_0">
     </a>
     说明
    </h3>
    <p>
     一开始尝试按源码启动的方式（https://ragflow.io/docs/dev/launch_ragflow_from_source），直接运行 Ragflow，但是在安装 Python 依赖的时候就报错了。于是尝试使用 Docker 的方式运行（https://ragflow.io/docs/dev/build_docker_image）。为什么呢？因为目前官方提供的所有 Docker 镜像均基于 x86 架构构建，并不提供基于 ARM64 的 Docker 镜像。
    </p>
    <p>
     我这里使用 OrbStack 工具来管理容器。
    </p>
    <h3>
     <a id="_6">
     </a>
     编译
    </h3>
    <h4>
     <a id="_8">
     </a>
     步骤
    </h4>
    <ol>
     <li>
      安装uv
     </li>
    </ol>
    <pre><code class="prism language-Bash"># On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh

# 根据提示，执行 source 命令，或者重开 termial
</code></pre>
    <ol start="2">
     <li>
      获取源码编译镜像
     </li>
    </ol>
    <pre><code class="prism language-Bash">git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv run download_deps.py
docker build -f Dockerfile.deps -t infiniflow/ragflow_deps .
docker build --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .
</code></pre>
    <p>
     其中
     <code>
      download_deps.py
     </code>
     会下载一些基础依赖和模型，可以根据自己的实际情况调整模型的下载，比如是否下载模型，模型的下载地址等（未经完全验证是否可以屏蔽模型）。
    </p>
    <p>
     以下模型下载的关键代码：
    </p>
    <pre><code class="prism language-Python"># 镜像列表
repos = [
    "InfiniFlow/text_concat_xgb_v1.0",
    "InfiniFlow/deepdoc",
    "InfiniFlow/huqie",
    "BAAI/bge-large-zh-v1.5",
    "BAAI/bge-reranker-v2-m3",
    "maidalun1020/bce-embedding-base_v1",
    "maidalun1020/bce-reranker-base_v1",
]

# 下载镜像
for repo_id in repos:
    print(f"Downloading huggingface repo {repo_id}...")
    download_model(repo_id)
</code></pre>
    <h4>
     <a id="_51">
     </a>
     注意事项
    </h4>
    <p>
     特别重要的是，编译镜像的过程，注意开启全局的科学上网。主要可能碰到的问题，比如组件下载不成功，组件下载不完整等等。
    </p>
    <p>
     如果组件下载不成功的话，重新执行一遍就好了。
    </p>
    <p>
     我就在最后一步 docker build 的时候出现问题，看提示就是需要安装的包出错了，在 ragflow 目录下找到对应下载的文件删掉，然后重新执行
     <code>
      uv run download_deps.py
     </code>
     。
    </p>
    <p>
     编译的过程时间可能比较长，需要耐心等待。
    </p>
    <h3>
     <a id="_61">
     </a>
     运行
    </h3>
    <p>
     需要远行 Ragflow 还需要做些配置。打开
     <code>
      docker/.env
     </code>
     ，找到
     <code>
      RAGFLOW_IMAGE
     </code>
     的配置，把镜像地址修改成
     <code>
      infiniflow/ragflow:nightly-slim
     </code>
     。
    </p>
    <p>
     接下来通过docker compose 启动服务。
    </p>
    <pre><code class="prism language-Python">cd docker
$ docker compose -f docker-compose-macos.yml up -d
</code></pre>
    <p>
     耐心等待相关的服务启动，然后使用http://127.0.0.1 访问。
    </p>
    <p>
     默认没有用户，直接注册一个就可以，第一个用户会成为管理员。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9c1b97b40a834c988495d786e700f767.png"/>
    </p>
    <h4>
     <a id="_79">
     </a>
     问题
    </h4>
    <p>
     自己碰的的问题，是添加模型的时候出错。看了ragflow 群里的问题，这个问题也蛮多人问的。
    </p>
    <p>
     ragflow 提供的 docker compose 使用的是 ragflow 的对应的网桥就是 docker_ragflow，也可以通过如下命令查看。
    </p>
    <pre><code class="prism language-Python">docker network ls

# 结果
881b4fc04bf4   bridge           bridge    local
67e9e0de36bc   docker_ragflow   bridge    local
4e76407faa32   host             host      local
d611379d4316   none             null      local
</code></pre>
    <p>
     找到了网桥，我们可以通过 docker inspect 查看具体的ip，找到 Gateway 的部分就是宿主机的 ip
    </p>
    <pre><code class="prism language-Python">docker inspect docker_ragflow

# Gateway
[
    {
        "Name": "docker_ragflow",
        "Id": "67e9e0de36bc646402f69919aaafa30d663f27eb66a6e63bfaf8f75fc59c8a01",
        "Created": "2025-03-14T14:19:59.789673667+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "192.168.97.0/24",
                    "Gateway": "192.168.97.1"
                }
            ]
</code></pre>
    <p>
     找到了正确的地址，但还是不能通过宿主机地址访问，那么检查 ollama 的配置，是否导出对应的变量。
    </p>
    <pre><code class="prism language-Python">export OLLAMA_ORIGINS="*"
export OLLAMA_HOST=0.0.0.0:11434

# 如果没有添加，需要补上
# source .zshrc
</code></pre>
    <p>
     如果没有添加，重新添加后重启 ollama。然后通过本地 IP，而不是 127.0.0.1 或者 localhost 访问，是否正常。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/320eea02ba8a4a6b82686fb924516cd5.png"/>
    </p>
    <p>
     如果此时提示网页无法访问，这时应该运行
     <code>
      ollama serve
     </code>
     ，开启 ollama 的服务。
    </p>
    <pre><code class="prism language-Python">ollama serve

# 可以看到类似的提示信息。
2025/03/14 14:48:19 routes.go:1225: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/airhead/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"
</code></pre>
    <p>
     填写网桥的 ip 之后还是无法访问，那需要确定实际的网桥 ip 是否真的存在。
    </p>
    <pre><code class="prism language-Python">ipconfig grep | 192.168.7.1
</code></pre>
    <p>
     如果没有结果就说明 ip 没有真正起来，导致 ip 无法访问。接着通过
     <code>
      ifconfig
     </code>
     ，找到可访问的 ip，比如我这里实际可访问的网桥是 bridge102（这里暂时不确定为什么。）
    </p>
    <pre><code class="prism language-Python">ipconfig

bridge102: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
        options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt;
        ether fa:4d:89:d7:be:66
        inet 198.19.249.3 netmask 0xfffffe00 broadcast 198.19.249.255
        inet6 fe80::f84d:89ff:fed7:be66%bridge102 prefixlen 64 scopeid 0x1e 
        inet6 fd07:b51a:cc66:0:a617:db5e:ab7:e9f1 prefixlen 64 
        Configuration:
                id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0
                maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200
                root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0
                ipfilter disabled flags 0x0
        member: vmenet3 flags=10003&lt;LEARNING,DISCOVER,CSUM&gt;
                ifmaxaddr 0 port 29 priority 0 path cost 0
        nd6 options=201&lt;PERFORMNUD,DAD&gt;
        media: autoselect
        status: active
        
bridge100: flags=8863&lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&gt; mtu 1500
        options=63&lt;RXCSUM,TXCSUM,TSO4,TSO6&gt;
        ether fa:4d:89:d7:be:64
        inet 192.168.97.0 netmask 0xffffff00 broadcast 192.168.97.255
        inet6 fe80::f84d:89ff:fed7:be64%bridge100 prefixlen 64 scopeid 0x18 
        Configuration:
                id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0
                maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200
                root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0
                ipfilter disabled flags 0x0
        member: vmenet0 flags=10003&lt;LEARNING,DISCOVER,CSUM&gt;
                ifmaxaddr 0 port 23 priority 0 path cost 0
        nd6 options=201&lt;PERFORMNUD,DAD&gt;
        media: autoselect
        status: active
        
ifconfig |grep 198.19.249.3         
        inet 198.19.249.3 netmask 0xfffffe00 broadcast 198.19.249.255
</code></pre>
    <p>
     之后使用了 198.19.249.3 这个ip 可以正常访问，配置 llm 也正常。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5020aad51490420189e7cd931b3881a6.png"/>
    </p>
    <p>
     为什么不用本机 ip 而用 docker 网桥的 ip 呢？因为是笔记本，通过 Wi-Fi 获取的 ip 可能变化，而网桥 ip 是稳定的。
    </p>
    <h3>
     <a id="_198">
     </a>
     开发
    </h3>
    <p>
     VS Code 安装 Remote Development 插件。它包含多个插件，其中 Dev Containters 可以连接到 Docker 容器作为开发环境，这样的好处是开发环境与部署环境一致。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7210f15d3315498bad0ead6a8a7b31d3.png"/>
    </p>
    <p>
     当安装好 Remote Development，就可以通过 Remote Explorer 查看已经安装的容器，并通过点击对应容器的箭头（注意下图红色框的部分）。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f1b6a54b922143e08c70477989221c03.png"/>
    </p>
    <p>
     进入到容器内部，选对应的目录，就可以打开
     <code>
      /ragflow
     </code>
     ，这样就能看到实际的代码了。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c87f8c0475ba4d6ca83ccc2457811f22.png"/>
    </p>
    <h3>
     <a id="_215">
     </a>
     其他
    </h3>
    <ol>
     <li>
      还需要调整 Dockerfile 方便挂载本地的源码位置，这样方便通过外部的 Git 管理开发代码。
     </li>
     <li>
      在配置 LLM 模型的使用有个支持的最大 Token 数，这个值应该怎么填呢？通过
      <code>
       ollama show
      </code>
      查看对应模型的信息，其中 context length 就是模型的最大 Token 数了。
     </li>
    </ol>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/80b3b0a375c4458abd96ace34c5698f5.png"/>
    </p>
    <pre><code class="prism language-Bash">ollama show deepseek-r1:7b

# 
  Model
    architecture        qwen2     
    parameters          7.6B      
    context length      131072    
    embedding length    3584      
    quantization        Q4_K_M   
</code></pre>
    <ol start="3">
     <li>
      因为 ollama 暂时不支持重排，需要安装 xinterface。目前通过容器安装 xinterface，可能镜像或者配置选错了，导致 xinterface 的镜像无法启动。
     </li>
     <li>
      因为 OrbStack 也可以运行虚拟机，理论上来说，可以通过安装 Ubuntu 虚拟机的方式，照着 Dockerfile 文件安装相关的依赖。
     </li>
    </ol>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="6874747073:3a2f2f626c6f672e6373646e2e6e65742f656e646572676f2f:61727469636c652f64657461696c732f313436323738383938" class_="artid" style="display:none">
 </p>
</div>


