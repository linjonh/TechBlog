---
arturl_encode: "6874747073:3a2f2f626c6f672e6373646e2e6e65742f656e646572676f2f:61727469636c652f64657461696c732f313436323738383938"
layout: post
title: "Mac-上编译-Ragflow"
date: 2025-03-15 14:26:07 +08:00
description: "一开始尝试按源码启动的方式（https://ragflow.io/docs/dev/launch_ragflow_from_source），直接运行 Ragflow，但是在安装 Python 依赖的时候就报错了。ragflow 提供的 docker compose 使用的是 ragflow 的对应的网桥就是 docker_ragflow，也可以通过如下命令查看。会下载一些基础依赖和模型，可以根据自己的实际情况调整模型的下载，比如是否下载模型，模型的下载地址等（未经完全验证是否可以屏蔽模型）。"
keywords: "Mac 上编译 Ragflow"
categories: ['Ai']
tags: ['人工智能', 'Ragflow']
artid: "146278898"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146278898
    alt: "Mac-上编译-Ragflow"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146278898
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146278898
cover: https://bing.ee123.net/img/rand?artid=146278898
image: https://bing.ee123.net/img/rand?artid=146278898
img: https://bing.ee123.net/img/rand?artid=146278898
---

# Mac 上编译 Ragflow

### 说明

一开始尝试按源码启动的方式（https://ragflow.io/docs/dev/launch_ragflow_from_source），直接运行 Ragflow，但是在安装 Python 依赖的时候就报错了。于是尝试使用 Docker 的方式运行（https://ragflow.io/docs/dev/build_docker_image）。为什么呢？因为目前官方提供的所有 Docker 镜像均基于 x86 架构构建，并不提供基于 ARM64 的 Docker 镜像。

我这里使用 OrbStack 工具来管理容器。

### 编译

#### 步骤

1. 安装uv

```Bash
# On macOS and Linux.
curl -LsSf https://astral.sh/uv/install.sh | sh

# 根据提示，执行 source 命令，或者重开 termial

```

2. 获取源码编译镜像

```Bash
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/
uv run download_deps.py
docker build -f Dockerfile.deps -t infiniflow/ragflow_deps .
docker build --build-arg LIGHTEN=1 -f Dockerfile -t infiniflow/ragflow:nightly-slim .

```

其中
`download_deps.py`
会下载一些基础依赖和模型，可以根据自己的实际情况调整模型的下载，比如是否下载模型，模型的下载地址等（未经完全验证是否可以屏蔽模型）。

以下模型下载的关键代码：

```Python
# 镜像列表
repos = [
    "InfiniFlow/text_concat_xgb_v1.0",
    "InfiniFlow/deepdoc",
    "InfiniFlow/huqie",
    "BAAI/bge-large-zh-v1.5",
    "BAAI/bge-reranker-v2-m3",
    "maidalun1020/bce-embedding-base_v1",
    "maidalun1020/bce-reranker-base_v1",
]

# 下载镜像
for repo_id in repos:
    print(f"Downloading huggingface repo {repo_id}...")
    download_model(repo_id)

```

#### 注意事项

特别重要的是，编译镜像的过程，注意开启全局的科学上网。主要可能碰到的问题，比如组件下载不成功，组件下载不完整等等。

如果组件下载不成功的话，重新执行一遍就好了。

我就在最后一步 docker build 的时候出现问题，看提示就是需要安装的包出错了，在 ragflow 目录下找到对应下载的文件删掉，然后重新执行
`uv run download_deps.py`
。

编译的过程时间可能比较长，需要耐心等待。

### 运行

需要远行 Ragflow 还需要做些配置。打开
`docker/.env`
，找到
`RAGFLOW_IMAGE`
的配置，把镜像地址修改成
`infiniflow/ragflow:nightly-slim`
。

接下来通过docker compose 启动服务。

```Python
cd docker
$ docker compose -f docker-compose-macos.yml up -d

```

耐心等待相关的服务启动，然后使用http://127.0.0.1 访问。

默认没有用户，直接注册一个就可以，第一个用户会成为管理员。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9c1b97b40a834c988495d786e700f767.png)

#### 问题

自己碰的的问题，是添加模型的时候出错。看了ragflow 群里的问题，这个问题也蛮多人问的。

ragflow 提供的 docker compose 使用的是 ragflow 的对应的网桥就是 docker_ragflow，也可以通过如下命令查看。

```Python
docker network ls

# 结果
881b4fc04bf4   bridge           bridge    local
67e9e0de36bc   docker_ragflow   bridge    local
4e76407faa32   host             host      local
d611379d4316   none             null      local

```

找到了网桥，我们可以通过 docker inspect 查看具体的ip，找到 Gateway 的部分就是宿主机的 ip

```Python
docker inspect docker_ragflow

# Gateway
[
    {
        "Name": "docker_ragflow",
        "Id": "67e9e0de36bc646402f69919aaafa30d663f27eb66a6e63bfaf8f75fc59c8a01",
        "Created": "2025-03-14T14:19:59.789673667+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": [
                {
                    "Subnet": "192.168.97.0/24",
                    "Gateway": "192.168.97.1"
                }
            ]

```

找到了正确的地址，但还是不能通过宿主机地址访问，那么检查 ollama 的配置，是否导出对应的变量。

```Python
export OLLAMA_ORIGINS="*"
export OLLAMA_HOST=0.0.0.0:11434

# 如果没有添加，需要补上
# source .zshrc

```

如果没有添加，重新添加后重启 ollama。然后通过本地 IP，而不是 127.0.0.1 或者 localhost 访问，是否正常。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/320eea02ba8a4a6b82686fb924516cd5.png)

如果此时提示网页无法访问，这时应该运行
`ollama serve`
，开启 ollama 的服务。

```Python
ollama serve

# 可以看到类似的提示信息。
2025/03/14 14:48:19 routes.go:1225: INFO server config env="map[HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:2048 OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/Users/airhead/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_SCHED_SPREAD:false http_proxy: https_proxy: no_proxy:]"

```

填写网桥的 ip 之后还是无法访问，那需要确定实际的网桥 ip 是否真的存在。

```Python
ipconfig grep | 192.168.7.1

```

如果没有结果就说明 ip 没有真正起来，导致 ip 无法访问。接着通过
`ifconfig`
，找到可访问的 ip，比如我这里实际可访问的网桥是 bridge102（这里暂时不确定为什么。）

```Python
ipconfig

bridge102: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
        options=63<RXCSUM,TXCSUM,TSO4,TSO6>
        ether fa:4d:89:d7:be:66
        inet 198.19.249.3 netmask 0xfffffe00 broadcast 198.19.249.255
        inet6 fe80::f84d:89ff:fed7:be66%bridge102 prefixlen 64 scopeid 0x1e 
        inet6 fd07:b51a:cc66:0:a617:db5e:ab7:e9f1 prefixlen 64 
        Configuration:
                id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0
                maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200
                root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0
                ipfilter disabled flags 0x0
        member: vmenet3 flags=10003<LEARNING,DISCOVER,CSUM>
                ifmaxaddr 0 port 29 priority 0 path cost 0
        nd6 options=201<PERFORMNUD,DAD>
        media: autoselect
        status: active
        
bridge100: flags=8863<UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST> mtu 1500
        options=63<RXCSUM,TXCSUM,TSO4,TSO6>
        ether fa:4d:89:d7:be:64
        inet 192.168.97.0 netmask 0xffffff00 broadcast 192.168.97.255
        inet6 fe80::f84d:89ff:fed7:be64%bridge100 prefixlen 64 scopeid 0x18 
        Configuration:
                id 0:0:0:0:0:0 priority 0 hellotime 0 fwddelay 0
                maxage 0 holdcnt 0 proto stp maxaddr 100 timeout 1200
                root id 0:0:0:0:0:0 priority 0 ifcost 0 port 0
                ipfilter disabled flags 0x0
        member: vmenet0 flags=10003<LEARNING,DISCOVER,CSUM>
                ifmaxaddr 0 port 23 priority 0 path cost 0
        nd6 options=201<PERFORMNUD,DAD>
        media: autoselect
        status: active
        
ifconfig |grep 198.19.249.3         
        inet 198.19.249.3 netmask 0xfffffe00 broadcast 198.19.249.255

```

之后使用了 198.19.249.3 这个ip 可以正常访问，配置 llm 也正常。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5020aad51490420189e7cd931b3881a6.png)

为什么不用本机 ip 而用 docker 网桥的 ip 呢？因为是笔记本，通过 Wi-Fi 获取的 ip 可能变化，而网桥 ip 是稳定的。

### 开发

VS Code 安装 Remote Development 插件。它包含多个插件，其中 Dev Containters 可以连接到 Docker 容器作为开发环境，这样的好处是开发环境与部署环境一致。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7210f15d3315498bad0ead6a8a7b31d3.png)

当安装好 Remote Development，就可以通过 Remote Explorer 查看已经安装的容器，并通过点击对应容器的箭头（注意下图红色框的部分）。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f1b6a54b922143e08c70477989221c03.png)

进入到容器内部，选对应的目录，就可以打开
`/ragflow`
，这样就能看到实际的代码了。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c87f8c0475ba4d6ca83ccc2457811f22.png)

### 其他

1. 还需要调整 Dockerfile 方便挂载本地的源码位置，这样方便通过外部的 Git 管理开发代码。
2. 在配置 LLM 模型的使用有个支持的最大 Token 数，这个值应该怎么填呢？通过
   `ollama show`
   查看对应模型的信息，其中 context length 就是模型的最大 Token 数了。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/80b3b0a375c4458abd96ace34c5698f5.png)

```Bash
ollama show deepseek-r1:7b

# 
  Model
    architecture        qwen2     
    parameters          7.6B      
    context length      131072    
    embedding length    3584      
    quantization        Q4_K_M   

```

3. 因为 ollama 暂时不支持重排，需要安装 xinterface。目前通过容器安装 xinterface，可能镜像或者配置选错了，导致 xinterface 的镜像无法启动。
4. 因为 OrbStack 也可以运行虚拟机，理论上来说，可以通过安装 Ubuntu 虚拟机的方式，照着 Dockerfile 文件安装相关的依赖。