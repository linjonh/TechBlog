---
layout: post
title: "笔记一下RAG-专题基础学习"
date: 2025-03-13 11:26:38 +0800
description: "检索增强生成（RAG）是指对大型语言模型输出进行优化，使其能够在生成响应之前引用训练数据来源之外的权威知识库。大型语言模型（LLM）用海量数据进行训练，使用数十亿个参数为回答问题、翻译语言和完成句子等任务生成原始输出。在 LLM 本就强大的功能基础上，RAG 将其扩展为能访问特定领域或组织的内部知识库，所有这些都无需重新训练模型。这是一种经济高效地改进 LLM 输出的方法，让它在各种情境下都能保持相关性、准确性和实用性。CSVLoader接受一个csv_args。"
keywords: "【笔记一下】RAG 专题基础学习"
categories: ['Rag', 'Llm', 'Embedding']
tags: ['笔记', '学习', 'Rag']
artid: "146226308"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146226308
    alt: "笔记一下RAG-专题基础学习"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146226308
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146226308
cover: https://bing.ee123.net/img/rand?artid=146226308
image: https://bing.ee123.net/img/rand?artid=146226308
img: https://bing.ee123.net/img/rand?artid=146226308
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【笔记一下】RAG 专题基础学习
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h4>
     <a id="RAG__0">
     </a>
     一、RAG 简介
    </h4>
    <h5>
     <a id="_1">
     </a>
     什么是检索增强生成？
    </h5>
    <p>
     检索增强生成（RAG）是指对大型语言模型输出进行优化，使其能够在生成响应之前引用训练数据来源之外的权威知识库。大型语言模型（LLM）用海量数据进行训练，使用数十亿个参数为回答问题、翻译语言和完成句子等任务生成原始输出。在 LLM 本就强大的功能基础上，RAG 将其扩展为能访问特定领域或组织的内部知识库，所有这些都无需重新训练模型。这是一种经济高效地改进 LLM 输出的方法，让它在各种情境下都能保持相关性、准确性和实用性。
    </p>
    <h5>
     <a id="_3">
     </a>
     为什么检索增强生成很重要？
    </h5>
    <p>
     LLM 是一项关键的人工智能（AI）技术，为智能聊天机器人和其他自然语言处理（NLP）应用程序提供支持。目标是通过交叉引用权威知识来源，创建能够在各种环境中回答用户问题的机器人。不幸的是，LLM 技术的本质在 LLM 响应中引入了不可预测性。此外，LLM 训练数据是静态的，并引入了其所掌握知识的截止日期。
    </p>
    <p>
     LLM 面临的已知挑战包括：
     <br/>
     在没有答案的情况下提供虚假信息。
    </p>
    <ul>
     <li>
      当用户需要特定的当前响应时，提供过时或通用的信息。
     </li>
    </ul>
    <ul>
     <li>
      从非权威来源创建响应。
     </li>
     <li>
      由于术语混淆，不同的培训来源使用相同的术语来谈论不同的事情，因此会产生不准确的响应。
     </li>
    </ul>
    <p>
     您可以将
     <a href="https://aws.amazon.com/what-is/large-language-model/" rel="nofollow">
      大型语言模型
     </a>
     看作是一个过于热情的新员工，他拒绝随时了解时事，但总是会绝对自信地回答每一个问题。不幸的是，这种态度会对用户的信任产生负面影响，这是您不希望聊天机器人效仿的！
    </p>
    <p>
     RAG 是解决其中一些挑战的一种方法。它会重定向 LLM，从权威的、预先确定的知识来源中检索相关信息。组织可以更好地控制生成的文本输出，并且用户可以深入了解 LLM 如何生成响应。
    </p>
    <h5>
     <a id="font_18">
     </a>
     检索增强生成有哪些好处？
    </h5>
    <p>
     RAG 技术为组织的
     <a href="https://aws.amazon.com/what-is/generative-ai/" rel="nofollow">
      生成式人工智能
     </a>
     工作带来了多项好处。
    </p>
    <h5>
     <a id="font_stylecolor000000font_21">
     </a>
     <strong>
      经济高效的实施
     </strong>
    </h5>
    <p>
     聊天机器人开发通常从
     <a href="https://aws.amazon.com/what-is/foundation-models/" rel="nofollow">
      基础模型
     </a>
     开始。基础模型（FM）是在广泛的广义和未标记数据上训练的 API 可访问 LLM。针对组织或领域特定信息重新训练 FM 的计算和财务成本很高。RAG 是一种将新数据引入 LLM 的更加经济高效的方法。它使生成式人工智能技术更广泛地获得和使用。
    </p>
    <h5>
     <a id="font_stylecolor000000font_24">
     </a>
     <strong>
      当前信息
     </strong>
    </h5>
    <p>
     即使 LLM 的原始训练数据来源适合您的需求，但保持相关性也具有挑战性。RAG 允许开发人员为生成模型提供最新的研究、统计数据或新闻。他们可以使用 RAG 将 LLM 直接连接到实时社交媒体提要、新闻网站或其他经常更新的信息来源。然后，LLM 可以向用户提供最新信息。
    </p>
    <h5>
     <a id="font_stylecolor000000font_27">
     </a>
     <strong>
      增强用户信任度
     </strong>
    </h5>
    <p>
     RAG 允许 LLM 通过来源归属来呈现准确的信息。输出可以包括对来源的引文或引用。如果需要进一步说明或更详细的信息，用户也可以自己查找源文档。这可以增加对您的生成式人工智能解决方案的信任和信心。
    </p>
    <h5>
     <a id="font_stylecolor000000font_30">
     </a>
     <strong>
      更多开发人员控制权
     </strong>
    </h5>
    <p>
     借助 RAG，开发人员可以更高效地测试和改进他们的聊天应用程序。他们可以控制和更改 LLM 的信息来源，以适应不断变化的需求或跨职能使用。开发人员还可以将敏感信息的检索限制在不同的授权级别内，并确保 LLM 生成适当的响应。此外，如果 LLM 针对特定问题引用了错误的信息来源，他们还可以进行故障排除并进行修复。组织可以更自信地为更广泛的应用程序实施生成式人工智能技术。
    </p>
    <h4>
     <a id="font_33">
     </a>
     二、检索增强生成的工作原理是什么？
    </h4>
    <p>
     如果没有 RAG，LLM 会接受用户输入，并根据它所接受训练的信息或它已经知道的信息创建响应。RAG 引入了一个信息检索组件，该组件利用用户输入首先从新数据源提取信息。用户查询和相关信息都提供给 LLM。LLM 使用新知识及其训练数据来创建更好的响应。以下各部分概述了该过程。
    </p>
    <h5>
     <a id="font_stylecolor000000backgroundcolorrgb251_251_251font_36">
     </a>
     <strong>
      创建外部数据
     </strong>
    </h5>
    <p>
     LLM 原始训练数据集之外的新数据称为
     <em>
      外部数据
     </em>
     。它可以来自多个数据来源，例如 API、数据库或文档存储库。数据可能以各种格式存在，例如文件、数据库记录或长篇文本。另一种称为
     <em>
      嵌入语言模型
     </em>
     的 AI 技术将数据转换为数字表示形式并将其存储在向量数据库中。这个过程会创建一个生成式人工智能模型可以理解的知识库。
    </p>
    <h5>
     <a id="font_stylecolor000000backgroundcolorrgb251_251_251font_39">
     </a>
     <strong>
      检索相关信息
     </strong>
    </h5>
    <p>
     下一步是执行相关性搜索。用户查询将转换为向量表示形式，并与向量数据库匹配。例如，考虑一个可以回答组织的人力资源问题的智能聊天机器人。如果员工搜索
     <em>
      ：“我有多少年假？”
     </em>
     ，系统将检索年假政策文件以及员工个人过去的休假记录。这些特定文件将被退回，因为它们与员工输入的内容高度相关。相关性是使用数学向量计算和表示法计算和建立的。
    </p>
    <h5>
     <a id="font_stylecolor000000backgroundcolorrgb251_251_251_LLM_font_42">
     </a>
     <strong>
      增强 LLM 提示
     </strong>
    </h5>
    <p>
     接下来，RAG 模型通过在上下文中添加检索到的相关数据来增强用户输入（或提示）。此步骤使用提示工程技术与 LLM 进行有效沟通。增强提示允许大型语言模型为用户查询生成准确的答案。
    </p>
    <h5>
     <a id="font_stylecolor000000backgroundcolorrgb251_251_251font_45">
     </a>
     <strong>
      更新外部数据
     </strong>
    </h5>
    <p>
     下一个问题可能是：如果外部数据过时了怎么办？ 要维护当前信息以供检索，请异步更新文档并更新文档的嵌入表示形式。您可以通过自动化实时流程或定期批处理来执行此操作。这是数据分析中常见的挑战：可以使用不同的数据科学方法进行变更管理。
    </p>
    <p>
     下图显示了将 RAG 与 LLM 配合使用的概念流程。
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/b0dca4810c21b4356cdeb372f6be87af.jpeg">
      <br/>
     </img>
    </p>
    <h4>
     <a id="_font_stylecolor000000font_53">
     </a>
     三、 检索增强生成和语义搜索有什么区别？
    </h4>
    <p>
     语义搜索可以提高 RAG 结果，适用于想要在其 LLM 应用程序中添加大量外部知识源的组织。现代企业在各种系统中存储大量信息，例如手册、常见问题、研究报告、客户服务指南和人力资源文档存储库等。上下文检索在规模上具有挑战性，因此会降低生成输出质量。
    </p>
    <p>
     语义搜索技术可以扫描包含不同信息的大型数据库，并更准确地检索数据。例如，他们可以回答诸如
     <em>
      “去年在机械维修上花了多少钱？”
     </em>
     之类的问题，方法是将问题映射到相关文档并返回特定文本而不是搜索结果。然后，开发人员可以使用该答案为 LLM 提供更多上下文。
    </p>
    <p>
     RAG 中的传统或关键字搜索解决方案对知识密集型任务产生的结果有限。开发人员在手动准备数据时还必须处理单词嵌入、文档分块和其他复杂问题。相比之下，语义搜索技术可以完成知识库准备的所有工作，因此开发人员不必这样做。它们还生成语义相关的段落和按相关性排序的标记词，以最大限度地提高 RAG 有效载荷的质量。
    </p>
    <h4>
     <a id="Document_loadersText_splitters_60">
     </a>
     四、Document loaders和Text splitters
    </h4>
    <h5>
     <a id="font_stylecolor000000Document_loadersfont_61">
     </a>
     Document loaders(文档加载器)
    </h5>
    <p>
     Document loaders(文档加载器) 这些类加载文档对象。LangChain与各种数据源有数百个集成，可以从中加载数据：Slack、Notion、Google Drive等。 每个文档加载器都有自己特定的参数，但它们可以通过相同的方式使用
     <code>
      .load
     </code>
     方法调用。 以下是一个示例用法：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders<span class="token punctuation">.</span>csv_loader <span class="token keyword">import</span> CSVLoader
loader <span class="token operator">=</span> CSVLoader<span class="token punctuation">(</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token comment"># &lt;-- 在这里添加特定于集成的参数</span>
<span class="token punctuation">)</span>
data <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_CSV__72">
     </a>
     如何加载 CSV 文件
    </h5>
    <p>
     <a href="https://zh.wikipedia.org/wiki/%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E5%80%BC" rel="nofollow">
      逗号分隔值（CSV）
     </a>
     文件是一种使用逗号分隔值的定界文本文件。文件的每一行是一个数据记录。每个记录由一个或多个字段组成，字段之间用逗号分隔。
    </p>
    <p>
     LangChain 实现了一个
     <a href="https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.csv_loader.CSVLoader.html" rel="nofollow">
      CSV 加载器
     </a>
     ，可以将 CSV 文件加载为一系列
     <a href="https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document" rel="nofollow">
      Document
     </a>
     对象。CSV 文件的每一行都会被翻译为一个文档。
    </p>
    <pre><code class="prism language-python"><span class="token comment">#示例：csv_loader.py</span>
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders<span class="token punctuation">.</span>csv_loader <span class="token keyword">import</span> CSVLoader

file_path <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"../../resource/doc_search.csv"</span>
<span class="token punctuation">)</span>
loader <span class="token operator">=</span> CSVLoader<span class="token punctuation">(</span>file_path<span class="token operator">=</span>file_path<span class="token punctuation">,</span>encoding<span class="token operator">=</span><span class="token string">"UTF-8"</span><span class="token punctuation">)</span>
data <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> record <span class="token keyword">in</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>record<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-python">page_content<span class="token operator">=</span>'名称<span class="token punctuation">:</span> 狮子
种类<span class="token punctuation">:</span> 哺乳动物
年龄<span class="token punctuation">:</span> <span class="token number">8</span>
栖息地<span class="token punctuation">:</span> 非洲草原<span class="token string">' metadata={'</span>source<span class="token string">': '</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span>resource<span class="token operator">/</span>doc_search<span class="token punctuation">.</span>csv<span class="token string">', '</span>row'<span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">}</span>
page_content<span class="token operator">=</span>'名称<span class="token punctuation">:</span> 大熊猫
种类<span class="token punctuation">:</span> 哺乳动物
年龄<span class="token punctuation">:</span> <span class="token number">5</span>
栖息地<span class="token punctuation">:</span> 中国竹林<span class="token string">' metadata={'</span>source<span class="token string">': '</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">/</span>resource<span class="token operator">/</span>doc_search<span class="token punctuation">.</span>csv<span class="token string">', '</span>row'<span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">}</span>
</code></pre>
    <h5>
     <a id="_CSV__101">
     </a>
     自定义 CSV 解析和加载
    </h5>
    <p>
     <code>
      CSVLoader
     </code>
     接受一个
     <code>
      csv_args
     </code>
     关键字参数，用于自定义传递给 Python 的
     <code>
      csv.DictReader
     </code>
     的参数。有关支持的 csv 参数的更多信息，请参阅
     <a href="https://docs.python.org/zh-cn/3/library/csv.html" rel="nofollow">
      csv 模块
     </a>
     文档。
    </p>
    <pre><code class="prism language-python"><span class="token comment">#示例：csv_custom.py</span>
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders<span class="token punctuation">.</span>csv_loader <span class="token keyword">import</span> CSVLoader

file_path <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"../../resource/doc_search.csv"</span>
<span class="token punctuation">)</span>
loader <span class="token operator">=</span> CSVLoader<span class="token punctuation">(</span>
    file_path<span class="token operator">=</span>file_path<span class="token punctuation">,</span>
    encoding<span class="token operator">=</span><span class="token string">"UTF-8"</span><span class="token punctuation">,</span>
    csv_args<span class="token operator">=</span><span class="token punctuation">{<!-- --></span>
        <span class="token string">"delimiter"</span><span class="token punctuation">:</span> <span class="token string">","</span><span class="token punctuation">,</span>
        <span class="token string">"quotechar"</span><span class="token punctuation">:</span> <span class="token string">'"'</span><span class="token punctuation">,</span>
        <span class="token string">"fieldnames"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"Name"</span><span class="token punctuation">,</span> <span class="token string">"Species"</span><span class="token punctuation">,</span> <span class="token string">"Age"</span><span class="token punctuation">,</span> <span class="token string">"Habitat"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
data <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> record <span class="token keyword">in</span> data<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>record<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">page_content='Name: 名称
Species: 种类
Age: 年龄
Habitat: 栖息地' metadata={'source': '../../resource/doc_search.csv', 'row': 0}
page_content='Name: 狮子
Species: 哺乳动物
Age: 8
Habitat: 非洲草原' metadata={'source': '../../resource/doc_search.csv', 'row': 1}
</code></pre>
    <h5>
     <a id="_HTML_138">
     </a>
     如何加载 HTML
    </h5>
    <p>
     超文本标记语言（HTML）是用于在Web浏览器中显示的文档的标准标记语言。
    </p>
    <p>
     这里介绍了如何将HTML文档加载到LangChain的
     <a href="https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document" rel="nofollow">
      Document
     </a>
     对象中，以便我们可以在下游使用。
    </p>
    <p>
     解析HTML文件通常需要专门的工具。在这里，我们演示了如何通过
     <a href="https://unstructured-io.github.io/unstructured/" rel="nofollow">
      Unstructured
     </a>
     和
     <a href="https://beautiful-soup-4.readthedocs.io/en/latest/" rel="nofollow">
      BeautifulSoup4
     </a>
     进行解析，可以通过pip安装。请前往集成页面查找与其他服务的集成，例如
     <a href="http://www.aidoczh.com/langchain/v0.2/docs/integrations/document_loaders/azure_document_intelligence/" rel="nofollow">
      Azure AI Document Intelligence
     </a>
     或
     <a href="http://www.aidoczh.com/langchain/v0.2/docs/integrations/document_loaders/firecrawl/" rel="nofollow">
      FireCrawl
     </a>
     。
    </p>
    <h5>
     <a id="UnstructuredHTML_145">
     </a>
     使用Unstructured加载HTML
    </h5>
    <pre><code class="prism language-python"><span class="token operator">%</span>pip install <span class="token string">"unstructured[html]"</span>
</code></pre>
    <pre><code class="prism language-python"><span class="token comment">#示例：html_loader.py</span>
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> UnstructuredHTMLLoader

file_path <span class="token operator">=</span> <span class="token string">"../../resource/content.html"</span>
loader <span class="token operator">=</span> UnstructuredHTMLLoader<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> encodings<span class="token operator">=</span><span class="token string">"UTF-8"</span><span class="token punctuation">)</span>
data <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">[Document(metadata={'source': '../../resource/content.html'}, page_content='风景展示\n\n黄山\n\n黄山位于中国安徽省南部，是中国著名的风景名胜区，以奇松、怪石、云海和温泉“四绝”闻名。\n\n大峡谷\n\n大峡谷位于美国亚利桑那州，是世界上最著名的自然景观之一，以其壮观的地质奇观和深邃的峡谷闻名。')]
</code></pre>
    <h5>
     <a id="BeautifulSoup4HTML_164">
     </a>
     使用BeautifulSoup4加载HTML
    </h5>
    <p>
     我们还可以使用BeautifulSoup4使用
     <code>
      BSHTMLLoader&lt;加载HTML文档。这将将HTML中的文本提取到page_content中
     </code>
     ，并将页面标题提取到metadata的
     <code>
      title
     </code>
     中。
    </p>
    <pre><code class="prism language-python"><span class="token comment">#pip install bs4</span>
</code></pre>
    <pre><code class="prism language-python"><span class="token comment">#示例：html_bs4.py</span>
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> BSHTMLLoader

file_path <span class="token operator">=</span> <span class="token string">"../../resource/content.html"</span>
loader <span class="token operator">=</span> BSHTMLLoader<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> open_encoding<span class="token operator">=</span><span class="token string">"UTF-8"</span><span class="token punctuation">)</span>
data <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">[Document(metadata={'source': '../../resource/content.html', 'title': '风景展示'}, page_content='\n\n\n\n风景展示\n\n\n\n风景展示\n\n黄山\n黄山位于中国安徽省南部，是中国著名的风景名胜区，以奇松、怪石、云海和温泉“四绝”闻名。\n\n\n\n大峡谷\n大峡谷位于美国亚利桑那州，是世界上最著名的自然景观之一，以其壮观的地质奇观和深邃的峡谷闻名。\n\n\n\n')]
</code></pre>
    <hr/>
    <h5>
     <a id="_PDF_189">
     </a>
     如何加载 PDF文件
    </h5>
    <p>
     <a href="https://zh.wikipedia.org/wiki/PDF" rel="nofollow">
      便携式文档格式（PDF）
     </a>
     是由Adobe于1992年开发的一种文件格式，标准化为ISO 32000。它以一种与应用软件、硬件和操作系统无关的方式呈现文档，包括文本格式和图像。
    </p>
    <p>
     本指南介绍了如何将PDF文档加载到我们在下游使用的LangChain
     <a href="https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document" rel="nofollow">
      Document
     </a>
     格式中。
    </p>
    <p>
     LangChain集成了许多PDF解析器。有些解析器简单且相对低级，而其他解析器支持OCR和图像处理，或进行高级文档布局分析。选择合适的解析器将取决于您的应用程序。下面我们列举了一些可能的选择。
    </p>
    <h5>
     <a id="PyPDF_196">
     </a>
     使用PyPDF
    </h5>
    <p>
     这里我们使用
     <code>
      pypdf
     </code>
     将PDF加载为文档数组，其中每个文档包含页面内容和带有
     <code>
      page
     </code>
     编号的元数据。
    </p>
    <pre><code class="prism language-python"><span class="token operator">%</span>pip install pypdf
</code></pre>
    <pre><code class="prism language-python"><span class="token comment">#示例：pdf_loader.py</span>
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> PyPDFLoader
file_path <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"../../resource/pytorch.pdf"</span><span class="token punctuation">)</span>
loader <span class="token operator">=</span> PyPDFLoader<span class="token punctuation">(</span>file_path<span class="token punctuation">)</span>
pages <span class="token operator">=</span> loader<span class="token punctuation">.</span>load_and_split<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>pages<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">page_content='PyTorch: An Imperative Style, High-Performance
Deep Learning Library
Adam Paszke
University of Warsaw
adam.paszke@gmail.comSam Gross
Facebook AI Research
sgross@fb.comFrancisco Massa
Facebook AI Research
fmassa@fb.com
Adam Lerer
Facebook AI Research
alerer@fb.comJames Bradbury
Google
jekbradbury@gmail.comGregory Chanan
Facebook AI Research
gchanan@fb.com
Trevor Killeen
Self Employed
killeent@cs.washington.eduZeming Lin
Facebook AI Research
zlin@fb.comNatalia Gimelshein
NVIDIA
ngimelshein@nvidia.com
Luca Antiga
Orobix
luca.antiga@orobix.comAlban Desmaison
Oxford University
alban@robots.ox.ac.ukAndreas Köpf
Xamla
andreas.koepf@xamla.com
Edward Yang
Facebook AI Research
ezyang@fb.comZach DeVito
Facebook AI Research
zdevito@cs.stanford.eduMartin Raison
Nabla
martinraison@gmail.com
Alykhan Tejani
Twitter
atejani@twitter.comSasank Chilamkurthy
Qure.ai
sasankchilamkurthy@gmail.comBenoit Steiner
Facebook AI Research
benoitsteiner@fb.com
Lu Fang
Facebook
lufang@fb.comJunjie Bai
Facebook
jbai@fb.comSoumith Chintala
Facebook AI Research
soumith@gmail.com
Abstract
Deep learning frameworks have often focused on either usability or speed, but
not both. PyTorch is a machine learning library that shows that these two goals
are in fact compatible: it provides an imperative and Pythonic programming style
that supports code as a model, makes debugging easy and is consistent with other
popular scientiﬁc computing libraries, while remaining efﬁcient and supporting
hardware accelerators such as GPUs.
In this paper, we detail the principles that drove the implementation of PyTorch
and how they are reﬂected in its architecture. We emphasize that every aspect of
PyTorch is a regular Python program under the full control of its user. We also
explain how the careful and pragmatic implementation of the key components of
its runtime enables them to work together to achieve compelling performance.
We demonstrate the efﬁciency of individual subsystems, as well as the overall
speed of PyTorch on several common benchmarks.
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.' metadata={'source': '../../resource/pytorch.pdf', 'page': 0}
</code></pre>
    <p>
     这种方法的优点是可以通过页码检索文档。
    </p>
    <h6>
     <a id="PDF_284">
     </a>
     对PDF进行向量搜索
    </h6>
    <p>
     一旦我们将PDF加载到LangChain的
     <code>
      Document
     </code>
     对象中，我们可以像通常一样对它们进行索引（例如，RAG应用程序）。
    </p>
    <pre><code class="prism language-python"><span class="token comment">#示例：pdf_search.py</span>
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>vectorstores <span class="token keyword">import</span> FAISS
<span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> OpenAIEmbeddings
faiss_index <span class="token operator">=</span> FAISS<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>pages<span class="token punctuation">,</span> OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
docs <span class="token operator">=</span> faiss_index<span class="token punctuation">.</span>similarity_search<span class="token punctuation">(</span><span class="token string">"What is LayoutParser?"</span><span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> doc <span class="token keyword">in</span> docs<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">str</span><span class="token punctuation">(</span>doc<span class="token punctuation">.</span>metadata<span class="token punctuation">[</span><span class="token string">"page"</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">":"</span><span class="token punctuation">,</span> doc<span class="token punctuation">.</span>page_content<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">300</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">0: PyTorch: An Imperative Style, High-Performance
Deep Learning Library
Adam Paszke
University of Warsaw
adam.paszke@gmail.comSam Gross
Facebook AI Research
sgross@fb.comFrancisco Massa
Facebook AI Research
fmassa@fb.com
Adam Lerer
Facebook AI Research
alerer@fb.comJames Bradbury
Google
jekbradbury@gma
1: 1 Introduction
With the increased interest in deep learning in recent years, there has been an explosion of machine
learning tools. Many popular frameworks such as Caffe [ 1], CNTK [ 2], TensorFlow [ 3], and
Theano [ 4], construct a static dataﬂow graph that represents the computation and which can 

</code></pre>
    <p>
     从图像中提取文本一些 PDF 包含文本图像，例如扫描文档或图表。使用
     <code>
      rapidocr-onnxruntime
     </code>
     软件包，我们也可以将图像提取为文本：
    </p>
    <pre><code class="prism language-python"><span class="token comment">#示例：pdf_image_text.py</span>
<span class="token comment">#pip install rapidocr-onnxruntime</span>
file_path <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">"../../resource/pytorch.pdf"</span><span class="token punctuation">)</span>
loader <span class="token operator">=</span> PyPDFLoader<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> extract_images<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
pages <span class="token operator">=</span> loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment">#识别第9页图片文字</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>pages<span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">.</span>page_content<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">6.4 Adoption
The validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy,
we tried to quantify how well the machine learning community received PyTorch by counting how
often various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch,
TensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in
January 2017. In Figure 3 we report the monthly number of mentions of the word "PyTorch" as a
percentage of all mentions among these deep learning frameworks. We counted tools mentioned
multiple times in a given paper only once, and made the search case insensitive to account for various
spellings.
Figure 3: Among arXiv papers each month that mention common deep learning frameworks, percentage of
them that mention PyTorch.
7 Conclusion and future work
PyTorch has become a popular tool in the deep learning research community by combining a focus
on usability with careful performance considerations. In addition to continuing to support the latest
trends and advances in deep learning, in the future we plan to continue to improve the speed and
scalability of PyTorch. Most notably, we are working on the PyTorch JIT: a suite of tools that
allow PyTorch programs to be executed outside of the Python interpreter where they can be further
optimized. We also intend to improve support for distributed computation by providing efﬁcient
primitives for data parallelism as well as a Pythonic library for model parallelism based around
remote procedure calls.
8 Acknowledgements
We are grateful to the PyTorch community for their feedback and contributions that greatly inﬂuenced
the design and implementation of PyTorch. We thank all the PyTorch core team members, contributors
and package maintainers including Ailing Zhang, Alex Suhan, Alfredo Mendoza, Alican Bozkurt,
Andrew Tulloch, Ansha Yu, Anthony Shoumikhin, Bram Wasti, Brian Vaughan, Christian Puhrsch,
David Reiss, David Riazati, Davide Libenzi, Dmytro Dzhulgakov, Dwaraj Rajagopal, Edward Yang,
Elias Ellison, Fritz Obermeyer, George Zhang, Hao Lu, Hong Xu, Hung Duong, Igor Fedan, Ilia
Cherniavskii, Iurii Zdebskyi, Ivan Kobzarev, James Reed, Jeff Smith, Jerry Chen, Jerry Zhang, Jiakai
Liu, Johannes M. Dieterich, Karl Ostmo, Lin Qiao, Martin Yuan, Michael Suo, Mike Ruberry, Mikhail
Zolothukhin, Mingzhe Li, Neeraj Pradhan, Nick Korovaiko, Owen Anderson, Pavel Belevich, Peter
Johnson, Pritam Damania, Raghuraman Krishnamoorthi, Richard Zou, Roy Li, Rui Zhu, Sebastian
Messmer, Shen Li, Simon Wang, Supriya Rao, Tao Xu, Thomas Viehmann, Vincent Quenneville-
Belair, Vishwak Srinivasan, Vitaly Fedyunin, Wanchao Liang, Wei Yang, Will Feng, Xiaomeng Yang,
Xiaoqiang Zheng, Xintao Chen, Yangqing Jia, Yanli Zhao, Yinghai Lu and Zafar Takhirov.
References
[1]Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature
embedding. arXiv preprint arXiv:1408.5093 , 2014.
[2]Frank Seide and Amit Agarwal. Cntk: Microsoft’s open-source deep-learning toolkit. In
Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining , KDD ’16, pages 2135–2135, New York, NY , USA, 2016. ACM.
950%
40%
30%
20%
10%
0%
Jul2017
Jan2018
Jul2018
Jan2019
</code></pre>
    <h4>
     <a id="Text_splitters_389">
     </a>
     五、Text splitters(文本分割器)
    </h4>
    <h5>
     <a id="_390">
     </a>
     如何递归分割文本
    </h5>
    <p>
     递归分割(recursively)，这个文本分割器是用于通用文本的推荐工具。它接受一个字符列表作为参数。它会按顺序尝试在这些字符上进行分割，直到块足够小。默认的字符列表是 [“\n\n”, “\n”, " ", “”]。这样做的效果是尽可能保持所有段落（然后是句子，再然后是单词）在一起，因为这些通常看起来是语义上相关的文本块。
    </p>
    <ol>
     <li>
      文本如何分割：根据字符列表。
     </li>
     <li>
      块大小如何衡量：根据字符数量。
     </li>
    </ol>
    <p>
     下面我们展示一个使用示例。
    </p>
    <p>
     要直接获取字符串内容，请使用
     <code>
      .split_text
     </code>
    </p>
    <p>
     要创建 LangChain
     <a href="https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html" rel="nofollow">
      Document
     </a>
     对象（例如，用于下游任务），请使用
     <code>
      .create_documents
     </code>
    </p>
    <pre><code class="prism language-python"><span class="token operator">%</span>pip install <span class="token operator">-</span>qU langchain<span class="token operator">-</span>text<span class="token operator">-</span>splitters
</code></pre>
    <pre><code class="prism language-python"><span class="token comment">#示例：recursively_split.py</span>
<span class="token keyword">from</span> langchain_text_splitters <span class="token keyword">import</span> RecursiveCharacterTextSplitter

<span class="token comment"># 加载示例文档</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"../../resource/knowledge.txt"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    state_of_the_union <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>
    <span class="token comment"># 设置一个非常小的块大小，只是为了展示。</span>
    chunk_size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    chunk_overlap<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>
    length_function<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">,</span>
    is_separator_regex<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
texts <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>create_documents<span class="token punctuation">(</span><span class="token punctuation">[</span>state_of_the_union<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>texts<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">page_content='﻿I am honored to be with you today at your commencement from one of the finest universities in the'
page_content='universities in the world. I never graduated from college. Truth be told, this is the closest I've'
</code></pre>
    <pre><code class="prism language-python">text_splitter<span class="token punctuation">.</span>split_text<span class="token punctuation">(</span>knowledge<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>
</code></pre>
    <pre><code class="prism language-python"><span class="token punctuation">[</span><span class="token string">'\ufeffI am honored to be with you today at your commencement from one of the finest universities in the'</span><span class="token punctuation">,</span> <span class="token string">"universities in the world. I never graduated from college. Truth be told, this is the closest I've"</span><span class="token punctuation">]</span>
</code></pre>
    <p>
     让我们来看看上述
     <code>
      RecursiveCharacterTextSplitter
     </code>
     的参数设置：
    </p>
    <ul>
     <li>
      <code>
       chunk_size
      </code>
      ：块的最大大小，大小由
      <code>
       length_function
      </code>
      决定。
     </li>
     <li>
      <code>
       chunk_overlap
      </code>
      ：块之间的目标重叠。重叠的块有助于在上下文分割时减少信息丢失。
     </li>
     <li>
      <code>
       length_function
      </code>
      ：确定块大小的函数。
     </li>
     <li>
      <code>
       is_separator_regex：分隔符列表（默认为["\n\n", "\n", " ", ""])
      </code>
      是否应被解释为正则表达式。
     </li>
    </ul>
    <h5>
     <a id="_444">
     </a>
     从没有词边界的语言中分割文本
    </h5>
    <p>
     一些书写系统没有
     <a href="https://en.wikipedia.org/wiki/Category:Writing_systems_without_word_boundaries" rel="nofollow">
      词边界
     </a>
     ，例如中文、日文和泰文。使用默认分隔符列表
     <code>
      ["\n\n", "\n", " ", ""]
     </code>
     分割文本可能会导致单词被分割在不同块之间。为了保持单词在一起，您可以覆盖分隔符列表，包括额外的标点符号：
    </p>
    <ul>
     <li>
      添加 ASCII 句号
      <code>
       .
      </code>
      ，
      <a href="https://en.wikipedia.org/wiki/Halfwidth_and_Fullwidth_Forms_%28Unicode_block%29" rel="nofollow">
       Unicode 全角
      </a>
      句号 “
      <code>
       .
      </code>
      （用于中文文本），以及
      <a href="https://en.wikipedia.org/wiki/CJK_Symbols_and_Punctuation" rel="nofollow">
       表意句号
      </a>
      ”
      <code>
       。
      </code>
      （用于日文和中文）
     </li>
     <li>
      添加
      <a href="https://en.wikipedia.org/wiki/Zero-width_space" rel="nofollow">
       零宽空格
      </a>
      用于泰文、缅甸文、高棉文和日文。
     </li>
     <li>
      添加 ASCII 逗号 “
      <code>
       ,
      </code>
      ”，Unicode 全角逗号 “
      <code>
       ，
      </code>
      ”，以及 Unicode 表意逗号 “
      <code>
       、
      </code>
      ”
     </li>
    </ul>
    <pre><code class="prism language-python"><span class="token comment">#示例：recursively_separator.py</span>
text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>
    separators<span class="token operator">=</span><span class="token punctuation">[</span>
        <span class="token string">"\n\n"</span><span class="token punctuation">,</span>
        <span class="token string">"\n"</span><span class="token punctuation">,</span>
        <span class="token string">" "</span><span class="token punctuation">,</span>
        <span class="token string">"."</span><span class="token punctuation">,</span>
        <span class="token string">","</span><span class="token punctuation">,</span>
        <span class="token string">"\u200b"</span><span class="token punctuation">,</span>  <span class="token comment"># 零宽空格</span>
        <span class="token string">"\uff0c"</span><span class="token punctuation">,</span>  <span class="token comment"># 全角逗号</span>
        <span class="token string">"\u3001"</span><span class="token punctuation">,</span>  <span class="token comment"># 表意逗号</span>
        <span class="token string">"\uff0e"</span><span class="token punctuation">,</span>  <span class="token comment"># 全角句号</span>
        <span class="token string">"\u3002"</span><span class="token punctuation">,</span>  <span class="token comment"># 表意句号</span>
        <span class="token string">""</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token comment"># 已有的参数</span>
<span class="token punctuation">)</span>
</code></pre>
    <hr/>
    <h5>
     <a id="_475">
     </a>
     按照语义块分割文本
    </h5>
    <p>
     下面介绍如何根据语义相似性拆分文本块(semantic chunks)。如果嵌入足够远，文本块将被拆分。
    </p>
    <p>
     在高层次上，这将文本拆分成句子，然后分组为每组3个句子，最后合并在嵌入空间中相似的句子。
    </p>
    <h5>
     <a id="_480">
     </a>
     安装依赖项
    </h5>
    <pre><code class="prism language-python"><span class="token comment">#pip install --quiet langchain_experimental langchain_openai</span>
</code></pre>
    <h5>
     <a id="_485">
     </a>
     载入示例数据
    </h5>
    <pre><code class="prism language-python"><span class="token comment">#示例：semantic_split.py</span>
<span class="token comment"># 这是一个长文档，我们可以将其拆分。</span>
<span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"../../resource/knowledge.txt"</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
    knowledge <span class="token operator">=</span> f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_493">
     </a>
     创建文本拆分器
    </h5>
    <p>
     要实例化一个
     <a href="https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html" rel="nofollow">
      SemanticChunker
     </a>
     ，我们必须指定一个嵌入模型。下面我们将使用
     <a href="https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.openai.OpenAIEmbeddings.html" rel="nofollow">
      OpenAIEmbeddings
     </a>
     。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> langchain_experimental<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> SemanticChunker
<span class="token keyword">from</span> langchain_openai<span class="token punctuation">.</span>embeddings <span class="token keyword">import</span> OpenAIEmbeddings
text_splitter <span class="token operator">=</span> SemanticChunker<span class="token punctuation">(</span>OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="_502">
     </a>
     拆分文本
    </h5>
    <p>
     我们按照通常的方式拆分文本，例如，通过调用
     <code>
      .create_documents
     </code>
     来创建 LangChain
     <a href="https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html" rel="nofollow">
      Document
     </a>
     对象：
    </p>
    <pre><code class="prism language-python">docs <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>create_documents<span class="token punctuation">(</span><span class="token punctuation">[</span>knowledge<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>docs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>page_content<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college. Truth be told, this is the closest I've ever gotten to a college graduation. Today I want to tell you three stories from my life. That's it. No big deal.
</code></pre>
    <h5>
     <a id="_514">
     </a>
     断点
    </h5>
    <p>
     这个拆分器的工作原理是确定何时“断开”句子。这是通过查找任意两个句子之间的嵌入差异来完成的。当该差异超过某个阈值时，它们就会被拆分。
    </p>
    <p>
     有几种方法可以确定该阈值，这由
     <code>
      breakpoint_threshold_type
     </code>
     关键字参数控制。
    </p>
    <h6>
     <a id="_519">
     </a>
     百分位数
    </h6>
    <p>
     拆分的默认方式是基于百分位数。在此方法中，计算所有句子之间的差异，然后任何大于X百分位数的差异都会被拆分。
    </p>
    <pre><code class="prism language-python"><span class="token comment">#示例：semantic_split_percentile.py</span>
text_splitter <span class="token operator">=</span> SemanticChunker<span class="token punctuation">(</span>
    OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> breakpoint_threshold_type<span class="token operator">=</span><span class="token string">"percentile"</span><span class="token punctuation">,</span> breakpoint_threshold_amount<span class="token operator">=</span><span class="token number">50</span>
<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-python">docs <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>create_documents<span class="token punctuation">(</span><span class="token punctuation">[</span>knowledge<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>docs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>page_content<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">I am honored to be with you today at your commencement from one of the finest universities in the world. I never graduated from college.
</code></pre>
    <pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>docs<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-plain">71
</code></pre>
    <h4>
     <a id="_font_stylecolor000000Loader_Splitter_Embedding_Vector_Store_Retrievers_font_546">
     </a>
     六、 Loader, Splitter, Embedding, Vector Store, Retrievers 的综合应用
    </h4>
    <h5>
     <a id="font_stylecolorrgb51_51_51_Streamlit__RAG__APPfont_547">
     </a>
     使用 Streamlit 实现一个支持记忆的 RAG 问答 APP
    </h5>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_548">
     </a>
     实现流程
    </h5>
    <p>
     一个 RAG 程序的 APP 主要有以下流程：
    </p>
    <ol>
     <li>
      用户在 RAG 客户端上传一个txt文件
     </li>
     <li>
      服务器端接收客户端文件，存储在服务端
     </li>
     <li>
      服务器端程序对文件进行读取
     </li>
     <li>
      对文件内容进行拆分，防止一次性塞给 Embedding 模型超 token 限制
     </li>
     <li>
      把 Embedding 后的内容存储在向量数据库，生成检索器
     </li>
     <li>
      程序准备就绪，允许用户进行提问
     </li>
     <li>
      用户提出问题，大模型调用检索器检索文档，把相关片段找出来后，组织后，回复用户。
     </li>
    </ol>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/2aab7b406a662bde799d2449fbe02eda.png"/>
    </p>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_561">
     </a>
     引入记忆
    </h5>
    <p>
     以上流程实现起来没有太大难度，但是如果你想在这个基础上实现模型记忆功能，就会比较困难了。梳理下为什么引入记忆会存在困难。我们把引入模型记忆功能之后的使用场景演绎下：
    </p>
    <p>
     假如我们上传了一份最新的新闻内容 txt 文件，如下：
    </p>
    <pre><code class="prism language-plain">2024 世界人工智能大会暨人工智能全球治理高级别会议（简称“WAIC 2024”）将于 7 月4日-6日在上海世博中心、世博展览馆举行。
同时外交部消息称，重要领导人将于7月4日出席开幕式，并发表主旨演讲。
大会围绕“以共商促共享 以善治促善智（Governing AI for Good and for All）”主题，
聚焦大模型、算力、机器人、自动驾驶等重点领域，首发一批创新产品。
目前，已有500余家企业确认参展，市外企业和国际企业占比超50%，展品数量已超1500项，参展企业数 、亮点展品数和首发新品数均创历史最高，
进一步释放大会对人工智能产业的“磁场效应”。
市场普遍认为，AI产业链有望迎来新突破，行业配置价值凸显.
</code></pre>
    <p>
     如下是针对这份文档的提问：
    </p>
    <pre><code class="prism language-python">用户：<span class="token number">2024</span> 世界人工智能大会那一天开始，地点在哪?
大模型：<span class="token number">2024</span> 世界人工智能大会暨人工智能全球治理高级别会议将于<span class="token number">7</span>月<span class="token number">4</span>日至<span class="token number">6</span>日在上海世博中心、世博展览馆举行。
用户：大会主题是什么?
大模型：大会主题是<span class="token string">"以共商促共享 以善治促善智（Governing AI for Good and for All）"</span>。
</code></pre>
    <p>
     用户的第二次提问是针对大模型的回复上下文场景下的，所以大模型第二次提问，需要根据整个对话内容重新理解提问，并且改写用户的提问，再通过检索器进行检索后，最终回答用户的第二次提问。
    </p>
    <p>
     所以引入记忆带来的额外问题的核心就是，如果需要 RAG 程序要支持记忆，就需要额外加入模型理解对话上下文后改写问题，再次进行检索并回答的流程。
    </p>
    <p>
     整个流程需要改成如下：
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/606b5010a9f70b57836835e1769a7b44.png"/>
    </p>
    <h4>
     <a id="font_stylecolorrgb51_51_51font_593">
     </a>
     七、代码实现
    </h4>
    <p>
     使用 Streamlit 实现文件上传，我这里只实现了 txt 文件上传，其实这里可以在
     <a href="https://so.csdn.net/so/search?q=type&amp;spm=1001.2101.3001.7020">
      type
     </a>
     参数里面设置多个文件类型，在后面的检索器方法里面针对每个类型进行处理即可。
    </p>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_596">
     </a>
     实现文件上传
    </h5>
    <pre><code class="prism language-python"><span class="token comment">#示例：txt_search.py</span>
<span class="token keyword">import</span> streamlit <span class="token keyword">as</span> st

<span class="token comment"># 上传txt文件，允许上传多个文件</span>
uploaded_files <span class="token operator">=</span> st<span class="token punctuation">.</span>sidebar<span class="token punctuation">.</span>file_uploader<span class="token punctuation">(</span>
    label<span class="token operator">=</span><span class="token string">"上传txt文件"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"txt"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> accept_multiple_files<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> uploaded_files<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"请先上传按TXT文档。"</span><span class="token punctuation">)</span>
    st<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     stremlit run 一下，看下效果
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/237f63093b73d8c0c0c8fd20c135373e.png"/>
    </p>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_614">
     </a>
     实现检索器
    </h5>
    <p>
     注意 chunk_size 最大设置数值取决于 Embedding 模型允许单词的最大字符数限制。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> tempfile
<span class="token keyword">import</span> os
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> TextLoader
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>embeddings <span class="token keyword">import</span> QianfanEmbeddingsEndpoint
<span class="token keyword">from</span> langchain_chroma <span class="token keyword">import</span> Chroma
<span class="token keyword">from</span> langchain_text_splitters <span class="token keyword">import</span> RecursiveCharacterTextSplitter

<span class="token comment"># 实现检索器</span>
<span class="token decorator annotation punctuation">@st<span class="token punctuation">.</span>cache_resource</span><span class="token punctuation">(</span>ttl<span class="token operator">=</span><span class="token string">"1h"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">configure_retriever</span><span class="token punctuation">(</span>uploaded_files<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 读取上传的文档，并写入一个临时目录</span>
    docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    temp_dir <span class="token operator">=</span> tempfile<span class="token punctuation">.</span>TemporaryDirectory<span class="token punctuation">(</span><span class="token builtin">dir</span><span class="token operator">=</span><span class="token string">r"D:\tmp"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> <span class="token builtin">file</span> <span class="token keyword">in</span> uploaded_files<span class="token punctuation">:</span>
        temp_filepath <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>temp_dir<span class="token punctuation">.</span>name<span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token punctuation">.</span>name<span class="token punctuation">)</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>temp_filepath<span class="token punctuation">,</span> <span class="token string">"wb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">.</span>getvalue<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        loader <span class="token operator">=</span> TextLoader<span class="token punctuation">(</span>temp_filepath<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>
        docs<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 进行文档分割</span>
    text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>chunk_size<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>
    splits <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>split_documents<span class="token punctuation">(</span>docs<span class="token punctuation">)</span>

    <span class="token comment"># 这里使用了OpenAI向量模型</span>
    embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vectordb <span class="token operator">=</span> Chroma<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>splits<span class="token punctuation">,</span> embeddings<span class="token punctuation">)</span>

    retriever <span class="token operator">=</span> vectordb<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> retriever


retriever <span class="token operator">=</span> configure_retriever<span class="token punctuation">(</span>uploaded_files<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_654">
     </a>
     创建检索工具
    </h5>
    <p>
     langchain 提供了 create_retriever_tool 工具，可以直接用。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 创建检索工具</span>
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>tools<span class="token punctuation">.</span>retriever <span class="token keyword">import</span> create_retriever_tool

tool <span class="token operator">=</span> create_retriever_tool<span class="token punctuation">(</span>
    retriever<span class="token punctuation">,</span>
    <span class="token string">"文档检索"</span><span class="token punctuation">,</span>
    <span class="token string">"用于检索用户提出的问题，并基于检索到的文档内容进行回复."</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
tools <span class="token operator">=</span> <span class="token punctuation">[</span>tool<span class="token punctuation">]</span>
</code></pre>
    <h5>
     <a id="font_stylecolorrgb51_51_51_React_Agentfont_669">
     </a>
     创建 React Agent
    </h5>
    <pre><code class="prism language-python">instructions <span class="token operator">=</span> <span class="token triple-quoted-string string">"""您是一个设计用于查询文档来回答问题的代理。
您可以使用文档检索工具，并基于检索内容来回答问题
您可能不查询文档就知道答案，但是您仍然应该查询文档来获得答案。
如果您从文档中找不到任何信息用于回答问题，则只需返回“抱歉，这个问题我还不知道。”作为答案。
"""</span>

base_prompt_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
{instructions}

TOOLS:
------

You have access to the following tools:

{tools}

To use a tool, please use the following format:

•```
Thought: Do I need to use a tool? Yes
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
•```

When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:

•```
Thought: Do I need to use a tool? No
Final Answer: [your response here]
•```

Begin!

Previous conversation history:
{chat_history}

New input: {input}
{agent_scratchpad}"""</span>

base_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>base_prompt_template<span class="token punctuation">)</span>

prompt <span class="token operator">=</span> base_prompt<span class="token punctuation">.</span>partial<span class="token punctuation">(</span>instructions<span class="token operator">=</span>instructions<span class="token punctuation">)</span>

<span class="token comment"># 创建llm</span>
llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 创建react Agent</span>
agent <span class="token operator">=</span> create_react_agent<span class="token punctuation">(</span>llm<span class="token punctuation">,</span> tools<span class="token punctuation">,</span> prompt<span class="token punctuation">)</span>

agent_executor <span class="token operator">=</span> AgentExecutor<span class="token punctuation">(</span>agent<span class="token operator">=</span>agent<span class="token punctuation">,</span> tools<span class="token operator">=</span>tools<span class="token punctuation">,</span> memory<span class="token operator">=</span>memory<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="font_stylecolorrgb51_51_51_Agent_font_724">
     </a>
     实现 Agent 回复
    </h5>
    <p>
     获取用户输入，并回复用户，这里使用 StreamlitCallbackHandler 实现了 React 推理回调，可以让模型的推理过程可见。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 创建聊天输入框</span>
user_query <span class="token operator">=</span> st<span class="token punctuation">.</span>chat_input<span class="token punctuation">(</span>placeholder<span class="token operator">=</span><span class="token string">"请开始提问吧!"</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> user_query<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> user_query<span class="token punctuation">}</span><span class="token punctuation">)</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>user_query<span class="token punctuation">)</span>

    <span class="token keyword">with</span> st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">"assistant"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        st_cb <span class="token operator">=</span> StreamlitCallbackHandler<span class="token punctuation">(</span>st<span class="token punctuation">.</span>container<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"callbacks"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>st_cb<span class="token punctuation">]</span><span class="token punctuation">}</span>
        response <span class="token operator">=</span> agent_executor<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> user_query<span class="token punctuation">}</span><span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>
        st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> response<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
        st<span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     ‍
    </p>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_745">
     </a>
     完整代码
    </h5>
    <pre><code class="prism language-python"><span class="token comment">#示例：txt_search.py</span>
<span class="token comment">#pip install streamlit==1.37.0</span>
<span class="token keyword">import</span> streamlit <span class="token keyword">as</span> st
<span class="token keyword">import</span> tempfile
<span class="token keyword">import</span> os
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>chat_message_histories <span class="token keyword">import</span> StreamlitChatMessageHistory
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>document_loaders <span class="token keyword">import</span> TextLoader
<span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> OpenAIEmbeddings
<span class="token keyword">from</span> langchain_chroma <span class="token keyword">import</span> Chroma
<span class="token keyword">from</span> langchain_core<span class="token punctuation">.</span>prompts <span class="token keyword">import</span> PromptTemplate
<span class="token keyword">from</span> langchain_text_splitters <span class="token keyword">import</span> RecursiveCharacterTextSplitter
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>agents <span class="token keyword">import</span> create_react_agent<span class="token punctuation">,</span> AgentExecutor
<span class="token keyword">from</span> langchain_community<span class="token punctuation">.</span>callbacks<span class="token punctuation">.</span>streamlit <span class="token keyword">import</span> StreamlitCallbackHandler
<span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> ChatOpenAI

<span class="token comment"># 设置Streamlit应用的页面标题和布局</span>
st<span class="token punctuation">.</span>set_page_config<span class="token punctuation">(</span>page_title<span class="token operator">=</span><span class="token string">"Rag Agent"</span><span class="token punctuation">,</span> layout<span class="token operator">=</span><span class="token string">"wide"</span><span class="token punctuation">)</span>
<span class="token comment"># 设置应用的标题</span>
st<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"Rag Agent"</span><span class="token punctuation">)</span>

<span class="token comment"># 上传txt文件，允许上传多个文件</span>
uploaded_files <span class="token operator">=</span> st<span class="token punctuation">.</span>sidebar<span class="token punctuation">.</span>file_uploader<span class="token punctuation">(</span>
    label<span class="token operator">=</span><span class="token string">"上传txt文件"</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"txt"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> accept_multiple_files<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
<span class="token comment"># 如果没有上传文件，提示用户上传文件并停止运行</span>
<span class="token keyword">if</span> <span class="token keyword">not</span> uploaded_files<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"请先上传按TXT文档。"</span><span class="token punctuation">)</span>
    st<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 实现检索器</span>
<span class="token decorator annotation punctuation">@st<span class="token punctuation">.</span>cache_resource</span><span class="token punctuation">(</span>ttl<span class="token operator">=</span><span class="token string">"1h"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">configure_retriever</span><span class="token punctuation">(</span>uploaded_files<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 读取上传的文档，并写入一个临时目录</span>
    docs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    temp_dir <span class="token operator">=</span> tempfile<span class="token punctuation">.</span>TemporaryDirectory<span class="token punctuation">(</span><span class="token builtin">dir</span><span class="token operator">=</span><span class="token string">r"D:\tmp"</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> <span class="token builtin">file</span> <span class="token keyword">in</span> uploaded_files<span class="token punctuation">:</span>
        temp_filepath <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>temp_dir<span class="token punctuation">.</span>name<span class="token punctuation">,</span> <span class="token builtin">file</span><span class="token punctuation">.</span>name<span class="token punctuation">)</span>
        <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>temp_filepath<span class="token punctuation">,</span> <span class="token string">"wb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">.</span>getvalue<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># 使用TextLoader加载文本文件</span>
        loader <span class="token operator">=</span> TextLoader<span class="token punctuation">(</span>temp_filepath<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span>
        docs<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>loader<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># 进行文档分割</span>
    text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>chunk_size<span class="token operator">=</span><span class="token number">300</span><span class="token punctuation">,</span> chunk_overlap<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span>
    splits <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>split_documents<span class="token punctuation">(</span>docs<span class="token punctuation">)</span>

    <span class="token comment"># 使用OpenAI的向量模型生成文档的向量表示</span>
    embeddings <span class="token operator">=</span> OpenAIEmbeddings<span class="token punctuation">(</span><span class="token punctuation">)</span>
    vectordb <span class="token operator">=</span> Chroma<span class="token punctuation">.</span>from_documents<span class="token punctuation">(</span>splits<span class="token punctuation">,</span> embeddings<span class="token punctuation">)</span>

    <span class="token comment"># 创建文档检索器</span>
    retriever <span class="token operator">=</span> vectordb<span class="token punctuation">.</span>as_retriever<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> retriever

<span class="token comment"># 配置检索器</span>
retriever <span class="token operator">=</span> configure_retriever<span class="token punctuation">(</span>uploaded_files<span class="token punctuation">)</span>

<span class="token comment"># 如果session_state中没有消息记录或用户点击了清空聊天记录按钮，则初始化消息记录</span>
<span class="token keyword">if</span> <span class="token string">"messages"</span> <span class="token keyword">not</span> <span class="token keyword">in</span> st<span class="token punctuation">.</span>session_state <span class="token keyword">or</span> st<span class="token punctuation">.</span>sidebar<span class="token punctuation">.</span>button<span class="token punctuation">(</span><span class="token string">"清空聊天记录"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">[</span><span class="token string">"messages"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> <span class="token string">"您好，我是聚客AI助手，我可以查询文档"</span><span class="token punctuation">}</span><span class="token punctuation">]</span>

<span class="token comment"># 加载历史聊天记录</span>
<span class="token keyword">for</span> msg <span class="token keyword">in</span> st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">:</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span>msg<span class="token punctuation">[</span><span class="token string">"role"</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>msg<span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># 创建检索工具</span>
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>tools<span class="token punctuation">.</span>retriever <span class="token keyword">import</span> create_retriever_tool

<span class="token comment"># 创建用于文档检索的工具</span>
tool <span class="token operator">=</span> create_retriever_tool<span class="token punctuation">(</span>
    retriever<span class="token punctuation">,</span>
    <span class="token string">"文档检索"</span><span class="token punctuation">,</span>
    <span class="token string">"用于检索用户提出的问题，并基于检索到的文档内容进行回复."</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
tools <span class="token operator">=</span> <span class="token punctuation">[</span>tool<span class="token punctuation">]</span>

<span class="token comment"># 创建聊天消息历史记录</span>
msgs <span class="token operator">=</span> StreamlitChatMessageHistory<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 创建对话缓冲区内存</span>
memory <span class="token operator">=</span> ConversationBufferMemory<span class="token punctuation">(</span>
    chat_memory<span class="token operator">=</span>msgs<span class="token punctuation">,</span> return_messages<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> memory_key<span class="token operator">=</span><span class="token string">"chat_history"</span><span class="token punctuation">,</span> output_key<span class="token operator">=</span><span class="token string">"output"</span>
<span class="token punctuation">)</span>

<span class="token comment"># 指令模板</span>
instructions <span class="token operator">=</span> <span class="token triple-quoted-string string">"""您是一个设计用于查询文档来回答问题的代理。
您可以使用文档检索工具，并基于检索内容来回答问题
您可能不查询文档就知道答案，但是您仍然应该查询文档来获得答案。
如果您从文档中找不到任何信息用于回答问题，则只需返回“抱歉，这个问题我还不知道。”作为答案。
"""</span>

<span class="token comment"># 基础提示模板</span>
base_prompt_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
{instructions}

TOOLS:
------

You have access to the following tools:

{tools}

To use a tool, please use the following format:

‍```
Thought: Do I need to use a tool? Yes
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
‍```

When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:

‍```
Thought: Do I need to use a tool? No
Final Answer: [your response here]
‍```

Begin!

Previous conversation history:
{chat_history}

New input: {input}
{agent_scratchpad}"""</span>



<span class="token comment"># 创建基础提示模板</span>
base_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">.</span>from_template<span class="token punctuation">(</span>base_prompt_template<span class="token punctuation">)</span>

<span class="token comment"># 创建部分填充的提示模板</span>
prompt <span class="token operator">=</span> base_prompt<span class="token punctuation">.</span>partial<span class="token punctuation">(</span>instructions<span class="token operator">=</span>instructions<span class="token punctuation">)</span>

<span class="token comment"># 创建llm</span>
llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 创建react Agent</span>
agent <span class="token operator">=</span> create_react_agent<span class="token punctuation">(</span>llm<span class="token punctuation">,</span> tools<span class="token punctuation">,</span> prompt<span class="token punctuation">)</span>

<span class="token comment"># 创建Agent执行器</span>
agent_executor <span class="token operator">=</span> AgentExecutor<span class="token punctuation">(</span>agent<span class="token operator">=</span>agent<span class="token punctuation">,</span> tools<span class="token operator">=</span>tools<span class="token punctuation">,</span> memory<span class="token operator">=</span>memory<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> handle_parsing_errors<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 创建聊天输入框</span>
user_query <span class="token operator">=</span> st<span class="token punctuation">.</span>chat_input<span class="token punctuation">(</span>placeholder<span class="token operator">=</span><span class="token string">"请开始提问吧!"</span><span class="token punctuation">)</span>

<span class="token comment"># 如果有用户输入的查询</span>
<span class="token keyword">if</span> user_query<span class="token punctuation">:</span>
    <span class="token comment"># 添加用户消息到session_state</span>
    st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> user_query<span class="token punctuation">}</span><span class="token punctuation">)</span>
    <span class="token comment"># 显示用户消息</span>
    st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>user_query<span class="token punctuation">)</span>

    <span class="token keyword">with</span> st<span class="token punctuation">.</span>chat_message<span class="token punctuation">(</span><span class="token string">"assistant"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 创建Streamlit回调处理器</span>
        st_cb <span class="token operator">=</span> StreamlitCallbackHandler<span class="token punctuation">(</span>st<span class="token punctuation">.</span>container<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># agent执行过程日志回调显示在Streamlit Container (如思考、选择工具、执行查询、观察结果等)</span>
        config <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token string">"callbacks"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>st_cb<span class="token punctuation">]</span><span class="token punctuation">}</span>
        <span class="token comment"># 执行Agent并获取响应</span>
        response <span class="token operator">=</span> agent_executor<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"input"</span><span class="token punctuation">:</span> user_query<span class="token punctuation">}</span><span class="token punctuation">,</span> config<span class="token operator">=</span>config<span class="token punctuation">)</span>
        <span class="token comment"># 添加助手消息到session_state</span>
        st<span class="token punctuation">.</span>session_state<span class="token punctuation">.</span>messages<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> response<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span>
        <span class="token comment"># 显示助手响应</span>
        st<span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="font_stylecolorrgb51_51_51font_915">
     </a>
     实现效果
    </h5>
    <p>
     上传 threebody.txt 内容：
    </p>
    <pre><code class="prism language-plain">在《三体》系列中，执剑人是指掌握着地球与三体文明之间威慑平衡的关键人物。执剑人负责控制一种名为“引力波发射器”的装置，这个装置能够向宇宙广播地球和三体星系的坐标，从而触发宇宙中的黑暗森林法则，导致三体文明和地球文明同时被其他文明消灭。因此，执剑人实际上是维持地球与三体之间脆弱和平的关键。
第一任执剑人罗辑之所以让三体人感到害怕，是因为他通过与三体文明的接触，逐渐理解了三体文明的弱点和宇宙中的黑暗森林法则。罗辑利用这些知识，采取了一系列策略，使得三体文明无法轻易地消灭人类，甚至可能对人类文明构成威胁。
罗辑通过与三体文明的对话，发现了三体文明对地球的入侵计划，并利用自己的智慧和策略，成功地将地球文明置于一个相对安全的位置。他通过与三体文明的博弈，展示了人类的智慧和坚韧，使得三体文明意识到人类并非易于征服的对手。
此外，罗辑还揭示了宇宙中文明的生存状态，即所谓的“黑暗森林法则”，这个法则认为宇宙中的文明如同森林中的猎人，每个文明都在暗中观察其他文明，一旦发现潜在的威胁，就会先发制人。罗辑利用这一法则，使得三体文明不敢轻易对人类采取极端行动，因为他们担心会引来其他文明的注意和攻击。
因此，罗辑的智慧、策略以及对宇宙法则的理解，使得三体人对他感到害怕，不敢轻视人类的力量和潜力。
第二任执剑人是程心，程心在小说中接替了罗辑成为执剑人，负责控制引力波发射器，维持地球与三体文明之间的威慑平衡。当执剑人更换时，三体文明认为新执剑人可能不具备前任执剑人的威慑力和决断力，或者无法像前任那样坚定地执行威慑策略。这种不确定性使得三体文明认为发动攻击的风险降低了，因此他们决定抓住机会，发起了对地球的攻击。
</code></pre>
    <p>
     基于上述内容进行提问，效果如下：
    </p>
    <pre><code class="prism language-plain">用户：执剑人是什么角色
大模型：在《三体》系列中，执剑人是掌握着地球与三体文明之间威慑平衡的关键人物，负责控制引力波发射器，维持地球与三体之间的平衡。第一任执剑人是罗辑，第二任为程心。罗辑通过智慧和策略，成功维护地球文明。

用户：他通过什么办法威胁三体文明
大模型：罗辑通过与三体文明的对话和博弈，利用自己的智慧、坚韧和对宇宙法则的理解，成功威胁三体文明，使他们不敢轻易对地球采取极端行动。程心则在接替罗辑成为执剑人后，由于三体文明对新执剑人的不确定性，导致他们认为发动攻击的风险降低了，从而决定发起对地球的攻击。
</code></pre>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/07985a09074430e1bb2670f35cf607d3.png"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f6c6270303132333435362f:61727469636c652f64657461696c732f313436323236333038" class_="artid" style="display:none">
 </p>
</div>


