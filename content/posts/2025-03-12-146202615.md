---
layout: post
title: "数学基础线性代数1向量和矩阵初步"
date: 2025-03-12 13:33:40 +08:00
description: "向量-矩阵-范数-特征分解-奇异值分解-行列式"
keywords: "【数学基础】线性代数#1向量和矩阵初步"
categories: ['数学']
tags: ['线性代数', '矩阵']
artid: "146202615"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146202615
    alt: "数学基础线性代数1向量和矩阵初步"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146202615
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146202615
cover: https://bing.ee123.net/img/rand?artid=146202615
image: https://bing.ee123.net/img/rand?artid=146202615
img: https://bing.ee123.net/img/rand?artid=146202615
---

# 【数学基础】线性代数#1向量和矩阵初步
> 本系列内容介绍：
>
> ![](https://i-blog.csdnimg.cn/direct/ba19606df94042ca938baf77c617285c.png)
> 主要参考资料：
>
> 《深度学习》[美]伊恩·古德菲洛 等 著
>
> 《机器人数学基础》吴福朝 张铃 著
>
> 文章为自学笔记，仅供参考。
### 标量、向量、矩阵和张量
**标量**
**标量** 是一个单独的数。
**向量**
**向量** 是一列有序排列的数：
x = [ x 1 x 2 ⋮ x n ] \boldsymbol
x=\displaystyle\begin{bmatrix}x_1\\\x_2\\\\\vdots\\\x_n\end{bmatrix} x=
​x1​x2​⋮xn​​ ​
向量元素的个数为向量的维度。若 n n n维向量的每个元素都属于 R \mathbb{R} R，则该向量属于 R \mathbb{R} R的 n n
n次笛卡尔乘积构成的集合 R n \mathbb{R}^n Rn。
向量元素的下标为元素的索引。指定索引的集合 S = { 1 , 3 , 6 } S=\\{1,3,6\\} S={1,3,6}，则向量 x S
\boldsymbol x_S xS​包含集合中索引的元素。负号表示集合的补集中的索引，向量 x − 1 \boldsymbol x_{-1}
x−1​包含除了 x 1 x_1 x1​的所有元素。
**矩阵**
**矩阵** 是一个二维数组：
A = [ A 1 , 1 A 1 , 2 A 2 , 1 A 2 , 2 ] \boldsymbol
A=\displaystyle\begin{bmatrix}A_{1,1}&A_{1,2}\\\A_{2,1}&A_{2,2}\end{bmatrix}
A=[A1,1​A2,1​​A1,2​A2,2​​]
高度为 m m m、宽度为 n n n的实数矩阵属于 R m × n \mathbb R^{m\times n} Rm×n。
A i , j A_{i,j} Ai,j​表示第 i i i行第 j j j列的元素， A i , : \boldsymbol A_{i,:}
Ai,:​表示第 i i i行的所有元素， A : , i \boldsymbol A_{:,i} A:,i​表示第 i i i列的所有元素， f ( A
) i , j f(\boldsymbol A)_{i,j} f(A)i,j​表示函数 f f f作用在 A \boldsymbol A A的第 i i
i行第 j j j列的元素。
**张量**
**张量** 可以表示任意维度的数组。
张量 A \boldsymbol{\mathsf A} A中坐标为 ( i , j , k ) (i,j,k) (i,j,k)的元素记作 A ( i , j
, k ) \mathsf A_{(i,j,k)} A(i,j,k)​。
### 矩阵运算
**转置**
**转置** 操作将矩阵以主对角线为轴镜像，从左上角到右下角的对角线被称为主对角线。矩阵 A \boldsymbol A A的转置为 A T
\boldsymbol A^T AT。
( A T ) i , j = A j , i (\boldsymbol A^T)_{i,j}=A_{j,i} (AT)i,j​=Aj,i​
向量经转置可以表示在一行上：
x = [ x 1 , x 2 , x 3 ] T \boldsymbol x=[x_1,x_2,x_3]^T x=[x1​,x2​,x3​]T
**矩阵相加**
两个形状相同的矩阵相加是指对应位置的元素相加：
C = A \+ B \boldsymbol C=\boldsymbol A+\boldsymbol B C=A+B
C i , j = A i , j \+ B i , j C_{i,j}=A_{i,j}+B_{i,j} Ci,j​=Ai,j​+Bi,j​
**矩阵和标量**
标量和矩阵相乘或相加时，矩阵的每个元素都和标量相乘或相加：
D = a ⋅ B \+ c \boldsymbol D=a\cdot\boldsymbol B+c D=a⋅B+c
D i , j = a ⋅ B i , j \+ c D_{i,j}=a\cdot B_{i,j}+c Di,j​=a⋅Bi,j​+c
**矩阵和向量**
深度学习允许矩阵与向量相加，此时向量和矩阵的每一列相加：
C = A \+ b \boldsymbol C=\boldsymbol A+\boldsymbol b C=A+b
C i , j = A i , j \+ b j C_{i,j}=A_{i,j}+b_j Ci,j​=Ai,j​+bj​
这种隐式地复制向量到很多位置的方式称为**广播** 。
**矩阵相乘**
m × n m\times n m×n的矩阵 A \boldsymbol A A与 n × p n\times p n×p的矩阵 B \boldsymbol
B B的矩阵乘积为 m × p m\times p m×p的矩阵 C \boldsymbol C C：
C = A B \boldsymbol C=\boldsymbol{AB} C=AB
C i , j = ∑ k A i , k B k , j C_{i,j}=\displaystyle\sum_kA_{i,k}B_{k,j}
Ci,j​=k∑​Ai,k​Bk,j​
两个维数相同的向量 x \boldsymbol x x和 y \boldsymbol y y的**点积** 可看作矩阵乘积 x ⊤ y
\boldsymbol x^\top\boldsymbol y x⊤y。
矩阵乘积服从分配律和结合律，不服从交换律：
A ( B \+ C ) = A B \+ A C \boldsymbol A(\boldsymbol B+\boldsymbol
C)=\boldsymbol{AB}+\boldsymbol{AC} A(B+C)=AB+AC
A ( B C ) = ( A B ) C \boldsymbol
A(\boldsymbol{BC})=(\boldsymbol{AB})\boldsymbol C A(BC)=(AB)C
向量点积服从交换律：
x ⊤ y = y ⊤ x \boldsymbol x^\top\boldsymbol y=\boldsymbol y^\top\boldsymbol x
x⊤y=y⊤x
两个形状相同的矩阵中对应位置的元素乘积称为**元素对应乘积** 或者**Hadamard乘积** ：
C = A ⊙ B \boldsymbol C=\boldsymbol A\odot\boldsymbol B C=A⊙B
C i , j = A i , j B i , j C_{i,j}=A_{i,j}B_{i,j} Ci,j​=Ai,j​Bi,j​
矩阵乘积可以表示**线性方程组** ：
A x = b \boldsymbol{Ax}=\boldsymbol b Ax=b
### 单位矩阵和逆矩阵
**单位矩阵**
n n n阶**单位矩阵** I n \boldsymbol I_n In​沿主对角线的元素都是 1 1 1，其他元素都是 0 0 0：
I 3 = [ 1 0 0 0 1 0 0 0 1 ] \boldsymbol
I_3=\displaystyle\begin{bmatrix}1&0&0\\\0&1&0\\\0&0&1\end{bmatrix} I3​=
​100​010​001​ ​
任何向量和单位矩阵相乘都不变：
∀ x ∈ R n , I n x = x \forall\boldsymbol x\in\mathbb R^n,\boldsymbol
I_n\boldsymbol x=\boldsymbol x ∀x∈Rn,In​x=x
**逆矩阵**
矩阵 A \boldsymbol A A的**逆矩阵** 记作 A − 1 \boldsymbol A^{-1} A−1，满足如下条件：
A − 1 A = I n \boldsymbol A^{-1}\boldsymbol A=\boldsymbol I_n A−1A=In​
也可以定义逆矩阵右乘：
A A − 1 = I n \boldsymbol A\boldsymbol A^{-1}=\boldsymbol I_n AA−1=In​
这两个逆矩阵分别为左逆和右逆，方阵的左逆和右逆相等。
通过矩阵求逆可以求解上述线性方程组：
A x = b \boldsymbol{Ax}=\boldsymbol b Ax=b
x = A − 1 b \boldsymbol x=\boldsymbol A^{-1}\boldsymbol b x=A−1b
### 线性相关和生成子空间
一组向量的**线性组合** 是每个向量乘以对应标量系数之后的和：
∑ i c i v ( i ) \displaystyle\sum_ic_i\boldsymbol v^{(i)} i∑​ci​v(i)
一组向量的**生成子空间** 是该组向量的线性组合能指向的点的集合。
A x = b \boldsymbol{Ax}=\boldsymbol b Ax=b是否有解的问题可以转化为向量 b \boldsymbol b b是否在
A \boldsymbol A A列向量的生成子空间中，该生成子空间被称为 A \boldsymbol A A的**列空间** 或**值域** 。
如果一组向量中任意一个向量都不能表示成其他向量的线性组合，那么这组向量**线性无关** ，即 n n n个向量的线性组合可以覆盖整个 R n \mathbb
R^n Rn空间，反之则**线性相关** 。
所以 A x = b \boldsymbol{Ax}=\boldsymbol b Ax=b有唯一解的充要条件是 A \boldsymbol A
A为方阵，且所有列向量线性无关，否则线性方程组无法用矩阵逆求解。列向量线性相关的方阵被称为**奇异的** 。
### 范数
机器学习经常使用**范数** 衡量向量大小，形式上 L p L^p Lp范数的定义如下：
∣ ∣ x ∣ ∣ p = ( ∑ i ∣ x i ∣ p ) 1 p , p ∈ R , p ⩾ 1 \displaystyle||\boldsymbol
x||_p=\left(\sum_i|x_i|^p\right)^{\frac1p},p\in\mathbb R,p\geqslant1
∣∣x∣∣p​=(i∑​∣xi​∣p)p1​,p∈R,p⩾1
更严格地说，范数是满足下列性质的任意函数：
* f ( x ) = 0 ⇒ x = 0 f(\boldsymbol x)=0\Rightarrow\boldsymbol x=\boldsymbol0 f(x)=0⇒x=0；
* f ( x \+ y ) ⩽ f ( x ) \+ f ( y ) f(\boldsymbol x+\boldsymbol y)\leqslant f(\boldsymbol x)+f(\boldsymbol y) f(x+y)⩽f(x)+f(y)（三角不等式）；
* ∀ α ∈ R , f ( α x ) = ∣ α ∣ f ( x ) \forall\alpha\in\mathbb R,f(\alpha\boldsymbol x)=|\alpha|f(\boldsymbol x) ∀α∈R,f(αx)=∣α∣f(x)
当 p = 2 p=2 p=2时， L 2 L^2 L2范数称为**欧几里得范数**
，表示从原点出发到向量终点的欧几里得距离，在机器学习中使用频繁，常简记为 ∣ ∣ x ∣ ∣ ||x|| ∣∣x∣∣。平方 L 2 L^2
L2范数也很常用，通过点积 x ⊤ x \boldsymbol x^\top\boldsymbol x
x⊤x计算，更为简便，且对每个元素的导数导数只取决于对应的元素，而 L 2 L^2 L2范数对每个元素的导数和整个向量相关。但平方 L 2 L^2
L2范数在原点附近增长十分缓慢，不适合一些注重区分零元素和非零极小元素的机器学习应用，此时各个位置斜率相同、数学形式简单的 L 1 L^1
L1范数效果更好：
∣ ∣ x ∣ ∣ 1 = ∑ i ∣ x i ∣ ||\boldsymbol x||_1=\displaystyle\sum_i|x_i|
∣∣x∣∣1​=i∑​∣xi​∣
另一个机器学习经常出现的范数是**最大范数** L ∞ L^\infty L∞范数：
∣ ∣ x ∣ ∣ ∞ = max ⁡ i ∣ x i ∣ ||\boldsymbol x||_\infty=\underset{i}{\max}|x_i|
∣∣x∣∣∞​=imax​∣xi​∣
衡量矩阵大小最常见的做法是使用**Frobenius范数** ：
∣ ∣ A ∣ ∣ F = ∑ i , j A i , j 2 ||\boldsymbol
A||_F=\displaystyle\sqrt{\sum_{i,j}A^2_{i,j}} ∣∣A∣∣F​=i,j∑​Ai,j2​ ​
两个向量的点积可以用范数表示：
x ⊤ y = ∣ ∣ x ∣ ∣ 2 ∣ ∣ y ∣ ∣ 2 cos ⁡ θ \boldsymbol x^\top\boldsymbol
y=||\boldsymbol x||_2||\boldsymbol y||_2\cos\theta x⊤y=∣∣x∣∣2​∣∣y∣∣2​cosθ
其中 θ \theta θ为两个向量之间的夹角。
### 特殊类型的矩阵和向量
**对角矩阵** 只在主对角线上含有非零元素，其他位置都是零。 d i a g ( v ) \mathrm{diag}(\boldsymbol v)
diag(v)表示对角元素由向量 v \boldsymbol v
v中元素给定的对角矩阵。机器学习中将一些矩阵限制为对角矩阵可以得到计算代价较低的算法，因为对角矩阵的乘法和逆矩阵计算很高效：
d i a g ( v ) x = v ⊙ x \mathrm{diag}(\boldsymbol v)\boldsymbol x=\boldsymbol
v\odot\boldsymbol x diag(v)x=v⊙x
d i a g ( v ) − 1 = d i a g ( [ 1 / v 1 , ⋯ , 1 / v n ] ⊤ )
\mathrm{diag}(\boldsymbol v)^{-1}=\mathrm{diag}([1/v_1,\cdots,1/v_n]^\top)
diag(v)−1=diag([1/v1​,⋯,1/vn​]⊤)
**对称矩阵** 的转置和自己相等：
A = A ⊤ \boldsymbol A=\boldsymbol A^\top A=A⊤
A i , j = A j , i \boldsymbol A_{i,j}=\boldsymbol A_{j,i} Ai,j​=Aj,i​
当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常出现，例如距离度量。
**单位向量** 是具有**单位范数** 的向量：
∣ ∣ x ∣ ∣ 2 = 1 ||\boldsymbol x||_2=1 ∣∣x∣∣2​=1
如果 x ⊤ y = 0 \boldsymbol x^\top\boldsymbol y=0 x⊤y=0，则向量 x \boldsymbol x x和向量
y \boldsymbol y y互相**正交** ，若它们的范数都为 1 1 1，则又是**标准正交** 。在 R n \mathbb R^n
Rn中至多有 n n n个范数非零向量互相正交。
**正交矩阵** 是行向量和列向量分别标准正交的方阵：
A ⊤ A = A A ⊤ = I \boldsymbol A^\top\boldsymbol A=\boldsymbol A\boldsymbol
A^\top=\boldsymbol I A⊤A=AA⊤=I
因此正交矩阵的求逆计算代价小：
A − 1 = A ⊤ \boldsymbol A^{-1}=\boldsymbol A^\top A−1=A⊤
### 特征分解
正如分解质因数可以发现整数的一些内在性质，分解矩阵也能发现矩阵表示成数组元素时不明显的函数形式。
**特征分解** 将矩阵分解成一组特征向量和特征值，是使用最广的矩阵分解之一。
方阵 A \boldsymbol A A的**特征向量** 是指与 A \boldsymbol A A相乘后相当于对该向量进行缩放的非零向量 v
\boldsymbol v v：
A v = λ v \boldsymbol {Av}=\lambda\boldsymbol v Av=λv
其中标量 λ \lambda λ为这个特征向量对应的**特征值** 。
如果 v \boldsymbol v v是 A \boldsymbol A A的特征向量，那么任何缩放后的向量 s v ( s ∈ R , s ≠ 0 )
s\boldsymbol v(s\in\mathbb R,s\neq0) sv(s∈R,s=0)也是 A \boldsymbol A
A的特征向量，并都有相同的特征值，因此通常我们只考虑单位特征向量。
假设 A \boldsymbol A A有 n n n个线性无关的特征向量 { v ( 1 ) , ⋯ , v ( n ) } \\{\boldsymbol
v^{(1)},\cdots,\boldsymbol v^{(n)}\\} {v(1),⋯,v(n)}，对应着特征值 { λ 1 , ⋯ , λ n }
\\{\lambda_1,\cdots,\lambda_n\\} {λ1​,⋯,λn​}。将特征向量连接成一个矩阵 V = [ v ( 1 ) , ⋯ ,
v ( n ) ] V=[\boldsymbol v^{(1)},\cdots,\boldsymbol v^{(n)}]
V=[v(1),⋯,v(n)]，将特征值连接成一个向量 λ = [ λ 1 , ⋯ , λ n ]
\boldsymbol\lambda=[\lambda_1,\cdots,\lambda_n] λ=[λ1​,⋯,λn​]，那么 A A
A的**特征分解** 记作：
A = V d i a g ( λ ) V − 1 \boldsymbol A=\boldsymbol
V\mathrm{diag}(\boldsymbol\lambda)\boldsymbol V^{-1} A=Vdiag(λ)V−1
有的矩阵无法特征分解，有的矩阵的特征分解涉及复数，而每个实对称矩阵都能分解成实特征向量和实特征值：
A = Q Λ Q ⊤ \boldsymbol A=\boldsymbol Q\boldsymbol\Lambda\boldsymbol Q^\top
A=QΛQ⊤
其中 Q \boldsymbol Q Q是 A \boldsymbol A A的特征向量组成的正交矩阵， Λ \boldsymbol\Lambda
Λ是对角矩阵，特征值 Λ i , i \Lambda_{i,i} Λi,i​对应的特征向量是 Q \boldsymbol Q Q的第 i i i列。
实对称矩阵的特征分解可能并不唯一，如果两个或多个特征向量有相同的特征值，那么由这些向量产生的生成子空间中，任意一组正交向量都是该特征值的对应特征向量，可以等价地构成
Q \boldsymbol Q Q。我们通常按降序排列 Λ \boldsymbol\Lambda Λ的元素，该惯例下如果所有特征值唯一，那么特征分解唯一。
特征分解给出了很多关于矩阵的有用信息，例如矩阵奇异的充分条件是它含有零特征值。
实对称矩阵的特征分解可以用于优化二次方程 f ( x ) = x ⊤ A x f(\boldsymbol x)=\boldsymbol
x^\top\boldsymbol A\boldsymbol x f(x)=x⊤Ax，其中限制 ∣ ∣ x ∣ ∣ 2 = 1 ||\boldsymbol
x||_2=1 ∣∣x∣∣2​=1。当 x \boldsymbol x x是 A \boldsymbol A A的特征向量时， f f
f将返回对应的特征值。在限制条件下， f f f的最大值是最大特征值，最小值是最小特征值。
所有特征值都是正数的矩阵称为**正定** ，所有特征值都是非负数的矩阵称为**半正定** 。反之所有特征值都是负数的矩阵称为**负定**
，所有特征值都是非正数的矩阵称为**半负定** 。
半正定矩阵的意义在于：
∀ x , x ⊤ A x ⩾ 0 \forall\boldsymbol x,\boldsymbol x^\top\boldsymbol
A\boldsymbol x\geqslant0 ∀x,x⊤Ax⩾0
而对于正定矩阵则有：
x ⊤ A x = 0 ⇒ x = 0 \boldsymbol x^\top\boldsymbol A\boldsymbol
x=0\Rightarrow\boldsymbol x=\boldsymbol0 x⊤Ax=0⇒x=0
### 奇异值分解
**奇异值分解SVD** 将矩阵分解为**奇异向量** 和**奇异值** ，应用更为广泛。每个实数矩阵都有一个奇异值分解，不一定有特征分解。
奇异值分解为：
A = U D V ⊤ \boldsymbol A=\boldsymbol U\boldsymbol D\boldsymbol V^\top A=UDV⊤
对于 m × n m\times n m×n的矩阵 A \boldsymbol A A， U \boldsymbol U U是 m × m m\times
m m×m的矩阵， D \boldsymbol D D是 m × n m\times n m×n的矩阵， V \boldsymbol V V是 n × n
n\times n n×n的矩阵。 U \boldsymbol U U和 V \boldsymbol V V为正交矩阵， D \boldsymbol D
D为对角矩阵，但不一定为方阵。
D \boldsymbol D D对角线上的元素 A \boldsymbol A A的奇异值， U \boldsymbol U U的列向量称为左奇异向量，
V \boldsymbol V V的列向量称为右奇异向量。用特征分解来解释， A \boldsymbol A A的左奇异向量是 A A ⊤
\boldsymbol A\boldsymbol A^\top AA⊤的特征向量， A \boldsymbol A A的右奇异向量是 A ⊤ A
\boldsymbol A^\top\boldsymbol A A⊤A的特征向量。 A \boldsymbol A A的非零奇异值是 A ⊤ A
\boldsymbol A^\top\boldsymbol A A⊤A特征值的平方根，也是 A A ⊤ \boldsymbol A\boldsymbol
A^\top AA⊤特征值的平方根。
### Moore-Penrose伪逆
对于不可逆矩阵 A \boldsymbol A A，线性方程组 A x = y \boldsymbol {Ax}=\boldsymbol y
Ax=y无解或有无穷多解。**Moore-Penrose伪逆** 在该问题上取得了进展， A \boldsymbol A A的伪逆定义为：
A \+ = lim ⁡ α → 0 ( A ⊤ A \+ α I ) − 1 A ⊤ \boldsymbol
A^+=\displaystyle\lim_{\alpha\rightarrow0}(\boldsymbol A^\top\boldsymbol
A+\alpha\boldsymbol I)^{-1}\boldsymbol A^\top A+=α→0lim​(A⊤A+αI)−1A⊤
但计算伪逆实际上通过奇异值分解使用如下公式：
A \+ = V D \+ U ⊤ \boldsymbol A^+=\boldsymbol{VD}^+\boldsymbol U^\top A+=VD+U⊤
其中对角矩阵 D \boldsymbol D D的伪逆 D \+ \boldsymbol D^+ D+是其非零元素取倒数之后再转置得到的。
使用伪逆替代逆矩阵得到的 x = A \+ y \boldsymbol x=\boldsymbol A^+\boldsymbol y
x=A+y，在有无穷多解的情况下是所有解中 ∣ ∣ x ∣ ∣ 2 ||\boldsymbol x||_2 ∣∣x∣∣2​最小的一个，在无解的情况下使 ∣
∣ A x − y ∣ ∣ 2 ||\boldsymbol{Ax}-\boldsymbol y||_2 ∣∣Ax−y∣∣2​最小。
### 迹运算
**迹运算** 返回矩阵对角元素的和：
T r ( A ) = ∑ i A i , i \mathrm{Tr}(\boldsymbol
A)=\displaystyle\sum_i\boldsymbol A_{i,i} Tr(A)=i∑​Ai,i​
迹运算提供了另一种描述矩阵Frobenius范数的方式：
∣ ∣ A ∣ ∣ F = T r ( A A ⊤ ) ||\boldsymbol
A||_F=\displaystyle\sqrt{\mathrm{Tr}(\boldsymbol{AA}^\top)} ∣∣A∣∣F​=Tr(AA⊤) ​
迹运算在转置运算下不变：
T r ( A ) = T r ( A ⊤ ) \mathrm{Tr}(\boldsymbol A)=\mathrm{Tr}(\boldsymbol
A^\top) Tr(A)=Tr(A⊤)
多个矩阵相乘得到方阵的迹，在保证乘积定义良好的情况下，将这些矩阵中的最后一个挪到最前面之后迹不变：
T r ( ∏ i = 1 n F ( i ) ) = T r ( F ( n ) ∏ i = 1 n − 1 F ( i ) )
\mathrm{Tr}\left(\displaystyle\prod^n_{i=1}\boldsymbol
F^{(i)}\right)=\mathrm{Tr}\left(\displaystyle\boldsymbol
F^{(n)}\prod^{n-1}_{i=1}\boldsymbol F^{(i)}\right)
Tr(i=1∏n​F(i))=Tr(F(n)i=1∏n−1​F(i))
标量在迹运算后仍是它自己：
a = T r ( a ) a=\mathrm{Tr}(a) a=Tr(a)
### 行列式
**行列式** 是一个将方阵 A \boldsymbol A A映射到实数的函数，记作 d e t ( A )
\mathrm{det}(\boldsymbol A) det(A)，等于矩阵特征值的乘积。
行列式的绝对值可以用来衡量矩阵参与矩阵乘法后空间扩大或缩小了多少。行列式为0则空间失去所有体积，行列式为1则空间体积不变。