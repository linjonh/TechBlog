---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34343138343835322f:61727469636c652f64657461696c732f313436303830393835"
layout: post
title: "目标检测CVPR-2025DEIM具有改进匹配机制的DETR以实现快速收敛"
date: 2025-03-06 21:44:46 +0800
description: "我们介绍了DEIM，这是一种创新且高效的训练框架，旨在加速基于Transformer架构（DETR）的实时目标检测的收敛速度。为了缓解DETR模型中一对一（O2O）匹配固有的稀疏监督问题，DEIM采用了密集O2O匹配策略。该方法通过引入额外目标并使用标准数据增强技术，增加了每张图像的正样本数量。虽然密集O2O匹配加快了收敛速度，但也引入了大量低质量匹配，可能影响性能。为了解决这一问题，我们提出了可匹配性感知损失（MAL），这是一种新颖的损失函数，能够优化不同质量水平的匹配，从而增强密集O2O的有效性。"
keywords: "【目标检测】【CVPR 2025】DEIM：具有改进匹配机制的DETR以实现快速收敛"
categories: ['目标检测']
tags: ['计算机视觉', '目标检测', '人工智能']
artid: "146080985"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146080985
    alt: "目标检测CVPR-2025DEIM具有改进匹配机制的DETR以实现快速收敛"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146080985
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146080985
cover: https://bing.ee123.net/img/rand?artid=146080985
image: https://bing.ee123.net/img/rand?artid=146080985
img: https://bing.ee123.net/img/rand?artid=146080985
---

# 【目标检测】【CVPR 2025】DEIM：具有改进匹配机制的DETR以实现快速收敛

DEIM：DETR with Improved Matching for Fast Convergence
  
DEIM：具有改进匹配机制的DETR以实现快速收敛

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/020576199260479581516140861aca73.png)

## 0.论文摘要

我们介绍了DEIM，这是一种创新且高效的训练框架，旨在加速基于Transformer架构（DETR）的实时目标检测的收敛速度。为了缓解DETR模型中一对一（O2O）匹配固有的稀疏监督问题，DEIM采用了密集O2O匹配策略。该方法通过引入额外目标并使用标准数据增强技术，增加了每张图像的正样本数量。虽然密集O2O匹配加快了收敛速度，但也引入了大量低质量匹配，可能影响性能。为了解决这一问题，我们提出了可匹配性感知损失（MAL），这是一种新颖的损失函数，能够优化不同质量水平的匹配，从而增强密集O2O的有效性。在COCO数据集上的大量实验验证了DEIM的有效性。当与RT-DETR和D-FINE结合时，它持续提升了性能，同时将训练时间减少了50%。值得注意的是，与RT-DETRv2结合使用时，DEIM在NVIDIA 4090 GPU上仅用一天训练就达到了53.2%的AP。此外，DEIM训练的实时模型在NVIDIA T4 GPU上分别以124和78 FPS的速度实现了54.7%和56.5%的AP，超越了领先的实时目标检测器，且无需额外数据。我们相信DEIM为实时目标检测的进步设定了新的基准。

[论文链接](https://arxiv.org/abs/2412.04234)
  
[代码链接](https://github.com/ShihuaHuang95/DEIM)

## 1.引言

目标检测是计算机视觉中的一项基础任务，广泛应用于自动驾驶[5, 6]、机器人导航[9]等领域。对高效检测器的日益增长的需求推动了实时检测方法的发展。特别是，YOLO因其在延迟和准确性之间的出色权衡，成为实时目标检测的主要范式之一[1, 28, 32, 34, 44]。YOLO模型被广泛认为是基于卷积神经网络的一阶段检测器。YOLO系列中广泛使用了一对多（O2M）分配策略[1, 28, 34, 44]，其中每个目标框与多个锚点相关联。该策略被认为是有效的，因为它提供了密集的监督信号，加速了收敛并提升了性能[44]。然而，它也会为每个对象生成多个重叠的边界框，需要手工设计的非极大值抑制（NMS）来去除冗余，从而引入了延迟和不稳定性[32, 43]。

基于Transformer的检测（DETR）范式的出现[3]引起了广泛关注[4, 39, 46]，其利用多头注意力机制捕捉全局上下文，从而提升了定位和分类能力。DETR采用一对一（O2O）匹配策略，通过匈牙利算法[16]在训练过程中建立预测框与真实目标之间的唯一对应关系，消除了对非极大值抑制（NMS）的需求。这一端到端框架为实时目标检测提供了一个引人注目的替代方案。

为了解决DETR中监督信号不足的问题，近期研究通过将一对多（O2M）分配引入一对一（O2O）训练中，放宽了O2O匹配的限制，从而为每个目标引入辅助正样本以增加监督。Group DETR [4]通过使用多个查询组来实现这一点，每个组都有独立的O2O匹配，而Co-DETR [46]则结合了来自Faster R-CNN [29]和FCOS [31]等目标检测器的O2M方法。尽管这些方法成功增加了正样本的数量，但它们也需要额外的解码器，这增加了计算开销，并可能像传统检测器一样生成冗余的高质量预测。相比之下，我们提出了一种新颖且简单的方法，称为密集一对一（Dense O2O）匹配。我们的核心思想是增加每张训练图像中的目标数量，从而在训练过程中生成更多的正样本。值得注意的是，这可以通过经典技术如mosaic [1]和mixup [38]增强轻松实现，这些技术可以在保持一对一匹配框架的同时，为每张图像生成额外的正样本。Dense O2O匹配可以提供与O2M方法相当的监督水平，而无需O2M方法通常带来的复杂性和开销。

尽管通过使用先验信息改进了查询初始化[18, 39, 43, 45]，使得查询分布能够更有效地围绕对象展开，但这些改进的初始化方法通常依赖于从编码器中提取的有限特征信息[39, 43]，往往会导致查询集中在少数显著对象周围。相比之下，大多数非显著对象缺乏附近的查询，导致匹配质量较低。在使用密集一对一匹配（Dense O2O）时，这一问题变得更加明显。随着目标数量的增加，显著目标与非显著目标之间的差距扩大，尽管匹配数量整体增加，但低质量匹配也随之增多。在这种情况下，如果损失函数在处理这些低质量匹配时存在局限性，这种差距将持续存在，阻碍模型实现更好的性能。

现有的DETR损失函数[19, 40]，如Varifocal Loss (VFL) [40]，主要针对密集锚点设计，其中低质量匹配的数量相对较少。这些损失函数主要惩罚高质量匹配，尤其是具有高IoU但低置信度的匹配，并舍弃低质量匹配。为了解决低质量匹配问题并进一步提升Dense O2O，我们提出了匹配感知损失（Matchability-Aware Loss, MAL）。MAL通过结合匹配查询与目标之间的IoU和分类置信度，根据匹配性调整惩罚力度。对于高质量匹配，MAL的表现与VFL类似，但更加强调低质量匹配，从而在训练过程中提高有限正样本的利用率。此外，MAL的数学公式比VFL更为简洁。

提出的DEIM方法将Dense O2O与MAL相结合，构建了一个高效的训练框架。我们在COCO [20]数据集上进行了大量实验，以评估DEIM的有效性。图1(a)中的结果表明，DEIM显著加速了RTDETRv2 [24]和D-FINE [27]的收敛，并提升了性能。具体而言，在仅使用一半训练轮次的情况下，我们的方法分别比RT-DETRv2和D-FINE高出0.2和0.6 AP。此外，我们的方法使得基于ResNet50的DETR模型能够在单张4090 GPU上进行训练，并在一天内（约24个轮次）达到53.2%的mAP。通过引入更高效的模型，我们还提出了一系列新的实时检测器，这些检测器超越了现有模型，包括最新的YOLOv11 [13]，为实时目标检测设定了新的最先进水平（SoTA）（图1(b)）。

本工作的主要贡献总结如下：

• 我们提出了DEIM，一种简单且灵活的实时目标检测训练框架。

• DEIM通过分别使用Dense O2O和MAL提升匹配的数量和质量，从而加速了收敛。

• 通过我们的方法，现有的实时DETR在减少一半训练成本的同时实现了更好的性能。具体而言，我们的方法与D-FINE中的高效模型结合后，超越了YOLO，并在实时目标检测中建立了新的SoTA。

## 2.相关工作

### 基于Transformer的目标检测（DETR）

基于Transformer的目标检测（DETR）[3]标志着从传统CNN架构向Transformer的转变。通过使用匈牙利算法[16]进行一对一匹配的损失计算，DETR消除了对手工设计的非极大值抑制（NMS）作为后处理的需求，实现了端到端的目标检测。然而，它存在收敛速度慢和计算密集的问题。

### 增加阳性样本

一对一匹配将每个目标限制为单个正样本，提供的监督信息远少于一对多匹配，从而阻碍了优化。一些研究探索了在一对一框架内增加监督的方法。例如，Group DETR [4] 采用了“组”的概念来近似一对多匹配。它使用K组查询（其中K > 1），并在每组内独立执行一对一匹配。

这使得每个目标可以被分配K个正样本。然而，为了防止组间通信，每个组需要一个独立的解码器层，最终形成K个并行解码器。H-DETR [15]中的混合匹配方案与Group DETR的工作方式类似。Co-DETR [46]揭示了一对多分配方法有助于模型学习更具区分性的特征信息，因此它提出了一种协作混合分配方案，通过辅助头进行一对多标签分配来增强编码器表示，如Faster R-CNN [29]和FCOS [31]。现有方法旨在增加每个目标的正样本数量以增强监督。相比之下，我们的Dense O2O探索了另一个方向——增加每张训练图像中的目标数量，以有效提升监督。与现有方法不同，这些方法需要额外的解码器或头，从而增加训练资源消耗，而我们的方法无需额外计算。

### 优化低质量匹配

稀疏且随机初始化的查询与目标缺乏空间对齐，导致低质量匹配比例较高，阻碍了模型的收敛。已有多种方法在查询初始化中引入了先验知识，例如锚点查询[35]、DAB-DETR[21]、DN-DETR[18]以及密集区分查询[41]。最近，受两阶段范式[29, 45]的启发，DINO[39]和RT-DETR[43]等方法利用编码器密集输出的高排名预测来优化解码器查询[36]。这些策略使得查询初始化更接近目标区域，从而更加有效。然而，低质量匹配仍然是一个显著的挑战[22]。在RT-DETR[43]中，采用了Varifocal Loss（VFL）来减少分类置信度与框质量之间的不确定性，从而提升实时性能。然而，VFL主要针对传统检测器设计，这些检测器低质量匹配较少，且侧重于高IoU优化，导致低IoU匹配由于损失值较小且平坦而未能得到充分优化。基于这些先进的初始化方法，我们引入了一种匹配感知损失，以更好地优化不同质量水平的匹配，显著提升了密集O2O匹配的有效性。

### 降低计算成本

标准的注意力机制涉及密集计算。为了提高效率并促进与多尺度特征的交互，已经开发了几种先进的注意力机制，例如可变形注意力[45]、多尺度可变形注意力[42]、动态注意力[7]和级联窗口注意力[37]。此外，最近的研究集中在创建更高效的编码器上。例如，Lite DETR[17]引入了一种在高层次和低层次特征之间交替更新的编码器块，而RTDETR[43]在其编码器中结合了CNN和自注意力机制。这两种设计都显著减少了资源消耗，尤其是RT-DETR。RT-DETR是DETR框架中第一个实时目标检测模型。基于这种混合编码器，D-FINE[27]通过额外的模块进一步优化了RTDETR，并通过迭代更新概率分布而不是预测固定坐标来改进回归过程。这种方法使D-FINE在延迟和性能之间实现了更有利的权衡，略微超越了最近的YOLO模型。利用这些实时DETR的进展，我们的方法在减少训练成本的同时取得了令人印象深刻的性能，在实时目标检测中大幅超越了YOLO模型。

## 3.方法

### 3.1. 预备知识

#### O2M与O2O

O2M分配策略[10, 44]在传统目标检测器中被广泛采用，其监督机制可以表述如下：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4ab921f253dd4c1c971c9b74292668f2.png)

其中，

N
N





N
是目标的总数，

M
i
M\_i






M









i

​

是第

i
i





i
个目标的匹配数量，

y
^
i
j
\hat{y}\_{ij}













y





^

​











ij

​

表示第

i
i





i
个目标的第

j
j





j
个匹配，

y
i
y\_i






y









i

​

表示第

i
i





i
个真实标签，

f
f





f
是损失函数。O2M 通过增加

M
i
M\_i






M









i

​

来增强监督，即为每个目标分配多个查询（

M
i
>
1
M\_i > 1






M









i

​




>





1
），从而提供密集监督，如图 2a 所示。相比之下，O2O 分配仅将每个目标与单个最佳预测配对，该预测通过匈牙利算法确定，该算法最小化了平衡分类和定位误差的成本函数（图 2b）。O2O 可以视为 O2M 的一种特殊情况，其中所有目标的

M
i
=
1
M\_i = 1






M









i

​




=





1
。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1d638dc6c00e4f6f98187232cae72e32.png)

图2展示了我们提出的DEIM示意图。黄色、红色和绿色框分别代表GT、正样本和负样本。“pos.”表示正样本。上图：我们的Dense O2O（图2c）能够提供与O2M（图2a）相同质量的正样本。下图：对于低质量匹配，使用VFL [40]和MAL时的损失值用⋆标记，表明MAL能够更有效地优化这些情况。

#### 焦点损失

焦点损失（FL）[19] 的引入是为了防止训练过程中大量简单负样本淹没检测器，转而将注意力集中在少量困难样本上。它作为基于DETR的检测器[39, 45]中的默认分类损失，其定义如下：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/80a83e62945349c6b99d8a082205d4fc.png)

其中

y
∈
{
0
,
1
}
y ∈ \{0, 1\}





y



∈





{

0

,



1

}
表示真实类别，

p
∈
[
0
,
1
]
p ∈ [0, 1]





p



∈





[

0

,



1

]
代表前景类别的预测概率。参数

γ
γ





γ
控制简单样本与困难样本之间的平衡，而

α
α





α
调整前景类别与背景类别之间的权重。在 FL 中，仅考虑样本的类别和置信度，而未关注边界框的质量，即定位问题。

### 3.2. 提升匹配效率：密集O2O

一对一（O2O）匹配方案通常用于基于DETR的模型中，它将每个目标仅匹配到一个预测查询。这种方法通过匈牙利算法[16]实现，支持端到端训练，并消除了对NMS的需求。然而，O2O的一个关键局限性在于，与传统的一对多（O2M）方法（如SimOTA[44]）相比，它生成的阳性样本数量显著减少。这导致监督稀疏，可能会减缓训练期间的收敛速度。

为了更好地理解这一问题，我们在MS COCO数据集上使用ResNet50骨干网络训练了RTDETRv2 [24]，并比较了匈牙利算法（O2O）和SimOTA（O2M）策略生成的阳性匹配数量。如图3a所示，O2O在每张图像中生成的阳性匹配数量呈现一个尖锐的峰值，通常低于10个，而O2M生成的分布更为广泛，阳性匹配数量显著增加，有时单张图像的阳性样本数量甚至超过80个。图3b进一步表明，在极端情况下，SimOTA生成的匹配数量约为O2O的10倍。这表明O2O的阳性匹配数量较少，可能会减缓优化过程。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/23d16fccfd034d34b5a085f68422c0cd.png)

图3. 锚点/查询匹配对比。比较了在一个COCO周期中，使用一对多（SimOTA [44]）和一对一（匈牙利算法 [3]）匹配方案时，每张图像中匹配的锚点/查询数量。

我们提出Dense O2O作为一种高效的替代方案。该策略保留了O2O的一对一匹配结构（

M
i
=
1
M\_i = 1






M









i

​




=





1
），但增加了每张图像中的目标数量（N），从而实现更密集的监督。例如，如图2c所示，我们将原始图像复制为四个象限，并将它们组合成一张复合图像，同时保持原始图像的尺寸不变。这样，目标数量从1增加到4，提升了公式1中的监督水平，同时保持匹配结构不变。Dense O2O实现了与O2M相当的监督水平，但避免了额外的复杂性和计算开销。

### 3.3. 提升匹配质量：匹配感知损失

#### VFL的局限性

基于FL [19]构建的VariFocal Loss (VFL) [40]已被证明能够提升目标检测性能，尤其是在DETR模型 [2, 24, 43] 中。VFL损失的表达式为：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/394dd9692378496cb8699d98b80e4472.png)

其中，

q
q





q
表示预测边界框与其目标框之间的 IoU（交并比）。对于前景样本（

q
>
0
q > 0





q



>





0
），目标标签设置为

q
q





q
，而背景样本（

q
=
0
q = 0





q



=





0
）的目标标签为 0。VFL 通过引入 IoU 来提升 DETR [43] 中查询的质量。

然而，VFL在优化低质量匹配时存在两个关键限制：i) 低质量匹配。VFL主要关注高质量匹配（高IoU）。对于低质量匹配（低IoU），损失仍然较小，阻碍了模型对低质量框的预测进行优化。对于低质量匹配（如低IoU，例如图2d），损失仍然极小（在图2e中用⋆标记）。ii) 负样本。VFL将没有重叠的匹配视为负样本，这减少了正样本的数量，限制了有效训练。

#### 匹配感知损失

为了解决这些问题，我们提出了匹配感知损失（Matchability-Aware Loss, MAL），它在保留垂直联邦学习（VFL）优势的同时，弥补了其不足。MAL将匹配质量直接纳入损失函数，使其对低质量匹配更加敏感。MAL的公式如下：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/76fcec74f17f42a19ce48d7df7b5ecaf.png)

与VFL相比，我们引入了几处微小但重要的改动。具体而言，目标标签从

q
q





q
修改为

q
γ
q^γ






q









γ
，简化了正负样本的损失权重，并移除了用于平衡正负样本的超参数

α
α





α
。这一改动有助于避免对高质量框的过度强调，并改善了整体训练过程。这一点可以从VFL（图2e）和MAL（图2f）的损失曲线中明显看出。需要注意的是，

γ
γ





γ
的影响在第4.5节中进行了讨论。

#### 与VFL的对比

我们比较了MAL和VFL在处理低质量和高质量匹配时的表现。在低质量匹配的情况下（IoU = 0.05，如图4a所示），随着预测置信度的增加，MAL的损失呈现出更急剧的上升，而VFL的损失几乎保持不变。对于高质量匹配（IoU = 0.95，如图4b所示），MAL和VFL的表现相似，这证实了MAL在不影响高质量匹配性能的情况下提高了训练效率。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c921f8417c424f28b05b6b374d3af928.png)

图4. VFL与MAL对比。比较了VFL和我们的MAL在低质量（IoU = 0.05，图4a）和高质量（IoU = 0.95，图4b）匹配情况下的表现。

## 4.实验

### 4.1 训练细节

对于Dense O2O，我们应用了马赛克增强[1]和混合增强[38]来为每张图像生成额外的正样本。这些增强方法的影响在第4.5节中进行了讨论。我们在MS-COCO数据集[20]上使用AdamW优化器[23]训练模型。与RT-DETR[24, 43]和D-FINE[27]类似，我们使用了标准的数据增强方法，如颜色抖动和缩小。我们采用了平坦余弦学习率调度器[25]，并提出了一种新的数据增强调度器。在训练的前几个周期（通常为四个）中，我们使用了数据增强预热策略，以简化注意力学习。在训练周期的50%后禁用Dense O2O会带来更好的结果。遵循RT-DETRv2[43]，我们在最后两个周期中关闭数据增强。我们的学习率和数据增强调度器在图5中具体展示。我们的骨干网络在ImageNet1k[8]上进行了预训练。我们在分辨率为640×640的MS-COCO验证集上评估模型。关于超参数的更多细节在补充材料中提供。

### 4.2. 与实时检测器的比较

我们将我们的方法集成到D-FINE-L [27]和D-FINEX [27]中，构建了DEIM-D-FINE-L和DEIM-D-FINEX。随后，我们评估了这些模型，并将其实时目标检测性能与最先进的模型进行了基准测试，包括YOLOv8 [12]、YOLOv9 [34]、YOLOv10 [34]、YOLOv11 [13]，以及基于DETR的模型如RT-DETRv2 [24]和D-FINE [27]。表1比较了这些模型在训练轮次、参数量、GFLOPs、延迟和检测精度方面的表现。补充材料中还包含了较小模型变体（S和M）的额外比较。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/07425cdb0dfe4cd8863faf954acec782.png)

表1. 与COCO [20] val2017上的实时目标检测器的比较。通过将我们的方法集成到D-FINE-L [27]和D-FINE-X [27]中，我们构建了DEIM-D-FINE-L和DEIM-D-FINE-X。我们将我们的方法与基于YOLO和基于DETR的实时目标检测器进行了比较。⋆表示NMS的置信度阈值调整为0.01。

我们的方法在训练成本、推理延迟和检测精度方面均优于当前最先进的模型，为实时目标检测设立了新的基准。需要注意的是，D-FINE [27] 是一项最新工作，通过引入蒸馏和边界框优化技术，提升了RT-DETRv2 [24]的性能，成为领先的实时检测器。我们的DEIM进一步提升了D-FINE的性能，在训练成本降低30%的同时，实现了0.7 AP的提升，且未增加推理延迟。最显著的改进体现在小目标检测上，使用我们的方法训练的D-FINE-X [27] 作为DEIM-D-FINE-X，实现了1.5 AP的提升。

与YOLOv11-X [13]直接相比，我们的方法在这一最先进的YOLO模型上表现更优，实现了略高的性能（54.7 vs. 54.1 AP），并将推理时间减少了20%（8.07 ms vs. 10.74 ms）。尽管YOLOv10 [34]采用了混合的O2M和O2O分配策略，但我们的模型始终优于YOLOv10，证明了我们Dense O2O策略的有效性。

### 4.3. 与基于ResNet [14]的DETR的比较

大多数DETR研究使用ResNet [14]作为骨干网络，为了全面比较现有的DETR变体，我们也将我们的方法应用于RTDETRv2 [24]，这是一种最先进的DETR变体。结果总结在表2中。与原始DETR需要500个epoch才能有效训练不同，包括我们的方法在内的近期DETR变体在减少训练时间的同时提升了模型性能。我们的方法展示了最显著的改进，仅用36个epoch就超越了所有变体。具体而言，DEIM在RT-DETRv2 [24]上使用ResNet-50 [14]和ResNet-101 [14]骨干网络时，分别将训练时间减少了一半，并将AP提高了0.5和0.9。此外，在使用ResNet-50 [14]骨干网络时，它比DINO-DeformableDETR [39]高出2.7 AP。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/590ee888fdb44a658fbaa0cc81c2a030.png)

表2. 与基于ResNet的DETR在COCO [20] val2017上的对比。通过将我们的方法集成到ResNet50 [14]和ResNet101 [14]中，我们构建了DEIM-RT-DETRv2-R50和DEIM-RT-DETRv2-R101。我们将我们的方法与使用ResNet50 [14]或ResNet101 [14]作为骨干的竞争性DETR目标检测器进行了比较。

DEIM 在小目标检测方面也有显著提升。例如，在整体 AP 与 RT-DETRv2 [24] 相当的情况下，我们的 DEIM-RT-DETRv2-R50 在小目标上比 RT-DETRv2 高出 1.3 AP。在使用更大的 ResNet101 骨干网络时，这一改进更为明显，我们的 DEIM-RT-DETRv2-R101 在小目标上比 RT-DETRv2-R101 高出 2.1 AP。将训练扩展到 72 个周期进一步提升了整体性能，尤其是在 ResNet-50 骨干网络上，这表明较小的模型可以从额外的训练中受益。

### 4.4. 在CrowdHuman上的比较

CrowdHuman [30] 是一个专为评估密集人群场景中目标检测器性能而设计的基准数据集。我们按照官方仓库1中的配置，将D-FINE和我们提出的方法应用于CrowdHuman数据集。如表3所示，我们的方法（DFINE-L结合DEIM增强）相比D-FINE-L实现了1.5 AP的显著提升。特别是，我们的方法在小目标检测（APs）和高精度检测（AP75）上表现出显著的性能提升（超过3%的改进），展示了其在复杂场景中更准确检测目标的能力。此外，该实验进一步验证了我们的方法在不同数据集上的强大泛化能力，进一步证实了其鲁棒性。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/99b3606969b5448fa4677b9894bdf27e.png)

表3. D-FINE与我们的DEIM在CrowdHuman [30]上的对比。两者均训练了120个周期。

### 4.5 分析

在以下研究中，我们使用RT-DETRv2 [24]与ResNet50 [14]配对进行实验，并默认在MS-COCO val2017数据集上报告性能，除非另有说明。

#### 实现密集一对一的方法

我们探索了两种实现密集一对一（Dense O2O）的方法：mosaic [1] 和 mixup [38]。Mosaic 是一种数据增强技术，它将四张图像合成为一张，而 mixup 则以随机比例叠加两张图像。这两种方法都有效地增加了每张图像中的目标数量，从而增强了训练过程中的监督效果。

如表4所示，与未使用目标增强的训练相比，经过12个epoch后，mosaic和mixup都带来了显著的提升，这凸显了Dense O2O的有效性。此外，结合mosaic和mixup加速了模型的收敛，进一步强调了增强监督的优势。我们还跟踪了一个训练epoch中每张图像的正样本数量，结果如图6所示。与传统的O2O匹配相比，Dense O2O显著增加了正样本的数量。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a9133d327cd04aedbd2f9478d20e0613.png)

表4. 不同马赛克和混合增强策略组合的密集O2O方法对比。概率值表示在训练过程中每个小批次应用马赛克和混合增强的可能性。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c8b76a1cb26d43b7b3fb26f477328858.png)

图6. 训练一个周期内使用和不使用密集O2O的正样本数量。Base表示未使用密集O2O。

总体而言，Dense O2O通过增加每张图像中的目标数量来加强监督，从而加快模型收敛速度。Mosaic和mixup是简单且计算效率高的技术，能够实现这一目标，它们的有效性表明在训练期间探索其他方法来增加目标数量具有进一步潜力。

#### γ在MAL（公式4）中的影响

表5中的结果展示了不同γ值在24个训练周期后对MAL的影响。基于这些实验，我们经验性地将γ设置为1.5，因为它能带来最佳性能。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/88adac113aeb4885926d3b11a458182f.png)

#### 密集O2O和MAL的有效性

表6展示了两个核心组件的有效性：Dense O2O和MAL。Dense O2O显著加速了模型的收敛，仅需36个epoch即可达到与基线相似的性能，而原始模型则需要72个epoch。当与MAL结合时，我们的方法进一步提升了性能。这一改进主要得益于更好的框质量，这与我们优化低质量匹配以提升高质量框预测的目标一致。总体而言，Dense O2O和MAL在RT-DETRv2和DFINE上均持续带来性能提升，展示了它们的鲁棒性和泛化能力。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a831be6087a042d9bae4e5c9bc289b5e.png)

表6. 密集O2O和MAL的影响。我们使用RT-DETRv2-R50 [24]和D-FINE-L [27]进行了实验。

## 5.结论

在本文中，我们提出了DEIM方法，旨在通过改进匹配来加速基于DETR的实时目标检测器的收敛。DEIM集成了密集一对一（Dense O2O）匹配，增加了每张图像的正样本数量，并结合了MAL（一种新颖的损失函数），该损失函数旨在优化不同质量匹配，并特别增强低质量匹配。这种组合显著提高了训练效率，使DEIM在较少的训练周期内实现了优于YOLOv11等模型的性能。与RT-DETR和D-FINE等最先进的DETR模型相比，DEIM在检测精度和训练速度上表现出明显的优势，且不牺牲推理延迟。这些特性使DEIM成为实时应用中的高效解决方案，并具有进一步优化和应用于其他高性能检测任务的潜力。

## 6.补充材料

### 6. 实验设置

数据集和评估指标。我们在COCO [20]数据集上评估我们的方法，使用train2017训练DEIM，并在val2017上进行验证。报告了标准的COCO指标，包括AP（在IoU阈值从0.50到0.95，步长为0.05的情况下取平均值）、AP50、AP75，以及在不同物体尺度下的AP：APS、APM和APL。

实现细节。我们使用D-FINE [27]和RT-DETRv2 [24, 43]框架实现并验证了我们的方法。大多数超参数遵循其原始设置，具体差异详见表7和表8。受RTMDet [25]中FlatCosine学习率调度器的启发，我们提出了一种专为Dense O2O设计的新型数据增强调度器。DETR中的注意力机制对于提取准确的物体特征以进行定位和分类至关重要。然而，在没有归纳偏置的情况下从头学习注意力可能具有挑战性。为了缓解这一问题，我们引入了一种数据增强预热策略，称为DataAug Warmup，该策略通过在初始训练周期中禁用高级数据增强来简化学习过程。图5展示了60个训练周期中FlatCosine学习率调度器和我们提出的DataAug调度器的示例。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f33a5562fc7f442eb5fc69802c626ed6.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/efb1df9bf7264845ac33506a8013c575.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c6e2fba5c6774c0fb0b2315b0856e674.png)

图5. 我们提出的学习率和数据增强调度器新训练方案的图示示例。

### 7. 与轻量级YOLO检测器的比较

我们在表9中展示了与更轻量级实时模型（S和M尺寸）的对比结果。基于强大的实时检测器RT-DETRv2 [24]和D-FINE [27]，我们的DEIM模型在各方面均取得了显著提升。值得注意的是，在RT-DETRv2中，所有三种模型尺寸均显示出约1 AP的提升，其中DEIM-RT-DETRv2-M⋆实现了显著的1.3 AP增益。与其他方法相比，我们的方法达到了最新的最先进水平。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/375564987efc4e37836d44f464f922bb.png)

表9. 在COCO [20] val2017数据集上与S和M尺寸的实时目标检测器的比较。⋆表示NMS的置信度阈值调整为0.01。

### 8. 其他结果

#### 细微修改的有效性

我们对D-FINE-L和D-FINE-X进行了小幅修改，包括解冻Backbone中的BN层、采用FlatCosine学习率调度器，并将Decoder的激活函数替换为SiLU。经过36个epoch的训练后，我们观察到这些修改对D-FINE-L没有影响，但使D-FINE-X的AP提升了0.1（55.4 vs. 55.5）。此配置将作为我们实验的新基线。

#### 使用与不使用Dense O2O之间的阳性样本数量

在一个训练周期中，我们比较了使用和不使用Dense O2O时同一训练图像中正样本的数量，如图6所示。在引入Dense O2O后，正样本的数量显著增加。这进一步支持了我们的观点，即Dense O2O有效增强了监督效果。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/32888a42e0bf47f7a61f6c3b58529d83.png)

图6. 训练一个周期内使用和不使用密集O2O的正样本数量。Base表示未使用密集O2O。

### 9.可视化

我们在图7中展示了定性对比结果。这些结果表明，DEIM有效解决了D-FINE-L面临的两个关键问题：高置信度的重复预测和误报。例如，在顶部行中，一个风筝被错误地分配了四个高度重叠的边界框，每个框都具有高置信度分数。此外，如底部行所示，D-FINE-L将插座和壁挂物体误分类为时钟，同时未能检测到瓶子。通过在训练中引入DEIM，检测器成功解决了这些挑战。这一可视化结果突显了DEIM带来的显著进步，强调了其在提高检测准确性方面的潜力。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/54ed4123abcf4b82980d31cf9ea57bc7.png)

图7. D-FINE-L与DEIM的定性比较。在每对图像中，左侧来自D-FINE-L，右侧由DEIM-D-FINE-L预测（得分阈值=0.5）。

## 7.引用文献

* [1] Alexey Bochkovskiy, Chien-Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv, 2020. 1, 2, 6, 8
* [2] Zhi Cai, Songtao Liu, Guodong Wang, Zheng Ge, Xiangyu Zhang, and Di Huang. Align-detr: Improving detr with simple iou-aware bce loss. In BMVC, 2024. 5
* [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In ECCV, 2020. 2, 3, 4, 7
* [4] Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, Kun Yao, Haocheng Feng, Junyu Han, Errui Ding, Gang Zeng, and Jingdong Wang. Group detr: Fast detr training with group-wise one-to-many assignment. In ICCV, 2023. 2, 3
* [5] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monocular 3d object detection for autonomous driving. In CVPR, 2016. 1
* [6] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In CVPR, 2017. 1
* [7] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-toend object detection with dynamic attention. In ICCV, 2021. 4
* [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 6
* [9] Andreas Ess, Konrad Schindler, Bastian Leibe, and Luc Van Gool. Object detection and tracking for autonomous navigation in dynamic environments. The International Journal of Robotics Research, 2010. 1
* [10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R Scott, and Weilin Huang. Tood: Task-aligned one-stage object detection. In ICCV, 2021. 4
* [11] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fast convergence of detr with spatially modulated co-attention. In ICCV, 2021. 7
* [12] Jocher Glenn. Yolov8. https://docs.ultralytics. com/models/yolov8/, 2023. 6, 7, 2
* [13] Jocher Glenn. Yolo11. https://docs.ultralytics. com/models/yolo11/, 2024. 2, 6, 7
* [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6, 7
* [15] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. In CVPR, 2023. 3
* [16] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 1955. 2, 3, 4
* [17] Feng Li, Ailing Zeng, Shilong Liu, Hao Zhang, Hongyang Li, Lei Zhang, and Lionel M Ni. Lite detr: An interleaved multi-scale encoder for efficient detr. In CVPR, 2023. 4
* [18] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In CVPR, 2022. 2, 3, 7
* [19] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ́ar. Focal loss for dense object detection. In ICCV, 2017. 2, 4, 5
* [20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dolla ́r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 1, 2, 5, 6, 7, 8
* [21] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. In ICLR, 2022. 3, 7
* [22] Shilong Liu, Tianhe Ren, Jiayu Chen, Zhaoyang Zeng, Hao Zhang, Feng Li, Hongyang Li, Jun Huang, Hang Su, Jun Zhu, et al. Detection transformer with stable matching. In ICCV, 2023. 4
* [23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2017. 6
* [24] Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, and Yi Liu. Rt-detrv2: Improved baseline with bag-of-freebies for real-time detection transformer. arXiv, 2024. 2, 5, 6, 7, 8, 1
* [25] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen. Rtmdet: An empirical study of designing real-time object detectors. arXiv, 2022. 6, 1
* [26] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In ICCV, 2021. 7
* [27] Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu. D-fine: Redefine regression task in detrs as fine-grained distribution refinement. arXiv, 2024. 2, 4, 6, 7, 8, 1
* [28] J Redmon. You only look once: Unified, real-time object detection. In CVPR, 2016. 1
* [29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. TPAMI, 2016. 2, 3, 4
* [30] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. arXiv, 2018. 6, 8
* [31] Zhi Tian, Xiangxiang Chu, Xiaoming Wang, Xiaolin Wei, and Chunhua Shen. Fully convolutional one-stage 3d object detection on lidar range images. In NIPS, 2022. 2, 3
* [32] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time end-toend object detection. 2024. 1, 2, 7
* [33] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. NeurIPS, 2023. 7, 2
* [34] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using programmable gradient information. arXiv, 2024. 1, 6, 7, 2
* [35] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In AAAI, 2022. 3, 7
* [36] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient detr: improving end-to-end object detector with dense prior. arXiv, 2021. 4, 7
* [37] Mingqiao Ye, Lei Ke, Siyuan Li, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Cascade-detr: delving into high-quality universal object detection. In ICCV, 2023. 4
* [38] Hongyi Zhang. mixup: Beyond empirical risk minimization. In ICLR, 2017. 2, 6, 8
* [39] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. In ICLR, 2023. 2, 4, 6, 7
* [40] Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sunderhauf. Varifocalnet: An iou-aware dense object detector. In CVPR, 2021. 2, 3, 5
* [41] Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping Luo, and Kai Chen. Dense distinct query for end-to-end object detection. In CVPR, 2023. 3
* [42] Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, and Jingdong Wang. Ms-detr: Efficient detr training with mixed supervision. In CVPR, 2024. 4
* [43] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In CVPR, 2024. 2, 4, 5, 6, 7, 1
* [44] Ge Zheng, Liu Songtao, Wang Feng, Li Zeming, and Sun Jian. Yolox: Exceeding yolo series in 2021. arXiv, 2021. 1, 2, 4, 5
* [45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021. 2, 4, 7
* [46] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with collaborative hybrid assignments training. In ICCV, 2023. 2, 3