---
arturl_encode: "68747470733a2f2f626c:6f672e6373646e2e6e65742f6c697275697169616e6730352f:61727469636c652f64657461696c732f313436303437323336"
layout: post
title: "前馈神经网络-参数学习梯度下降法-二分类任务"
date: 2025-03-05 17:33:33 +0800
description: "梯度下降法是训练前馈神经网络最常用的优化方法之一，其基本思想是：“根据当前参数下的损失函数的梯度信息，沿着使损失减小的方向更新参数，直到损失函数收敛到较低值（或满足停止条件）为止。”"
keywords: "初始策略参数在任务上如何计算的损失"
categories: ['Ai']
tags: ['神经网络', '机器学习', '学习', '分类', '人工智能']
artid: "146047236"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146047236
    alt: "前馈神经网络-参数学习梯度下降法-二分类任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146047236
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146047236
cover: https://bing.ee123.net/img/rand?artid=146047236
image: https://bing.ee123.net/img/rand?artid=146047236
img: https://bing.ee123.net/img/rand?artid=146047236
---

# 前馈神经网络 - 参数学习（梯度下降法 - 二分类任务）

梯度下降法是训练前馈神经网络最常用的优化方法之一，其基本思想是：“根据当前参数下的损失函数的梯度信息，沿着使损失减小的方向更新参数，直到损失函数收敛到较低值（或满足停止条件）为止。”

本文中，基于二分类任务，我们来学习使用梯度下降法训练前馈神经网络。

以下通过一个具体的二分类任务，逐步展示梯度下降法如何调整神经网络参数。假设网络结构如下：

* **输入层**
  ：2个神经元（特征 x1​,x2​）
* **隐藏层**
  ：3个神经元（使用 ReLU 激活函数）
* **输出层**
  ：1个神经元（使用 Sigmoid 激活函数）

#### **1. 初始化参数**

假设初始参数为：

* **输入层到隐藏层权重**
  W1​ 和偏置 b1​:

  ![](https://i-blog.csdnimg.cn/direct/620fa112a67b480685e15f916b40b41e.png)
* **隐藏层到输出层权重**
  W2​ 和偏置 b2:

  ![](https://i-blog.csdnimg.cn/direct/8038e0446bef456c85780599f3cd82e0.png)

#### **2. 前向传播**

输入样本 x=[2,3]，真实标签 y=1。

**步骤 2.1：计算隐藏层输入**

![](https://i-blog.csdnimg.cn/direct/a7f51d0bad0e4379a3823bb0ae12dccf.png)

**步骤 2.2：隐藏层激活（ReLU）**

![](https://i-blog.csdnimg.cn/direct/dab8f18a9d2546338d2aa2f733787ba8.png)

**步骤 2.3：计算输出层输入**

![](https://i-blog.csdnimg.cn/direct/d82b21cd166e42dfab16fb36bd072413.png)

**步骤 2.4：输出层激活（Sigmoid）**

![](https://i-blog.csdnimg.cn/direct/3ef9b75044ba4f42b67be23b3259367b.png)

#### **3. 计算损失**

使用交叉熵损失函数：

![](https://i-blog.csdnimg.cn/direct/a1a8d70068ec48db861055ec0291872b.png)

#### **4. 反向传播（计算梯度）**

目标：计算损失对每个参数（W1,b1,W2,b2）的梯度。

##### **步骤 4.1：输出层梯度**

* **损失对 z2​ 的导数**
  ：

  ![](https://i-blog.csdnimg.cn/direct/52370ac242154bafbadf384c83b0cdbb.png)
* **权重 W2​ 的梯度**
  ：

  ![](https://i-blog.csdnimg.cn/direct/1a92e54a55f64a809349e685d18cf5da.png)
* **偏置 b2​ 的梯度**
  ：

  ![](https://i-blog.csdnimg.cn/direct/0fc8fefff62a45b89076390ed5fdad1d.png)

##### **步骤 4.2：隐藏层梯度**

* **损失对 a1​ 的导数**
  ：

  ![](https://i-blog.csdnimg.cn/direct/23aefe911a4f405b8e23b7eeab6b4424.png)
* **损失对 z1​ 的导数**
  （ReLU导数为1，因为 z1​>0）：

  ![](https://i-blog.csdnimg.cn/direct/cf50c3b6dda44c9e8182d8f3e3609f82.png)
* **权重 W1​ 的梯度**
  ：

  ![](https://i-blog.csdnimg.cn/direct/8522d60dcf974eec8b5b0a2d4d63b53b.png)
* **偏置 b1​ 的梯度**
  ：

  ![](https://i-blog.csdnimg.cn/direct/6033ad1f9f674c11a091d5c1a394fbaa.png)
  ​​

#### **5. 参数更新（梯度下降法）**

假设学习率 η=0.1，更新公式：

![](https://i-blog.csdnimg.cn/direct/610e069e1e2944cc8c4e1b5b1d4b3452.png)

* **更新 W2**
  ：

  ![](https://i-blog.csdnimg.cn/direct/a0d734b9081f482ab03210118cc24edd.png)
* **更新 b2​**
  ：

  ![](https://i-blog.csdnimg.cn/direct/0dfa0ae9dc454a83bf098dad1957f4bf.png)
* **更新 W1​**
  ：

  ![](https://i-blog.csdnimg.cn/direct/5638f7d5de454603943cf5395071ec17.png)
* **更新 b1​**
  ：

  ![](https://i-blog.csdnimg.cn/direct/7fb4c51906d64ee3a444f46a0facb4b7.png)

#### **6. 迭代优化**

重复以下步骤直至损失收敛：

1. **前向传播**
   ：计算当前参数下的预测值。
2. **损失计算**
   ：评估预测值与真实值的差距。
3. **反向传播**
   ：计算所有参数的梯度。
4. **参数更新**
   ：沿梯度负方向调整参数。

#### **关键点总结**

1. **梯度计算**
   ：通过链式法则从输出层反向传播至输入层。
2. **矩阵维度对齐**
   ：确保梯度矩阵与参数矩阵形状一致（如 ∂L∂W1∂W1​∂L​ 应与 W1W1​ 同维度）。
3. **学习率选择**
   ：过大的学习率会导致震荡，过小则收敛缓慢。
4. **批量训练**
   ：实际应用中通常使用小批量样本（而非单个样本）计算梯度，以提高稳定性。

通过反复迭代，梯度下降法逐步调整参数，使神经网络的预测结果逼近真实值，最终完成模型训练。