---
layout: post
title: "在16卡服务器上使用最新版的CUDA和驱动训练llama-2-7b和llama-2-70b模型,并生成训练指标数据"
date: 2025-03-07 23:41:27 +0800
description: "要在16卡服务器上使用最新版的CUDA和驱动训练。"
keywords: "在16卡服务器上使用最新版的CUDA和驱动训练`llama - 2 - 7b`和`llama - 2 - 70b`模型，并生成训练指标数据"
categories: ['算法', '神经网络', '深度学习']
tags: ['运维', '服务器', 'Llama']
artid: "146108185"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146108185
    alt: "在16卡服务器上使用最新版的CUDA和驱动训练llama-2-7b和llama-2-70b模型,并生成训练指标数据"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146108185
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146108185
cover: https://bing.ee123.net/img/rand?artid=146108185
image: https://bing.ee123.net/img/rand?artid=146108185
img: https://bing.ee123.net/img/rand?artid=146108185
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     在16卡服务器上使用最新版的CUDA和驱动训练`llama - 2 - 7b`和`llama - 2 - 70b`模型，并生成训练指标数据
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     要在16卡服务器上使用最新版的CUDA和驱动训练
     <code>
      llama - 2 - 7b
     </code>
     和
     <code>
      llama - 2 - 70b
     </code>
     模型，并生成训练指标数据，你可以按照以下步骤进行：
    </p>
    <h4>
     <a id="1__2">
     </a>
     1. 环境准备
    </h4>
    <p>
     确保你的服务器已经安装了最新版的CUDA和驱动，并且安装了必要的Python库，如
     <code>
      torch
     </code>
     、
     <code>
      transformers
     </code>
     、
     <code>
      datasets
     </code>
     等。可以使用以下命令安装：
    </p>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> torch transformers datasets accelerate deepspeed
</code></pre>
    <h4>
     <a id="2__8">
     </a>
     2. 代码实现
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> <span class="token punctuation">(</span>
    AutoModelForCausalLM<span class="token punctuation">,</span>
    AutoTokenizer<span class="token punctuation">,</span>
    TrainingArguments<span class="token punctuation">,</span>
    Trainer<span class="token punctuation">,</span>
    default_data_collator
<span class="token punctuation">)</span>
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">import</span> time

<span class="token comment"># 定义模型名称</span>
model_names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"meta-llama/Llama-2-7b-hf"</span><span class="token punctuation">,</span> <span class="token string">"meta-llama/Llama-2-70b-hf"</span><span class="token punctuation">]</span>

<span class="token comment"># 加载数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"wikitext"</span><span class="token punctuation">,</span> <span class="token string">"wikitext-2-raw-v1"</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> model_name <span class="token keyword">in</span> model_names<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Training </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_name<span class="token punctuation">}</span></span><span class="token string">..."</span></span><span class="token punctuation">)</span>

    <span class="token comment"># 加载模型和分词器</span>
    tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
    tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token
    model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">,</span> torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float16<span class="token punctuation">)</span>

    <span class="token comment"># 预处理数据集</span>
    <span class="token keyword">def</span> <span class="token function">preprocess_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
        inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>examples<span class="token punctuation">[</span><span class="token string">"text"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token string">"max_length"</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> inputs

    tokenized_dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>preprocess_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

    <span class="token comment"># 定义训练参数</span>
    training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
        output_dir<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"./results/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_name<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span>
        num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
        per_device_train_batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
        gradient_accumulation_steps<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
        fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        logging_steps<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
        save_steps<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>
        evaluation_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>
        eval_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
        warmup_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span>
        weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
        logging_dir<span class="token operator">=</span><span class="token string-interpolation"><span class="token string">f"./logs/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_name<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">,</span>
        deepspeed<span class="token operator">=</span><span class="token string">"ds_config.json"</span>  <span class="token comment"># 使用DeepSpeed进行分布式训练</span>
    <span class="token punctuation">)</span>

    <span class="token comment"># 定义Trainer</span>
    trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
        model<span class="token operator">=</span>model<span class="token punctuation">,</span>
        args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
        train_dataset<span class="token operator">=</span>tokenized_dataset<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        eval_dataset<span class="token operator">=</span>tokenized_dataset<span class="token punctuation">[</span><span class="token string">"validation"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        data_collator<span class="token operator">=</span>default_data_collator<span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    <span class="token comment"># 开始训练并记录时间</span>
    start_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>
    trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    end_time <span class="token operator">=</span> time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># 计算训练指标</span>
    total_steps <span class="token operator">=</span> trainer<span class="token punctuation">.</span>state<span class="token punctuation">.</span>global_step
    total_time <span class="token operator">=</span> end_time <span class="token operator">-</span> start_time
    throughput <span class="token operator">=</span> total_steps <span class="token operator">/</span> total_time

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Model: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model_name<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Total steps: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>total_steps<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Total time (s): </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>total_time<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Throughput (steps/s): </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>throughput<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>


</code></pre>
    <h4>
     <a id="3_DeepSpeedds_configjson_88">
     </a>
     3. DeepSpeed配置文件（
     <code>
      ds_config.json
     </code>
     ）
    </h4>
    <pre><code class="prism language-json"><span class="token punctuation">{<!-- --></span>
    <span class="token string-property property">"train_batch_size"</span><span class="token operator">:</span> <span class="token number">64</span><span class="token punctuation">,</span>
    <span class="token string-property property">"optimizer"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"type"</span><span class="token operator">:</span> <span class="token string">"Adam"</span><span class="token punctuation">,</span>
        <span class="token string-property property">"params"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string-property property">"lr"</span><span class="token operator">:</span> <span class="token number">0.0001</span><span class="token punctuation">,</span>
            <span class="token string-property property">"betas"</span><span class="token operator">:</span> <span class="token punctuation">[</span>
                <span class="token number">0.9</span><span class="token punctuation">,</span>
                <span class="token number">0.999</span>
            <span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token string-property property">"eps"</span><span class="token operator">:</span> <span class="token number">1e-8</span><span class="token punctuation">,</span>
            <span class="token string-property property">"weight_decay"</span><span class="token operator">:</span> <span class="token number">0.01</span>
        <span class="token punctuation">}</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string-property property">"fp16"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"enabled"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
        <span class="token string-property property">"loss_scale"</span><span class="token operator">:</span> <span class="token number">0</span><span class="token punctuation">,</span>
        <span class="token string-property property">"initial_scale_power"</span><span class="token operator">:</span> <span class="token number">16</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token string-property property">"zero_optimization"</span><span class="token operator">:</span> <span class="token punctuation">{<!-- --></span>
        <span class="token string-property property">"stage"</span><span class="token operator">:</span> <span class="token number">2</span><span class="token punctuation">,</span>
        <span class="token string-property property">"allgather_partitions"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
        <span class="token string-property property">"allgather_bucket_size"</span><span class="token operator">:</span> <span class="token number">2e8</span><span class="token punctuation">,</span>
        <span class="token string-property property">"overlap_comm"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
        <span class="token string-property property">"reduce_scatter"</span><span class="token operator">:</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
        <span class="token string-property property">"reduce_bucket_size"</span><span class="token operator">:</span> <span class="token number">2e8</span><span class="token punctuation">,</span>
        <span class="token string-property property">"contiguous_gradients"</span><span class="token operator">:</span> <span class="token boolean">true</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
    <h4>
     <a id="4__121">
     </a>
     4. 运行代码
    </h4>
    <p>
     将上述代码保存为
     <code>
      train_llama.py
     </code>
     ，并在终端中运行：
    </p>
    <pre><code class="prism language-bash">deepspeed <span class="token parameter variable">--num_gpus</span> <span class="token number">16</span> train_llama.py
</code></pre>
    <h4>
     <a id="_127">
     </a>
     注意事项
    </h4>
    <ul>
     <li>
      <strong>
       模型权限
      </strong>
      ：
      <code>
       Llama - 2
      </code>
      系列模型需要在Hugging Face上申请访问权限，确保你已经获得了相应的权限。
     </li>
     <li>
      <strong>
       硬件资源
      </strong>
      ：
      <code>
       llama - 2 - 70b
      </code>
      模型非常大，需要足够的显存和内存资源。确保你的服务器能够支持该模型的训练。
     </li>
     <li>
      <strong>
       数据处理
      </strong>
      ：这里使用的是
      <code>
       wikitext - 2 - raw - v1
      </code>
      数据集，你可以根据需要替换为自己的数据集。
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6875616e67686d38382f:61727469636c652f64657461696c732f313436313038313835" class_="artid" style="display:none">
 </p>
</div>


