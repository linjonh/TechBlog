---
layout: post
title: "文献阅读On-Device-Language-Models-A-Comprehensive-Review"
date: 2025-03-06 22:01:42 +0800
description: "arXiv:2409.00088v2 [cs.CL] 2024年9月14日大语言模型（LLMs）的出现彻底改变了自然语言处理应用，在边缘设备上运行大语言模型因具有减少延迟、数据本地化以及提供个性化用户体验等优势而越来越具有吸引力。本全面综述探讨了在资源受限的设备上部署计算成本高昂的大语言模型所面临的挑战，并探索了多个领域的创新解决方案。本文研究了设备端语言模型的发展、其高效架构（包括参数共享和模块化设计），以及最先进的压缩技术，如量化、剪枝和知识蒸馏。分析了硬件加速策略和边缘 - 云协同部署方法，强调了性能"
keywords: "【文献阅读】On-Device Language Models: A Comprehensive Review"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "146063259"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146063259
    alt: "文献阅读On-Device-Language-Models-A-Comprehensive-Review"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146063259
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146063259
cover: https://bing.ee123.net/img/rand?artid=146063259
image: https://bing.ee123.net/img/rand?artid=146063259
img: https://bing.ee123.net/img/rand?artid=146063259
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【文献阅读】On-Device Language Models: A Comprehensive Review
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p style="text-align:justify">
     <span style="color:#0d0016">
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      arXiv:2409.00088v2 [cs.CL] 2024年9月14日
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      摘要
     </span>
    </h2>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      大语言模型（LLMs）的出现彻底改变了自然语言处理应用，在边缘设备上运行大语言模型因具有减少延迟、数据本地化以及提供个性化用户体验等优势而越来越具有吸引力。本全面综述探讨了在资源受限的设备上部署计算成本高昂的大语言模型所面临的挑战，并探索了多个领域的创新解决方案。本文研究了设备端语言模型的发展、其高效架构（包括参数共享和模块化设计），以及最先进的压缩技术，如量化、剪枝和知识蒸馏。分析了硬件加速策略和边缘 - 云协同部署方法，强调了性能与资源利用之间的复杂平衡。主要移动制造商的设备端语言模型案例研究展示了实际应用和潜在优势。该综述还涉及自适应学习、多模态能力和个性化等关键方面。通过确定关键研究方向和开放挑战，本文为设备端语言模型的未来发展提供了路线图，强调需要跨学科努力，以充分实现无处不在的智能计算的潜力，同时确保负责任和符合道德的部署。有关设备端大语言模型（LLMs）的研究工作和教育资源的全面综述，请访问https://github.com/NexaAI/Awesome-LLMs-on-device。要下载并运行设备端大语言模型，请访问https://www.nexaai.com/models。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      1 引言
     </span>
    </h2>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      大语言模型（LLMs）的出现推动了自然语言处理（NLP）应用的变革性转变。借助Transformer架构（Vaswani等人，2017年），诸如OpenAI的GPT系列（Radford等人，2019年；Brown等人，2020年；Achiam等人，2023年）和Meta的LLaMA系列（Touvron等人，2023a；2023b；Meta，2024年；Dubey等人，2024年）等大语言模型在理解和生成类似人类的文本方面展现出了无与伦比的熟练度，深刻影响了从自动客户支持到高级内容创作等诸多领域。这些模型能够无缝执行各种自然语言处理任务，使其成为现代人工智能驱动应用的支柱（Wu等人，2023b；Ge等人，2024年；Nam等人，2024年；Zheng等人，2024a；Yang等人，2024b）。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      然而，传统的大语言模型主要部署在云服务器上，这带来了一些挑战，尤其是在延迟、安全性以及对持续互联网连接的需求方面。这些问题促使人们对在边缘设备上部署大语言模型的兴趣日益浓厚，这种转变有望减少响应时间，并在智能手机、汽车系统和个人可穿戴设备等用户设备上直接提供个性化的用户体验。这种范式转变不仅符合用户对即时和个性化帮助日益增长的需求，还降低了与云计算相关的带宽和能源成本。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      <img alt="" height="784" src="https://i-blog.csdnimg.cn/direct/5a5fef8896064fe385b62498b1039b9c.png" width="1052"/>
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图1 2022年至2032年按终端用户划分的全球
      <strong>
       设备端边缘人工智能市场规模
      </strong>
      （单位：十亿美元）。市场将以25.9%的复合年增长率增长。预计2032年的市场规模为1436亿美元（Market.us，2024年）
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      对设备端人工智能部署日益增长的兴趣反映在迅速扩张的边缘人工智能市场上。如图1所示，从2022年到2032年，边缘人工智能市场预计将在各个行业实现大幅增长。市场规模预计将从2022年的152亿美元增加到2032年的1436亿美元，十年间增长近十倍（Market.us，2024年）。这种增长涵盖多个行业，制造业、汽车业和政府部门贡献显著。预计的市场扩张凸显了对边缘人工智能解决方案（包括设备端语言模型）日益增长的需求，这是由各种应用对更快、更私密和更高效的人工智能能力的需求所驱动的。这一市场趋势与向更本地化的人工智能处理的技术发展方向一致，进一步强调了开发高效的设备端大语言模型解决方案的重要性。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      尽管具有显著优势，但在边缘设备的限制条件下集成计算密集型的语言模型仍面临重大挑战。主要障碍包括计算能力有限、内存容量较小以及能源限制，这些因素共同使得直接采用基于云的大语言模型架构变得复杂。例如，如果不显著牺牲模型性能和能源效率，在智能手机上运行一个拥有4050亿参数的最先进模型（Dubey等人，2024年）是不可行的。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      <img alt="" height="1726" src="https://i-blog.csdnimg.cn/direct/162b02a955804e809d8a0de04f73c478.png" width="1178"/>
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图2 本文架构
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      本综述论文全面探讨了当前在
      <strong>
       边缘设备上部署大语言模型的策略和进展
      </strong>
      。我们旨在批判性地分析为使大语言模型适应边缘计算的限制而开发的各种技术和架构。这包括详细研究模型压缩技术、节能计算策略以及新型轻量级模型架构的开发。此外，本文还将深入探讨能够在边缘场景中有效使用大语言模型的部署策略，突出关键的行业应用和由此带来的好处。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      通过本次综述，我们旨在阐明从基于云的语言模型过渡到设备端语言模型的路径和挑战，深入了解这种转变如何重新定义应用领域和人工智能的可及性。本文的结构如图2所示。我们首先在第2节中探索基础和预备知识，包括设备端大语言模型的发展、架构基础和设备端训练技术。第3节深入探讨设备端语言模型的高效架构，讨论创新设计原则、模型压缩和协同方法。第4节继续深入研究模型压缩和优化技术，涵盖量化、剪枝、知识蒸馏和低秩分解。第5节研究硬件加速和部署策略，突出流行的设备端大语言模型框架和特定硬件的优化。为了将这些进展置于实际背景中，在第6节中，我们展示了现有的设备端语言模型的示例及其在各个领域的实际应用。最后，第7节讨论了该领域的未来方向和开放挑战，第8节对我们的综述进行总结。通过关注大语言模型能力与边缘计算需求的交叉点，本文为人工智能研究的持续讨论做出贡献，提供了在资源受限环境中实现模型性能与计算效率之间微妙平衡的全面视角。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      2 基础与预备知识
     </span>
    </h2>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      2.1 设备端大语言模型的发展
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      设备端大语言模型的发展是一个与技术进步紧密相关的过程。图3提供了自2023年以来设备端语言模型发展的全面时间表，展示了该领域的快速进展。如图所示，2023年人们开始认真探索和试验在边缘设备上运行大语言模型。我们看到出现了几个有影响力的参数低于100亿的模型系列，这使得大语言模型能够在边缘设备上运行。值得注意的例子包括：
      <br/>
      - Meta的LLaMA系列（Touvron等人（2023a；2023b）；Meta（2024年）；Dubey等人（2024年））
      <br/>
      - 微软的Phi系列（Gunasekar等人（2023年）；Li等人（2023c）；Abdin等人（2024年））
      <br/>
      - 智普清言的ChatGLM系列（GLM等人（2024年））
      <br/>
      - 阿里云的Qwen系列（Bai等人（2023a）；Qwen团队（2024年））
      <br/>
      - 01.AI的Yi系列（Young等人（2024年）；01.AI（2024年））
      <br/>
      - Mistral系列（Jiang等人（2023年；2024a））
      <br/>
      - 上海人工智能实验室的InternLM系列（团队（2023年）；Cai等人（2024b））
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      此外，还有诸如TII发布的Falcon（Almazrouei等人，2023年）和Mosaic ML发布的MPT模型（MosaicML，2023年）等也参与到了这类模型的竞争中。尽管这些小参数模型的性能不如传统的大参数模型，但它们使大语言模型在边缘设备上运行成为可能。它们的出现标志着语言模型行业对使用大语言模型的边缘设备应用场景的重视。同时，随着混合专家、量化和压缩等技术的应用，小参数模型在保持参数数量的同时，性能也在不断取得巨大进步。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      <img alt="" height="950" src="https://i-blog.csdnimg.cn/direct/c1b29134a7714ccc9bc5b95ddb7f6ed2.png" width="1186"/>
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图3 设备端大语言模型发展综述
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图3还突出显示了自
      <strong>
       2023年以来多模态模型
      </strong>
      的出现，例如LLaVa系列（Liu等人，2024a；2024b）、QwenVL（Bai等人，2023b）、Gemini Nano（团队等人，2023年）和Yi VL（Young等人，2024年）。这些模型代表了在边缘设备上部署多模态大语言模型的有价值尝试，以适应移动设备上更复杂多变的用户场景。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      进入2024年，创新步伐加快，从图中最右侧密集的新模型群可以明显看出这一点。这一时期推出了：
      <br/>
      - Nexa AI的Octopus系列（Chen &amp; Li，2024a；2024b；2024c）
      <br/>
      - ModelBest的MiniCPM系列（Hu等人，2024b；清华大学，2024年）
      <br/>
      - 谷歌的Gemma系列（团队等人，2024年；谷歌，2024a）
      <br/>
      - 苹果的
      <strong>
       OpenELM
      </strong>
      （Mehta等人，2024年）和DataComp-LM（Li等人，2024a）
      <br/>
      - AI2的
      <strong>
       OLMo
      </strong>
      （Soldaini等人，2024年；Groeneveld等人，2024年）
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图3清晰地展示了
      <strong>
       2024年对多模态能力
      </strong>
      的关注度增加，许多新模型同时提供文本和多模态功能，以应对多样化的任务处理场景。从模型的多样性和发展进程可以看出，设备端语言模型正在迅速发展和多样化。这一趋势，再加上智能硬件和软件技术的不断成熟，使得这些模型能够集成到
      <strong>
       智能手机、联网汽车、计算机、机器人
      </strong>
      和其他终端设备中，展示出它们日益增长的应用潜力和价值。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      2.2 大语言模型架构基础
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      1. 传统基于文本的大语言模型：让我们从源头开始。Transformer是一种基于注意力机制的深度学习模型（Vaswani等人，2017年），广泛用于处理序列数据，尤其是在自然语言处理任务中。它由两部分组成：编码器和解码器。如今，流行的大语言模型主要使用仅解码器架构（Fu等人，2023年），代表性模型如GPT（生成式预训练变换器）、LLaMA（Meta AI的大语言模型）等。GPT模型由多个解码器层组成（Radford等人，2018年；2019年；Brown等人，2020年），每个解码器层都包含一个自注意力机制。GPT模型在每个子层之后还应用了层归一化（Floridi &amp; Chiriatti，2020年）。相比之下，LLaMA在每个子层操作之前应用归一化（Ioffe &amp; Szegedy，2015年；Zhang &amp; Sennrich，2019年；Xiong等人，2020年），这有助于提高训练过程的稳定性（Touvron等人，2023a）。在注意力机制的应用方面，GPT模型使用标准的自注意力机制，这使得模型在生成序列时可以考虑输入序列中所有位置的信息，而LLaMA使用分组查询注意力（GQA）（Ainslie等人，2023年），这是一种优化技术，可以减少模型的计算量和内存占用，并提高效率。混合专家（MoE）概念起源于1991年（Jacobs等人，1991年），在当今的语言模型预训练中起着关键作用。它能够以比密集模型少得多的计算资源进行高效的预训练。该机制由两个关键组件组成：一个包含多个“专家”的稀疏MoE层，每个“专家”本身就是一个独立的神经网络（Shazeer等人，2017年；Chen等人，2022年；Du等人，2022年）；以及一个门控网络或路由：这个组件用于确定哪些令牌被发送到哪个专家模型进行处理。该架构用一个MoE层取代了传统Transformer模型中的每个前馈网络（FFN）层，MoE层由两个核心组件组成：一个门控网络和多个专家（Masoudnia &amp; Ebrahimpour，2014年）。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      2. 多模态大语言模型：凭借Transformer强大的学习架构，大型多模态模型可以同时处理多种不同的模态，如图文、图像、声音、数据表等（Xie等人，2024年；Wu等人，2023a）。其内部运行机制如下：
      <br/>
      - A）使用标准的交叉注意力层在模型内部层对多模态输入进行深度融合（如MultiModal - GPT（Gong等人，2023年））
      <br/>
      - B）使用自定义设计的层在模型内部层对多模态输入进行深度融合（LLaMA - Adapter（Zhang等人（2023a）），MoE - LLaVa（Lin等人（2024a）））
      <br/>
      - C）在模型的输入阶段对多模态输入进行早期融合，使用特定模态的编码器（LLaVa（Liu等人，2024b），Qwen - VL（Bai等人，2023a））
      <br/>
      - D）在输入阶段进行早期融合，但使用标记化技术（如分词器）来处理模态（Wadekar等人，2024年）。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      2.3 设备端大语言模型训练
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在资源受限的设备上部署大语言模型面临着诸如内存和计算能力有限等挑战（Loukas等人（2023年））。为了解决这些问题，协作和分层模型方法通过分配计算负载和利用具有不同能力的模型提供了创新的解决方案。在资源受限设备上进行训练的经典方法包括：
      <br/>
      1. 量化感知缩放：通过自动缩放不同比特精度张量的梯度来稳定训练过程，解决量化图中不同比特宽度张量的梯度尺度不一致问题，使量化模型的训练精度与浮点模型相当（Nagel等人，2022年；Huang等人，2024a）。
      <br/>
      2. 稀疏更新：有选择地更新网络中部分层的权重，跳过不太重要的层和子张量的梯度计算，从而减少内存使用和计算成本（Liu等人，2023年；Ansell等人，2024年）。
      <br/>
      3. 微型训练引擎（TTE）：在反向图中包含冗余节点，如冻结权重的梯度节点，并重新排序操作以实现就地更新（Lin等人，2023a；Khouas等人，2024年）。
      <br/>
      4. 贡献分析：自动确定稀疏更新方案，即确定哪些参数（权重/偏差）对下游精度贡献最大，以便在有限的内存预算下选择应更新哪些层或张量的哪些部分（Lin等人，2022年；Ren等人，2024年；Zeng等人，2023a）。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      2.4 基于云的大语言模型推理的局限性和设备端推理的优势
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      大语言模型的边缘 - 云（本地 - 远程）协作部署是更优选择，而现有的仅云（仅远程）（例如ChatGPT）部署
      <strong>
       并不是广泛可接受
      </strong>
      的解决方案。如图4所示，88%的参与者倾向于边缘 - 云协作架构，58.33%的人支持本地部署，81.82%的人对现有的仅云解决方案不满意。他们主要关注的问题是：
      <strong>
       1）远程大语言模型服务的高延迟；2）将个人数据传输到云端的风险；3）基于云的大语言模型服务的成本（Li等人，2024c）。
      </strong>
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      <img alt="" height="524" src="https://i-blog.csdnimg.cn/direct/d859f894f19d4082a6f65f8450d3bf4e.png" width="788"/>
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图4 个人大语言模型策略中不同大语言模型部署策略的投票分布（Li等人，2024c）
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      尽管基于云的大语言模型提供了强大的功能，但它们存在一些缺点，包括潜在的延迟问题（Wang等人，2024b）以及由于依赖网络而产生的数据问题。因此，通过边缘计算进行设备端部署的概念应运而生，以减少延迟并保护用户数据（Gerganov，2023年）。处理在本地进行，无需数据传输。此外，移动设备上定制硬件加速器的普及使得在设备上直接运行拥有数十亿参数的大型大语言模型成为可能。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      设备端推理为减少延迟提供了有力的支持，因为它允许模型直接在用户设备上运行，而无需将数据发送到云服务器。这种方法对于需要实时响应的应用特别有益。以基于云获取响应的GPT - 4为例，其每个令牌的生成速度约为200毫秒，而常见的终端侧模型已经能够比这更快地生成令牌（taivo，2023年）。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      模型离线运行的能力降低了对网络连接的依赖，使应用在网络覆盖不佳的地区或其他离线环境中更易使用。例如，谷歌基于Gemini Nano的TalkBack功能，利用多模态能力识别图像内容，为残障人士提供语音播报，即使在完全离线的情况下也能正常工作（谷歌，2024b）。设备端推理还通过模型量化等技术优化了有限计算资源的使用，使语言模型即使在内存有限的设备上也能高效运行。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      大语言模型在移动设备上的部署，因用户友好的界面而变得更加便捷。这些界面抽象掉了人工智能的复杂性，让没有专业知识的用户也能使用这项技术。此外，这些应用不仅限于文本生成，还可以通过创新的文本到动作功能，扩展其与设备功能的交互，比如拨打电话、进行网络搜索和管理日历事件等。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      2.5 设备端大语言模型的性能指标
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      延迟是指从用户输入请求到系统开始响应所花费的时间。它通常是指从模型接收到输入文本到开始生成第一个输出的时间。我们一般使用首次生成token时间（TTFT，Time-to-First-Token）来衡量这个指标（Hu等人，2024a；Agrawal等人，2024b；2024a）。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      推理速度是指大语言模型根据目前看到的所有先前token，对下一个token进行自回归预测的速度。然而，除了初始Prompt解码外，推断下一个token还需要一次解码一个token的逻辑。这是因为每个新token都依赖于前一个token，而前一个token无法提前得知。这一步骤在大语言模型的推理过程中占用的时间最多。因此，这一步骤的速度将主要决定用户对话模型是否流畅，从而直接影响用户体验（Çöpü等人，2023；Cai等人，2024a；Zheng等人，2024b）。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      运行时使用的随机存取存储器（RAM）/显存（VRAM）大小，也是语言模型性能的指标之一。由于语言模型的运行机制，它们在推理时会根据模型参数的大小消耗相应的随机存取存储器。例如，在个人办公笔记本电脑上部署一个有700亿参数的模型是不切实际的。对于许多随机存取存储器大小有限的边缘设备来说，这一点至关重要。工程师们必须使用各种模型压缩技术，来尽量减少语言模型推理时占用的内存（Kwon等人，2023；Zhao等人，2024b；2024c）。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      此外，模型占用的存储空间和推理过程中消耗的能量，在边缘设备上也会成为重要指标。这些指标对于大语言模型能否在边缘设备上运行，以及能运行多长时间尤为关键。在大多数情况下，大语言模型推理会使处理器处于满载工作状态。如果运行时间过长，会严重消耗移动设备的电池电量，从而带来新的问题。例如，一个有70亿参数的大语言模型推理时，每个token大约会消耗0.7焦耳的能量。对于电池容量约为50千焦的iPhone来说，这意味着与模型的对话最多只能持续两个小时。这还没有考虑到模型推理导致的设备发热等其他问题（Liu等人，2024c；Stojkovic等人，2024；Jiang等人，2024b）。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      3 设备端大语言模型的高效架构
     </span>
    </h2>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      3.1 设备端大语言模型的架构设计原则与创新
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      为设备端部署设计语言模型，需要遵循若干架构原则并进行创新，以克服移动和边缘设备常见的资源限制。关键策略包括：
      <br/>
      1. 参数共享（Lin等人，2023b；Cao等人，2024），即跨模型的不同部分重用权重，以减少总体参数数量；
      <br/>
      2. 模块化架构（Ning等人，2023；Ostapenko等人，2024；Shen等人，2024），将大语言模型分解为更小的、独立的组件或模块，这些组件或模块可以单独或并行处理；
      <br/>
      3. 紧凑表示，即通过量化和权重剪枝等技术，专注于减少大语言模型的内存占用（Liu等人，2024c；Zhang等人，2024b；Xu等人，2023）。为全面比较这些架构，我们考虑了它们的性能、计算效率和内存需求，相关内容总结在表1中。
     </span>
    </p>
    <p style="text-align:justify">
     <img alt="" height="1106" src="https://i-blog.csdnimg.cn/direct/8e7691d18342479b86e14f8333bf4e08.png" width="1182"/>
    </p>
    <table>
     <thead>
      <tr>
       <th style="text-align:justify">
        模型
       </th>
       <th style="text-align:justify">
        性能
       </th>
       <th style="text-align:justify">
        计算效率
       </th>
       <th style="text-align:justify">
        内存需求
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td style="text-align:justify">
        MobileLLM（Liu 等人，2024c）
       </td>
       <td style="text-align:justify">
        高精度，针对十亿以下参数模型进行了优化
       </td>
       <td style="text-align:justify">
        嵌入共享、分组查询注意力机制
       </td>
       <td style="text-align:justify">
        由于采用深窄结构，模型尺寸减小
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        EdgeShard（Zhang 等人，2024b）
       </td>
       <td style="text-align:justify">
        延迟最多可降低 50%，吞吐量提高 2 倍
       </td>
       <td style="text-align:justify">
        协作式边缘云计算、优化分片放置
       </td>
       <td style="text-align:justify">
        分布式模型组件减轻单个设备负载
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        LLMCad（Xu 等人，2023）
       </td>
       <td style="text-align:justify">
        令牌生成速度提升高达 9.3 倍
       </td>
       <td style="text-align:justify">
        先生成后验证、令牌树生成
       </td>
       <td style="text-align:justify">
        较小的大语言模型用于令牌生成，较大的大语言模型用于验证
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        Any-Precision LLM（Park 等人，2024）
       </td>
       <td style="text-align:justify">
        高效支持多种精度
       </td>
       <td style="text-align:justify">
        训练后量化、内存高效设计
       </td>
       <td style="text-align:justify">
        通过多种模型精度实现大量内存节省
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        Breakthrough Memory（Kim 等人，2024c）
       </td>
       <td style="text-align:justify">
        性能提升高达 4.5 倍
       </td>
       <td style="text-align:justify">
        内存内处理（PIM）和近内存处理（PNM）技术增强内存处理能力
       </td>
       <td style="text-align:justify">
        增强内存带宽和容量
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        MELTing Point（Laskaridis 等人，2024）
       </td>
       <td style="text-align:justify">
        提供系统的性能评估
       </td>
       <td style="text-align:justify">
        分析量化的影响、高效的模型评估
       </td>
       <td style="text-align:justify">
        评估内存和计算效率的权衡
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        LLMaaS on MD（Yin 等人，2024）
       </td>
       <td style="text-align:justify">
        显著降低上下文切换延迟
       </td>
       <td style="text-align:justify">
        有状态执行、细粒度键值缓存压缩
       </td>
       <td style="text-align:justify">
        通过容错感知压缩和交换实现高效内存管理
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        LocMoE（Li 等人，2024b）
       </td>
       <td style="text-align:justify">
        每个训练周期的训练时间最多减少 22.24%
       </td>
       <td style="text-align:justify">
        正交门控权重、基于局部性的专家正则化
       </td>
       <td style="text-align:justify">
        通过组间全对全通信和重新计算管道最小化通信开销
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        EdgeMoE（Yi 等人，2023）
       </td>
       <td style="text-align:justify">
        在边缘设备上性能显著提升
       </td>
       <td style="text-align:justify">
        按专家比特宽度自适应、预加载专家
       </td>
       <td style="text-align:justify">
        通过按专家重新排序计算实现高效内存管理
       </td>
      </tr>
      <tr>
       <td style="text-align:justify">
        JetMoE（Shen 等人，2024）
       </td>
       <td style="text-align:justify">
        以更少的参数超越 Llama2-7B 和 13B-Chat
       </td>
       <td style="text-align:justify">
        使用稀疏激活将推理计算量减少 70%
       </td>
       <td style="text-align:justify">
        总共有 80 亿参数，每个输入令牌仅激活 20 亿参数
       </td>
      </tr>
     </tbody>
    </table>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      表1  最先进的设备端大语言模型架构对比分析
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      3.2 模型压缩与参数共享
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在智能手机和边缘设备等资源受限的设备上高效部署大语言模型，通常需要在不显著牺牲性能的前提下减小模型大小。模型压缩和参数共享技术在实现这一平衡方面起着关键作用。本小节回顾了一些关键研究工作，这些工作专注于通过创新的压缩和参数共享方法，优化参数在十亿以下的大语言模型。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      Lin等人（2024b）提出了一种新颖的仅权重量化方法，该方法聚焦于大语言模型中权重的重要性。激活感知权重量化（AWQ）保护一小部分关键权重（0.1%-1%），减少量化损失，并在不同领域和模态中保持大语言模型的泛化能力。与传统方法不同，AWQ无需反向传播或重构，因此能保持效率和性能。所提出的TinyChat推理框架实现了AWQ，在桌面和移动GPU上，与传统的半精度浮点数（FP16）实现相比，速度提升显著（高达3倍）。MobileLLM通过提出一种针对十亿以下参数数量优化的深窄架构，满足了在移动设备上高效运行大语言模型的需求（Liu等人，2024c）。这种方法挑战了“更宽的模型更好”这一普遍观点，证明深窄结构可以有效地捕捉复杂模式。其关键技术包括嵌入共享、分组查询注意力和块级即时权重共享。MobileLLM相较于之前的最先进模型，在准确率上有显著提升（例如，相较于1.25亿和3.5亿参数的模型，准确率分别提高了2.7%和4.3%）。其增强版本MobileLLM-LS在保持较小模型大小的同时，进一步提高了准确率，非常适合设备端应用。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      AWQ和MobileLLM都展示了模型压缩和参数共享技术，在使大语言模型能够在移动和边缘设备上部署方面的潜力。AWQ专注于权重量化以减小模型大小并提高推理速度，而MobileLLM则强调架构优化和权重共享，以创建高效的十亿以下参数模型。这些创新对于在资源受限环境中提高大语言模型的性能和可及性至关重要，使个人设备在不牺牲准确性或效率的情况下，具备先进的人工智能能力。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      3.3 协作与分层模型方法
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在资源受限的设备上部署语言模型面临着诸如内存和计算能力有限等重大挑战。
      <strong>
       协作和分层
      </strong>
      模型方法通过分配计算负载，并利用多个具有不同能力的模型，提供了创新的解决方案来克服这些限制。本小节回顾了一些关键研究工作，这些工作实施协作和分层策略，以提高设备端大语言模型的效率和可扩展性。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      EdgeShard引入了
      <strong>
       EdgeShard
      </strong>
      框架，该框架将大型大语言模型划分为较小的片段（分片），并战略性地分布在边缘设备和云服务器之间（Zhang等人，2024b）。这种方法通过同时利用多个设备的计算能力，降低了延迟并提高了吞吐量。一种动态规划算法优化了分片放置，平衡了计算负载并最小化了通信开销。实验结果表明，与传统的基于云的方法相比，在延迟降低（高达50%）和吞吐量提升（高达2倍）方面有显著改进。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      LLMCad提出了一种新颖的推理引擎，它将一个较小的、驻留在内存中的大语言模型，与一个更大、更准确的大语言模型相结合（Xu等人，2023）。较小的大语言模型生成候选token，而较大的大语言模型对这些token进行验证和纠正。这种“
      <strong>
       先生成后验证
      </strong>
      ”的方法利用了较小模型的效率，并保持了较大模型的准确性。LLMCad引入了多种技术，包括
      <strong>
       token树生成和验证
      </strong>
      、自适应回退策略和推测生成管道。这些创新使LLMCad在不牺牲准确性的情况下，实现了高达9.3倍的token生成加速，使其适用于移动设备上的实时应用。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      WDMoE提出了一种在无线通信系统中部署大语言模型的新范式（Xue等人，2024a）。通过进行混合专家（MoE）层分解，在基站部署门控网络，将专家网络分布在移动设备上，以优化性能并降低延迟。此外，还提出了专家选择策略，根据无线信道条件动态调整专家选择，以确保最佳性能。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      协作和分层模型方法，如EdgeShard和LLMCad中提出的方法，为在资源受限设备上部署大语言模型的挑战提供了有效的解决方案。通过将计算负载分布在多个设备上，并使用较小的模型执行初步任务，这些方法提高了大语言模型推理的可扩展性和效率。EdgeShard框架展示了协作式边缘 - 云计算的优势，而LLMCad则展示了分层模型协作在保持准确性和提高推理速度方面的潜力。这些方法对于在移动和边缘设备上实现先进的人工智能能力至关重要，能够提供实时性能和高效的资源利用。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      3.4 内存与计算效率
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在移动和边缘设备上部署大语言模型时，高效利用内存和计算资源至关重要。各种技术和创新旨在优化有限资源的使用，以确保大语言模型能够在不超出设备能力的情况下有效运行。本小节回顾了一些关键研究工作，这些工作专注于提高设备端大语言模型的内存和计算效率。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      三星电子的研究人员提出了创新的内存解决方案，以解决大语言模型部署中的内存瓶颈问题（Kim等人，2024c）。作者介绍了内存内处理（PIM）和近内存处理（PNM）技术：Aquabolt-XL（Kim等人，2021）和LPDDR-PIM（Kim等人，2024a）。这些PIM设备在内存核心中嵌入逻辑，提高了内部内存带宽，并支持高性能计算任务，包括大语言模型加速。AXDIMM（Ke等人，2021）和CXL-PNM：这些PNM解决方案将计算逻辑放置在靠近内存核心的位置，提高了内存带宽和容量。CXL-PNM将计算逻辑集成到CXL内存控制器中，显著提高了内存容量和性能。实验结果表明，与传统内存架构相比，这些内存解决方案实现了高达4.5倍的性能提升和71%的能耗降低，使其非常适合在资源受限的设备上进行大语言模型推理。MELTing Point引入了MELT基础设施，旨在促进大语言模型在移动设备上的执行和基准测试（Laskaridis等人，2024）。MELT框架支持安卓、iOS和英伟达Jetson设备，并提供详细的性能和能耗指标。MELT系统地评估了设备端大语言模型的执行情况，提供了关于各种模型的性能、能源效率和内存使用情况的见解。该论文研究了模型量化对性能和准确性的影响，表明虽然量化减少了内存需求，但会导致准确性下降。结果强调了在内存和计算效率与性能之间取得平衡，以使大语言模型适用于移动应用的重要性。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      内存和计算效率对于在移动和边缘设备上部署大语言模型至关重要。本小节回顾的研究工作提出了创新的解决方案，以克服内存墙并优化资源使用。三星的内存解决方案，如PIM和PNM，显著提高了内存带宽和容量，实现了高效的大语言模型推理。MELT基础设施提供了一个全面的评估框架，为性能、能源效率和内存使用之间的权衡提供了有价值的见解。这些进展对于确保大语言模型能够在资源受限的设备上有效运行至关重要，为在移动和边缘环境中实现更实用、高效的人工智能应用铺平了道路。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      3.5 混合专家（MoE）架构
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      混合专家（MoE）架构通过利用稀疏激活和动态路由来提高效率，为在边缘设备上部署大语言模型提供了一种有前景的方法。本小节回顾了一些关键研究工作，这些工作专注于基于MoE的模型，旨在优化设备端部署中的性能和资源利用。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      EdgeMoE引入了一个框架，旨在在边缘设备上高效执行MoE模型（Yi等人，2023）。作者提出了按专家比特宽度自适应方法，使用逐通道线性量化在最小化精度损失的情况下减小专家权重的大小。通过利用新颖的专家管理方法，他们将专家权重预加载到计算 - 输入/输出（I/O）管道中，以减少I/O交换开销。实验结果表明，与基线解决方案相比，在内存节省和性能提升方面有显著改进，推理速度提升高达2.78倍。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      LocMoE提出了一种路由策略和通信优化方案，以提高基于MoE的大语言模型的训练效率（Li等人，2024b）。采用正交门控权重方法来降低计算成本，并便于进行明确的路由决策。此外，他们引入了基于局部性的专家正则化，以鼓励本地专家竞争，减少通信时间并避免训练不足。他们还包括组间全对全通信和通信重叠，通过将计算与通信重叠来掩盖延迟，优化全对全操作。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      Yin等人（2024）提出了大语言模型即服务（LLMaaS）范式，将大语言模型作为移动设备上的系统服务进行集成。在他们提出的设计中，有状态执行允许系统在多次调用之间保持持久状态（键值缓存），以提高性能。统一接口通过将大语言模型及其基础设施作为系统功能暴露给移动应用程序，帮助减少内存使用。他们还引入了诸如按块键值缓存压缩和交换等技术，以最小化上下文切换开销。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      JetMoE提出了一种使用稀疏门控混合专家（SMoE）架构进行大语言模型训练的高效方法（Shen等人，2024）。作者在注意力层和前馈层都应用了稀疏激活，在保持高性能的同时显著降低了计算成本。使用1250亿个token和30000个H100 GPU小时进行训练的JetMoE-8B，成本不到10万美元，其性能优于Llama2-7B，而JetMoE-8B Chat的性能则超过了Llama2-13B-Chat。该模型总共有80亿个参数，每个输入token仅激活20亿个，与Llama2-7B相比，推理计算量减少了约70%。MoE架构为在边缘设备上部署大语言模型的挑战提供了创新的解决方案。这些方法利用稀疏激活和动态路由来提高计算效率和资源利用率。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      3.6 综合效率和性能提升
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在边缘设备上实现大语言模型的高效部署，需要一系列策略来提高整体性能，同时管理计算和内存约束。本小节回顾了一些关键研究工作，这些工作引入了创新方法来提高设备端大语言模型的效率和有效性。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      Any-Precision LLM提出了一种新颖的方法，以内存高效的方式部署具有不同精度的各种大语言模型（Park等人，2024）。Any-Precision模型将任意精度的深度神经网络扩展到大语言模型，允许单个n位量化模型支持低至3位的多个较低比特宽度模型。这在不显著降低性能的情况下减少了内存使用。训练后量化（PTQ）创建低比特模型，并逐步将其扩展到更高的比特宽度。这避免了为每个精度进行多个训练阶段，节省了时间和资源。一个针对任意精度支持进行优化的新软件引擎管理内存带宽，并提高服务效率，确保大语言模型在边缘设备上的实际部署。实验结果表明，该方法在内存节省和服务效率提升方面效果显著，使任意精度的大语言模型适用于各种设备端应用。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      Yan等人（2023）探索了在软硬件协同设计中使用大语言模型，以优化内存计算（CiM）深度神经网络（DNN）加速器的开发。LCDA框架将大语言模型集成到硬件和软件的设计过程中，利用它们在各种数据集上的广泛训练来加速协同设计。通过整合预训练大语言模型的启发式知识，该框架避免了冷启动问题，能够更快地收敛到最优解决方案。与最先进的方法相比，该框架在设计过程中实现了25倍的加速，同时在设计高效的DNN模型和硬件架构方面保持了相当的性能水平。这种方法凸显了大语言模型在增强协同设计过程方面的潜力，提高了先进人工智能应用的软件和硬件效率。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      综合效率和性能提升对于大语言模型在边缘设备上的实际部署至关重要。本小节回顾的研究工作引入了创新方法来提高内存效率、计算速度和整体性能。Any-Precision LLM方法为部署具有不同精度的多个大语言模型提供了灵活且内存高效的解决方案，而LCDA框架则展示了将大语言模型集成到协同设计过程中，以优化软件和硬件的优势。这些进展有助于使大语言模型在资源受限的环境中更易获取和更有效，为在移动和边缘设备上实现更广泛的人工智能应用做出了贡献。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      4 设备端大语言模型的模型压缩与优化技术
     </span>
    </h2>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在大语言模型领域，在保持性能的同时优化计算效率至关重要，这对于在边缘设备上的部署尤为关键。本节探讨四种关键的模型压缩技术：量化、剪枝、知识蒸馏和低秩分解。这些方法提高了大语言模型的运行效率，通过平衡性能、内存占用和推理速度，确保它们适用于设备端应用。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      4.1 量化
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      神经网络中的量化是指将高精度（浮点型）的权重和激活值转换为较低比特宽度（整型）的过程。这项技术大幅减小了模型大小，降低了计算需求，能够实现更快的推理速度，并减少内存消耗，同时保持准确性。
      <br/>
      1. 训练后量化（PTQ）：训练后量化是在模型训练完成后应用的，无需重新训练，因此比量化感知训练（QAT）更快，且资源消耗更少。有几种值得注意的训练后量化方法。GPTQ（Frantar等人，2022）利用二阶信息进行误差补偿，有效地将权重的比特宽度降低到3或4比特。这种方法在最小化困惑度增加的同时保持了较高的准确性，使得像OPT-175B这样的语言模型能够在单个高端GPU上运行。激活感知权重量化（AWQ）（Lin等人，2024c）基于这样的观察：一小部分（0.1%-1%）的权重对大语言模型的性能至关重要。通过有选择地跳过这些关键权重的量化，AWQ显著减少了量化损失。
      <br/>
      - 仅权重量化：在仅权重量化中，仅对神经网络的权重进行量化。这种方法简化了量化过程，当激活值的范围变化不大或计算资源严重受限的时候，这种方法特别有效。
      <br/>
      - 权重 - 激活值协同量化：对权重和激活值都进行量化，增强了对计算复杂度的降低效果。由于在神经计算中高效的矩阵乘法（Dettmers等人，2022），这种方法在硬件实现中具有优势。BitNet b1.58（Ma等人，2024）对每个参数使用三值量化（-1、0、1），显著提升了延迟、内存、吞吐量和能耗等指标。
      <br/>
      2. 量化感知训练（QAT）：量化感知训练将量化直接融入训练过程，使模型能够内在地适应降低的精度约束。这种整合通常在量化后能产生更高的准确率，因为模型在训练阶段会主动学习补偿潜在的量化误差。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      4.2 剪枝
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      神经网络中的剪枝是指有选择地去除权重或神经元，以降低模型复杂度并提高计算效率，同时不会显著影响性能。这个过程针对模型中不太关键的组件，注重效率和功能完整性。
      <br/>
      1. 结构化剪枝：这种方法会去除整个参数子集，如层、通道或滤波器。由于更规则的内存访问模式和简化的计算，它对硬件优化有益。“LLM-Pruner”（Kaddour等人，2023）采用结构化剪枝，根据梯度数据消除非必要的组，从而保持关键功能。它还通过诸如低秩自适应（LoRA）等技术促进性能恢复，能够以最少的数据实现高效恢复。
      <br/>
      2. 非结构化剪枝：与结构化剪枝不同，非结构化剪枝会在整个模型中去除单个权重，提供更细的粒度，可能实现更高的压缩率（Li等人，2023a）。然而，这种方法通常会导致稀疏矩阵，与传统硬件架构的兼容性较差，从而影响计算效率。它最适用于需要最大程度压缩且对结构保留没有限制的情况。
      <br/>
      3. 上下文剪枝：这是一种先进的方法，根据模型的操作上下文进行剪枝，针对仅在特定条件下或特定任务中相关的权重或神经元。上下文剪枝确保减少的部分能动态地与模型的操作需求保持一致，从而在最重要的地方保持性能。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      4.3 知识蒸馏
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      知识蒸馏（KD）是一种将知识从大型、计算密集型模型（教师模型）转移到较小、更高效模型（学生模型）的技术。这种方法对于将大语言模型（LLMs）的能力压缩成更易于管理的形式至关重要，同时不会显著影响性能。
      <br/>
      1. 黑盒知识蒸馏：这种方法中，学生模型仅从教师模型的输出中学习，无法访问其内部机制或参数。当教师模型的细节是专有的，或者教师模型和学生模型的架构差异很大时，这种方法特别有优势。例如，Gu等人（2023）证明，黑盒知识蒸馏可以仅使用来自像ChatGPT这样的大语言模型应用程序编程接口（API）的输出数据来有效地训练模型。学生模型根据输入 - 输出对训练以模仿教师模型的输出分布，这个过程虽然有效，但将学习限制在外部行为上，无法挖掘教师模型更深层次的内部状态。
      <br/>
      2. 白盒知识蒸馏：相比之下，白盒知识蒸馏允许学生模型访问教师模型的内部状态和工作机制，促进更深入、更精确的学习过程。这种方法使学生模型不仅能够模仿教师模型的输出，还能模仿其内部状态分布，增强学习效果和深度。对教师模型详细工作机制的更多访问有助于指导学生模型的学习，从而产生更准确、更强大的模型。然而，这种技术需要仔细对齐模型架构，以确保有效的知识转移，并且通常实现起来更复杂。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      4.4 低秩分解
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      低秩分解（LRF）是一种将矩阵分解为较小组件的技术，可显著降低计算复杂度，同时不会大幅影响模型准确性。利用许多实际矩阵中普遍存在的固有低秩结构，低秩分解通过低秩因子的乘积来近似这些矩阵，这在诸如图像处理、机器学习模型的降维和数据压缩等应用中已被证明是不可或缺的（Saha等人，2023）。这种方法不仅保留了基本的数据特征，还确保了高效的存储和处理，凸显了其在现代计算任务中的关键作用。进一步扩展其应用，Yao等人（2024b）的一项研究将低秩分解与大语言模型中的训练后量化（PTQ）相结合。这种创新方法被称为低秩补偿（LoRC），通过显著减小模型大小并保持准确性来提高模型效率，有效地减轻了激活量化的不利影响。低秩分解和训练后量化的这种结合，展示了在优化计算效率的同时保持复杂模型性能完整性方面的重大进展。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      5 硬件加速与部署策略
     </span>
    </h2>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图形处理单元（GPUs）、张量处理单元（TPUs）和专用人工智能芯片等硬件加速器，通过提供强大的计算能力和高内存带宽，在实现设备端大语言模型的高效推理方面发挥着关键作用。在选择GPU、TPU、现场可编程门阵列（FPGA）和其他专用人工智能芯片时，需要仔细考虑性能、功耗和成本等方面的权衡。例如，GPU因其并行处理能力而受到青睐，TPU用于其专门的矩阵运算，FPGA则因其可定制的硬件以适应特定任务，在功耗效率方面表现更优。软硬件协同设计方法，包括量化感知训练和模型压缩，进一步提高了效率，使得大语言模型在从高功率服务器到低功率边缘设备的一系列设备上都能可行。诸如参数共享和先进内存管理技术等优化策略，对于减少大语言模型的占用空间至关重要，确保在不同的计算环境中实现更快、更具成本效益的部署。这些策略共同改进了大语言模型的部署和执行，满足了各种应用需求和硬件约束。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      5.1 流行的设备端大语言模型框架
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      大语言模型的部署策略可能因用例和可用基础设施的不同而有很大差异，从完全基于云的解决方案到仅在边缘设备部署的方案都有。
      <br/>
      1. 仅在边缘设备部署
      <br/>
      - Llama.cpp：描述：Llama.cpp（Gerganov，2023）是一个C/C++库，旨在在广泛的硬件平台上高效推理大语言模型。它支持整数量化、GPU加速以及CPU + GPU混合推理。训练：支持在设备上微调低秩自适应（LORA）适配器。推理：支持在ARM和x86架构上进行CPU和CPU + GPU混合推理。
      <br/>
      - MNN：描述：MNN（阿里巴巴，2024）利用移动神经网络技术，在各种平台上高效推理大语言模型，针对具有动态输入和多模态交互的移动设备进行了优化。训练：支持在设备上进行全尺寸微调以及LORA微调。推理：支持在包括CPU、CUDA和OpenCL等多种后端上部署ONNX和MNN格式的模型。
      <br/>
      - PowerInfer：描述：PowerInfer（Song等人，2023）和PowerInfer2（Xue等人，2024b）是一种高速推理引擎，针对在配备消费级GPU的个人电脑上部署大语言模型进行了优化，采用以局部性为中心的设计。训练：没有内置训练功能。推理：支持包括x86 - 64 CPU和苹果M芯片在内的各种计算平台，针对Windows和Linux系统进行了优化。
      <br/>
      - ExecuTorch：描述：ExecuTorch（PyTorch，2024）是PyTorch Edge生态系统的一部分，旨在在移动电话和可穿戴设备等边缘设备上高效部署PyTorch模型。训练：没有内置训练功能。推理：利用包括CPU、神经网络处理单元（NPUs）和数字信号处理器（DSPs）在内的各种计算平台的全部硬件能力。
      <br/>
      - MediaPipe：描述：由谷歌开发的MediaPipe（AI，2024b）是一个用于构建和部署涉及视频、音频和其他时间序列数据的多模态机器学习管道的框架。训练：没有内置训练功能。推理：支持包括安卓、iOS、macOS、Windows和Linux在内的多个平台，利用CPU和GPU资源。
      <br/>
      2. 边缘 - 云协同部署
      <br/>
      - MLC - LLM：描述：MLC - LLM（团队，2023）是一个机器学习编译器和高性能部署引擎，支持在边缘设备和云环境中通用部署大语言模型。训练：没有内置训练功能。推理：支持在包括ARM和x86架构的CPU和GPU等各种平台上进行推理。
      <br/>
      - VLLM：描述：VLLM（团队，2024）针对边缘 - 云环境进行了优化，支持在推理过程中使用先进的量化方法，以实现高效的键值内存管理。训练：没有内置训练功能。推理：支持多个GPU平台，并与Vulkan、CUDA、Metal和WebGPU技术集成。
      <br/>
      - OpenLLM by BentoML：描述：OpenLLM（BentoML，2024）能够将各种开源大语言模型部署为与OpenAI兼容的应用程序编程接口（API）端点，针对高吞吐量和简化的云部署进行了优化。训练：没有内置训练功能。推理：与各种模型架构和后端实现兼容，以便在生产环境中高效部署。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      5.2 硬件加速
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      硬件技术的不断进步对设备端大语言模型的部署和性能产生了重大影响。
      <br/>
      1. GPU：图形处理单元（GPUs）因其大规模并行性和高内存带宽，已成为训练和加速大语言模型的标准选择。英伟达在Volta架构中引入并在后续几代中改进的Tensor Cores，为混合精度矩阵乘加运算提供了专用硬件，这对于基于Transformer的模型至关重要。最近的进展，如具有80GB HBM2e内存的英伟达A100 GPU，使得在单个设备上训练具有数十亿参数的模型成为可能。诸如张量并行和流水线并行等技术，在Megatron - LM等框架中得以实现，可实现大语言模型在多个GPU上的高效扩展。混合精度训练的使用，特别是FP16和BF16格式，显著减少了内存占用，并提高了现代GPU上的计算吞吐量。
      <br/>
      2. NPU：神经网络处理单元（NPUs），也称为人工智能加速器，是为机器学习工作负载设计的专用芯片。谷歌的张量处理单元（TPUs）是一个突出的例子，最新的v4版本每个芯片提供275 TFLOPS的BF16性能。TPUs采用脉动阵列架构进行高效的矩阵乘法，这特别适合大语言模型中的Transformer层。TPU Pod配置允许扩展到数千个芯片，能够训练像GPT - 3和PaLM这样的模型。华为的Ascend AI处理器和苹果的Neural Engine是其他NPUs的例子，它们利用量化和剪枝等技术来减小模型大小和计算需求，为较小的大语言模型提供设备端加速推理。
      <br/>
      3. FPGA：现场可编程门阵列（FPGAs）为加速大语言模型提供了灵活的硬件平台，特别是在推理方面。最近的工作展示了在FPGA上高效实现Transformer层，利用了诸如稀疏矩阵乘法和量化等技术。例如，微软的Project Brainwave使用英特尔Stratix 10 FPGAs来加速BERT推理，实现了低延迟和高吞吐量。FPGAs在能源效率方面表现出色，并且可以针对特定的模型架构进行优化，使其适合在边缘设备上部署较小的大语言模型。然而，与GPU和专用集成电路（ASICs）相比，它们较低的计算密度限制了其在训练大规模模型中的应用。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      6 示例与应用
     </span>
    </h2>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      近年来，人工智能技术的快速发展和移动设备硬件的不断升级，使得在边缘设备上部署大语言模型成为现实。智能手机是人们日常生活中最常用的设备之一，其上的语言模型尤其引人注目。目前，全球主要的手机品牌制造商已经开发并发布了许多先进的模型，这些模型要么在设备端部署，要么采用设备 - 云协作策略，如表2所示。这些模型不仅标志着移动计算的重大飞跃，还为用户带来了一系列传统云部署无法比拟的优势。
     </span>
    </p>
    <p style="text-align:justify">
     <img alt="" height="278" src="https://i-blog.csdnimg.cn/direct/4bba06387fe74c92af15484e43c63809.png" width="1097"/>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      表2 手机制造商发布的最先进的设备端大语言模型
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      6.1 设备端语言模型示例
     </span>
    </h3>
    <ul>
     <li style="text-align:justify">
      1. Gemini Nano：移动操作系统会将大语言模型及其推理基础设施作为系统功能暴露给移动应用程序，就像定位或通知服务一样。用户可以通过谷歌AI Edge SDK访问AI核心。在AI核心内部，谷歌提供了Gemini Nano模型，该模型比在云端运行推理的其他Gemini模型更小，但速度更快且推理延迟更低。AI核心负责Gemini Nano模型的分发，因此可以很好地管理内存。此外，AI核心利用设备端硬件加速推理，能够以最佳速度运行。Gemini Nano模型是通过从更大的Gemini模型中进行知识蒸馏训练得到的。它在部署时进行了4位量化，并提供了同类最佳的性能（团队等人，2023）。
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       2. Nexa AI Octopus系列模型：一个在边缘设备上运行的20亿参数模型，在准确性和延迟方面超过了GPT - 4，并且将上下文长度缩短了95%。通过对核心功能名称进行标记化，并使用功能token对模型进行微调，该模型可以理解软件应用程序的功能，并学习将功能描述映射到特定的token。在移动设备上部署Octopus模型展示了快速的响应时间，对于一个典型的20到30个token的查询，即使在标准的安卓手机上，也能在1.1到1.7秒内完成功能调用（Chen等人，2024b；Chen &amp; Li，2024a；2024b；2024c）。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       3. 苹果OpenELM和Ferret - v2：苹果开发了OpenELM（Mehta等人，2024），这是一个集成在iOS系统中的大型语言模型，用于增强应用程序的功能，类似于位置跟踪等基本系统服务。OpenELM采用层归一化架构，有效地部署其11亿个参数，与之前的模型相比，准确性提高了2.36%，同时仅需要一半的预训练token。此外，它与MLX库兼容，便于在苹果设备上进行直接微调。与此同时，Ferret - v2（Zhang等人，2024a）相比其前身有了显著升级，融入了诸如任意分辨率定位、通过集成DINOv2编码器实现多粒度视觉编码，以及复杂的三阶段训练方案等功能。这些增强通过推进高分辨率图像处理和丰富视觉理解，显著提高了性能，从而为iOS用户确保了强大的设备端功能。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       4. 微软Phi系列：微软最新的Phi - 3 - mini（Abdin等人，2024）是一个紧凑但功能强大的38亿参数语言模型，在3.3万亿个token的数据集上进行训练。尽管其规模较小适合移动部署，但Phi - 3 - mini的性能可与Mixtral 8x7B和GPT - 3.5等更大的模型相媲美，在大规模多任务语言理解（MMLU）测试中得分达到69%，在MT - bench测试中得分为8.38。该模型得益于独特的训练数据集，这是用于Phi - 2的数据集的扩展版本，结合了经过大量筛选的公开网络数据和合成数据，增强了模型的稳健性、安全性和聊天功能。此外，我们展示了我们的缩放模型Phi - 3 - small和Phi - 3 - medium的初步结果，它们分别在4.8万亿个token上进行训练，具有70亿和140亿个参数，展现出卓越的能力（在MMLU测试中分别为75%和78%，在MT - bench测试中的得分分别为8.7和8.9）。进一步扩展，我们推出了Phi - 3 - vision，这是一个基于Phi - 3 - mini的42亿参数模型，设计用于增强对图像和文本Prompt的推理能力。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       5. MiniCPM：MiniCPM - Llama3 - V 2.5是清华大学和ModelBest合作开发的开源MiniCPM - V系列的新成员，拥有85亿个参数（清华大学，2024）。该模型在OpenCompass评估平台上表现出色，该平台涵盖了11个多模态基准测试。MiniCPM - Llama3 - V 2.5的平均得分达到了65.1，超过了包括GPT - 4V - 1106（得分63.5）、Gemini Pro（得分62.9）、Claude 3和Qwen - VL - Max在内的领先行业模型，尽管它的参数数量仅为这些模型的一小部分。在专注于光学字符识别（OCR）和场景文本理解的特定评估中，MiniCPM - Llama3 - V 2.5表现优异，在OCRBench上的得分超过700分，超过了GPT - 4和Gemini Pro等竞争对手。此外，它在TextVQA基准测试中达到了76.6%的准确率，在DocVQA测试中达到了令人印象深刻的84.8%的准确率，有效地为这些领域的开源模型性能树立了新的标准。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       6. Gemma2 - 9B：Gemma是谷歌开发的轻量级、最先进的开源模型系列。Gemma2是Gemma的升级版，有90亿和270亿参数两种版本。对于90亿参数的版本，Gemma2的训练数据量为8TB的网页数据、代码和数学数据。作者采用了一种新颖的注意力组合方法，即一层滑动窗口注意力和一层全局注意力。还使用了知识蒸馏、模型合并等技术。Gemma2 - 9B模型在其同等规模的类别中也表现出色，在推理、数学和代码等多个领域优于Llama 3 - 8B和其他类似的开源模型。该模型与HuggingFace等主要AI框架以及Keras 3.0、vLLM、Gemma.cpp和Llama.cpp等也有良好的兼容性（谷歌，2024a）。7. Qwen2-0.5B：阿里云的Qwen团队将Qwen模型系列升级到Qwen2，并推出了五个不同规模的模型。其中，Qwen2-0.5B是参数数量最少的模型，上下文长度为32K。在多项测试中，Qwen2-0.5B的表现与Gemma-2B和Phi-2相似（Qwen团队，2024），但其参数数量更少，这使得它在智能家居行业的未来发展中具有巨大潜力。此外，针对上下文长度较短的问题，Qwen-Agent框架采用了智能体检索增强生成（Agentic RAG）的思路，能够将处理上下文扩展到1M，从而实现长文本理解（Bai等人，2023a）。
      </span>
     </li>
    </ul>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      6.2 设备端大语言模型的应用
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      设备端语言模型正在开启一个智能、响应迅速且个性化的应用新时代。通过将先进的自然语言处理能力直接带到终端用户设备上，这些模型正在改变我们在日常生活和专业工作中与技术交互的方式。从即时消息建议到实时语言翻译，从保密的医疗咨询到前沿的自动驾驶汽车，设备端大语言模型被证明是具有广泛影响的多功能工具。以下示例（总结在图5中）展示了设备端大语言模型应用的广度和深度，展示了这项技术不仅在增强现有服务，还在各个领域实现了全新类别的智能、响应迅速且安全的应用。
     </span>
    </p>
    <p style="text-align:justify">
     <img alt="" height="666" src="https://i-blog.csdnimg.cn/direct/110f6680a8bf449085435813c10455d9.png" width="662"/>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图5 设备端大语言模型的不同应用领域
     </span>
    </p>
    <ol>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       消息文本生成：过去，基于云大语言模型的快速回复功能受生成速度和网络延迟的限制，为用户生成回复的速度较慢，在节奏快速的即时对话中效率低下。得益于设备端大语言模型，谷歌的键盘应用Gboard可以使用谷歌的设备端大语言模型Gemini Nano（AI，2024a）。当检测到用户正在进行在线聊天时，Gemini Nano可以根据聊天内容快速为用户生成具有对话感知的快速回复选项。由于所使用的语言模型无需连接互联网等待服务器响应，该功能能够体现出真正的响应速度。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       翻译：大语言模型在语言翻译领域得到了广泛应用。这种方法可以使用适合特定领域的术语和风格进行翻译，这是传统机器翻译方法无法做到的。然而，基于云的大语言模型仍然面临响应速度慢和需要上传信息等问题。设备端大语言模型能更好地解决这些问题，其参数较少、响应速度更快，还可以在离线环境中运行，为许多场景提供了数据安全性。在翻译质量方面，使用小尺寸模型并不会显著降低翻译的准确性。使用T5-small模型的token生成准确率仅比T5大语言模型低4%（Xu等人，2023）。此外，更快的响应速度意味着设备端模型将更适合诸如同声传译等更即时的翻译场景。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       会议总结：亚马逊首席技术官发布的基于云的解决方案Distill-CLI，使用Anthropic的Claude 3 Sonnet模型和亚马逊转录技术生成实时会议总结（Vogels，2024）。类似的应用还有使用GPT-4o模型的Plaud Note（Plaud，2024）、Zoom-IQ（Zoom，2024）等。然而，使用基于云的模型的缺点是会产生订阅服务费用，以及联网带来的网络延迟问题。通过采用设备端模型，数据可以保持本地化，无需上传到基于云的服务器。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       医疗保健应用：目前的医疗模型，如Med-Palm Multimodal（Tu等人，2024），可以结合并分析患者的陈述、电子病历信息、X光和其他医学图像，以生成高精度的长篇回复。边缘部署可以帮助患者离线回答问题，从而确保模型的应急可用性，并使患者的病情信息保持本地化。令人兴奋的是，已经出现了基于专业医学领域预训练模型进行微调的模型，如BioMistral-7B（Labrak等人，2024）、华佗GPT-7B-II（Chen等人，2023）等。这些低参数模型有潜力部署在终端设备上。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       科学研究支持：传统的研究支持大语言模型，如GatorTronGPT（Peng等人，2023），使用大量特定的专业数据进行训练。这使它们能够生成高质量的专业文本，从而加速科学研究的进展，特别是在数据稀缺或敏感的研究领域。改用设备端大语言模型后，可以降低使用语言模型辅助科学研究任务的硬件成本，获得更快的响应，并保护科学研究信息的机密性。
      </span>
     </li>
     <li style="text-align:justify">
      <strong>
       <span style="color:#0d0016">
        陪伴机器人：已经有一些研究案例使用语言模型来增强机器人或物联网（IoT）设备的能力（Ahn等人，2022；Xu等人，2024a）。大语言模型强大的规划和推理能力可以将人类指令分解为一系列文本子任务，使机器人能够更好地理解自然语言指令（Zeng等人，2023b）。例如，基于OpenAI多模态语言模型的Figure 01机器人可以与人进行深入交流，并根据对话内容做出独立决策和行动（AI，2024c）。随着小尺寸模型的兴起，部署设备端语言模型的机器人在响应生成速度方面可以超越传统的基于云模型的机器人。同时，客户端模型可以确保机器人在离线时仍能保持其智能能力。
       </span>
      </strong>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       残障辅助：对于视障用户来说，将图像转换为文本是一项非常基本且重要的功能。目前，有许多设备端大型多模态模型，如Octopus v3（Chen &amp; Li，2024b）、MiniCPM-Llama3-V 2.5（清华大学，2024），可以通过多模态能力实现这一功能。有了这些模型，盲人也能轻松了解对话中的图片和视频信息。谷歌即将推出基于Gemini Nano的Talkback功能，帮助盲人或视力低下的人更丰富、清晰地描述图像中发生的事情（谷歌，2024b）。由于Gemini Nano是部署在边缘的模型，这些描述会快速出现，并且即使没有网络连接也能工作。类似的能力也可以用于手语识别，有项目使用ChatGPT模型进行手语翻译（Sincan等人，2024）。相比之下，设备端模型可以以更低的延迟生成与手语对应的文本翻译，并确保其离线可用性。
      </span>
     </li>
     <li style="text-align:justify">
      <span style="color:#0d0016">
       自动驾驶汽车：使用语言模型驱动自动驾驶汽车可能是未来的理想场景，但如今我们已经有了相关示例。DriveVLM Dual是一个将自动驾驶技术与大规模视觉语言模型（VLM）相结合的系统，用于提高对城市环境中复杂和长尾场景的理解。该系统使用语言描述驾驶环境并识别场景中的关键对象，从元动作和决策描述逐步制定到路标点的计划。DriveVLM在公共基准测试和研究人员自己的基准测试中都超越了现有的最先进方法，特别是在处理复杂和动态场景方面。令人兴奋的是，DriveVLM可以在汽车本地部署，这也为其即时响应提供了便利（Tian等人，2024）。
      </span>
     </li>
    </ol>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      7 未来方向与开放挑战
     </span>
    </h2>
    <p class="img-center">
     <img alt="" height="431" src="https://i-blog.csdnimg.cn/direct/15e99d034956412eb321949bf5e14937.png" width="431"/>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      图6 设备端大语言模型的未来方向与开放挑战
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      \
      <br/>
      随着设备端大语言模型的不断发展，几个关键领域成为未来有前景的研究和开发方向。设备端大语言模型领域正在迅速发展，这是由对边缘设备上1）数据安全、2）低延迟和3）个性化人工智能体验日益增长的需求所驱动的。像TinyLlama（Zhang等人，2024c）、MobileVLM（Murthy等人，2024；Chu等人，2024）等近期的发展，以及OpenELM（Mehta等人，2024）等新颖方法，都体现了这一进展。然而，在资源受限的设备上部署大语言模型带来了与传统基于云的实现显著不同的独特挑战。这些挑战涵盖多个领域，包括模型压缩、高效推理、安全性、能源效率以及与各种硬件平台的无缝集成。此外，边缘环境的动态特性和对持续适应的需求引入了必须考虑的额外复杂性。我们概述了推进设备端大语言模型领域最紧迫的挑战和机遇。通过确定这些关键领域并激发在开发更强大、高效和可靠的设备端语言模型方面的创新，我们旨在为未来的研究工作提供见解。需要注意的是，这里讨论的挑战和机遇是相互关联的：一个领域的进展往往会对其他领域产生影响。因此，一种考虑设备端大语言模型部署不同方面之间相互作用的整体方法，对于在该领域取得重大进展至关重要。我们深入研究当前的研究现状，确定关键挑战并提出未来工作的潜在方向，总结在图6中。通过应对这些挑战，研究人员和从业者可以突破设备端大语言模型的可能性边界，最终在各种应用和领域中实现更智能、高效和以用户为中心的计算体验。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.1 数据安全技术
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      设备端语言模型可能具有内在的数据安全优势，因为所有数据都可以保持本地化。未来的工作应集中在：
      <br/>
      - 开发高效的隐私技术，包括查询混淆（Yuan等人，2024）、Prompt调整（Li等人，2023b）和先进的随机化技术（Zhang等人，2024e），在保证数据安全的同时，平衡模型效用和计算约束。
      <br/>
      - 加强风险评估和监测，创建复杂的基准测试系统（Yuan等人，2024），实施实时监测（Das等人，2024），并设计系统以检测和减轻推理过程中潜在的个人可识别信息（PII）泄露（Kim等人，2024d）。
      <br/>
      - 优化模型架构和通信策略，专注于高效的模型分片（Yang等人，2024a）、增强安全性的架构（Yao等人，2024a）以及最小化数据传输（Wang等人，2023）。
      <br/>
      - 解决协作和分布式学习场景中的安全挑战，通过安全多方计算（Das等人，2024）、保护长时间对话的数据（Yuan等人，2024）以及扩展像隐私优先推理委托框架（PFID）这样的框架，以支持更广泛的大语言模型架构和任务（Yang等人，2024a）。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.2 自适应边缘 - 云协作
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      随着设备端语言模型的不断发展，边缘计算和云基础设施之间的协同作用既带来了机遇也带来了挑战。未来关于设备端大语言模型自适应边缘 - 云协作的研究应探索：
      <br/>
      - 发明先进的缓存和请求分析技术，包括复杂的向量数据库缓存策略、针对各种大语言模型请求的特征提取模型（Yao等人，2024c），以及不确定性引导的token采样方法，以优化边缘设备和云服务器之间的数据传输（Wang等人，2024a）。
      <br/>
      - 设计智能调度和资源分配算法，纳入个性化推理调度（Yao等人，2024c）、针对异构基础设施的自适应资源分配（Yang等人，2024c），以及考虑批大小的优化技术，以在边缘 - 云环境中高效分配大语言模型组件和工作负载（Zhang等人，2024b）。
      <br/>
      - 创造高效的知识转移和模型压缩方法，如用于多模态大语言模型的基于适配器的知识蒸馏（Zhang等人，2024f）、针对各种大语言模型架构的动态量化技术，以及自适应权重更新压缩策略，以实现语言模型在资源受限设备上的有效部署（Wang等人，2024a）。
      <br/>
      - 通过开发用于token级协作的自适应控制机制（Yang等人，2024c）、用于实时决策的高效约束满足算法，以及减少混合边缘 - 云系统中延迟和改进流水线执行的创新技术（Hao等人，2024；Zhang等人，2024b），提高协作系统的性能优化。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.3 多模态和跨模态学习
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      随着大语言模型扩展到包含多种模态，对适合设备端部署的高效多模态架构的需求日益增长（Carreira等人，2023；Liu等人，2024c）。关键研究方向包括：
     </span>
    </p>
    <ul>
     <li style="text-align: justify;">
      -
      <strong>
       开发高效的多模态处理和压缩技术
      </strong>
      ，包括先进的
      <strong>
       不确定性引导的token采样
      </strong>
      方法、用于
      <strong>
       云到设备模型
      </strong>
      更新的
      <strong>
       动态权重更新
      </strong>
      压缩策略（Wang等人，2024a；McKinzie等人，2024），以及为设备端模型有效组合音频、文本和视频等多种模态的创新方法（Wagner等人，2024）。
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 增强
       <strong>
        知识转移
       </strong>
       和
       <strong>
        适应
       </strong>
       能力，例如探索用于将知识从更大的云模型转移到更小的设备端模型的先进基于适配器的知识蒸馏方法，提高跨模态的少样本和零样本能力（Chen等人，2024a；Han等人，2024；McKinzie等人，2024），以及研究结合生成式和检索式方法的混合方法用于多模态内容生成（Wu等人，2023c）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 扩展模态支持并改善多模态理解，通过开发用于非图像模态的大规模数据集，设计用于对高分辨率图像、长视频序列和复杂音频输入进行细粒度多模态理解的新编码器（Han等人，2024），以及纳入对网页、3D视觉、热图和表格/图形等更多模态和任务的支持（Wu等人，2023c）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 推进
       <strong>
        时间和上下文处理
       </strong>
       能力，研究结合先前交互特征的更长上下文窗口，开发用于处理和理解跨模态时间和顺序信息的复杂技术，以及探索在与虚拟助手交互过程中有用的任务，如音频字幕和声学场景分类（Wagner等人，2024）。
      </span>
     </li>
    </ul>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.4 资源高效解决方案
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      在边缘设备上部署大语言模型引发了对能源消耗和环境影响的担忧。未来的研究应优先考虑：
      <br/>
      - 创建高效的
      <strong>
       模型压缩和执行
      </strong>
      算法：开发用于大语言模型的先进剪枝、量化和知识蒸馏技术。探索针对大于内存的模型的优化执行方法。研究动态和自适应推理技术，根据输入和可用资源调整模型复杂度（Bai等人，2024）。
      <br/>
      - 利用
      <strong>
       模型稀疏性
      </strong>
      ：研究利用语言模型运行时激活稀疏性的技术，即在给定任务中仅激活模型的一小部分。这可以显著减少推理时间和内存占用，实现更高效的模型规模扩展（Xu等人，2024b）。
      <br/>
      - 开发
      <strong>
       能源感知的训练和部署策略
      </strong>
      ，包括节能算法和运行时优化（Bai等人，2024）。探索自适应参数高效微调方法，在边缘设备上平衡安全性、能源效率和性能（He等人，2024）。
     </span>
    </p>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.5 软硬件协同设计
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      硬件和软件开发之间更紧密的集成对于优化设备端大语言模型的性能至关重要。未来的研究方向包括：
     </span>
    </p>
    <ul>
     <li style="text-align: justify;">
      - 推进针对各种内存类型的内存内处理（PIM）/近内存处理（PNM）架构，包括对基于计算快速链接（CXL）系统的优化以及针对边缘设备的低功耗解决方案（Kim等人，2024b）。
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 开发硬件感知的优化技术，如剪枝感知量化、上下文稀疏性利用（Wan等人，2024）和动态稀疏注意力优化（Kachris，2024）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 增强专用人工智能编译器和运行时系统，以自动识别并针对PIM/PNM硬件优化操作（Huang等人，2024b），同时考虑图级和硬件特定的优化（Kim等人，2024b；Wan等人，2024）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 为边缘计算和多设备系统设计高效策略，包括动态稀疏树优化（Luk等人，2024）、自适应比特宽度技术和能源感知协同设计方法。
      </span>
     </li>
    </ul>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.6 稳健性和可靠性
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      确保设备端语言模型在各种运行条件下的稳健性和可靠性，对于其广泛应用至关重要。未来的工作应解决：
     </span>
    </p>
    <ul>
     <li style="text-align: justify;">
      - 研究检测和减轻设备端大语言模型输出中潜在偏差和幻觉的方法，特别是在安全关键应用中（Ailem等人，2024）。
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 探索用于评估设备端语言模型在现实场景中可靠性的形式验证和确认框架（Zhang等人，2023b）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 利用集成方法减少方差和偏差（Xu &amp; Sen，2023；2024）。探索概率推理方法，以量化并在大语言模型管道中传播不确定性。
      </span>
     </li>
    </ul>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.7 可扩展性和部署优化
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      有效地扩展设备端大语言模型以支持不断增长的用户数量和应用程序，面临着重大挑战。未来的研究应探索：
     </span>
    </p>
    <ul>
     <li style="text-align: justify;">
      - 开发用于在异构边缘设备上进行分布式大语言模型推理的动态资源分配和负载平衡技术（Yang等人，2024c；Wilkins等人，2024）。
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 研究在协作边缘计算场景中减少延迟和提高吞吐量的优化策略，可能利用模型分片和流水线推理等技术（Zhang等人，2024b；Dhar等人，2024）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 探索在各种边缘设备上管理和更新多个大语言模型版本的有效方法，考虑网络约束和设备能力等因素。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 构建网络基础设施，以提高模型和数据集的可重用性和可重复性（Wolf等人，2019；Lhoest等人，2021；Deng等人，2019）。
      </span>
     </li>
    </ul>
    <h3 style="text-align:justify">
     <span style="color:#0d0016">
      7.8 持续学习和个性化
     </span>
    </h3>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      设备端大语言模型的部署为个性化人工智能体验提供了前所未有的机会。然而，它也带来了在保持模型相关性以及随时间适应新信息和用户偏好方面的独特挑战。未来的研究应集中在：
     </span>
    </p>
    <ul>
     <li style="text-align: justify;">
      - 实现可控的知识保留和遗忘，例如在模型遇到新数据流时，有选择地保留或遗忘信息。这对于管理错误信息和确保持续准确性至关重要。
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 增强模型基于用户交互和本地数据自主学习新技能和提升现有能力的能力（Li等人，2024d）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 开发有效的历史跟踪机制，以了解大语言模型在各个学习阶段的演变（Qi等人，2024）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 推进理论基础和实际优化，通过为理解和预测设备端环境中持续学习的大语言模型行为建立稳健的理论基础。这还包括进行大规模用户研究，以完善个性化框架，并确定在不同用户群体和场景中有效的服务交付方式（Zhang等人，2024d），以及改进关键生成和检索过程，以便在向量空间中更好地表示任务分布（Peng等人，2024）。
      </span>
     </li>
     <li style="text-align: justify;">
      <span style="color:#0d0016">
       - 开发高效的持续学习机制，包括复杂的数据混合策略和高效的重放样本选择（Shi等人，2024）。这包括探索可控的记忆系统，并设计用于持续模型适应的自适应微调机制（Wu等人，2024；Li等人，2024d）。
      </span>
     </li>
    </ul>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      展望这些未来的发展路径和未解决的问题（Gao等人，2024；Su等人，2024；Schwartz等人，2023；Mahmood等人，2023；Zhao等人，2024a），研究人员和从业者有机会将设备端大语言模型推向新的高度，并改变边缘计算的格局。这些技术的有效进步和整合有潜力解锁用于智能和定制应用的创新框架，同时解决围绕安全性、效率和可靠性的关键问题。这些进步的影响远远超出了理论上的增强，为广泛领域的实质性变革提供了潜力。在移动计算领域，基于增强型设备端大语言模型的人工智能代理（Chen &amp; Li，2024c）有潜力促进先进的自然语言界面和情境感知服务，从而显著提升用户体验。在物联网应用中，这些进步使系统能够更自主、更灵活地实时处理复杂的语言输入，即使在资源受限的环境中也是如此。在汽车领域，改进后的设备端大语言模型可以提升自动驾驶汽车中的人机交互。此外，这些技术可以在医疗保健领域实现更个性化、响应更迅速的人工智能辅助患者护理。这些进步旨在使复杂的人工智能能力更普及，使其在广泛的设备和用例中更易获取、更高效。因此，该领域的持续研究和开发在技术上是必要的，在社会上也具有重要意义，有望开创一个更易获取、更高效、更可靠的人工智能驱动应用的新时代，对社会和行业的各个方面产生积极影响。
     </span>
    </p>
    <h2 style="text-align:justify">
     <span style="color:#0d0016">
      8 结论
     </span>
    </h2>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      本全面综述阐明了设备端语言模型的当前发展水平。本文所进行的广泛分析突出了在模型压缩技术、高效架构设计和软硬件协同优化策略方面的重大进展，这些进展共同促进了在资源受限的边缘设备上部署复杂语言模型。这些改进的潜在影响广泛，能够在不同行业和应用中提供更好的数据保护、减少延迟，并让人们平等地获取先进的人工智能能力。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      从以云为中心到基于边缘的大语言模型部署的转变，不仅仅是一种技术进步，它代表了人机交互范式的转变。通过将先进的自然语言处理能力直接带到终端用户设备上，这种转变为个性化、情境感知和即时的人工智能体验开辟了新途径。从移动电话和物联网到医疗保健和自动驾驶系统，设备端大语言模型将彻底改变用户交互方式，并推动更智能、响应更迅速的技术发展。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      然而，实现无处不在的设备端大语言模型的道路面临着重大挑战。在模型性能与边缘设备固有的资源限制之间找到最佳平衡仍然是一个关键的研究问题。确保模型在异构操作条件下的稳健性，以及开发有效的持续学习机制，也带来了额外的障碍。此外，随着设备端人工智能边界的不断拓展，关于能源效率、可持续性和负责任部署的问题变得越来越突出，这需要创新的解决方案和谨慎的伦理考量。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      要充分发挥设备端语言模型的潜力，需要多学科的共同努力。研究界必须继续推进模型压缩技术和高效架构设计的前沿，同时解决数据安全和系统可靠性的潜在问题。该领域的从业者应探索新颖的软硬件协同设计方法和自适应边缘 - 云协作策略，以优化实际部署。行业利益相关者在开发专用硬件加速器和推广设备端人工智能部署的开放标准方面发挥着关键作用。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      随着该领域研究的不断发展，设备端语言模型处于即将到来的技术突破的前沿。越来越高效的模型、更强大的边缘硬件和创新的部署策略的融合，有望开启人机交互的无限可能。通过应对本综述中提出的挑战并抓住机遇，研究界可以朝着一个未来努力，在这个未来中，复杂的人工智能能力无缝融入日常生活，在尊重个性化和个体差异的同时增强人类能力。迈向无处不在的智能计算的旅程已经在顺利进行，设备端大语言模型有望在塑造这个激动人心的未来中发挥关键作用。
     </span>
    </p>
    <p style="text-align:justify">
     <span style="color:#0d0016">
      总之，本综述为研究人员和从业者提供了全面的资源，深入分析了设备端大语言模型的当前状态，并阐明了未来研究和开发的关键领域。随着设备端大语言模型领域的快速发展，研究界必须致力于应对挑战，并抓住这项变革性技术带来的机遇。
     </span>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f546f6b795f6d696e2f:61727469636c652f64657461696c732f313436303633323539" class_="artid" style="display:none">
 </p>
</div>


