---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f5261696e7369726975732f:61727469636c652f64657461696c732f313436323131363535"
layout: post
title: "Milvus-中常见相似度度量方法"
date: 2025-03-12 18:16:57 +08:00
description: "在 Milvus 中，相似度度量方法用于衡量向量之间的相似程度，不同的度量方法有不同的特点、优缺点和适用场景。以下是对 Milvus 中常见相似度度量方法的详细介绍以及对应的search参数示例。"
keywords: "Milvus 中常见相似度度量方法"
categories: ['未分类']
tags: ['算法', '机器学习', 'Milvus']
artid: "146211655"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146211655
    alt: "Milvus-中常见相似度度量方法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146211655
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146211655
cover: https://bing.ee123.net/img/rand?artid=146211655
image: https://bing.ee123.net/img/rand?artid=146211655
img: https://bing.ee123.net/img/rand?artid=146211655
---

# Milvus 中常见相似度度量方法

在 Milvus 中，相似度度量方法用于衡量向量之间的相似程度，不同的度量方法有不同的特点、优缺点和适用场景。以下是对 Milvus 中常见相似度度量方法的详细介绍以及对应的
`search`
参数示例。

#### 1. 欧氏距离（L2 Distance， `L2` ）

##### 特点

欧氏距离是最常用的距离度量方法之一，它计算的是两个向量在欧几里得空间中的直线距离。对于两个

n
n





n
维向量

x
⃗
=
(
x
1
,
x
2
,
⋯
 
,
x
n
)
\vec{x}=(x\_1,x\_2,\cdots,x\_n)












x




=





(


x









1

​


,




x









2

​


,



⋯





,




x









n

​


)
和

y
⃗
=
(
y
1
,
y
2
,
⋯
 
,
y
n
)
\vec{y}=(y\_1,y\_2,\cdots,y\_n)












y


​




=





(


y









1

​


,




y









2

​


,



⋯





,




y









n

​


)
，欧氏距离的计算公式为：
  
[d(\vec{x},\vec{y})=\sqrt{\sum\_{i = 1}^{n}(x\_i - y\_i)^2}]

##### 优点

* **直观易懂**
  ：欧氏距离的几何意义明确，容易理解和解释。
* **广泛应用**
  ：在许多领域都有广泛的应用，例如图像处理、计算机视觉等。

##### 缺点

* **对向量的尺度敏感**
  ：如果向量的各个维度的尺度差异较大，可能会导致距离计算结果受到较大影响。
* **计算复杂度较高**
  ：在高维空间中，欧氏距离的计算量相对较大。

##### 适用场景

* **向量空间分布较为均匀的场景**
  ：当向量在空间中的分布比较均匀时，欧氏距离能够较好地反映向量之间的相似性。
* **对向量的绝对值差异比较敏感的场景**
  ：例如在一些需要考虑向量具体数值差异的应用中，欧氏距离比较适用。

##### `search` 参数示例

```python
search_params = {
    "metric_type": "L2",
    "params": {
        # 对于 IVF 索引，nprobe 表示搜索时要访问的聚类中心数量
        "nprobe": 10  
    }
}

```

#### 2. 内积（Inner Product， `IP` ）

##### 特点

内积是两个向量对应元素乘积之和。对于两个

n
n





n
维向量

x
⃗
=
(
x
1
,
x
2
,
⋯
 
,
x
n
)
\vec{x}=(x\_1,x\_2,\cdots,x\_n)












x




=





(


x









1

​


,




x









2

​


,



⋯





,




x









n

​


)
和

y
⃗
=
(
y
1
,
y
2
,
⋯
 
,
y
n
)
\vec{y}=(y\_1,y\_2,\cdots,y\_n)












y


​




=





(


y









1

​


,




y









2

​


,



⋯





,




y









n

​


)
，内积的计算公式为：
  
[\vec{x}\cdot\vec{y}=\sum\_{i = 1}^{n}x\_iy\_i]

##### 优点

* **计算效率高**
  ：内积的计算相对简单，在大规模向量搜索中可以提高计算效率。
* **适用于向量的方向比较重要的场景**
  ：内积可以反映向量之间的方向相似性。

##### 缺点

* **对向量的长度敏感**
  ：如果向量的长度差异较大，内积的结果可能会受到较大影响。

##### 适用场景

* **文本挖掘和信息检索领域**
  ：在这些领域中，向量通常表示文本的特征，内积可以用来衡量文本之间的相关性。
* **基于向量方向的相似性比较场景**
  ：例如在推荐系统中，通过比较用户向量和物品向量的内积来推荐相关物品。

##### `search` 参数示例

```python
search_params = {
    "metric_type": "IP",
    "params": {
        # 对于 HNSW 索引，ef 表示搜索时的动态访问列表大小
        "ef": 20  
    }
}

```

#### 3. 余弦相似度（Cosine Similarity， `COSINE` ）

##### 特点

余弦相似度通过计算两个向量的夹角余弦值来衡量它们的相似性。它将向量的长度归一化，只考虑向量的方向。对于两个非零向量

x
⃗
\vec{x}












x

和

y
⃗
\vec{y}












y


​

，余弦相似度的计算公式为：
  
[cos(\theta)=\frac{\vec{x}\cdot\vec{y}}{|\vec{x}||\vec{y}|}]
  
其中，

∥
x
⃗
∥
\|\vec{x}\|





∥








x


∥
和

∥
y
⃗
∥
\|\vec{y}\|





∥








y


​


∥
分别是向量

x
⃗
\vec{x}












x

和

y
⃗
\vec{y}












y


​

的模。

##### 优点

* **不受向量长度影响**
  ：余弦相似度只关注向量的方向，不考虑向量的长度，因此在比较不同长度的向量时具有优势。
* **适用于高维空间**
  ：在高维空间中，余弦相似度能够较好地反映向量之间的相似性。

##### 缺点

* **不考虑向量的具体数值差异**
  ：余弦相似度只关注向量的方向，对于向量的具体数值差异不敏感。

##### 适用场景

* **文本相似度计算**
  ：在自然语言处理中，常用于计算文档之间的相似度。
* **图像特征匹配**
  ：在计算机视觉中，用于比较图像的特征向量。

##### `search` 参数示例

```python
search_params = {
    "metric_type": "COSINE",
    "params": {
        # 对于 ANNOY 索引，search_k 表示搜索时访问的节点数量
        "search_k": 100  
    }
}

```

#### 4. Hamming 距离（Hamming Distance， `HAMMING` ）

##### 特点

Hamming 距离主要用于二进制向量，它计算的是两个二进制向量中对应位不同的位数。例如，对于二进制向量

x
⃗
=
(
0
,
1
,
0
,
1
)
\vec{x}=(0,1,0,1)












x




=





(

0

,



1

,



0

,



1

)
和

y
⃗
=
(
1
,
1
,
0
,
0
)
\vec{y}=(1,1,0,0)












y


​




=





(

1

,



1

,



0

,



0

)
，它们的 Hamming 距离为 2。

##### 优点

* **计算简单**
  ：Hamming 距离的计算只需要比较二进制位，计算复杂度较低。
* **适用于二进制数据**
  ：在处理二进制编码的向量时，Hamming 距离是一种非常合适的度量方法。

##### 缺点

* **只适用于二进制向量**
  ：对于非二进制向量，Hamming 距离无法使用。

##### 适用场景

* **哈希算法和二进制编码数据**
  ：例如在图像检索中，使用哈希算法将图像特征编码为二进制向量，然后使用 Hamming 距离进行快速匹配。

##### `search` 参数示例

```python
search_params = {
    "metric_type": "HAMMING",
    "params": {}
}

```

#### 5. Jaccard 距离（Jaccard Distance， `JACCARD` ）

##### 特点

Jaccard 距离用于衡量两个集合之间的不相似性，它是 Jaccard 相似度的补集。对于两个集合

A
A





A
和

B
B





B
，Jaccard 相似度的计算公式为：
  
[J(A,B)=\frac{|A\cap B|}{|A\cup B|}]
  
Jaccard 距离的计算公式为：
  
[d\_J(A,B)=1 - J(A,B)]

##### 优点

* **适用于集合数据**
  ：Jaccard 距离能够很好地处理集合数据，反映集合之间的重叠程度。

##### 缺点

* **对集合的大小和元素分布敏感**
  ：Jaccard 距离的结果可能会受到集合大小和元素分布的影响。

##### 适用场景

* **文本分类和聚类**
  ：在文本处理中，可以将文本表示为词的集合，然后使用 Jaccard 距离进行文本的分类和聚类。
* **社交网络分析**
  ：用于分析用户之间的兴趣相似度，例如比较用户关注的话题集合。

##### `search` 参数示例

```python
search_params = {
    "metric_type": "JACCARD",
    "params": {}
}

```

综上所述，在选择相似度度量方法时，需要根据数据的特点和具体的应用场景来进行综合考虑。不同的度量方法在不同的场景下可能会有不同的表现，合理选择度量方法可以提高向量搜索的准确性和效率。