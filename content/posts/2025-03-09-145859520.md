---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f36303034303330362f:61727469636c652f64657461696c732f313435383539353230"
layout: post
title: "äººä½“éª¨æž¶è¯†åˆ«æ–‡çŒ®é˜…è¯»ST-TRåŸºäºŽæ—¶ç©ºTransformerç½‘ç»œçš„éª¨æž¶åŠ¨ä½œè¯†åˆ«"
date: 2025-03-09 12:35:00 +0800
description: "æœ¬å‘¨é˜…è¯»çš„è®ºæ–‡é¢˜ç›®æ˜¯ã€ŠSkeleton-based action recognition via spatial and temporal transformer networksã€‹(ã€ŠåŸºäºŽæ—¶ç©ºTransformerç½‘ç»œçš„éª¨æž¶åŠ¨ä½œè¯†åˆ«ã€‹)ã€‚åœ¨å‰å‡ å‘¨ä¸­å­¦ä¹ çš„ST-GCNä»¥åŠåŸºäºŽST-GCNåšå‡ºæ”¹è¿›çš„2s-AGCNå’ŒDGNNåœ¨éª¨éª¼å›¾è¿™ç±»éžæ¬§å‡ é‡Œæ•°æ®ä¸Šçš„ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚ä½†æ˜¯ä¾æ—§ä¸èƒ½å¯¹3Déª¨éª¼ä¸­æ½œåœ¨ä¿¡æ¯çš„æœ‰æ•ˆç¼–ç è¿›è¡Œæå–ã€‚"
keywords: "äººä½“éª¨æž¶è¯†åˆ«æ–‡çŒ®é˜…è¯»â€”â€”ST-TRï¼šåŸºäºŽæ—¶ç©ºTransformerç½‘ç»œçš„éª¨æž¶åŠ¨ä½œè¯†åˆ«"
categories: ['æ–‡çŒ®é˜…è¯»']
tags: ['äººå·¥æ™ºèƒ½']
artid: "145859520"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145859520
    alt: "äººä½“éª¨æž¶è¯†åˆ«æ–‡çŒ®é˜…è¯»ST-TRåŸºäºŽæ—¶ç©ºTransformerç½‘ç»œçš„éª¨æž¶åŠ¨ä½œè¯†åˆ«"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145859520
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145859520
cover: https://bing.ee123.net/img/rand?artid=145859520
image: https://bing.ee123.net/img/rand?artid=145859520
img: https://bing.ee123.net/img/rand?artid=145859520
---

# äººä½“éª¨æž¶è¯†åˆ«æ–‡çŒ®é˜…è¯»â€”â€”ST-TRï¼šåŸºäºŽæ—¶ç©ºTransformerç½‘ç»œçš„éª¨æž¶åŠ¨ä½œè¯†åˆ«

---

## æ‘˜è¦

æœ¬å‘¨é˜…è¯»çš„è®ºæ–‡é¢˜ç›®æ˜¯ã€ŠSkeleton-based action recognition via spatial and temporal transformer networksã€‹(ã€Š

åŸºäºŽæ—¶ç©ºTransformerç½‘ç»œçš„éª¨æž¶åŠ¨ä½œè¯†åˆ«
ã€‹)ã€‚åœ¨å‰å‡ å‘¨ä¸­å­¦ä¹ çš„ST-GCNä»¥åŠåŸºäºŽST-GCNåšå‡ºæ”¹è¿›çš„2s-AGCNå’ŒDGNNåœ¨éª¨éª¼å›¾è¿™ç±»éžæ¬§å‡ é‡Œæ•°æ®ä¸Šçš„ç©ºé—´å’Œæ—¶é—´ä¾èµ–æ€§æ–¹é¢æ˜¯æœ‰æ•ˆçš„ã€‚ä½†æ˜¯ä¾æ—§ä¸èƒ½å¯¹3Déª¨éª¼ä¸­æ½œåœ¨ä¿¡æ¯çš„æœ‰æ•ˆç¼–ç è¿›è¡Œæå–ã€‚ç”±æ­¤ï¼Œæœ¬æ–‡ä¸­æå‡ºäº†ä¸€ç§æ–°çš„æ—¶ç©ºå˜æ¢å™¨ç½‘ç»œï¼ˆST-TRï¼‰ï¼Œè¯¥ç½‘ç»œä½¿ç”¨Transformerè‡ªæ³¨æ„åŠ›ç®—å­æ¥å»ºæ¨¡å…³èŠ‚ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä½¿ç”¨ç©ºé—´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆSSAï¼‰æ¥ç†è§£ä¸åŒèº«ä½“éƒ¨åˆ†ä¹‹é—´çš„å¸§å†…äº¤äº’ï¼Œä»¥åŠä½¿ç”¨æ—¶é—´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆTSAï¼‰æ¥å»ºæ¨¡å¸§é—´ç›¸å…³æ€§ã€‚ç„¶åŽå°†SSAå’ŒTSAåœ¨åŒæµç½‘ç»œä¸­ç»“åˆï¼ŒæŒç»­æå‡äº†éª¨éª¼å›¾çš„è¯†åˆ«ç»“æžœå’Œæ€§èƒ½ã€‚

## Abstract

The title of the paper read this week is "Skeleton-based action recognition via spatial and temporal transformer networks" . In the previous few weeks, the ST-GCN and the improved 2s-AGCN and DGNN based on ST-GCN have been effective in handling the spatial and temporal dependencies on non-Euclidean data such as skeletal graphs. However, they still cannot effectively extract the potential information encoding in 3D skeletons. Therefore, this paper proposes a new spatial-temporal transformer network (ST-TR), which uses transformer self-attention operators to model the dependency relationships between joints, uses spatial self-attention modules (SSA) to understand the intra-frame interactions between different body parts, and uses temporal self-attention modules (TSA) to model inter-frame correlations. Then, the SSA and TSA are combined in a dual-stream network, continuously improving the recognition results and performance of skeletal graphs.

> æ–‡çŒ®é“¾æŽ¥ðŸ”—ï¼š
> [Skeleton-based action recognition via spatial and temporal transformer networks](https://arxiv.org/pdf/2008.07404 "Skeleton-based action recognition via spatial and temporal transformer networks")

## 1 å¼•è¨€

å¦‚ä»Šï¼ŒåŸºäºŽéª¨æž¶çš„åŠ¨ä½œè¯†åˆ«æœ€å¹¿æ³›çš„æ–¹æ³•æ˜¯GNNï¼Œç‰¹åˆ«æ˜¯GCNï¼Œå› ä¸ºå®ƒä»¬æ˜¯éžæ¬§å‡ é‡Œå¾—æ•°æ®çš„æœ‰æ•ˆè¡¨ç¤ºï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•èŽ·ç©ºé—´ï¼ˆå¸§å†…ï¼‰å’Œæ—¶é—´ï¼ˆå¸§é—´ï¼‰ä¿¡æ¯ã€‚ä»¥ST-GCNä¸ºé¦–çš„GCNæ¨¡åž‹è¢«å¼€å§‹å¼•å…¥åˆ°åŸºäºŽéª¨æž¶çš„åŠ¨ä½œè¯†åˆ«ä¸­ï¼ŒST-GCNé€šè¿‡åœ¨ç©ºé—´ä¸Šæ“ä½œéª¨éª¼éª¨è¿žæŽ¥æ¥å¤„ç†ç©ºé—´ä¿¡æ¯ï¼Œå¹¶ä¸”é€šè¿‡è€ƒè™‘æ¯ä¸ªéª¨éª¼å…³èŠ‚æ²¿æ—¶é—´é¢å¤–çš„æ—¶åºè¿žæŽ¥æ¥èŽ·å–ä¿¡æ¯ã€‚å°½ç®¡åœ¨éª¨éª¼æ•°æ®ä¸Šå·²è¢«è¯æ˜Žè¡¨çŽ°è‰¯å¥½ï¼ŒST-GCNæ¨¡åž‹å­˜åœ¨ä¸€äº›ç»“æž„é™åˆ¶ï¼Œå…¶ä¸­ä¸€äº›å·²è¢«2s-AGCNã€DGNNç­‰ç®—æ³•è§£å†³ï¼Œä½†æ˜¯ä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼š

* è¡¨ç¤ºäººä½“çš„å›¾æ‹“æ‰‘å¯¹æ‰€æœ‰å±‚å’Œæ‰€æœ‰åŠ¨ä½œéƒ½æ˜¯å›ºå®šçš„ã€‚è¿™å¯èƒ½ä¼šé˜»æ­¢åœ¨æ—¶é—´ä¸Šæå–ä¸°å¯Œçš„éª¨éª¼è¿åŠ¨è¡¨ç¤ºï¼Œå°¤å…¶æ˜¯åœ¨å›¾è¿žæŽ¥æ˜¯å®šå‘çš„ï¼Œä¿¡æ¯åªèƒ½æ²¿ç€é¢„å®šä¹‰è·¯å¾„æµåŠ¨çš„æƒ…å†µä¸‹ï¼›
* ç©ºé—´å’Œæ—¶åºå·ç§¯éƒ½æ˜¯ä»Žæ ‡å‡†çš„2Då·ç§¯å®žçŽ°çš„ã€‚å› æ­¤ï¼Œå®ƒä»¬è¢«é™åˆ¶åœ¨å±€éƒ¨é‚»åŸŸå†…æ“ä½œï¼ŒæŸç§ç¨‹åº¦ä¸Šå—åˆ°å·ç§¯æ ¸å¤§å°çš„é™åˆ¶ï¼›
* åœ¨ä¸Žâ€œæ‹æ‰‹â€ç­‰åŠ¨ä½œç›¸å…³çš„æƒ…å†µä¸‹ï¼Œäººä½“éª¨éª¼ä¸­æœªè¿žæŽ¥çš„å…³èŠ‚ä¹‹é—´çš„ç›¸å…³æ€§ä¼šè¢«ä½Žä¼°ã€‚

ç”±æ­¤ï¼Œæœ¬æ–‡é€šè¿‡é‡‡ç”¨æ”¹è¿›çš„Transformerè‡ªæ³¨æ„åŠ›ç®—å­æ¥åº”å¯¹æ‰€æœ‰è¿™äº›é—®é¢˜å’Œé™åˆ¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨éª¨éª¼å…³èŠ‚ä¸Šé‡‡ç”¨è‡ªæ³¨æ„åŠ›ï¼š

![](https://i-blog.csdnimg.cn/direct/10a4c4df4ff84c198076164f37e3b681.png)

æ­¥éª¤å¦‚ä¸‹ï¼š

1. å¯¹äºŽæ¯ä¸ªèº«ä½“å…³èŠ‚ï¼Œè®¡ç®—ä¸€ä¸ªæŸ¥è¯¢
   ![\mathbf{q}](https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D)
   ã€ä¸€ä¸ªé”®
   ![\mathbf{k}](https://latex.csdn.net/eq?%5Cmathbf%7Bk%7D)
   å’Œä¸€ä¸ªå€¼å‘é‡
   ![\mathbf{v}](https://latex.csdn.net/eq?%5Cmathbf%7Bv%7D)
   ï¼›
2. æ‰§è¡Œå…³èŠ‚æŸ¥è¯¢ä¸Žæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹é”®çš„ç‚¹ç§¯ï¼ˆ
   ![\bigodot](https://latex.csdn.net/eq?%5Cbigodot)
   ï¼‰ï¼Œè¡¨ç¤ºæ¯å¯¹èŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥å¼ºåº¦ï¼›
3. æ¯ä¸ªèŠ‚ç‚¹é€šè¿‡å…¶ä¸Žå½“å‰èŠ‚ç‚¹çš„ç›¸å…³æ€§è¿›è¡Œç¼©æ”¾ï¼›
4. é€šè¿‡å°†åŠ æƒèŠ‚ç‚¹ç›¸åŠ èŽ·å¾—å…¶æ–°ç‰¹å¾ã€‚

äººç±»éª¨éª¼åºåˆ—çš„é¡ºåºæ€§å’Œå±‚æ¬¡ç»“æž„ï¼Œä»¥åŠTransformerè‡ªæ³¨æ„åŠ›åœ¨å»ºæ¨¡é•¿è·ç¦»ä¾èµ–å…³ç³»ä¸­çš„çµæ´»æ€§ï¼Œä½¿Transformeræˆä¸ºè§£å†³ST-GCNå¼±ç‚¹çš„å®Œç¾Žè§£å†³æ–¹æ¡ˆã€‚åœ¨æœ¬æ–‡ä¸­æ—¨åœ¨å°†Transformeråº”ç”¨äºŽæ—¶ç©ºéª¨éª¼æž¶æž„ï¼Œç‰¹åˆ«æ˜¯åº”ç”¨äºŽä»£è¡¨äººç±»éª¨éª¼çš„å…³èŠ‚ï¼Œç›®æ ‡æ˜¯é€šè¿‡å¯¹ç©ºé—´é€šè¿‡ç©ºé—´è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰æ¨¡å—å’Œæ—¶é—´é€šè¿‡æ—¶é—´è‡ªæ³¨æ„åŠ›ï¼ˆTSAï¼‰æ¨¡å—å»ºæ¨¡ï¼Œæ¥æ¨¡æ‹Ÿäººç±»åŠ¨ä½œä¸­çš„é•¿è·ç¦»äº¤äº’ï¼Œä»¥å–å¾—ä¼˜å¼‚çš„æ€§èƒ½ã€‚

## 2 æ—¶ç©ºTransformerç½‘ç»œ

æœ¬æ–‡ä¸­æå‡ºçš„ç©ºé—´-æ—¶é—´Transformerï¼ˆST-TRï¼‰ç½‘ç»œæ˜¯ä¸€ç§ä½¿ç”¨Transformerè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸ŠåŒæ—¶æ“ä½œçš„æž¶æž„ã€‚é€šè¿‡ä½¿ç”¨ä¸¤ä¸ªæ¨¡å—æ¥å®žçŽ°è¿™ä¸€ç›®æ ‡ï¼Œå³ç©ºé—´è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰æ¨¡å—å’Œæ—¶åºè‡ªæ³¨æ„åŠ›ï¼ˆTSAï¼‰æ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—ä¸“æ³¨äºŽæå–ä¸¤ä¸ªç»´åº¦ä¹‹ä¸€çš„ç›¸å…³æ€§ã€‚

åŽŸå§‹Transformerè‡ªæ³¨æ„åŠ›èƒŒåŽçš„æƒ³æ³•æ˜¯å…è®¸ç¼–ç å¥å­ä¸­å•è¯ä¹‹é—´çš„çŸ­è·ç¦»å’Œé•¿è·ç¦»ç›¸å…³æ€§ã€‚æ‰€ä»¥ï¼ŒåŒæ ·çš„æ–¹æ³•ä¹Ÿå¯ä»¥åº”ç”¨äºŽåŸºäºŽéª¨æž¶çš„åŠ¨ä½œè¯†åˆ«ï¼Œå› ä¸ºèŠ‚ç‚¹åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šçš„ç›¸å…³æ€§éƒ½è‡³å…³é‡è¦ã€‚é€šè¿‡å°†æž„æˆéª¨æž¶çš„å…³èŠ‚è§†ä¸ºä¸€ä¸ªè¯è¢‹ï¼Œå¹¶åˆ©ç”¨Transformerè‡ªæ³¨æ„åŠ›æ¥æå–èŠ‚ç‚¹åµŒå…¥ï¼Œè¿™äº›åµŒå…¥ç¼–ç äº†å‘¨å›´å…³èŠ‚ä¹‹é—´çš„å…³ç³»ï¼Œå°±åƒNLPä¸­çŸ­è¯­ä¸­çš„å•è¯ä¸€æ ·ã€‚

ä¸Žæ ‡å‡†å›¾å·ç§¯ä¸åŒï¼Œåœ¨æ ‡å‡†å›¾å·ç§¯ä¸­åªæœ‰ç›¸é‚»èŠ‚ç‚¹è¢«æ¯”è¾ƒï¼Œè€ŒST-TRæ”¾å¼ƒäº†ä»»ä½•é¢„å®šä¹‰çš„éª¨æž¶ç»“æž„ï¼Œè®©Transformerè‡ªæ³¨æ„åŠ›è‡ªåŠ¨å‘çŽ°å¯¹é¢„æµ‹å½“å‰åŠ¨ä½œç›¸å…³çš„å…³èŠ‚å…³ç³»ã€‚è¯¥æ“ä½œçš„ä½œç”¨ç±»ä¼¼äºŽå›¾å·ç§¯ï¼Œä½†å…¶ä¸­çš„æ ¸å€¼æ˜¯åŸºäºŽå‘çŽ°çš„å…³èŠ‚å…³ç³»åŠ¨æ€é¢„æµ‹çš„ã€‚ åŒä¸€æ€æƒ³ä¹Ÿåº”ç”¨äºŽåºåˆ—å±‚é¢ï¼Œé€šè¿‡åˆ†æžæ¯ä¸ªå…³èŠ‚åœ¨åŠ¨ä½œä¸­çš„å˜åŒ–å¹¶å»ºç«‹è·¨è¶Šä¸åŒå¸§çš„é•¿æœŸå…³ç³»ï¼Œç±»ä¼¼äºŽåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æž„å»ºçŸ­è¯­ä¹‹é—´çš„å…³ç³»ã€‚ç»“æžœæ“ä½œç¬¦èƒ½å¤ŸèŽ·å¾—åœ¨ç©ºé—´å’Œæ—¶é—´ç»´åº¦ä¸Šæ‰©å±•çš„åŠ¨æ€è¡¨ç¤ºã€‚

### 2.1 ç©ºé—´è‡ªæ³¨æ„åŠ›ï¼ˆSSAï¼‰

ç©ºé—´è‡ªæ³¨æ„åŠ›æ¨¡å—åœ¨æ¯ä¸ªå¸§å†…åº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œä»¥æå–ä½Žçº§ç‰¹å¾ï¼ŒåµŒå…¥èº«ä½“éƒ¨åˆ†ä¹‹é—´çš„å…³ç³»ã€‚è¿™æ˜¯é€šè¿‡ç‹¬ç«‹è®¡ç®—æ¯å¸§ä¸­æ¯å¯¹å…³èŠ‚ä¹‹é—´çš„ç›¸å…³æ€§æ¥å®žçŽ°çš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/direct/4cea5cf6fe9c481185bfd0d748e10111.png)

å…¶ä¸­ï¼Œç»™å®šæ—¶é—´
![t](https://latex.csdn.net/eq?t)
çš„å¸§ï¼Œå¯¹äºŽéª¨éª¼ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹
![v_{ti}](https://latex.csdn.net/eq?v_%7Bti%7D)
ï¼š

1. é¦–å…ˆï¼Œé€šè¿‡åº”ç”¨å¯è®­ç»ƒçš„çº¿æ€§å˜æ¢åˆ°èŠ‚ç‚¹ç‰¹å¾
   ![\mathbf{n}_i^t\in \mathbb{R}^{ C_{in}}](https://latex.csdn.net/eq?%5Cmathbf%7Bn%7D_i%5Et%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%7D)
   ï¼ˆå‚æ•°
   ![\mathbf{W}_q\in \mathbb{R}^{ C_{in}\times dq}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_q%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%5Ctimes%20dq%7D)
   ã€
   ![\mathbf{W}_k\in \mathbb{R}^{ C_{in}\times dk}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_k%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%5Ctimes%20dk%7D)
   ã€
   ![\mathbf{W}_v\in \mathbb{R}^{ C_{in}\times dv}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_v%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%5Ctimes%20dv%7D)
   ï¼Œæ‰€æœ‰èŠ‚ç‚¹å…±äº«ï¼‰æ¥è®¡ç®—æŸ¥è¯¢å‘é‡
   ![\mathbf{q}_i^t\in \mathbb{R}^{dq}](https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D_i%5Et%5Cin%20%5Cmathbb%7BR%7D%5E%7Bdq%7D)
   ã€é”®å‘é‡
   ![\mathbf{k}_i^t\in \mathbb{R}^{dk}](https://latex.csdn.net/eq?%5Cmathbf%7Bk%7D_i%5Et%5Cin%20%5Cmathbb%7BR%7D%5E%7Bdk%7D)
   å’Œå€¼å‘é‡
   ![\mathbf{v}_i^t\in \mathbb{R}^{dv}](https://latex.csdn.net/eq?%5Cmathbf%7Bv%7D_i%5Et%5Cin%20%5Cmathbb%7BR%7D%5E%7Bdv%7D)
   ï¼›
2. ç„¶åŽï¼Œå¯¹äºŽæ¯å¯¹èº«ä½“èŠ‚ç‚¹
   ![(v_{ti},v_{tj})](https://latex.csdn.net/eq?%28v_%7Bti%7D%2Cv_%7Btj%7D%29)
   åº”ç”¨æŸ¥è¯¢-é”®ç‚¹ç§¯ä»¥èŽ·å¾—æƒé‡
   ![\alpha _{ij}^t=\mathbf{q}_i^t\cdot \mathbf{k}_j^{t^T}\in \mathbb{R}](https://latex.csdn.net/eq?%5Calpha%20_%7Bij%7D%5Et%3D%5Cmathbf%7Bq%7D_i%5Et%5Ccdot%20%5Cmathbf%7Bk%7D_j%5E%7Bt%5ET%7D%5Cin%20%5Cmathbb%7BR%7D)
   ï¼Œ
   ![\forall t\in T](https://latex.csdn.net/eq?%5Cforall%20t%5Cin%20T)
   ï¼Œè¡¨ç¤ºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„ç›¸å…³æ€§å¼ºåº¦ï¼›
3. æœ€åŽï¼Œä½¿ç”¨å¾—åˆ°çš„åˆ†æ•°
   ![\alpha _{ij}^t](https://latex.csdn.net/eq?%5Calpha%20_%7Bij%7D%5Et)
   æ¥åŠ æƒæ¯ä¸ªå…³èŠ‚å€¼
   ![\mathbf{v}_j^t](https://latex.csdn.net/eq?%5Cmathbf%7Bv%7D_j%5Et)
   ï¼Œå¹¶è®¡ç®—åŠ æƒæ±‚å’Œä»¥èŽ·å¾—èŠ‚ç‚¹
   ![v_{ti}](https://latex.csdn.net/eq?v_%7Bti%7D)
   çš„æ–°åµŒå…¥
   ![\mathbf{z}_i^t=\sum _jsoftmax_j(\frac{\alpha _{ij}^t}{\sqrt{d_k}})\mathbf{v}_j^t](https://latex.csdn.net/eq?%5Cmathbf%7Bz%7D_i%5Et%3D%5Csum%20_jsoftmax_j%28%5Cfrac%7B%5Calpha%20_%7Bij%7D%5Et%7D%7B%5Csqrt%7Bd_k%7D%7D%29%5Cmathbf%7Bv%7D_j%5Et)
   ï¼Œ
   ![\mathbf{z}_i^t\in \mathbb{R}^{C_{out}}](https://latex.csdn.net/eq?%5Cmathbf%7Bz%7D_i%5Et%5Cin%20%5Cmathbb%7BR%7D%5E%7BC_%7Bout%7D%7D)
   ï¼ˆè¾“å‡ºé€šé“æ•°ä¸º
   ![C_{out}](https://latex.csdn.net/eq?C_%7Bout%7D)
   ï¼‰ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
multi_matmul = False

class spatial_attention(nn.Module):
    def __init__(self, in_channels, kernel_size, dk, dv, Nh, complete, relative, layer, A, more_channels, drop_connect,
                 adjacency, num, num_point,
                 shape=25, stride=1,
                 last_graph=False, data_normalization=True, skip_conn=True, visualization=True):
        super(spatial_attention, self).__init__()
        self.in_channels = in_channels
        self.complete = complete
        self.kernel_size = 1
        self.dk = dk
        self.dv = dv
        self.num = num
        self.layer = layer
        self.more_channels = more_channels
        self.drop_connect = drop_connect
        self.visualization = visualization
        self.data_normalization = data_normalization
        self.skip_conn = skip_conn
        self.adjacency = adjacency
        self.Nh = Nh
        self.num_point=num_point
        self.A = A[0] + A[1] + A[2]
        if self.adjacency:
            self.mask = nn.Parameter(torch.ones(self.A.size()))
        self.shape = shape
        self.relative = relative
        self.last_graph = last_graph
        self.stride = stride
        self.padding = (self.kernel_size - 1) // 2

        assert self.Nh != 0, "æ•´æ•°é™¤æ³•æˆ–ä»¥é›¶ä¸ºæ¨¡æ•°ï¼ŒNh>=1"
        assert self.dk % self.Nh == 0, "dkåº”é™¤ä»¥Nhã€‚ï¼ˆä¾‹å¦‚ï¼šout_channelsï¼š20ï¼Œdkï¼š40ï¼ŒNhï¼š4ï¼‰"
        assert self.dv % self.Nh == 0, "dvåº”é™¤ä»¥Nh. (ä¾‹å¦‚: out_channels: 20, dv: 4, Nh: 4)"
        assert stride in [1, 2], str(stride) + " Up to 2 strides are allowed."


        if (self.more_channels):

            self.qkv_conv = nn.Conv2d(self.in_channels, (2 * self.dk + self.dv) * self.Nh // self.num,
                                      kernel_size=self.kernel_size,
                                      stride=stride,
                                      padding=self.padding)
        else:
            self.qkv_conv = nn.Conv2d(self.in_channels, 2 * self.dk + self.dv, kernel_size=self.kernel_size,
                                      stride=stride,
                                      padding=self.padding)
        if (self.more_channels):

            self.attn_out = nn.Conv2d(self.dv * self.Nh // self.num, self.dv, kernel_size=1, stride=1)
        else:
            self.attn_out = nn.Conv2d(self.dv, self.dv, kernel_size=1, stride=1)

        if self.relative:
            # åˆå§‹åŒ–ä¸¤ä¸ªå‚æ•°ä»¥å®žçŽ°ç›¸å¯¹ä½ç½®ç¼–ç 
            # åœ¨å¯¹è§’çº¿ä¸Šé‡å¤ä¸€ä¸ªæƒé‡
            # V^2-V+1 å‚æ•°ä½äºŽå¯¹è§’çº¿ä¹‹å¤–çš„ä½ç½®
            if self.more_channels:
                self.key_rel = nn.Parameter(torch.randn(((self.num_point ** 2) - self.num_point, self.dk // self.num), requires_grad=True))
            else:
                self.key_rel = nn.Parameter(torch.randn(((self.num_point ** 2) - self.num_point, self.dk // Nh), requires_grad=True))
            if self.more_channels:
                self.key_rel_diagonal = nn.Parameter(torch.randn((1, self.dk // self.num), requires_grad=True))
            else:
                self.key_rel_diagonal = nn.Parameter(torch.randn((1, self.dk // self.Nh), requires_grad=True))

    def forward(self, x, label, name):
        # è¾“å…¥x
        # (batch_size, channels, 1, joints)
        B, _, T, V = x.size()

        # è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼
        # flat_q, flat_k, flat_v
        # (batch_size, Nh, dvh or dkh, joints)
        # dvh = dv / Nh, dkh = dk / Nh
        # q, k, v obtained by doing 2D convolution on the input (q=XWq, k=XWk, v=XWv)
        flat_q, flat_k, flat_v, q, k, v = self.compute_flat_qkv(x, self.dk, self.dv, self.Nh)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œé€šè¿‡æ‰§è¡Œq*k
        # (batch_size, Nh, joints, dkh)*(batch_size, Nh, dkh, joints) =  (batch_size, Nh, joints,joints)
        # å¦‚æžœç©ºé—´é—®é¢˜ï¼Œä¹˜æ³•ä¹Ÿå¯ä»¥è¢«é™¤ä»¥ ï¼ˆmulti_matmulï¼‰
        if (multi_matmul):
            for i in range(0, 5):
                flat_q_5 = flat_q[:, :, :, (5 * i):(5 * (i + 1))]
                product = torch.matmul(flat_q_5.transpose(2, 3), flat_k)
                if (i == 0):
                    logits = product
                else:
                    logits = torch.cat((logits, product), dim=2)
        else:
            logits = torch.matmul(flat_q.transpose(2, 3), flat_k)

        # é‚»æŽ¥çŸ©é˜µè¢«åŠ æƒå¹¶æ·»åŠ åˆ°transformerçš„attention logitsä¸­åŽ»
        # åŽŸå§‹éª¨æž¶ç»“æž„çš„ä¿¡æ¯
        if (self.adjacency):
            self.A = self.A.cuda(device)
            logits = logits.reshape(-1, V, V)
            M, V, V = logits.shape
            A = self.A
            A *= self.mask
            A = A.unsqueeze(0).expand(M, V, V)
            logits = logits+A
            logits = logits.reshape(B, self.Nh, V, V)

        # ä½¿ç”¨æˆ–ä¸ä½¿ç”¨ç›¸å¯¹ä½ç½®ç¼–ç 
        if self.relative:
            rel_logits = self.relative_logits(q)
            logits_sum = torch.add(logits, rel_logits)

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        if self.relative:
            weights = F.softmax(logits_sum, dim=-1)
        else:
            weights = F.softmax(logits, dim=-1)

        # Drop connectå®žçŽ°ä»¥é¿å…è¿‡æ‹Ÿåˆ
        if (self.drop_connect and self.training):
            mask = torch.bernoulli((0.5) * torch.ones(B * self.Nh * V, device=device))
            mask = mask.reshape(B, self.Nh, V).unsqueeze(2).expand(B, self.Nh, V, V)
            weights = weights * mask
            weights = weights / (weights.sum(3, keepdim=True) + 1e-8)



        # attn_out
        # (batch, Nh, joints, dvh)
        # weights*V
        # (batch, Nh, joints, joints)*(batch, Nh, joints, dvh)=(batch, Nh, joints, dvh)
        attn_out = torch.matmul(weights, flat_v.transpose(2, 3))

        if not self.more_channels:
            attn_out = torch.reshape(attn_out, (B, self.Nh, T, V, self.dv // self.Nh))
        else:
            attn_out = torch.reshape(attn_out, (B, self.Nh, T, V, self.dv // self.num))

        attn_out = attn_out.permute(0, 1, 4, 2, 3)

        # combine_heads_2dï¼Œä»…åœ¨æ¯ä¸ª Z åˆ†åˆ«è®¡ç®—åŽåˆå¹¶å¤´éƒ¨
        # (batch, Nh*dv, 1, joints)
        attn_out = self.combine_heads_2d(attn_out)

        # ä¹˜ä»¥ W0ï¼ˆæ‰¹æ¬¡ï¼Œè¾“å‡ºé€šé“ï¼Œ1ï¼Œå…³èŠ‚ï¼‰å…¶ä¸­è¾“å‡ºé€šé“=dv
        attn_out = self.attn_out(attn_out)
        return attn_out

    def compute_flat_qkv(self, x, dk, dv, Nh):
        qkv = self.qkv_conv(x)
        # T=1 åœ¨æ­¤æƒ…å†µä¸‹ï¼Œå› ä¸ºæ­£åœ¨åˆ†åˆ«è€ƒè™‘æ¯ä¸€å¸§
        N, _, T, V = qkv.size()

        # å¦‚æžœ self.more_channels=Trueï¼Œåˆ™æ¯ä¸ªå¤´åˆ†é… dk*self.Nh//self.num ä¸ªé€šé“
        if self.more_channels:
            q, k, v = torch.split(qkv, [dk * self.Nh // self.num, dk * self.Nh // self.num, dv * self.Nh // self.num],
                                  dim=1)
        else:
            q, k, v = torch.split(qkv, [dk, dk, dv], dim=1)
        q = self.split_heads_2d(q, Nh)
        k = self.split_heads_2d(k, Nh)
        v = self.split_heads_2d(v, Nh)

        dkh = dk // Nh
        q = q*(dkh ** -0.5)
        if self.more_channels:
            flat_q = torch.reshape(q, (N, Nh, dk // self.num, T * V))
            flat_k = torch.reshape(k, (N, Nh, dk // self.num, T * V))
            flat_v = torch.reshape(v, (N, Nh, dv // self.num, T * V))
        else:
            flat_q = torch.reshape(q, (N, Nh, dkh, T * V))
            flat_k = torch.reshape(k, (N, Nh, dkh, T * V))
            flat_v = torch.reshape(v, (N, Nh, dv // self.Nh, T * V))
        return flat_q, flat_k, flat_v, q, k, v

    def split_heads_2d(self, x, Nh):
        B, channels, T, V = x.size()
        ret_shape = (B, Nh, channels // Nh, T, V)
        split = torch.reshape(x, ret_shape)
        return split

    def combine_heads_2d(self, x):
        batch, Nh, dv, T, V = x.size()
        ret_shape = (batch, Nh * dv, T, V)
        return torch.reshape(x, ret_shape)

    def relative_logits(self, q):
        B, Nh, dk, T, V = q.size()
        q = torch.transpose(q, 2, 4).transpose(2, 3)
        q_first = q.unsqueeze(4).expand((B, Nh, T, V, V - 1, dk))
        q_first = torch.reshape(q_first, (B * Nh * T, -1, dk))

        # q ç”¨äºŽå¯¹è§’çº¿å‚æ•°çš„åµŒå…¥è¿›è¡Œä¹˜æ³•
        q = torch.reshape(q, (B * Nh * T, V, dk))
        # key_rel_diagonal: (1, dk) -> (V, dk)
        param_diagonal = self.key_rel_diagonal.expand((V, dk))
        rel_logits = self.relative_logits_1d(q_first, q, self.key_rel, param_diagonal, T, V, Nh)
        return rel_logits

    def relative_logits_1d(self, q_first, q, rel_k, param_diagonal, T, V, Nh):
        # è®¡ç®—ä¸€ç»´ä¸Šçš„ç›¸å¯¹å¯¹æ•°
        # (B*Nh*1,V^2-V, self.dk // Nh)*(V^2 - V, self.dk // Nh)

        # (B*Nh*1, V^2-V)
        rel_logits = torch.einsum('bmd,md->bm', q_first, rel_k)
        # (B*Nh*1, V)
        rel_logits_diagonal = torch.einsum('bmd,md->bm', q, param_diagonal)

        # é‡å¡‘ä»¥èŽ·å¾— Srel
        rel_logits = self.rel_to_abs(rel_logits, rel_logits_diagonal)

        rel_logits = torch.reshape(rel_logits, (-1, Nh, V, V))
        return rel_logits

    def rel_to_abs(self, rel_logits, rel_logits_diagonal):
        B, L = rel_logits.size()
        B, V = rel_logits_diagonal.size()

        # (B, V-1, V) -> (B, V, V)
        rel_logits = torch.reshape(rel_logits, (B, V - 1, V))
        row_pad = torch.zeros(B, 1, V).to(rel_logits)
        rel_logits = torch.cat((rel_logits, row_pad), dim=1)

        # è¿žæŽ¥å·¦ä¾§çš„å…¶ä»–åµŒå…¥
        # (B, V, V) -> (B, V, V+1) -> (B, V+1, V)
        rel_logits_diagonal = torch.reshape(rel_logits_diagonal, (B, V, 1))
        rel_logits = torch.cat((rel_logits_diagonal, rel_logits), dim=2)
        rel_logits = torch.reshape(rel_logits, (B, V + 1, V))

        # slice
        flat_sliced = rel_logits[:, :V, :]
        final_x = torch.reshape(flat_sliced, (B, V, V))
        return final_x

```

å¤šå¤´æ³¨æ„åŠ›é€šè¿‡é‡å¤æ‰§è¡Œæ­¤åµŒå…¥æå–è¿‡ç¨‹
![N_h](https://latex.csdn.net/eq?N_h)
æ¬¡æ¥å®žçŽ°ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸€ç»„ä¸åŒçš„å¯å­¦ä¹ å‚æ•°ã€‚å› æ­¤èŽ·å¾—çš„èŠ‚ç‚¹åµŒå…¥é›†
![(\mathbf{z}_{i_1}^t,\cdots ,\mathbf{z}_{i_H}^t)](https://latex.csdn.net/eq?%28%5Cmathbf%7Bz%7D_%7Bi_1%7D%5Et%2C%5Ccdots%20%2C%5Cmathbf%7Bz%7D_%7Bi_H%7D%5Et%29)
ï¼Œæ‰€æœ‰è¿™äº›éƒ½æŒ‡ä»£ç›¸åŒçš„èŠ‚ç‚¹
![v_{ti}](https://latex.csdn.net/eq?v_%7Bti%7D)
ï¼Œç„¶åŽä¸Žä¸€ä¸ªå¯å­¦ä¹ çš„å˜æ¢ç›¸ç»“åˆï¼Œå³
![concat(\mathbf{z}_{i_1}^t,\cdots ,\mathbf{z}_{i_H}^t)\cdot \mathbf{W_o}](https://latex.csdn.net/eq?concat%28%5Cmathbf%7Bz%7D_%7Bi_1%7D%5Et%2C%5Ccdots%20%2C%5Cmathbf%7Bz%7D_%7Bi_H%7D%5Et%29%5Ccdot%20%5Cmathbf%7BW_o%7D)
ï¼Œå¹¶æž„æˆSSAçš„è¾“å‡ºç‰¹å¾ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼š

* SSAåœ¨å®Œå…¨è¿žæŽ¥å›¾ä¸Šç±»ä¼¼äºŽå›¾å·ç§¯æ“ä½œï¼Œä½†æ˜¯æ ¸å€¼ï¼ˆå³èŠ‚ç‚¹ä¹‹é—´çš„å…³ç³»
  ![\alpha _{ij}^t](https://latex.csdn.net/eq?%5Calpha%20_%7Bij%7D%5Et)
  åˆ†æ•°ï¼‰åœ¨SSAä¸­æ˜¯åŸºäºŽéª¨æž¶å§¿æ€åŠ¨æ€é¢„æµ‹ï¼›
* éª¨æž¶ä¸­çš„ç›¸å…³ç»“æž„å¯¹äºŽæ‰€æœ‰åŠ¨ä½œä¸æ˜¯å›ºå®šçš„ï¼Œè€Œæ˜¯å¯¹æ¯ä¸ªæ ·æœ¬è‡ªé€‚åº”åœ°å˜åŒ–ã€‚

### 2.2 æ—¶é—´è‡ªæ³¨æ„åŠ›ï¼ˆTSAï¼‰

æ—¶é—´è‡ªæ³¨æ„åŠ›ï¼ˆTSAï¼‰æ¨¡å—ï¼Œåˆ†åˆ«ç ”ç©¶æ¯ä¸ªå…³èŠ‚åœ¨æ‰€æœ‰å¸§ä¸­çš„åŠ¨æ€ï¼Œå³æ¯ä¸ªå•ç‹¬çš„å…³èŠ‚è¢«è§†ä¸ºç‹¬ç«‹ï¼Œé€šè¿‡æ¯”è¾ƒåŒä¸€èº«ä½“å…³èŠ‚åœ¨æ—¶é—´ç»´åº¦ä¸Šçš„åµŒå…¥å˜åŒ–æ¥è®¡ç®—å¸§ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/direct/ebf7a6450e414214b765319fdebf8e1d.png)

TSAçš„å…¬å¼ä¸ŽSSAå…¬å¼å¯¹ç§°ï¼š

![\alpha _{tu}^v=\mathbf{q}_t^v\cdot \mathbf{k}_u^{v}\; \; \; \forall v\in V](https://latex.csdn.net/eq?%5Calpha%20_%7Btu%7D%5Ev%3D%5Cmathbf%7Bq%7D_t%5Ev%5Ccdot%20%5Cmathbf%7Bk%7D_u%5E%7Bv%7D%5C%3B%20%5C%3B%20%5C%3B%20%5Cforall%20v%5Cin%20V)
ï¼Œ
![\mathbf{z}_t^v=\sum _jsoftmax_u(\frac{\alpha _{tu}^v}{\sqrt{d_k}})\mathbf{v}_u^v](https://latex.csdn.net/eq?%5Cmathbf%7Bz%7D_t%5Ev%3D%5Csum%20_jsoftmax_u%28%5Cfrac%7B%5Calpha%20_%7Btu%7D%5Ev%7D%7B%5Csqrt%7Bd_k%7D%7D%29%5Cmathbf%7Bv%7D_u%5Ev)

å…¶ä¸­ï¼š

* ![v_{ti}](https://latex.csdn.net/eq?v_%7Bti%7D)
  ã€
  ![v_{ui}](https://latex.csdn.net/eq?v_%7Bui%7D)
  è¡¨ç¤ºç›¸åŒçš„å…³èŠ‚ç‚¹
  ![v](https://latex.csdn.net/eq?v)
  åœ¨ä¸¤ä¸ªä¸åŒæ—¶åˆ»
  ![t](https://latex.csdn.net/eq?t)
  ã€
  ![u](https://latex.csdn.net/eq?u)
  ï¼›
* ![\alpha _{tu}^i\in \mathbb{R}](https://latex.csdn.net/eq?%5Calpha%20_%7Btu%7D%5Ei%5Cin%20%5Cmathbb%7BR%7D)
  æ˜¯ç›¸å…³æ€§å¾—åˆ†ï¼›
* ![\mathbf{q} _{t}^i\in \mathbb{R}^{dq}](https://latex.csdn.net/eq?%5Cmathbf%7Bq%7D%20_%7Bt%7D%5Ei%5Cin%20%5Cmathbb%7BR%7D%5E%7Bdq%7D)
  æ˜¯ä¸Ž
  ![v_{ti}](https://latex.csdn.net/eq?v_%7Bti%7D)
  ç›¸å…³çš„æŸ¥è¯¢ï¼Œ
  ![\mathbf{k} _{u}^i\in \mathbb{R}^{dk}](https://latex.csdn.net/eq?%5Cmathbf%7Bk%7D%20_%7Bu%7D%5Ei%5Cin%20%5Cmathbb%7BR%7D%5E%7Bdk%7D)
  å’Œ
  ![\mathbf{v} _{u}^i\in \mathbb{R}^{dv}](https://latex.csdn.net/eq?%5Cmathbf%7Bv%7D%20_%7Bu%7D%5Ei%5Cin%20%5Cmathbb%7BR%7D%5E%7Bdv%7D)
  æ˜¯ä¸Žå…³èŠ‚
  ![v_{ui}](https://latex.csdn.net/eq?v_%7Bui%7D)
  ç›¸å…³çš„é”®å’Œå€¼ï¼ˆæ‰€æœ‰è®¡ç®—å‡ä½¿ç”¨å¯è®­ç»ƒçš„çº¿æ€§å˜æ¢ï¼Œå¦‚ SSA ä¸­æ‰€ç¤ºï¼‰ï¼›
* ![\mathbf{z}_t^i\in \mathbb{R}^{C_{out}}](https://latex.csdn.net/eq?%5Cmathbf%7Bz%7D_t%5Ei%5Cin%20%5Cmathbb%7BR%7D%5E%7BC_%7Bout%7D%7D)
  æ˜¯ç»“æžœèŠ‚ç‚¹åµŒå…¥ã€‚

å¯ä»¥çœ‹å‡ºï¼ŒTSAä½¿ç”¨çš„ç¬¦å·ä¸ŽSSAä¸­ä½¿ç”¨çš„ç¬¦å·ç›¸åï¼šä¸‹æ ‡è¡¨ç¤ºæ—¶é—´ï¼Œè€Œä¸Šæ ‡è¡¨ç¤ºå…³èŠ‚ã€‚åœ¨TSAä¸­åŒæ ·ä¹Ÿåº”ç”¨å¦‚ SSA ä¸­æ‰€ç¤ºçš„å¤šå¤´æ³¨æ„åŠ›ã€‚

TSA æ¨¡å—é€šè¿‡æå–æ—¶é—´èŠ‚ç‚¹ä¹‹é—´çš„å¸§é—´å…³ç³»ï¼Œå¯ä»¥å­¦ä¹ å¦‚ä½•å…³è”ä¸åŒçš„å¸§ï¼ˆä¾‹å¦‚ï¼Œç¬¬ä¸€å¸§ä¸­çš„èŠ‚ç‚¹ä¸Žæœ€åŽä¸€å¸§ä¸­çš„èŠ‚ç‚¹ï¼‰ï¼Œæ•æ‰åˆ°æ ‡å‡†ST-GCNå·ç§¯æ— æ³•æ•æ‰åˆ°çš„åˆ¤åˆ«ç‰¹å¾ï¼Œå› ä¸ºè¿™ç§å·ç§¯å—å†…æ ¸å¤§å°çš„é™åˆ¶ã€‚

### 2.3 åŒæµæ—¶ç©ºTransformerç½‘ç»œ

ä¸ºäº†ç»“åˆSSAå’ŒTSAæ¨¡å—ï¼Œæ‰€ä»¥é‡‡ç”¨äº†ä¸€ç§åä¸ºST-TRçš„åŒæµæž¶æž„ï¼Œåœ¨æœ¬æ–‡çš„å…¬å¼ä¸­ï¼ŒåŒæµåŒºåˆ†æ‰€æå‡ºçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„åº”ç”¨æ–¹å¼ï¼šSSAä½œç”¨äºŽç©ºé—´æµï¼ˆç§°ä¸º S-TRï¼‰ï¼Œè€ŒTSAä½œç”¨äºŽæ—¶é—´æµï¼ˆç§°ä¸º T-TRï¼‰ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/direct/35496aa1f1a84b97ba4028c9af411cb2.png)

åœ¨S-TRå’ŒT-TRè¿™ä¸¤ä¸ªæµä¸­ï¼š

1. èŠ‚ç‚¹ç‰¹å¾é¦–å…ˆé€šè¿‡ä¸€ä¸ªä¸‰å±‚æ®‹å·®ç½‘ç»œæå–ä½Žçº§ç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸€å±‚é€šè¿‡å›¾å·ç§¯ï¼ˆGCNï¼‰åœ¨ç©ºé—´ç»´åº¦ä¸Šå¤„ç†è¾“å…¥ï¼Œé€šè¿‡æ ‡å‡†2Då·ç§¯ï¼ˆTCNï¼‰åœ¨æ—¶é—´ç»´åº¦ä¸Šå¤„ç†è¾“å…¥ï¼›
2. ç„¶åŽï¼ŒSSAå’ŒTSAåˆ†åˆ«åœ¨STRå’ŒT-TRæµä¸­çš„åŽç»­å±‚åº”ç”¨äºŽæ›¿ä»£GCNå’ŒTCNç‰¹å¾æå–æ¨¡å—ï¼ŒS-TRæµå’ŒT-TRæµåŠå…¶ç›¸åº”çš„ç‰¹å¾æå–å±‚åˆ†åˆ«è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒï¼›
3. æœ€ç»ˆï¼Œé€šè¿‡å°†å®ƒä»¬çš„softmaxè¾“å‡ºåˆ†æ•°ç›¸åŠ ï¼Œå°†å­ç½‘ç»œçš„è¾“å‡ºèžåˆåœ¨ä¸€èµ·ï¼Œä»¥èŽ·å¾—æœ€ç»ˆçš„é¢„æµ‹ã€‚

åœ¨S-TRä¸­ï¼Œé€šè¿‡SSAæ¨¡å—åœ¨éª¨éª¼çº§åˆ«åº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œè¯¥æ¨¡å—å…³æ³¨å…³èŠ‚ä¹‹é—´çš„ç©ºé—´å…³ç³»ï¼ŒSSAæ¨¡å—çš„è¾“å‡ºä¼ é€’åˆ°å…·æœ‰æ ¸
![K_t](https://latex.csdn.net/eq?K_t)
çš„2Då·ç§¯æ¨¡å—ï¼ˆTCNï¼‰ï¼Œä»¥æå–æ—¶é—´ç›¸å…³çš„ç‰¹å¾ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![\mathbf{S-TR}(x)=Conv_{2D(1\times K_t)}(\mathbf{SSA}(x))](https://latex.csdn.net/eq?%5Cmathbf%7BS-TR%7D%28x%29%3DConv_%7B2D%281%5Ctimes%20K_t%29%7D%28%5Cmathbf%7BSSA%7D%28x%29%29)

2Då·ç§¯å•å…ƒä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import numpy as np
import math


class Unit2D(nn.Module):
    def __init__(self,
                 D_in,
                 D_out,
                 kernel_size,
                 stride=1,
                 dim=2,
                 dropout=0,
                 bias=True):
        super(Unit2D, self).__init__()
        pad = int((kernel_size - 1) / 2)
        print("Pad Temporal ", pad)

        if dim == 2:
            self.conv = nn.Conv2d(
                D_in,
                D_out,
                kernel_size=(kernel_size, 1),
                padding=(pad, 0),
                stride=(stride, 1),
                bias=bias)
        elif dim == 3:
            print("Pad Temporal ", pad)
            self.conv = nn.Conv2d(
                D_in,
                D_out,
                kernel_size=(1, kernel_size),
                padding=(0, pad),
                stride=(1, stride),
                bias=bias)
        else:
            raise ValueError()

        self.bn = nn.BatchNorm2d(D_out)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout, inplace=True)

        # initialize
        conv_init(self.conv)

    def forward(self, x):
        x = self.dropout(x)
        x = self.relu(self.bn(self.conv(x)))
        return x


def conv_init(module):
    # he_normal
    n = module.out_channels
    for k in module.kernel_size:
        n = n*k
    module.weight.data.normal_(0, math.sqrt(2. / n))


def import_class(name):
    components = name.split('.')
    mod = __import__(components[0])
    for comp in components[1:]:
        mod = getattr(mod, comp)
    return mod
```

éµå¾ªåŽŸå§‹Transformerï¼Œè¾“å…¥é€šè¿‡æ‰¹é‡å½’ä¸€åŒ–å±‚ï¼Œå¹¶ä½¿ç”¨è·³è¿‡è¿žæŽ¥å°†è¾“å…¥ä¸Ž SSA æ¨¡å—çš„è¾“å‡ºç›¸åŠ ã€‚

ç›¸åï¼ŒT-TRä¸“æ³¨äºŽå‘çŽ°å¸§é—´æ—¶é—´å…³ç³»ã€‚ç±»ä¼¼äºŽS-TRæµï¼Œåœ¨æ¯ä¸ªT-TRå±‚å†…éƒ¨æ˜¯ä¸€ä¸ªæ ‡å‡†çš„å›¾å·ç§¯å­æ¨¡å—ï¼Œä¹‹åŽè·Ÿéšæ‰€æå‡ºçš„æ—¶åºè‡ªæ³¨æ„åŠ›æ¨¡å—ï¼š

![\mathbf{T-TR}(x)=\mathbf{TSA}(GCN(x))](https://latex.csdn.net/eq?%5Cmathbf%7BT-TR%7D%28x%29%3D%5Cmathbf%7BTSA%7D%28GCN%28x%29%29)

TSAåœ¨æ‰€æœ‰æ—¶é—´ç»´åº¦ä¸Šè¿žæŽ¥ç›¸åŒå…³èŠ‚çš„å›¾ä¸Šè¿è¡Œã€‚

### 2.4 SSAå’ŒTSAçš„å®žçŽ°

SSAçš„çŸ©é˜µå®žçŽ°æ˜¯åŸºäºŽå¯¹åƒç´ çš„Transformerå®žçŽ°çš„æ”¹è¿›ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/direct/80794c3f05ed44a6a7619b987e71da90.png)

è¾“å…¥ð‘“é€šè¿‡åœ¨æ‰¹æ¬¡ç»´åº¦ä¸­ç§»åŠ¨ð‘‡è¿›è¡Œé‡å¡‘ï¼Œä»¥ä¾¿è‡ªæ³¨æ„åŠ›åˆ†åˆ«å¯¹æ¯ä¸ªæ—¶é—´å¸§è¿›è¡Œæ“ä½œã€‚SSA ä½œä¸ºçŸ©é˜µä¹˜æ³•å®žçŽ°ï¼Œå…¶ä¸­ðã€ðŠå’Œð•åˆ†åˆ«æ˜¯æŸ¥è¯¢ã€é”®å’Œå€¼çŸ©é˜µï¼ŒâŠ—è¡¨ç¤ºçŸ©é˜µä¹˜æ³•ã€‚

* é¦–å…ˆï¼Œç»™å®šä¸€ä¸ªå½¢çŠ¶ä¸º
  ![(C_{in},T,V)](https://latex.csdn.net/eq?%28C_%7Bin%7D%2CT%2CV%29)
  çš„è¾“å…¥å¼ é‡
  ![f_{in}](https://latex.csdn.net/eq?f_%7Bin%7D)
  ï¼Œå…¶ä¸­
  ![C_{in}](https://latex.csdn.net/eq?C_%7Bin%7D)
  æ˜¯è¾“å…¥ç‰¹å¾çš„æ•°é‡ï¼Œ
  ![T](https://latex.csdn.net/eq?T)
  æ˜¯å¸§æ•°ï¼Œ
  ![V](https://latex.csdn.net/eq?V)
  æ˜¯èŠ‚ç‚¹æ•°ï¼›
* å†é€šè¿‡æ‰¹å½’ä¸€åŒ–ç„¶åŽé‡æ–°æŽ’åˆ—è¾“å…¥å¾—åˆ°ä¸€ä¸ª
  ![\mathbf{X}_V\in \mathbb{R}^{T\times C_{in}\times V}](https://latex.csdn.net/eq?%5Cmathbf%7BX%7D_V%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%5Ctimes%20C_%7Bin%7D%5Ctimes%20V%7D)
  çš„çŸ©é˜µï¼Œåœ¨è¿™é‡Œï¼Œå°†
  ![T](https://latex.csdn.net/eq?T)
  ç»´åº¦ç§»åŠ¨åˆ°æ‰¹å¤„ç†ç»´åº¦å†…éƒ¨ï¼Œæœ‰æ•ˆåœ°åœ¨æ—¶é—´ç»´åº¦ä¸Šå®žçŽ°äº†å‚æ•°å…±äº«ï¼›
* ç„¶åŽï¼Œå¯¹æ¯ä¸ªå¸§åˆ†åˆ«åº”ç”¨å˜æ¢ï¼š

![](https://i-blog.csdnimg.cn/direct/ae4f1ffc0ec84fbcb400e1efa1121989.png)

* å…¶ä¸­ï¼Œæ ¹æ®æƒé‡
  ![\mathbf{W}_q\in \mathbb{R}^{ C_{in}\times N_h\times d_q^h}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_q%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%5Ctimes%20N_h%5Ctimes%20d_q%5Eh%7D)
  ã€
  ![\mathbf{W}_k\in \mathbb{R}^{ C_{in}\times N_h\times d^h_k}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_k%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%5Ctimes%20N_h%5Ctimes%20d%5Eh_k%7D)
  å’Œ
  ![\mathbf{W}_v\in \mathbb{R}^{ C_{in}\times N_h\times d^h_v}](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D_v%5Cin%20%5Cmathbb%7BR%7D%5E%7B%20C_%7Bin%7D%5Ctimes%20N_h%5Ctimes%20d%5Eh_v%7D)
  åˆ†åˆ«äº§ç”Ÿ
  ![\mathbf{Q}\in \mathbb{R}^{T\times N_h\times d_q^h\times V}](https://latex.csdn.net/eq?%5Cmathbf%7BQ%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%5Ctimes%20N_h%5Ctimes%20d_q%5Eh%5Ctimes%20V%7D)
  ã€
  ![\mathbf{K}\in \mathbb{R}^{T\times N_h\times d_k^h\times V}](https://latex.csdn.net/eq?%5Cmathbf%7BK%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%5Ctimes%20N_h%5Ctimes%20d_k%5Eh%5Ctimes%20V%7D)
  å’Œ
  ![\mathbf{V}\in \mathbb{R}^{T\times N_h\times d_v^h\times V}](https://latex.csdn.net/eq?%5Cmathbf%7BV%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BT%5Ctimes%20N_h%5Ctimes%20d_v%5Eh%5Ctimes%20V%7D)
  ï¼Œ
  ![N_h](https://latex.csdn.net/eq?N_h)
  æ˜¯å¤´éƒ¨çš„æ•°é‡ï¼Œ
  ![\mathbf{W}^o](https://latex.csdn.net/eq?%5Cmathbf%7BW%7D%5Eo)
  æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„çº¿æ€§å˜æ¢ï¼Œå®ƒç»“åˆäº†å¤´éƒ¨çš„è¾“å‡ºï¼›
* æœ€åŽï¼Œç©ºé—´å˜æ¢å™¨çš„è¾“å‡ºè¢«é‡æ–°æŽ’åˆ—ï¼Œå¾—åˆ°
  ![\mathbb{R}^{C_{out}\times T\times V}](https://latex.csdn.net/eq?%5Cmathbb%7BR%7D%5E%7BC_%7Bout%7D%5Ctimes%20T%5Ctimes%20V%7D)
  çš„è¾“å‡ºå¼ é‡
  ![f_{out}](https://latex.csdn.net/eq?f_%7Bout%7D)
  ã€‚

ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
from net import conv_init
from spatial_transformer import spatial_attention
import numpy as np

scale_norm = False


class gcn_unit_attention(nn.Module):
    def __init__(self, in_channels, out_channels, incidence, num, dv_factor, dk_factor, Nh, complete, relative,
                 only_attention, layer, more_channels, drop_connect, data_normalization, skip_conn, adjacency, num_point, padding=0,
                 kernel_size=1,
                 stride=1, bn_flag=True,
                 t_dilation=1, last_graph=False, visualization=True):
        super().__init__()
        # åˆå§‹åŒ–é‚»æŽ¥çŸ©é˜µ
        self.incidence = incidence
        self.incidence = incidence
        self.relu = nn.ReLU()
        self.visualization=visualization
        self.in_channels = in_channels
        self.more_channels = more_channels
        self.drop_connect = drop_connect
        self.data_normalization=data_normalization
        self.skip_conn=skip_conn
        self.num_point=num_point
        self.adjacency = adjacency
        print("Nh ", Nh)
        print("Dv ", dv_factor)
        print("Dk ", dk_factor)

        self.last_graph = last_graph
        # æ˜¯å¦ä»…ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶
        if (not only_attention):
            self.out_channels = out_channels - int((out_channels) * dv_factor)
        else:
            self.out_channels = out_channels
        # æ•°æ®å½’ä¸€åŒ–å±‚
        self.data_bn = nn.BatchNorm1d(self.in_channels * self.num_point)
        self.bn = nn.BatchNorm2d(out_channels)
        self.only_attention = only_attention
        self.bn_flag = bn_flag
        self.layer = layer

        # é‚»æŽ¥çŸ©é˜µè½¬æ¢
        self.incidence = Variable(self.incidence.clone(), requires_grad=False).view(-1, self.incidence.size()[-1],
                                                                                    self.incidence.size()[-1])

        # æ¯ä¸ª Conv2d å•å…ƒå®žçŽ° 2d å·ç§¯ä»¥å¯¹æ¯ä¸ªå•ç‹¬çš„åˆ†åŒºï¼ˆæ»¤æ³¢å™¨å¤§å° 1x1ï¼‰è¿›è¡ŒåŠ æƒ
        # æ¯ä¸ªåˆ†åŒºéƒ½æœ‰ä¸€ä¸ªå·ç§¯å•å…ƒ
        # è¿™æ˜¯ä»…åœ¨ç©ºé—´å˜æ¢å™¨å’Œå›¾å·ç§¯è¿žæŽ¥çš„æƒ…å†µä¸‹è¿›è¡Œçš„
        # å›¾å·ç§¯å±‚
        if (not self.only_attention):
            self.g_convolutions = nn.ModuleList(

                [nn.Conv2d(in_channels, self.out_channels, kernel_size=(kernel_size, 1), padding=(padding, 0),
                           stride=(stride, 1), dilation=(t_dilation, 1)) for i in
                 range(self.incidence.size()[0])]
            )
            for conv in self.g_convolutions:
                conv_init(conv)

            self.attention_conv = spatial_attention(in_channels=self.in_channels, kernel_size=1,
                                                dk=int(out_channels * dk_factor),
                                                dv=int(out_channels * dv_factor), Nh=Nh, complete=complete,
                                                relative=relative,
                                                stride=stride, layer=self.layer, A=self.incidence, num=num,
                                                more_channels=self.more_channels,
                                                drop_connect=self.drop_connect,
                                                data_normalization=self.data_normalization, skip_conn=self.skip_conn,
                                                adjacency=self.adjacency, visualization=self.visualization, num_point=self.num_point)
        else:
            self.attention_conv = spatial_attention(in_channels=self.in_channels, kernel_size=1,
                                                dk=int(out_channels * dk_factor),
                                                dv=int(out_channels), Nh=Nh, complete=complete,
                                                relative=relative,
                                                stride=stride, last_graph=self.last_graph, layer=self.layer,
                                                A=self.incidence, num=num, more_channels=self.more_channels,
                                                drop_connect=self.drop_connect,
                                                data_normalization=self.data_normalization, skip_conn=self.skip_conn,
                                                adjacency=self.adjacency, visualization=self.visualization, num_point=self.num_point)


    def forward(self, x, label, name):
        # N: number of samples, equal to the batch size
        # C: number of channels, in our case 3 (coordinates x, y, z)
        # T: number of frames
        # V: number of nodes
        N, C, T, V = x.size()
        x_sum = x
        # æ•°æ®å½’ä¸€åŒ–
        if (self.data_normalization):
            x = x.permute(0, 1, 3, 2).reshape(N, C * V, T)
            x = self.data_bn(x)
            x = x.reshape(N, C, V, T).permute(0, 1, 3, 2)

        # é‚»æŽ¥çŸ©é˜µè½¬æ¢
        self.incidence = self.incidence.cuda(x.get_device())

        # å¯å­¦ä¹ å‚æ•°
        incidence = self.incidence

        # N, T, C, V > NT, C, 1, V
        xa = x.permute(0, 2, 1, 3).reshape(-1, C, 1, V)

        # å¦ä¸€ç§åœ¨æ•°æ®ä¸Šå°è¯•çš„å½’ä¸€åŒ–æ–¹æ³•ï¼Œç§°ä¸ºâ€œScaleNormâ€
        if scale_norm:
            self.scale = ScaleNorm(scale=C ** 0.5)
            xa = self.scale(xa)

        # S-TR
        attn_out = self.attention_conv(xa, label, name)
        # N, T, C, V > N, C, T, V
        attn_out = attn_out.reshape(N, T, -1, V).permute(0, 2, 1, 3)

        if (not self.only_attention):

            # å¯¹äºŽæ¯ä¸ªåˆ†åŒºï¼Œå°†è¾“å…¥ç›¸ä¹˜ï¼Œå¹¶å°†ç»“æžœåº”ç”¨ 1x1 å·ç§¯ä»¥åŠ æƒæ¯ä¸ªåˆ†åŒº
            for i, partition in enumerate(incidence):
                # print(partition)
                # NCTxV
                xp = x.reshape(-1, V)
                # (NCTxV)*(VxV)
                xp = xp.mm(partition.float())
                # NxCxTxV
                xp = xp.reshape(N, C, T, V)

                if i == 0:
                    y = self.g_convolutions[i](xp)
                else:
                    y = y + self.g_convolutions[i](xp)

            # åœ¨é€šé“ç»´åº¦ä¸Šè¿žæŽ¥ä¸¤ä¸ªå·ç§¯
            y = torch.cat((y, attn_out), dim=1)
        else:
            if self.skip_conn and self.in_channels == self.out_channels:
                y = attn_out + x_sum
            else:
                y = attn_out
        if (self.bn_flag):
            y = self.bn(y)

        y = self.relu(y)

        return y


class ScaleNorm(nn.Module):
    """ScaleNorm"""

    def __init__(self, scale, eps=1e-5):
        super(ScaleNorm, self).__init__()
        self.scale = scale

        self.eps = eps

    def forward(self, x):
        norm = self.scale / torch.norm(x, dim=1, keepdim=True).clamp(min=self.eps)
        return x * norm
```

TSAçš„å®žçŽ°æ–¹å¼ä¸ŽSSAç›¸åŒï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯ç»´åº¦
![V](https://latex.csdn.net/eq?V)
å¯¹åº”äºŽ
![T](https://latex.csdn.net/eq?T)
ï¼Œåä¹‹äº¦ç„¶ï¼Œå³ä¸ºäº†è¢«æ¯ä¸ªTSAæ¨¡å—å¤„ç†ï¼Œè¾“å…¥è¢«é‡å¡‘ä¸ºä¸€ä¸ªçŸ©é˜µ
![\mathbf{X}_T\in \mathbb{R}^{V\times C_{in}\times T}](https://latex.csdn.net/eq?%5Cmathbf%7BX%7D_T%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%5Ctimes%20C_%7Bin%7D%5Ctimes%20T%7D)
ï¼Œä»¥ä¾¿æ²¿ç€æ—¶é—´ç»´åº¦åˆ†åˆ«å¯¹æ¯ä¸ªå…³èŠ‚è¿›è¡Œæ“ä½œï¼š

![](https://i-blog.csdnimg.cn/direct/81714c5ec9714e6b9843471f2710a81b.jpeg)

åŒæ—¶çŸ©é˜µå½¢çŠ¶ä¹Ÿå˜ä¸º
![\mathbf{Q}\in \mathbb{R}^{V\times N_h\times d_q^h\times T}](https://latex.csdn.net/eq?%5Cmathbf%7BQ%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%5Ctimes%20N_h%5Ctimes%20d_q%5Eh%5Ctimes%20T%7D)
ã€
![\mathbf{K}\in \mathbb{R}^{V\times N_h\times d_k^h\times T}](https://latex.csdn.net/eq?%5Cmathbf%7BK%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%5Ctimes%20N_h%5Ctimes%20d_k%5Eh%5Ctimes%20T%7D)
ã€
![\mathbf{V}\in \mathbb{R}^{V\times N_h\times d_v^h\times T}](https://latex.csdn.net/eq?%5Cmathbf%7BV%7D%5Cin%20%5Cmathbb%7BR%7D%5E%7BV%5Ctimes%20N_h%5Ctimes%20d_v%5Eh%5Ctimes%20T%7D)
ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/direct/59f39ff4679e4af2bbda4066dbbf3ec3.jpeg)

ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F_func
from net import Unit2D
import math
import numpy as np
import time

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")
dropout = False
scale_norm = False
save = False
multi_matmul = False

class tcn_unit_attention(nn.Module):

    def forward(self, x):
        # è¾“å…¥ x
        # (batch_size, channels, time, joints)
        N, C, T, V = x.size()
        x_sum = x

        # æ•°æ®å½’ä¸€åŒ–
        if (self.data_normalization):
            x = x.permute(0, 1, 3, 2).reshape(N, C * V, T)
            x = self.data_bn(x)
            x = x.reshape(N, C, V, T).permute(0, 1, 3, 2)

        # è”åˆç»´åº¦è¢«æ”¾å…¥æ‰¹æ¬¡ä¸­ï¼Œä»¥ä¾¿åˆ†åˆ«æ²¿æ—¶é—´å¤„ç†æ¯ä¸ªå…³èŠ‚
        x = x.permute(0, 3, 1, 2).reshape(-1, C, 1, T)

        if scale_norm:
            self.scale = ScaleNorm(scale=C ** 0.5)
            x = self.scale(x)

        flat_q, flat_k, flat_v, q, k, v = self.compute_flat_qkv(x, self.dk, self.dv, self.Nh)
        B, self.Nh, C, T = flat_q.size()

        # è®¡ç®—é€šè¿‡ q*k èŽ·å¾—çš„åˆ†æ•°
        # (batch_size, Nh, time, dkh)*(batch_size, Nh,dkh, time) =  (batch_size, Nh, time, time)

        if (multi_matmul):
            for i in range(0, 5):
                flat_q_5 = flat_q[:, :, :, (60 * i):(60 * (i + 1))]
                product = torch.matmul(flat_q_5.transpose(2, 3), flat_k)
                if (i == 0):
                    logits = product
                else:
                    logits = torch.cat((logits, product), dim=2)
        else:
            logits = torch.matmul(flat_q.transpose(2, 3), flat_k)

        if self.relative:
            rel_logits = self.relative_logits(q)
            logits_sum = torch.add(logits, rel_logits)

        # è®¡ç®—æƒé‡
        if self.relative:

            weights = F_func.softmax(logits_sum, dim=-1)

        else:
            weights = F_func.softmax(logits, dim=-1)

        # åº”ç”¨ä¸¢å¼ƒè¿žæŽ¥
        if (self.drop_connect and self.training):
            mask = torch.bernoulli((0.5) * torch.ones(B * self.Nh * T, device=device))
            mask = mask.reshape(B, self.Nh, T).unsqueeze(2).expand(B, self.Nh, T, T)
            weights = weights * mask
            weights = weights / (weights.sum(3, keepdim=True) + 1e-8)

        # attn_out
        # (batch, Nh, time, dvh)
        # weights*V
        # (batch, Nh, time, time)*(batch, Nh, time, dvh)=(batch, Nh, time, dvh)

        attn_out = torch.matmul(weights, flat_v.transpose(2, 3))
        if not self.more_channels:
            attn_out = torch.reshape(attn_out, (B, self.Nh, 1, T, self.dv // self.Nh))
        else:
            attn_out = torch.reshape(attn_out, (B, self.Nh, 1, T, self.dv // self.num))

        attn_out = attn_out.permute(0, 1, 4, 2, 3)

        # combine_heads_2dï¼Œä»…åœ¨æ¯ä¸ª Z åˆ†åˆ«è®¡ç®—åŽåˆå¹¶å¤´éƒ¨
        # (batch, Nh*dv, time, 1)
        attn_out = self.combine_heads_2d(attn_out)

        # ä¹˜ä»¥ W0ï¼ˆæ‰¹æ¬¡ï¼Œè¾“å‡ºé€šé“ï¼Œæ—¶é—´ï¼Œå…³èŠ‚ï¼‰ï¼Œå…¶ä¸­è¾“å‡ºé€šé“=dv
        attn_out = self.attn_out(attn_out)
        attn_out = attn_out.reshape(N, V, -1, T).permute(0, 2, 3, 1)

        # æ®‹å·®è¿žæŽ¥
        if self.skip_conn:
            if dropout:
                attn_out = self.dropout(attn_out)

                if (not self.only_temporal_att):
                    x = self.tcn_conv(x_sum)
                    result = torch.cat((x, attn_out), dim=1)
                else:
                    result = attn_out

                result = result+(x_sum if (self.down is None) else self.down(x_sum))


            else:
                if (not self.only_temporal_att):
                    x = self.tcn_conv(x_sum)
                    result = torch.cat((x, attn_out), dim=1)
                else:
                    result = attn_out

                result = result+(x_sum if (self.down is None) else self.down(x_sum))


        else:
            result = attn_out

        if (self.bn_flag):
            result = self.bn(result)
        result = self.relu(result)
        return result

    def compute_flat_qkv(self, x, dk, dv, Nh):
        qkv = self.qkv_conv(x)

        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒV=1ï¼Œå› ä¸ºæ¯ä¸ªå…³èŠ‚åˆ†åˆ«åº”ç”¨äº†æ—¶é—´å˜æ¢å™¨
        N, C, V1, T1 = qkv.size()
        if self.more_channels:
            q, k, v = torch.split(qkv, [dk * self.Nh // self.num, dk * self.Nh // self.num, dv * self.Nh // self.num],
                                  dim=1)
        else:
            q, k, v = torch.split(qkv, [dk, dk, dv], dim=1)

        q = self.split_heads_2d(q, Nh)
        k = self.split_heads_2d(k, Nh)
        v = self.split_heads_2d(v, Nh)

        dkh = dk // Nh
        q = q* (dkh ** -0.5)
        if self.more_channels:

            flat_q = torch.reshape(q, (N, Nh, dk // self.num, V1 * T1))
            flat_k = torch.reshape(k, (N, Nh, dk // self.num, V1 * T1))
            flat_v = torch.reshape(v, (N, Nh, dv // self.num, V1 * T1))
        else:
            flat_q = torch.reshape(q, (N, Nh, dkh, V1 * T1))
            flat_k = torch.reshape(k, (N, Nh, dkh, V1 * T1))
            flat_v = torch.reshape(v, (N, Nh, dv // self.Nh, V1 * T1))
        return flat_q, flat_k, flat_v, q, k, v

    def split_heads_2d(self, x, Nh):
        B, channels, F, V = x.size()
        ret_shape = (B, Nh, channels // Nh, F, V)
        split = torch.reshape(x, ret_shape)
        return split

    def combine_heads_2d(self, x):
        batch, Nh, dv, F, V = x.size()
        ret_shape = (batch, Nh * dv, F, V)
        return torch.reshape(x, ret_shape)

    def relative_logits(self, q):
        B, Nh, dk, _, T = q.size()
        # B, Nh, V, T, dk -> B, Nh, F, 1, dk
        q = q.permute(0, 1, 3, 4, 2)
        q = q.reshape(B, Nh, T, dk)
        rel_logits = self.relative_logits_1d(q, self.key_rel)
        return rel_logits

    def relative_logits_1d(self, q, rel_k):
        # compute relative logits along one dimension
        # (B, Nh,  1, V, channels // Nh)*(2 * K - 1, self.dk // Nh)
        # (B, Nh,  1, V, 2 * K - 1)
        rel_logits = torch.einsum('bhld,md->bhlm', q, rel_k)
        rel_logits = self.rel_to_abs(rel_logits)
        B, Nh, L, L = rel_logits.size()
        return rel_logits

    def rel_to_abs(self, x):
        B, Nh, L, _ = x.size()
        print(x.shape)
        col_pad = torch.zeros((B, Nh, L, 1)).to(x)
        x = torch.cat((x, col_pad), dim=3)
        flat_x = torch.reshape(x, (B, Nh, L * 2 * L))
        flat_pad = torch.zeros((B, Nh, L - 1)).to(x)
        flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)

        final_x = torch.reshape(flat_x_padded, (B, Nh, L + 1, 2 * L - 1))
        final_x = final_x[:, :, :L, L - 1:]
        return final_x


class ScaleNorm(nn.Module):
    """ScaleNorm"""

    def __init__(self, scale, eps=1e-5):
        super(ScaleNorm, self).__init__()
        self.scale = scale

        self.eps = eps

    def forward(self, x):
        norm = self.scale / torch.norm(x, dim=1, keepdim=True).clamp(min=self.eps)
        return x * norm

```

## 3 ST-TRç½‘ç»œç»“æž„

ST-TRçš„ç½‘ç»œç»“æž„å¯ä»¥å‚è€ƒST-GCNçš„ç½‘ç»œç»“æž„ï¼Œå¦‚ä¸‹ï¼š
![](https://i-blog.csdnimg.cn/direct/8ddb1297dd9b4dd6b137c47c52008561.png)
ä¹ŸåŒ…æ‹¬TCN-GCNå•å…ƒï¼Œä»£ç å¦‚ä¸‹ï¼š

```python
class TCN_GCN_unit(nn.Module):
    def __init__(self,
                 in_channel,
                 out_channel,
                 A,
                 attention,
                 only_attention,
                 tcn_attention,
                 only_temporal_attention,
                 relative,
                 device,
                 attention_3,
                 dv,
                 dk,
                 Nh,
                 num,
                 dim_block1,
                 dim_block2,
                 dim_block3,
                 num_point,
                 weight_matrix,
                 more_channels,
                 drop_connect,
                 starting_ch,
                 all_layers,
                 adjacency,
                 data_normalization,
                 visualization,
                 skip_conn,
                 layer=0,
                 kernel_size=9,
                 stride=1,
                 dropout=0.5,
                 use_local_bn=False,
                 mask_learning=False,
                 last=False,
                 last_graph=False,
                 agcn = False
                 ):
        super(TCN_GCN_unit, self).__init__()
        half_out_channel = out_channel / 2
        self.A = A

        self.V = A.shape[-1]
        self.C = in_channel
        self.last = last
        self.data_normalization = data_normalization
        self.skip_conn = skip_conn
        self.num_point = num_point
        self.adjacency = adjacency
        self.last_graph = last_graph
        self.layer = layer
        self.stride = stride
        self.drop_connect = drop_connect
        self.visualization = visualization
        self.device = device
        self.all_layers = all_layers
        self.more_channels = more_channels

        if (out_channel >= starting_ch and attention or (self.all_layers and attention)):

            self.gcn1 = gcn_unit_attention(in_channel, out_channel, dv_factor=dv, dk_factor=dk, Nh=Nh,
                                           complete=True,
                                           relative=relative, only_attention=only_attention, layer=layer, incidence=A,
                                           bn_flag=True, last_graph=self.last_graph, more_channels=self.more_channels,
                                           drop_connect=self.drop_connect, adjacency=self.adjacency, num=num,
                                           data_normalization=self.data_normalization, skip_conn=self.skip_conn,
                                           visualization=self.visualization, num_point=self.num_point)
        else:

            if not agcn:
                self.gcn1 = unit_gcn(
                    in_channel,
                    out_channel,
                    A,
                    use_local_bn=use_local_bn,
                    mask_learning=mask_learning)
            else:
                self.gcn1 = unit_agcn(
                    in_channel,
                    out_channel,
                    A,
                    use_local_bn=use_local_bn,
                    mask_learning=mask_learning)

        if (out_channel >= starting_ch and tcn_attention or (self.all_layers and tcn_attention)):

            if out_channel <= starting_ch and self.all_layers:
                self.tcn1 = tcn_unit_attention_block(out_channel, out_channel, dv_factor=dv,
                                                     dk_factor=dk, Nh=Nh,
                                                     relative=relative, only_temporal_attention=only_temporal_attention,
                                                     dropout=dropout,
                                                     kernel_size_temporal=9, stride=stride,
                                                     weight_matrix=weight_matrix, bn_flag=True, last=self.last,
                                                     layer=layer,
                                                     device=self.device, more_channels=self.more_channels,
                                                     drop_connect=self.drop_connect, n=num,
                                                     data_normalization=self.data_normalization,
                                                     skip_conn=self.skip_conn,
                                                     visualization=self.visualization, dim_block1=dim_block1,
                                                     dim_block2=dim_block2, dim_block3=dim_block3, num_point=self.num_point)
            else:
                self.tcn1 = tcn_unit_attention(out_channel, out_channel, dv_factor=dv,
                                               dk_factor=dk, Nh=Nh,
                                               relative=relative, only_temporal_attention=only_temporal_attention,
                                               dropout=dropout,
                                               kernel_size_temporal=9, stride=stride,
                                               weight_matrix=weight_matrix, bn_flag=True, last=self.last,
                                               layer=layer,
                                               device=self.device, more_channels=self.more_channels,
                                               drop_connect=self.drop_connect, n=num,
                                               data_normalization=self.data_normalization, skip_conn=self.skip_conn,
                                               visualization=self.visualization, num_point=self.num_point)



        else:
            self.tcn1 = Unit2D(
                out_channel,
                out_channel,
                kernel_size=kernel_size,
                dropout=dropout,
                stride=stride)
        if ((in_channel != out_channel) or (stride != 1)):
            self.down1 = Unit2D(
                in_channel, out_channel, kernel_size=1, stride=stride)
        else:
            self.down1 = None

    def forward(self, x, label, name):
        # N, C, T, V = x.size()
        x = self.tcn1(self.gcn1(x, label, name)) + (x if
                                                    (self.down1 is None) else self.down1(x))

        return x


class TCN_GCN_unit_multiscale(nn.Module):
    def __init__(self,
                 in_channels,
                 out_channels,
                 A,
                 kernel_size=9,
                 stride=1,
                 **kwargs):
        super(TCN_GCN_unit_multiscale, self).__init__()
        self.unit_1 = TCN_GCN_unit(
            in_channels,
            out_channels / 2,
            A,
            kernel_size=kernel_size,
            stride=stride,
            **kwargs)
        self.unit_2 = TCN_GCN_unit(
            in_channels,
            out_channels - out_channels / 2,
            A,
            kernel_size=kernel_size * 2 - 1,
            stride=stride,
            **kwargs)

    def forward(self, x):
        return torch.cat((self.unit_1(x), self.unit_2(x)), dim=1)
```

ST-TRæ•´ä½“ç½‘ç»œç»“æž„çš„ä»£ç å¦‚ä¸‹ï¼š

```python
class Model(nn.Module):

    def forward(self, x, label, name):
        N, C, T, V, M = x.size()
        print(x.shape)
        if (self.concat_original):
            x_coord = x
            x_coord = x_coord.permute(0, 4, 1, 2, 3).reshape(N * M, C, T, V)

        # æ•°æ®å½’ä¸€åŒ–
        if self.use_data_bn:
            if self.M_dim_bn:
                x = x.permute(0, 4, 3, 1, 2).contiguous().view(N, M * V * C, T)
            else:
                x = x.permute(0, 4, 3, 1, 2).contiguous().view(N * M, V * C, T)
            x = self.data_bn(x)
            # to (N*M, C, T, V)
            x = x.view(N, M, V, C, T).permute(0, 1, 3, 4, 2).contiguous().view(
                N * M, C, T, V)
        else:
            # from (N, C, T, V, M) to (N*M, C, T, V)
            x = x.permute(0, 4, 1, 2, 3).contiguous().view(N * M, C, T, V)

        # ä¸»å¹²ç½‘ç»œ
        if not self.all_layers:
            x = self.gcn0(x, label, name)
            x = self.tcn0(x)

        for i, m in enumerate(self.backbone):
            if i == 3 and self.concat_original:
                x = m(torch.cat((x, x_coord), dim=1), label, name)
            else:
                x = m(x, label, name)

        # æ± åŒ–å’Œåˆ†ç±»
        # V pooling
        x = F.avg_pool2d(x, kernel_size=(1, V))

        # M pooling
        c = x.size(1)
        t = x.size(2)
        x = x.view(N, M, c, t).mean(dim=1).view(N, c, t)

        # T pooling
        x = F.avg_pool1d(x, kernel_size=x.size()[2])

        # C fcn
        x = self.fcn(x)
        x = F.avg_pool1d(x, x.size()[2:])
        x = x.view(N, self.num_class)
        return x
```

## æ€»ç»“

åœ¨æœ¬æ–‡ä¸­ï¼Œæå‡ºäº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå°†Transformerè‡ªæ³¨æ„åŠ›å¼•å…¥éª¨éª¼æ´»åŠ¨è¯†åˆ«ï¼Œä½œä¸ºå›¾å·ç§¯çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå±•çŽ°äº†ç©ºé—´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆSSAï¼‰çš„æ›´çµæ´»å’ŒåŠ¨æ€çš„è¡¨ç¤ºã€‚åŒæ ·ï¼Œæ—¶é—´è‡ªæ³¨æ„åŠ›æ¨¡å—ï¼ˆTSAï¼‰å…‹æœäº†æ ‡å‡†å·ç§¯çš„ä¸¥æ ¼å±€éƒ¨æ€§ï¼Œèƒ½å¤Ÿæå–åŠ¨ä½œä¸­çš„é•¿è·ç¦»ä¾èµ–ã€‚ç”±SSAå’ŒTSAç»„æˆçš„åŒæµST-TRç½‘ç»œèƒ½å¤Ÿæœ‰æ•ˆæå–åŠ¨ä½œæ•°æ®çš„ç©ºé—´å’Œæ—¶é—´ç‰¹å¾ï¼Œä»Žè€Œåœ¨ä»¥å…³èŠ‚åæ ‡ä½œä¸ºè¾“å…¥æ—¶åœ¨æ‰€æœ‰æ•°æ®é›†å’Œåœ¨æ·»åŠ éª¨éª¼ä¿¡æ¯æ—¶çš„æ•°æ®é›†ä¸Šéƒ½è¡¨çŽ°å‡ºä¼˜å¼‚çš„æˆç»©ã€‚ç”±äºŽæœ¬æ–‡ä¸­ä»…æ¶‰åŠè‡ªæ³¨æ„åŠ›æ¨¡å—çš„é…ç½®å·²è¢«è¯æ˜Žæ˜¯æ¬¡ä¼˜çš„ï¼Œä¸€ä¸ªå¯èƒ½çš„æœªæ¥å·¥ä½œæ˜¯å¯»æ‰¾èƒ½å¤Ÿæ›¿ä»£å„ç§ä»»åŠ¡ä¸­å›¾å·ç§¯çš„ç»Ÿä¸€Transformeræž¶æž„æ¥å®žçŽ°æ›´å¥½çš„è¯†åˆ«æ€§èƒ½ã€‚