---
layout: post
title: "PyTorch分布式训练"
date: 2025-03-12 21:50:05 +0800
description: "（数据划分原理见引用[3]中描述的补充采样机制）"
keywords: "PyTorch分布式训练"
categories: ['未分类']
tags: ['分布式', '人工智能', 'Pytorch']
artid: "146215299"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146215299
    alt: "PyTorch分布式训练"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146215299
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146215299
cover: https://bing.ee123.net/img/rand?artid=146215299
image: https://bing.ee123.net/img/rand?artid=146215299
img: https://bing.ee123.net/img/rand?artid=146215299
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     PyTorch分布式训练
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     本文结构：
    </p>
    <ol>
     <li>
      分布式训练概述
     </li>
     <li>
      环境设置
     </li>
     <li>
      数据并行（DDP）
     </li>
     <li>
      模型并行
     </li>
     <li>
      启动训练
     </li>
     <li>
      性能优化建议
     </li>
     <li>
      示例代码
     </li>
     <li>
      参考资料和相关问题
     </li>
    </ol>
    <p>
     以下是为您整理的PyTorch分布式训练教程指南：
    </p>
    <h4>
     <a id="PyTorch_12">
     </a>
     一、PyTorch分布式训练核心概念
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        数据并行
       </strong>
       ：通过分割数据集实现多GPU并行训练，主流方法包括：
      </p>
      <ul>
       <li>
        <code>
         DistributedDataParallel
        </code>
        (DDP)：官方推荐的分布式训练接口
       </li>
       <li>
        <code>
         DataParallel
        </code>
        (DP)：单机多卡方案（已逐步被DDP取代）
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        模型并行
       </strong>
       ：
      </p>
      <ul>
       <li>
        流水线并行：将模型按层拆分到不同设备
       </li>
       <li>
        张量并行：拆分单个运算的矩阵维度
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        通信协议
       </strong>
       ：
      </p>
      <ul>
       <li>
        NCCL：NVIDIA GPU专用通信库
       </li>
       <li>
        Gloo：支持CPU和GPU的跨平台协议
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="DDP_25">
     </a>
     二、DDP实战步骤
    </h4>
    <h5>
     <a id="1__26">
     </a>
     1. 环境初始化
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist

<span class="token keyword">def</span> <span class="token function">setup</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>
        backend<span class="token operator">=</span><span class="token string">'nccl'</span><span class="token punctuation">,</span>  <span class="token comment"># GPU推荐NCCL</span>
        init_method<span class="token operator">=</span><span class="token string">'env://'</span><span class="token punctuation">,</span>
        rank<span class="token operator">=</span>rank<span class="token punctuation">,</span>
        world_size<span class="token operator">=</span>world_size
    <span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="2__40">
     </a>
     2. 数据分片
    </h5>
    <p>
     通过
     <code>
      DistributedSampler
     </code>
     实现数据集自动划分：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed <span class="token keyword">import</span> DistributedSampler

sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>
    dataset<span class="token punctuation">,</span>
    num_replicas<span class="token operator">=</span>world_size<span class="token punctuation">,</span>
    rank<span class="token operator">=</span>rank<span class="token punctuation">,</span>
    shuffle<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>sampler<span class="token punctuation">)</span>
</code></pre>
    <p>
     （数据划分原理见引用[3]中描述的补充采样机制）
    </p>
    <h5>
     <a id="3__55">
     </a>
     3. 模型封装
    </h5>
    <pre><code class="prism language-python">model <span class="token operator">=</span> NeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>
model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>rank<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="4__61">
     </a>
     4. 训练循环
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>  <span class="token comment"># 保证shuffle有效性</span>
    <span class="token keyword">for</span> batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_72">
     </a>
     三、多节点启动方法
    </h4>
    <p>
     引用[2]展示了多节点启动命令示例，推荐使用官方启动工具：
    </p>
    <pre><code class="prism language-bash"><span class="token comment"># 单机多卡启动（4 GPU）</span>
torchrun <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> train.py

<span class="token comment"># 多节点启动（需配置MASTER_ADDR）</span>
torchrun <span class="token parameter variable">--nnodes</span><span class="token operator">=</span><span class="token number">2</span> <span class="token parameter variable">--nproc_per_node</span><span class="token operator">=</span><span class="token number">4</span> <span class="token parameter variable">--master_addr</span><span class="token operator">=</span><span class="token number">192.168</span>.1.1 train.py
</code></pre>
    <h4>
     <a id="_82">
     </a>
     四、性能优化建议
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        通信优化
       </strong>
       ：
      </p>
      <ul>
       <li>
        使用梯度累积减少通信频率
       </li>
       <li>
        设置
        <code>
         find_unused_parameters=False
        </code>
        （当模型有未使用参数时需设为True）
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        计算优化
       </strong>
       ：
      </p>
      <ul>
       <li>
        增大批次尺寸
        <span class="katex--inline">
         <span class="katex">
          <span class="katex-mathml">
           B 
           
          
         
           B
          </span>
          <span class="katex-html">
           <span class="base">
            <span class="strut" style="height: 0.6833em;">
            </span>
            <span class="mord mathnormal" style="margin-right: 0.0502em;">
             B
            </span>
           </span>
          </span>
         </span>
        </span>
        可提升计算效率（引用[4]中的
        <span class="katex--inline">
         <span class="katex">
          <span class="katex-mathml">
           T 
            
            
            
              c 
             
            
              o 
             
            
              m 
             
            
              p 
             
            
           
          
         
           T_{comp}
          </span>
          <span class="katex-html">
           <span class="base">
            <span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;">
            </span>
            <span class="mord">
             <span class="mord mathnormal" style="margin-right: 0.1389em;">
              T
             </span>
             <span class="msupsub">
              <span class="vlist-t vlist-t2">
               <span class="vlist-r">
                <span class="vlist" style="height: 0.1514em;">
                 <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                  <span class="pstrut" style="height: 2.7em;">
                  </span>
                  <span class="sizing reset-size6 size3 mtight">
                   <span class="mord mtight">
                    <span class="mord mathnormal mtight">
                     co
                    </span>
                    <span class="mord mathnormal mtight">
                     m
                    </span>
                    <span class="mord mathnormal mtight">
                     p
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
                <span class="vlist-s">
                 ​
                </span>
               </span>
               <span class="vlist-r">
                <span class="vlist" style="height: 0.2861em;">
                 <span class="">
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
        优化）
       </li>
       <li>
        混合精度训练
       </li>
      </ul>
      <pre><code class="prism language-python">scaler <span class="token operator">=</span> GradScaler<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">with</span> autocast<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
scaler<span class="token punctuation">.</span>scale<span class="token punctuation">(</span>loss<span class="token punctuation">)</span><span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
scaler<span class="token punctuation">.</span>step<span class="token punctuation">(</span>optimizer<span class="token punctuation">)</span>
</code></pre>
     </li>
     <li>
      <p>
       <strong>
        网络优化
       </strong>
       ：
      </p>
      <ul>
       <li>
        使用RDMA高速网络降低
        <span class="katex--inline">
         <span class="katex">
          <span class="katex-mathml">
           T 
            
            
            
              s 
             
            
              y 
             
            
              n 
             
            
              c 
             
            
           
          
         
           T_{sync}
          </span>
          <span class="katex-html">
           <span class="base">
            <span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;">
            </span>
            <span class="mord">
             <span class="mord mathnormal" style="margin-right: 0.1389em;">
              T
             </span>
             <span class="msupsub">
              <span class="vlist-t vlist-t2">
               <span class="vlist-r">
                <span class="vlist" style="height: 0.1514em;">
                 <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                  <span class="pstrut" style="height: 2.7em;">
                  </span>
                  <span class="sizing reset-size6 size3 mtight">
                   <span class="mord mtight">
                    <span class="mord mathnormal mtight" style="margin-right: 0.0359em;">
                     sy
                    </span>
                    <span class="mord mathnormal mtight">
                     n
                    </span>
                    <span class="mord mathnormal mtight">
                     c
                    </span>
                   </span>
                  </span>
                 </span>
                </span>
                <span class="vlist-s">
                 ​
                </span>
               </span>
               <span class="vlist-r">
                <span class="vlist" style="height: 0.2861em;">
                 <span class="">
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
        （引用[4]）
       </li>
       <li>
        梯度压缩技术
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="_102">
     </a>
     五、完整代码示例
    </h4>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>distributed <span class="token keyword">as</span> dist
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel <span class="token keyword">import</span> DistributedDataParallel <span class="token keyword">as</span> DDP

<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 初始化进程组</span>
    setup<span class="token punctuation">(</span>rank<span class="token punctuation">,</span> world_size<span class="token punctuation">)</span>
    
    <span class="token comment"># 准备数据</span>
    dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>
    sampler <span class="token operator">=</span> DistributedSampler<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> world_size<span class="token punctuation">,</span> rank<span class="token punctuation">)</span>
    dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> sampler<span class="token operator">=</span>sampler<span class="token punctuation">)</span>
    
    <span class="token comment"># 构建模型</span>
    model <span class="token operator">=</span> DDP<span class="token punctuation">(</span>MyModel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>rank<span class="token punctuation">)</span><span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>rank<span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 训练循环</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        sampler<span class="token punctuation">.</span>set_epoch<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
            inputs <span class="token operator">=</span> batch<span class="token punctuation">.</span>to<span class="token punctuation">(</span>rank<span class="token punctuation">)</span>
            outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> outputs<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    world_size <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span>
    torch<span class="token punctuation">.</span>multiprocessing<span class="token punctuation">.</span>spawn<span class="token punctuation">(</span>main<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span>world_size<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nprocs<span class="token operator">=</span>world_size<span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_137">
     </a>
     六、官方学习资源
    </h4>
    <ol>
     <li>
      <a href="https://pytorch.org/docs/stable/distributed.html" rel="nofollow">
       PyTorch分布式训练官方文档
      </a>
     </li>
     <li>
      <a href="https://pytorch.org/assets/pdfs/eurosys21-ddp.pdf" rel="nofollow">
       DDP设计原理白皮书
      </a>
     </li>
     <li>
      <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html" rel="nofollow">
       AWS分布式训练最佳实践
      </a>
     </li>
    </ol>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35323831303136362f:61727469636c652f64657461696c732f313436323135323939" class_="artid" style="display:none">
 </p>
</div>


