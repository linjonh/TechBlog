---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33363333323636302f:61727469636c652f64657461696c732f313436323135363431"
layout: post
title: "大模型微调使用-LLaMA-Factory-微调-Llama3-8B-Chinese-Chat-完成知识问答任务"
date: 2025-03-12 23:16:54 +08:00
description: "本篇博客记录如何使用 LLaMA-Factory 微调 Llama3-8B-Chinese-Chat 完成知识问答任务，并介绍相应报错的解决方法。"
keywords: "为了实现您的目标,即使用llamafactory进行微调训练,并借助dify将pdf文件转换为问"
categories: ['碎片笔记', '大模型']
tags: ['微调', '大模型微调', '大模型', 'Unsloth', 'Llm', 'Llama', 'Factory']
artid: "146215641"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146215641
    alt: "大模型微调使用-LLaMA-Factory-微调-Llama3-8B-Chinese-Chat-完成知识问答任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146215641
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146215641
cover: https://bing.ee123.net/img/rand?artid=146215641
image: https://bing.ee123.net/img/rand?artid=146215641
img: https://bing.ee123.net/img/rand?artid=146215641
---

# 大模型微调｜使用 LLaMA-Factory 微调 Llama3-8B-Chinese-Chat 完成知识问答任务

> **前言**
> ：本篇博客分享如何基于LLaMA-Factory使用现有金融数据集实现LLaMA3的微调，以完成金融领域的知识问答任务。
>
> –
>   
> 参考教程：
> <https://github.com/echonoshy/cgft-llm/tree/master/llama-factory>
>   
> 相关视频：
> <https://www.bilibili.com/video/BV1uw4m1S7Cd/>

📝写在前面：其实github教程已经很详细了，但是自己在按照教程里的步骤实现的时候，由于环境版本各种不适配等原因，还是遇到了很多问题，在此记录一下，希望能够帮到大家。

---

## 0. 计算资源介绍（显卡&cuda）

使用的服务器资源信息：

1. RTX 4090 (24GB) × 1
2. cuda版本 12.0
     
   ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5e6e7716deb24751bf5d3365c7522051.png)

---

## 1. Conda环境搭建（Python 3.10）

新建conda环境，⚠️python版本要高于3.9（否则无法正常使用llamafactory），这里选取python 3.10版本。

```python
conda create -n demo python==3.10
conda activate demo #切换到新建的demo环境中

```

然后，安装必要的库，以下是关键库的版本信息：

⚠️：我这里给出的版本不一定适用于各位，要结合自己的cuda版本（使用
`nvidia-smi`
命令查看）来安装，保证版本的兼容。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/772eeb3abdaa4916ab2e703e3cb09c13.png)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/612f26a8c65d4755b065e07ca5511f2f.png)

⚠️：这一步很关键！一定要选择适合硬件版本的库，否则在后续的微调过程中会经历长时间的debug……

---

## 2. 基座模型下载（LLaMA3 8B）

选用基于中文数据训练过的 LLaMA3 8B 模型：
[shenzhi-wang/Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat)

依次执行以下命令下载上述模型：

```python
pip install -U huggingface_hub
export HF_ENDPOINT=https://hf-mirror.com  # （可选）配置 hf 国内镜像站 
huggingface-cli download --resume-download shenzhi-wang/Llama3-8B-Chinese-Chat --local-dir /newdata/z_demo/Llama3-8B-Chinese-Chat 

```

–local-dir参数
`/newdata/z_demo/Llama3-8B-Chinese-Chat`
替换为下载路径。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1ab3250776c548efbcffaaf37ec9f13b.png)

---

## 3. LLaMA-Factory 框架安装

首先下载LLaMA-Factory，有以下两种方式：

* 执行
  `git clone https://github.com/hiyouga/LLaMA-Factory.git`
  自动下载LLaMA-Factory，但该方法常常由于网络原因下载失败。
* 手动下载该项目后上传到服务器对应目录下。

下载完成后，终端执行
`cd LLaMA-Factory-main`
命令切换到对应目录下，然后执行
`pip install -e .`
安装。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/16a42d552c65425683b90dfea26da42a.png)
  
（安装需要一定时间，本人这里网速较慢，大约1h完成）
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8b01e487a36e4a738252768c3b6b32dd.png)

---

## 4. 训练数据准备

下载训练数据，将其放到LLaMA-Factory-main的data子目录下，包括
`identity.json（457行）`
、
`fintech.json（2648行）`
以及
`dataset_info.json（484行）`
这三个文件。

⚠️：这里的训练数据就是接下来模型微调会用到的数据。

---

## 5. 模型微调

⚠️：可以在步骤5.1的UI界面中开启微调，也可以采用步骤5.2中命令行的方法微调（本博客中选择后者）。

### 5.1 基于WebUI的微调

终端执行
`cd LLaMA-Factory-main`
切换到该目录，然后执行
`llamafactory-cli webui`
打开UI界面：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4b21d5ee821a4e30b2081fb6b16c75fb.png)

### 5.2 基于命令的微调

首先将
`LLaMA-Factory-main/cust/train_llama3_lora_sft.yaml`
文件中的模型加载路径
`model_name_or_path`
和保存路径
`output_dir`
进行如实修改：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c2d398f33c9c4f4e8771f0d9cd2c9ae2.png)

然后在终端依次执行命令：

```python
export NCCL_IB_DISABLE=1
export NCCL_P2P_DISABLE=1
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train cust/train_llama3_lora_sft.yaml # 指定使用id=0的显卡

```

成功训练：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/bbb15620919d4c5790eb48a3d1acacc5.png)
  
经过一段时间（30min左右）的等待，训练完毕：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/57f4692a9fcb444a9c0086c95a95e9d0.png)

至此，得到训练好的 LoRA Adapter（150M左右），保存在之前设置好的
`output_dir`
（saves/LLaMA3-8B-Chinese-Chat/lora/train_demo）目录下，训练loss变化图也在其中（如下）。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/134fe8f7e2b84a11b3e6bc3634e3dd9d.png)

## 6. 微调Debug历程故障排除

⚠️：如果不执行上面那两个export相关命令会报如下错误：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5681df09eef04654b3fd27df91938dcb.png)

⚠️： 如果在环境搭建时未安装unsloth库，则会报错如下：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9a9b4f30b8464a0f8a532d76d838ac17.png)

⚠️：如果遇到下述错误：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/106db6a45562493f921fd7d185a2e835.png)

根据
[相关资料](https://github.com/huggingface/datasets/issues/1784)
，解决方案为：

在
`LLaMA-Factory-main/src/llamafactory/extras/env.py`
文件中添加下列代码：

```python
datasets.builder.has_sufficient_disk_space = lambda needed_bytes, directory='.': True

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/81f0953d8c1d4dc3b3ebf199500485cf.png)

⚠️：如果遇到如下错误，说明triton库的版本出了问题，参见
`步骤1 环境搭建`
安装合适版本的triton库。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c63a394aa6514b409e2906babac80f9b.png)

---

## 7. 在线对话

这里挑选了
`fintech.json`
文件中的一个例子：

> 根据金融研报，为什么白酒板块的估值下探空间已经不大？\n基于以下材料回答上述问题:\n\n投资建议：前期估值和市值对预期反应充分，近期刺激消费的政策陆续出台，我们认为居民消费信心有望逐步恢复，下半年改善节奏有望加速。 白酒板块：三重底部估值底、预期底、基本面底或已确认，预计当前在宏观环境边际改善、业绩确定性较强情况下估值下探空间或已不大、下半年基本面压力趋缓的情况下可以逐步乐观。 首推优质龙头，推荐泸州老窖、贵州茅台、五粮液、山西汾酒、古井贡酒；其次看多超跌的弹性品种，老白干酒、金种子酒、酒鬼酒、顺鑫农业。 大众品板块：消费本身中长期逻辑（消费升级+集中度提升）未发生重大变化，宏观政策向好提振消费信心，我们预计与餐饮、出行相关的消费场景有望优先恢复。

### 7.1 Web UI 对话

**方式一**
：使用 Web UI 界面进行对话：在
`LLaMA-Factory-main`
目录下执行命令
`CUDA_VISIBLE_DEVICES=0 llamafactory-cli webchat cust/train_llama3_lora_sft.yaml`
，报错如下：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0e714c69495f4e24adb9609b18dbf40f.png)
  
根据
[相关资料](https://github.com/hiyouga/LLaMA-Factory/issues/6757)
参考，尝试删除
`train_llama3_lora_sft.yaml`
文件中的未使用参数
`['do_train', 'fp16', 'gradient_accumulation_steps', 'learning_rate', 'logging_steps', 'lr_scheduler_type', 'max_grad_norm', 'num_train_epochs', 'optim', 'output_dir', 'per_device_train_batch_size', 'report_to', 'save_steps', 'warmup_steps']`
，再次运行：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fb8c4fb918784ba5ad5a02d3331f7f64.png)
  
成功跳转到下述页面：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b96c2a1817af466b98d9e3e62824eb4e.png)

Web UI 界面语言默认
`en`
（英文），改成
`zh`
（中文），浅浅尝试一下效果：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/61fbe9370f0f4385b828616fdff3e554.png)
  
回答如下：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2bfed1138e4f422697998785aab4feb0.png)

### 7.2 终端对话

**方式二**
：直接在终端对话，在
`LLaMA-Factory-main`
目录下执行命令
`CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat cust/train_llama3_lora_sft.yaml`
：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ec6c6f7b3edb419683c919b6ec31329b.png)

### 7.3 OpenAI API 对话

**方式三**
：使用OpenAI API风格进行对话：

```python
# 指定多卡和端口
CUDA_VISIBLE_DEVICES=0 API_PORT=8000 
llamafactory-cli api cust/train_llama3_lora_sft.yaml

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/884f5c5c5b0145ef9e58d3ff9a1511db.png)

⚠️：这种方式未成功，知道原因的小伙伴可以分享一下解决方案，在这里不深究了

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c60428f8ae984f7289e48258178856b5.png)

---

## 8. 模型合并

运行
`llamafactory-cli export cust/merge_llama3_lora_sft.yaml`
命令，将步骤2中的基座模型与步骤6中训练好的 LoRA Adapter 合并成一个新的模型。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/69b807b4595449c9b2b18e95ec5263b3.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5083e34f63e3470f95629801cab316a8.png)

---

> **收获**
> ：本次复现首次了解到unsloth库，能够加速⏩大模型微调。此前只是使用过LoRA微调Qwen/llama大模型完成文本分类任务，这次也尝试了使用LLaMA-Factory微调llama，完成专业领域的知识问答任务。之后可以尝试更换微调数据集，完成其他领域的大模型微调。