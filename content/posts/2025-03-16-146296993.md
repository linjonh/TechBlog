---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34333834393530352f:61727469636c652f64657461696c732f313436323936393933"
layout: post
title: "论文阅读Adversarial-Patch-Attacks-on-Monocular-Depth-Estimation-Networks"
date: 2025-03-16 16:55:51 +08:00
description: "使用攻击图像对单目深度估计进行攻击，通过调整区域内的扰动，让深度估计的输出尽可能接近给定值，考虑到现实的攻击需求引入了NPS约束"
keywords: "【论文阅读】Adversarial Patch Attacks on Monocular Depth Estimation Networks"
categories: ['无人驾驶安全']
tags: ['论文阅读', '自动驾驶']
artid: "146296993"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146296993
    alt: "论文阅读Adversarial-Patch-Attacks-on-Monocular-Depth-Estimation-Networks"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146296993
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146296993
cover: https://bing.ee123.net/img/rand?artid=146296993
image: https://bing.ee123.net/img/rand?artid=146296993
img: https://bing.ee123.net/img/rand?artid=146296993
---

# 【论文阅读】Adversarial Patch Attacks on Monocular Depth Estimation Networks

### 一、背景

单目深度估计是CV领域一个比较热门的研究方向，但是现有的方法过度依赖于非深度特征，使得单目深度估计的模型容易受到外界的攻击，针对这一问题该论文设计了一种攻击贴图的方法，用于攻击深度学习实现的单目深度估计模型。通过对特定区域增加扰动，来使得攻击位置附近的深度输出趋近于我们预设的值。

### 二、攻击方法

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e296400f08e74d71a9f5ef368612a4ba.png)
  
和现有的针对CNN的攻击方法很像，这篇文章的攻击方法使用的扰动+贴图的方法，通过在图像上叠加一个贴图，来最大化攻击区域的深度偏差，为了让攻击能够部署在现实中，作者还将成像的过程纳入到了损失函数的构建中。

#### 攻击目标

总的来说，该论文攻击的目标就是让深度估计尽可能偏离真实值。这里作者规定I为模型的输入图像，F为模型的推理，得到的深度为D。原始的贴图被记为P，通过Tθ的调整之后进行叠加。根据这个定义，最主要的损失函数被定义为：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9bcf9d94975449039a94b035b97b94cf.png)
  
其中dt表示的是预设的深度，后面的一半表示的是模型在叠加扰动之后的输出。可以看出这部分损失函数的目标就是让模型输出的深度尽可能接近我们设置的深度值dt。模型调整Rθ区域内的像素的扰动，让深度的输出尽可能接近dt。在调整的过程中，扰动一共有四个参数可以进行调整：随机亮度变化（random brightness shift），对比度（contrast shift），噪声（addition of noise）以及尺度（scaling），通过在小范围内进行微调，实现扰动的内容上的变化。之后再对扰动进行形状上的改变，通过透视变换对原本的矩形区域进行调整：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4cb9697e111b4cd9aa97c79120177206.png)

#### 损失函数

整个攻击的损失函数一共包括三部分，第一部分是上一节说的深度损失，即让深度估计值尽可能接近预设的值。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a3e32b1065984f4ba1b67a90ae21589d.png)
  
对于使用多模型进行攻击时，我们也可以尝试多模型下的深度损失，也就是将上面的公式扩展到多个模型中：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/229a93deae014dc09ad2469e5ee9fee2.png)
  
为了考虑到模型部署在现实中的需求，作者引入了NPS损失来调整攻击图像的色域，让打印出来的图像的色彩尽可能接近原始值：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c2431d4b095344bb81a40d5c8f3a03dc.png)
  
最后作者加了一项平滑约束，让攻击图像尽可能平滑，或者说相邻元素之间的梯度尽可能小：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ac80e45417e04ef7b52c042ffce2c257.png)
  
最终三项损失加权叠加，就能组成最终的损失函数：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6c3a194bdee4456cb0203416dcbdeec1.png)