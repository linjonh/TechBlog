---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34333431343532312f:61727469636c652f64657461696c732f313435353736343931"
layout: post
title: "ç¬¬N5å‘¨Pytorchæ–‡æœ¬åˆ†ç±»å…¥é—¨"
date: 2025-03-10 12:02:21 +08:00
description: "è¿™é‡Œæˆ‘ä»¬å®šä¹‰TextClassificationModel æ¨¡å‹ï¼Œé¦–å…ˆå¯¹æ–‡æœ¬è¿›è¡ŒåµŒå…¥ï¼Œç„¶åå¯¹å¥å­åµŒå…¥ä¹‹åçš„ç»“æœè¿›è¡Œå‡å€¼èšåˆã€‚vocab_size, # è¯å…¸å¤§å°embed_dim, # è¯å…¸ç»´åº¦sparse=False # æ˜¯å¦ä½¿ç”¨ç¨€ç–æ¢¯åº¦ï¼ˆFalseä¸ºä¸ä½¿ç”¨ï¼‰self.embedding.weight.data.uniform_(-initrange,initrange) # åˆå§‹åŒ–åµŒå…¥å±‚çš„æƒé‡ã€‚"
keywords: "ç¬¬N5å‘¨ï¼šPytorchæ–‡æœ¬åˆ†ç±»å…¥é—¨"
categories: ['æœªåˆ†ç±»']
tags: ['äººå·¥æ™ºèƒ½', 'Pytorch', 'Python']
artid: "145576491"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145576491
    alt: "ç¬¬N5å‘¨Pytorchæ–‡æœ¬åˆ†ç±»å…¥é—¨"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145576491
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145576491
cover: https://bing.ee123.net/img/rand?artid=145576491
image: https://bing.ee123.net/img/rand?artid=145576491
img: https://bing.ee123.net/img/rand?artid=145576491
---

# ç¬¬N5å‘¨ï¼šPytorchæ–‡æœ¬åˆ†ç±»å…¥é—¨

> æœ¬äººå¾€æœŸæ–‡ç« å¯æŸ¥é˜…ï¼š
> [æ·±åº¦å­¦ä¹ æ€»ç»“](https://blog.csdn.net/weixin_43414521/article/details/140057978?fromshare=blogdetail&sharetype=blogdetail&sharerId=140057978&sharerefer=PC&sharesource=weixin_43414521&sharefrom=from_link "æ·±åº¦å­¦ä¹ æ€»ç»“")

**æœ¬å‘¨ä»»åŠ¡ï¼š**

* 1.äº†è§£æ–‡æœ¬åˆ†ç±»çš„åŸºæœ¬æµç¨‹
* 2.å­¦ä¹ å¸¸ç”¨çš„æ•°æ®æ¸…æ´—æ–¹æ³•
* 3.å­¦ä¹ å¦‚ä½•ä½¿ç”¨jiebaå®ç°è‹±æ–‡åˆ†è¯
* 4.å­¦ä¹ å¦‚ä½•æ„å»ºæ–‡æœ¬å‘é‡

![](https://i-blog.csdnimg.cn/direct/845b504806c74b8ba7e50d02f163e70e.png)

**ğŸ¡**
**æˆ‘çš„ç¯å¢ƒï¼š**

* è¯­è¨€ç¯å¢ƒï¼šPython3.11

* ç¼–è¯‘å™¨ï¼šPyCharm

* æ·±åº¦å­¦ä¹ ç¯å¢ƒï¼šPytorch

* + torch==2.0.0+cu118

* + torchvision==0.18.1+cu118
* æ˜¾å¡ï¼šNVIDIA GeForce GTX 1660

## ä¸€ã€å‰æœŸå‡†å¤‡

### 1. ç¯å¢ƒå®‰è£…

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨pytorchå®ç°çš„ç®€å•æ–‡æœ¬åˆ†ç±»å®æˆ˜æ¡ˆä¾‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨AG Newsæ•°æ®é›†è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚

AG Newsï¼ˆAG's News Topic Classification Datasetï¼‰æ˜¯ä¸€ä¸ªå¹¿æ³›ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„æ•°æ®é›†ï¼Œå°¤å…¶æ˜¯åœ¨æ–°é—»é¢†åŸŸã€‚è¯¥æ•°æ®é›†æ˜¯æœ‰AG's Corpus of News Articles æ”¶é›†æ•´ç†è€Œæ¥ï¼ŒåŒ…å«äº†å››ä¸ªä¸»è¦çš„ç±»åˆ«ï¼šä¸–ç•Œã€ä½“è‚²ã€å•†ä¸šå’Œç§‘æŠ€ã€‚

é¦–å…ˆï¼Œç¡®ä¿å·²ç»å®‰è£…äº† torchtext ä¸ portalocker åº“

æˆ‘çš„ç‰ˆæœ¬å·æ˜¯ï¼š

* torchtext==0.15.1
* portalocker==2.7.0

æ³¨ï¼šç›¸è¿‘ç‰ˆæœ¬ä¹Ÿå¯ï¼Œä¸å¿…å®Œå…¨ä¸€è‡´

å…¶ä¸­ï¼Œtorchtext
[å®‰è£…ç‰ˆæœ¬å‚è€ƒ](https://github.com/pytorch/text/#installation "å®‰è£…ç‰ˆæœ¬å‚è€ƒ")
ï¼š

|  |  |  |
| --- | --- | --- |
| PyTorch version | tochtext version | Supported Python  version |
| nightly build | main | >=3.8, <=3.11 |
| 1.14.0 | 0.15.0 | >=3.8, <=3.11 |
| 1.13.0 | 0.14.0 | >=3.7, <=3.10 |
| 1.12.0 | 0.13.0 | >=3.7, <=3.10 |
| 1.11.0 | 0.12.0 | >=3.6, <=3.9 |
| 1.10.0 | 0.11.0 | >=3.6, <=3.9 |
| 1.9.1 | 0.10.1 | >=3.6, <=3.9 |
| 1.9 | 0.10 | >=3.6, <=3.9 |
| 1.8.1 | 0.9.1 | >=3.6, <=3.9 |
| 1.8 | 0.9 | >=3.6, <=3.9 |
| 1.7.1 | 0.8.1 | >=3.6, <=3.9 |
| 1.7 | 0.8 | >=3.6, <=3.8 |
| 1.6 | 0.7 | >=3.6, <=3.8 |
| 1.5 | 0.6 | >=3.5, <=3.8 |
| 1.4 | 0.5 | 2.7, >=3.5, <=3.8 |
| 0.4 and below | 0.2.3 | 2.7, >=3.5, <=3.8 |

### 2. åŠ è½½æ•°æ®

```
import torch
import torch.nn as nn
import torchvision
from torchvision import transforms,datasets
import os,PIL,pathlib,warnings

warnings.filterwarnings("ignore")  # å¿½ç•¥è­¦å‘Šä¿¡æ¯

# win10ç³»ç»Ÿï¼Œè°ƒç”¨GPUè¿è¡Œ
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

# è‹¹æœç³»ç»Ÿï¼Œè°ƒç”¨M2èŠ¯ç‰‡
# device=torch.device("mps")
# device
```

è¿è¡Œç»“æœï¼š

```
device(type='cuda')
```

---

torchtext.datasets.AG_NEWS()
æ˜¯ä¸€ä¸ªç”¨äºåŠ è½½ AG News æ•°æ®é›†çš„ TorchText æ•°æ®é›†ç±»ã€‚AG News æ•°æ®é›†æ˜¯ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„å¸¸è§æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å››ä¸ªç±»åˆ«çš„æ–°é—»æ–‡ç« ï¼šä¸–ç•Œã€ç§‘æŠ€ã€ä½“è‚²å’Œå•†ä¸šã€‚
torchtext.datasets.AG_NEWS()
ç±»åŠ è½½çš„æ•°æ®é›†æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªæ¡ç›®éƒ½æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªå…ƒç´ ï¼š

* ä¸€æ¡æ–°é—»æ–‡ç« çš„æ–‡æœ¬å†…å®¹
* æ–°é—»æ–‡ç« æ‰€å±çš„ç±»åˆ«ï¼ˆä¸€ä¸ªæ•´æ•°ï¼Œä»1åˆ°4ï¼Œåˆ†åˆ«å¯¹åº”ä¸–ç•Œã€ç§‘æŠ€ã€ä½“è‚²å’Œå•†ä¸šï¼‰

torchtext.datasets.AG_NEWS()
ç±»æ„é€ å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š

* root ï¼šæ•°æ®é›†çš„æ ¹ç›®å½•ã€‚é»˜è®¤å€¼æ˜¯" .data "
* split ï¼šæ•°æ®é›†çš„æ‹†åˆ† trainã€test
* **kwargs ï¼šå¯é€‰çš„å…³é”®å­—å‚æ•°ï¼Œå¯ä¼ é€’ç»™ torchtext.datasets.TextClassificationDataset ç±»çš„æ„é€ å‡½æ•°

æ³¨ï¼šå¸¸è§çš„åŠ è½½ AG Newsæ•°æ®çš„å‘½ä»¤å¦‚ä¸‹ï¼š

```
import torchtext.datasets as datasets

# åŠ è½½ AG Newsæ•°æ®é›†
train_dataset,test_dataset=datasets.AG_NEWS(root=".data",split=("train","test"))
```

ä½†ç”±äºæœ¬äººç½‘é€Ÿé—®é¢˜ï¼Œä¸€ç›´æ˜¾ç¤ºè¿æ¥è¶…æ—¶ï¼Œæ— æ³•ä¸‹è½½ã€‚æ•…æˆ‘å…ˆåˆ›å»ºè®¾å®šçš„æ–‡ä»¶å¤¹ï¼Œå°†æ•°æ®é›†å•ç‹¬ä¸‹è½½ä¹‹åï¼Œæ”¾å…¥è¯¥æ–‡ä»¶å¤¹å†…Â ï¼Œç„¶åå†è¿›è¡ŒåŠ è½½ã€‚å®é™…æ“ä½œä¸­å¯ä»¥æ ¹æ®ä¸ªäººéœ€æ±‚è¿›è¡Œæ“ä½œã€‚

![](https://i-blog.csdnimg.cn/direct/e1615e8e05654c419384800039c77612.png)

```
from torchtext.datasets import AG_NEWS

# åˆ›å»ºæ•°æ®å­˜å‚¨ç›®å½•
os.makedirs(r'E:\DATABASE\N-series\N5',exist_ok=True)

# åŠ è½½ AG News æ•°æ®é›†
train_iter,test_iter=AG_NEWS(root=r'E:\DATABASE\N-series\N5',split=('train','test')) 
```

### 3. æ„å»ºè¯å…¸

```
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

# è¿”å›åˆ†è¯å™¨å‡½æ•°
tokenizer=get_tokenizer('basic_english')

# å®šä¹‰ç”Ÿæˆ tokens çš„å‡½æ•°
def yield_tokens(data_iter):
    for _,text in data_iter: #_è¡¨ç¤ºä¸å…³å¿ƒè¿­ä»£å™¨ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªæ ‡ç­¾ï¼‰ï¼Œæˆ‘ä»¬åªå…³å¿ƒæ–‡æœ¬å†…å®¹ã€‚
        yield tokenizer(text)

vocab=build_vocab_from_iterator(
    yield_tokens(train_iter),
    specials=["<unk>"]
)
# è®¾ç½®é»˜è®¤ç´¢å¼•ï¼Œå¦‚æœæ‰¾ä¸åˆ°å•è¯ï¼Œåˆ™ä¼šé€‰æ‹©é»˜è®¤ç´¢å¼•
vocab.set_default_index(vocab["<unk>"])
```

torchtext.data.utils.get_tokenizer()

æ˜¯ä¸€ä¸ªç”¨äºå°†æ–‡æœ¬æ•°æ®åˆ†è¯çš„å‡½æ•°ã€‚
ä½œç”¨æ˜¯æ ¹æ®æŒ‡å®šçš„åˆ†è¯å™¨åç§°æˆ–è‡ªå®šä¹‰å‡½æ•°ï¼Œç”Ÿæˆä¸€ä¸ªåˆ†è¯å™¨å¯¹è±¡ï¼ˆtokenizerï¼‰
ï¼Œå¯ä»¥å°†ä¸€ä¸ªå­—ç¬¦ä¸²è½¬æ¢æˆä¸€ä¸ªå•è¯çš„åˆ—è¡¨ã€‚è¿™ä¸ªå‡½æ•°å¯ä»¥æ¥å—ä¸¤ä¸ªå‚æ•°ï¼štokenizer å’Œ languageï¼Œtokenizer å‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„åˆ†è¯å™¨çš„åç§°ã€‚
å®ƒæ”¯æŒå¤šç§é¢„å®šä¹‰çš„åˆ†è¯å™¨ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„åˆ†è¯å‡½æ•°ã€‚

> #### ç¤ºä¾‹ï¼š
>
> ```
> from torchtext.data.utils import get_tokenizer
>
> # è·å–åŸºæœ¬è‹±æ–‡åˆ†è¯å™¨
> tokenizer = get_tokenizer("basic_english")
>
> # ç¤ºä¾‹æ–‡æœ¬
> text = "This is a sample sentence for tokenization."
>
> # åˆ†è¯
> tokens = tokenizer(text)
> print("Original text:", text)
> print("Tokenized text:", tokens)
> ```
>
> è¾“å‡ºï¼š
>
> ```
> Original text: This is a sample sentence for tokenization.
> Tokenized text: ['this', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']
> ```

> `get_tokenizer()`
> æ”¯æŒä»¥ä¸‹åˆ†è¯å™¨ï¼š
>
> * **`"basic_english"`**
>   : ç®€å•çš„è‹±æ–‡åˆ†è¯å™¨ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å¹¶æŒ‰ç©ºæ ¼åˆ†å‰²ã€‚
> * **`"spacy"`**
>   : ä½¿ç”¨
>   `spacy`
>   åº“çš„åˆ†è¯å™¨ã€‚
> * **`"moses"`**
>   : ä½¿ç”¨
>   `sacremoses`
>   åº“çš„åˆ†è¯å™¨ã€‚
> * **`"toktok"`**
>   : ä½¿ç”¨
>   `nltk`
>   åº“çš„åˆ†è¯å™¨ã€‚
> * **`"revtok"`**
>   : ä½¿ç”¨
>   `revtok`
>   åº“çš„åˆ†è¯å™¨ã€‚
> * **`"subword"`**
>   : ä½¿ç”¨
>   `subword`
>   åº“çš„åˆ†è¯å™¨ã€‚

---

> ```
> torchtext.vocab.build_vocab_from_iterator(
>     iterator,
>     specials=None,
>     min_freq=1,
>     max_size=None,
>     vectors=None,
>     vectors_cache=None,
>     unk_init=None,
>     special_first=True
> )
> ```
>
> `build_vocab_from_iterator`
> çš„å‚æ•°å¦‚ä¸‹ï¼š
>
> * **`iterator`**
>   : ä¸€ä¸ªç”Ÿæˆå™¨æˆ–è¿­ä»£å™¨ï¼Œé€ä¸ªè¿”å›åˆ†è¯åçš„æ–‡æœ¬ï¼ˆé€šå¸¸æ˜¯æ ‡è®°åˆ—è¡¨ï¼‰ã€‚
> * **`specials`**
>   : ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚
>   `<unk>`
>   ã€
>   `<pad>`
>   ç­‰ï¼‰ã€‚è¿™äº›æ ‡è®°ä¼šè¢«æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼Œå¹¶åˆ†é…å›ºå®šçš„ç´¢å¼•ã€‚
> * **`min_freq`**
>   : æœ€å°é¢‘ç‡ï¼Œé»˜è®¤ä¸º 1ã€‚åªæœ‰å‡ºç°æ¬¡æ•°å¤§äºæˆ–ç­‰äºè¯¥å€¼çš„å•è¯æ‰ä¼šè¢«åŠ å…¥è¯æ±‡è¡¨ã€‚
> * **`max_size`**
>   : è¯æ±‡è¡¨çš„æœ€å¤§å¤§å°ã€‚å¦‚æœæŒ‡å®šï¼Œè¯æ±‡è¡¨çš„å¤§å°ä¸ä¼šè¶…è¿‡è¯¥å€¼ã€‚
> * **`vectors`**
>   : å¯é€‰å‚æ•°ï¼Œç”¨äºåŠ è½½é¢„è®­ç»ƒçš„è¯åµŒå…¥å‘é‡ã€‚
> * **`vectors_cache`**
>   : é¢„è®­ç»ƒå‘é‡çš„ç¼“å­˜è·¯å¾„ã€‚
> * **`unk_init`**
>   : æœªçŸ¥å•è¯çš„åˆå§‹åŒ–æ–¹æ³•ï¼Œé»˜è®¤ä¸ºéšæœºåˆå§‹åŒ–ã€‚
>
> ---

---

> åŸå‡½æ•°ä¸­ï¼š
>
> * **`yield_tokens(train_iter)`**
>   : è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œé€ä¸ªè¿”å›åˆ†è¯åçš„æ–‡æœ¬ã€‚
> * **`specials=["<unk>"]`**
>   : æŒ‡å®šç‰¹æ®Šæ ‡è®°ï¼Œ
>   `<unk>`
>   é€šå¸¸ç”¨äºè¡¨ç¤ºæœªçŸ¥å•è¯ã€‚
> * **`vocab`**
>   : æ„å»ºçš„è¯æ±‡è¡¨å¯¹è±¡ï¼Œå¯ä»¥å°†å•è¯æ˜ å°„åˆ°ç´¢å¼•ã€‚
>
> å‡è®¾æ•°æ®é›†ï¼š
>
> ```
> train_iter = [
>     (0, "This is a sample sentence."),
>     (1, "Another example text."),
>     (0, "Text classification is fun!")
> ]
> ```
>
> ä½¿ç”¨ç”Ÿæˆå™¨æ„å»ºè¯æ±‡è¡¨å¹¶è®¾ç½®é»˜è®¤ç´¢å¼•åï¼Œæ‰“å°è¯æ±‡è¡¨ï¼š
>
> ```
> print("Vocabulary:", vocab)
> print("Token to Index:", vocab["sample"])
> print("Unknown Token Index:", vocab["unknown_token"])
> ```
>
> è¾“å‡ºï¼š
>
> ```
> Vocabulary: <torchtext.vocab.Vocab object at 0x...>
> Token to Index: 4
> Unknown Token Index: 0
> ```

---

```
print(vocab(['here','is','an','example']))
```

è¿è¡Œç»“æœï¼š

```
[475, 21, 30, 5297]
```

```
text_pipeline=lambda x:vocab(tokenizer(x))
label_pipeline=lambda x:int(x)-1

text_pipeline('here is the an example')
```

è¿è¡Œç»“æœï¼š

```
[475, 21, 2, 30, 5297]
```

---

```
label_pipeline('10')
```

è¿è¡Œç»“æœï¼š

```
9
```

### 4. ç”Ÿæˆæ•°æ®æ‰¹æ¬¡å’Œè¿­ä»£å™¨

```
from torch.utils.data import DataLoader

def collate_batch(batch):
    label_list,text_list,offsets=[],[],[0]
    
    for (_label,_text) in batch:
        # æ ‡ç­¾åˆ—è¡¨
        label_list.append(label_pipeline(_label))
        
        # æ–‡æœ¬åˆ—è¡¨
        processed_text=torch.tensor(text_pipeline(_text),dtype=torch.int64)
        text_list.append(processed_text)
        
        # åç§»é‡ï¼Œå³è¯­å¥çš„æ€»è¯æ±‡é‡
        offsets.append(processed_text.size(0))
        
    label_list=torch.tensor(label_list,dtype=torch.int64)
    text_list=torch.cat(text_list)
    offsets=torch.tensor(offsets[:-1]).cumsum(dim=0) # è¿”å›ç»´åº¦dimä¸­è¾“å…¥å…ƒç´ çš„ç´¯è®¡å’Œ
    
    return label_list.to(device),text_list.to(device),offsets.to(device)

# æ•°æ®åŠ è½½å™¨
dataloader=DataLoader(
    train_iter,
    batch_size=8,
    shuffle=False,
    collate_fn=collate_batch
)
```

`collate_batch`
å‡½æ•°å®šä¹‰äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„æ‰¹å¤„ç†å‡½æ•°ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æ•°æ®å’Œæ ‡ç­¾ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹è¾“å…¥çš„æ ¼å¼ã€‚è¿™ä¸ªå‡½æ•°ç‰¹åˆ«é€‚ç”¨äºå¤„ç†æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ‰¹æ¬¡æ•°æ®ã€‚

#### 1.4.1 è¾“å…¥

`collate_batch`
çš„è¾“å…¥æ˜¯ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®
`batch`
ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå…ƒç»„
`(_label, _text)`
ï¼Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬çš„æ ‡ç­¾å’Œæ–‡æœ¬å†…å®¹ã€‚

#### 1.4.2 å˜é‡åˆå§‹åŒ–

```
label_list, text_list, offsets = [], [], [0]
```

* **`label_list`**
  : ç”¨äºå­˜å‚¨å¤„ç†åçš„æ ‡ç­¾ã€‚
* **`text_list`**
  : ç”¨äºå­˜å‚¨å¤„ç†åçš„æ–‡æœ¬æ•°æ®ï¼ˆç´¢å¼•åˆ—è¡¨ï¼‰ã€‚
* **`offsets`**
  : ç”¨äºå­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬é•¿åº¦çš„ç´¯ç§¯å’Œï¼Œç”¨äºåç»­çš„åµŒå…¥æ“ä½œã€‚

#### 1.4.3 å¤„ç†æ¯ä¸ªæ ·æœ¬

```
for (_label, _text) in batch:
    label_list.append(label_pipeline(_label))  # å¤„ç†æ ‡ç­¾
    processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)  # å¤„ç†æ–‡æœ¬
    text_list.append(processed_text)  # å°†å¤„ç†åçš„æ–‡æœ¬æ·»åŠ åˆ°åˆ—è¡¨
    offsets.append(processed_text.size(0))  # è®°å½•æ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬é•¿åº¦
```

#### 1.4.4 è½¬æ¢ä¸ºå¼ é‡

```
label_list = torch.tensor(label_list, dtype=torch.int64)  # è½¬æ¢æ ‡ç­¾åˆ—è¡¨ä¸ºå¼ é‡
text_list = torch.cat(text_list)  # å°†æ–‡æœ¬åˆ—è¡¨æ‹¼æ¥æˆä¸€ä¸ªå¼ é‡
offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # è®¡ç®—åç§»é‡çš„ç´¯ç§¯å’Œ
```

* `offsets`
  æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œè®°å½•äº†æ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬é•¿åº¦ã€‚
* `offsets[:-1]`
  è¡¨ç¤ºå–
  `offsets`
  åˆ—è¡¨çš„æ‰€æœ‰å…ƒç´ ï¼Œé™¤äº†æœ€åä¸€ä¸ªå…ƒç´ ã€‚
* åŸå› æ˜¯æœ€åä¸€ä¸ªå…ƒç´ æ˜¯æ‰€æœ‰æ ·æœ¬çš„æ€»é•¿åº¦ï¼Œè€Œæˆ‘ä»¬åªéœ€è¦æ¯ä¸ªæ ·æœ¬çš„ç´¯ç§¯é•¿åº¦ã€‚
* **`torch.tensor(offsets[:-1])ï¼š`**
  å°†
  `offsets[:-1]`
  è½¬æ¢ä¸ºä¸€ä¸ª PyTorch å¼ é‡ã€‚
* **`.cumsum(dim=0)ï¼š`**
  è®¡ç®—å¼ é‡çš„ç´¯ç§¯å’Œï¼ˆcumulative sumï¼‰ã€‚
  `dim=0`
  è¡¨ç¤ºæ²¿ç€ç¬¬ 0 ç»´åº¦ï¼ˆå³åˆ—è¡¨çš„ä¸»ç»´åº¦ï¼‰è¿›è¡Œç´¯ç§¯å’Œè®¡ç®—ã€‚

ç¤ºä¾‹ï¼š

> å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹æ–‡æœ¬æ•°æ®ï¼š
>
> ```
> texts = ["This is a sample text.", "Another example text."]
> ```
>
> åˆ†è¯åçš„ç»“æœå¦‚ä¸‹ï¼š
>
> ```
> text_pipeline("This is a sample text.") -> [1, 2, 3, 4, 5]
> text_pipeline("Another example text.") -> [6, 7, 8]
> ```
>
> è®¡ç®—
> `offsetsï¼š`
>
> ```
> offsets = [0, 5, 8]  # æ¯ä¸ªæ ·æœ¬çš„æ–‡æœ¬é•¿åº¦
> ```
>
> å¤„ç†
> `offsets`
> ï¼š
>
> ```
> offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)
> ```
>
> è¾“å‡ºï¼š
>
> ```
> print(offsets)
> ```
>
> ç»“æœï¼š
>
> ```
> tensor([0, 5])
> ```
>
> è§£é‡Š
>
> * **`offsets`**
>   çš„å«ä¹‰ï¼š
>
>   + `0`
>     : ç¬¬ä¸€ä¸ªæ ·æœ¬çš„èµ·å§‹ä½ç½®ã€‚
>   + `5`
>     : ç¬¬äºŒä¸ªæ ·æœ¬çš„èµ·å§‹ä½ç½®ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬çš„é•¿åº¦ä¸º 5ï¼‰ã€‚
> * ç´¯ç§¯å’Œï¼š
>
>   + `cumsum`
>     çš„ä½œç”¨æ˜¯å°†æ¯ä¸ªå…ƒç´ æ›¿æ¢ä¸ºä»åˆ—è¡¨å¼€å¤´åˆ°è¯¥å…ƒç´ çš„ç´¯ç§¯å’Œã€‚
>   + ä¾‹å¦‚ï¼Œ
>     `[0, 5, 8]`
>     çš„ç´¯ç§¯å’Œä¸º
>     `[0, 5]`
>     ï¼ˆæœ€åä¸€ä¸ªå…ƒç´ è¢«ä¸¢å¼ƒï¼‰ã€‚

## äºŒã€å‡†å¤‡æ¨¡å‹

### 1. å®šä¹‰æ¨¡å‹

è¿™é‡Œæˆ‘ä»¬å®šä¹‰TextClassificationModel æ¨¡å‹ï¼Œé¦–å…ˆå¯¹æ–‡æœ¬è¿›è¡ŒåµŒå…¥ï¼Œç„¶åå¯¹å¥å­åµŒå…¥ä¹‹åçš„ç»“æœè¿›è¡Œå‡å€¼èšåˆã€‚

![](https://i-blog.csdnimg.cn/direct/fe6c3bc8a1944548bfe2960c5e9cf980.png)

```
from torch import nn

class TextClassificationModel(nn.Module):
    def __init__(self,vocab_size,embed_dim,num_class):
        super(TextClassificationModel,self).__init__()
        
        self.embedding=nn.EmbeddingBag(
            vocab_size,  # è¯å…¸å¤§å°
            embed_dim,  # è¯å…¸ç»´åº¦
            sparse=False  # æ˜¯å¦ä½¿ç”¨ç¨€ç–æ¢¯åº¦ï¼ˆFalseä¸ºä¸ä½¿ç”¨ï¼‰
        )
        
        self.fc=nn.Linear(embed_dim,num_class)
        self.init_weights()
        
    def init_weights(self):
        initrange=0.5
        self.embedding.weight.data.uniform_(-initrange,initrange) # åˆå§‹åŒ–åµŒå…¥å±‚çš„æƒé‡ 
        self.fc.weight.data.uniform_(-initrange,initrange) # åˆå§‹åŒ–å…¨è¿æ¥å±‚çš„æƒé‡
        self.fc.bias.data.zero_() # åˆå§‹åŒ–å…¨è¿æ¥å±‚çš„åç½®é¡¹ä¸ºé›¶
        
    def forward(self,text,offsets):
        embedded=self.embedding(text,offsets)
        return self.fc(embedded)
```

self.embedding.weight.data.uniform_(-initrange,initrange)Â  è¿™æ®µä»£ç æ˜¯åœ¨PyTorch æ¡†æ¶ä¸‹ç”¨äºåˆå§‹åŒ–ç¥ç»ç½‘ç»œçš„è¯åµŒå…¥å±‚ï¼ˆembedding layerï¼‰æƒé‡çš„ä¸€ç§æ–¹æ³•ã€‚è¿™é‡Œä½¿ç”¨äº†å‡åŒ€åˆ†å¸ƒçš„éšæœºå€¼æ¥åˆå§‹åŒ–æƒé‡ï¼Œå…·ä½“æ¥è¯´ï¼Œå…¶ä½œç”¨å¦‚ä¸‹ï¼š

* 1. self.embeddingÂ ï¼šè¿™æ˜¯ç¥ç»ç½‘ç»œä¸­çš„è¯åµŒå…¥å±‚ï¼ˆembedding layerï¼‰ã€‚è¯åµŒå…¥å±‚çš„ä½œç”¨æ˜¯å°†ç¦»æ•£çš„å•è¯è¡¨ç¤ºï¼ˆé€šå¸¸ä¸ºæ•´æ•°ç´¢å¼•ï¼‰æ˜ å°„ä¸ºå›ºå®šå¤§å°çš„è¿ç»­å‘é‡ã€‚è¿™äº›å‘é‡æ•æ‰äº†å•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ï¼Œå¹¶ä½œä¸ºç½‘ç»œçš„è¾“å…¥ã€‚
* 2.Â  self.embedding.weight ï¼šè¿™æ˜¯è¯åµŒå…¥å±‚çš„æƒé‡çŸ©é˜µï¼Œå®ƒçš„å½¢çŠ¶ä¸º ï¼ˆvocab_size,embedding_dimï¼‰ï¼Œå…¶ä¸­ vocab_size æ˜¯è¯æ±‡è¡¨çš„å¤§å°ï¼Œembedding_dim æ˜¯åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚
* 3.Â  self.embedding.weight.data ï¼šè¿™æ˜¯æƒé‡çŸ©é˜µçš„æ•°æ®éƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œç›´æ¥æ“ä½œå…¶åº•å±‚çš„å¼ é‡
* 4.Â .uniform_(-initrange,initrange) ï¼šè¿™æ˜¯ä¸€ä¸ªåŸåœ°æ“ä½œï¼ˆin-place operationï¼‰ï¼Œç”¨äºå°†æƒé‡çŸ©é˜µçš„å€¼ç”¨ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒè¿›è¡Œåˆå§‹åŒ–ã€‚å‡åŒ€åˆ†å¸ƒçš„èŒƒå›´ä¸º [-initrange,initrange] ï¼Œå…¶ä¸­initrangeæ˜¯ä¸€ä¸ªæ­£æ•°ã€‚

é€šè¿‡è¿™ç§æ–¹å¼åˆå§‹åŒ–è¯åµŒå…¥å±‚çš„æƒé‡ï¼Œå¯ä»¥ä½¿å¾—æ¨¡å‹åœ¨è®­ç»ƒæ—¶å…·æœ‰ä¸€å®šçš„éšæœºæ€§ï¼Œæœ‰åŠ©äºé¿å…æ¢¯åº¦å°æ—¶æˆ–æ¢¯åº¦çˆ†ç‚¸ç­‰é—®é¢˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™äº›æƒé‡å°†é€šè¿‡ä¼˜åŒ–ç®—æ³•ä¸æ–­æ›´æ–°ï¼Œä»¥æ•æ‰æ›´å¥½çš„å•è¯è¡¨ç¤ºã€‚

---

```
def forward(self,text,offsets):
        embedded=self.embedding(text,offsets)
        return self.fc(embedded)
```

ï¼ˆ1ï¼‰å‚æ•°

* **`text`**
  : ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ŒåŒ…å«æ‰€æœ‰æ ·æœ¬çš„å•è¯ç´¢å¼•ã€‚
* **`offsets`**
  : ä¸€ä¸ªä¸€ç»´å¼ é‡ï¼ŒåŒ…å«æ¯ä¸ªæ ·æœ¬çš„èµ·å§‹ç´¢å¼•ä½ç½®ã€‚

ï¼ˆ2ï¼‰
**`self.embedding`**

* `self.embedding`
  æ˜¯ä¸€ä¸ª
  `torch.nn.EmbeddingBag`
  å±‚ã€‚
* å®ƒæ¥å—ä¸¤ä¸ªå‚æ•°ï¼š

  + `text`
    : åŒ…å«æ‰€æœ‰å•è¯ç´¢å¼•çš„å¼ é‡ã€‚
  + `offsets`
    : æ¯ä¸ªæ ·æœ¬çš„èµ·å§‹ç´¢å¼•ä½ç½®ã€‚
* `EmbeddingBag`
  ä¼šæ ¹æ®
  `offsets`
  å°†
  `text`
  åˆ†å‰²æˆå¤šä¸ªâ€œè¢‹â€ï¼ˆå³æ ·æœ¬ï¼‰ï¼Œå¹¶å¯¹æ¯ä¸ªâ€œè¢‹â€ä¸­çš„åµŒå…¥å‘é‡è¿›è¡Œèšåˆï¼ˆå¦‚æ±‚å’Œæˆ–å¹³å‡ï¼‰ã€‚

ï¼ˆ3ï¼‰ self.fc

* `self.fc`
  æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚ï¼Œç”¨äºå°†åµŒå…¥å‘é‡æ˜ å°„åˆ°è¾“å‡ºç»´åº¦ã€‚
* å®ƒæ¥å—
  `EmbeddingBag`
  çš„è¾“å‡ºï¼Œå¹¶è¿”å›æœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚

### 2. å®šä¹‰å®ä¾‹

```
num_class=len(set([label for (label,text) in train_iter]))
vocab_size=len(vocab)
em_size=64
model=TextClassificationModel(vocab_size,em_size,num_class).to(device)
```

### 3. å®šä¹‰è®­ç»ƒå‡½æ•°ä¸è¯„ä¼°å‡½æ•°

```
import time

criterion = nn.CrossEntropyLoss()

def train(dataloader):
    model.train() # åˆ‡æ¢ä¸ºè®­ç»ƒæ¨¡å¼
    # åˆå§‹åŒ–å˜é‡ï¼Œåˆ†åˆ«ç”¨äºç´¯è®¡è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡†ç¡®ç‡ã€æŸå¤±å’Œæ ·æœ¬æ•°é‡
    total_acc,train_loss,total_count=0,0,0
    # æ¯éš” log_interval æ­¥è®°å½•ä¸€æ¬¡è®­ç»ƒè¿›åº¦ï¼Œæ–¹ä¾¿ç›‘æ§è®­ç»ƒè¿‡ç¨‹
    log_interval=500
    # ç”¨äºè®¡ç®—è®­ç»ƒè¿‡ç¨‹ä¸­çš„è€—æ—¶
    start_time=time.time()
    
    for idx,(label,text,offsets) in enumerate(dataloader):
        
        predicted_label=model(text,offsets)
        
        optimizer.zero_grad() # gradå±æ€§å½’é›¶
        loss=criterion(predicted_label,label) # è®¡ç®—ç½‘ç»œè¾“å‡ºå’ŒçœŸå®å€¼ä¹‹é—´çš„å·®è·ï¼Œlabelä¸ºçœŸå®å€¼
        loss.backward() # åå‘ä¼ æ’­
        optimizer.step() # æ¯ä¸€æ­¥è‡ªåŠ¨æ›´æ–°
        
        # è®°å½•accä¸loss
        total_acc+=(predicted_label.argmax(1)==label).sum().item()
        train_loss+=loss.item()
        total_count+=label.size(0)
        
        if idx % log_interval==0 and idx > 0:
            elapsed=time.time()-start_time
            print('| epoch{:1d} | {:4d}/{:4d} batches '
                  '| train_acc {:4.3f} | train_loss {:4.5f}'.format(epoch,idx,len(dataloader),
                                                                    total_acc/total_count,
                                                                    train_loss/total_count))
            total_acc,train_loss,total_count=0,0,0
            start_time=time.time()
            
def evaluate(dataloader):
    model.eval() # åˆ‡æ¢ä¸ºæµ‹è¯•æ¨¡å¼
    total_acc,train_loss,total_count=0,0,0
    
    with torch.no_grad():
        for idx,(label,text,offsets) in enumerate(dataloader):
            predicted_label=model(text,offsets)
            
            loss=criterion(predicted_label,label) # è®¡ç®—losså€¼
            # è®°å½•æµ‹è¯•æ•°æ®
            total_acc+=(predicted_label.argmax(1)==label).sum().item() # ç´¯è®¡å‡†ç¡®ç‡
            train_loss+=loss.item()  # ç´¯è®¡æŸå¤±
            total_count+=label.size(0) # ç´¯ç§¯å¤„ç†çš„æ ·æœ¬æ€»æ•°
    
    # è¿”å›å¹³å‡å‡†ç¡®ç‡å’Œå¹³å‡æŸå¤±
    return total_acc/total_count,train_loss/total_count
```

## ä¸‰ã€è®­ç»ƒæ¨¡å‹

### 1. æ‹†åˆ†æ•°æ®é›†å¹¶è¿è¡Œæ¨¡å‹

```
from torch.utils.data.dataset import random_split
from torchtext.data.functional import to_map_style_dataset
# è¶…å‚æ•°
EPOCHS=10 # epoch
LR=5 # å­¦ä¹ ç‡
BATCH_SIZE=64 # batch size for training

criterion=torch.nn.CrossEntropyLoss()
optimizer=torch.optim.SGD(model.parameters(),lr=LR)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,1.0,gamma=0.1)
total_accu=None

train_iter,test_iter=AG_NEWS(root=r'E:\DATABASE\N-series\N5',split=('train','test')) # åŠ è½½æ•°æ®
train_dataset=to_map_style_dataset(train_iter)
test_dataset=to_map_style_dataset(test_iter)
num_train=int(len(train_dataset)*0.95)

split_train_,split_valid_=random_split(
    train_dataset,
    [num_train,len(train_dataset)-num_train]
)
train_dataloader=DataLoader(
    split_train_,batch_size=BATCH_SIZE,
    shuffle=True,collate_fn=collate_batch
)
valid_dataloader=DataLoader(
    split_valid_,batch_size=BATCH_SIZE,
    shuffle=True,collate_fn=collate_batch
)
test_dataloader=DataLoader(
    test_dataset,batch_size=BATCH_SIZE,
    shuffle=True,collate_fn=collate_batch
)

for epoch in range(1,EPOCHS+1):
    epoch_start_time=time.time()
    train(train_dataloader)
    val_acc,val_loss=evaluate(valid_dataloader)
    
    if total_accu is not None and total_accu>val_acc:
        scheduler.step()
    else:
        total_accu=val_acc
    print('-'*69)
    print('| epoch{:1d} | time:{:4.2f}s |'
          'valid_acc {:4.3f} valid_loss {:4.3f}'.format(epoch,
                                                        time.time()-epoch_start_time,
                                                        val_acc,val_loss))
    print('-'*69)
```

è¿è¡Œç»“æœï¼š

```
| epoch1 |  500/1782 batches | train_acc 0.711 | train_loss 0.01143
| epoch1 | 1000/1782 batches | train_acc 0.867 | train_loss 0.00615
| epoch1 | 1500/1782 batches | train_acc 0.886 | train_loss 0.00532
---------------------------------------------------------------------
| epoch1 | time:10.26s |valid_acc 0.857 valid_loss 0.006
---------------------------------------------------------------------
| epoch2 |  500/1782 batches | train_acc 0.905 | train_loss 0.00447
| epoch2 | 1000/1782 batches | train_acc 0.905 | train_loss 0.00438
| epoch2 | 1500/1782 batches | train_acc 0.901 | train_loss 0.00462
---------------------------------------------------------------------
| epoch2 | time:10.28s |valid_acc 0.896 valid_loss 0.005
---------------------------------------------------------------------
| epoch3 |  500/1782 batches | train_acc 0.919 | train_loss 0.00374
| epoch3 | 1000/1782 batches | train_acc 0.916 | train_loss 0.00388
| epoch3 | 1500/1782 batches | train_acc 0.914 | train_loss 0.00391
---------------------------------------------------------------------
| epoch3 | time:10.65s |valid_acc 0.896 valid_loss 0.005
---------------------------------------------------------------------
| epoch4 |  500/1782 batches | train_acc 0.929 | train_loss 0.00330
| epoch4 | 1000/1782 batches | train_acc 0.925 | train_loss 0.00341
| epoch4 | 1500/1782 batches | train_acc 0.921 | train_loss 0.00360
---------------------------------------------------------------------
| epoch4 | time:11.16s |valid_acc 0.883 valid_loss 0.005
---------------------------------------------------------------------
| epoch5 |  500/1782 batches | train_acc 0.943 | train_loss 0.00272
| epoch5 | 1000/1782 batches | train_acc 0.942 | train_loss 0.00278
| epoch5 | 1500/1782 batches | train_acc 0.943 | train_loss 0.00271
---------------------------------------------------------------------
| epoch5 | time:11.58s |valid_acc 0.918 valid_loss 0.004
---------------------------------------------------------------------
| epoch6 |  500/1782 batches | train_acc 0.948 | train_loss 0.00260
| epoch6 | 1000/1782 batches | train_acc 0.945 | train_loss 0.00263
| epoch6 | 1500/1782 batches | train_acc 0.944 | train_loss 0.00267
---------------------------------------------------------------------
| epoch6 | time:12.00s |valid_acc 0.919 valid_loss 0.004
---------------------------------------------------------------------
| epoch7 |  500/1782 batches | train_acc 0.946 | train_loss 0.00264
| epoch7 | 1000/1782 batches | train_acc 0.948 | train_loss 0.00248
| epoch7 | 1500/1782 batches | train_acc 0.946 | train_loss 0.00260
---------------------------------------------------------------------
| epoch7 | time:10.66s |valid_acc 0.918 valid_loss 0.004
---------------------------------------------------------------------
| epoch8 |  500/1782 batches | train_acc 0.952 | train_loss 0.00239
| epoch8 | 1000/1782 batches | train_acc 0.949 | train_loss 0.00253
| epoch8 | 1500/1782 batches | train_acc 0.947 | train_loss 0.00252
---------------------------------------------------------------------
| epoch8 | time:11.81s |valid_acc 0.919 valid_loss 0.004
---------------------------------------------------------------------
| epoch9 |  500/1782 batches | train_acc 0.947 | train_loss 0.00253
| epoch9 | 1000/1782 batches | train_acc 0.948 | train_loss 0.00250
| epoch9 | 1500/1782 batches | train_acc 0.951 | train_loss 0.00244
---------------------------------------------------------------------
| epoch9 | time:11.17s |valid_acc 0.919 valid_loss 0.004
---------------------------------------------------------------------
| epoch10 |  500/1782 batches | train_acc 0.948 | train_loss 0.00254
| epoch10 | 1000/1782 batches | train_acc 0.950 | train_loss 0.00244
| epoch10 | 1500/1782 batches | train_acc 0.949 | train_loss 0.00247
---------------------------------------------------------------------
| epoch10 | time:12.12s |valid_acc 0.920 valid_loss 0.004
---------------------------------------------------------------------
```

torchtext.data.functional.to_map_style_dataset
å‡½æ•°çš„ä½œç”¨æ˜¯å°†ä¸€ä¸ªè¿­ä»£å¼çš„æ•°æ®é›†ï¼ˆIterable-style datasetï¼‰è½¬æ¢ä¸ºæ˜ å°„å¼çš„æ•°æ®é›†ï¼ˆMap-style datasetï¼‰ã€‚è¿™ä¸ªè½¬æ¢ä½¿å¾—æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•ï¼ˆä¾‹å¦‚ï¼šæ•´æ•°ï¼‰æ›´æ–¹ä¾¿åœ°è®¿é—®æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

åœ¨PyTorchä¸­ï¼Œæ•°æ®é›†å¯ä»¥åˆ†ä¸ºä¸¤ç§ç±»å‹ï¼šIterable-style å’Œ Map-style ã€‚Iterable-style æ•°æ®é›†å®ç°äº†
__iter__()
æ–¹æ³•ï¼Œå¯ä»¥è¿­ä»£è®¿é—®æ•°æ®é›†ä¸­çš„å…ƒç´ ï¼Œä½†ä¸æ”¯æŒé€šè¿‡ç´¢å¼•è®¿é—®ã€‚è€Œ Map-style æ•°æ®é›†å®ç°äº†
__getitem__()
å’Œ
__len__()
æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡ç´¢å¼•è®¿é—®ç‰¹å®šå…ƒç´ ï¼Œå¹¶èƒ½è·å–æ•°æ®é›†çš„å¤§å°ã€‚

TorchText æ˜¯ PyTorch çš„ä¸€ä¸ªæ‰©å±•åº“ï¼Œä¸“æ³¨äºå¤„ç†æ–‡æœ¬æ•°æ®ã€‚torchtext.data.functional ä¸­çš„ to_map_style_dataset å‡½æ•°å¯ä»¥å¸®åŠ©æˆ‘ä»¬å°†ä¸€ä¸ª Iterable-style æ•°æ®é›†è½¬æ¢ä¸ºä¸€ä¸ªæ˜“äºæ“ä½œçš„ Map-style æ•°æ®é›†ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•ç›´æ¥è®¿é—®æ•°æ®é›†ä¸­çš„ç‰¹å®šæ ·æœ¬ï¼Œä»è€Œç®€åŒ–äº†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•è¿‡ç¨‹ä¸­çš„æ•°æ®å¤„ç†ã€‚

### 2. ä½¿ç”¨æµ‹è¯•æ•°æ®é›†è¯„ä¼°æ¨¡å‹

```
print('Checking the results of test dataset.')
test_acc,test_loss=evaluate(test_dataloader)
print('test accuracy {:8.3f}'.format(test_acc))
```

è¿è¡Œç»“æœï¼š

```
Checking the results of test dataset.
test accuracy    0.910
```

## å››ã€å¿ƒå¾—ä½“ä¼š

é€šè¿‡ä¸Šè¿°å­¦ä¹ ï¼Œäº†è§£äº†æ–‡æœ¬åˆ†ç±»çš„åŸºæœ¬æµç¨‹ï¼Œå¯¹å…¶ä¸­å»ºæ¨¡è¿‡ç¨‹æœ‰äº†æ›´åŠ æ·±åˆ»çš„è®¤è¯†ã€‚