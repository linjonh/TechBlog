---
layout: post
title: "用于-RGB-D-显著目标检测的点感知交互和-CNN-诱导的细化网络"
date: 2025-03-13 14:50:05 +0800
description: "受人类视觉系统的启发，显著性目标检测（SOD）旨在定位给定场景中最具吸引力的目标或区域[3, 5, 6, 9-13, 27, 54, 59]，该技术已成功应用于众多任务中。此外，RGB-D显著性目标检测任务在显著性目标检测任务中额外引入了深度图，以便更好地模拟人类双目视觉系统的能力，并获得感知物体间距离关系的能力。"
keywords: "用于 RGB-D 显著目标检测的点感知交互和 CNN 诱导的细化网络"
categories: ['论文']
tags: ['计算机视觉', '网络', '神经网络', '目标检测', '深度学习', '人工智能', 'Cnn']
artid: "146214962"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146214962
    alt: "用于-RGB-D-显著目标检测的点感知交互和-CNN-诱导的细化网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146214962
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146214962
cover: https://bing.ee123.net/img/rand?artid=146214962
image: https://bing.ee123.net/img/rand?artid=146214962
img: https://bing.ee123.net/img/rand?artid=146214962
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     用于 RGB-D 显著目标检测的点感知交互和 CNN 诱导的细化网络
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     摘要
    </h2>
    <p>
     通过整合来自RGB图像和深度图的互补信息，能够提升在复杂且具有挑战性场景下的显著性目标检测（SOD）能力。近年来，卷积神经网络（CNNs）在特征提取和跨模态交互方面的重要作用已得到充分挖掘，但在对自模态和跨模态的全局长距离依赖关系进行建模方面仍显不足。为此，我们引入了卷积神经网络辅助的Transformer架构，并提出了一种新颖的具有点感知交互和卷积神经网络引导优化功能的RGBD显著性目标检测网络（PICR-Net）。一方面，考虑到RGB模态和深度模态之间的先验相关性，我们设计了一个注意力触发的跨模态点感知交互（CmPI）模块，用于探索在位置约束下不同模态的特征交互。另一方面，为了缓解Transformer天然带来的块效应和细节破坏问题，我们设计了一个卷积神经网络引导优化（CNNR）单元，用于内容的优化和补充。在五个RGB-D显著性目标检测数据集上进行的大量实验表明，所提出的网络在定量和定性比较中均取得了具有竞争力的结果。我们的代码已在以下网址公开：https://github.com/rmcong/PICR-Net_ACMMM23。
    </p>
    <p>
     <strong>
      CCS概念
     </strong>
    </p>
    <p>
     计算方法→兴趣点和显着区域检测；
    </p>
    <p>
     <strong>
      关键字
     </strong>
    </p>
    <p>
     <strong>
      显著对象检测，RGB-D图像，CNNS辅助变压器体系结构，点感知互动
     </strong>
    </p>
    <h2 style="background-color:transparent">
     一 简介
    </h2>
    <p>
     受人类视觉系统的启发，显著性目标检测（SOD）旨在定位给定场景中最具吸引力的目标或区域[3, 5, 6, 9-13, 27, 54, 59]，该技术已成功应用于众多任务中。此外，RGB-D显著性目标检测任务在显著性目标检测任务中额外引入了深度图，以便更好地模拟人类双目视觉系统的能力，并获得感知物体间距离关系的能力。自从进入深度学习时代[20, 23, 36, 57]以来，基于卷积神经网络（CNNs）的RGB-D显著性目标检测框架得到了蓬勃发展[2, 8, 15, 24, 29, 30, 39, 47, 51, 52, 58]，其性能远远超过了基于手工设计特征的方法。然而，研究表明，尽管从理论上讲，卷积神经网络可以通过网络的深度化获得更大的感受野，但在实际应用中，卷积操作的感受野仍然局限于局部范围。遗憾的是，显著性目标的判定需要全局对比感知，因此对全局关系进行建模的能力在显著性目标检测中起着至关重要的作用。因此，一些研究工作利用Transformer的全局建模能力来实现RGB-D显著性目标检测[33, 35]。
    </p>
    <p>
     从模型架构的角度来看，现有的RGB-D显著性目标检测（SOD）方法可分为三类：纯卷积神经网络（CNNs）模型、纯Transformer模型以及Transformer辅助的CNNs模型。对于纯CNNs架构而言，由于卷积操作具有出色的局部感知能力，其显著性检测结果在描述某些局部细节（如边界）方面表现更佳，但检测结果可能不完整，例如图1中第一张图片里多视角显著性网络（MVSalNet）[58]的检测结果。对于纯Transformer结构，由于Transformer能够捕捉长距离依赖关系，在一定程度上提高了检测结果的完整性，然而分块操作可能会破坏细节质量，引发块效应，甚至引入额外的误检情况，如图1中视觉Transformer（VST）[33]的检测结果。Transformer辅助的CNNs结构引入Transformer来辅助CNNs进行全局上下文建模，通过将两者结合，可以缓解上述单一方案的缺点。然而，在逐层解码的过程中，卷积操作会逐渐淡化Transformer获取的全局信息，所以这种方案仍然会导致目标的漏检或误检，如图1中TriTransNet[35]的检测结果。因此，在本文中，我们重新思考了Transformer和CNNs之间的关系，并提出了一种CNNs辅助的Transformer网络架构。具体来说，我们利用Transformer完成大部分的编码和解码过程，并设计了一个可插拔的卷积神经网络引导优化（CNNR）单元，以便在网络末端实现内容优化。通过这种方式，可以充分利用Transformer和CNNs的优势，且二者互不干扰，从而获得全局和细节感知能力，并生成准确且高质量的显著图。
    </p>
    <p>
     <img alt="" height="504" src="https://i-blog.csdnimg.cn/direct/aeaa5e48a7d4421b9c5c60d0eab4134f.png" width="814"/>
    </p>
    <p>
     图1：具有不同体系结构的代表性网络的视觉比较，其中MVSALNET [58]，VST [33]和Tritransnet [35]分别是纯CNN，纯变压器和Transformer辅助CNNS架构。
    </p>
    <p>
     对于跨模态特征交互问题，传统的特征交互机制已经引起了计算机视觉和模式识别领域的高度关注，甚至在模态对应信息缺失的情况下也取得了成功的成果[25, 48]。在基于Transformer的模型背景下，交叉注意力机制[19, 50]是一种常用的方法。例如，视觉与语言任务中的跨模态交互通过交替使用来自视觉和语言模态的查询（queries）和键（keys）来计算不同模态之间的相似度。同样地，交叉注意力机制也可以直接应用于RGB-D显著性目标检测（SOD）任务，以对RGB和深度特征之间的关系进行建模，但存在两个主要挑战。首先，与图像和语言之间的关系不同，RGB图像和深度图仅在对应位置的特征上具有明确的相关性，所以上述交叉注意力方法在一定程度上是盲目且冗余的。其次，由于计算复杂度与特征图的大小呈二次方比例关系，这种不加区分的“一刀切”计算方式会带来不必要的计算负担。为了解决上述两个问题，我们提出了一种跨模态点感知交互（CmPI）模块，该模块通过对来自不同模态的对应点特征进行分组，简化了跨模态交互的建模过程。通过这种方式，RGB和深度特征的交互被限制在相同的位置，使其更具方向性，并将计算复杂度降低到线性水平。此外，我们还在CmPI模块中引入了全局显著性引导向量，以便在进行跨模态交互时强调全局约束，使交互更加全面。具体来说，通过使用带有精心设计的掩码约束的两步注意力操作来实现上述跨模态以及全局与局部关系的建模过程。
    </p>
    <p>
     总的来说，本文做出了以下三个主要贡献：
    </p>
    <p>
     （1) 为了充分发挥Transformer和卷积神经网络（CNNs）的优势，我们提出了一种新的由CNNs辅助的Transformer架构，以实现RGB-D显著性目标检测（SOD），该架构被称为PICR-Net。在五个广泛使用的数据集上，与16种最先进的方法相比，PICR-Net取得了具有竞争力的性能表现。
    </p>
    <p>
     (2) 考虑到RGB模态与深度模态之间的先验相关性，我们提出了一种跨模态点感知交互模块，该模块能够在全局引导和位置约束的条件下，动态地融合不同模态的特征表示。
    </p>
    <p>
     (3) 为了缓解由Transformer架构所导致的块效应和细节破坏问题，我们在网络末端设计了一个可插拔的卷积神经网络引导优化单元，以实现内容的优化和细节的补充。
    </p>
    <h2>
     二 有关工作
    </h2>
    <p>
     早期，传统的RGB-D显著性目标检测（SOD）方法[4, 7, 40]依赖于手工设计的特征，其性能非常有限。近年来，得益于深度学习强大的特征表示能力，大量基于学习的RGB-D显著性目标检测模型被提出。在2020年视觉Transformer（Vision Transformer）发布[14]之前，RGB-D显著性目标检测任务仍以卷积神经网络（CNNs）为主流架构，并且在跨模态交互、深度质量感知和轻量化设计等方面提出了各种模型[1, 2, 17, 18, 22, 51, 53, 56, 60]。例如，Zhang等人[51]设计了一种跨模态差异交互策略，以在RGB-D显著性目标检测任务中实现高效的融合。Cong等人[2]在RGB-D显著性目标检测任务中考虑了深度图的质量问题，并提出了一种深度潜力感知门控注意力网络，以解决低质量深度图带来的负面影响。Chen等人[1]堆叠了三维卷积层作为编码器来实现RGB-D显著性目标检测，这种方法无需专用或复杂的模块就能有效地融合跨模态特征。Huang等人[22]仅在特定的一个特征层级上而非所有层级上进行跨模态特征融合，从而形成了一个轻量级的模型。
    </p>
    <p>
     随着Transformer在计算机视觉领域崭露头角，一些纯Transformer模型或Transformer与卷积神经网络（CNNs）相结合的模型应运而生。Liu等人[33]从序列到序列建模的新视角，为RGB-D显著性目标检测（SOD）任务设计了一种纯Transformer架构，其中采用交叉注意力机制进行跨模态交互。Song等人[42]在RGB-D显著性目标检测任务中充分利用自注意力机制和交叉注意力机制，实现外观特征与几何特征之间的交互。Liu等人[35]将Transformer嵌入到CNNs之后，对卷积特征之间的长距离依赖关系进行建模，同时实现特征融合。
    </p>
    <p>
     然而，这些现有的纯卷积神经网络（CNNs）或纯Transformer解决方案也存在一些问题。例如，基于CNNs的方法在获取全局信息以准确定位显著目标的能力方面稍显不足，而基于Transformer的解决方案计算量较大，并且容易受到块效应的影响。尽管一些采用Transformer辅助CNNs架构的混合结构方法能够在一定程度上缓解上述问题，但在解码过程中的多层卷积操作会淡化Transformer获取的全局信息，进而影响预测性能。我们应该重新思考Transformer和CNNs在网络中的作用，充分利用它们各自的优势，并探索有效的跨模态交互方式。因此，我们尝试使用一种由CNNs辅助的Transformer架构来对全局上下文和局部细节进行建模，并提出一种在位置约束下的点感知交互机制，以使跨模态交互更加高效且具有针对性。
    </p>
    <h2>
     三 提出的方法
    </h2>
    <h3>
     3.1 网络概述
    </h3>
    <p>
     <img alt="" height="750" src="https://i-blog.csdnimg.cn/direct/1d1c4afab22749a3a94628ceaa27e7f4.png" width="1120"/>
    </p>
    <p>
     <strong>
      图2
     </strong>
     ：所提出的PICR-Net的整体框架。首先，将RGB图像和深度图像输入到双流编码器中，以提取相应的多级特征
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/60ae4fdb740b43d6b70b836793ee43d3.png" width="45">
      和
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/bd334a96c9284fc4a262b3e48e506245.png" width="54">
       。随后，同一层的特征通过跨模态点感知交互模块进行多维度交互，在这个过程中，之前输出的显著图
       <img alt="" height="29" src="https://i-blog.csdnimg.cn/direct/0a582addd2604f40a666e774ae6299c1.png" width="26">
        被用于提取全局引导信息。在网络的末端，卷积神经网络引导优化（CNNR）单元从预训练的VGG16模型中提供具有更高分辨率和更多细节的卷积特征，以优化并输出最终的高质量显著图
        <img alt="" height="25" src="https://i-blog.csdnimg.cn/direct/9a07f1f68ea14d7686032c2da3a8841f.png" width="39">
         。
        </img>
       </img>
      </img>
     </img>
    </p>
    <p>
     如图2所示，所提出的网络整体上遵循编码器-解码器结构。顶部和底部的分支分别是RGB图像和深度图的特征编码器，二者均采用共享权重的Swin-Transformer模型[34]，而中间分支是自下而上的解码过程。在每个解码阶段，首先通过跨模态点感知交互（CmPI）模块对不同模态相同位置处的交互关系进行建模，从而获得跨模态表示。此后，我们使用基于Swin-Transformer的解码模块，从全局角度在解码过程中对跨模态特征的长距离依赖关系进行建模。具体来说，由CmPI模块生成的跨模态特征
     <img alt="" height="31" src="https://i-blog.csdnimg.cn/direct/2f166dece06c4e8f8a3a218a49b4f4b1.png" width="27">
      以及前一解码阶段（如果有的话）经过上采样的输出特征
      <img alt="" height="36" src="https://i-blog.csdnimg.cn/direct/7c105dcafe6b477f8a05e8c10fd26e3b.png" width="53">
       被输入到两个级联的Swin-Transformer模块中，以对全局关系进行建模：
      </img>
     </img>
    </p>
    <p>
     <img alt="" height="170" src="https://i-blog.csdnimg.cn/direct/418d75bb98fd47f38988265dd6688364.png" width="691"/>
    </p>
    <p>
     其中，“cat”表示在特征维度上的拼接操作，“Linear”是线性层，“ST”代表两个Swin-Transformer模块，“Exp”是将特征转换回空间分辨率的操作。最后，在解码器的末端，我们提出了一个可插拔的卷积神经网络引导优化（CNNR）单元，以低成本解决Transformer架构下存在的块效应和细节破坏问题，并生成最终的显著图
     <img alt="" height="33" src="https://i-blog.csdnimg.cn/direct/c430a7b6134746e992a6d053b8badc56.png" width="38"/>
     。
    </p>
    <h3>
     3.2 跨模式点感知交互模块
    </h3>
    <p>
     在提取了RGB模态和深度模态的多级编码特征之后，如何实现全面的交互是编码阶段需要重点关注的一个重要问题。现有的基于Transformer架构的跨模态交互方案通常会对两种模态所有位置之间的关系进行建模。但众所周知，RGB图像和深度图本身之间存在对应关系，也就是说，这两种模态仅在对应位置上存在明确的关系。因此，如果对不同模态的所有像素之间的关系进行建模，就会存在计算冗余，而且由于这种强行的关联建模，还可能引入不必要的噪声。考虑到这些因素，从RGB-D显著性目标检测（SOD）任务中跨模态建模的实际情况出发，我们引入了位置约束因素，并提出了一种跨模态点感知交互方案，其核心是通过多头注意力机制来探索不同模态特征在相同位置处的交互关系。与直接组合特征向量相比，多头并行注意力机制允许跨模态特征在不同的嵌入空间中进行动态交互，从而能够在不同场景中自适应地调整两种模态特征的参与程度。此外，为了从全局角度引导这一交互过程，并感知当前位置在整体特征图中的作用，我们还在交互过程中添加了全局显著性引导向量。
    </p>
    <p>
     <img alt="" height="753" src="https://i-blog.csdnimg.cn/direct/93b8b9da045b4092b9128259d0666a37.png" width="1751"/>
     <strong>
      图3
     </strong>
     ：CMPI模块中的跨模式点感知的RM，其中RGB和深度在同一空间位置和两种模态的全球显着性引导量都得到充分有效相互作用。
    </p>
    <p>
     图 3 展示了跨模态点感知交互（CmPI）模块中最关键的跨模态点感知关系建模（RM）部分。设 RGB 模态和深度模态特征上任意位置(x,y)对应的点特征向量分别表示为
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/80479a4afcdc4a4d80dcb2960735d3d5.png" width="237"/>
     ，其中c为嵌入维度。首先，为了给每个位置的交互过程提供全局引导，利用从上一级解码得到的上采样侧边输出显著图
     <img alt="" height="37" src="https://i-blog.csdnimg.cn/direct/91f21f42fa424eeeac7d6f7c12ccb190.png" width="41"/>
     生成两种模态的显著性引导向量，在计算过程中，当前尺度下的所有位置共享该引导向量：
    </p>
    <p>
     <img alt="" height="76" src="https://i-blog.csdnimg.cn/direct/c99e3470337e4f5d8536c6c2a6dbd3ca.png" width="624"/>
    </p>
    <p>
     其中，“MAP”表示掩码平均池化操作[46]，并且
     <img alt="" height="39" src="https://i-blog.csdnimg.cn/direct/276fcfd08e754097bbd797fe3cab5681.png" width="41"/>
     被用作加权掩码。然后，位置(x, y)处的RGB/深度特征以及RGB/深度显著性引导向量共同构成了一个具有更全面表示的逐点特征组
     <img alt="" height="36" src="https://i-blog.csdnimg.cn/direct/51289c4258d240b3bd19f3d093e082a7.png" width="217"/>
    </p>
    <p>
     <img alt="" height="66" src="https://i-blog.csdnimg.cn/direct/3bc128c5b54f4d228fbb37977dcd5b2e.png" width="629"/>
    </p>
    <p>
     其中，“Stack”表示将特征在一个新的维度上拼接在一起。
    </p>
    <p>
     随后，通过关系建模操作来执行点特征组之间的交互：
    </p>
    <p>
     <img alt="" height="70" src="https://i-blog.csdnimg.cn/direct/7ef33b49242045c4a4257f2643509942.png" width="690"/>
    </p>
    <p>
     其中，
     <img alt="" height="34" src="https://i-blog.csdnimg.cn/direct/f68291f390914d238f008ab4ef58fe1c.png" width="89"/>
     是在位置(x, y)处RGB模态和深度模态之间的关系建模操作，其可以定义为：
    </p>
    <p>
     <img alt="" height="57" src="https://i-blog.csdnimg.cn/direct/75fb18278a54400b84ad2887cb2cf386.png" width="693"/>
    </p>
    <p>
     其中，
     <img alt="" height="33" src="https://i-blog.csdnimg.cn/direct/1320342c1a724b2ba0efecd8eb59ca0e.png" width="56"/>
     表示不同头（即不同特征空间）的注意力输出结果。这种关系建模操作与多头注意力机制[44]类似，但也存在明显差异：一方面，并非特征组内的所有特征都需要进行交互，例如引导向量与不同模态的特征之间
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/65602b1342a740b59f8f5368c7a7500e.png" width="256"/>
     。因为它们处于不同尺度且来自不同模态，强行进行交互反而可能产生负面影响。因此，我们在注意力操作中引入了精心设计的掩码来抑制这种负面交互。另一方面，在特征组内的注意力交互之后，全局向量会由其他跨模态全局向量以及自模态局部向量进行更新。为了更好地利用这些信息并强调全局 - 局部引导的作用，我们还使用新的掩码约束在自模态中进行了第二步的全局 - 局部交互。上述过程可以用以下公式表示：
    </p>
    <p>
     <img alt="" height="49" src="https://i-blog.csdnimg.cn/direct/9d7147593d464b54a2d84f2cf51a5723.png" width="697"/>
    </p>
    <p>
     在第一步注意力计算中，将
     <img alt="" height="24" src="https://i-blog.csdnimg.cn/direct/0e288bc79be54e2ea734bb28e7cb2157.png" width="25"/>
     设为一个值为 -100.0 的反对角矩阵。这能在交互过程中削弱深度引导向量对 RGB 特征、RGB 引导向量对深度特征的负面影响。随后进行第二步注意力操作，在这一步中，通过将
     <img alt="" height="29" src="https://i-blog.csdnimg.cn/direct/af57f8762da74a109bb40f4e03f6a74b.png" width="26"/>
     设为图 3 中的值来进行自模态内的全局 - 局部交互，从而增强全局向量对同一模态中局部表示的引导作用。具体而言，带有掩码的注意力操作如下进行：
    </p>
    <p>
     <img alt="" height="96" src="https://i-blog.csdnimg.cn/direct/9c9dd2158b6148b4a0a017d5165224ad.png" width="702"/>
    </p>
    <p>
     其中，
     <img alt="" height="38" src="https://i-blog.csdnimg.cn/direct/79930c60c2c942c8bed4d81dfab79afa.png" width="259"/>
     通过线性映射生成的，并且j是注意力头的索引。
    </p>
    <p>
     经过上述过程，两种模态的信息能够在显著性引导向量的引导下充分交互，最后，通过一个线性层将这两种特征组合起来，作为最终的跨模态特征：
    </p>
    <p>
     <img alt="" height="62" src="https://i-blog.csdnimg.cn/direct/b2b1ab1fa35c4602af1f89c6ffb817f6.png" width="710"/>
    </p>
    <p>
     其中，“MLP”指的是多层感知机。
    </p>
    <h3>
     3.3 CNN诱导的精炼单元
    </h3>
    <p>
     在Transformer解码器的输出端，显著目标的主体基本上已经确定，但由于Transformer结构中采用了分块操作，得到的显著图可能存在块效应和细节破坏的问题。为此，我们在解码器的末端提出了一个可插拔的卷积神经网络引导优化单元。这主要是受到了卷积神经网络（CNNs）在处理局部细节方面优势的启发。此外，在这个阶段特征分辨率较大，从参数数量和计算成本的角度来看，卷积操作更为合理。由于这一步骤的主要目的是优化细节内容，无需引入完整的CNNs编码器-解码器网络，仅使用VGG16[41]中前两层具有丰富纹理细节的浅层特征就足够了，分别记为
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/7db37c2ce8b3435dbaa381faba749b74.png" width="84"/>
     。首先，将来自最后一层Transformer的解码器特征
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/789a8c42dde04439b1feb40e4c0c3abe.png" width="55"/>
     转换到像素级别，并上采样到与
     <img alt="" height="22" src="https://i-blog.csdnimg.cn/direct/6cf4ef120c5d410d8ee967be55453bca.png" width="33"/>
     相同的分辨率，为后续的优化做准备：
    </p>
    <p>
     <img alt="" height="61" src="https://i-blog.csdnimg.cn/direct/ee048144226d42e69b5aa9eaf6ad39d4.png" width="695"/>
    </p>
    <p>
     其中，“Baseconv”由一个3×3的卷积层以及随后的一个ReLU激活函数组成，“up”表示上采样操作。此后，
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/01f175859f1e457f9d4d57dc19fc7c48.png" width="85"/>
     被用于进一步恢复分辨率。考虑到简单地使用拼接操作来融合特征并不能有效地捕捉嵌入在某些通道中的细节信息，我们使用通道注意力机制[21]来发现那些包含细节信息的重要通道，同时保留显著目标的主体部分，以实现自适应融合。逐步优化的过程可以表示如下：
    </p>
    <p>
     <img alt="" height="95" src="https://i-blog.csdnimg.cn/direct/bfdc292fa900487281f25399a0d3a427.png" width="696"/>
    </p>
    <p>
     其中，“CA”表示带有残差连接的通道注意力操作，
     <img alt="" height="23" src="https://i-blog.csdnimg.cn/direct/e3b8ccadf95c4de49c6555c9e81cb1de.png" width="36"/>
     是最终的显著图。通过这种方式，来自卷积的细粒度信息能够得到补充，从而生成更精确的显著图。
    </p>
    <h3>
     3.4 损失功能
    </h3>
    <p>
     为了获得具有清晰边界的高质量显著图，所提出的整个网络由多种损失函数进行监督，其中包括常用的二元交叉熵损失、用于衡量结构相似性的结构相似性指数（SSIM）损失以及交并比（IoU）损失，这些损失的组合记为
     <img alt="" height="27" src="https://i-blog.csdnimg.cn/direct/1b004ad4fc0d45ae94b19de8791746fb.png" width="30"/>
     。该网络的总损失定义为：
    </p>
    <p>
     <img alt="" height="187" src="https://i-blog.csdnimg.cn/direct/f40579b9cb5248659d7215b374c5a107.png" width="708"/>
    </p>
    <p>
     其中G表示相应的真实标签（ground truth），
     <img alt="" height="34" src="https://i-blog.csdnimg.cn/direct/a1558e16e1cc4e47baab9d81a62df695.png" width="30"/>
     是侧边输出监督，它是通过将G下采样到合适的尺寸得到的。需要注意的是，侧边输出的损失函数设置了较小的权重，以此来引导训练过程。
    </p>
    <h2>
     四 实验
    </h2>
    <h3>
     4.1 数据集和评估指标
    </h3>
    <p>
     采用了五个广泛使用的RGB - D显著性目标检测（SOD）基准数据集来评估我们的PCIR - Net的性能。NLPR数据集[38]是通过Kinect相机获取的，它包含来自室内和室外场景的1000对RGB图像和深度图。按照文献[30, 39]的做法，我们采用2985对图像作为训练数据，其中包括来自NJU2K数据集的1485个样本、来自NLPR数据集的700个样本以及来自DUT数据集的800个样本。这些训练数据集中剩余的所有图像，以及LFSD[32]和STERE1000[37]数据集都用于测试。
    </p>
    <p>
     我们采用显著性目标检测（SOD）任务中常用的三个指标来对性能进行定量评估。F值度量[37]通过将二值化的显著图与真实标签（ground truth）进行比较，来表示精确率（precision）和召回率（recall）的加权调和平均数。平均绝对误差（MAE）得分[6]是逐像素计算差异。S值度量[16]用于评估预测的显著图与真实标签之间的目标感知
     <img alt="" height="32" src="https://i-blog.csdnimg.cn/direct/7da37fb52fc04f96881920ebbcf86076.png" width="40"/>
     和区域感知结构
     <img alt="" height="35" src="https://i-blog.csdnimg.cn/direct/f9e240abe15a4fe19be5265bbe1b9771.png" width="44"/>
     相似性。
    </p>
    <h3>
     4.2 实施详细信息
    </h3>
    <p>
     所提出的网络借助PyTorch和MindSpore Lite工具1实现，并使用单块NVIDIA GeForce RTX 3090 GPU进行加速。所有训练和测试样本都被调整为Swin - Transformer指定的224×224大小。此外，所有深度图都经过归一化处理，并复制为三个通道以适配输入尺寸。同时还采用随机翻转和旋转操作进行数据增强。在训练过程中，编码器使用在ImageNet上预训练的参数进行初始化。采用Adam算法对所提出的网络进行优化，批量大小设为32。初始学习率设定为
     <img alt="" height="25" src="https://i-blog.csdnimg.cn/direct/dcf138493fad4e8aa9a786b5cd66e8b1.png" width="47"/>
     ，并采用逐步衰减策略，每40个epoch衰减为前一阶段的五分之一。整个训练过程包含90个epoch。
    </p>
    <h3>
     4.3 与最先进的比较
    </h3>
    <p>
     为了证明我们所提出的PICR - Net的有效性，我们将其与16种最先进的模型进行了比较，这些模型包括DSA2F [43]、DCF [24]、DFM - Net [55]、TriTransNet [35]、BTS - Net [56]、VST [33]、SPNet [60]、CDNet [26]、HAINet [31]、CCAFNet [61]、RD3D [1]、JLDCF [18]、SPSN [28]、MVSalNet [58]、CIRNet [8]和DCMF [45]。其中，VST [33]是纯Transformer架构，TriTransNet [35]是Transformer辅助的卷积神经网络（CNNs）架构，其余的则是基于纯CNNs的架构。为了进行公平比较，我们使用了各模型作者提供的显著图，或者通过官方测试代码得到的显著图来进行评估。
    </p>
    <h4>
     4.3.1 定量评估
    </h4>
    <p>
     <img alt="" height="541" src="https://i-blog.csdnimg.cn/direct/fc6b3b51e06545769fb6ff2aafc27f54.png" width="1180"/>
    </p>
    <p>
     表1：在五个基准数据集上，依据S值度量
     <img alt="" height="31" src="https://i-blog.csdnimg.cn/direct/e9bfc0a301e3497bae6453cf685cd17e.png" width="47"/>
     、最大F值度量
     <img alt="" height="36" src="https://i-blog.csdnimg.cn/direct/b3b6db057a2749de862920a4e315697b.png" width="46"/>
     以及平均绝对误差（MAE）得分的定量比较结果。“↑”和“↓”分别表示数值越高越好和越低越好。每行中的粗体数字代表最佳性能。
    </p>
    <p>
     <img alt="" height="262" src="https://i-blog.csdnimg.cn/direct/6a6567a228b047c4b30b56ff90c890a0.png" width="811"/>
    </p>
    <p>
     表2：我们的PICR-NET的推理速度和一些典型的SOTA方法。黑色粗字体表示最佳性能
    </p>
    <p>
     表1直观地展示了所提出的PICR - Net在五个广泛使用的数据集上的定量结果，其中最佳性能以粗体标记。除了在LFSD数据集上的S值度量外，我们提出的方法在这五个数据集上均优于所有比较方法。例如，与第二优的方法相比，在DUT - test、LFSD、NLPR - test和STERE1000数据集上，平均绝对误差（MAE）得分的提升百分比分别达到了16.7%、1.9%、9.5%和6.1%。在其他度量指标上也能观察到类似的提升。推理速度一直是限制深度学习模型发展和应用的关键因素[49]。因此，我们还评估了PICRNet以及其他典型的最先进（SOTA）模型的推理速度，这些模型包括基于Transformer的模型VST [33]、TriTransNet [35]以及先进的基于卷积神经网络（CNNs）的模型SP - Net [60]。如表2所示，我们的模型在取得更好性能的同时，在推理速度方面也具有优势。然而，我们的模型尚未实现实时高效性，这也是未来进一步提高基于Transformer的模型推理速度的一个研究方向。
    </p>
    <h4>
     4.3.2 定性比较
    </h4>
    <p>
     <img alt="" height="651" src="https://i-blog.csdnimg.cn/direct/377236eed8484cb49ae5877288ddb2d2.png" width="1192"/>
    </p>
    <p>
     图4：我们的PICR-Net与最先进（SOTA）方法在不同具有挑战性的场景下的可视化对比，这些场景包括小目标（即图a、c和d）、多个目标（即图c）、低对比度（即图d和f）、低质量深度图（即图b和e）以及光照不均匀（即图g）。
    </p>
    <p>
     图4展示了不同方法的一些可视化结果，涵盖了具有挑战性的场景，如小目标场景（即图a、c和d）、多目标场景（即图c）、低对比度场景（即图d和f）、低质量深度图场景（即图b和e）以及光照不均匀场景（即图g）。 由此可见，我们的方法不仅能在这些具有挑战性的场景中准确检测出显著目标，还能获得更好的完整性和局部细节。值得注意的是，基于Transformer的模型（即VST、TriTransNet以及我们的PICR-Net）能够对全局依赖关系进行建模，因此在显著目标定位方面往往优于其余基于卷积神经网络（CNNs）的网络。此外，由于精心设计的跨模态交互，当深度图质量相对较差时（例如图4(a)和(e)），或者在RGB图像中存在光影干扰时（例如图4(g)），我们的网络能够充分提取另一个模态的信息，以实现准确且完整的预测。同时，由于CNNR单元提供了更细粒度的细节信息，与其他方法相比，我们的方法在边界精度和细节描述方面具有更大优势（例如图4(c)、(d)和(g)）。上述的定量和定性实验都证明了我们所提出方法的有效性。
    </p>
    <h3>
     4.4 消融研究
    </h3>
    <p>
     我们在NJU2K测试集和NLPR测试集上进行了消融实验，以验证所提出的PICR-Net中每个模块所起的作用。
    </p>
    <h4>
     4.4.1 一般结构的有效性
    </h4>
    <p>
     首先，为了验证CmPI模块的作用，我们设计了如下的替换实验：
    </p>
    <p>
     （1）全模型（编号为0）表示我们所提出的完整模型PICR-Net。
    </p>
    <p>
     （2）带加法（编号1）、带乘法（编号2）以及带拼接（编号3）分别表示将CmPI模块替换为逐元素加法、乘法以及拼接操作，以此来实现RGB特征与深度特征之间的交互。
    </p>
    <p>
     （3）带交叉注意力（编号4）意味着使用传统的交叉注意力[33]操作来替换CmPI模块中的RM（可能是特定的子模块或操作，需根据上下文确定具体含义）。
    </p>
    <p>
     <img alt="" height="428" src="https://i-blog.csdnimg.cn/direct/58ae01c92e17414781c277ddf5697824.png" width="825"/>
    </p>
    <p>
     <strong>
      表3
     </strong>
     ：一般结构的定量消融评估。黑色粗字体表示最佳性能。
    </p>
    <p>
     如表3所示，我们设计的CmPI模块比其他简单的交互策略取得了更好的性能。此外，对比编号0和编号4可以发现，我们带有CmPI模块的完整模型表现更优，并且也优于会带来更多计算负担的交叉注意力机制。图5展示了不同消融研究的一些可视化结果。从第二张图像可以看出，低质量的深度图会对使用交叉注意力的交互产生负面影响，导致目标被遗漏。相比之下，我们的方法仍然能够准确且完整地检测出显著目标。
    </p>
    <p>
     此外，为了验证带有CNNR单元的基于Transformer的解码器的有效性，我们设计了如下的剥离实验：
    </p>
    <p>
     （1）无TD（编号5）表示用数量相等的卷积层来替代基于Transformer的解码器，以进行显著性解码。
    </p>
    <p>
     （2）无CNNR（编号6）指的是将解码器末端的CNNR单元移除。
    </p>
    <p>
     如表3所示，在替换掉基于Transformer的解码器后，在两个数据集上的F值度量得分分别下降了1.1%和1.3%，这表明使用卷积神经网络（CNNs）来完成解码会削弱Transformer提取的全局信息，进而降低性能。此外，从图5中可以发现，CNNR单元有助于提升显著图的边界质量和清晰度，这一点也得到了定量结果的支持。
    </p>
    <p>
     <img alt="" height="417" src="https://i-blog.csdnimg.cn/direct/03b68b2d0cb447daa24f10fae9e94235.png" width="729"/>
    </p>
    <p>
     <strong>
      图5
     </strong>
     ：不同消融研究的视觉比较。
    </p>
    <h4>
     4.4.2 设计细节的有效性
    </h4>
    <p>
     此外，为了验证CmPI模块详细设计的有效性，我们设计了以下实验：
    </p>
    <p>
     （1）无RM（编号7）表示移除CmPI模块中的关键组件RM。
    </p>
    <p>
     （2）采用单步操作（编号8）意味着仅保留RM中的第一步注意力操作，也就是说，去除了第二步的注意力计算。
    </p>
    <p>
     （3）无
     <img alt="" height="17" src="https://i-blog.csdnimg.cn/direct/f5a389282e0d44dc829c03e7b90addf4.png" width="56"/>
     （编号9）表示在RM的计算公式（6）中去除掩码约束，也就是去掉M1和M2。此外，无M1（编号10）和无M2（编号11）分别表示仅去除M1和仅去除M2。
    </p>
    <p>
     （4）无
     <img alt="" height="25" src="https://i-blog.csdnimg.cn/direct/eb681c4f4e724ff19e2b3ada3c9ead14.png" width="47"/>
     （编号12）表示移除RM中的全局引导向量
     <img alt="" height="34" src="https://i-blog.csdnimg.cn/direct/242e9b9cd2d947b4a2a1d758d78ad072.png" width="58"/>
     。
    </p>
    <p>
     （5）窗口大小为3（编号13）和窗口大小为5（编号14）表示将RM中用于注意力交互的窗口大小从1（点感知）分别调整为3和5。
    </p>
    <p>
     （6) 1×1卷积（编号15）用同样属于点感知操作的1×1卷积替换了RM。
    </p>
    <p>
     <img alt="" height="353" src="https://i-blog.csdnimg.cn/direct/5df40e42922b4cb98e65958cb1b218cd.png" width="558"/>
    </p>
    <p>
     <strong>
      表4
     </strong>
     ：CmPI模块详细设计的定量消融评估。黑色加粗字体表示最佳性能。
    </p>
    <p>
     <img alt="" height="296" src="https://i-blog.csdnimg.cn/direct/48e8374ffcc348b996f1411c483f4f5f.png" width="597"/>
    </p>
    <p>
     <strong>
      图6
     </strong>
     ：关于CMPI设计细节的消融研究的定性比较。
    </p>
    <p>
     相关结果见表4。总体而言，所有的消融验证结果都不如我们的全模型设计。具体来说，如果移除整个RM模块，性能损失非常明显，如编号7的结果所示。此外，第二步注意力中的自模态全局-局部引导（如编号8的结果所示）以及RM中对负向交互的抑制（如编号9、10和11的结果所示）都是非常必要且有效的。对于引导向量
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/e7e1000c69924de39b5a2f206327deb1.png" width="58"/>
     ，在移除它们之后，性能下降，这也导致了如图6所示的更差的目标完整性。对于注意力操作的交互范围，由于位置之间的强相关性，增大窗口大小（3或5）并不能明显提高性能，反而带来了指数级的计算成本。我们直接使用1×1卷积来替换CmPI模块（为了公平比较，也通过扩展和拼接引入了引导向量），如表4中编号15所示。在两个数据集上所有的评估指标都降低了，这表明CmPI相比1×1卷积能够实现更全面的交互。
    </p>
    <h2>
     五 结论
    </h2>
    <p>
     考虑到Transformer和卷积神经网络（CNNs）各自的特点与优势，我们提出了一个名为PICR-Net的网络，以实现基于RGB-D数据的显著性目标检测（SOD）。该网络整体上采用基于Transformer的编码器-解码器架构，并在末端添加了一个可插拔的CNNR单元，用于细节优化。此外，与传统的交叉注意力机制相比，我们提出的CmPI模块考虑了RGB和深度模态之间的先验相关性，通过引入空间约束和全局显著性引导，实现了更有效的跨模态交互。全面的实验表明，在五个基准数据集上，我们的网络与16种最先进的方法相比，取得了具有竞争力的性能。
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f35393839393330352f:61727469636c652f64657461696c732f313436323134393632" class_="artid" style="display:none">
 </p>
</div>


