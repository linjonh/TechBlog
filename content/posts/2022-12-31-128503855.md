---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34343934393034312f:61727469636c652f64657461696c732f313238353033383535"
layout: post
title: "XGBoost模型调参GridSearchCV方法网格搜索优化参数"
date: 2022-12-31 12:23:31 +08:00
description: "GridSearchCV是XGBoost模型最常用的调参方法。本文主要介绍了如何使用GridSear"
keywords: "xgboost网格搜索"
categories: ['机器学习']
tags: ['模型优化', '机器学习', '参数调优', 'Xgboost', 'Gridsearchcv']
artid: "128503855"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=128503855
    alt: "XGBoost模型调参GridSearchCV方法网格搜索优化参数"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=128503855
featuredImagePreview: https://bing.ee123.net/img/rand?artid=128503855
---

# XGBoost模型调参：GridSearchCV方法网格搜索优化参数

#### 文章目录

* [一、前言](#_1)
* [二、数据处理](#_3)
* [三、XGBoost参数调优](#XGBoost_27)
* + [3.1 常见可调参数](#31__28)
  + [3.2 GridSearchCV调参函数](#32_GridSearchCV_61)
  + [3.3 一般调参顺序](#33__73)
  + [3.4 调参结果可视化](#34__103)
* [四、总结](#_121)

## 一、前言

本篇文章是继上一篇文章：
[使用K-Fold训练和预测XGBoost模型的方法](https://blog.csdn.net/qq_44949041/article/details/128500239)
，探讨对XGBoost模型调优的方法，所使用的代码和数据文件均是基于上一篇文章的，需要的小伙伴可以跳转链接自行获取。

## 二、数据处理

程序和上篇文章中的完全一致，不再赘述。

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor

feature_file = pd.read_csv("./DataHousePricePrediction/train.csv")

x = []# 特征数据
y = []# 标签
for index in feature_file.index.values:
    #print('index', index)
    #print(feature_file.values[0])
    #print(feature_file.ix[index].values) 
    x.append(feature_file.values[index][2: -1]) # 从原文件中提取输入变量数据
    y.append(feature_file.values[index][1])     # 从原文件中提取输出变量标签
   
x, y = np.array(x), np.array(y)
# 划分训练集和验证集
X_train,X_valid,y_train,y_valid = train_test_split(x,y,test_size=0.2,random_state=12345)

```

## 三、XGBoost参数调优

### 3.1 常见可调参数

一般调参会考虑以下几个超参数(需要在模型中初始化)：

> • learning_rate
>   
> • n_estimators
>   
> • max_depth
>   
> • min_child_weight
>   
> • subsample
>   
> • colsample_bytree
>   
> • gamma
>   
> • reg_alpha
>   
> • reg_lambda

这些参数的具体含义可见：
[XGBoost常用参数](https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters)
  
定义模型：

```python
#定义xgboost模型
xgb = XGBRegressor(learning_rate =0.1,
                   n_estimators=150, 
                   max_depth=5,
                   min_child_weight=1,
                   gamma=0,
                   subsample=0.8,
                   colsample_bytree=0.8,
                   objective= 'reg:squarederror',
                   reg_alpha= 0,
                   reg_lambda= 1,
                   nthread=4,
                   scale_pos_weight=1,
                   seed=27)


```

### 3.2 GridSearchCV调参函数

不同于CV领域的神经网络，Scikit-learn为XGBoost模型提供了一个网格搜索最优化参数的方法：GridSearchCV(网格搜索交叉验证调参)。详细介绍见：
[sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

在本文中，主要使用到了GridSearchCV中的以下几个参数：

> * estimator：表示所要调优的模型。
> * param_grid：字典类型变量。主要存储的是要尝试的参数，每一个参数中要尝试的值组成一个列表，不同的参数列表构成一个字典。
> * n_jobs，int类型，表示要并行运行的作业数，-1表示使用所有的处理器。通过此参数可以认为控制使用CPU的核数。
> * cv，int类型，表示要交叉验证拆分的数量，也就是K-Fold的数量。

**GridSearchCV搜索原理**
：对param_grid中要尝试的变量进行排列组合，遍历每一种组合，通过交叉验证的方式返回所有参数组合下的评价指标得分，最后选择分数最高的组合对应的参数作为最优值。简单来说，
GridSearchCV的搜索原理就是枚举，暴力搜索
。

### 3.3 一般调参顺序

调参的要旨是：每次调一个或两个超参数，然后将找到的最优超参数代入到模型中继续调余下的参数
。
  
XGBoost一般的调参顺序和排列组合是：

> 1. 最佳迭代次数(树模型的个数)：n_estimators
> 2. min_child_weight以及max_depth
> 3. gamma
> 4. subsample以及colsample_bytree
> 5. reg_alpha以及reg_lambda
> 6. learning_rate

下面以min_child_weight以及max_depth两个参数为例展示对应的调参程序：

```python
from sklearn.model_selection import GridSearchCV
#Need to research
#research_one: n_epoch
#research_one: max_depth
param_test1 = {
    'min_child_weight': [1, 2, 3],
    'max_depth':[2, 3, 4, 5, 6, 7]
    }

xgb_res = GridSearchCV(estimator = xgb, 
                       param_grid = param_test1, 
                       n_jobs=4, 
                       cv=5)

xgb_res.fit(X_train, y_train)

```

### 3.4 调参结果可视化

在搜索完成后，本文使用了
`cv_results_、best_params_、best_score_`
作为搜索输出，这三个方法都是
`GridSearchCV`
方法的对象，含义是：

* `cv_results_`
  :输出cv（交叉验证）结果的，可以是字典形式也可以是numpy形式，还可以转换成DataFrame格式
* `best_params_`
  ：通过网格搜索得到的score最好对应的参数
* `best_score_`
  ：输出最好的成绩

```python
print('max_depth_min_child_weight')
print('gsearch1.grid_scores_', xgb_res.cv_results_)
print('gsearch1.best_params_', xgb_res.best_params_)
print('gsearch1.best_score_', xgb_res.best_score_)

```

程序的输出为：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/b1c1fb6c21e866166f2c6cd38cfa8d94.jpeg#pic_center)
  
从输出的结果看出，GridSearchCV搜索确定了最佳的max_depth为3，最佳的min_child_weight为3，综合两种参数下模型的最佳得分为：0.65，获得了我们要的结果。
  
注:这里没有展示
`xgb_res.cv_results_`
的输出结果（太长了），从上面的对
`param_test1`
的定义可知，此次搜索中min_child_weight有3中取值，max_depth有6种取值，进行排列组合后有18种可能。
`cv_results_`
展示的就是这18种情况对应的交叉验证值。

## 四、总结

`GridSearchCV`
是XGBoost模型最常用的调参方法，在调参时要注意调参顺序并且要有效设置参数的变化范围，提高效率。受限于暴力搜索的设计逻辑，
`GridSearchCV`
并不适用于数据量大和超参数数量多的场景。当数据量大时，可以考虑
坐标下降
方法；当所调超参数数量多时，可以考虑使用
随机搜索
`RandomizedSearchCV`
方法。
  
总的来说，
有效的数据清洗和挖掘、符合使用场景的模型、灵活的训练和调参技巧
是提高预测准确度的三大手段。