---
layout: post
title: "python爬虫Scrapy5之CrawlSpider"
date: 2025-03-15 10:43:30 +0800
description: "起始url：https://wz.sun0769.com/political/index/politicsNewest?crawlspider其实就是scrapy封装好的一个爬虫类，通过该类提供的相关的方法和属性就可以实现全新高效形式的全站数据爬取。8.需要向可以被共享的调度器的队列（redis_key的值）中放入一个起始的url。5.修改redis数据库的配置文件（redis.windows.conf）如何是的scrapy可以实现分布式呢？指定可以被共享的调度器。"
keywords: "python爬虫Scrapy(5)之CrawlSpider"
categories: ['Python']
tags: ['爬虫', 'Scrapy', 'Python']
artid: "146275674"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146275674
    alt: "python爬虫Scrapy5之CrawlSpider"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146275674
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146275674
cover: https://bing.ee123.net/img/rand?artid=146275674
image: https://bing.ee123.net/img/rand?artid=146275674
img: https://bing.ee123.net/img/rand?artid=146275674
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     python爬虫Scrapy(5)之CrawlSpider
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-tomorrow-night" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h4>
     <a id="CrawlSpider_0">
     </a>
     CrawlSpider
    </h4>
    <ul>
     <li>
      <p>
       实现网站的全站数据爬取
      </p>
      <ul>
       <li>
        就是将网站中所有页码对应的页面数据进行爬取。
       </li>
      </ul>
     </li>
     <li>
      <p>
       crawlspider其实就是scrapy封装好的一个爬虫类，通过该类提供的相关的方法和属性就可以实现全新高效形式的全站数据爬取。
      </p>
     </li>
     <li>
      <p>
       使用流程：
      </p>
      <ul>
       <li>
        <p>
         新建一个scrapy项目
        </p>
       </li>
       <li>
        <p>
         cd 项目
        </p>
       </li>
       <li>
        <p>
         创建爬虫文件（*）：
        </p>
        <ul>
         <li>
          <p>
           scrapy genspider-t crawl spiderName www.xxx.com
          </p>
         </li>
         <li>
          <p>
           爬虫文件中发生的变化有哪些？
          </p>
          <ul>
           <li>
            当前爬虫类的父类为CrawlSpider
           </li>
           <li>
            爬虫类中多了一个类变量叫做rules
            <ul>
             <li>
              LinkExtractor：链接提取器
              <ul>
               <li>
                可以根据allow参数表示的正则在当前页面中提取符合正则要求的链接
               </li>
              </ul>
             </li>
             <li>
              Rule：规则解析器
              <ul>
               <li>
                可以接收链接提取器提取到的链接，并且对每一个链接进行请求发送
               </li>
               <li>
                可以根据callback指定的回调函数对每一次请求到的数据进行数据解析
               </li>
              </ul>
             </li>
            </ul>
           </li>
           <li>
            思考:如何将一个网站中所有的链接都提取到呢？
            <ul>
             <li>
              只需要在链接提取器的allow后面赋值一个空正则表达式即可
             </li>
            </ul>
           </li>
           <li>
            目前在scrapy中有几种发送请求的方式？
            <ul>
             <li>
              start_urls列表可以发送请求
             </li>
             <li>
              scrapy.Request()
             </li>
             <li>
              scrapy.FormRequest()
             </li>
             <li>
              Rule规则解析器
             </li>
            </ul>
           </li>
          </ul>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      <p>
       注意：
      </p>
      <ul>
       <li>
        链接提取器和规则解析器是一一对应的（一对一的关系）
       </li>
       <li>
        建议在使用crawlSpider实现深度爬取的时候，需要配合手动请求发送的方式进行搭配！
       </li>
      </ul>
     </li>
     <li>
      <pre><code>USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'
</code></pre>
     </li>
    </ul>
    <h4>
     <a id="_45">
     </a>
     分布式
    </h4>
    <ul>
     <li>
      <p>
       分布式在日常开发中并不常用，只是一个噱头！
      </p>
     </li>
     <li>
      <p>
       概念：
      </p>
      <ul>
       <li>
        可以使用多台电脑搭建一个分布式机群，使得多台对电脑可以对同一个网站的数据进行联合且分布的数据爬取。
       </li>
      </ul>
     </li>
     <li>
      <p>
       声明：
      </p>
      <ul>
       <li>
        原生的scrapy框架并无法实现分布式操作！why？
        <ul>
         <li>
          多台电脑之间无法共享同一个调度器
         </li>
         <li>
          多台电脑之间无法共享同一个管道
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      <p>
       如何是的scrapy可以实现分布式呢？
      </p>
      <ul>
       <li>
        借助于一个组件：scrapy-redis
       </li>
       <li>
        scrapy-redis的作用是什么？
        <ul>
         <li>
          可以给原生的scrapy框架提供可被共享的调度器和管道！
         </li>
         <li>
          环境安装：pip install scrapy-redis
          <ul>
           <li>
            注意：scrapy-redis该组件只可以将爬取到的数据存储到redis数据库
           </li>
          </ul>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      <p>
       编码流程（重点）：
      </p>
      <ul>
       <li>
        <p>
         1.创建项目
        </p>
       </li>
       <li>
        <p>
         2.cd 项目
        </p>
       </li>
       <li>
        <p>
         3.创建基于crawlSpider的爬虫文件
        </p>
        <ul>
         <li>
          3.1 修改爬虫文件
          <ul>
           <li>
            导包：from scrapy_redis.spiders import RedisCrawlSpider
           </li>
           <li>
            修改当前爬虫类的父类为 RedisCrawlSpider
           </li>
           <li>
            将start_urls替换成redis_key的操作
            <ul>
             <li>
              redis_key变量的赋值为字符串，该字符串表示调度器队列的名称
             </li>
            </ul>
           </li>
           <li>
            进行常规的请求操作和数据解析
           </li>
          </ul>
         </li>
        </ul>
       </li>
       <li>
        <p>
         4.settings配置文件的修改
        </p>
        <ul>
         <li>
          <p>
           常规内容修改（robots和ua等），先不指定日志等级
          </p>
         </li>
         <li>
          <p>
           指定可以被共享的管道类
          </p>
          <ul>
           <li>
            <pre><code>ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 400
}
</code></pre>
           </li>
          </ul>
         </li>
         <li>
          <p>
           指定可以被共享的调度器
          </p>
          <ul>
           <li>
            <pre><code># 使用scrapy-redis组件的去重队列
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 使用scrapy-redis组件自己的调度器
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# 是否允许暂停
SCHEDULER_PERSIST = True
</code></pre>
           </li>
          </ul>
         </li>
         <li>
          <p>
           指定数据库
          </p>
          <ul>
           <li>
            <pre><code>REDIS_HOST = '127.0.0.1'
REDIS_PORT = 6379
</code></pre>
           </li>
          </ul>
         </li>
        </ul>
       </li>
       <li>
        <p>
         5.修改redis数据库的配置文件（redis.windows.conf）
        </p>
        <ul>
         <li>
          <p>
           在配置文件中改行代码是没有没注释的：
          </p>
          <ul>
           <li>
            <pre><code>bind 127.0.0.1
#将上述代码注释即可（解除本机绑定，实现外部设备访问本机数据库

如果配置文件中还存在：protected-mode = true，将true修改为false，
修改为false后表示redis数据库关闭了保护模式，表示其他设备可以远程访问且修改你数据库中的数据
</code></pre>
           </li>
          </ul>
         </li>
        </ul>
       </li>
       <li>
        <p>
         6.启动redis数据库的服务端和客户端
        </p>
       </li>
       <li>
        <p>
         7.运行项目,发现程序暂定一直在等待，等待爬取任务
        </p>
       </li>
       <li>
        <p>
         8.需要向可以被共享的调度器的队列（redis_key的值）中放入一个起始的url
        </p>
        <ul>
         <li>
          <p>
           在redis数据库的客户端执行如下操作：
          </p>
          <ul>
           <li>
            <p>
             lpush 队列名称 起始的url
            </p>
           </li>
           <li>
            <p>
             起始url：https://wz.sun0769.com/political/index/politicsNewest?id=1&amp;page=1
            </p>
           </li>
          </ul>
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37303136313135382f:61727469636c652f64657461696c732f313436323735363734" class_="artid" style="display:none">
 </p>
</div>


