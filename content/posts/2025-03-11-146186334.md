---
layout: post
title: "langchain入门使用langchain调用本地部署的大模型以llama.cpp以及ollama为例"
date: 2025-03-11 19:58:42 +0800
description: "langchain入门教程第二篇，使用本地部署的大模型（包含ollama以及llama.cpp）"
keywords: "【langchain/入门】使用langchain调用本地部署的大模型(以llama.cpp以及ollama为例)"
categories: ['Python', 'Llm']
tags: ['Ollama', 'Llama', 'Langchain', 'Deepseek']
artid: "146186334"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146186334
    alt: "langchain入门使用langchain调用本地部署的大模型以llama.cpp以及ollama为例"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146186334
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146186334
cover: https://bing.ee123.net/img/rand?artid=146186334
image: https://bing.ee123.net/img/rand?artid=146186334
img: https://bing.ee123.net/img/rand?artid=146186334
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【langchain/入门】使用langchain调用本地部署的大模型(以llama.cpp以及ollama为例)
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-kimbie-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <p>
    </p>
    <h2>
     <a id="_1">
     </a>
     说在前面
    </h2>
    <blockquote>
     <ul>
      <li>
       操作系统：windows
      </li>
      <li>
       python版本：3.9
      </li>
      <li>
       langchain版本：0.3.20
      </li>
      <li>
       pycharm版本：2023.1.2 (Community Edition)
      </li>
      <li>
       ollama版本：0.5.4
      </li>
      <li>
       llama.cpp版本：b4870
      </li>
     </ul>
    </blockquote>
    <h2>
     <a id="ollamaqwen25coder7b_9">
     </a>
     ollama(qwen2.5-coder:7b)
    </h2>
    <h3>
     <a id="_10">
     </a>
     部署模型
    </h3>
    <ul>
     <li>
      <code>
       ollama
      </code>
      部署大模型比较简单，到
      <a href="https://ollama.com/" rel="nofollow">
       官网
      </a>
      下载安装包后安装
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/cfcec07bf4954ef89d6e1ec8fb1b1f1f.png#pic_center"/>
     </li>
     <li>
      根据自己电脑的条件选择合适的模型，比如
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/2f9b46a42d304184b2e742e3e96af67f.png#pic_center"/>
     </li>
     <li>
      然后打开命令行，执行
      <pre><code class="prism language-shell">ollama run qwen2.5-coder
</code></pre>
     </li>
     <li>
      然后就可以直接在命令行对话了
      <pre><code class="prism language-shell">$ ollama run qwen2.5-coder:latest
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> 你好
你好！有什么我可以帮忙的吗？

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> Send a message <span class="token punctuation">(</span>/? <span class="token keyword">for</span> <span class="token builtin class-name">help</span><span class="token punctuation">)</span>
</code></pre>
     </li>
    </ul>
    <h3>
     <a id="langchain_27">
     </a>
     使用langchain
    </h3>
    <ul>
     <li>
      langchain提供了直接调用ollama api的package，安装后直接使用即可
      <pre><code class="prism language-shell">pip <span class="token function">install</span> langchain-ollama
</code></pre>
     </li>
     <li>
      代码环节
      <pre><code class="prism language-shell">from langchain_ollama <span class="token function">import</span> OllamaLLM

ollm <span class="token operator">=</span> OllamaLLM<span class="token punctuation">(</span>model<span class="token operator">=</span><span class="token string">"qwen2.5-coder:latest"</span><span class="token punctuation">)</span>
print<span class="token punctuation">(</span>ollm.invoke<span class="token punctuation">(</span><span class="token string">"你好"</span><span class="token punctuation">))</span>
</code></pre>
      运行
      <pre><code class="prism language-shell"><span class="token punctuation">(</span>venv<span class="token punctuation">)</span> PS D:<span class="token punctuation">\</span>Code<span class="token punctuation">\</span>langchain<span class="token operator">&gt;</span> python .<span class="token punctuation">\</span>main.py
你好！有什么我可以帮忙的吗？
</code></pre>
     </li>
    </ul>
    <h2>
     <a id="llamacppdeepseekr115b_45">
     </a>
     llama.cpp(deepseek-r1:1.5b)
    </h2>
    <h3>
     <a id="_46">
     </a>
     模型部署
    </h3>
    <ul>
     <li>
      算力不足，搞个1.5b测试吧
     </li>
     <li>
      llama.cpp部署也挺简单的，到
      <a href="https://github.com/ggml-org/llama.cpp/releases">
       github
      </a>
      选择合适的版本
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fa6ebd41bc854ba7b288ff7546f4925c.png"/>
     </li>
     <li>
      x64-windows-nvdia gpu
      <br/>
      下载
      <code>
       cudart-llama-bin-win-cuxx.x-x64.zip
      </code>
      以及
      <code>
       llama-b4870-bin-win-cuda-cuxx.x-x64.zip
      </code>
      ，其中
      <code>
       cudart
      </code>
      是cuda相关的依赖，解压后将里面的文件放到
      <code>
       llama...zip
      </code>
      解压后的同级目录即可
      <br/>
      例如
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/eee2c1fbf0594fc790dda981fac81c3d.png"/>
     </li>
     <li>
      mac-m4
      <br/>
      下载
      <code>
       llama-b4870-bin-macos-arm64.zip
      </code>
      解压即可
     </li>
     <li>
      使用llama-client即可在命令行下进行交互，例如
      <pre><code class="prism language-shell">./llama-cli <span class="token parameter variable">-m</span> DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf
<span class="token operator">&gt;</span> 你好
<span class="token operator">&lt;</span>think<span class="token operator">&gt;</span>

<span class="token operator">&lt;</span>/think<span class="token operator">&gt;</span>
你好！很高兴见到你，有什么我可以帮忙的吗？无论是聊天、解答问题还是提供建议，我都在这里为你服务。😊
</code></pre>
     </li>
     <li>
      如果需要让langchain能够使用，需要部署服务，即使用llama-server
      <pre><code class="prism language-shell">./llama-sever <span class="token parameter variable">-m</span> DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf <span class="token parameter variable">--port</span> <span class="token number">50052</span> <span class="token parameter variable">--host</span> <span class="token number">0.0</span>.0.0 <span class="token parameter variable">-c</span> <span class="token number">2048</span>
</code></pre>
     </li>
    </ul>
    <h3>
     <a id="langchain_69">
     </a>
     使用langchain
    </h3>
    <ul>
     <li>
      <code>
       llama.cpp
      </code>
      部署的服务使用的API格式是与
      <code>
       openai
      </code>
      兼容的，所以在
      <code>
       langchain
      </code>
      中，我们可以使用openai对应的package
      <pre><code class="prism language-shell">pip <span class="token function">install</span> langchain-openai
</code></pre>
     </li>
     <li>
      代码环节
      <pre><code class="prism language-python"><span class="token keyword">from</span> langchain_openai <span class="token keyword">import</span> ChatOpenAI

llm <span class="token operator">=</span> ChatOpenAI<span class="token punctuation">(</span>max_tokens<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 timeout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 openai_api_base<span class="token operator">=</span><span class="token string">"http://127.0.0.1:50052"</span><span class="token punctuation">,</span>
                 openai_api_key<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">)</span>
<span class="token comment"># openai_api_base 就是llama-server 部署时监听的地址</span>
<span class="token comment"># openai_api_key 必须要填 随便填就行 不能为 ""</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>llm<span class="token punctuation">.</span>invoke<span class="token punctuation">(</span><span class="token string">"你好"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>content<span class="token punctuation">)</span>
</code></pre>
      运行
      <pre><code class="prism language-shell"><span class="token punctuation">(</span>venv<span class="token punctuation">)</span> PS D:<span class="token punctuation">\</span>Code<span class="token punctuation">\</span>langchain<span class="token operator">&gt;</span> python .<span class="token punctuation">\</span>main.py
<span class="token operator">&lt;</span>think<span class="token operator">&gt;</span>

<span class="token operator">&lt;</span>/think<span class="token operator">&gt;</span>

你好！很高兴见到你，有什么我可以帮忙的吗？无论是聊天、解答问题还是提供建议，我都在这里为你服务。😊
</code></pre>
     </li>
    </ul>
   </div>
   <link href="./../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="./../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33333434363130302f:61727469636c652f64657461696c732f313436313836333334" class_="artid" style="display:none">
 </p>
</div>


