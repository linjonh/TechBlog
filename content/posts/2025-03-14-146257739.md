---
layout: post
title: "Language-Models-are-Few-Shot-Learners,GPT-3详细讲解"
date: 2025-03-14 15:14:07 +0800
description: "Language Models are Few-Shot Learners，GPT-3详细讲解"
keywords: "Language Models are Few-Shot Learners，GPT-3详细讲解"
categories: ['Transformer', 'Llm']
tags: ['语言模型', '人工智能', '3']
artid: "146257739"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146257739
    alt: "Language-Models-are-Few-Shot-Learners,GPT-3详细讲解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146257739
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146257739
cover: https://bing.ee123.net/img/rand?artid=146257739
image: https://bing.ee123.net/img/rand?artid=146257739
img: https://bing.ee123.net/img/rand?artid=146257739
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Language Models are Few-Shot Learners，GPT-3详细讲解
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/78d09a21ee9441d49da86f160e392385.png">
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/dfe7c4198efd46f1bef6b062e34587f6.png"/>
     </img>
    </p>
    <p>
     GPT的训练范式：预训练+Fine-Tuning
     <br/>
     GPT2的训练范式：预训练+Prompt predict （zero-shot learning）
     <br/>
     GPT3的训练范式：预训练+Prompt predict （few-shot learning）
    </p>
    <p>
     GPT2的性能太差，新意高，但有效性低 -&gt; GPT3
     <br/>
     GPT-3的本质还是通过海量的参数学习海量的数据，然后依赖transformer强大的拟合能力使得模型能够收敛。
    </p>
    <p>
     <strong>
      1.GPT-3结构
     </strong>
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4ff35aa766934a38b0dc0c69504bff5b.png">
      <br/>
      在Transformer的各层中交替使用密集注意力（dense attention）与局部带状稀疏注意力模式（locally banded sparse attention patterns），类似于稀疏Transformer（Sparse Transformer）。
     </img>
    </p>
    <p>
     Sparse Transforme允许我们处理更长更稀疏的输入序列。Sparse Transformer是一种旨在处理高维、稀疏和长序列数据的Transformer拓展版，由OpenAI团队在2019年提出。相比于传统的Transformer架构，Sparse Transformer通过在自注意力机制中引入稀疏性，减少了网络中计算的数量，从而可以处理更长的序列数据。在处理高维、稀疏数据时，Sparse Transformer可以避免对所有输入的位置进行计算，只计算与当前位置相关的位置，从而提高了计算效率。
    </p>
    <p>
     <strong>
      2.评估方法
     </strong>
     <br/>
     人看两三个例子就可以学会一件事了，给预训练好的语言模型一点样本，不finetune，语言模型就可以迅速学会下游的任务，相当于举几个例子告诉模型。
    </p>
    <p>
     In-context learning (上下文学习) 一种新的范式，在不进行参数更新的情况下，只做预测，只在输入中加入示例就能让模型进行学习。给定几个任务示例和一个任务说明，模型通过理解任务本身对任务中的实例进行补全。
     <br/>
     1、Few Shot Learning（FS）：用自然语言告诉模型任务；对每个子任务，提供10～100个训练样本。
     <br/>
     2、One-shot Learning（1S）：用自然语言告诉模型任务，而后只给该任务提供1个样本。
     <br/>
     3、Zero-shot learning（0S）：用自然语言告诉模型任务，但一个样本都不给。
    </p>
    <p>
     GPT3中的few-shot learning，只是在预测时候给几个例子，并不微调网络。
     <br/>
     GPT-2用zero-shot去讲了multi-task Learning的故事
     <br/>
     GPT-3使用meta-learning和in-context learning去讲故事
     <br/>
     meta-learning元学习的核心思想在于通过少量的数据寻找一个合适的初始化范围，使得模型能够在有限的数据集上快速拟合，并获得不错的效果。
    </p>
    <p>
     <strong>
      评估设置：
     </strong>
     <br/>
     （1）随着few shot输入的上下文例子数量的增加，不同模型参数下的acc变化
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a3745e186dde44c588fecad30e68ee57.png">
      <br/>
      （2）随着模型参数的增加，zero shot、one shot、few shot下的acc变化
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c62f699a5cdb446e9f1394ebcc08ad99.png">
       <strong>
        3.训练数据
       </strong>
       <br/>
       <strong>
        3.训练数据
       </strong>
       <br/>
       训练中从给定数据集中提取的样例的比例。在训练过程中，采样的频次和数据集大小无关，而和数据集的重要程度相关。
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c055161cf5984a9dab5608ca47d324c5.png">
        <br/>
        数据处理方式
        <br/>
        <strong>
         A
        </strong>
        .高质量参考语料库的相似度对CommonCrawl进行过滤
        <br/>
        高质量的语料库例如WebText, Wikiedia, and web books corpus 作为正样例，将Common Crawl作为负样例训练了一个分类器，然后用这个分类器对原始Common Crawl中的样例进行打分，筛选出高质量的样例。
        <br/>
        <strong>
         B
        </strong>
        . 对训练语料去重：在数据集内部和跨数据集之间执行模糊文档去重。基于MinHash算法计算文档相似度，并删除相似度高的文档。
        <br/>
        <strong>
         C
        </strong>
        . 对训练语料中和测试集/验证集重叠的部分去重：清洗掉训练文档中与验证集和测试集中的样例有重叠的部分；
        <br/>
        对于训练集中的文档如果和验证集以及测试集中的样例有13-gram的重叠，则删去该文档中重叠的13-gram以及周围的200个字节，并将该文档从该处分成片段，并删去小于200个字节的片段。对于片段数多于10的文档，则被整个删除。
        <br/>
        <strong>
         D
        </strong>
        . 在训练数据中添加了几个高质量的数据集，包括WebText数据集的扩展版本，两个基于互联网的图书语料库(Books1和Books2)和英语Wikipedia。
       </img>
      </img>
     </img>
    </p>
    <p>
     <strong>
      4.GPT-3训练过程
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a422ea28783241a6b512332fa0c490fb.png"/>
    </p>
    <p>
     1.GPT-3的batch size达到了320万，为什么要选择这么大的批大小呢？
     <br/>
     （1）大模型相比于小模型更不容易过拟合，所以用更大的、噪音更少的Batch也不会带来太多负面影响；
     <br/>
     （2）a.多台机器分布式并行计算，同数据量，batch size上去了，通讯量减少时间的优势&gt;batch size增加导致梯度计算的时间复杂度增加量 b.减少梯度方差可以提高模型稳定性 c.工程经验
    </p>
    <p>
     2.模型变大、batch size提升的时候，OpenAI选择的学习率反而下降了
     <br/>
     模型参数量增加，高学习率可能导致优化过程不稳定（梯度爆炸等），降低学习率可以缓解。
     <br/>
     batch size增大降低梯度估计的噪声（更接近真实梯度），但过高的学习率可能导致单步更新幅度过大，破坏模型的稳定性。
     <br/>
     学习率下降核心逻辑：模型变大、batch size提升时，优先保障优化过程的稳定性和泛化性，而非单纯追求更新速度。这一权衡在大模型训练中尤其关键，因为训练成本极高，一旦优化失败代价巨大。
    </p>
    <p>
     <strong>
      5.9大数据集任务
     </strong>
     <br/>
     1.传统语言建模：完形填空与文本补全任务，预测特定词汇、完成句子/段落，或在多个文本补全选项中选择最佳答案。
     <br/>
     2.闭卷问答（closed book QA）任务，要求模型利用参数中存储的知识回答通用领域问题；
     <br/>
     3.语言翻译能力评估（重点关注单样本与少样本场景）；
     <br/>
     4.类Winograd Schema任务的表现测试；
     <br/>
     5.涉及常识推理或问答的数据集；
     <br/>
     6.阅读理解任务；
     <br/>
     7.SuperGLUE基准测试套件；
     <br/>
     8.自然语言推理（NLI）的初步探索；
     <br/>
     9.专门设计的情境学习能力探针任务，重点测试即时推理、动态适应与开放式文本生成能力
    </p>
    <p>
     <strong>
      6.实验结论
     </strong>
     <br/>
     1.GPT-3生成能力较强，生成能力随着给出的样例数和模型的参数量而增加，生成的长新闻可以以假乱真，有一定的代码生成能力；
     <br/>
     2.事实知识储备比较充足，科学知识储备欠缺，不能正确地用该类知识进行推理；
     <br/>
     3.将其它语言翻译成英语的能力较强，该能力随着模型大小和样例数而增加，将英语翻译成其它语言的能力弱；
     <br/>
     4.常识推理和精细阅读理解能力较弱，原因可能是因为GPT-3是单向语言模型，不能对文档进行回顾，比较，精读等；
     <br/>
     5.在大多数任务上，给出足够的样例，GPT-3的能力超过了fine-tune后的主流模型的能力；
     <br/>
     6.具有一些自主学习能力，能够简单处理没有见过的任务和识别一些简单的模式；
     <br/>
     7.总的来说GPT-3知识储备多，生成能力强，已经能够理解人类语言并具有自主学习能力；
    </p>
    <p>
     <strong>
      7.GPT-3模型的局限性 Limitations
     </strong>
    </p>
    <p>
     In-Context Learning上下文学习，这个模存在缺陷：
     <br/>
     （1）GPT-3的输入窗口长度是有限的，不可能无限的堆叠example的数量，即有限的输入窗口限制了我们利用海量数据的能力。
     <br/>
     （2）每次做一次新的预测，模型都要从输入的中间抓取有用的信息；可是做不到把从上一个输入中抓取到的信息存起来，存在模型中，用到下一次输入里。
    </p>
    <p>
     GPT-3模型的局限性 Limitations
     <br/>
     1.长文本生成能力还是很弱，2048输入
     <br/>
     2.基于物理等世界尝试的逻辑推理能力可能还不够。
     <br/>
     3.很难摆脱数据集的分布情况，对于一些明显不在这个分布或者和这个分布有冲突的任务来说，GPT-3还是无能为力
     <br/>
     4.每次做一次新的预测，模型都要从输入的中间抓取有用的信息；可是做不到把从上一个输入中抓取到的信息存起来，存在模型中，用到下一次输入里
     <br/>
     这就是GPT-3明明早在2020年5月就已经用巨大参数取得了很好的效果，但是学术界用Few-shot做上下文学习的热度好像还是不那么高的原因
     <br/>
     5.可能会输出有害的、有偏见的内容，和人类的价值观或者社会主义核心价值观不对齐
    </p>
    <p>
     <strong>
      8.GPT-3这篇论文在预训练时代的意义是什么？
     </strong>
     <br/>
     1.改变人机交互方式。以程序开发为例，在未来开发人员只需要定义需求，而AI将自动帮助开发人员完成编程，测试，部署等功能，整个开发周期大大缩短；
     <br/>
     2.让AI像人类一样学习和思考；
     <br/>
     3.随着计算能力变得愈发廉价，大模型越来越接近一个超级大脑，帮助人类完成各项任务。
     <br/>
     4.改变了传统的预训练加微调的范式，转向训练大规模通用模型。引领了大规模语言模型的时代。
    </p>
    <p>
     <strong>
      9.GPT-3是怎么进化成ChatGPT的
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ea9743fa375e46a3a0b7ae4318c6374f.png"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f7a68616f73757975616e2f:61727469636c652f64657461696c732f313436323537373339" class_="artid" style="display:none">
 </p>
</div>


