---
layout: post
title: "现代深度学习技术卷积神经网络06卷积神经网络LeNet"
date: 2025-03-11 10:23:10 +0800
description: "本文讲解深度学习中的卷积神经网络(LeNet)。在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。"
keywords: "【现代深度学习技术】卷积神经网络06：卷积神经网络（LeNet）"
categories: ['Pytorch']
tags: ['神经网络', '深度学习', '人工智能', 'Pytorch', 'Cnn']
artid: "146169602"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146169602
    alt: "现代深度学习技术卷积神经网络06卷积神经网络LeNet"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146169602
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146169602
cover: https://bing.ee123.net/img/rand?artid=146169602
image: https://bing.ee123.net/img/rand?artid=146169602
img: https://bing.ee123.net/img/rand?artid=146169602
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【现代深度学习技术】卷积神经网络06：卷积神经网络（LeNet）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/884c69c7065c4ca58ce77cc4068d7cff.gif#pic_center" width="500"/>
    </p>
    <blockquote>
     <p>
      【作者主页】
      <a href="https://blog.csdn.net/Morse_Chen?type=blog">
       Francek Chen
      </a>
      <br/>
      【专栏介绍】
      <a href="https://blog.csdn.net/morse_chen/category_12803271.html">
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          ⌈ 
          
         
        
          ⌈
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 1em; vertical-align: -0.25em;">
           </span>
           <span class="mopen">
            ⌈
           </span>
          </span>
         </span>
        </span>
       </span>
       PyTorch深度学习
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          ⌋ 
          
         
        
          ⌋
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 1em; vertical-align: -0.25em;">
           </span>
           <span class="mclose">
            ⌋
           </span>
          </span>
         </span>
        </span>
       </span>
      </a>
      深度学习 (DL, Deep Learning) 特指基于深层神经网络模型和方法的机器学习。它是在统计机器学习、人工神经网络等算法模型基础上，结合当代大数据和大算力的发展而发展出来的。深度学习最重要的技术特征是具有自动提取特征的能力。神经网络算法、算力和数据是开展深度学习的三要素。深度学习在计算机视觉、自然语言处理、多模态数据分析、科学探索等领域都取得了很多成果。本专栏介绍基于PyTorch的深度学习算法实现。
      <br/>
      【GitCode】专栏资源保存在我的GitCode仓库：
      <a href="https://gitcode.com/Morse_Chen/PyTorch_deep_learning">
       https://gitcode.com/Morse_Chen/PyTorch_deep_learning
      </a>
      。
     </p>
    </blockquote>
    <p>
    </p>
    <p>
    </p>
    <hr/>
    <p>
     通过之前几节，我们学习了构建一个完整卷积神经网络的所需组件。回想一下，之前我们将softmax回归模型（
     <a href="https://blog.csdn.net/Morse_Chen/article/details/145180868">
      softmax回归的从零开始实现
     </a>
     ）和多层感知机模型（
     <a href="https://blog.csdn.net/Morse_Chen/article/details/145269891">
      多层感知机的实现
     </a>
     ）应用于Fashion-MNIST数据集中的服装图片。为了能够应用softmax回归和多层感知机，我们首先将每个大小为
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        28 
        
       
         × 
        
       
         28 
        
       
      
        28\times28
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          28
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          28
         </span>
        </span>
       </span>
      </span>
     </span>
     的图像展平为一个784维的固定长度的一维向量，然后用全连接层对其进行处理。而现在，我们已经掌握了卷积层的处理方法，我们可以在图像中保留空间结构。同时，用卷积层代替全连接层的另一个好处是：模型更简洁、所需的参数更少。
    </p>
    <p>
     本节将介绍LeNet，它是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注。这个模型是由AT&amp;T贝尔实验室的研究员Yann LeCun在1989年提出的（并以其命名），目的是识别图像中的手写数字。当时，Yann LeCun发表了第一篇通过反向传播成功训练卷积神经网络的研究，这项工作代表了十多年来神经网络研究开发的成果。
    </p>
    <p>
     当时，LeNet取得了与支持向量机（support vector machines）性能相媲美的成果，成为监督学习的主流方法。LeNet被广泛用于自动取款机（ATM）机中，帮助识别处理支票的数字。时至今日，一些自动取款机仍在运行Yann LeCun和他的同事Leon Bottou在上世纪90年代写的代码呢！
    </p>
    <h3>
     <a id="LeNet_16">
     </a>
     一、LeNet
    </h3>
    <p>
     总体来看，LeNet（LeNet-5）由两个部分组成：
    </p>
    <ul>
     <li>
      <em>
       卷积编码器
      </em>
      ：由两个卷积层组成;
     </li>
     <li>
      <em>
       全连接层密集块
      </em>
      ：由三个全连接层组成。
     </li>
    </ul>
    <p>
     该架构如图1所示。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/72f51edfa7dc4cd299dd665dae0b0116.jpeg#pic_center" width="900"/>
    </p>
    <center>
     <font face="仿宋">
      图1 LeNet中的数据流。输入是手写数字，输出为10种可能结果的概率
     </font>
    </center>
    <p>
    </p>
    <p>
     每个卷积块中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。请注意，虽然ReLU和最大汇聚层更有效，但它们在20世纪90年代还没有出现。每个卷积层使用
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        5 
        
       
         × 
        
       
         5 
        
       
      
        5\times 5
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          5
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          5
         </span>
        </span>
       </span>
      </span>
     </span>
     卷积核和一个sigmoid激活函数。这些层将输入映射到多个二维特征输出，通常同时增加通道的数量。第一卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        2 
        
       
         × 
        
       
         2 
        
       
      
        2\times2
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          2
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          2
         </span>
        </span>
       </span>
      </span>
     </span>
     池操作（步幅2）通过空间下采样将维数减少4倍。卷积的输出形状由批量大小、通道数、高度、宽度决定。
    </p>
    <p>
     为了将卷积块的输出传递给稠密块，我们必须在小批量中展平每个样本。换言之，我们将这个四维输入转换成全连接层所期望的二维输入。这里的二维表示的第一个维度索引小批量中的样本，第二个维度给出每个样本的平面向量表示。LeNet的稠密块有三个全连接层，分别有120、84和10个输出。因为我们在执行分类任务，所以输出层的10维对应于最后输出结果的数量。
    </p>
    <p>
     通过下面的LeNet代码，可以看出用深度学习框架实现此类模型非常简单。我们只需要实例化一个
     <code>
      Sequential
     </code>
     块并将需要的层连接在一起。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l

net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     我们对原始模型做了一点小改动，去掉了最后一层的高斯激活。除此之外，这个网络与最初的LeNet-5一致。
    </p>
    <p>
     下面，我们将一个大小为
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        28 
        
       
         × 
        
       
         28 
        
       
      
        28 \times 28
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          28
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          28
         </span>
        </span>
       </span>
      </span>
     </span>
     的单通道（黑白）图像通过LeNet。通过在每一层打印输出的形状，我们可以检查模型，以确保其操作与我们期望的图2一致。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/df5770e10cad499c8703ccac9650bd63.png#pic_center" width="200"/>
    </p>
    <center>
     <font face="仿宋">
      图2 LeNet 的简化版
     </font>
    </center>
    <p>
    </p>
    <pre><code class="prism language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
<span class="token keyword">for</span> layer <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    X <span class="token operator">=</span> layer<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>layer<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__<span class="token punctuation">,</span><span class="token string">'output shape: \t'</span><span class="token punctuation">,</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fd6d7ad16a664fddaeb2a91f5b5d5b4c.png" width="450"/>
    </p>
    <p>
     请注意，在整个卷积块中，与上一层相比，每一层特征的高度和宽度都减小了。第一个卷积层使用2个像素的填充，来补偿
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        5 
        
       
         × 
        
       
         5 
        
       
      
        5 \times 5
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          5
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          5
         </span>
        </span>
       </span>
      </span>
     </span>
     卷积核导致的特征减少。相反，第二个卷积层没有填充，因此高度和宽度都减少了4个像素。随着层叠的上升，通道的数量从输入时的1个，增加到第一个卷积层之后的6个，再到第二个卷积层之后的16个。同时，每个汇聚层的高度和宽度都减半。最后，每个全连接层减少维数，最终输出一个维数与结果分类数相匹配的输出。
    </p>
    <h3>
     <a id="_65">
     </a>
     二、模型训练
    </h3>
    <p>
     现在我们已经实现了LeNet，让我们看看LeNet在Fashion-MNIST数据集上的表现。
    </p>
    <pre><code class="prism language-python">batch_size <span class="token operator">=</span> <span class="token number">256</span>
train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
</code></pre>
    <p>
     虽然卷积神经网络的参数较少，但与深度的多层感知机相比，它们的计算成本仍然很高，因为每个参数都参与更多的乘法。通过使用GPU，可以用它加快训练。
    </p>
    <p>
     为了进行评估，我们需要对
     <a href="https://blog.csdn.net/Morse_Chen/article/details/145180868">
      softmax回归的从零开始实现
     </a>
     中描述的
     <code>
      evaluate_accuracy
     </code>
     函数进行轻微的修改。由于完整的数据集位于内存中，因此在模型使用GPU计算数据集之前，我们需要将其复制到显存中。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">evaluate_accuracy_gpu</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> data_iter<span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#@save</span>
    <span class="token triple-quoted-string string">"""使用GPU计算模型在数据集上的精度"""</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        net<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 设置为评估模式</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> device<span class="token punctuation">:</span>
            device <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>device
    <span class="token comment"># 正确预测的数量，总预测的数量</span>
    metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> data_iter<span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># BERT微调所需的（之后将介绍）</span>
                X <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> X<span class="token punctuation">]</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                X <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y <span class="token operator">=</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre>
    <p>
     为了使用GPU，我们还需要一点小改动。与
     <a href="https://blog.csdn.net/Morse_Chen/article/details/145180868">
      softmax回归的从零开始实现
     </a>
     中定义的
     <code>
      train_epoch_ch3
     </code>
     不同，在进行正向和反向传播之前，我们需要将每一小批量数据移动到我们指定的设备（例如GPU）上。
    </p>
    <p>
     如下所示，训练函数
     <code>
      train_ch6
     </code>
     也类似于
     <a href="https://blog.csdn.net/Morse_Chen/article/details/145180868">
      softmax回归的从零开始实现
     </a>
     中定义的
     <code>
      train_ch3
     </code>
     。由于我们将实现多层神经网络，因此我们将主要使用高级API。以下训练函数假定从高级API创建的模型作为输入，并进行相应的优化。我们使用在
     <a href="https://blog.csdn.net/Morse_Chen/article/details/145388431">
      参数管理
     </a>
     中介绍的Xavier随机初始化模型参数。与全连接层一样，我们使用交叉熵损失函数和小批量随机梯度下降。
    </p>
    <pre><code class="prism language-python"><span class="token comment">#@save</span>
<span class="token keyword">def</span> <span class="token function">train_ch6</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""用GPU训练模型(在第六章定义)"""</span>
    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear <span class="token keyword">or</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'training on'</span><span class="token punctuation">,</span> device<span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span>xlabel<span class="token operator">=</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">,</span>
                            legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'train loss'</span><span class="token punctuation">,</span> <span class="token string">'train acc'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    timer<span class="token punctuation">,</span> num_batches <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Timer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># 训练损失之和，训练准确率之和，样本数</span>
        metric <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Accumulator<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>
        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_iter<span class="token punctuation">)</span><span class="token punctuation">:</span>
            timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                metric<span class="token punctuation">.</span>add<span class="token punctuation">(</span>l <span class="token operator">*</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
            train_l <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            train_acc <span class="token operator">=</span> metric<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">/</span> metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token punctuation">(</span>num_batches <span class="token operator">//</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">or</span> i <span class="token operator">==</span> num_batches <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">:</span>
                animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> num_batches<span class="token punctuation">,</span>
                             <span class="token punctuation">(</span>train_l<span class="token punctuation">,</span> train_acc<span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        test_acc <span class="token operator">=</span> evaluate_accuracy_gpu<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span>
        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_l<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, train acc </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, '</span></span>
          <span class="token string-interpolation"><span class="token string">f'test acc </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>test_acc<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>metric<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> num_epochs <span class="token operator">/</span> timer<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">}</span></span><span class="token string"> examples/sec '</span></span>
          <span class="token string-interpolation"><span class="token string">f'on </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">str</span><span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre>
    <p>
     现在，我们训练和评估LeNet-5模型。
    </p>
    <pre><code class="prism language-python">lr<span class="token punctuation">,</span> num_epochs <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">10</span>
train_ch6<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9660bf81a04e49759cbe1b65e5a30049.png" width="400">
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/747cb3d8a20b45cbbbbb785d8c1b1d4b.png" width="300"/>
     </img>
    </p>
    <h3>
     <a id="_156">
     </a>
     小结
    </h3>
    <ul>
     <li>
      卷积神经网络（CNN）是一类使用卷积层的网络。
     </li>
     <li>
      在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。
     </li>
     <li>
      为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。
     </li>
     <li>
      在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。
     </li>
     <li>
      LeNet是最早发布的卷积神经网络之一。
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f4d6f7273655f4368656e2f:61727469636c652f64657461696c732f313436313639363032" class_="artid" style="display:none">
 </p>
</div>


