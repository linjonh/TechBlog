---
layout: post
title: "Pytorch-转向TFConv过程中的卷积转换"
date: 2025-03-10 09:24:55 +0800
description: "图像中使用的卷积一般为，正方形卷积核针对一个同等面积邻域的，进行相乘后邻域叠加到中心，相当于考虑中心像素的周围信息，做了一定的信息融合。卷积中: kernel 卷积核  stride 步长  padding 填充  group 分组卷积   w预训练权重。卷积后: BN 归一化  act  激活函数。卷积前: input c1。"
keywords: "Pytorch 转向TFConv过程中的卷积转换"
categories: ['未分类']
tags: ['人工智能', 'Pytorch', 'Python']
artid: "146144150"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146144150
    alt: "Pytorch-转向TFConv过程中的卷积转换"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146144150
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146144150
cover: https://bing.ee123.net/img/rand?artid=146144150
image: https://bing.ee123.net/img/rand?artid=146144150
img: https://bing.ee123.net/img/rand?artid=146144150
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Pytorch 转向TFConv过程中的卷积转换
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     转换知识基础
    </p>
    <p>
     图像中使用的卷积一般为，正方形卷积核针对一个同等面积邻域的，进行相乘后邻域叠加到中心，相当于考虑中心像素的周围信息，做了一定的信息融合。
    </p>
    <p>
     卷积相关参数
    </p>
    <p>
     卷积前: input c1
    </p>
    <p>
     卷积中: kernel 卷积核  stride 步长  padding 填充  group 分组卷积   w预训练权重
    </p>
    <p>
     卷积后: BN 归一化  act  激活函数
    </p>
    <p>
     SAME: 相同的PAD
    </p>
    <p>
     VALD: 不做任何填充
    </p>
    <p>
    </p>
    <p>
     SAME和VASLID的优劣对比
    </p>
    <h4>
     <strong>
      总结：
      <code>
       stride=1
      </code>
      时使用
      <code>
       "SAME"
      </code>
      的优势
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        维度
       </th>
       <th>
        说明
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         输出尺寸稳定
        </strong>
       </td>
       <td>
        保持输入与输出尺寸一致，避免网络深层特征图过度缩小。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         对称填充保证
        </strong>
       </td>
       <td>
        自动计算对称填充量，与 PyTorch 的默认行为对齐，便于模型迁移。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         代码简洁性
        </strong>
       </td>
       <td>
        无需手动计算和添加填充层，减少冗余代码。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         计算效率
        </strong>
       </td>
       <td>
        框架原生优化填充操作，通常比手动填充更高效。
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h4>
     附：不同参数下的填充行为对比表
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        参数
       </th>
       <th>
        <code>
         stride=1
        </code>
        +
        <code>
         "SAME"
        </code>
       </th>
       <th>
        <code>
         stride=2
        </code>
        + 手动填充 +
        <code>
         "VALID"
        </code>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         输出尺寸
        </strong>
       </td>
       <td>
        与输入相同
       </td>
       <td>
        按公式 ⌊H+2p−ks⌋+1⌊sH+2p−k​⌋+1 计算
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         填充方式
        </strong>
       </td>
       <td>
        自动对称或轻微不对称
       </td>
       <td>
        手动对称填充
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         跨框架一致性
        </strong>
       </td>
       <td>
        与 PyTorch 的
        <code>
         padding=k//2
        </code>
        对齐
       </td>
       <td>
        强制对齐 PyTorch 的填充逻辑
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         典型应用场景
        </strong>
       </td>
       <td>
        密集特征提取（如残差块）
       </td>
       <td>
        下采样（如网络前半部分）
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
    <pre><code class="language-python"># 注释版本 2025 03 10
class TFConv(keras.layers.Layer):
    """实现标准卷积层，可选批归一化与激活函数，适用于TensorFlow。"""

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True, w=None):
        """
        参数说明:
            c1 (int): 输入通道数（代码中未直接使用，可能通过权重自动推断）
            c2 (int): 输出通道数（即卷积核数量）
            k (int/tuple): 卷积核尺寸，默认为1
            s (int/tuple): 卷积步长，默认为1
            p (int/tuple/None): 填充大小，None时自动计算为k//2
            g (int): 分组卷积组数，默认1（代码支持但原注释提示可能限制）
            act (bool/str): 是否使用激活函数，默认True
            w (object): 预训练权重对象，包含卷积和BN参数
        """
        super().__init__()
        # 当步长s=1时使用"SAME"填充，否则手动填充后使用"VALID"卷积以对齐PyTorch行为
        # 创建Conv2D层，配置参数
        conv = keras.layers.Conv2D(
            filters=c2,
            kernel_size=k,
            strides=s,
            padding="SAME" if s == 1 else "VALID",  # s=1用自动填充，s&gt;1需手动填充
            use_bias=not hasattr(w, "bn"),  # 存在BN层时禁用偏置
            groups=g,  # 分组卷积参数（需确认TF版本支持性）
            kernel_initializer=keras.initializers.Constant(
                w.conv.weight.permute(2, 3, 1, 0).numpy()  # 调整PyTorch权重维度到TF格式 (kH, kW, in, out)
            ),
            bias_initializer="zeros" if hasattr(w, "bn") else keras.initializers.Constant(w.conv.bias.numpy()),
        )
        # 组合层：当s&gt;1时，先填充再卷积
        self.conv = conv if s == 1 else keras.Sequential([TFPad(autopad(k, p)), conv])  # 自定义填充层处理非1步长
        # 批归一化层（存在BN参数时）或恒等映射
        self.bn = TFBN(w.bn) if hasattr(w, "bn") else tf.identity
        # 激活函数（根据act参数选择）或恒等函数
        self.act = activations(w.act) if act else tf.identity

    def call(self, inputs):
        """执行卷积、批归一化与激活函数的前向传播"""
        return self.act(self.bn(self.conv(inputs)))  # 顺序: Conv -&gt; BN -&gt; Act
</code></pre>
    <p>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f753031333539303332372f:61727469636c652f64657461696c732f313436313434313530" class_="artid" style="display:none">
 </p>
</div>


