---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35343739333936302f:61727469636c652f64657461696c732f313436323739313132"
layout: post
title: "GreenKGC-A-Lightweight-Knowledge-Graph-Completion-Method论文笔记"
date: 2025-03-15 17:20:46 +0800
description: "一种基于知识图谱嵌入（KGC）的降低维度方法"
keywords: "GreenKGC: A Lightweight Knowledge Graph Completion Method（论文笔记）"
categories: ['知识图谱补全']
tags: ['知识图谱']
artid: "146279112"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146279112
    alt: "GreenKGC-A-Lightweight-Knowledge-Graph-Completion-Method论文笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146279112
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146279112
cover: https://bing.ee123.net/img/rand?artid=146279112
image: https://bing.ee123.net/img/rand?artid=146279112
img: https://bing.ee123.net/img/rand?artid=146279112
---

# GreenKGC: A Lightweight Knowledge Graph Completion Method（论文笔记）

CCF等级：A

发布时间：2023年7月

[代码位置](https://github.com/yunchengwang/GreenKGC "代码位置")

25年3月17日交

---

## 一、简介

传统知识图谱补全方法中，嵌入维度的选择很关键，维度过低会导致高误差，而过高则易过拟合且计算成本大。GreenKGC通过降低维度来解决这些问题。它首先获得实体和关系的高维表示，然后通过计算保留最关键维度，减少过拟合并降低计算复杂度。这样，GreenKGC在低维度下也能保持良好性能，同时显著减小模型大小。

---

## 二、原理

### 1.整体

GreenKGC由三个模块组成：表示学习、特征修剪、决策学习。

首先使用现有的知识图谱嵌入方法获取实体和关系的高维表示。然后，在特征剪枝阶段，通过一种称为判别特征测试（DFT）的方法来评估每个维度的重要性。具体来说，通过每一个三元组（h，r，t），计算各维度对区分正样本和负样本的能力。

在这个过程中，对每一个三元组，通过线性变换将头实体h、关系r、尾实体t 的各维度组合成一个单一变量，并基于这些变换后的特征计算其对正负样本的判别能力（通过交叉熵）。根据计算出的权重（即判别能力），保留那些具有较高权重的维度，从而形成低维并且具有区分性的特征能力。这种方法有助于减少不必要的参数，提高模型效率，同时保持甚至提升性能。

### 2.表示学习

可以使用现有的模型例如TransE、RotatE等，这一块不是文章重点考虑的对象。

### 3.特征修剪

考虑到知识图谱中的关系不可能是完全无关的，因此首先将他们划分为不相交的关系组，魅族关系具有相似的属性，方便后续对每个关系组内进行特征修剪。文章使用k-Means对嵌入关系进行聚类。

![](https://i-blog.csdnimg.cn/direct/289aa4f383da44f48d0d448b04dbdd3a.png)

使用不同的分类数k，会呈现不同的效果。交叉熵越小证明有效地识别正样本和负样本。

**（文章中这一部分是在DFT以后写的，但在程序上是先进行特征分区再DFT，因此这块提前写。）**

DFT可用于降低实体和关系嵌入的维数，同时保留它们在下游任务中的能力。

文章将DFT扩展到多个维度，通过SVD（奇异值分解，简单来说就是将一个矩阵分解为三个矩阵的的乘积）和PCA（主成分分析，简单来说就是确定权重以及找到权重大的维度）学习每个维度的线性变换。

但是由于PCA的线性变换是无监督的，难以将正负三元组区分开（后面需要使用）。因此使用最小化二元交叉熵损失的逻辑回归来实现。整体相当于DFT=SVD+PCA+最小化二元交叉熵。

![\zeta = -y\log(\sigma (\omega_i[h_i,r_i,t_i]^{T}))-(1-y)\log(1-\sigma(\omega_i[h_i,r_i,t_i]^{T}))](https://latex.csdn.net/eq?%5Czeta%20%3D%20-y%5Clog%28%5Csigma%20%28%5Comega_i%5Bh_i%2Cr_i%2Ct_i%5D%5E%7BT%7D%29%29-%281-y%29%5Clog%281-%5Csigma%28%5Comega_i%5Bh_i%2Cr_i%2Ct_i%5D%5E%7BT%7D%29%29)

将正三元组取y=1，负三元组取y=0。以此计算每个维度。DFT采用交叉熵（CE）评估每个维度的判别能力。因为CE是二值分类的损失函数，CE越低的维度意味着判别能力越强。因此保留最低CE的特征维度，并对剩余的特征进行修剪以获取低维特征。

### 4.决策学习

文章采用二元分类器作为解码器，性能比评分函数更强大，输入三元组的特征，输出三元组正确概率（0到1）。文章使用负样本进行训练非线性的二元分类器。将每一个特征带入公式计算得出一个二元分类器。y= 1是正样本，y=0是负样本。

![l(y,\widehat{y})=-y\log(\widehat{y})-(1-y)\log(1-\widehat{y})](https://latex.csdn.net/eq?l%28y%2C%5Cwidehat%7By%7D%29%3D-y%5Clog%28%5Cwidehat%7By%7D%29-%281-y%29%5Clog%281-%5Cwidehat%7By%7D%29)

文章考虑两种负抽样。一种基于本体的负抽样，一种基于嵌入的负抽样。文章通过实验选择基于嵌入的负抽样。

---

## 三、实验性能

### 1.主要结果

![](https://i-blog.csdnimg.cn/direct/98899cd53cff43dc9237ac1562cec856.png)

表3证明以前的模型再加上GreenKGC后都有了大的性能提升。比另外几种模型性能高。

![](https://i-blog.csdnimg.cn/direct/cfeb04905e594c54913d7da75f2d4057.png)

图四表示加入GreenKGC的模型在较低维度时就具有较高的正确率。

![](https://i-blog.csdnimg.cn/direct/e9cc54e456c24a2bb1078056aacf4131.png)

![](https://i-blog.csdnimg.cn/direct/4abc062c65cb46b2a09680a9d972ef6b.png)

GreenKGC可以在模型尺寸缩小约5倍的情况下获得具有竞争力甚至更好的性能。在有限资源下
  
将GreenKGC应用于大规模KGs的性能优势。

### 2.消融实验

实验了
**不进行特征剪枝、随机剪枝、基于方差的剪枝、基于特征重要性的剪枝、基于交叉熵的剪枝。数据如下。**

![](https://i-blog.csdnimg.cn/direct/89c73e4f5a504b109f6520850eae1613.png)

---

## 四、结论和未来工作

1.
**增强特征剪枝的灵活性**

**2.改进负抽样策略**

**3.与其他模型的融合（如GNN）**