---
layout: post
title: "GreenKGC-A-Lightweight-Knowledge-Graph-Completion-Method论文笔记"
date: 2025-03-15 17:20:46 +0800
description: "一种基于知识图谱嵌入（KGC）的降低维度方法"
keywords: "GreenKGC: A Lightweight Knowledge Graph Completion Method（论文笔记）"
categories: ['知识图谱补全']
tags: ['知识图谱']
artid: "146279112"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146279112
    alt: "GreenKGC-A-Lightweight-Knowledge-Graph-Completion-Method论文笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146279112
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146279112
cover: https://bing.ee123.net/img/rand?artid=146279112
image: https://bing.ee123.net/img/rand?artid=146279112
img: https://bing.ee123.net/img/rand?artid=146279112
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     GreenKGC: A Lightweight Knowledge Graph Completion Method（论文笔记）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     CCF等级：A
    </p>
    <p>
     发布时间：2023年7月
    </p>
    <p>
     <a href="https://github.com/yunchengwang/GreenKGC" title="代码位置">
      代码位置
     </a>
    </p>
    <p>
     25年3月17日交
    </p>
    <hr/>
    <h2 id="%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B" name="%E4%B8%80%E3%80%81%E7%AE%80%E4%BB%8B">
     一、简介
    </h2>
    <p>
     传统知识图谱补全方法中，嵌入维度的选择很关键，维度过低会导致高误差，而过高则易过拟合且计算成本大。GreenKGC通过降低维度来解决这些问题。它首先获得实体和关系的高维表示，然后通过计算保留最关键维度，减少过拟合并降低计算复杂度。这样，GreenKGC在低维度下也能保持良好性能，同时显著减小模型大小。
    </p>
    <hr/>
    <h2 id="%E4%BA%8C%E3%80%81%E5%8E%9F%E7%90%86" name="%E4%BA%8C%E3%80%81%E5%8E%9F%E7%90%86">
     二、原理
    </h2>
    <h3 id="1.%E6%95%B4%E4%BD%93" name="1.%E6%95%B4%E4%BD%93">
     1.整体
    </h3>
    <p>
     GreenKGC由三个模块组成：表示学习、特征修剪、决策学习。
    </p>
    <p>
     首先使用现有的知识图谱嵌入方法获取实体和关系的高维表示。然后，在特征剪枝阶段，通过一种称为判别特征测试（DFT）的方法来评估每个维度的重要性。具体来说，通过每一个三元组（h，r，t），计算各维度对区分正样本和负样本的能力。
    </p>
    <p>
     在这个过程中，对每一个三元组，通过线性变换将头实体h、关系r、尾实体t 的各维度组合成一个单一变量，并基于这些变换后的特征计算其对正负样本的判别能力（通过交叉熵）。根据计算出的权重（即判别能力），保留那些具有较高权重的维度，从而形成低维并且具有区分性的特征能力。这种方法有助于减少不必要的参数，提高模型效率，同时保持甚至提升性能。
    </p>
    <h3 id="2.%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0" name="2.%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0">
     2.表示学习
    </h3>
    <p>
     可以使用现有的模型例如TransE、RotatE等，这一块不是文章重点考虑的对象。
    </p>
    <h3 id="3.%E7%89%B9%E5%BE%81%E4%BF%AE%E5%89%AA" name="3.%E7%89%B9%E5%BE%81%E4%BF%AE%E5%89%AA">
     3.特征修剪
    </h3>
    <p>
     考虑到知识图谱中的关系不可能是完全无关的，因此首先将他们划分为不相交的关系组，魅族关系具有相似的属性，方便后续对每个关系组内进行特征修剪。文章使用k-Means对嵌入关系进行聚类。
    </p>
    <p>
     <img alt="" height="453" src="https://i-blog.csdnimg.cn/direct/289aa4f383da44f48d0d448b04dbdd3a.png" width="578"/>
    </p>
    <p>
     使用不同的分类数k，会呈现不同的效果。交叉熵越小证明有效地识别正样本和负样本。
    </p>
    <p>
     <strong>
      （文章中这一部分是在DFT以后写的，但在程序上是先进行特征分区再DFT，因此这块提前写。）
     </strong>
    </p>
    <p>
     DFT可用于降低实体和关系嵌入的维数，同时保留它们在下游任务中的能力。
    </p>
    <p>
     文章将DFT扩展到多个维度，通过SVD（奇异值分解，简单来说就是将一个矩阵分解为三个矩阵的的乘积）和PCA（主成分分析，简单来说就是确定权重以及找到权重大的维度）学习每个维度的线性变换。
    </p>
    <p>
     但是由于PCA的线性变换是无监督的，难以将正负三元组区分开（后面需要使用）。因此使用最小化二元交叉熵损失的逻辑回归来实现。整体相当于DFT=SVD+PCA+最小化二元交叉熵。
    </p>
    <p style="text-align:center">
     <img alt="\zeta = -y\log(\sigma (\omega_i[h_i,r_i,t_i]^{T}))-(1-y)\log(1-\sigma(\omega_i[h_i,r_i,t_i]^{T}))" class="mathcode" src="https://latex.csdn.net/eq?%5Czeta%20%3D%20-y%5Clog%28%5Csigma%20%28%5Comega_i%5Bh_i%2Cr_i%2Ct_i%5D%5E%7BT%7D%29%29-%281-y%29%5Clog%281-%5Csigma%28%5Comega_i%5Bh_i%2Cr_i%2Ct_i%5D%5E%7BT%7D%29%29"/>
    </p>
    <p>
     将正三元组取y=1，负三元组取y=0。以此计算每个维度。DFT采用交叉熵（CE）评估每个维度的判别能力。因为CE是二值分类的损失函数，CE越低的维度意味着判别能力越强。因此保留最低CE的特征维度，并对剩余的特征进行修剪以获取低维特征。
    </p>
    <h3 id="4.%E5%86%B3%E7%AD%96%E5%AD%A6%E4%B9%A0" name="4.%E5%86%B3%E7%AD%96%E5%AD%A6%E4%B9%A0">
     4.决策学习
    </h3>
    <p>
     文章采用二元分类器作为解码器，性能比评分函数更强大，输入三元组的特征，输出三元组正确概率（0到1）。文章使用负样本进行训练非线性的二元分类器。将每一个特征带入公式计算得出一个二元分类器。y= 1是正样本，y=0是负样本。
    </p>
    <p style="text-align:center">
     <img alt="l(y,\widehat{y})=-y\log(\widehat{y})-(1-y)\log(1-\widehat{y})" class="mathcode" src="https://latex.csdn.net/eq?l%28y%2C%5Cwidehat%7By%7D%29%3D-y%5Clog%28%5Cwidehat%7By%7D%29-%281-y%29%5Clog%281-%5Cwidehat%7By%7D%29"/>
    </p>
    <p>
     文章考虑两种负抽样。一种基于本体的负抽样，一种基于嵌入的负抽样。文章通过实验选择基于嵌入的负抽样。
    </p>
    <hr/>
    <h2 id="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%80%A7%E8%83%BD" name="%E4%B8%89%E3%80%81%E5%AE%9E%E9%AA%8C%E6%80%A7%E8%83%BD">
     三、实验性能
    </h2>
    <h3 id="1.%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C" name="1.%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C">
     1.主要结果
    </h3>
    <p class="img-center">
     <img alt="" height="371" src="https://i-blog.csdnimg.cn/direct/98899cd53cff43dc9237ac1562cec856.png" width="813"/>
    </p>
    <p>
     表3证明以前的模型再加上GreenKGC后都有了大的性能提升。比另外几种模型性能高。
    </p>
    <p class="img-center">
     <img alt="" height="602" src="https://i-blog.csdnimg.cn/direct/cfeb04905e594c54913d7da75f2d4057.png" width="628"/>
    </p>
    <p>
     图四表示加入GreenKGC的模型在较低维度时就具有较高的正确率。
    </p>
    <p class="img-center">
     <img alt="" height="294" src="https://i-blog.csdnimg.cn/direct/e9cc54e456c24a2bb1078056aacf4131.png" width="650"/>
    </p>
    <p class="img-center">
     <img alt="" height="255" src="https://i-blog.csdnimg.cn/direct/4abc062c65cb46b2a09680a9d972ef6b.png" width="509"/>
    </p>
    <p>
     GreenKGC可以在模型尺寸缩小约5倍的情况下获得具有竞争力甚至更好的性能。在有限资源下
     <br/>
     将GreenKGC应用于大规模KGs的性能优势。
    </p>
    <h3 id="2.%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" name="2.%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">
     2.消融实验
    </h3>
    <p>
     实验了
     <strong>
      不进行特征剪枝、随机剪枝、基于方差的剪枝、基于特征重要性的剪枝、基于交叉熵的剪枝。数据如下。
     </strong>
    </p>
    <p class="img-center">
     <img alt="" height="277" src="https://i-blog.csdnimg.cn/direct/89c73e4f5a504b109f6520850eae1613.png" width="502"/>
    </p>
    <hr/>
    <h2 id="%E5%9B%9B%E3%80%81%E7%BB%93%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C" name="%E5%9B%9B%E3%80%81%E7%BB%93%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C">
     四、结论和未来工作
    </h2>
    <p>
     1.
     <strong>
      增强特征剪枝的灵活性
     </strong>
    </p>
    <p>
     <strong>
      2.改进负抽样策略
     </strong>
    </p>
    <p>
     <strong>
      3.与其他模型的融合（如GNN）
     </strong>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35343739333936302f:61727469636c652f64657461696c732f313436323739313132" class_="artid" style="display:none">
 </p>
</div>


