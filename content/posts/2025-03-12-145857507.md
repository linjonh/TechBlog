---
layout: post
title: "机器学习吴恩达"
date: 2025-03-12 21:19:20 +0800
description: "机器学习涵盖监督学习、无监督学习与强化学习。核心算法包括线性模型、树模型、集成方法、核技巧及概率模型。深度学习以神经网络为基础，涉及CNN、RNN、Transformer、激活函数及优化技术。评估指标含分类、回归与聚类，正则化手段应对过拟合，交叉验证保障泛化性，强化学习结合值函数与策略优化，形成从传统统计到深度强化学习的完整体系"
keywords: "机器学习(吴恩达)"
categories: ['人工智能']
tags: ['机器学习', '人工智能']
artid: "145857507"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145857507
    alt: "机器学习吴恩达"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145857507
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145857507
cover: https://bing.ee123.net/img/rand?artid=145857507
image: https://bing.ee123.net/img/rand?artid=145857507
img: https://bing.ee123.net/img/rand?artid=145857507
---

# 机器学习(吴恩达)

## 一, 机器学习

**机器学习定义:** 计算机能够在没有明确的编程情况下学习

**特征:** 特征是描述样本的属性或变量，是模型用来学习和预测的基础。如: 房屋面积, 地理位置

**标签:** 监督学习中需要预测的目标变量，是模型的输出目标。如: 房屋价格

**样本:** 如: {面积=100㎡, 卧室=3, 位置=市中心, 价格=500万}, 数据集中的一个独立实例, 包含一组特征及对应的标签。

**样本向量形式:**

![](https://i-blog.csdnimg.cn/direct/f1eeb35df6594f3d93f14e9bf37469d0.png)

**独热编码举例:**

![](https://i-blog.csdnimg.cn/direct/9fc9e02d7ae84307822d731173d98dbe.png)

### 1.1 机器学习的分类

**1) 监督学习:** 根据带标签的数据训练模型，预测新样本的标签。如**回归** , **分类** 。

**回归应用:** 房价预测

![](https://i-blog.csdnimg.cn/direct/baeda87745d64ecdada34332c38e680c.png)

**分类算法:** 根据年龄和肿瘤大小判断肿瘤良/恶性

![](https://i-blog.csdnimg.cn/direct/1b24700e3fef4bb0af54706e106a8542.png)

**2) 无监督学习:** 从未标注数据中发现潜在结构或模式。如**聚类** , **异常检测** , **降维**(大数据集压缩成更小的数据集,
并可能少地丢失信息)

**聚类:** 谷歌新闻示例

![](https://i-blog.csdnimg.cn/direct/9233b92223084a4a83f4778aa7930d02.png)

**3) ​半监督学习:** 结合少量标注数据和大量未标注数据进行训练。如医学图像分析

**4) ​强化学习:** 通过与环境交互学习策略，最大化累积奖励。如游戏AI, 自动驾驶

### 1.2 线性回归

#### 1.2.1 损失函数与成本函数

**第i个数据特征:**

![](https://i-blog.csdnimg.cn/direct/9c31f0d992fc4bac91c5ae758678b2e3.png)

**损失函数（Loss Function）​** ：衡量**单个样本** 的预测值与真实值的差异。

**成本函数（Cost Function）​** ：衡量**整个训练集** 的平均损失。

**平方误差损失:**

![](https://i-blog.csdnimg.cn/direct/5ecb399368e74416b37709c342438c69.png)

**平方误差成本函数:** 一定是凸函数，确保只有一个全局最小值

![](https://i-blog.csdnimg.cn/direct/1d14f5d779b34ebaae490cec33207b01.png)

**模型(y=wx)与成本函数示例 (左图w=-0.5、0、0.5、1时的情况):**

![](https://i-blog.csdnimg.cn/direct/308808d8e4194f5298ffd96e74b53192.png)

**模型(y=wx+b)下的成本函数:**

![](https://i-blog.csdnimg.cn/direct/ea7c611a64d6450386f13481810d3724.png)

**模型与J(w,b)的平面等高线:**

![](https://i-blog.csdnimg.cn/direct/5d33507789ce49bda2df851663748d34.png)

![](https://i-blog.csdnimg.cn/direct/f7dd3f7ce97e426bb299c0af1b88225d.png)![](https://i-blog.csdnimg.cn/direct/dc91501f9e234d8b985851a9fbb727f0.png)

#### 1.2.2 学习率与梯度下降算法

**学习率（α）​** ：控制模型参数更新步长的超参数。

**学习率的取值的两种情况:**

**1) 学习率过大** ：参数更新步长过大，可能导致损失值震荡甚至发散。

**2) 学习率过小** ：收敛速度极慢，可能陷入局部极小值。

**示例:**

![](https://i-blog.csdnimg.cn/direct/893d997c5a224a618537d0df7bd5ad72.png)

**梯度下降公式:**

![](https://i-blog.csdnimg.cn/direct/33c65ae8598349c18ad38f3f3b9f3b97.png)

![](https://i-blog.csdnimg.cn/direct/78ffb5be08cf4afc8eb6505a5a7ae8b6.png)

**推导过程:**

![](https://i-blog.csdnimg.cn/direct/8bdd29319e1641ada79cd9448f86dc65.png)

**梯度算法演示:**

![](https://i-blog.csdnimg.cn/direct/3791f37f981245e49da5f4c6bf5849a9.png)

**两个特征的多元线性回归举例:**

![](https://i-blog.csdnimg.cn/direct/4c20cdde0f924810b8fbb280fd94e156.png)

![](https://i-blog.csdnimg.cn/direct/b4ad887540144748a858bfc402c54328.png)

![](https://i-blog.csdnimg.cn/direct/b55cb6545ada42848c29245fb224cce0.png)

![](https://i-blog.csdnimg.cn/direct/0451c075c5264c57a792567759ebc72e.png)

![](https://i-blog.csdnimg.cn/direct/f001de64b0704241afa4de6cd29fd589.png)

![](https://i-blog.csdnimg.cn/direct/15e8b40553f24553869977efea1ab368.png)

**批量梯度下降:** 每次迭代使用**全部训练数据** 计算梯度。

**随机梯度下降:** 每次迭代**随机选取一个样本** 计算梯度。

**小批量处理实现流程:**

  1. **数据分块** ：将训练集随机划分为多个小批量。
  2. **前向传播** ：对当前小批量计算模型输出。
  3. **损失计算** ：根据预测值和真实标签计算损失（如交叉熵、均方误差）。
  4. **反向传播** ：计算损失对参数的梯度。
  5. **参数更新** ：使用优化算法（如SGD、Adam）更新模型参数。
  6. **重复** ：遍历所有小批量完成一个训练周期（Epoch）。

#### 1.2.3 特征缩放

> 加速模型收敛。有如下方法

**标准化（Z-Score标准化）:**

**![](https://i-blog.csdnimg.cn/direct/22838c724c9248c7bfc19671b23296cb.png)**

**标准差:**

![](https://i-blog.csdnimg.cn/direct/65aee970bcbc442aa9e026a157bf08a2.png)

**归一化（Min-Max缩放）:**

![](https://i-blog.csdnimg.cn/direct/8ed85649e99a4d5795e261dabf097ad0.png)

**标准化与归一化的区别:**

![](https://i-blog.csdnimg.cn/direct/8464077bfea0466cb88479d5e4fea119.png)

**举例 (标准化前后的数据集) :**

![](https://i-blog.csdnimg.cn/direct/4c20cdde0f924810b8fbb280fd94e156.png)

![](https://i-blog.csdnimg.cn/direct/d7e5ceabd67e4ca9894ab2bd224940df.png)

#### **1.2.4 正则化**

**解决过拟合情况:**

1) 收集更多数据

2) 仅用特征的一个子集

3) 正则化

**欠拟合(高偏差), 适中, 过拟合(高方差)**

![](https://i-blog.csdnimg.cn/direct/c699ff6e711e43a3bc8a735d84ff8297.png)

**正则化项:**

![](https://i-blog.csdnimg.cn/direct/89ebe14f3cc54d7e8f2effe7188f067a.png)

**添加正则化项后的梯度算法:**

![](https://i-blog.csdnimg.cn/direct/0f41019744a64c93ae6ee46b5fa49193.png)

**原理:** 通过在损失函数中添加与模型参数相关的惩罚项，限制参数的复杂度，从而提升模型的泛化能力。 (使得**W** 尽可能小以此使得函数趋于平滑)

**λ过大:** 参数被过度压缩，模型过于简单，无法捕捉数据中的有效规律。

**λ过小:** 正则化作用微弱，模型过度依赖训练数据中的噪声或局部特征。

**备注:** 只要正则化得当, 更大的神经网络总是更好的。

**例图:**

![](https://i-blog.csdnimg.cn/direct/48b03e241cef450794c90ccb63658fe0.png)

**根据交叉验证误差找到适合的λ:**

![](https://i-blog.csdnimg.cn/direct/8e71fce02c4942829b911ff7795feab5.png)

**λ取值与交叉验证误差及训练集误差的关系:**

![](https://i-blog.csdnimg.cn/direct/7d260c54a2cc4ca5a8f81539db45f01c.png)

### 1.3 逻辑回归

> 通过线性组合特征与参数，结合Sigmoid函数将输出映射到概率区间（0-1），用于解决**分类问题** ​（尤其是二分类）。

**Sigmoid函数模型:**

![](https://i-blog.csdnimg.cn/direct/daef4e55d0ae4a8f900d3e19688ddd54.png)

**图形:**

![](https://i-blog.csdnimg.cn/direct/e384884f41254ded882cfbaf78d0876e.png)

**对数损失函数（交叉熵损失）:**

![](https://i-blog.csdnimg.cn/direct/e3d260b0ead043f88b37e222ccfa1a76.png)

**对应图形:**

![](https://i-blog.csdnimg.cn/direct/cf02898b05914247b2f6d8a757076281.png)

![](https://i-blog.csdnimg.cn/direct/8586a84d2d5648dca7fb55a1c553c633.png)

**为什么不使用均方误差(MSE)作为损失函数:** 当预测值接近 0 或 1 时, 梯度接近于0, 权重几乎无法更新。

![](https://i-blog.csdnimg.cn/direct/9a517d2946f74715ab3b0aeeae7ba4c7.png)

**对应成本函数:**

![](https://i-blog.csdnimg.cn/direct/4fabb0125c4547a29144aab403d4c925.png)

![](https://i-blog.csdnimg.cn/direct/7d8c35927cfb46feb3ad4e38f2e55888.png)

![](https://i-blog.csdnimg.cn/direct/bbe57a4d02574a50b40139b223a5fbc9.png)

**为什么选择对数损失函数:**

**1) 概率视角：** 最大似然估计（MLE）

**2) 优化视角：** 凸性

**梯度下降算法:**

![](https://i-blog.csdnimg.cn/direct/6fefb484f6994f9db7b347f79b9b738f.png)

**与线性回归梯度算法的区别: 模型定义不同:**

![](https://i-blog.csdnimg.cn/direct/403ae63f2da2498e9a89a38cb3ae403d.png)

**线性回归与逻辑回归区别:**

![](https://i-blog.csdnimg.cn/direct/b7bebd0576e94c89820d5321e566f793.png)

### 1.4 决策树

> 一种树形结构的监督学习模型，通过选择最优特征对数据进行分割，目标是使子节点的数据尽可能“纯净”（同类样本集中）。

**递归分裂过程:** ​

  1. 从根节点开始，计算所有特征的分裂指标（如信息增益）。
  2. 选择最优特征作为当前节点的分裂特征。
  3. 根据特征的取值将数据集划分为子集，生成子节点。
  4. 对每个子节点递归执行步骤1-3，直到满足停止条件。

**停止条件:** ​

  * 节点样本数小于预设阈值。
  * 所有样本属于同一类别。
  * 特征已用完或分裂后纯度提升不显著。

​**预剪枝:** 在树生长过程中提前终止分裂。如设置最大深度

**信息熵** ：度量数据集的混乱程度。值**越小** 分类**越明确** 。

![](https://i-blog.csdnimg.cn/direct/8a4c2ad61ccb4573afb0626051525051.png)

**二分类示例图:**

![](https://i-blog.csdnimg.cn/direct/c5f9a67adf8f41aa996804ebd8cce605.png)

**信息增益（IG）** ：特征分裂后熵的减少量( 父节点熵 - 子节点加权平均熵)。值**越大** 特征越重要。

**多分类推广:**

![](https://i-blog.csdnimg.cn/direct/59d3bd2d859e4cfc9ec7b8c9992f8948.png)

**符号含义:**

![](https://i-blog.csdnimg.cn/direct/4668c44f26224049a0e606ce39e7b905.png)

**举例:**

![](https://i-blog.csdnimg.cn/direct/16bd2d492c6c408dbc7174d10616387c.png)

**二分类分裂决策举例:**

![](https://i-blog.csdnimg.cn/direct/c4dde511104d4471a1986a47c9ccb811.png)

**信息增益率（IGR）** : 在信息增益的基础上，对特征本身的分布熵进行标准化。

**公式:**

![](https://i-blog.csdnimg.cn/direct/ffb9b115fefa41a4a0d75df7c8c90b45.png)

**基尼系数:** 另一种不纯度度量, 可视为熵的近似, 但计算更高效

**公式:**

![](https://i-blog.csdnimg.cn/direct/b0591b03d93843f7a135136a2e5beaf5.png)

**三种经典决策树算法:**

![](https://i-blog.csdnimg.cn/direct/cd75252d77b248caa8e2b6a3d637bcff.png)

**决策树处理处理连续值特征:**

**1)** ​**特征排序:** 从小到大排序

**2) 候选分割点生成** ：相邻值的中间点作为候选分割点。

**3) ​计算分裂指标:** 计算分裂后的信息增益（分类）或均方误差（回归）。

**4) 选择最优分割点**

**5) 递归分裂**

**示例图: 体重(离散值)作为分裂点**

![](https://i-blog.csdnimg.cn/direct/954c655dd22b43fcb54bee7656a479af.png)

**示例图: 根据特征预测体重(** MSE**)**

![](https://i-blog.csdnimg.cn/direct/91bdc1327064469eb9ee87563beed099.png)

**随机森林**

> 通过构建多棵决策树，结合投票（分类）或平均（回归）实现预测。

**训练步骤** ：

**1) Bootstrap抽样**
：从D中有放回地抽取N个样本，形成子集![D_{t}](https://latex.csdn.net/eq?D_%7Bt%7D)​。

**2) 构建决策树**
：在![D_{t}](https://latex.csdn.net/eq?D_%7Bt%7D)上训练一棵CART（分类与回归树）树，每次分裂时仅考虑m个随机选择的特征。m=math.sqrt(总特征数)

**3) 保存模型** ：将训练好的树ht​加入森林。

**4) 预测:**

**· 多数投票法(分类)** ：每棵树对样本预测一个类别，最终选择得票最多的类别。

**· 平均值(回归)** ：所有树的预测结果取平均。

**放回抽样:** 每次从总体中随机抽取一个样本后，​**将该样本放回总体** ，确保它在后续抽取中仍有可能被再次选中。

**基尼系数公式:**

![](https://i-blog.csdnimg.cn/direct/706cb97e8df64723acbb022a776b95f0.png)

**符号含义:**

![](https://i-blog.csdnimg.cn/direct/99c402732bb942cc84996ed02446e28b.png)

**作用** ：衡量数据集的不纯度。基尼系数越小，数据越“纯净”（同一类样本占比越高）。

**基尼指数公式:**

![](https://i-blog.csdnimg.cn/direct/47c8a81c5f194b13bc30e7741eb0db84.png)

**符号含义:**![](https://i-blog.csdnimg.cn/direct/a9f00e39f5ca4e3c89e8f27a6bcb1c45.png)

**作用** ：衡量按特征 A 分裂后的整体不纯度。决策树选择基尼指数最小的特征进行分裂。

**XGBoost思想:** 在每一轮迭代中，通过拟合前序模型的预测残差（负梯度方向），并自动调整对预测不准样本的关注度，同时结合正则化防止过拟合。

**ID3、C4.5、CART算法的对比:**

![](https://i-blog.csdnimg.cn/direct/de9f7bf7170a463c94d522cada2db6bc.png)

**决策树 vs 逻辑回归:**

![](https://i-blog.csdnimg.cn/direct/eb33a0071f0b449e88bd62aab23e5c9f.png)

### 1.5 聚类算法

> 将未标记的数据划分为若干组（簇）, **组内相似性** 高, **组间差异性** 大。

**K-means算法:** 随机初始化K个中心点 → 分配数据点到最近中心 → 更新中心点 → 迭代至收敛。

​**K-means算法流程:**

![](https://i-blog.csdnimg.cn/direct/d0278f7389a74c9d845e808b52f02f4c.png)

**k-means工作示例:**

![](https://i-blog.csdnimg.cn/direct/1feb7dbe6eb54ed0bb1664cb90a9b0db.png)

![](https://i-blog.csdnimg.cn/direct/785567a5491845208fc0c7244c2548ac.png)

![](https://i-blog.csdnimg.cn/direct/d648d55cc7cd49cf9c59c457557f6d09.png)

![](https://i-blog.csdnimg.cn/direct/b4877812b6e04e5b9c33e21b64ad7758.png)

![](https://i-blog.csdnimg.cn/direct/d2ebcc41b44d43eb932c65fac0aa44e4.png)

![](https://i-blog.csdnimg.cn/direct/d90ba9e5875e41058a6ce4b8eb2724f8.png)

质心:

![](https://i-blog.csdnimg.cn/direct/67390c5cfcfc42578d4416171341c1a4.png)

![](https://i-blog.csdnimg.cn/direct/f38518cb12474f57b6d523e29be595f9.png)

**符号含义:**

![](https://i-blog.csdnimg.cn/direct/2989437f502949eb92e7d8485f3b29a2.png)

![](https://i-blog.csdnimg.cn/direct/df247988d2a841898b3be5bcc90bc791.png)

**不同初始化时的可能情况:**

![](https://i-blog.csdnimg.cn/direct/b432ff44e70145fd91de9d61aecb31bd.png)

**肘部算法:** 选取合适的K值

![](https://i-blog.csdnimg.cn/direct/41858409d5ad4871bc603286d9f57916.png)

### 1.6 异常检测

**密度评估:** 当P(x)小于某个值时, 为可疑异常, 相比较监督算法, 更容易发现从未出现过的异常

**正态分布（高斯分布）的概率密度函数**

![](https://i-blog.csdnimg.cn/direct/399d1c3aa5d74bd28c337656f085977c.png)

**推广(向量):**

![](https://i-blog.csdnimg.cn/direct/392f1b90bac443c1a4fc8bdd9dcc0a48.png)

![](https://i-blog.csdnimg.cn/direct/abf087171b7741139bb75ca698928bd3.png)

非高斯特征转化 :

![](https://i-blog.csdnimg.cn/direct/15bb6c895e2a4a30993616960b22ddf5.png)

**协调过滤:**

**回归成本函数:**

![](https://i-blog.csdnimg.cn/direct/ade1c2e157c14b008cccbdce8a7c29a2.png)

**梯度算法:**

![](https://i-blog.csdnimg.cn/direct/a226a89907d44294b88d62c803711f04.png)

**均值归一化作用:** 若无评分数据，使用全局均值 μglobal​ 作为初始预测值。

![](https://i-blog.csdnimg.cn/direct/ffc0978926234845aff766803b3bc9a2.png)

**预测值:**![](https://i-blog.csdnimg.cn/direct/f13ed7419a3940d9b48170b19ec6f16e.png)

基于内容的过滤算法:

![](https://i-blog.csdnimg.cn/direct/397cd402aaed4b94af184e129d0cc8ea.png)

![](https://i-blog.csdnimg.cn/direct/d81419e45a45492591dd853edc9069f8.png)

![](https://i-blog.csdnimg.cn/direct/5d4ad6d4c97649b1861d62e54e91239e.png)

**PCA算法:** 无监督的线性降维方法，**通过正交变换将高维数据投影到低维空间** ，保留数据中的最大**方差**
（即信息量）。以期用更少的特征（**主成分** ）解释原始数据中的大部分变异性。

**与线性回归的区别:**

![](https://i-blog.csdnimg.cn/direct/863ecf7f71614874b38f73cb2169d0a9.png)

### **1.7 强化学习**

**贝尔曼方程:**

![](https://i-blog.csdnimg.cn/direct/d6afe3c86468431187af9f02f403c49e.png)

  * **Agent（智能体）** ：决策主体，执行动作（Action）。
  * **State（状态 s）** ：环境在某一时刻的描述。
  * **Action（动作 a）** ：Agent的行为选择。
  * **Reward（奖励 R(s)）** ：环境对Agent动作的即时反馈。
  * **Value Function（价值函数）** ：衡量状态或动作的长期价值（Q(s,a)）。
  * 其中γ∈[0,1]为折扣因子

**软更新**![](https://i-blog.csdnimg.cn/direct/da02a28f9d3e4c778a4408f76e39360a.png)

## 二、深度学习

### 2.1 基本概念

**输入层** : x向量表示原始数据

**隐藏层:** 如下图**** layer1 到 layer3输出激活值(向量)。通过权重和激活函数提取抽象特征。

**输出层:** layer4, 生成最终预测结果（如分类概率）。

**神经元（节点）​** ：每层的圆圈代表一个神经元，负责接收输入信号并计算输出。

**激活函数:** 引入非线性，使网络能够拟合复杂函数。

![](https://i-blog.csdnimg.cn/direct/d85013c87d80458d906291debe850b07.png)

**前向传播示例图:**

![](https://i-blog.csdnimg.cn/direct/628ab568a8ac479da96692338a7d65d2.png)

![](https://i-blog.csdnimg.cn/direct/7c50953f90e7458497cec7df3529b281.png)

**三种激活函数:**![](https://i-blog.csdnimg.cn/direct/77f41b63c3da4a3181844dcd351714d2.png)

![](https://i-blog.csdnimg.cn/direct/156b5c9ee09c44279657e0701ebb315d.png)![](https://i-blog.csdnimg.cn/direct/3134e140678a4817bb1561c556d5f4a2.png)

**备注:** 梯度下降时sigmoid两端导函数为0, 二ReLu只有一端。

**为什么模型需要激活函数:** 使得模型非线性。神经都是线性回归则神经网络只是一个线性回归。

**反向传播:** 通过链式法则，依次计算每一层的梯度

![](https://i-blog.csdnimg.cn/direct/047696bfb9834d239cc2e3adfe9248c5.png)

**举例:**

![](https://i-blog.csdnimg.cn/direct/c58842ac17c5427b9defd362d5d556ed.png)

**梯度下降:** 利用反向传播计算的梯度，梯度下降通过以下公式更新参数

![](https://i-blog.csdnimg.cn/direct/a2c64ce140e94a8d8a0ebd342fb084c6.png)

### 2.3 多分类与多标签分类

**多分类:** 将样本分配到**唯一一个** 类别中, 如数字识别

**多标签分类:** 为样本分配**多个相关标签, 如** 图像标注（包含“山”“湖”“树”）

**多分类举例:** 输出每个类别的概率，选择最大概率对应的类别。

![](https://i-blog.csdnimg.cn/direct/38c121c59c3e4bacab3454d94c733958.png)

**损失函数:**

![](https://i-blog.csdnimg.cn/direct/df75e981f445426b98dec4c3f62a4e28.png)

![](https://i-blog.csdnimg.cn/direct/5a7ad1444bda414ea494550e24508ddf.png)

网络层

密集层

卷积层

### 2.4 模型评估

​**数据集划分** ：

**1) 训练集** ​（Training Set）：用于模型训练（通常占70%）。

**2) 验证集:** 用于调参, 学习数据中的潜在规律。(通常占15%)

**3) 测试集** ​（Test Set）：模拟“未知数据”，用于最终评估。(通常占15%)

**意义:** 若模型仅在训练集上表现好，但在测试集上差，说明模型过拟合（过度记忆训练数据细节），泛化能力弱。

**备注:** 避免测试集调参, 若根据测试集结果反复调整模型，导致模型间接拟合测试集。

**交叉验证:**

> **** 通过多次数据划分**减少** 评估结果的**方差** ,**防止** 模型因单次数据划分的**随机性** 导致评估结果不稳定。

**1) K折交叉验证:** 将数据均等分为**K个子集,** 每次用**1个子集作为验证集**
，其余K-1个作为训练集，重复K次。计算K次验证结果的平均值作为最终评估指标

**2) 留一交叉验证:** K折交叉验证的极端情况（**K=N** ，N为样本总数）适合小数据集。

**3) 分层交叉验证:**
分类任务中类别分布不均衡时，确保每折的类别比例与原始数据一致。(如数据中90%为类别A，10%为类别B，分层交叉验证会保证每折中类别A和B的比例仍为9:1。)

#### 2.4.1 偏差与方差

**分解公式** ：泛化误差 = 偏差² + 方差 + 噪声。

**偏差（Bias）​**
：指模型预测值的期望与真实值之间的差距，反映了模型对数据的拟合能力。高偏差意味着模型过于简单，无法捕捉数据中的潜在关系，导致**欠拟合**
​（Underfitting）。

**方差（Variance）​**
：指模型对训练数据中微小变化的敏感程度，反映了模型的稳定性。高方差意味着模型过于复杂，过度拟合训练数据中的噪声，导致**过拟合**
​（Overfitting）。

**高偏差(左), 高方差(右)**

![](https://i-blog.csdnimg.cn/direct/7dffaa81badb455b9d024acc926b1fc5.png)

#### 2.4.2 诊断偏差与方差

**高偏差（欠拟合）​** ：训练集和验证集误差均高。

**解决方案:**

1) 可增加模型复杂度（如使用更高阶多项式、深层神经网络）

2) 添加更多特征或改进特征工程

3) 减少正则化强度（如降低λ值）

**高方差（过拟合）​** ：训练误差低，验证误差高且差距大。表现: J(验证集)>>J(训练集)

**解决方案:**

1) 可降低模型复杂度（如减少神经网络层数、剪枝决策树）。

2) 增加训练数据量或使用数据增强。

3) 增强正则化

**多项式阶数(x轴) 与 交叉验证误差 及 训练集误差 的关系:**

![](https://i-blog.csdnimg.cn/direct/b25d8a845cbd4ed993d15b95ccdd2774.png)

**学习曲线:**

![](https://i-blog.csdnimg.cn/direct/f14f11d8c5ee45528a6c8cfef555f47f.png)

**高偏差学习曲线情况(红线, 较人类水平相比):**

![](https://i-blog.csdnimg.cn/direct/fa54db4872584bffaf80ddc16182f97f.png)

**高方差学习曲线情况(前半段):**

![](https://i-blog.csdnimg.cn/direct/83b9b3dde1f946d3a7ecff71bb424d0a.png)

**训练神经网络的一般步骤:**

![](https://i-blog.csdnimg.cn/direct/30969aa880c74796a5f21119d5c32a94.png)

**数据增强:** 在现有的训练样本上修改生成另一个训练样本

![](https://i-blog.csdnimg.cn/direct/b668908644544ce9bd514ca1989dd715.png)

**迁移学习:** 预训练

![](https://i-blog.csdnimg.cn/direct/547ac0f2977a4ce39a9bc8010692b754.png)

**两者区别:**

​**维度** ​| ​**数据增强** ​| ​**迁移学习** ​  
---|---|---  
​**核心目标** ​| 增加数据多样性，提升模型泛化能力| 复用已有知识，降低目标领域训练成本  
​**依赖条件** ​| 需要少量原始数据| 需要源领域模型或相关数据  
​**适用阶段** ​| 数据准备阶段| 模型训练阶段  
​**技术范畴** ​| 数据预处理/正则化| 模型优化/跨任务学习  
​**典型应用领域** ​| 图像、文本、语音等所有数据驱动的任务| 深度学习、跨领域任务（如医疗、金融）  
  
#### 2.4.3 精确度与召**回** 率

**混淆矩阵** ：TP（真正例）、TN（真负例）、FP（假正例）、FN（假负例）。

**1) 真正例（TP, True Positive）​** ：实际为正类，预测也为正类。

**​2) 假正例（FP, False Positive）​** ：实际为负类，预测为正类（误报）。

**​3) 真负例（TN, True Negative）​** ：实际为负类，预测也为负类。

**​4) 假负例（FN, False Negative）​** ：实际为正类，预测为负类（漏报）。

**精确率（Precision）​**
：![\\frac{TP}{TP+FP}](https://latex.csdn.net/eq?%5Cfrac%7BTP%7D%7BTP&plus;FP%7D)（预测为正的样本中实际为正的比例）。**关注预测**
的准确性

**召回率（Recall）​**
：![\\frac{TP}{TP+FN}](https://latex.csdn.net/eq?%5Cfrac%7BTP%7D%7BTP&plus;FN%7D)​（实际为正的样本中被正确预测的比例）。**关注正类**
的覆盖率

**高精率确低召回率:** 预测产品质量永远为合格(绝大部分产品都合格)

**实际场景举例** ：

  * **高精确率优先** ：垃圾邮件分类（宁可漏判也要避免误判正常邮件）
  * **高召回率优先** ：疾病检测（宁可误诊也要减少漏诊）。

**例子：癌症检测**

> 设测试集有 ​100 名患者，其中 ​10 人患癌(正类) , 90 人健康(负类)​。模型预测结果如下

  * **正确预测** ： 
    * 癌症患者：8 人（TP = 8）。
    * 健康患者：83 人（TN = 83）。
  * ​**错误预测** ： 
    * 将 7 名健康人误诊为癌症（FP = 7）。
    * 漏诊 2 名癌症患者（FN = 2）。

**混淆矩阵** ：

| 预测患癌| 预测健康  
---|---|---  
​**实际患癌** ​| TP = 8| FN = 2  
​**实际健康** ​| FP = 7| TN = 83



