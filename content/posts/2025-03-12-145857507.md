---
layout: post
title: "机器学习吴恩达"
date: 2025-03-12 21:19:20 +0800
description: "机器学习涵盖监督学习、无监督学习与强化学习。核心算法包括线性模型、树模型、集成方法、核技巧及概率模型。深度学习以神经网络为基础，涉及CNN、RNN、Transformer、激活函数及优化技术。评估指标含分类、回归与聚类，正则化手段应对过拟合，交叉验证保障泛化性，强化学习结合值函数与策略优化，形成从传统统计到深度强化学习的完整体系"
keywords: "机器学习(吴恩达)"
categories: ['人工智能']
tags: ['机器学习', '人工智能']
artid: "145857507"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145857507
    alt: "机器学习吴恩达"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145857507
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145857507
cover: https://bing.ee123.net/img/rand?artid=145857507
image: https://bing.ee123.net/img/rand?artid=145857507
img: https://bing.ee123.net/img/rand?artid=145857507
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     机器学习(吴恩达)
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     一, 机器学习
    </h2>
    <p>
     <strong>
      机器学习定义:
     </strong>
     计算机能够在没有明确的编程情况下学习
    </p>
    <p>
     <strong>
      特征:
     </strong>
     特征是描述样本的属性或变量，是模型用来学习和预测的基础。如: 房屋面积, 地理位置
    </p>
    <p>
     <strong>
      标签:
     </strong>
     监督学习中需要预测的目标变量，是模型的输出目标。如: 房屋价格
    </p>
    <p>
     <strong>
      样本:
     </strong>
     如: {面积=100㎡, 卧室=3, 位置=市中心, 价格=500万}, 数据集中的一个独立实例, 包含一组特征及对应的标签。
    </p>
    <p>
     <strong>
      样本向量形式:
     </strong>
    </p>
    <p>
     <img alt="" height="120" src="https://i-blog.csdnimg.cn/direct/f1eeb35df6594f3d93f14e9bf37469d0.png" width="867"/>
    </p>
    <p>
     <strong>
      独热编码举例:
     </strong>
    </p>
    <p>
     <img alt="" height="232" src="https://i-blog.csdnimg.cn/direct/9fc9e02d7ae84307822d731173d98dbe.png" width="1040"/>
    </p>
    <h3>
     1.1 机器学习的分类
    </h3>
    <p>
     <strong>
      1) 监督学习:
     </strong>
     根据带标签的数据训练模型，预测新样本的标签。如
     <strong>
      回归
     </strong>
     ,
     <strong>
      分类
     </strong>
     。
    </p>
    <p>
     <strong>
      回归应用:
     </strong>
     房价预测
    </p>
    <p>
     <img alt="" height="580" src="https://i-blog.csdnimg.cn/direct/baeda87745d64ecdada34332c38e680c.png" width="1191"/>
    </p>
    <p>
     <strong>
      分类算法:
     </strong>
     根据年龄和肿瘤大小判断肿瘤良/恶性
    </p>
    <p>
     <img alt="" height="326" src="https://i-blog.csdnimg.cn/direct/1b24700e3fef4bb0af54706e106a8542.png" width="447"/>
    </p>
    <p>
     <strong>
      2) 无监督学习:
     </strong>
     从未标注数据中发现潜在结构或模式。如
     <strong>
      聚类
     </strong>
     ,
     <strong>
      异常检测
     </strong>
     ,
     <strong>
      降维
     </strong>
     (大数据集压缩成更小的数据集, 并可能少地丢失信息)
    </p>
    <p>
     <strong>
      聚类:
     </strong>
     谷歌新闻示例
    </p>
    <p>
     <img alt="" height="801" src="https://i-blog.csdnimg.cn/direct/9233b92223084a4a83f4778aa7930d02.png" width="1237"/>
    </p>
    <p>
     <strong>
      3) ​半监督学习:
     </strong>
     结合少量标注数据和大量未标注数据进行训练。如医学图像分析
    </p>
    <p>
     <strong>
      4) ​强化学习:
     </strong>
     通过与环境交互学习策略，最大化累积奖励。如游戏AI, 自动驾驶
    </p>
    <h3>
     1.2 线性回归
    </h3>
    <h4>
     1.2.1 损失函数与成本函数
    </h4>
    <p>
     <strong>
      第i个数据特征:
     </strong>
    </p>
    <p>
     <img alt="" height="135" src="https://i-blog.csdnimg.cn/direct/9c31f0d992fc4bac91c5ae758678b2e3.png" width="704"/>
    </p>
    <p>
     <strong>
      损失函数（Loss Function）​
     </strong>
     ：衡量
     <span style="color:#fe2c24">
      <strong>
       单个样本
      </strong>
     </span>
     的预测值与真实值的差异。
    </p>
    <p>
     <strong>
      成本函数（Cost Function）​
     </strong>
     ：衡量
     <span style="color:#fe2c24">
      <strong>
       整个训练集
      </strong>
     </span>
     的平均损失。
    </p>
    <p>
     <strong>
      平方误差损失:
     </strong>
    </p>
    <p>
     <img alt="" height="91" src="https://i-blog.csdnimg.cn/direct/5ecb399368e74416b37709c342438c69.png" width="456"/>
    </p>
    <p>
     <strong>
      平方误差成本函数:
     </strong>
     一定是凸函数，确保只有一个全局最小值
    </p>
    <p>
     <img alt="" height="187" src="https://i-blog.csdnimg.cn/direct/1d14f5d779b34ebaae490cec33207b01.png" width="897"/>
    </p>
    <p>
     <strong>
      模型(y=wx)与成本函数示例 (左图w=-0.5、0、0.5、1时的情况):
     </strong>
    </p>
    <p>
     <img alt="" height="817" src="https://i-blog.csdnimg.cn/direct/308808d8e4194f5298ffd96e74b53192.png" width="1769"/>
    </p>
    <p>
     <strong>
      模型(y=wx+b)下的成本函数:
     </strong>
    </p>
    <p>
     <img alt="" height="845" src="https://i-blog.csdnimg.cn/direct/ea7c611a64d6450386f13481810d3724.png" width="1567"/>
    </p>
    <p>
     <strong>
      模型与J(w,b)的平面等高线:
     </strong>
    </p>
    <p>
     <img alt="" height="519" src="https://i-blog.csdnimg.cn/direct/5d33507789ce49bda2df851663748d34.png" width="1335"/>
    </p>
    <p>
     <img alt="" height="516" src="https://i-blog.csdnimg.cn/direct/f7dd3f7ce97e426bb299c0af1b88225d.png" width="1334"/>
     <img alt="" height="549" src="https://i-blog.csdnimg.cn/direct/dc91501f9e234d8b985851a9fbb727f0.png" width="1325"/>
    </p>
    <h4>
     1.2.2 学习率与梯度下降算法
    </h4>
    <p>
     <strong>
      学习率（α）​
     </strong>
     ：控制模型参数更新步长的超参数。
    </p>
    <p>
     <strong>
      学习率的取值的两种情况:
     </strong>
    </p>
    <p>
     <strong>
      1) 学习率过大
     </strong>
     ：参数更新步长过大，可能导致损失值震荡甚至发散。
    </p>
    <p>
     <strong>
      2) 学习率过小
     </strong>
     ：收敛速度极慢，可能陷入局部极小值。
    </p>
    <p>
     <strong>
      示例:
     </strong>
    </p>
    <p>
     <img alt="" height="292" src="https://i-blog.csdnimg.cn/direct/893d997c5a224a618537d0df7bd5ad72.png" width="909"/>
    </p>
    <p>
     <strong>
      梯度下降公式:
     </strong>
    </p>
    <p>
     <img alt="" height="275" src="https://i-blog.csdnimg.cn/direct/33c65ae8598349c18ad38f3f3b9f3b97.png" width="672"/>
    </p>
    <p>
     <img alt="" height="371" src="https://i-blog.csdnimg.cn/direct/78ffb5be08cf4afc8eb6505a5a7ae8b6.png" width="1001"/>
    </p>
    <p>
     <strong>
      推导过程:
     </strong>
    </p>
    <p>
     <img alt="" height="951" src="https://i-blog.csdnimg.cn/direct/8bdd29319e1641ada79cd9448f86dc65.png" width="1910"/>
    </p>
    <p>
     <strong>
      梯度算法演示:
     </strong>
    </p>
    <p>
     <img alt="" height="905" src="https://i-blog.csdnimg.cn/direct/3791f37f981245e49da5f4c6bf5849a9.png" width="1070"/>
    </p>
    <p>
     <strong>
      两个特征的多元线性回归举例:
     </strong>
    </p>
    <p>
     <img alt="" height="290" src="https://i-blog.csdnimg.cn/direct/4c20cdde0f924810b8fbb280fd94e156.png" width="1052"/>
    </p>
    <p>
     <img alt="" height="359" src="https://i-blog.csdnimg.cn/direct/b4ad887540144748a858bfc402c54328.png" width="1026"/>
    </p>
    <p>
     <img alt="" height="421" src="https://i-blog.csdnimg.cn/direct/b55cb6545ada42848c29245fb224cce0.png" width="986"/>
    </p>
    <p>
     <img alt="" height="417" src="https://i-blog.csdnimg.cn/direct/0451c075c5264c57a792567759ebc72e.png" width="1034"/>
    </p>
    <p>
     <img alt="" height="394" src="https://i-blog.csdnimg.cn/direct/f001de64b0704241afa4de6cd29fd589.png" width="1020"/>
    </p>
    <p>
     <img alt="" height="227" src="https://i-blog.csdnimg.cn/direct/15e8b40553f24553869977efea1ab368.png" width="971"/>
    </p>
    <p>
     <strong>
      批量梯度下降:
     </strong>
     每次迭代使用
     <strong>
      全部训练数据
     </strong>
     计算梯度。
    </p>
    <p>
     <strong>
      随机梯度下降:
     </strong>
     每次迭代
     <strong>
      随机选取一个样本
     </strong>
     计算梯度。
    </p>
    <p>
     <strong>
      小批量处理实现流程:
     </strong>
    </p>
    <ol>
     <li>
      <strong>
       数据分块
      </strong>
      ：将训练集随机划分为多个小批量。
     </li>
     <li>
      <strong>
       前向传播
      </strong>
      ：对当前小批量计算模型输出。
     </li>
     <li>
      <strong>
       损失计算
      </strong>
      ：根据预测值和真实标签计算损失（如交叉熵、均方误差）。
     </li>
     <li>
      <strong>
       反向传播
      </strong>
      ：计算损失对参数的梯度。
     </li>
     <li>
      <strong>
       参数更新
      </strong>
      ：使用优化算法（如SGD、Adam）更新模型参数。
     </li>
     <li>
      <strong>
       重复
      </strong>
      ：遍历所有小批量完成一个训练周期（Epoch）。
     </li>
    </ol>
    <h4>
     1.2.3 特征缩放
    </h4>
    <blockquote>
     <p>
      加速模型收敛。有如下方法
     </p>
    </blockquote>
    <p>
     <strong>
      标准化（Z-Score标准化）:
     </strong>
    </p>
    <p>
     <strong>
      <img alt="" height="210" src="https://i-blog.csdnimg.cn/direct/22838c724c9248c7bfc19671b23296cb.png" width="745"/>
     </strong>
    </p>
    <p>
     <strong>
      标准差:
     </strong>
    </p>
    <p>
     <img alt="" height="124" src="https://i-blog.csdnimg.cn/direct/65aee970bcbc442aa9e026a157bf08a2.png" width="872"/>
    </p>
    <p>
     <strong>
      归一化（Min-Max缩放）:
     </strong>
    </p>
    <p>
     <img alt="" height="210" src="https://i-blog.csdnimg.cn/direct/8ed85649e99a4d5795e261dabf097ad0.png" width="769"/>
    </p>
    <p>
     <strong>
      标准化与归一化的区别:
     </strong>
    </p>
    <p>
     <img alt="" height="184" src="https://i-blog.csdnimg.cn/direct/8464077bfea0466cb88479d5e4fea119.png" width="1035"/>
    </p>
    <p>
     <strong>
      举例 (标准化前后的数据集) :
     </strong>
    </p>
    <p>
     <img alt="" height="290" src="https://i-blog.csdnimg.cn/direct/4c20cdde0f924810b8fbb280fd94e156.png" width="1052"/>
    </p>
    <p>
     <img alt="" height="160" src="https://i-blog.csdnimg.cn/direct/d7e5ceabd67e4ca9894ab2bd224940df.png" width="666"/>
    </p>
    <h4>
     <strong>
      1.2.4 正则化
     </strong>
    </h4>
    <p>
     <strong>
      解决过拟合情况:
     </strong>
    </p>
    <p>
     1) 收集更多数据
    </p>
    <p>
     2) 仅用特征的一个子集
    </p>
    <p>
     3) 正则化
    </p>
    <p>
     <strong>
      欠拟合(高偏差), 适中, 过拟合(高方差)
     </strong>
    </p>
    <p>
     <img alt="" height="471" src="https://i-blog.csdnimg.cn/direct/c699ff6e711e43a3bc8a735d84ff8297.png" width="1796"/>
    </p>
    <p>
     <strong>
      正则化项:
     </strong>
    </p>
    <p>
     <img alt="" height="122" src="https://i-blog.csdnimg.cn/direct/89ebe14f3cc54d7e8f2effe7188f067a.png" width="602"/>
    </p>
    <p>
     <strong>
      添加正则化项后的梯度算法:
     </strong>
    </p>
    <p>
     <img alt="" height="465" src="https://i-blog.csdnimg.cn/direct/0f41019744a64c93ae6ee46b5fa49193.png" width="1274"/>
    </p>
    <p>
     <strong>
      原理:
     </strong>
     通过在损失函数中添加与模型参数相关的惩罚项，限制参数的复杂度，从而提升模型的泛化能力。 (使得
     <strong>
      W
     </strong>
     尽可能小以此使得函数趋于平滑)
    </p>
    <p>
     <strong>
      λ过大:
     </strong>
     参数被过度压缩，模型过于简单，无法捕捉数据中的有效规律。
    </p>
    <p>
     <strong>
      λ过小:
     </strong>
     正则化作用微弱，模型过度依赖训练数据中的噪声或局部特征。
    </p>
    <p>
     <strong>
      备注:
     </strong>
     只要正则化得当, 更大的神经网络总是更好的。
    </p>
    <p>
     <strong>
      例图:
     </strong>
    </p>
    <p>
     <img alt="" height="517" src="https://i-blog.csdnimg.cn/direct/48b03e241cef450794c90ccb63658fe0.png" width="1901"/>
    </p>
    <p>
     <strong>
      根据交叉验证误差找到适合的λ:
     </strong>
    </p>
    <p>
     <img alt="" height="555" src="https://i-blog.csdnimg.cn/direct/8e71fce02c4942829b911ff7795feab5.png" width="1672"/>
    </p>
    <p>
     <strong>
      λ取值与交叉验证误差及训练集误差的关系:
     </strong>
    </p>
    <p>
     <img alt="" height="666" src="https://i-blog.csdnimg.cn/direct/7d260c54a2cc4ca5a8f81539db45f01c.png" width="1427"/>
    </p>
    <p>
    </p>
    <h3>
     1.3 逻辑回归
    </h3>
    <blockquote>
     <p>
      通过线性组合特征与参数，结合Sigmoid函数将输出映射到概率区间（0-1），用于解决
      <strong>
       分类问题
      </strong>
      ​（尤其是二分类）。
     </p>
    </blockquote>
    <p>
     <strong>
      Sigmoid函数模型:
     </strong>
    </p>
    <p>
     <img alt="" height="157" src="https://i-blog.csdnimg.cn/direct/daef4e55d0ae4a8f900d3e19688ddd54.png" width="662"/>
    </p>
    <p>
     <strong>
      图形:
     </strong>
    </p>
    <p>
     <img alt="" height="495" src="https://i-blog.csdnimg.cn/direct/e384884f41254ded882cfbaf78d0876e.png" width="827"/>
    </p>
    <p>
     <strong>
      对数损失函数（交叉熵损失）:
     </strong>
    </p>
    <p>
     <img alt="" height="389" src="https://i-blog.csdnimg.cn/direct/e3d260b0ead043f88b37e222ccfa1a76.png" width="1799"/>
    </p>
    <p>
     <strong>
      对应图形:
     </strong>
    </p>
    <p>
     <img alt="" height="291" src="https://i-blog.csdnimg.cn/direct/cf02898b05914247b2f6d8a757076281.png" width="1000"/>
    </p>
    <p>
     <img alt="" height="174" src="https://i-blog.csdnimg.cn/direct/8586a84d2d5648dca7fb55a1c553c633.png" width="1021"/>
    </p>
    <p>
     <strong>
      为什么不使用均方误差(MSE)作为损失函数:
     </strong>
     当预测值接近 0 或 1 时, 梯度接近于0, 权重几乎无法更新。
    </p>
    <p>
     <img alt="" height="54" src="https://i-blog.csdnimg.cn/direct/9a517d2946f74715ab3b0aeeae7ba4c7.png" width="450"/>
    </p>
    <p>
     <strong>
      对应成本函数:
     </strong>
    </p>
    <p>
     <img alt="" height="410" src="https://i-blog.csdnimg.cn/direct/4fabb0125c4547a29144aab403d4c925.png" width="1686"/>
    </p>
    <p>
     <img alt="" height="245" src="https://i-blog.csdnimg.cn/direct/7d8c35927cfb46feb3ad4e38f2e55888.png" width="912"/>
    </p>
    <p>
     <img alt="" height="251" src="https://i-blog.csdnimg.cn/direct/bbe57a4d02574a50b40139b223a5fbc9.png" width="894"/>
    </p>
    <p>
     <strong>
      为什么选择对数损失函数:
     </strong>
    </p>
    <p>
     <strong>
      1) 概率视角：
     </strong>
     最大似然估计（MLE）
    </p>
    <p>
     <strong>
      2) 优化视角：
     </strong>
     凸性
    </p>
    <p>
     <strong>
      梯度下降算法:
     </strong>
    </p>
    <p>
     <img alt="" height="339" src="https://i-blog.csdnimg.cn/direct/6fefb484f6994f9db7b347f79b9b738f.png" width="1115"/>
    </p>
    <p>
     <strong>
      与线性回归梯度算法的区别: 模型定义不同:
     </strong>
    </p>
    <p>
     <img alt="" height="240" src="https://i-blog.csdnimg.cn/direct/403ae63f2da2498e9a89a38cb3ae403d.png" width="1039"/>
    </p>
    <p>
     <strong>
      线性回归与逻辑回归区别:
     </strong>
    </p>
    <p>
     <img alt="" height="362" src="https://i-blog.csdnimg.cn/direct/b7bebd0576e94c89820d5321e566f793.png" width="1007"/>
    </p>
    <h3 style="background-color:transparent">
     1.4 决策树
    </h3>
    <blockquote>
     <p>
      一种树形结构的监督学习模型，通过选择最优特征对数据进行分割，目标是使子节点的数据尽可能“纯净”（同类样本集中）。
     </p>
    </blockquote>
    <p>
     <strong>
      递归分裂过程:
     </strong>
     ​
    </p>
    <ol>
     <li>
      从根节点开始，计算所有特征的分裂指标（如信息增益）。
     </li>
     <li>
      选择最优特征作为当前节点的分裂特征。
     </li>
     <li>
      根据特征的取值将数据集划分为子集，生成子节点。
     </li>
     <li>
      对每个子节点递归执行步骤1-3，直到满足停止条件。
     </li>
    </ol>
    <p>
     <strong>
      停止条件:
     </strong>
     ​
    </p>
    <ul>
     <li>
      节点样本数小于预设阈值。
     </li>
     <li>
      所有样本属于同一类别。
     </li>
     <li>
      特征已用完或分裂后纯度提升不显著。
     </li>
    </ul>
    <p>
     ​
     <strong>
      预剪枝:
     </strong>
     在树生长过程中提前终止分裂。如设置最大深度
    </p>
    <p>
     <strong>
      信息熵
     </strong>
     ：度量数据集的混乱程度。值
     <strong>
      越小
     </strong>
     分类
     <strong>
      越明确
     </strong>
     。
    </p>
    <p>
     <img alt="" height="109" src="https://i-blog.csdnimg.cn/direct/8a4c2ad61ccb4573afb0626051525051.png" width="763"/>
    </p>
    <p>
    </p>
    <p>
     <strong>
      二分类示例图:
     </strong>
    </p>
    <p>
     <img alt="" height="565" src="https://i-blog.csdnimg.cn/direct/c5f9a67adf8f41aa996804ebd8cce605.png" width="1734"/>
    </p>
    <p>
     <strong>
      信息增益（IG）
     </strong>
     ：特征分裂后熵的减少量( 父节点熵 - 子节点加权平均熵)。值
     <strong>
      越大
     </strong>
     特征越重要。
    </p>
    <p>
    </p>
    <p>
     <strong>
      多分类推广:
     </strong>
    </p>
    <p>
     <img alt="" height="97" src="https://i-blog.csdnimg.cn/direct/59d3bd2d859e4cfc9ec7b8c9992f8948.png" width="902"/>
    </p>
    <p>
     <strong>
      符号含义:
     </strong>
    </p>
    <p>
     <img alt="" height="350" src="https://i-blog.csdnimg.cn/direct/4668c44f26224049a0e606ce39e7b905.png" width="885"/>
    </p>
    <p>
     <strong>
      举例:
     </strong>
    </p>
    <p>
     <img alt="" height="675" src="https://i-blog.csdnimg.cn/direct/16bd2d492c6c408dbc7174d10616387c.png" width="1592"/>
    </p>
    <p>
     <strong>
      二分类分裂决策举例:
     </strong>
    </p>
    <p>
     <img alt="" height="741" src="https://i-blog.csdnimg.cn/direct/c4dde511104d4471a1986a47c9ccb811.png" width="1824"/>
    </p>
    <p>
     <strong>
      信息增益率（IGR）
     </strong>
     : 在信息增益的基础上，对特征本身的分布熵进行标准化。
    </p>
    <p>
     <strong>
      公式:
     </strong>
    </p>
    <p>
     <img alt="" height="107" src="https://i-blog.csdnimg.cn/direct/ffb9b115fefa41a4a0d75df7c8c90b45.png" width="920"/>
    </p>
    <p>
     <strong>
      基尼系数:
     </strong>
     另一种不纯度度量, 可视为熵的近似, 但计算更高效
    </p>
    <p>
     <strong>
      公式:
     </strong>
    </p>
    <p>
     <img alt="" height="105" src="https://i-blog.csdnimg.cn/direct/b0591b03d93843f7a135136a2e5beaf5.png" width="775"/>
    </p>
    <p>
     <strong>
      三种经典决策树算法:
     </strong>
    </p>
    <p>
     <img alt="" height="449" src="https://i-blog.csdnimg.cn/direct/cd75252d77b248caa8e2b6a3d637bcff.png" width="1189"/>
    </p>
    <p>
     <strong>
      决策树处理处理连续值特征:
     </strong>
    </p>
    <p>
     <strong>
      1)
     </strong>
     ​
     <strong>
      特征排序:
     </strong>
     从小到大排序
    </p>
    <p>
     <strong>
      2) 候选分割点生成
     </strong>
     ：相邻值的中间点作为候选分割点。
    </p>
    <p>
     <strong>
      3) ​计算分裂指标:
     </strong>
     计算分裂后的信息增益（分类）或均方误差（回归）。
    </p>
    <p>
     <strong>
      4) 选择最优分割点
     </strong>
    </p>
    <p>
     <strong>
      5) 递归分裂
     </strong>
    </p>
    <p>
     <strong>
      示例图: 体重(离散值)作为分裂点
     </strong>
    </p>
    <p>
     <img alt="" height="756" src="https://i-blog.csdnimg.cn/direct/954c655dd22b43fcb54bee7656a479af.png" width="1720"/>
    </p>
    <p>
    </p>
    <p>
     <strong>
      示例图: 根据特征预测体重(
     </strong>
     MSE
     <strong>
      )
     </strong>
    </p>
    <p>
     <img alt="" height="782" src="https://i-blog.csdnimg.cn/direct/91bdc1327064469eb9ee87563beed099.png" width="1641"/>
    </p>
    <p>
     <strong>
      随机森林
     </strong>
    </p>
    <blockquote>
     <p>
      通过构建多棵决策树，结合投票（分类）或平均（回归）实现预测。
     </p>
    </blockquote>
    <p>
     <strong>
      训练步骤
     </strong>
     ：
    </p>
    <p>
     <strong>
      1) Bootstrap抽样
     </strong>
     ：从D中有放回地抽取N个样本，形成子集
     <img alt="D_{t}" class="mathcode" src="https://latex.csdn.net/eq?D_%7Bt%7D"/>
     ​。
    </p>
    <p>
     <strong>
      2) 构建决策树
     </strong>
     ：在
     <img alt="D_{t}" class="mathcode" src="https://latex.csdn.net/eq?D_%7Bt%7D"/>
     上训练一棵CART（分类与回归树）树，每次分裂时仅考虑m个随机选择的特征。m=math.sqrt(总特征数)
    </p>
    <p>
     <strong>
      3) 保存模型
     </strong>
     ：将训练好的树ht​加入森林。
    </p>
    <p>
     <strong>
      4) 预测:
     </strong>
    </p>
    <p>
     <strong>
      · 多数投票法(分类)
     </strong>
     ：每棵树对样本预测一个类别，最终选择得票最多的类别。
    </p>
    <p>
     <strong>
      · 平均值(回归)
     </strong>
     ：所有树的预测结果取平均。
    </p>
    <p>
     <strong>
      放回抽样:
     </strong>
     每次从总体中随机抽取一个样本后，​
     <strong>
      将该样本放回总体
     </strong>
     ，确保它在后续抽取中仍有可能被再次选中。
    </p>
    <p>
     <strong>
      基尼系数公式:
     </strong>
    </p>
    <p>
     <img alt="" height="52" src="https://i-blog.csdnimg.cn/direct/706cb97e8df64723acbb022a776b95f0.png" width="794"/>
    </p>
    <p>
     <strong>
      符号含义:
     </strong>
    </p>
    <p>
     <img alt="" height="120" src="https://i-blog.csdnimg.cn/direct/99c402732bb942cc84996ed02446e28b.png" width="687"/>
    </p>
    <p>
     <strong>
      作用
     </strong>
     ：衡量数据集的不纯度。基尼系数越小，数据越“纯净”（同一类样本占比越高）。
    </p>
    <p>
     <strong>
      基尼指数公式:
     </strong>
    </p>
    <p>
     <img alt="" height="56" src="https://i-blog.csdnimg.cn/direct/47c8a81c5f194b13bc30e7741eb0db84.png" width="920"/>
    </p>
    <p>
     <strong>
      符号含义:
     </strong>
     <img alt="" height="192" src="https://i-blog.csdnimg.cn/direct/a9f00e39f5ca4e3c89e8f27a6bcb1c45.png" width="685"/>
    </p>
    <p>
     <strong>
      作用
     </strong>
     ：衡量按特征 A 分裂后的整体不纯度。决策树选择基尼指数最小的特征进行分裂。
    </p>
    <p>
     <strong>
      XGBoost思想:
     </strong>
     在每一轮迭代中，通过拟合前序模型的预测残差（负梯度方向），并自动调整对预测不准样本的关注度，同时结合正则化防止过拟合。
    </p>
    <p>
     <strong>
      ID3、C4.5、CART算法的对比:
     </strong>
    </p>
    <p>
     <img alt="" height="446" src="https://i-blog.csdnimg.cn/direct/de9f7bf7170a463c94d522cada2db6bc.png" width="1212"/>
    </p>
    <p>
     <strong>
      决策树 vs 逻辑回归:
     </strong>
    </p>
    <p>
     <img alt="" height="494" src="https://i-blog.csdnimg.cn/direct/eb33a0071f0b449e88bd62aab23e5c9f.png" width="1192"/>
    </p>
    <h3 style="background-color:transparent">
     1.5 聚类算法
    </h3>
    <blockquote>
     <p>
      将未标记的数据划分为若干组（簇）,
      <strong>
       组内相似性
      </strong>
      高,
      <strong>
       组间差异性
      </strong>
      大。
     </p>
    </blockquote>
    <p>
     <strong>
      K-means算法:
     </strong>
     随机初始化K个中心点 → 分配数据点到最近中心 → 更新中心点 → 迭代至收敛。
    </p>
    <p>
     ​
     <strong>
      K-means算法流程:
     </strong>
    </p>
    <p>
     <img alt="" height="187" src="https://i-blog.csdnimg.cn/direct/d0278f7389a74c9d845e808b52f02f4c.png" width="866"/>
    </p>
    <p>
     <strong>
      k-means工作示例:
     </strong>
    </p>
    <p>
     <img alt="" height="909" src="https://i-blog.csdnimg.cn/direct/1feb7dbe6eb54ed0bb1664cb90a9b0db.png" width="1416"/>
    </p>
    <p>
     <img alt="" height="929" src="https://i-blog.csdnimg.cn/direct/785567a5491845208fc0c7244c2548ac.png" width="1237"/>
    </p>
    <p>
     <img alt="" height="915" src="https://i-blog.csdnimg.cn/direct/d648d55cc7cd49cf9c59c457557f6d09.png" width="1312"/>
    </p>
    <p>
     <img alt="" height="934" src="https://i-blog.csdnimg.cn/direct/b4877812b6e04e5b9c33e21b64ad7758.png" width="1199"/>
    </p>
    <p>
     <img alt="" height="931" src="https://i-blog.csdnimg.cn/direct/d2ebcc41b44d43eb932c65fac0aa44e4.png" width="1199"/>
    </p>
    <p>
     <img alt="" height="924" src="https://i-blog.csdnimg.cn/direct/d90ba9e5875e41058a6ce4b8eb2724f8.png" width="1197"/>
    </p>
    <p>
     质心:
    </p>
    <p>
     <img alt="" height="131" src="https://i-blog.csdnimg.cn/direct/67390c5cfcfc42578d4416171341c1a4.png" width="1079"/>
    </p>
    <p>
     <img alt="" height="274" src="https://i-blog.csdnimg.cn/direct/f38518cb12474f57b6d523e29be595f9.png" width="1702"/>
    </p>
    <p>
     <strong>
      符号含义:
     </strong>
    </p>
    <p>
     <img alt="" height="511" src="https://i-blog.csdnimg.cn/direct/2989437f502949eb92e7d8485f3b29a2.png" width="996"/>
    </p>
    <p>
     <img alt="" height="295" src="https://i-blog.csdnimg.cn/direct/df247988d2a841898b3be5bcc90bc791.png" width="900"/>
    </p>
    <p>
     <strong>
      不同初始化时的可能情况:
     </strong>
    </p>
    <p>
     <img alt="" height="429" src="https://i-blog.csdnimg.cn/direct/b432ff44e70145fd91de9d61aecb31bd.png" width="1110"/>
    </p>
    <p>
     <strong>
      肘部算法:
     </strong>
     选取合适的K值
    </p>
    <p>
     <img alt="" height="545" src="https://i-blog.csdnimg.cn/direct/41858409d5ad4871bc603286d9f57916.png" width="936"/>
    </p>
    <h3>
     1.6 异常检测
    </h3>
    <p>
     <strong>
      密度评估:
     </strong>
     当P(x)小于某个值时, 为可疑异常, 相比较监督算法, 更容易发现从未出现过的异常
    </p>
    <p>
     <strong>
      正态分布（高斯分布）的概率密度函数
     </strong>
    </p>
    <p>
     <img alt="" height="185" src="https://i-blog.csdnimg.cn/direct/399d1c3aa5d74bd28c337656f085977c.png" width="680"/>
    </p>
    <p>
     <strong>
      推广(向量):
     </strong>
    </p>
    <p>
     <img alt="" height="209" src="https://i-blog.csdnimg.cn/direct/392f1b90bac443c1a4fc8bdd9dcc0a48.png" width="1146"/>
    </p>
    <p>
     <img alt="" height="820" src="https://i-blog.csdnimg.cn/direct/abf087171b7741139bb75ca698928bd3.png" width="1495"/>
    </p>
    <p>
     非高斯特征转化 :
    </p>
    <p>
     <img alt="" height="396" src="https://i-blog.csdnimg.cn/direct/15bb6c895e2a4a30993616960b22ddf5.png" width="1357"/>
    </p>
    <p>
     <strong>
      协调过滤:
     </strong>
    </p>
    <p>
     <strong>
      回归成本函数:
     </strong>
    </p>
    <p>
     <img alt="" height="199" src="https://i-blog.csdnimg.cn/direct/ade1c2e157c14b008cccbdce8a7c29a2.png" width="1522"/>
    </p>
    <p>
     <strong>
      梯度算法:
     </strong>
    </p>
    <p>
     <img alt="" height="346" src="https://i-blog.csdnimg.cn/direct/a226a89907d44294b88d62c803711f04.png" width="1086"/>
    </p>
    <p>
    </p>
    <p>
     <strong>
      均值归一化作用:
     </strong>
     若无评分数据，使用全局均值 μglobal​ 作为初始预测值。
    </p>
    <p>
     <img alt="" height="405" src="https://i-blog.csdnimg.cn/direct/ffc0978926234845aff766803b3bc9a2.png" width="1680"/>
    </p>
    <p>
     <strong>
      预测值:
     </strong>
     <img alt="" height="116" src="https://i-blog.csdnimg.cn/direct/f13ed7419a3940d9b48170b19ec6f16e.png" width="1179"/>
    </p>
    <p>
     基于内容的过滤算法:
    </p>
    <p>
     <img alt="" height="612" src="https://i-blog.csdnimg.cn/direct/397cd402aaed4b94af184e129d0cc8ea.png" width="1235"/>
    </p>
    <p>
     <img alt="" height="215" src="https://i-blog.csdnimg.cn/direct/d81419e45a45492591dd853edc9069f8.png" width="1542"/>
    </p>
    <p>
     <img alt="" height="181" src="https://i-blog.csdnimg.cn/direct/5d4ad6d4c97649b1861d62e54e91239e.png" width="1586"/>
    </p>
    <p>
     <strong>
      PCA算法:
     </strong>
     无监督的线性降维方法，
     <strong>
      通过正交变换将高维数据投影到低维空间
     </strong>
     ，保留数据中的最大
     <strong>
      方差
     </strong>
     （即信息量）。以期用更少的特征（
     <strong>
      主成分
     </strong>
     ）解释原始数据中的大部分变异性。
    </p>
    <p>
     <strong>
      与线性回归的区别:
     </strong>
    </p>
    <p>
     <img alt="" height="666" src="https://i-blog.csdnimg.cn/direct/863ecf7f71614874b38f73cb2169d0a9.png" width="1919"/>
    </p>
    <h3>
     <strong>
      1.7 强化学习
     </strong>
    </h3>
    <p>
     <strong>
      贝尔曼方程:
     </strong>
    </p>
    <p>
     <img alt="" height="150" src="https://i-blog.csdnimg.cn/direct/d6afe3c86468431187af9f02f403c49e.png" width="1920"/>
    </p>
    <ul>
     <li>
      <strong>
       Agent（智能体）
      </strong>
      ：决策主体，执行动作（Action）。
     </li>
     <li>
      <strong>
       State（状态 s）
      </strong>
      ：环境在某一时刻的描述。
     </li>
     <li>
      <strong>
       Action（动作 a）
      </strong>
      ：Agent的行为选择。
     </li>
     <li>
      <strong>
       Reward（奖励 R(s)）
      </strong>
      ：环境对Agent动作的即时反馈。
     </li>
     <li>
      <strong>
       Value Function（价值函数）
      </strong>
      ：衡量状态或动作的长期价值（Q(s,a)）。
     </li>
     <li>
      其中γ∈[0,1]为折扣因子
     </li>
    </ul>
    <p>
     <strong>
      软更新
     </strong>
     <img alt="" height="480" src="https://i-blog.csdnimg.cn/direct/da02a28f9d3e4c778a4408f76e39360a.png" width="1590"/>
    </p>
    <ol>
    </ol>
    <h2>
     二、深度学习
    </h2>
    <h3>
     2.1 基本概念
    </h3>
    <p>
     <strong>
      输入层
     </strong>
     : x向量表示原始数据
    </p>
    <p>
     <strong>
      隐藏层:
     </strong>
     如下图
     <strong>
     </strong>
     layer1 到 layer3输出激活值(向量)。通过权重和激活函数提取抽象特征。
    </p>
    <p>
     <strong>
      输出层:
     </strong>
     layer4, 生成最终预测结果（如分类概率）。
    </p>
    <p>
     <strong>
      神经元（节点）​
     </strong>
     ：每层的圆圈代表一个神经元，负责接收输入信号并计算输出。
    </p>
    <p>
     <strong>
      激活函数:
     </strong>
     引入非线性，使网络能够拟合复杂函数。
    </p>
    <p>
     <img alt="" height="129" src="https://i-blog.csdnimg.cn/direct/d85013c87d80458d906291debe850b07.png" width="817"/>
    </p>
    <p>
     <strong>
      前向传播示例图:
     </strong>
    </p>
    <p>
     <img alt="" height="632" src="https://i-blog.csdnimg.cn/direct/628ab568a8ac479da96692338a7d65d2.png" width="1887"/>
    </p>
    <p>
     <img alt="" height="311" src="https://i-blog.csdnimg.cn/direct/7c50953f90e7458497cec7df3529b281.png" width="861"/>
    </p>
    <p>
     <strong>
      三种激活函数:
     </strong>
     <img alt="" height="757" src="https://i-blog.csdnimg.cn/direct/77f41b63c3da4a3181844dcd351714d2.png" width="1915"/>
    </p>
    <p>
     <img alt="" height="244" src="https://i-blog.csdnimg.cn/direct/156b5c9ee09c44279657e0701ebb315d.png" width="1191"/>
     <img alt="" height="242" src="https://i-blog.csdnimg.cn/direct/3134e140678a4817bb1561c556d5f4a2.png" width="1191"/>
    </p>
    <p>
     <strong>
      备注:
     </strong>
     梯度下降时sigmoid两端导函数为0, 二ReLu只有一端。
    </p>
    <p>
     <strong>
      为什么模型需要激活函数:
     </strong>
     使得模型非线性。神经都是线性回归则神经网络只是一个线性回归。
    </p>
    <p>
     <strong>
      反向传播:
     </strong>
     通过链式法则，依次计算每一层的梯度
    </p>
    <p>
     <img alt="" height="277" src="https://i-blog.csdnimg.cn/direct/047696bfb9834d239cc2e3adfe9248c5.png" width="810"/>
    </p>
    <p>
     <strong>
      举例:
     </strong>
    </p>
    <p>
     <img alt="" height="806" src="https://i-blog.csdnimg.cn/direct/c58842ac17c5427b9defd362d5d556ed.png" width="1905"/>
    </p>
    <p>
     <strong>
      梯度下降:
     </strong>
     利用反向传播计算的梯度，梯度下降通过以下公式更新参数
    </p>
    <p>
     <img alt="" height="195" src="https://i-blog.csdnimg.cn/direct/a2c64ce140e94a8d8a0ebd342fb084c6.png" width="912"/>
    </p>
    <h3>
     2.3 多分类与多标签分类
    </h3>
    <p>
     <strong>
      多分类:
     </strong>
     将样本分配到
     <strong>
      唯一一个
     </strong>
     类别中, 如数字识别
    </p>
    <p>
     <strong>
      多标签分类:
     </strong>
     为样本分配
     <strong>
      多个相关标签, 如
     </strong>
     图像标注（包含“山”“湖”“树”）
    </p>
    <p>
     <strong>
      多分类举例:
     </strong>
     输出每个类别的概率，选择最大概率对应的类别。
    </p>
    <p>
     <img alt="" height="851" src="https://i-blog.csdnimg.cn/direct/38c121c59c3e4bacab3454d94c733958.png" width="1047"/>
    </p>
    <p>
     <strong>
      损失函数:
     </strong>
    </p>
    <p>
     <img alt="" height="121" src="https://i-blog.csdnimg.cn/direct/df75e981f445426b98dec4c3f62a4e28.png" width="674"/>
    </p>
    <p>
     <img alt="" height="490" src="https://i-blog.csdnimg.cn/direct/5a7ad1444bda414ea494550e24508ddf.png" width="966"/>
    </p>
    <p>
     网络层
    </p>
    <p>
     密集层
    </p>
    <p>
     卷积层
    </p>
    <p>
    </p>
    <h3>
     2.4 模型评估
    </h3>
    <p>
     ​
     <strong>
      数据集划分
     </strong>
     ：
    </p>
    <p>
     <strong>
      1) 训练集
     </strong>
     ​（Training Set）：用于模型训练（通常占70%）。
    </p>
    <p>
     <strong>
      2) 验证集:
     </strong>
     用于调参, 学习数据中的潜在规律。(通常占15%)
    </p>
    <p>
     <strong>
      3) 测试集
     </strong>
     ​（Test Set）：模拟“未知数据”，用于最终评估。(通常占15%)
    </p>
    <p>
     <strong>
      意义:
     </strong>
     若模型仅在训练集上表现好，但在测试集上差，说明模型过拟合（过度记忆训练数据细节），泛化能力弱。
    </p>
    <p>
     <strong>
      备注:
     </strong>
     避免测试集调参, 若根据测试集结果反复调整模型，导致模型间接拟合测试集。
    </p>
    <p>
     <strong>
      交叉验证:
     </strong>
    </p>
    <blockquote>
     <p>
      <strong>
      </strong>
      通过多次数据划分
      <strong>
       减少
      </strong>
      评估结果的
      <strong>
       方差
      </strong>
      ,
      <strong>
       防止
      </strong>
      模型因单次数据划分的
      <strong>
       随机性
      </strong>
      导致评估结果不稳定。
     </p>
    </blockquote>
    <p>
     <strong>
      1) K折交叉验证:
     </strong>
     将数据均等分为
     <strong>
      K个子集,
     </strong>
     每次用
     <strong>
      1个子集作为验证集
     </strong>
     ，其余K-1个作为训练集，重复K次。计算K次验证结果的平均值作为最终评估指标
    </p>
    <p>
     <strong>
      2) 留一交叉验证:
     </strong>
     K折交叉验证的极端情况（
     <strong>
      K=N
     </strong>
     ，N为样本总数）适合小数据集。
    </p>
    <p>
     <strong>
      3) 分层交叉验证:
     </strong>
     分类任务中类别分布不均衡时，确保每折的类别比例与原始数据一致。(如数据中90%为类别A，10%为类别B，分层交叉验证会保证每折中类别A和B的比例仍为9:1。)
    </p>
    <h4>
     2.4.1 偏差与方差
    </h4>
    <p>
     <strong>
      分解公式
     </strong>
     ：泛化误差 = 偏差² + 方差 + 噪声。
    </p>
    <p>
     <strong>
      偏差（Bias）​
     </strong>
     ：指模型预测值的期望与真实值之间的差距，反映了模型对数据的拟合能力。高偏差意味着模型过于简单，无法捕捉数据中的潜在关系，导致
     <strong>
      欠拟合
     </strong>
     ​（Underfitting）。
    </p>
    <p>
     <strong>
      方差（Variance）​
     </strong>
     ：指模型对训练数据中微小变化的敏感程度，反映了模型的稳定性。高方差意味着模型过于复杂，过度拟合训练数据中的噪声，导致
     <strong>
      过拟合
     </strong>
     ​（Overfitting）。
    </p>
    <p>
     <strong>
      高偏差(左), 高方差(右)
     </strong>
    </p>
    <p>
     <img alt="" height="499" src="https://i-blog.csdnimg.cn/direct/7dffaa81badb455b9d024acc926b1fc5.png" width="1799"/>
    </p>
    <h4>
     2.4.2 诊断偏差与方差
    </h4>
    <p>
     <strong>
      高偏差（欠拟合）​
     </strong>
     ：训练集和验证集误差均高。
    </p>
    <p>
     <strong>
      解决方案:
     </strong>
    </p>
    <p>
     1) 可增加模型复杂度（如使用更高阶多项式、深层神经网络）
    </p>
    <p>
     2) 添加更多特征或改进特征工程
    </p>
    <p>
     3) 减少正则化强度（如降低λ值）
    </p>
    <p>
     <strong>
      高方差（过拟合）​
     </strong>
     ：训练误差低，验证误差高且差距大。表现: J(验证集)&gt;&gt;J(训练集)
    </p>
    <p>
     <strong>
      解决方案:
     </strong>
    </p>
    <p>
     1) 可降低模型复杂度（如减少神经网络层数、剪枝决策树）。
    </p>
    <p>
     2) 增加训练数据量或使用数据增强。
    </p>
    <p>
     3) 增强正则化
    </p>
    <p>
     <strong>
      多项式阶数(x轴) 与 交叉验证误差 及 训练集误差 的关系:
     </strong>
    </p>
    <p>
     <img alt="" height="700" src="https://i-blog.csdnimg.cn/direct/b25d8a845cbd4ed993d15b95ccdd2774.png" width="1642"/>
    </p>
    <p>
     <strong>
      学习曲线:
     </strong>
    </p>
    <p>
     <img alt="" height="596" src="https://i-blog.csdnimg.cn/direct/f14f11d8c5ee45528a6c8cfef555f47f.png" width="1024"/>
    </p>
    <p>
     <strong>
      高偏差学习曲线情况(红线, 较人类水平相比):
     </strong>
    </p>
    <p>
     <img alt="" height="556" src="https://i-blog.csdnimg.cn/direct/fa54db4872584bffaf80ddc16182f97f.png" width="1736"/>
    </p>
    <p>
     <strong>
      高方差学习曲线情况(前半段):
     </strong>
    </p>
    <p>
     <img alt="" height="542" src="https://i-blog.csdnimg.cn/direct/83b9b3dde1f946d3a7ecff71bb424d0a.png" width="1721"/>
    </p>
    <p>
     <strong>
      训练神经网络的一般步骤:
     </strong>
    </p>
    <p>
     <img alt="" height="686" src="https://i-blog.csdnimg.cn/direct/30969aa880c74796a5f21119d5c32a94.png" width="1606"/>
    </p>
    <p>
     <strong>
      数据增强:
     </strong>
     在现有的训练样本上修改生成另一个训练样本
    </p>
    <p>
     <img alt="" height="796" src="https://i-blog.csdnimg.cn/direct/b668908644544ce9bd514ca1989dd715.png" width="1490"/>
    </p>
    <p>
     <strong>
      迁移学习:
     </strong>
     预训练
    </p>
    <p>
     <img alt="" height="721" src="https://i-blog.csdnimg.cn/direct/547ac0f2977a4ce39a9bc8010692b754.png" width="1295"/>
    </p>
    <p>
     <strong>
      两者区别:
     </strong>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        ​
        <strong>
         维度
        </strong>
        ​
       </th>
       <th>
        ​
        <strong>
         数据增强
        </strong>
        ​
       </th>
       <th>
        ​
        <strong>
         迁移学习
        </strong>
        ​
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        ​
        <strong>
         核心目标
        </strong>
        ​
       </td>
       <td>
        增加数据多样性，提升模型泛化能力
       </td>
       <td>
        复用已有知识，降低目标领域训练成本
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         依赖条件
        </strong>
        ​
       </td>
       <td>
        需要少量原始数据
       </td>
       <td>
        需要源领域模型或相关数据
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         适用阶段
        </strong>
        ​
       </td>
       <td>
        数据准备阶段
       </td>
       <td>
        模型训练阶段
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         技术范畴
        </strong>
        ​
       </td>
       <td>
        数据预处理/正则化
       </td>
       <td>
        模型优化/跨任务学习
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         典型应用领域
        </strong>
        ​
       </td>
       <td>
        图像、文本、语音等所有数据驱动的任务
       </td>
       <td>
        深度学习、跨领域任务（如医疗、金融）
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     2.4.3 精确度与召
     <strong>
      回
     </strong>
     率
    </h4>
    <p>
     <strong>
      混淆矩阵
     </strong>
     ：TP（真正例）、TN（真负例）、FP（假正例）、FN（假负例）。
    </p>
    <p>
     <strong>
      1) 真正例（TP, True Positive）​
     </strong>
     ：实际为正类，预测也为正类。
    </p>
    <p>
     <strong>
      ​2) 假正例（FP, False Positive）​
     </strong>
     ：实际为负类，预测为正类（误报）。
    </p>
    <p>
     <strong>
      ​3) 真负例（TN, True Negative）​
     </strong>
     ：实际为负类，预测也为负类。
    </p>
    <p>
     <strong>
      ​4) 假负例（FN, False Negative）​
     </strong>
     ：实际为正类，预测为负类（漏报）。
    </p>
    <p>
     <strong>
      精确率（Precision）​
     </strong>
     ：
     <img alt="\frac{TP}{TP+FP}" class="mathcode" src="https://latex.csdn.net/eq?%5Cfrac%7BTP%7D%7BTP&amp;plus;FP%7D"/>
     （预测为正的样本中实际为正的比例）。
     <strong>
      关注预测
     </strong>
     的准确性
    </p>
    <p>
     <strong>
      召回率（Recall）​
     </strong>
     ：
     <img alt="\frac{TP}{TP+FN}" class="mathcode" src="https://latex.csdn.net/eq?%5Cfrac%7BTP%7D%7BTP&amp;plus;FN%7D"/>
     ​（实际为正的样本中被正确预测的比例）。
     <strong>
      关注正类
     </strong>
     的覆盖率
    </p>
    <p>
     <strong>
      高精率确低召回率:
     </strong>
     预测产品质量永远为合格(绝大部分产品都合格)
    </p>
    <p>
     <strong>
      实际场景举例
     </strong>
     ：
    </p>
    <ul>
     <li>
      <strong>
       高精确率优先
      </strong>
      ：垃圾邮件分类（宁可漏判也要避免误判正常邮件）
     </li>
     <li>
      <strong>
       高召回率优先
      </strong>
      ：疾病检测（宁可误诊也要减少漏诊）。
     </li>
    </ul>
    <p>
     <strong>
      例子：癌症检测
     </strong>
    </p>
    <blockquote>
     <p>
      设测试集有 ​100 名患者，其中 ​10 人患癌(正类) , 90 人健康(负类)​。模型预测结果如下
     </p>
    </blockquote>
    <ul>
     <li>
      <strong>
       正确预测
      </strong>
      ：
      <ul>
       <li>
        癌症患者：8 人（TP = 8）。
       </li>
       <li>
        健康患者：83 人（TN = 83）。
       </li>
      </ul>
     </li>
     <li>
      ​
      <strong>
       错误预测
      </strong>
      ：
      <ul>
       <li>
        将 7 名健康人误诊为癌症（FP = 7）。
       </li>
       <li>
        漏诊 2 名癌症患者（FN = 2）。
       </li>
      </ul>
     </li>
    </ul>
    <p>
     <strong>
      混淆矩阵
     </strong>
     ：
    </p>
    <table>
     <thead>
      <tr>
       <th>
       </th>
       <th>
        预测患癌
       </th>
       <th>
        预测健康
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        ​
        <strong>
         实际患癌
        </strong>
        ​
       </td>
       <td>
        TP = 8
       </td>
       <td>
        FN = 2
       </td>
      </tr>
      <tr>
       <td>
        ​
        <strong>
         实际健康
        </strong>
        ​
       </td>
       <td>
        FP = 7
       </td>
       <td>
        TN = 83
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="687474:70733a2f2f626c6f672e6373646e2e6e65742f73363636342f:61727469636c652f64657461696c732f313435383537353037" class_="artid" style="display:none">
 </p>
</div>


