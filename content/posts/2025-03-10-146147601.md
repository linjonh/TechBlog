---
layout: post
title: "machine-learning-DPData-Parallel-vs-DDPDistributed-Data-Parallel"
date: 2025-03-10 14:47:37 +0800
description: "DP和DDP是并行训练的两种方法，本文简单介绍它们两者的区别。"
keywords: "[machine learning] DP(Data Parallel) vs DDP(Distributed Data Parallel)"
categories: ['Machine', 'Learning']
tags: ['机器学习', '人工智能']
artid: "146147601"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146147601
    alt: "machine-learning-DPData-Parallel-vs-DDPDistributed-Data-Parallel"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146147601
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146147601
cover: https://bing.ee123.net/img/rand?artid=146147601
image: https://bing.ee123.net/img/rand?artid=146147601
img: https://bing.ee123.net/img/rand?artid=146147601
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     [machine learning] DP(Data Parallel) vs DDP(Distributed Data Parallel)
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     DP和DDP是并行训练的两种方法，本文简单介绍它们两者的区别。
    </p>
    <h2>
     <a id="DP_Data_Parallel_1">
     </a>
     一、DP (Data Parallel)
    </h2>
    <p>
     DP是单进程，多线程的，每个线程负责一个GPU，它只适用于一台机器。DP训练的流程如下图所示(图片转载自：https://medium.com/@mlshark/dataparallel-vs-distributeddataparallel-in-pytorch-whats-the-difference-0af10bb43bc7)：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ac0f09bb68744ab786264a8786c97af5.png#pic_center"/>
    </p>
    <ol>
     <li>
      选择一个GPU作为主GPU，后面梯度(
      <code>
       gradients
      </code>
      )的计算和模型参数的更新都是在主GPU上完成；
     </li>
     <li>
      每个GPU都会收到初始模型；
     </li>
     <li>
      训练数据被均分到每个显卡；
     </li>
     <li>
      每个GPU各自独立地进行前向传播和反向传播计算，得到各自的梯度。除了主GPU，其他GPU都会把梯度拷贝到共享内存中；
     </li>
     <li>
      主GPU从共享内存中收集
      <code>
       gather
      </code>
      其他所有GPU的梯度，然后做
      <code>
       reduce(mean or sum)
      </code>
      计算。得到最后梯度后，对模型参数进行更新，更新后的模型会拷贝到共享内存中，其他GPU会从共享内存中拷贝出(
      <code>
       scatter
      </code>
      )更新后的模型。至此，一轮训练就结束了，每个GPU上的模型都被更新了而且都保持一致(
      <code>
       sync
      </code>
      )。
     </li>
    </ol>
    <p>
     从上面的过程中可以看到，梯度的计算和模型的更新都只在一个GPU上进行，这限制了训练的效率；同时，为了保持各个GPU上的模型同步一致，DP训练涉及大量的内存拷贝。DP训练的这种
     <code>
      scatter updated model - gather gradients - per-iteration model copy
     </code>
     模式，以及Python中多线程
     <a href="https://realpython.com/python-gil/" rel="nofollow">
      GIL
     </a>
     的限制，带来了极大的性能负担，所以即使在一台机器上DP一般都会比DPP训练慢。因此，DP训练只适用于小型或中型模型，不适合大型模型。
    </p>
    <h2>
     <a id="DDP_Distributed_Data_Parallel_13">
     </a>
     二、DDP (Distributed Data Parallel)
    </h2>
    <p>
     DDP是多进程，每个进程负责一个GPU，支持在一台机器或多台机器上并行训练。DDP训练的过程如下：
    </p>
    <ol>
     <li>
      每个进程都会拿到相同的模型初始状态；
     </li>
     <li>
      训练数据被均分到每个GPU；
     </li>
     <li>
      每个GPU各自独立地进行前向传播和反向传播计算，得到各自的梯度；
     </li>
     <li>
      当前进程的梯度ready后，会异步调用
      <code>
       allreduce
      </code>
      方法对所有进程的梯度进行平均。此过程完成后，所有进程的梯度都会更新为相同的平均后的梯度；
     </li>
     <li>
      每个进程独立地更新模型参数。因为每个进程开始时拿到的都是相同的初始模型，每次反向传播后拿到的也是相同的平均梯度，所以每轮训练过后，所有进程中的模型都是保持一致的(
      <code>
       sync
      </code>
      )。
     </li>
    </ol>
    <p>
     跟DP相比，DDP中每个进程都是独立地更行模型参数，最大化利用了硬件资源，因此训练更高效快速。
    </p>
    <p>
     此外DPP比DP有更好的容错机制。如果训练过程中某个GPU出问题了，对于DP，整个训练就会中断甚至失败；对于DDP，它的内置容错机制可以让训练继续下去，最小程度地受到某个GPU断线的影响。
    </p>
    <h2>
     <a id="_25">
     </a>
     三、其他一些知识点
    </h2>
    <ul>
     <li>
      DDP中的
      <code>
       allreduce
      </code>
      具体是怎么对所有GPU的梯度进行平均的，可以参考这篇文章：https://bobondemon.github.io/2020/12/20/Distributed-Data-Parallel-and-Its-Pytorch-Example/
     </li>
     <li>
      在DDP训练中，机器又叫
      <code>
       node
      </code>
      ，GPU又叫
      <code>
       device
      </code>
      。假设有
      <code>
       N
      </code>
      个node，每个node上有
      <code>
       G
      </code>
      个GPU，那么所有的process数量总和(
      <code>
       N*G
      </code>
      )就是
      <code>
       World Size (W)
      </code>
      ，每个node上所有的process数量总和叫做
      <code>
       Local World Size (L)
      </code>
      。每个process都会有两个ID：
      <code>
       local rank
      </code>
      [0, L-1]和
      <code>
       global rank
      </code>
      [0, w-1]。
     </li>
     <li>
      <code>
       nccl
      </code>
      与
      <code>
       gloo
      </code>
      : 设置DDP环境的时候，通常会见到
      <code>
       nccl
      </code>
      和
      <code>
       gloo
      </code>
      两个配置。
      <code>
       nccl
      </code>
      是专门用于NVIDIA GPU与GPU之间通信的。
      <code>
       gloo
      </code>
      既支持CPU又支持GPU，但是性能肯定不如
      <code>
       nccl
      </code>
      。
     </li>
    </ul>
    <pre><code class="prism language-pyton">dist.init_process_group("gloo", rank=rank, world_size=world_size)
dist.init_process_group("nccl", rank=rank, world_size=world_size)
</code></pre>
    <br/>
    <br/>
    <p>
     参考整理自：
    </p>
    <ol>
     <li>
      https://www.run.ai/guides/distributed-computing/pytorch-parallel-training
     </li>
     <li>
      https://medium.com/@mlshark/dataparallel-vs-distributeddataparallel-in-pytorch-whats-the-difference-0af10bb43bc7
     </li>
     <li>
      https://pytorch.org/docs/main/notes/ddp.html
     </li>
     <li>
      https://github.com/pytorch/examples/blob/main/distributed/ddp/README.md
     </li>
     <li>
      https://pytorch.org/tutorials/intermediate/ddp_tutorial.html
     </li>
    </ol>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f7a7178663132333435363738392f:61727469636c652f64657461696c732f313436313437363031" class_="artid" style="display:none">
 </p>
</div>


