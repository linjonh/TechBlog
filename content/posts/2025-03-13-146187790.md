---
layout: post
title: "爬虫基础之爬取豆瓣同城信息保存为csv-excel-数据库"
date: 2025-03-13 11:00:01 +0800
description: "(发送HTTP请求)pandas(数据保存模块)"
keywords: "爬虫基础之爬取豆瓣同城信息(保存为csv excel 数据库)"
categories: ['爬虫基础']
tags: ['爬虫', 'Python', 'Pycharm', 'Mysql', 'Excel']
artid: "146187790"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146187790
    alt: "爬虫基础之爬取豆瓣同城信息保存为csv-excel-数据库"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146187790
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146187790
cover: https://bing.ee123.net/img/rand?artid=146187790
image: https://bing.ee123.net/img/rand?artid=146187790
img: https://bing.ee123.net/img/rand?artid=146187790
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     爬虫基础之爬取豆瓣同城信息(保存为csv excel 数据库)
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     <strong>
      网站:
     </strong>
     <a href="https://www.douban.com/location/changsha/events/week-drama" rel="nofollow" title="长沙最近一周戏剧活动_豆瓣">
      长沙最近一周戏剧活动_豆瓣
     </a>
     <br/>
     <strong>
      温馨提示: 本案例仅供学习交流使用
     </strong>
    </p>
    <table align="left" border="1" cellpadding="1" cellspacing="1" style="width:500px">
     <caption>
      <strong>
       本案例所使用的模块
      </strong>
     </caption>
     <tbody>
      <tr>
       <td style="text-align:center">
        <strong>
         requests
         <em>
          (发送HTTP请求)
         </em>
        </strong>
       </td>
       <td style="text-align:center">
        <strong>
         <em>
          pandas(数据保存模块)
         </em>
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align:center">
        <strong>
         lxml(用于解析数据模块)
        </strong>
       </td>
       <td style="text-align:center">
        <strong>
         csv(用于保存为csv文件)
        </strong>
       </td>
      </tr>
      <tr>
       <td style="text-align:center">
        <strong>
         pymysql(用于操作数据库)
        </strong>
       </td>
       <td style="text-align:center">
        <strong>
         parsel(解析数据的模块)
        </strong>
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     <img alt="" height="1200" src="https://i-blog.csdnimg.cn/direct/e1d24b44e8774bcea730923d213fe1be.png" width="2148"/>
    </p>
    <h3>
     <strong>
      确定爬取的信息内容：
     </strong>
    </h3>
    <ul>
     <li>
      <strong>
       戏剧的名称
      </strong>
     </li>
     <li>
      <strong>
       戏剧的时间
      </strong>
     </li>
     <li>
      <strong>
       戏剧的地点
      </strong>
     </li>
     <li>
      <strong>
       戏剧的费用
      </strong>
     </li>
     <li>
      <strong>
       参与与感兴趣人数
      </strong>
     </li>
    </ul>
    <h3>
     <strong>
      爬取步骤：
     </strong>
    </h3>
    <ol>
     <li>
      <strong>
       发送请求 模拟浏览器向服务器发送请求
      </strong>
     </li>
     <li>
      <strong>
       解析数据 提取我们想要的数据
      </strong>
     </li>
     <li>
      <strong>
       保存数据 对数据进行持久化保存 如csv excel mysql
      </strong>
     </li>
    </ol>
    <h3>
     一.发送请求
    </h3>
    <p>
     <strong>
      查看源代码 查看是否为静态数据
     </strong>
     <br/>
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/bd68a8ae601c4cad9adc26a8b7a9ff8c.png" width="241"/>
    </p>
    <p>
     <strong>
      打开快捷键 Ctrl+F 输入我们想要爬取的数据
     </strong>
    </p>
    <p>
     <img alt="" height="1261" src="https://i-blog.csdnimg.cn/direct/22fcdf0afad74cfab205af5eb9717666.png" width="2148">
      <br/>
      <strong>
       能够搜索到的为 静态数据  不能搜索到的为动态数据由服务器返回的 需要找到相关的返回接口
      </strong>
     </img>
    </p>
    <p>
     <strong>
      开始写代码  构建请求
      <br/>
      复制浏览器中的网址  请求中的数据在 网络中的标头中查看
     </strong>
    </p>
    <pre><code class="language-python">url = f'https://www.douban.com/location/changsha/events/week-drama'</code></pre>
    <p>
     <strong>
      F12 or 右击检查 点击网络 Ctrl+R 刷新接口  随便点击一个数据包
     </strong>
     <br/>
     <img alt="" height="1292" src="https://i-blog.csdnimg.cn/direct/634412338d3a43c2a9012be9d5afd52b.png" width="2043"/>
    </p>
    <p>
     <strong>
      这里面包含我们所有的请求信息  比如请求方式 请求地址 UA cookie等
      <br/>
      一般我们复制 UserAgent Referer cookie 这三个信息
     </strong>
    </p>
    <pre><code class="language-python"># 导包
import requests

from lxml import etree

headers = {
    'user-agent':
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0',
    'referer':
        'https://www.douban.com/location/changsha/',
    'cookie':
        '你的cookie'
}
resp = requests.get(url, headers=headers)
print(resp.text)</code></pre>
    <p>
     <strong>
      打印我们请求到的网页信息 确认需要爬取的数据是否在里面
     </strong>
    </p>
    <p>
     <strong>
      同样的在终端用Ctrl+F 快捷键打开搜索框 OK 数据在里头
     </strong>
     <br/>
     <img alt="" height="474" src="https://i-blog.csdnimg.cn/direct/505efa6a07cc43a69c0a042c6f37bb88.png" width="2038"/>
    </p>
    <h3>
     二.解析数据
    </h3>
    <p>
     <strong>
      接着我们解析数据 使用lxml 中的etree模块进行解析
     </strong>
    </p>
    <pre><code class="language-python"># 将页面解析成html
html = etree.HTML(resp.text)</code></pre>
    <p>
     <strong>
      然后我们分析网页页面  找到包含所有数据的标签 后续通过循环遍历提取
      <br/>
      Explain： 用开发者工具上面的小箭头 去页面中分析结构
     </strong>
     <br/>
     <img alt="" height="1292" src="https://i-blog.csdnimg.cn/direct/9662b037f65842e6b0404e2183f4f811.png" width="2081"/>
    </p>
    <p>
     <strong>
      我们取 ul class属性为events-list events-list-pic100 events-list-psmall 下面的li标签
      <br/>
      每一个li标签为一个戏剧
     </strong>
    </p>
    <pre><code class="language-python">lis = html.xpath('//ul[@class="events-list events-list-pic100 events-list-psmall"]/li')</code></pre>
    <p>
     <strong>
      接着遍历循环 提取数据
      <br/>
      name: 直接定位这个
     </strong>
     itemprop="summary"的
     <strong>
      span标签 提取里面的文字
     </strong>
     <br/>
     <img alt="" height="500" src="https://i-blog.csdnimg.cn/direct/b2d0b80b106b40699fbcee20d8ddfc92.png" width="593"/>
    </p>
    <pre><code class="language-python">for li in lis:
    name = li.xpath('.//a/span[@itemprop="summary"]/text()')[0]
    # 提取出来的是个列表 我们取出来</code></pre>
    <p>
     <strong>
      time: class属性为
     </strong>
     <strong>
      event-time的li标签 提取里面的文本
     </strong>
    </p>
    <p>
     <img alt="" height="249" src="https://i-blog.csdnimg.cn/direct/b0ebedf61c3f41e8b1b96d057567be03.png" width="971"/>
    </p>
    <p>
     <strong>
      打印查看文本 发现不是我们想要的格式  我们处理成想要的格式
      <br/>
      将星期去掉 只保留日期和时间 去除换行符
      <br/>
      将列表转换成字符串 去除空格 以一个空格分割
     </strong>
    </p>
    <p>
     <img alt="" height="352" src="https://i-blog.csdnimg.cn/direct/5971009b59184374b8a9089f2b2c06b0.png" width="1665"/>
    </p>
    <pre><code class="language-python">a = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')</code></pre>
    <p>
     <strong>
      打印出来看看结果  split 方法返回的是个列表通过取值得到我们想要的 之后做个字符串的拼接
     </strong>
     <br/>
     <img alt="" height="367" src="https://i-blog.csdnimg.cn/direct/6d58ee872cc4450997eb864005d09b77.png" width="1431"/>
    </p>
    <pre><code class="language-python">    # 这里我们选择把 星期去掉 只要时间和具体的时间
    t = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[0]
    i = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[2]
    m = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[-3]
    e = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[-1]
    # 转换成我们想要的格式  中间以||字符隔开
    time = t + i + '||' + m + e</code></pre>
    <p>
     <img alt="" height="187" src="https://i-blog.csdnimg.cn/direct/ffe8e6fdd5c9481ba1a6ae04ac908239.png" width="1369"/>
     <br/>
     <strong>
      时间提取完毕
     </strong>
     <br/>
     <strong>
      address：
     </strong>
     <strong>
      li标签下面meta标签中的itemprop=location 里面的content 属性
     </strong>
     <br/>
     <img alt="" height="1248" src="https://i-blog.csdnimg.cn/direct/92befc494a574cf3ae06e6c87724f29d.png" width="1418"/>
    </p>
    <p>
     <strong>
      但我不要长沙两个字 后续提取的数据中都有 可以像之前那样处理 先转换成字符串 后分割索引取值 做个拼接 这里我一气呵成
     </strong>
     <br/>
     <img alt="" height="180" src="https://i-blog.csdnimg.cn/direct/58526338813f4301a74f694eba5bdcb5.png" width="1335"/>
    </p>
    <pre><code class="language-python">    address = ''.join(li.xpath('.//li/meta[@itemprop="location"]/@content')).split(' ')[1] + ' ' + \
              ''.join(li.xpath('.//li/meta[@itemprop="location"]/@content')).split(' ')[2]</code></pre>
    <p>
     <img alt="" height="184" src="https://i-blog.csdnimg.cn/direct/3f43c9ef49ff4051b602a86382e86081.png" width="1361"/>
    </p>
    <p>
     <strong>
      接着提取费用: class属性为fee下面的strong标签 里面的文本
     </strong>
     <br/>
     <img alt="" height="161" src="https://i-blog.csdnimg.cn/direct/f9eb975611114ddba52e8b635688b82b.png" width="693"/>
    </p>
    <p>
     <strong>
      去除里面的文字只保留数值
     </strong>
    </p>
    <pre><code class="language-python">fee = ''.join(li.xpath('.//li[@class="fee"]/strong/text()')).replace('元起', '').replace('元', '')</code></pre>
    <p>
     <strong>
      最后感兴趣人数和参与人数 class属性为counts下面的第一个和第三个span标签 提取里面的文本
     </strong>
     <br/>
     <img alt="" height="191" src="https://i-blog.csdnimg.cn/direct/88ffc571690a415d938eaf2be47cb1f9.png" width="637"/>
    </p>
    <p>
     <strong>
      与之前的一样 删除文字 只保留数值
     </strong>
    </p>
    <pre><code class="language-python">    # 参与与感兴趣人数
    participate = ''.join(li.xpath('.//p[@class="counts"]/span[1]/text()')).replace('人参加', '')
    interesting = ''.join(li.xpath('.//p[@class="counts"]/span[3]/text()')).replace('人感兴趣', '')</code></pre>
    <p>
     <strong>
      到此我们所有的数据提取完毕
      <br/>
      多页采集的话 分析url请求地址的变化即可
     </strong>
     <br/>
     <img alt="" height="42" src="https://i-blog.csdnimg.cn/direct/5a269d15117f4df4b9fdd7b35088b2a4.png" width="956"/>
    </p>
    <p>
     <strong>
      第二页即第四页的数据
     </strong>
    </p>
    <p>
     <img alt="" height="42" src="https://i-blog.csdnimg.cn/direct/0635108a3d97483a979b6513da55885d.png" width="956"/>
     <strong>
      我们发现页码从0开始30结束 步长为10
      <br/>
      构建循环 遍历
     </strong>
    </p>
    <pre><code class="language-python">for page in range(0, 31, 10):
    url = f'https://www.douban.com/location/changsha/events/week-drama?start={page}'</code></pre>
    <h3>
     <strong>
      三.保存数据
     </strong>
    </h3>
    <p>
     <strong>
      先将我们的数据保存到字典当中 在外面定义一个空列表 后续将字典添加到列表中
     </strong>
    </p>
    <pre><code class="language-python"># 主体代码如下
data = []
for page in range(0, 31, 10):
    url = f'https://www.douban.com/location/changsha/events/week-drama?start={page}'
    headers = {
        'user-agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0',
        'referer':
            'https://www.douban.com/location/changsha/',
        'cookie':
            'bid=BY8EGmm9qc8; _vwo_uuid_v2=D1D9F5E9C68A518ED23DD0F263091E11E|05339352015c3656f34b21db6f6d7fac; _pk_id.100001.8cb4=273b64a736ae847b.1740225141.; dbcl2="287338786:YzuvHMiOWzs"; push_noty_num=0; push_doumail_num=0; __yadk_uid=PRahUgXe0eJLiRZgpFZjUYaLI78zwXW2; __utmv=30149280.28733; _ga_RXNMP372GL=GS1.1.1741225615.2.0.1741225615.60.0.0; ck=qsD8; __utmc=30149280; __utmz=30149280.1741595670.20.11.utmcsr=cn.bing.com|utmccn=(referral)|utmcmd=referral|utmcct=/; frodotk_db="5213e2611a4c65c4f63ba7234ee5314c"; _ga=GA1.2.1489580265.1740970320; _gid=GA1.2.1344269174.1741598633; _ga_Y4GN1R87RG=GS1.1.1741598632.1.0.1741598634.0.0.0; loc-last-index-location-id="118267"; ll="118267"; ap_v=0,6.0; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1741606351%2C%22https%3A%2F%2Fntp.msn.cn%2F%22%5D; _pk_ses.100001.8cb4=1; __utma=30149280.1636982792.1735051587.1741598189.1741606351.22; __utmt=1; __utmb=30149280.4.10.1741606351'
    }
    
    resp = requests.get(url, headers=headers)
    html = etree.HTML(resp.text)
    lis = html.xpath('//ul[@class="events-list events-list-pic100 events-list-psmall"]/li')
    
    dit = {}
    for li in lis:
        name = li.xpath('.//a/span[@itemprop="summary"]/text()')[0]
        # 这里我们选择把 星期去掉 只要时间和具体的时间
        t = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[0]
        i = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[2]
        m = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[-3]
        e = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[-1]
        # 转换成我们想要的格式
        time = t + i + '||' + m + e
        # 另外一种方法 处理时间
        # n = li.xpath('.//li[@class="event-time"]/text()')[1].replace('\n', '').replace(' ','')
        # address = ''.join(li.xpath('//ul[@class="event-meta"]/li[2]/text()')).replace('\n','').replace(' ','')
        # 电影费用  并只保留数值
        # 将前面的长沙去掉
        address = ''.join(li.xpath('.//li/meta[@itemprop="location"]/@content')).split(' ')[1] + ' ' + \
                  ''.join(li.xpath('.//li/meta[@itemprop="location"]/@content')).split(' ')[2]

        fee = ''.join(li.xpath('.//li[@class="fee"]/strong/text()')).replace('元起', '').replace('元', '')
        # 参与与感兴趣人数
        participate = ''.join(li.xpath('.//p[@class="counts"]/span[1]/text()')).replace('人参加', '')
        interesting = ''.join(li.xpath('.//p[@class="counts"]/span[3]/text()')).replace('人感兴趣', '')

        dit = {
            'name': name,
            'time': time,
            'fee': fee,
            'participate': participate,
            'interesting': interesting,
            'address': address,
        }
        data.append(dit)
pd.DataFrame(data).to_excel('city.xlsx', index=False)</code></pre>
    <p>
     <img alt="" height="901" src="https://i-blog.csdnimg.cn/direct/2e20ae448c8a40c8a36d2db41535d2ec.png" width="2160"/>
    </p>
    <p>
     <strong>
      以上为通过pandas保存为excel表格
     </strong>
    </p>
    <p>
     <strong>
      保存为csv 文件方式
     </strong>
    </p>
    <pre><code class="language-python"># 导入模块
import csv
#调用字典写入方法 写入文件  保存为csv的格式编码为utf-8-sig 不然会出现乱码
csv_writer = csv.DictWriter(open('city_csv.csv', 'w', encoding='utf-8-sig', newline=''),
                fieldnames=['name', 'time', 'fee', 'participate', 'interesting', 'address'])
# fieldnames 定义标头
# 写入表头
csv_writer.writeheader()</code></pre>
    <p>
     <img alt="" height="1368" src="https://i-blog.csdnimg.cn/direct/a4ecc861b69a4954960965974bf7b665.png" width="2160"/>
    </p>
    <p>
     <strong>
      保存到数据库
     </strong>
    </p>
    <p>
     <strong>
      前提需要建库建表 定义字段
     </strong>
     <br/>
     <img alt="" height="496" src="https://i-blog.csdnimg.cn/direct/e5fa10bc24424a8999bb6214e9671440.png" width="1546"/>
    </p>
    <pre><code class="language-python"># 导包
import pymysql

# 获取连接
connect = pymysql.connect(user='root',
                          password="112233",
                          host='localhost',
                          database='douban', )
# 拿到游标
cursor = connect.cursor()

# 准备sql语句
sql = 'insert into city values(%s,%s,%s,%s,%s,%s)'
# 执行sql
cursor.executemany(sql, [(name, time, fee, participate, interesting, address)])
# 提交事务
connect.commit()

</code></pre>
    <p>
     <img alt="" height="836" src="https://i-blog.csdnimg.cn/direct/c7ca41bc517d487f9df4e88adf726ce3.png" width="1546"/>
    </p>
    <p>
     <strong>
      本次案例的所有源代码  供大家交流学习使用
     </strong>
    </p>
    <pre><code class="language-python">import requests
from lxml import etree
import pandas as pd
import csv
import pymysql

# csv_writer = csv.DictWriter(open('city_csv.csv', 'w', encoding='utf-8-sig', newline=''),
#                fieldnames=['name', 'time', 'fee', 'participate', 'interesting', 'address'])
# csv_writer.writeheader()

# 获取连接
connect = pymysql.connect(user='root',
                          password="112233",
                          host='localhost',
                          database='douban', )
cursor = connect.cursor()

data = []
for page in range(0, 31, 10):
    url = f'https://www.douban.com/location/changsha/events/week-drama?start={page}'
    headers = {
        'user-agent':
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0',
        'referer':
            'https://www.douban.com/location/changsha/',
        'cookie':
            'bid=BY8EGmm9qc8; _vwo_uuid_v2=D1D9F5E9C68A518ED23DD0F263091E11E|05339352015c3656f34b21db6f6d7fac; _pk_id.100001.8cb4=273b64a736ae847b.1740225141.; dbcl2="287338786:YzuvHMiOWzs"; push_noty_num=0; push_doumail_num=0; __yadk_uid=PRahUgXe0eJLiRZgpFZjUYaLI78zwXW2; __utmv=30149280.28733; _ga_RXNMP372GL=GS1.1.1741225615.2.0.1741225615.60.0.0; ck=qsD8; __utmc=30149280; __utmz=30149280.1741595670.20.11.utmcsr=cn.bing.com|utmccn=(referral)|utmcmd=referral|utmcct=/; frodotk_db="5213e2611a4c65c4f63ba7234ee5314c"; _ga=GA1.2.1489580265.1740970320; _gid=GA1.2.1344269174.1741598633; _ga_Y4GN1R87RG=GS1.1.1741598632.1.0.1741598634.0.0.0; loc-last-index-location-id="118267"; ll="118267"; ap_v=0,6.0; _pk_ref.100001.8cb4=%5B%22%22%2C%22%22%2C1741606351%2C%22https%3A%2F%2Fntp.msn.cn%2F%22%5D; _pk_ses.100001.8cb4=1; __utma=30149280.1636982792.1735051587.1741598189.1741606351.22; __utmt=1; __utmb=30149280.4.10.1741606351'
    }

    resp = requests.get(url, headers=headers)
    html = etree.HTML(resp.text)
    lis = html.xpath('//ul[@class="events-list events-list-pic100 events-list-psmall"]/li')

    dit = {}
    for li in lis:
        name = li.xpath('.//a/span[@itemprop="summary"]/text()')[0]
        # 这里我们选择把 星期去掉 只要时间和具体的时间
        t = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[0]
        i = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[2]
        m = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[-3]
        e = ''.join(li.xpath('.//li[@class="event-time"]/text()')).strip().split(' ')[-1]
        # 转换成我们想要的格式
        time = t + i + '||' + m + e
        # 另外一种方法 处理时间
        # n = li.xpath('.//li[@class="event-time"]/text()')[1].replace('\n', '').replace(' ','')
        # address = ''.join(li.xpath('//ul[@class="event-meta"]/li[2]/text()')).replace('\n','').replace(' ','')
        # 电影费用  并只保留数值
        # 将前面的长沙去掉
        address = ''.join(li.xpath('.//li/meta[@itemprop="location"]/@content')).split(' ')[1] + ' ' + \
                  ''.join(li.xpath('.//li/meta[@itemprop="location"]/@content')).split(' ')[2]

        fee = ''.join(li.xpath('.//li[@class="fee"]/strong/text()')).replace('元起', '').replace('元', '')
        # 参与与感兴趣人数
        participate = ''.join(li.xpath('.//p[@class="counts"]/span[1]/text()')).replace('人参加', '')
        interesting = ''.join(li.xpath('.//p[@class="counts"]/span[3]/text()')).replace('人感兴趣', '')

        dit = {
            'name': name,
            'time': time,
            'fee': fee,
            'participate': participate,
            'interesting': interesting,
            'address': address,
        }
        # data.append(dit)
        # csv_writer.writerow(dit)

        sql = 'insert into city values(%s,%s,%s,%s,%s,%s)'
        cursor.executemany(sql, [(name, time, fee, participate, interesting, address)])
        connect.commit()

    # pd.DataFrame(data).to_excel('city.xlsx', index=False)
</code></pre>
    <p style="text-align:center">
     <strong>
      本次的案例到此结束 感谢大家的观看  您的点赞和关注是我更新的动力
     </strong>
    </p>
    <p style="text-align:center">
     <strong>
      也可以看看我之前的文章希望对你有帮助
     </strong>
     <br/>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f323330325f38303234333838372f:61727469636c652f64657461696c732f313436313837373930" class_="artid" style="display:none">
 </p>
</div>


