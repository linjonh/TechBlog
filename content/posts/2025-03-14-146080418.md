---
layout: post
title: "搜广推校招面经四十七"
date: 2025-03-14 01:00:00 +0800
description: "特性PostLNPreLNLN 位置子层输出后子层输入前训练稳定性较差，需要 warm-up较好，无需 warm-up深层模型表现较差较好实现复杂度简单简单推荐使用：在深层 Transformer 模型中，PreLN 通常是更好的选择。见【搜广推校招面经二十八】、【搜广推校招面经十二】SIM（Search-based Interest Model）是由阿里妈妈提出的一种基于搜索的用户兴趣建模方法，旨在解决如何利用用户的长期行为序列数据进行点击率（CTR）预测的问题。在推荐系统和广告系统中，"
keywords: "搜广推校招面经四十七"
categories: ['搜广推面经']
tags: ['深度学习', '机器学习', '数据挖掘', '推荐算法', '人工智能']
artid: "146080418"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146080418
    alt: "搜广推校招面经四十七"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146080418
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146080418
cover: https://bing.ee123.net/img/rand?artid=146080418
image: https://bing.ee123.net/img/rand?artid=146080418
img: https://bing.ee123.net/img/rand?artid=146080418
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     搜广推校招面经四十七
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="__0">
     </a>
     字节 推荐算法
    </h2>
    <h2>
     <a id="postlnpreln_1">
     </a>
     一、postln和preln介绍一下
    </h2>
    <h3>
     <a id="11_PostLayer_Normalization_PostLN_2">
     </a>
     1.1. Post-Layer Normalization (PostLN)
    </h3>
    <p>
     PostLN 是 Transformer 模型中的经典设计，将
     <strong>
      Layer Normalization放在残差连接之后
     </strong>
     。这意味着，在每个子层（如多头自注意力机制或前馈神经网络）处理完输入并添加了残差连接后，才会应用Layer Normalization。具体形式如下：
    </p>
    <ul>
     <li>
      <strong>
       特点
      </strong>
      ：
      <ul>
       <li>
        在深层 Transformer 中，容易导致梯度不稳定的问题，因为
        <strong>
         Layer Normalization可能会引入额外的梯度消失风险
        </strong>
       </li>
       <li>
        需要较小的学习率和
        <strong>
         warm-up
        </strong>
        阶段来稳定训练。
       </li>
      </ul>
     </li>
    </ul>
    <h3>
     <a id="12_PreLayer_Normalization_PreLN_7">
     </a>
     1.2. Pre-Layer Normalization (PreLN)
    </h3>
    <p>
     PreLN 是一种改进的设计，是
     <strong>
      在残差连接之前就应用Layer Normalization
     </strong>
    </p>
    <ul>
     <li>
      <strong>
       特点
      </strong>
      ：
      <ul>
       <li>
        训练更稳定，尤其是在深层 Transformer 中。也不需要 warm-up 阶段，可以使用较大的学习率。
       </li>
       <li>
        通常比 PostLN 表现更好，可以使得每一层接收到的输入更加标准化，有助于缓解梯度消失问题
       </li>
      </ul>
     </li>
    </ul>
    <h3>
     <a id="13__13">
     </a>
     1.3. 对比总结
    </h3>
    <table>
     <thead>
      <tr>
       <th>
        特性
       </th>
       <th>
        PostLN
       </th>
       <th>
        PreLN
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         LN 位置
        </strong>
       </td>
       <td>
        子层输出后
       </td>
       <td>
        子层输入前
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         训练稳定性
        </strong>
       </td>
       <td>
        较差，需要 warm-up
       </td>
       <td>
        较好，无需 warm-up
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         深层模型表现
        </strong>
       </td>
       <td>
        较差
       </td>
       <td>
        较好
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         实现复杂度
        </strong>
       </td>
       <td>
        简单
       </td>
       <td>
        简单
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     <strong>
      推荐使用
     </strong>
     ：在深层 Transformer 模型中，PreLN 通常是更好的选择。
    </p>
    <h2>
     <a id="SIMsoftsearchhardsearch_23">
     </a>
     二、了解行为序列建模？介绍SIM，softsearch和hardsearch分别是什么。
    </h2>
    <p>
     见【搜广推校招面经二十八】、【搜广推校招面经十二】
    </p>
    <ul>
     <li>
      SIM（Search-based Interest Model）是由阿里妈妈提出的一种基于搜索的用户兴趣建模方法，旨在
      <strong>
       解决如何利用用户的长期行为序列数据进行点击率（CTR）预测的问题
      </strong>
      。
     </li>
     <li>
      在推荐系统和广告系统中，
      <strong>
       SIM模型通过两阶段的搜索机制来处理超长用户行为序列，并从中提取出与当前候选商品相关的用户兴趣
      </strong>
     </li>
    </ul>
    <h3>
     <a id="21_SIM_27">
     </a>
     2.1. SIM概述
    </h3>
    <p>
     SIM模型主要由两个单元组成：通用搜索单元（General Search Unit, GSU）和精确搜索单元（Exact Search Unit, ESU）。GSU负责从原始的、任意长度的用户行为数据中筛选出与特定候选商品相关的子序列（SBS），而ESU则进一步对这些筛选后的子序列进行详细建模，以捕捉用户对于该候选商品的具体兴趣点
    </p>
    <h3>
     <a id="22_General_Search_Unit_GSU_29">
     </a>
     2.2. 通用搜索单元（General Search Unit, GSU）
    </h3>
    <p>
     根据候选物品的信息，对用户的长期行为序列进行搜索，得到长期行为序列的子集，这里称为Sub user Behavior Sequence (SBS)。对应文章中hard-search的方法就是使用候选item的类别信息，找到用户行为序列中相同类别的item作为SBS。
    </p>
    <h3>
     <a id="221_Soft_Search_31">
     </a>
     2.2.1. Soft Search
    </h3>
    <p>
     Soft Search是一种
     <strong>
      参数化的搜索方式
     </strong>
     ，
    </p>
    <ul>
     <li>
      首先将用户的行为和候选商品表示为向量形式，然后通过计算这些向量之间的相似度（通常采用内积的方式）来检索出最相关的Top-K个用户行为。
     </li>
    </ul>
    <p>
     这种方法依赖于深度神经网络（DNN）来学习每个候选行为序列的嵌入表示，并使用
     <strong>
      近似最近邻搜索算法（例如ALSH或MIPS）加速搜索过程
     </strong>
     。
     <br/>
     尽管Soft Search的效果较好，但由于其计算成本较高，在实际部署时可能会面临性能瓶颈。
    </p>
    <h3>
     <a id="222_Hard_Search_37">
     </a>
     2.2.2. Hard Search
    </h3>
    <p>
     hard Search则采取了一种
     <strong>
      更为直接且无参数的方法
     </strong>
     ，
    </p>
    <ul>
     <li>
      根据某种规则或策略（如商品类别匹配）直接筛选出与候选商品相关的行为。例如，在电商场景下，如果候选广告属于“电子产品”类别，则只选择用户历史行为中同样属于该类别的项目作为候选集。
     </li>
    </ul>
    <p>
     Hard Search虽然可能不如Soft Search准确，但其实现简单，计算效率高，非常适合在线服务环境中的快速响应需求
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f79696e323536373538383834312f:61727469636c652f64657461696c732f313436303830343138" class_="artid" style="display:none">
 </p>
</div>


