---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f79696e323536373538383834312f:61727469636c652f64657461696c732f313436303830343138"
layout: post
title: "搜广推校招面经四十七"
date: 2025-03-14 01:00:00 +0800
description: "特性PostLNPreLNLN 位置子层输出后子层输入前训练稳定性较差，需要 warm-up较好，无需 warm-up深层模型表现较差较好实现复杂度简单简单推荐使用：在深层 Transformer 模型中，PreLN 通常是更好的选择。见【搜广推校招面经二十八】、【搜广推校招面经十二】SIM（Search-based Interest Model）是由阿里妈妈提出的一种基于搜索的用户兴趣建模方法，旨在解决如何利用用户的长期行为序列数据进行点击率（CTR）预测的问题。在推荐系统和广告系统中，"
keywords: "搜广推校招面经四十七"
categories: ['搜广推面经']
tags: ['深度学习', '机器学习', '数据挖掘', '推荐算法', '人工智能']
artid: "146080418"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146080418
    alt: "搜广推校招面经四十七"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146080418
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146080418
cover: https://bing.ee123.net/img/rand?artid=146080418
image: https://bing.ee123.net/img/rand?artid=146080418
img: https://bing.ee123.net/img/rand?artid=146080418
---

# 搜广推校招面经四十七

## 字节 推荐算法

## 一、postln和preln介绍一下

### 1.1. Post-Layer Normalization (PostLN)

PostLN 是 Transformer 模型中的经典设计，将
**Layer Normalization放在残差连接之后**
。这意味着，在每个子层（如多头自注意力机制或前馈神经网络）处理完输入并添加了残差连接后，才会应用Layer Normalization。具体形式如下：

* **特点**
  ：
  + 在深层 Transformer 中，容易导致梯度不稳定的问题，因为
    **Layer Normalization可能会引入额外的梯度消失风险**
  + 需要较小的学习率和
    **warm-up**
    阶段来稳定训练。

### 1.2. Pre-Layer Normalization (PreLN)

PreLN 是一种改进的设计，是
**在残差连接之前就应用Layer Normalization**

* **特点**
  ：
  + 训练更稳定，尤其是在深层 Transformer 中。也不需要 warm-up 阶段，可以使用较大的学习率。
  + 通常比 PostLN 表现更好，可以使得每一层接收到的输入更加标准化，有助于缓解梯度消失问题

### 1.3. 对比总结

| 特性 | PostLN | PreLN |
| --- | --- | --- |
| **LN 位置** | 子层输出后 | 子层输入前 |
| **训练稳定性** | 较差，需要 warm-up | 较好，无需 warm-up |
| **深层模型表现** | 较差 | 较好 |
| **实现复杂度** | 简单 | 简单 |

**推荐使用**
：在深层 Transformer 模型中，PreLN 通常是更好的选择。

## 二、了解行为序列建模？介绍SIM，softsearch和hardsearch分别是什么。

见【搜广推校招面经二十八】、【搜广推校招面经十二】

* SIM（Search-based Interest Model）是由阿里妈妈提出的一种基于搜索的用户兴趣建模方法，旨在
  **解决如何利用用户的长期行为序列数据进行点击率（CTR）预测的问题**
  。
* 在推荐系统和广告系统中，
  **SIM模型通过两阶段的搜索机制来处理超长用户行为序列，并从中提取出与当前候选商品相关的用户兴趣**

### 2.1. SIM概述

SIM模型主要由两个单元组成：通用搜索单元（General Search Unit, GSU）和精确搜索单元（Exact Search Unit, ESU）。GSU负责从原始的、任意长度的用户行为数据中筛选出与特定候选商品相关的子序列（SBS），而ESU则进一步对这些筛选后的子序列进行详细建模，以捕捉用户对于该候选商品的具体兴趣点

### 2.2. 通用搜索单元（General Search Unit, GSU）

根据候选物品的信息，对用户的长期行为序列进行搜索，得到长期行为序列的子集，这里称为Sub user Behavior Sequence (SBS)。对应文章中hard-search的方法就是使用候选item的类别信息，找到用户行为序列中相同类别的item作为SBS。

### 2.2.1. Soft Search

Soft Search是一种
**参数化的搜索方式**
，

* 首先将用户的行为和候选商品表示为向量形式，然后通过计算这些向量之间的相似度（通常采用内积的方式）来检索出最相关的Top-K个用户行为。

这种方法依赖于深度神经网络（DNN）来学习每个候选行为序列的嵌入表示，并使用
**近似最近邻搜索算法（例如ALSH或MIPS）加速搜索过程**
。
  
尽管Soft Search的效果较好，但由于其计算成本较高，在实际部署时可能会面临性能瓶颈。

### 2.2.2. Hard Search

hard Search则采取了一种
**更为直接且无参数的方法**
，

* 根据某种规则或策略（如商品类别匹配）直接筛选出与候选商品相关的行为。例如，在电商场景下，如果候选广告属于“电子产品”类别，则只选择用户历史行为中同样属于该类别的项目作为候选集。

Hard Search虽然可能不如Soft Search准确，但其实现简单，计算效率高，非常适合在线服务环境中的快速响应需求