---
layout: post
title: "Pytorch中矩阵乘法使用及案例"
date: 2025-03-13 23:55:42 +0800
description: "Pytorch中矩阵乘法的使用以及使用案例"
keywords: "Pytorch中矩阵乘法使用及案例"
categories: ['Pytorch']
tags: ['矩阵', '人工智能', 'Pytorch']
artid: "146245349"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146245349
    alt: "Pytorch中矩阵乘法使用及案例"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146245349
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146245349
cover: https://bing.ee123.net/img/rand?artid=146245349
image: https://bing.ee123.net/img/rand?artid=146245349
img: https://bing.ee123.net/img/rand?artid=146245349
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Pytorch中矩阵乘法使用及案例
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h3>
     <a id="_1">
     </a>
     六种矩阵乘法
    </h3>
    <p>
     <code>
      torch
     </code>
     中包含许多矩阵乘法，大致可以分为以下几种：
    </p>
    <ul>
     <li>
      <p>
       <code>
        *
       </code>
       ：即
       <code>
        a * b
       </code>
       按位相乘，要求
       <code>
        a
       </code>
       和
       <code>
        b
       </code>
       的形状必须一致，支持广播操作
      </p>
     </li>
     <li>
      <p>
       <code>
        torch.matmul()
       </code>
       ：最广泛的矩阵乘法
      </p>
     </li>
     <li>
      <p>
       <code>
        @
       </code>
       ：与
       <code>
        torch.matmul()
       </code>
       效果一样（等价），即
       <code>
        torch.matmul(a, b) == a @ b
       </code>
      </p>
     </li>
     <li>
      <p>
       <code>
        torch.dot()
       </code>
       ：两个一维向量乘法，不支持广播
      </p>
     </li>
     <li>
      <p>
       <code>
        torch.mm()
       </code>
       ：两个二维矩阵的乘法，不支持广播
      </p>
     </li>
     <li>
      <p>
       <code>
        torch.bmm()
       </code>
       ：两个三维矩阵乘法（批次
       <code>
        batch
       </code>
       粒度），且两个矩阵必须是三维的，不支持广播操作
      </p>
     </li>
    </ul>
    <p>
     其中，
     <code>
      torch.matmul()
     </code>
     中包含
     <code>
      torch.dot()
     </code>
     、
     <code>
      torch.mm()
     </code>
     和
     <code>
      torch.bmm()
     </code>
    </p>
    <h3>
     <a id="_15">
     </a>
     代码验证
    </h3>
    <h4>
     <a id="torchdot_17">
     </a>
     <code>
      torch.dot()
     </code>
    </h4>
    <pre><code class="prism language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment">## 下面四个函数的结果是一样的  结果都是7</span>
a<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>b<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
a @ b
torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span>
</code></pre>
    <p>
     输出结果：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/8112e1ba543a4fb7a3be65a5f014d00f.png"/>
    </p>
    <p>
     但
     <code>
      torch.matmul()
     </code>
     和
     <code>
      torch.dot()
     </code>
     的主要区别就是，当两个向量（矩阵）的维度不一致时，
     <code>
      torch.matmul()
     </code>
     会进行
     <strong>
      广播
     </strong>
     ，而
     <code>
      torch.dot()
     </code>
     会报错
    </p>
    <h4>
     <a id="_35">
     </a>
     <code>
      *
     </code>
    </h4>
    <p>
     对向量
     <code>
      a
     </code>
     和
     <code>
      b
     </code>
     进行
     <strong>
      按位相乘
     </strong>
    </p>
    <pre><code class="prism language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

a <span class="token operator">*</span> b  <span class="token comment"># [4, 3]</span>
</code></pre>
    <h4>
     <a id="torchmm_46">
     </a>
     <code>
      torch.mm()
     </code>
    </h4>
    <p>
     用于二维矩阵的相乘——第一个向量的
     <strong>
      列
     </strong>
     和第二个向量的
     <strong>
      行
     </strong>
     必须
     <strong>
      相等
     </strong>
    </p>
    <pre><code class="prism language-python">mat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
mat2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

<span class="token comment">## 下面三个输出结果是一样的</span>
torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>mat1<span class="token punctuation">,</span> mat2<span class="token punctuation">)</span>
mat1<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>mat2<span class="token punctuation">)</span>
mat1 @ mat2
</code></pre>
    <p>
     输出结果：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/969893d91a8c4851bbba7cae60156dc7.png"/>
    </p>
    <p>
     但
     <code>
      torch.matmul()
     </code>
     和
     <code>
      torch.mm()
     </code>
     的主要区别就是，当两个矩阵的维度不一致时，
     <code>
      torch.matmul()
     </code>
     会进行
     <strong>
      广播
     </strong>
     ，而
     <code>
      torch.mm()
     </code>
     会报错
    </p>
    <h4>
     <a id="torchbmm_65">
     </a>
     <code>
      torch.bmm()
     </code>
    </h4>
    <p>
     应用于三维矩阵，要求：
    </p>
    <ul>
     <li>
      两个矩阵的第一个维度的大小
      <strong>
       必须相同
      </strong>
     </li>
     <li>
      必须满足第一个矩阵：
      <code>
       (b × n × m)
      </code>
      ，第二个矩阵：
      <code>
       (b × m × p)
      </code>
      ，即第一个矩阵的第三个维度必须和第二个矩阵的第二个维度相同
     </li>
     <li>
      输出大小：
      <code>
       (b × n × p)
      </code>
     </li>
    </ul>
    <p>
     该函数相当于
     <strong>
      分别对每个
      <code>
       batch
      </code>
      进行二维矩阵相乘
     </strong>
    </p>
    <pre><code class="prism language-python">bmat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
bmat2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment">## 下面三个输出是一样的</span>
torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>bmat1<span class="token punctuation">,</span> bmat2<span class="token punctuation">)</span>
bmat1<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>bmat2<span class="token punctuation">)</span>
bmat1 @ bmat2
</code></pre>
    <p>
     输出结果：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/65f1106768c749bf858adf82035c4c49.png"/>
    </p>
    <p>
     换一种角度想，
     <code>
      torch.bmm()
     </code>
     就是相当于按照批次
     <code>
      batch
     </code>
     进行索引，然后将每个批次内的二维矩阵进行相乘
    </p>
    <pre><code class="prism language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>bmat1<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 索引出来批次bmat1.shape[0]</span>
    temp <span class="token operator">=</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>bmat1<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bmat2<span class="token punctuation">[</span>i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>temp<span class="token punctuation">)</span>
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/21c2c55021b140189b72b8002468a717.png"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="6874747073:3a2f2f626c6f672e6373646e2e6e65742f496177667932322f:61727469636c652f64657461696c732f313436323435333439" class_="artid" style="display:none">
 </p>
</div>


