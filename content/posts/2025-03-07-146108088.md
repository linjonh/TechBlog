---
layout: post
title: "从零开始实现大语言模型十三预训练大语言模型GPTModel"
date: 2025-03-07 23:39:44 +0800
description: "本文使用交叉熵损失函数计算生成大语言模型`GPTModel`的预测输出与训练样本标签之间的loss，介绍大语言模型的预训练流程，并实现预训练大语言模型的函数`pretrain_model`。"
keywords: "从零开始实现大语言模型（十三）：预训练大语言模型GPTModel"
categories: ['从零开始实现大语言模型']
tags: ['大语言模型', '从零开始实现大语言模型', '人工智能', 'Llm', 'Deepseek', 'Chatgpt']
artid: "146108088"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146108088
    alt: "从零开始实现大语言模型十三预训练大语言模型GPTModel"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146108088
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146108088
cover: https://bing.ee123.net/img/rand?artid=146108088
image: https://bing.ee123.net/img/rand?artid=146108088
img: https://bing.ee123.net/img/rand?artid=146108088
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     从零开始实现大语言模型（十三）：预训练大语言模型GPTModel
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-github-gist" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="1__0">
     </a>
     1. 前言
    </h2>
    <p>
     使用梯度下降算法通过下一个token预测任务预训练大语言模型
     <code>
      GPTModel
     </code>
     ，前向传播流程每次会输入一个batch的长度均为
     <code>
      context_len
     </code>
     的训练样本，执行
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        batch_size 
        
       
         × 
        
       
         context_len 
        
       
      
        \text{batch\_size}\times\text{context\_len}
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           batch_size
          </span>
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           context_len
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     次下一个token预测任务，共预测输出
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        batch_size 
        
       
         × 
        
       
         context_len 
        
       
      
        \text{batch\_size}\times\text{context\_len}
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           batch_size
          </span>
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           context_len
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     个tokens。后向传播流程首先会使用交叉熵(Cross Entropy)损失函数计算大语言模型
     <code>
      GPTModel
     </code>
     的预测输出与训练样本标签之间的损失(loss)，再通过后向传播算法计算大语言模型参数梯度，最后使用梯度下降算法更新大语言模型的参数。
    </p>
    <p>
     本文使用交叉熵损失函数计算生成大语言模型
     <code>
      GPTModel
     </code>
     的预测输出与训练样本标签之间的loss，介绍大语言模型的预训练流程，并实现预训练大语言模型的函数
     <code>
      pretrain_model
     </code>
     。
    </p>
    <h2>
     <a id="2_Cross_Entropy_8">
     </a>
     2. 损失函数Cross Entropy
    </h2>
    <p>
     交叉熵损失函数可以度量大语言模型的预测输出与训练样本标签之间的差异。损失函数计算生成的loss值越大，表明大语言模型的预测输出与训练样本标签之间的差异越大，loss值越小，表明大语言模型的预测输出与训练样本标签之间的差异越小。
    </p>
    <p>
     对输入文本做tokenization，将输入文本转换成包含
     <code>
      context_len
     </code>
     个token ID的列表，并输入大语言模型
     <code>
      GPTModel
     </code>
     ，可以得到
     <code>
      context_len
     </code>
     个维度为
     <code>
      vocabulary_size
     </code>
     的logits向量，第
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        i 
        
       
      
        i
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.6595em;">
         </span>
         <span class="mord mathnormal">
          i
         </span>
        </span>
       </span>
      </span>
     </span>
     个logits向量是大语言模型根据前
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        i 
        
       
      
        i
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.6595em;">
         </span>
         <span class="mord mathnormal">
          i
         </span>
        </span>
       </span>
      </span>
     </span>
     个token预测生成的下一个token的概率分数向量，logits向量中的第
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        k 
        
       
      
        k
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.6944em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0315em;">
          k
         </span>
        </span>
       </span>
      </span>
     </span>
     个概率分数值越大，表明大语言模型预测生成的下一个token的ID为
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        k 
        
       
      
        k
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.6944em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0315em;">
          k
         </span>
        </span>
       </span>
      </span>
     </span>
     的概率越高。
    </p>
    <p>
     使用
     <code>
      softmax
     </code>
     函数将大语言模型预测生成的logits向量归一化，得到大语言模型预测生成的下一个token的概率分布，概率分布中对应样本标签位置的概率值表示大语言模型预测输出的token为相应训练样本标签的概率。对应样本标签位置的概率值越接近1，表明大语言模型预测输出的token为相应训练样本标签的概率越高，大语言模型的预测输出与训练样本标签之间的差异越小。
    </p>
    <p>
     <img alt="图一" src="https://i-blog.csdnimg.cn/direct/40364494e3124b76bd008151453f43ac.png#pic_center"/>
    </p>
    <p>
     使用梯度下降算法预训练大语言模型
     <code>
      GPTModel
     </code>
     的前向传播流程中，大语言模型每次会预测生成
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        batch_size 
        
       
         × 
        
       
         context_len 
        
       
      
        \text{batch\_size}\times\text{context\_len}
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           batch_size
          </span>
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           context_len
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     个下一个token的概率分布。如下图所示，交叉熵损失函数会分别获取
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        batch_size 
        
       
         × 
        
       
         context_len 
        
       
      
        \text{batch\_size}\times\text{context\_len}
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           batch_size
          </span>
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           context_len
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     个概率分布中对应样本标签位置的概率值，使用对数函数计算这些概率值的对数，并计算所有对数值的均值，最后将对数均值的相反数作为大语言模型
     <code>
      GPTModel
     </code>
     的预测输出与训练样本标签之间的损失loss。
    </p>
    <p>
     <img alt="图二" src="https://i-blog.csdnimg.cn/direct/b00651600ee8470c845460122c39d5ee.png#pic_center"/>
    </p>
    <p>
     如下面的代码所示，使用
     <code>
      torch.tensor
     </code>
     函数创建训练样本
     <code>
      inputs
     </code>
     及训练样本标签
     <code>
      targets
     </code>
     ，将训练样本
     <code>
      inputs
     </code>
     输入大语言模型
     <code>
      gpt2_small
     </code>
     ，并使用
     <code>
      softmax
     </code>
     函数将大语言模型的输出张量
     <code>
      logits
     </code>
     归一化，得到
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        2 
        
       
         × 
        
       
         3 
        
       
      
        2\times3
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          2
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          3
         </span>
        </span>
       </span>
      </span>
     </span>
     个下一个token的概率分布，其中每个概率分布的维度均等于词汇表的大小50257。分别获取
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        2 
        
       
         × 
        
       
         3 
        
       
      
        2\times3
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;">
         </span>
         <span class="mord">
          2
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 0.6444em;">
         </span>
         <span class="mord">
          3
         </span>
        </span>
       </span>
      </span>
     </span>
     个下一个token的概率分布中对应样本标签位置的概率值，使用
     <code>
      torch.log
     </code>
     函数计算这些概率值的对数，并计算所有对数值均值的相反数，可以得到大语言模型
     <code>
      gpt2_small
     </code>
     的预测输出与样本标签
     <code>
      targets
     </code>
     之间的交叉熵损失：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token comment"># from [从零开始实现大语言模型（七）：多头注意力机制] import MultiHeadAttention</span>
<span class="token comment"># from [从零开始实现大语言模型（八）：Layer Normalization] import LayerNorm</span>
<span class="token comment"># from [从零开始实现大语言模型（九）：前馈神经网络与GELU激活函数] import GELU, FeedForward</span>
<span class="token comment"># from [从零开始实现大语言模型（十一）：构建大语言模型GPTModel] import TransformerBlock, GPTModel</span>

torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">123</span><span class="token punctuation">)</span>

embedding_dim <span class="token operator">=</span> <span class="token number">768</span>
num_layers <span class="token operator">=</span> <span class="token number">12</span>
num_heads <span class="token operator">=</span> <span class="token number">12</span>
context_len <span class="token operator">=</span> <span class="token number">1024</span>
vocabulary_size <span class="token operator">=</span> <span class="token number">50257</span>
dropout <span class="token operator">=</span> <span class="token number">0.1</span>
qkv_bias <span class="token operator">=</span> <span class="token boolean">False</span>

gpt2_small <span class="token operator">=</span> GPTModel<span class="token punctuation">(</span>
    embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span>
    num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span>
    num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
    context_len<span class="token operator">=</span>context_len<span class="token punctuation">,</span>
    vocabulary_size<span class="token operator">=</span>vocabulary_size<span class="token punctuation">,</span>
    dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span>
    qkv_bias<span class="token operator">=</span>qkv_bias
<span class="token punctuation">)</span>

inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">16833</span><span class="token punctuation">,</span> <span class="token number">3626</span><span class="token punctuation">,</span> <span class="token number">6100</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># [["every effort moves"],</span>
     <span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">1107</span><span class="token punctuation">,</span> <span class="token number">588</span><span class="token punctuation">]</span><span class="token punctuation">]</span>      <span class="token comment"># ["I really like"]]</span>
<span class="token punctuation">)</span>

targets <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>
    <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3626</span><span class="token punctuation">,</span> <span class="token number">6100</span><span class="token punctuation">,</span> <span class="token number">345</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment"># [[" effort moves you"],</span>
     <span class="token punctuation">[</span><span class="token number">588</span><span class="token punctuation">,</span> <span class="token number">428</span><span class="token punctuation">,</span> <span class="token number">11311</span><span class="token punctuation">]</span><span class="token punctuation">]</span>  <span class="token comment"># [" really like chocolate"]]</span>
<span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    logits <span class="token operator">=</span> gpt2_small<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
probas <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

target_probas_1 <span class="token operator">=</span> probas<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> targets<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
target_probas_2 <span class="token operator">=</span> probas<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> targets<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

log_probas <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>target_probas_1<span class="token punctuation">,</span> target_probas_2<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
avg_log_probas <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>log_probas<span class="token punctuation">)</span>
neg_avg_log_probas <span class="token operator">=</span> avg_log_probas <span class="token operator">*</span> <span class="token operator">-</span><span class="token number">1</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"probas shape:"</span><span class="token punctuation">,</span> probas<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"target_probas_1:"</span><span class="token punctuation">,</span> target_probas_1<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"target_probas_2:"</span><span class="token punctuation">,</span> target_probas_2<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"log_probas:"</span><span class="token punctuation">,</span> log_probas<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"avg_log_probas:"</span><span class="token punctuation">,</span> avg_log_probas<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"cross entropy loss:"</span><span class="token punctuation">,</span> neg_avg_log_probas<span class="token punctuation">)</span>
</code></pre>
    <p>
     执行上面代码，打印结果如下：
    </p>
    <pre><code class="prism language-text">probas shape: torch.Size([2, 3, 50257])
target_probas_1: tensor([2.6369e-05, 1.5997e-05, 1.6926e-05])
target_probas_2: tensor([1.5638e-05, 8.9422e-06, 1.7967e-05])
log_probas: tensor([-10.5433, -11.0431, -10.9867, -11.0658, -11.6247, -10.9270])
avg_log_probas: tensor(-11.0318)
cross entropy loss: tensor(11.0318)
</code></pre>
    <p>
     可以直接使用PyTorch内置的
     <code>
      cross_entropy
     </code>
     函数执行上述计算流程，得到大语言模型
     <code>
      gpt2_small
     </code>
     的预测输出
     <code>
      logits
     </code>
     与样本标签
     <code>
      targets
     </code>
     之间的交叉熵损失：
    </p>
    <pre><code class="prism language-python">loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>logits<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> targets<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
</code></pre>
    <p>
     执行上面代码，打印结果如下：
    </p>
    <pre><code class="prism language-text">tensor(11.0318)
</code></pre>
    <blockquote>
     <p>
      根据打印结果可知，上述6个步骤计算得到的损失值与PyTorch内置的
      <code>
       cross_entropy
      </code>
      函数计算得到的交叉熵损失完全相同。交叉熵损失本质上就是大语言模型预测生成的
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         batch_size 
         
        
          × 
         
        
          context_len 
         
        
       
         \text{batch\_size}\times\text{context\_len}
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
          </span>
          <span class="mord text">
           <span class="mord">
            batch_size
           </span>
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
          <span class="mbin">
           ×
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
          </span>
          <span class="mord text">
           <span class="mord">
            context_len
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
      个下一个token的概率分布中对应样本标签位置概率值的对数均值的相反数。
     </p>
    </blockquote>
    <h2>
     <a id="3__110">
     </a>
     3. 大语言模型预训练流程
    </h2>
    <p>
     预训练大语言模型的流程与训练普通神经网络模型本质上并没有任何不同。如下图所示，预训练大语言模型可以把整个训练数据集扫几个epoch，每个epoch会把整个训练数据集扫一遍，每次会使用训练数据集中一个batch的训练样本训练一次大语言模型。前向传播流程会将一个batch的训练样本输入大语言模型，得到大语言模型的预测输出
     <code>
      logits
     </code>
     。后向传播流程首先会使用交叉熵损失函数计算大语言模型的预测输出
     <code>
      logits
     </code>
     与训练样本标签
     <code>
      targets
     </code>
     之间的损失loss，再通过后向传播算法计算大语言模型参数梯度，最后使用梯度下降算法更新大语言模型的参数。
    </p>
    <p>
     <img alt="图三" src="https://i-blog.csdnimg.cn/direct/c3b8d4dd70fb493d88ff8ae950e4b1b8.png#pic_center"/>
    </p>
    <p>
     可以使用如下代码定义计算一个batch样本数据交叉熵损失的函数
     <code>
      calc_loss_batch
     </code>
     ，以及计算整个数据集上所有样本数据交叉熵损失的函数
     <code>
      calc_loss_loader
     </code>
     。函数
     <code>
      calc_loss_batch
     </code>
     将一个batch的样本数据输入大语言模型，得到大语言模型的预测输出
     <code>
      logits
     </code>
     ，并使用
     <code>
      torch.nn.functional.cross_entropy
     </code>
     函数计算大语言模型的预测输出
     <code>
      logits
     </code>
     与样本标签
     <code>
      targets
     </code>
     之间的损失loss。函数
     <code>
      calc_loss_loader
     </code>
     每次取数据集中一个batch的样本数据，使用
     <code>
      calc_loss_batch
     </code>
     函数计算该batch样本数据的交叉熵损失，并返回数据集上所有样本数据损失loss的均值：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">calc_loss_batch</span><span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> target_batch<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_batch <span class="token operator">=</span> input_batch<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    target_batch <span class="token operator">=</span> target_batch<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    logits <span class="token operator">=</span> model<span class="token punctuation">(</span>input_batch<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>cross_entropy<span class="token punctuation">(</span>
        logits<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target_batch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> loss

<span class="token keyword">def</span> <span class="token function">calc_loss_loader</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> num_batches<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    total_loss <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">if</span> num_batches <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        num_batches <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        num_batches <span class="token operator">=</span> <span class="token builtin">min</span><span class="token punctuation">(</span>num_batches<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> target_batch<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token operator">&lt;</span> num_batches<span class="token punctuation">:</span>
                loss <span class="token operator">=</span> calc_loss_batch<span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> target_batch<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
                total_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token keyword">break</span>
    <span class="token keyword">return</span> total_loss <span class="token operator">/</span> num_batches
</code></pre>
    <p>
     实现预训练大语言模型的函数
     <code>
      pretrain_model
     </code>
     ，可以使用for循环将整个训练数据集扫
     <code>
      num_epochs
     </code>
     遍，并在每次训练大语言模型的循环中，首先使用
     <code>
      optimizer.zero_grad
     </code>
     函数将大语言模型所有参数的梯度置为0，然后使用函数
     <code>
      calc_loss_batch
     </code>
     计算一个batch训练样本的交叉熵损失
     <code>
      loss
     </code>
     。使用
     <code>
      loss.backward
     </code>
     函数可以执行后向传播流程，计算大语言模型所有参数的梯度，并通过
     <code>
      optimizer.step
     </code>
     函数使用梯度下降算法更新大语言模型参数。具体代码如下所示：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># from [从零开始实现大语言模型（十二）：文本生成策略] import generate_text</span>

<span class="token keyword">def</span> <span class="token function">pretrain_model</span><span class="token punctuation">(</span>
        model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
        eval_freq<span class="token punctuation">,</span> eval_iter<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> start_context<span class="token punctuation">,</span>
        save_freq<span class="token punctuation">,</span> checkpoint_dir<span class="token punctuation">,</span> checkpoint<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> val_loader<span class="token operator">=</span><span class="token boolean">None</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">)</span><span class="token punctuation">:</span>
        os<span class="token punctuation">.</span>makedirs<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> checkpoint <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        model_checkpoint_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"model_</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>checkpoint<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">.pth"</span></span><span class="token punctuation">)</span>
        optimizer_checkpoint_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"optimizer_</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>checkpoint<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">.pth"</span></span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>model_checkpoint_path<span class="token punctuation">)</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>optimizer_checkpoint_path<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        checkpoint <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span>

    train_losses<span class="token punctuation">,</span> val_losses<span class="token punctuation">,</span> track_tokens_seen <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    tokens_seen<span class="token punctuation">,</span> global_step <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span>

    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> target_batch<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> global_step <span class="token operator">%</span> eval_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            loss <span class="token operator">=</span> calc_loss_batch<span class="token punctuation">(</span>input_batch<span class="token punctuation">,</span> target_batch<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">)</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

            tokens_seen <span class="token operator">+=</span> input_batch<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span>
            global_step <span class="token operator">+=</span> <span class="token number">1</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string"> (Batch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">): Train loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

            checkpoint<span class="token punctuation">,</span> train_loss<span class="token punctuation">,</span> val_loss <span class="token operator">=</span> val_and_save<span class="token punctuation">(</span>
                model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> val_loader<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> global_step<span class="token punctuation">,</span> eval_freq<span class="token punctuation">,</span>
                eval_iter<span class="token punctuation">,</span> start_context<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> save_freq<span class="token punctuation">,</span> checkpoint_dir<span class="token punctuation">,</span> checkpoint<span class="token punctuation">,</span> device
            <span class="token punctuation">)</span>
            <span class="token keyword">if</span> train_loss <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                train_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>train_loss<span class="token punctuation">)</span>
                val_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span>
                track_tokens_seen<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tokens_seen<span class="token punctuation">)</span>

        checkpoint<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> val_and_save<span class="token punctuation">(</span>
            model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> val_loader<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> global_step<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span>
            eval_iter<span class="token punctuation">,</span> start_context<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> checkpoint_dir<span class="token punctuation">,</span> checkpoint<span class="token punctuation">,</span> device
        <span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string"> finished, checkpoint: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>checkpoint<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_losses<span class="token punctuation">,</span> val_losses<span class="token punctuation">,</span> track_tokens_seen


<span class="token keyword">def</span> <span class="token function">val_and_save</span><span class="token punctuation">(</span>
    model<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> val_loader<span class="token punctuation">,</span> epoch<span class="token punctuation">,</span> global_step<span class="token punctuation">,</span> eval_freq<span class="token punctuation">,</span>
    eval_iter<span class="token punctuation">,</span> start_context<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> save_freq<span class="token punctuation">,</span> checkpoint_dir<span class="token punctuation">,</span> checkpoint<span class="token punctuation">,</span> device
<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_loss<span class="token punctuation">,</span> val_loss <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span>
    <span class="token keyword">if</span> global_step <span class="token operator">%</span> eval_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> val_loader <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            train_loss <span class="token operator">=</span> calc_loss_loader<span class="token punctuation">(</span>train_loader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> eval_iter<span class="token punctuation">)</span>
            val_loss <span class="token operator">=</span> calc_loss_loader<span class="token punctuation">(</span>val_loader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> device<span class="token punctuation">,</span> eval_iter<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string"> (Step </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>global_step<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">): Train loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">, Val loss </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>val_loss<span class="token punctuation">:</span><span class="token format-spec">.3f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        generated_sample_text <span class="token operator">=</span> generate_text<span class="token punctuation">(</span>
            model<span class="token punctuation">,</span> start_context<span class="token punctuation">,</span> max_new_tokens<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span> tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
            context_size<span class="token operator">=</span>model<span class="token punctuation">.</span>pos_emb<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> top_k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> compact_format<span class="token operator">=</span><span class="token boolean">True</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Generated Sample Text: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>generated_sample_text<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"====================================================================="</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> global_step <span class="token operator">%</span> save_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        checkpoint <span class="token operator">+=</span> <span class="token number">1</span>
        model_checkpoint_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"model_</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>checkpoint<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">.pth"</span></span><span class="token punctuation">)</span>
        optimizer_checkpoint_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>checkpoint_dir<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"optimizer_</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>checkpoint<span class="token punctuation">:</span><span class="token format-spec">06d</span><span class="token punctuation">}</span></span><span class="token string">.pth"</span></span><span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model_checkpoint_path<span class="token punctuation">)</span>
        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> optimizer_checkpoint_path<span class="token punctuation">)</span>
    <span class="token keyword">return</span> checkpoint<span class="token punctuation">,</span> train_loss<span class="token punctuation">,</span> val_loss
</code></pre>
    <blockquote>
     <p>
      PyTorch中神经网络模型的
      <code>
       state_dict
      </code>
      是一个字典对象，字典中的key为神经网络模型中参数的名称，value为相应的参数。使用
      <code>
       .state_dict
      </code>
      函数可以一次性获取神经网络模型中的所有参数，并通过
      <code>
       torch.save
      </code>
      函数将所有参数保存为一个checkpoint。
      <code>
       torch.load
      </code>
      函数可以读取指定checkpoint，通过
      <code>
       .load_state_dict
      </code>
      函数可以将神经网络模型中的参数修改为checkpoint中的记录值。
     </p>
     <p>
      所有具有自适应能力的优化器（如AdamW可以根据历史梯度信息动态调整学习率）都需要记录每个神经网络参数的历史梯度等信息，同样可以使用
      <code>
       .state_dict
      </code>
      一次性获取优化器中的所有数据记录，以及通过
      <code>
       .load_state_dict
      </code>
      函数从指定checkpoint中还原这些记录数据。
     </p>
    </blockquote>
    <p>
     如下面的代码所示，使用
     <a href="https://blog.csdn.net/qq_24178985/article/details/140138208">
      从零开始实现大语言模型（二）：文本数据处理
     </a>
     中构建的
     <code>
      Dataset
     </code>
     创建训练集
     <code>
      train_dataset
     </code>
     及验证集
     <code>
      val_dataset
     </code>
     ，并通过PyTorch内置的
     <code>
      torch.utils.data.DataLoader
     </code>
     类创建训练集及验证集对应的
     <code>
      DataLoader
     </code>
     。使用
     <code>
      torch.optim.AdamW
     </code>
     实例化训练大语言模型的优化器
     <code>
      optimizer
     </code>
     ，最后使用函数
     <code>
      pretrain_model
     </code>
     预训练大语言模型
     <code>
      gpt2_small
     </code>
     ：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> random
<span class="token keyword">import</span> tiktoken
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader

<span class="token comment"># from [从零开始实现大语言模型（二）：文本数据处理] import LLMDataset</span>

train_data_path <span class="token operator">=</span> <span class="token string">"train_data"</span>
val_data_path <span class="token operator">=</span> <span class="token string">"val_data"</span>
vocabulary <span class="token operator">=</span> <span class="token string">"gpt2"</span>
special_token_id <span class="token operator">=</span> <span class="token number">50256</span>
context_len <span class="token operator">=</span> <span class="token number">1024</span>
stride <span class="token operator">=</span> <span class="token number">1024</span>
batch_size <span class="token operator">=</span> <span class="token number">2</span>

num_epochs <span class="token operator">=</span> <span class="token number">10</span>
eval_freq <span class="token operator">=</span> <span class="token number">5</span>
eval_iter <span class="token operator">=</span> <span class="token number">1</span>
save_freq <span class="token operator">=</span> <span class="token number">5</span>
checkpoint_dir <span class="token operator">=</span> <span class="token string">"checkpoint"</span>
start_context <span class="token operator">=</span> <span class="token string">"萧炎，斗之力，三段"</span>
tokenizer <span class="token operator">=</span> tiktoken<span class="token punctuation">.</span>encoding_for_model<span class="token punctuation">(</span>vocabulary<span class="token punctuation">)</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
gpt2_small<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>AdamW<span class="token punctuation">(</span>gpt2_small<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0006</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> LLMDataset<span class="token punctuation">(</span>train_data_path<span class="token punctuation">,</span> vocabulary<span class="token punctuation">,</span> special_token_id<span class="token punctuation">,</span> context_len<span class="token punctuation">,</span> stride<span class="token punctuation">)</span>
val_dataset <span class="token operator">=</span> LLMDataset<span class="token punctuation">(</span>val_data_path<span class="token punctuation">,</span> vocabulary<span class="token punctuation">,</span> special_token_id<span class="token punctuation">,</span> context_len<span class="token punctuation">,</span> stride<span class="token punctuation">)</span>
train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
val_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>val_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"train_loader len: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>

train_losses<span class="token punctuation">,</span> val_losses<span class="token punctuation">,</span> tokens_seen <span class="token operator">=</span> pretrain_model<span class="token punctuation">(</span>
    gpt2_small<span class="token punctuation">,</span> optimizer<span class="token punctuation">,</span> train_loader<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> device<span class="token punctuation">,</span>
    eval_freq<span class="token punctuation">,</span> eval_iter<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> start_context<span class="token punctuation">,</span>
    save_freq<span class="token punctuation">,</span> checkpoint_dir<span class="token punctuation">,</span> val_loader<span class="token operator">=</span>val_loader
<span class="token punctuation">)</span>
</code></pre>
    <p>
     执行上面代码，打印结果如下：
    </p>
    <pre><code class="prism language-text">train_loader len: 7
Epoch 1 (Batch 000000): Train loss 11.034
Epoch 1 (Step 000000): Train loss 9.827, Val loss 9.784
Generated Sample Text: 萧炎，斗之力，三段 Knowledge�缌�缌缌�703 clashes�缌 longest，，缌，���，缌缌�
=====================================================================
Epoch 1 (Batch 000001): Train loss 9.940
Epoch 1 (Batch 000002): Train loss 8.811
Epoch 1 (Batch 000003): Train loss 7.954
Epoch 1 (Batch 000004): Train loss 7.286
Epoch 1 (Batch 000005): Train loss 6.629
Epoch 1 (Step 000005): Train loss 5.980, Val loss 6.003
Generated Sample Text: 萧炎，斗之力，三段，，，，，，，，，，，，，，，，�
=====================================================================
Epoch 1 (Batch 000006): Train loss 6.027
Epoch 1 (Step 000006): Train loss 5.390, Val loss 5.479
Generated Sample Text: 萧炎，斗之力，三段，，，�，，，��，，，，，�，�，，，
=====================================================================
Epoch 1 finished, checkpoint: 000002
Epoch 2 (Batch 000000): Train loss 5.401
Epoch 2 (Batch 000001): Train loss 5.028
Epoch 2 (Batch 000002): Train loss 4.788
Epoch 2 (Batch 000003): Train loss 4.616
Epoch 2 (Step 000010): Train loss 4.511, Val loss 4.526
Generated Sample Text: 萧炎，斗之力，三段，�，�，�，�，�，�，�，�，�，��，�，
=====================================================================

[...]

Epoch 9 (Step 000060): Train loss 2.561, Val loss 3.470
Generated Sample Text: 萧炎，斗之力，三段���是在脸�的�，�炣�殸废是萧炣也是曰�，萧�
=====================================================================
Epoch 9 (Batch 000005): Train loss 2.560
Epoch 9 (Batch 000006): Train loss 2.558
Epoch 9 (Step 000062): Train loss 2.456, Val loss 3.455
Generated Sample Text: 萧炎，斗之力，三段���，脸庿，炎�，萧炎萧�炎�萧�，萧�的�
=====================================================================
Epoch 9 finished, checkpoint: 000021
Epoch 10 (Batch 000000): Train loss 2.525
Epoch 10 (Batch 000001): Train loss 2.388
Epoch 10 (Batch 000002): Train loss 2.663
Epoch 10 (Step 000065): Train loss 2.270, Val loss 3.468
Generated Sample Text: 萧炎，斗之力，三段��技萧�的萧炣也�，萧�讵��更中着曰萧�着�
=====================================================================
Epoch 10 (Batch 000003): Train loss 2.464
Epoch 10 (Batch 000004): Train loss 2.602
Epoch 10 (Batch 000005): Train loss 2.511
Epoch 10 (Batch 000006): Train loss 2.557
Epoch 10 (Step 000069): Train loss 2.117, Val loss 3.474
Generated Sample Text: 萧炎，斗之力，三段��，这的�法的萧�炼�萧�法，萧�级级父了�
=====================================================================
Epoch 10 finished, checkpoint: 000023
</code></pre>
    <blockquote>
     <p>
      从上面的打印结果可知，使用梯度下降算法训练大语言模型
      <code>
       gpt2_small
      </code>
      ，可以减小大语言模型的预测输出与样本标签之间的交叉熵损失，并显著提升大语言模型的文本生成能力。在训练刚开始时，将
      <code>
       萧炎，斗之力，三段
      </code>
      输入大语言模型
      <code>
       gpt2_small
      </code>
      ，生成的是
      <code>
       Knowledge�缌�缌缌�703 clashes�缌 longest，，缌，���，缌缌�
      </code>
      或者
      <code>
       ，，，，，，，，，，，，，，，，�
      </code>
      这样不包含任何有效信息的自然语言文本序列。在仅包含7个batch训练样本的数据集上训练10个epoch，大语言模型
      <code>
       gpt2_small
      </code>
      已经可以生成
      <code>
       ���，脸庿，炎�，萧炎萧�炎�萧�，萧�的�
      </code>
      以及
      <code>
       ��，这的�法的萧�炼�萧�法，萧�级级父了�
      </code>
      这样与训练数据集存在一定关联的自然语言文本了。
     </p>
    </blockquote>
    <p>
     可以使用如下代码，分别绘制大语言模型
     <code>
      gpt2_small
     </code>
     在训练集及验证集上交叉熵损失的变化情况图像：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token keyword">def</span> <span class="token function">plot_losses</span><span class="token punctuation">(</span>epochs_seen<span class="token punctuation">,</span> tokens_seen<span class="token punctuation">,</span> train_losses<span class="token punctuation">,</span> val_losses<span class="token punctuation">)</span><span class="token punctuation">:</span>
    fig<span class="token punctuation">,</span> ax1 <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>epochs_seen<span class="token punctuation">,</span> train_losses<span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Training loss"</span><span class="token punctuation">)</span>
    ax1<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>epochs_seen<span class="token punctuation">,</span> val_losses<span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">"-."</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Validation loss"</span><span class="token punctuation">)</span>
    ax1<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"Epochs"</span><span class="token punctuation">)</span>
    ax1<span class="token punctuation">.</span>set_ylabel<span class="token punctuation">(</span><span class="token string">"Loss"</span><span class="token punctuation">)</span>
    ax1<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">"upper right"</span><span class="token punctuation">)</span>
    ax2 <span class="token operator">=</span> ax1<span class="token punctuation">.</span>twiny<span class="token punctuation">(</span><span class="token punctuation">)</span>
    ax2<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>tokens_seen<span class="token punctuation">,</span> train_losses<span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    ax2<span class="token punctuation">.</span>set_xlabel<span class="token punctuation">(</span><span class="token string">"Tokens seen"</span><span class="token punctuation">)</span>
    fig<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>

epochs_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_losses<span class="token punctuation">)</span><span class="token punctuation">)</span>
plot_losses<span class="token punctuation">(</span>epochs_tensor<span class="token punctuation">,</span> tokens_seen<span class="token punctuation">,</span> train_losses<span class="token punctuation">,</span> val_losses<span class="token punctuation">)</span>
</code></pre>
    <p>
     执行上面代码，生成交叉熵损失的变化情况图像如下：
    </p>
    <p>
     <img alt="图四" src="https://i-blog.csdnimg.cn/direct/e0f154be1e964e4aae2c25dbb1e7c64d.png#pic_center"/>
    </p>
    <blockquote>
     <p>
      从上面的交叉熵损失变化情况图像可知，在训练刚开始时，训练集及验证集上的交叉熵损失都非常大。使用梯度下降算法训练大语言模型
      <code>
       gpt2_small
      </code>
      ，可以减小大语言模型的预测输出与样本标签之间的交叉熵损失，使大语言模型的预测输出与样本标签之间的差异性更小。
     </p>
     <p>
      随着训练的进行，训练集和验证集上交叉熵损失的差异会越来越大，训练集上的交叉熵损失值会比验证集小的越来越明显，表明大语言模型在训练数据集上的过拟合情况越来越严重。在工业界的预训练大语言模型实践中，并不会在一个很小的训练数据集上训练多个epoch，而是会在一个非常大的训练数据集上训练少数几个甚至只训练一个epoch，这种训练策略可以很大程度上解决预训练大语言模型时的过拟合问题。
     </p>
    </blockquote>
    <h2>
     <a id="4__365">
     </a>
     4. 结束语
    </h2>
    <p>
     前向传播流程将一个batch的训练样本输入大语言模型，共预测输出
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        batch_size 
        
       
         × 
        
       
         context_len 
        
       
      
        \text{batch\_size}\times\text{context\_len}
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           batch_size
          </span>
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
         <span class="mbin">
          ×
         </span>
         <span class="mspace" style="margin-right: 0.2222em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1.0044em; vertical-align: -0.31em;">
         </span>
         <span class="mord text">
          <span class="mord">
           context_len
          </span>
         </span>
        </span>
       </span>
      </span>
     </span>
     个维度为
     <code>
      vocabulary_size
     </code>
     的logits向量。后向传播流程首先使用交叉熵损失函数计算大语言模型的预测输出与训练样本标签之间的损失loss，并通过后向传播算法计算大语言模型参数梯度，最后使用梯度下降算法更新大语言模型的参数。
    </p>
    <p>
     预训练大语言模型就是不断从训练数据集中获取一个batch的训练样本，然后执行这个操作直至收敛的过程。
    </p>
   </div>
   <link href="./../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="./../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f32343137383938352f:61727469636c652f64657461696c732f313436313038303838" class_="artid" style="display:none">
 </p>
</div>


