---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34373333393931362f:61727469636c652f64657461696c732f313436333030373831"
layout: post
title: "Tsfresh-TA-Lib-LightGBM-A-è‚¡å¸‚åœºé‡åŒ–æŠ•èµ„ç­–ç•¥å®æˆ˜å…¥é—¨"
date: 2025-03-16 20:25:30 +08:00
description: "æœ¬æ–‡ä»‹ç»äº†ä¸€ä¸ªé¢å‘Aè‚¡å¸‚åœºçš„é‡åŒ–æŠ•èµ„ç­–ç•¥å®æˆ˜é¡¹ç›®ï¼Œæ¶µç›–æ•°æ®è·å–ã€ç‰¹å¾æå–ã€æ¨¡å‹è®­ç»ƒã€ç­–ç•¥åˆ¶å®šä¸å›æµ‹è¯„ä¼°ç­‰ç¯èŠ‚ï¼Œæ—¨åœ¨ä¸ºé‡åŒ–æŠ€æœ¯åˆå­¦è€…æä¾›ä¸€ä¸ªç³»ç»Ÿçš„å…¥é—¨æŒ‡å—ã€‚"
keywords: "Tsfresh + TA-Lib + LightGBM ï¼šA è‚¡å¸‚åœºé‡åŒ–æŠ•èµ„ç­–ç•¥å®æˆ˜å…¥é—¨"
categories: ['é‡‘èç§‘æŠ€', 'Python']
tags: ['é‡åŒ–æŠ€æœ¯', 'ç­–ç•¥å¼€å‘', 'Tsfresh', 'Python', 'Lightgbm', 'Lib']
artid: "146300781"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146300781
    alt: "Tsfresh-TA-Lib-LightGBM-A-è‚¡å¸‚åœºé‡åŒ–æŠ•èµ„ç­–ç•¥å®æˆ˜å…¥é—¨"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146300781
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146300781
cover: https://bing.ee123.net/img/rand?artid=146300781
image: https://bing.ee123.net/img/rand?artid=146300781
img: https://bing.ee123.net/img/rand?artid=146300781
---

# Tsfresh + TA-Lib + LightGBM ï¼šA è‚¡å¸‚åœºé‡åŒ–æŠ•èµ„ç­–ç•¥å®æˆ˜å…¥é—¨

## Tsfresh + TA-Lib + LightGBM ï¼šA è‚¡å¸‚åœºé‡åŒ–æŠ•èµ„ç­–ç•¥å®æˆ˜å…¥é—¨

> æœ¬é¡¹ç›®ä»¥ A è‚¡å¸‚åœºä¸ºç ”ç©¶å¯¹è±¡ï¼Œé€šè¿‡é‡åŒ–æŠ€æœ¯å¯¹å¸‚åœºæ•°æ®è¿›è¡Œåˆ†æï¼Œæ„å»ºé‡åŒ–æŠ•èµ„ç­–ç•¥ï¼Œå¹¶åˆ©ç”¨å†å²æ•°æ®å›æµ‹éªŒè¯ç­–ç•¥çš„æœ‰æ•ˆæ€§ã€‚é¡¹ç›®æ—¨åœ¨ä¸ºé‡åŒ–æŠ€æœ¯åˆå­¦è€…æä¾›ä¸€ä¸ªç³»ç»Ÿçš„å­¦ä¹ æ¡†æ¶ï¼Œå¸®åŠ©è¯»è€…æŒæ¡ä»æ•°æ®è·å–åˆ°ç­–ç•¥è¯„ä¼°çš„å…¨æµç¨‹æ“ä½œã€‚
>   
> *æ–‡ä¸­å†…å®¹ä»…é™æŠ€æœ¯å­¦ä¹ ä¸ä»£ç å®è·µå‚è€ƒï¼Œå¸‚åœºå­˜åœ¨ä¸ç¡®å®šæ€§ï¼ŒæŠ€æœ¯åˆ†æéœ€è°¨æ…éªŒè¯ï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€‚é€‚åˆé‡åŒ–æ–°æ‰‹å»ºç«‹ç³»ç»Ÿè®¤çŸ¥ï¼Œä¸ºç­–ç•¥å¼€å‘æ‰“ä¸‹åŸºç¡€ã€‚*

â€¼ï¸ å­¦ä¹ åŸºç¡€çŸ¥è¯†æ¨èé˜…è¯»
[Python é‡‘èç§‘æŠ€ æŠ€æœ¯ä¸“æ  ğŸš€](https://blog.csdn.net/weixin_47339916/category_12907174.html)
ï¼š

* [tsfreshï¼šæ—¶é—´åºåˆ—ç‰¹å¾è‡ªåŠ¨æå–ä¸åº”ç”¨ ğŸ”¥](https://stock.blog.csdn.net/article/details/146287009)
* [æ·±å…¥TA-Libï¼šé‡åŒ–æŠ€æœ¯æŒ‡æ ‡è¯¦è§£ ğŸ”¥](https://stock.blog.csdn.net/article/details/146275433)
* [Pandas+PyArrowï¼šè‚¡ç¥¨æ•°æ®å­˜å‚¨ Parquet å…¥é—¨æŒ‡å¼• ğŸ”¥](https://stock.blog.csdn.net/article/details/146050258)
* [Daskï¼šPythoné«˜æ•ˆå¹¶è¡Œè®¡ç®—åˆ©å™¨ ğŸ”¥](https://stock.blog.csdn.net/article/details/146264849)

### 1. é¡¹ç›®æ¦‚è¿°

#### 1.1 é¡¹ç›®ç›®æ ‡

1. **æ•°æ®è·å–ä¸å­˜å‚¨**
     
   ä½¿ç”¨ tushare å·¥å…·è·å– A è‚¡è‚¡ç¥¨çš„å†å²æ—¥çº¿æ•°æ®ï¼Œç¡®ä¿æ•°æ®çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚å€ŸåŠ© Dask å’Œ Parquet æŠ€æœ¯è¿›è¡Œæ•°æ®å­˜å‚¨ï¼Œä¼˜åŒ–æ•°æ®è¯»å†™æ•ˆç‡ï¼Œä¸ºåç»­åˆ†ææä¾›æ”¯æŒã€‚
2. **ç‰¹å¾æå–ä¸ç­›é€‰**
     
   åˆ©ç”¨ tsfresh æå–æ—¶é—´åºåˆ—ç‰¹å¾ï¼Œä»å†å²æ•°æ®ä¸­æŒ–æ˜ä¸è‚¡ç¥¨æœªæ¥æ”¶ç›Šç‡ç›¸å…³çš„ç‰¹å¾ã€‚é€šè¿‡ç›¸å…³æ€§åˆ†æå’Œç‰¹å¾ç­›é€‰ï¼Œç¡®å®šå¯¹é¢„æµ‹ç›®æ ‡å…·æœ‰æ˜¾è‘—å½±å“çš„ç‰¹å¾é›†åˆã€‚
3. **æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–**
     
   åŸºäºç­›é€‰åçš„ç‰¹å¾ï¼Œä½¿ç”¨ LightGBM æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œå¹¶é€šè¿‡ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªèƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹è‚¡ç¥¨æœªæ¥æ”¶ç›Šç‡çš„æ¨¡å‹ã€‚
4. **äº¤æ˜“ç­–ç•¥åˆ¶å®šä¸å›æµ‹è¯„ä¼°**
     
   æ ¹æ®æ¨¡å‹é¢„æµ‹ç»“æœï¼Œè®¾è®¡é‡åŒ–äº¤æ˜“ç­–ç•¥ï¼Œå¹¶é€šè¿‡å†å²æ•°æ®å›æµ‹éªŒè¯ç­–ç•¥æ€§èƒ½ã€‚ä¼˜åŒ–ç­–ç•¥çš„ä¿¡å·ç”Ÿæˆå’Œä¹°å–æ—¶æœºï¼Œè®¡ç®—ç­–ç•¥è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚å¤æ™®æ¯”ç‡ã€æœ€å¤§å›æ’¤ç­‰ï¼‰ï¼Œå¹¶ç»˜åˆ¶ç­–ç•¥è¡¨ç°æ›²çº¿ã€‚
5. **é£é™©æ”¶ç›Šåˆ†æä¸ç­–ç•¥ä¼˜åŒ–**
     
   åˆ†æç­–ç•¥çš„é£é™©æ”¶ç›Šç‰¹å¾ï¼Œç»“åˆå›æµ‹ç»“æœä¼˜åŒ–ç­–ç•¥å‚æ•°ã€‚é€šè¿‡è¯¦ç»†è§£è¯»ç­–ç•¥è¡¨ç°ï¼Œå¸®åŠ©è¯»è€…ç†è§£ç­–ç•¥çš„ä¼˜åŠ¿ä¸ä¸è¶³ï¼Œä¸ºè¿›ä¸€æ­¥ä¼˜åŒ–æä¾›æ–¹å‘ã€‚

#### 1.2 é¡¹ç›®æ„ä¹‰

æœ¬é¡¹ç›®æ—¨åœ¨ä¸ºé‡åŒ–æŠ€æœ¯åˆå­¦è€…æä¾›ä¸€ä¸ªå®Œæ•´çš„å®æˆ˜æ¡ˆä¾‹ï¼Œæ¶µç›–æ•°æ®å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€ç­–ç•¥å›æµ‹å’Œé£é™©åˆ†æç­‰å…³é”®ç¯èŠ‚ã€‚é€šè¿‡ç³»ç»Ÿçš„å­¦ä¹ å’Œå®è·µï¼Œè¯»è€…å¯ä»¥æŒæ¡é‡åŒ–æŠ€æœ¯çš„åŸºæœ¬æ–¹æ³•ï¼Œç†è§£é‡åŒ–ç­–ç•¥çš„æ„å»ºè¿‡ç¨‹ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ·±å…¥ç ”ç©¶æ‰“ä¸‹åšå®åŸºç¡€ã€‚

### 2. æŠ€æœ¯ä»‹ç»

#### 2.1 Tushare

**`Tushare`**
æ˜¯ä¸€ä¸ªå…è´¹ä¸”å¼€æºçš„é‡‘èæ•°æ®æ¥å£åº“ï¼Œå®ƒä¸»è¦æœåŠ¡äºPythonå¼€å‘è€…ã€‚é€šè¿‡Tushareï¼Œç”¨æˆ·å¯ä»¥è½»æ¾è·å–è‚¡ç¥¨ã€æœŸè´§ã€å¤–æ±‡ç­‰å¤šç§ç±»å‹çš„é‡‘èå¸‚åœºæ•°æ®ã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼š

* **ä¸°å¯Œçš„æ•°æ®æº**
  ï¼šæ”¯æŒä»Aè‚¡åˆ°ç¾è‚¡ã€æ¸¯è‚¡ç­‰å…¨çƒå¤šä¸ªå¸‚åœºçš„æ•°æ®ã€‚
* **æ˜“äºä½¿ç”¨**
  ï¼šæä¾›äº†ç®€æ´çš„APIæ¥å£ï¼Œæ–¹ä¾¿ç”¨æˆ·å¿«é€Ÿä¸Šæ‰‹ã€‚
* **ç¤¾åŒºæ´»è·ƒ**
  ï¼šæ‹¥æœ‰åºå¤§çš„ç”¨æˆ·ç¾¤ä½“å’Œæ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼Œä¸æ–­æ›´æ–°ç»´æŠ¤ã€‚
* **æ–‡æ¡£è¯¦å°½**
  ï¼šå®˜æ–¹æä¾›äº†è¯¦ç»†çš„æ–‡æ¡£æ•™ç¨‹ï¼Œå¸®åŠ©æ–°æ‰‹å…¥é—¨ã€‚

#### 2.2 Tsfresh

**`Tsfresh (Time Series Feature extraction based on scalable hypothesis tests)`**
æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ—¶é—´åºåˆ—ç‰¹å¾æå–å·¥å…·ï¼Œæ—¨åœ¨è‡ªåŠ¨è¯†åˆ«å‡ºå¯¹äºé¢„æµ‹ä»»åŠ¡æœ‰ç”¨çš„ç‰¹å¾ã€‚å®ƒçš„ä¼˜åŠ¿åœ¨äºï¼š

* **è‡ªåŠ¨åŒ–**
  ï¼šèƒ½å¤Ÿè‡ªåŠ¨ç”Ÿæˆæ•°ç™¾ç§å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„ç‰¹å¾ï¼Œå‡å°‘äº†æ‰‹åŠ¨é€‰æ‹©ç‰¹å¾çš„å·¥ä½œé‡ã€‚
* **é«˜æ•ˆæ€§**
  ï¼šåˆ©ç”¨å¹¶è¡Œå¤„ç†æŠ€æœ¯æé«˜ç‰¹å¾æå–çš„é€Ÿåº¦ã€‚
* **çµæ´»æ€§**
  ï¼šä¸ä»…é™äºç‰¹å®šç±»å‹çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œé€‚ç”¨äºå¤šç§åœºæ™¯ä¸‹çš„æ•°æ®åˆ†æã€‚
* **é›†æˆå‹å¥½**
  ï¼šä¸Scikit-learnç­‰æµè¡Œæœºå™¨å­¦ä¹ åº“è‰¯å¥½å…¼å®¹ã€‚

#### 2.3 Dask

**`Dask`**
æ˜¯ä¸€ç§çµæ´»çš„å¹¶è¡Œè®¡ç®—åº“ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†è¶…å‡ºå†…å­˜é™åˆ¶çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚å®ƒçš„ç‰¹æ€§æœ‰ï¼š

* **æ‰©å±•æ€§å¼º**
  ï¼šæ—¢å¯ä»¥åœ¨å•æœºä¸Šè¿è¡Œä¹Ÿå¯ä»¥éƒ¨ç½²åœ¨åˆ†å¸ƒå¼é›†ç¾¤ä¸­ã€‚
* **åŠ¨æ€è°ƒåº¦**
  ï¼šæ ¹æ®ä»»åŠ¡éœ€æ±‚æ™ºèƒ½è°ƒæ•´èµ„æºåˆ†é…ã€‚
* **æ¥å£ç†Ÿæ‚‰**
  ï¼šè®¾è®¡é£æ ¼ç±»ä¼¼äºPandaså’ŒNumPyï¼Œé™ä½äº†å­¦ä¹ æˆæœ¬ã€‚
* **ç”Ÿæ€ç³»ç»Ÿä¸°å¯Œ**
  ï¼šæ”¯æŒå¤šç§å­˜å‚¨æ ¼å¼å¦‚HDF5ã€Parquetç­‰ï¼Œå¹¶ä¸å…¶ä»–å¤§æ•°æ®æ¡†æ¶ï¼ˆå¦‚Sparkï¼‰å…·æœ‰è‰¯å¥½çš„äº’æ“ä½œæ€§ã€‚

#### 2.4 Parquet

**`Parquet`**
æ˜¯ä¸€ç§åˆ—å¼å­˜å‚¨æ ¼å¼ï¼Œå¹¿æ³›åº”ç”¨äºå¤§æ•°æ®é¢†åŸŸã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„è¡Œå¼å­˜å‚¨ï¼ŒParquetçš„ä¼˜åŠ¿åœ¨äºï¼š

* **å‹ç¼©æ•ˆç‡é«˜**
  ï¼šé‡‡ç”¨åˆ—å¼å­˜å‚¨å¯ä»¥æ›´æœ‰æ•ˆåœ°å‹ç¼©ç›¸åŒç±»å‹çš„æ•°æ®ã€‚
* **æŸ¥è¯¢é€Ÿåº¦å¿«**
  ï¼šä»…è¯»å–éœ€è¦åˆ†æçš„åˆ—ï¼Œé¿å…äº†ä¸å¿…è¦çš„I/Oå¼€é”€ã€‚
* **è·¨å¹³å°å…¼å®¹**
  ï¼šè¢«è®¸å¤šä¸»æµçš„å¤§æ•°æ®å¤„ç†ç³»ç»Ÿæ”¯æŒï¼Œå¦‚Apache Spark, Apache Hadoopç­‰ã€‚
* **å…ƒæ•°æ®ä¸°å¯Œ**
  ï¼šæ–‡ä»¶å†…åµŒå…¥äº†è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä¾¿äºè¿›è¡Œé¢„å¤„ç†ä¼˜åŒ–ã€‚

#### 2.5 LightGBM

**`LightGBM`**
æ˜¯ç”±å¾®è½¯å¼€å‘çš„ä¸€ç§åŸºäºæ¢¯åº¦æå‡å†³ç­–æ ‘ç®—æ³•çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œä»¥å…¶å®ç°é€Ÿåº¦å’Œå‡†ç¡®æ€§è‘—ç§°ã€‚ä¸»è¦ä¼˜ç‚¹å¦‚ä¸‹ï¼š

* **é«˜æ•ˆæ€§**
  ï¼šé‡‡ç”¨äº†GOSS (Gradient-based One-Side Sampling) å’Œ EFB (Exclusive Feature Bundling) ç­‰åˆ›æ–°æŠ€æœ¯æ¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚
* **å†…å­˜å ç”¨å°**
  ï¼šç›¸è¾ƒäºå…¶ä»–åŒç±»ç®—æ³•ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æ—¶æ›´åŠ èŠ‚çœå†…å­˜ã€‚
* **æ˜“äºä½¿ç”¨**
  ï¼šæä¾›äº†Pythonã€Rç­‰å¤šä¸ªè¯­è¨€ç‰ˆæœ¬çš„æ”¯æŒï¼Œå¹¶ä¸”é…ç½®é€‰é¡¹ä¸°å¯Œã€‚
* **æ€§èƒ½ä¼˜è¶Š**
  ï¼šåœ¨å¤šé¡¹åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†ç±»é—®é¢˜ä¸Šã€‚

#### 2.6 Optuna

**`Optuna`**
æ˜¯ä¸€æ¬¾ä¸“ä¸ºè¶…å‚æ•°ä¼˜åŒ–è®¾è®¡çš„å¼€æºè½¯ä»¶åº“ï¼Œå¯ä»¥å¸®åŠ©ç ”ç©¶äººå‘˜æˆ–å·¥ç¨‹å¸ˆæ›´å¿«åœ°æ‰¾åˆ°æœ€ä¼˜çš„æ¨¡å‹é…ç½®ã€‚ç‰¹è‰²åŠŸèƒ½åŒ…æ‹¬ï¼š

* **ç®€å•æ˜“ç”¨**
  ï¼šåªéœ€å‡ è¡Œä»£ç å³å¯å¯åŠ¨æœç´¢è¿‡ç¨‹ã€‚
* **çµæ´»æ€§é«˜**
  ï¼šæ”¯æŒå®šä¹‰å¤æ‚çš„æœç´¢ç©ºé—´ä»¥åŠè‡ªå®šä¹‰ç›®æ ‡å‡½æ•°ã€‚
* **å¯è§†åŒ–å¥½**
  ï¼šå†…ç½®äº†ä¸°å¯Œçš„å›¾è¡¨ç»˜åˆ¶åŠŸèƒ½ï¼Œä¾¿äºè§‚å¯Ÿä¼˜åŒ–è¿‡ç¨‹ã€‚
* **å¤šåç«¯æ”¯æŒ**
  ï¼šæ—¢å¯ä»¥å•ç‹¬è¿è¡Œä¹Ÿå¯ä»¥é›†æˆåˆ°ç°æœ‰çš„æœºå™¨å­¦ä¹ æµæ°´çº¿ä¸­ã€‚

#### 2.7 TA-Lib

**`TA-Lib (Technical Analysis Library)`**
æ˜¯ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¼€æºæŠ€æœ¯åˆ†æåº“ï¼Œä¸»è¦ç”¨äºé‡‘èå¸‚åœºçš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€‚å…¶ç‰¹ç‚¹åŒ…æ‹¬ï¼š

* **å…¨é¢çš„æŠ€æœ¯æŒ‡æ ‡**
  ï¼šæä¾›äº†è¶…è¿‡150ç§å¸¸ç”¨çš„æŠ€æœ¯åˆ†ææŒ‡æ ‡ï¼Œå¦‚ç§»åŠ¨å¹³å‡çº¿ã€MACDã€RSIç­‰ã€‚
* **é«˜æ€§èƒ½**
  ï¼šåº•å±‚å®ç°æ˜¯ç”¨Cè¯­è¨€ç¼–å†™çš„ï¼Œç¡®ä¿äº†è®¡ç®—é€Ÿåº¦ã€‚
* **å¤šè¯­è¨€æ”¯æŒ**
  ï¼šé™¤äº†åŸç”Ÿçš„Cè¯­è¨€å¤–ï¼Œè¿˜æä¾›äº†Pythonã€Javaã€C#ç­‰å¤šç§è¯­è¨€çš„ç»‘å®šã€‚
* **æ˜“äºé›†æˆ**
  ï¼šå¯ä»¥å¾ˆå®¹æ˜“åœ°ä¸Pandasç­‰æ•°æ®å¤„ç†åº“ç»“åˆä½¿ç”¨ï¼Œé€‚ç”¨äºå„ç§é‡‘èåˆ†æå’Œäº¤æ˜“ç­–ç•¥å¼€å‘ã€‚
* **ç¤¾åŒºæ´»è·ƒ**
  ï¼šæ‹¥æœ‰æ´»è·ƒçš„ç”¨æˆ·ç¤¾åŒºå’Œä¸°å¯Œçš„æ–‡æ¡£èµ„æºï¼Œä¾¿äºå­¦ä¹ å’Œä½¿ç”¨ã€‚

### 3. é¡¹ç›®ç»“æ„

```text
project_root/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data.parquet
â”‚   â”œâ”€â”€ selected_features.parquet
â”‚   â””â”€â”€ target.parquet
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ final_model.txt
â”‚   â”œâ”€â”€ model_training.py
â”‚   â”œâ”€â”€ strategy_backtesting.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

```

#### è¯¦ç»†è¯´æ˜

* **`data/`**
  ï¼šå­˜æ”¾æ•°æ®æ–‡ä»¶çš„ç›®å½•ã€‚

  + `raw_data.parquet`
    ï¼šä» Tushare è·å–çš„åŸå§‹è‚¡ç¥¨å†å²æ—¥çº¿æ•°æ®ã€‚
  + `selected_features.parquet`
    ï¼šç»è¿‡ç‰¹å¾å·¥ç¨‹å¤„ç†åé€‰æ‹©çš„ç‰¹å¾æ•°æ®ã€‚
  + `target.parquet`
    ï¼šç›®æ ‡å˜é‡æ•°æ®ï¼ˆæœªæ¥æ”¶ç›Šç‡ï¼‰ã€‚
* **`src/`**
  ï¼šå­˜æ”¾æºä»£ç æ–‡ä»¶çš„ç›®å½•ã€‚

  + `data_loader.py`
    ï¼šç”¨äºä» Tushare è·å–è‚¡ç¥¨å†å²æ—¥çº¿æ•°æ®å¹¶ä¿å­˜ä¸º Parquet æ–‡ä»¶ã€‚
  + `feature_engineering.py`
    ï¼šç”¨äºæå–æ—¶é—´åºåˆ—ç‰¹å¾å¹¶é€‰æ‹©ä¸ç›®æ ‡å˜é‡ç›¸å…³çš„ç‰¹å¾ã€‚
  + `final_model.txt`
    ï¼šè®­ç»ƒå¥½çš„ LightGBM æ¨¡å‹æ–‡ä»¶ã€‚
  + `model_training.py`
    ï¼šç”¨äºè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
  + `strategy_backtesting.py`
    ï¼šç”¨äºå›æµ‹ç­–ç•¥å¹¶è¯„ä¼°ç­–ç•¥æ€§èƒ½ã€‚
  + `utils.py`
    ï¼šåŒ…å«ä¸€äº›è¾…åŠ©å‡½æ•°ï¼Œå¦‚è®¡ç®—æœªæ¥æ”¶ç›Šç‡ã€æŠ€æœ¯æŒ‡æ ‡ç­‰ã€‚
* **`README.md`**
  ï¼šé¡¹ç›®è¯´æ˜æ–‡æ¡£ï¼ŒåŒ…å«é¡¹ç›®æ¦‚è¿°ã€å®‰è£…æ­¥éª¤ã€ä½¿ç”¨æ–¹æ³•ç­‰ä¿¡æ¯ã€‚

  ```bash
  # å®‰è£…é¡¹ç›®ä¾èµ–ï¼š
  pip install -r requirements.txt
  cd src
  # æ•°æ®è·å–
  python data_loader.py
  # ç‰¹å¾å·¥ç¨‹
  python feature_engineering.py
  # æ¨¡å‹è®­ç»ƒ
  python model_training.py
  # ç­–ç•¥å›æµ‹
  python strategy_backtesting.py

  ```
* **`requirements.txt`**
  ï¼šé¡¹ç›®ä¾èµ–åº“åˆ—è¡¨ï¼Œç”¨äºå®‰è£…æ‰€éœ€çš„ Python åŒ…ã€‚

  ```text
  tushare
  tsfresh
  lightgbm
  optuna
  pandas
  numpy
  scikit-learn
  ta-lib
  pyarrow
  matplotlib

  ```

### 3. ä»£ç å®ç°

#### 3.1 æ•°æ®åŠ è½½ ( `data_loader.py` )

```python
# src/data_loader.py
import tushare as ts

def load_stock_data(stock_code, start_date, end_date, save_path="../data/raw_data.parquet"):
    """
    ä»Tushare APIä¸‹è½½è‚¡ç¥¨å†å²æ—¥çº¿æ•°æ®å¹¶ä¿å­˜ã€‚

    :param stock_code: è‚¡ç¥¨ä»£ç 
    :param start_date: å¼€å§‹æ—¥æœŸ
    :param end_date: ç»“æŸæ—¥æœŸ
    :param save_path: ä¿å­˜è·¯å¾„
    """
    # Tushare Pro API token
    ts.set_token("your_token")  # è®¾ç½® Tushare API çš„è®¿é—®ä»¤ç‰Œ
    pro = ts.pro_api()  # åˆå§‹åŒ– Tushare Pro API å®¢æˆ·ç«¯

    df = pro.daily(ts_code=stock_code, start_date=start_date, end_date=end_date)  # è·å–æŒ‡å®šè‚¡ç¥¨åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ—¥çº¿æ•°æ®
    df.sort_values(by="trade_date", inplace=True)  # æŒ‰äº¤æ˜“æ—¥æœŸæ’åº

    df.to_parquet(
        path=save_path,
        engine="pyarrow",
        compression="snappy",
        index=False,
    )  # å°†æ•°æ®ä¿å­˜ä¸º Parquet æ–‡ä»¶ï¼Œä½¿ç”¨ Snappy å‹ç¼©

    print(f"Data saved to {save_path}")  # æ‰“å°ä¿å­˜è·¯å¾„

if __name__ == "__main__":
    load_stock_data("600519.SH", "20200101", "20240101")  # è´µå·èŒ…å°ï¼ˆ600519ï¼‰

```

#### 3.2 ç‰¹å¾å·¥ç¨‹ ( `feature_engineering.py` )

```python
# src/feature_engineering.py
import pandas as pd
import talib
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE, SelectFromModel, SelectKBest, f_regression, mutual_info_regression
from sklearn.linear_model import Lasso
from tsfresh import extract_features
from tsfresh.utilities.dataframe_functions import impute

from utils import calculate_compound_returns, calculate_future_return_class, calculate_future_returns, calculate_technical_indicator_changes

def select_features_with_rfe(features, target, n_features_to_select=10):
    """
    ä½¿ç”¨é€’å½’ç‰¹å¾æ¶ˆé™¤é€‰æ‹©ç‰¹å¾
    """
    estimator = RandomForestRegressor(n_estimators=100, random_state=42)  # åˆå§‹åŒ–éšæœºæ£®æ—å›å½’å™¨
    selector = RFE(estimator, n_features_to_select=n_features_to_select)  # åˆå§‹åŒ– RFE é€‰æ‹©å™¨
    selector = selector.fit(features, target)  # è®­ç»ƒ RFE é€‰æ‹©å™¨
    selected_features = features.iloc[:, selector.support_]  # é€‰æ‹©ç‰¹å¾
    return selected_features

def select_features_with_lasso(features, target):
    """
    ä½¿ç”¨LASSOå›å½’é€‰æ‹©ç‰¹å¾
    """
    lasso = Lasso(alpha=0.1)  # åˆå§‹åŒ– LASSO å›å½’
    lasso.fit(features, target)  # è®­ç»ƒ LASSO å›å½’
    model = SelectFromModel(lasso, prefit=True)  # åˆå§‹åŒ–ç‰¹å¾é€‰æ‹©å™¨
    selected_features = model.transform(features)  # é€‰æ‹©ç‰¹å¾
    return pd.DataFrame(selected_features, columns=features.columns[model.get_support()])

def add_technical_indicators(df):
    """
    æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    """
    df["SMA_5"] = talib.SMA(df["close"], timeperiod=5)  # 5æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    df["SMA_10"] = talib.SMA(df["close"], timeperiod=10)  # 10æ—¥ç®€å•ç§»åŠ¨å¹³å‡
    df["RSI"] = talib.RSI(df["close"], timeperiod=14)  # 14æ—¥ç›¸å¯¹å¼ºå¼±æŒ‡æ•°
    df["MACD"], df["MACD_signal"], _ = talib.MACD(df["close"], fastperiod=12, slowperiod=26, signalperiod=9)  # MACD æŒ‡æ ‡
    df["EMA_12"] = talib.EMA(df["close"], timeperiod=12)  # 12æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡
    df["EMA_26"] = talib.EMA(df["close"], timeperiod=26)  # 26æ—¥æŒ‡æ•°ç§»åŠ¨å¹³å‡
    df["ATR"] = talib.ATR(df["high"], df["low"], df["close"], timeperiod=14)  # 14æ—¥å¹³å‡çœŸå®æ³¢åŠ¨å¹…åº¦
    df["ADX"] = talib.ADX(df["high"], df["low"], df["close"], timeperiod=14)  # 14æ—¥å¹³å‡è¶‹å‘æŒ‡æ•°
    return df

def add_time_window_features(df):
    """
    æ·»åŠ æ—¶é—´çª—å£ç‰¹å¾
    """
    df["close_mean_5d"] = df["close"].rolling(window=5).mean()  # 5æ—¥æ”¶ç›˜ä»·å‡å€¼
    df["close_std_5d"] = df["close"].rolling(window=5).std()  # 5æ—¥æ”¶ç›˜ä»·æ ‡å‡†å·®
    df["close_mean_10d"] = df["close"].rolling(window=10).mean()  # 10æ—¥æ”¶ç›˜ä»·å‡å€¼
    df["close_std_10d"] = df["close"].rolling(window=10).std()  # 10æ—¥æ”¶ç›˜ä»·æ ‡å‡†å·®
    return df

def add_manual_features(df):
    """
    æ·»åŠ æ‰‹å·¥ç‰¹å¾
    """
    df["volume_change"] = df["vol"].pct_change()  # æˆäº¤é‡å˜åŒ–ç‡
    df["price_volatility"] = df["close"].rolling(window=5).std()  # 5æ—¥ä»·æ ¼æ³¢åŠ¨ç‡
    df["price_momentum"] = (df["close"] - df["close"].shift(5)) / df["close"].shift(5)  # 5æ—¥ä»·æ ¼åŠ¨é‡
    return df

def extract_and_select_features(data_path, target_variable):
    """
    æå–æ—¶é—´åºåˆ—ç‰¹å¾å¹¶é€‰æ‹©ä¸ç›®æ ‡å˜é‡ç›¸å…³çš„ç‰¹å¾ã€‚
    """
    df = pd.read_parquet(data_path)  # è¯»å– Parquet æ–‡ä»¶
    df["date"] = pd.to_datetime(df["trade_date"], format="%Y%m%d")  # å°†äº¤æ˜“æ—¥æœŸè½¬æ¢ä¸º datetime æ ¼å¼
    df.sort_values(by="date", inplace=True)  # æŒ‰æ—¥æœŸæ’åº

    df = calculate_future_returns(df, days_list=[3, 5, 7, 10])  # è®¡ç®—å¤šä¸ªæ—¶é—´çª—å£çš„æœªæ¥æ”¶ç›Šç‡
    df = calculate_compound_returns(df, days=5)  # è®¡ç®—å¤åˆæ”¶ç›Šç‡
    df = calculate_future_return_class(df, days=5)  # è®¡ç®—åˆ†ç±»ç›®æ ‡
    df = calculate_technical_indicator_changes(df, indicator="RSI", period=14, days=5)  # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å˜åŒ–
    df = add_technical_indicators(df)  # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    df = add_time_window_features(df)  # æ·»åŠ æ—¶é—´çª—å£ç‰¹å¾
    df = add_manual_features(df)  # æ·»åŠ æ‰‹å·¥ç‰¹å¾

    df.dropna(inplace=True)  # åˆ é™¤åŒ…å« NaN å€¼çš„è¡Œ
    df["id"] = range(0, len(df))  # æ·»åŠ  id åˆ—
    df.set_index("id", inplace=True)  # è®¾ç½® id ä¸ºç´¢å¼•
    df.reset_index(inplace=True)  # é‡ç½®ç´¢å¼•

    target = pd.DataFrame(df[target_variable])  # æå–ç›®æ ‡å˜é‡

    extracted_features = extract_features(df, column_id="id", column_sort="date", column_value="close")  # æå–æ—¶é—´åºåˆ—ç‰¹å¾
    impute(extracted_features)  # å¡«å……ç¼ºå¤±å€¼

    selected_features = select_features_with_lasso(extracted_features, target)  # é€‰æ‹©ç‰¹å¾

    return selected_features, target

if __name__ == "__main__":
    features, target = extract_and_select_features("../data/raw_data.parquet", "future_return_5d")  # æå–å’Œé€‰æ‹©ç‰¹å¾
    features.to_parquet("../data/selected_features.parquet", index=False)  # ä¿å­˜ç‰¹å¾
    target.to_parquet("../data/target.parquet", index=False)  # ä¿å­˜ç›®æ ‡å˜é‡

```

#### 3.3 æ¨¡å‹è®­ç»ƒ ( `model_training.py` )

```python
# src/model_training.py
import re
import lightgbm as lgb
import numpy as np
import optuna
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler

def clean_feature_names(df):
    df.columns = [re.sub(r"[^a-zA-Z0-9_]", "", col) for col in df.columns]  # æ¸…ç†ç‰¹å¾åç§°ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
    return df

def preprocess_data(features, target):
    features = features.interpolate(method="linear").fillna(features.mean())  # æ’å€¼å¡«å……ç¼ºå¤±å€¼
    scaler = StandardScaler()  # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
    features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)  # æ ‡å‡†åŒ–ç‰¹å¾
    return features, target

def objective(trial, X_train, y_train, X_val, y_val):
    params = {
        "objective": "regression",  # ç›®æ ‡å‡½æ•°
        "metric": "mse",  # è¯„ä»·æŒ‡æ ‡
        "boosting_type": "gbdt",  # æå‡ç±»å‹
        "num_leaves": trial.suggest_int("num_leaves", 2, 256),  # å¶å­èŠ‚ç‚¹æ•°
        "learning_rate": trial.suggest_loguniform("learning_rate", 1e-5, 1e-1),  # å­¦ä¹ ç‡
        "max_depth": trial.suggest_int("max_depth", 3, 15),  # æœ€å¤§æ·±åº¦
        "min_child_samples": trial.suggest_int("min_child_samples", 5, 100),  # æœ€å°æ ·æœ¬æ•°
        "subsample": trial.suggest_uniform("subsample", 0.5, 1.0),  # å­é‡‡æ ·æ¯”ä¾‹
        "colsample_bytree": trial.suggest_uniform("colsample_bytree", 0.5, 1.0),  # åˆ—é‡‡æ ·æ¯”ä¾‹
        "reg_alpha": trial.suggest_loguniform("reg_alpha", 1e-5, 1e2),  # L1 æ­£åˆ™åŒ–é¡¹
        "reg_lambda": trial.suggest_loguniform("reg_lambda", 1e-5, 1e2),  # L2 æ­£åˆ™åŒ–é¡¹
    }

    dtrain = lgb.Dataset(X_train, label=y_train)  # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    dval = lgb.Dataset(X_val, label=y_val)  # åˆ›å»ºéªŒè¯æ•°æ®é›†

    callbacks = [
        lgb.early_stopping(stopping_rounds=50, first_metric_only=True, verbose=False),  # æ—©åœå›è°ƒ
        lgb.log_evaluation(period=0),  # ç¦ç”¨æ—¥å¿—è¾“å‡º
    ]

    model = lgb.train(params, dtrain, valid_sets=[dtrain, dval], valid_names=["train", "val"], num_boost_round=1000, callbacks=callbacks)  # è®­ç»ƒæ¨¡å‹

    y_pred = model.predict(X_val)  # é¢„æµ‹éªŒè¯é›†
    mse = mean_squared_error(y_val, y_pred)  # è®¡ç®—å‡æ–¹è¯¯å·®

    return mse

def train_model(features_path, target_path, n_trials=100):
    features = pd.read_parquet(features_path)  # è¯»å–ç‰¹å¾
    target = pd.read_parquet(target_path)  # è¯»å–ç›®æ ‡å˜é‡

    features = clean_feature_names(features)  # æ¸…ç†ç‰¹å¾åç§°
    features, target = preprocess_data(features, target)  # é¢„å¤„ç†æ•°æ®

    X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)  # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†

    study = optuna.create_study(direction="minimize")  # åˆ›å»º Optuna ç ”ç©¶
    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=n_trials)  # ä¼˜åŒ–è¶…å‚æ•°

    best_params = study.best_params  # è·å–æœ€ä¼˜å‚æ•°
    best_mse = study.best_value  # è·å–æœ€ä¼˜ MSE

    print(f"Best MSE: {best_mse}")  # æ‰“å°æœ€ä¼˜ MSE
    print(f"Best Parameters: {best_params}")  # æ‰“å°æœ€ä¼˜å‚æ•°

    return best_params

def train_final_model(best_params, features, target):
    features, target = preprocess_data(features, target)  # é¢„å¤„ç†æ•°æ®
    kf = KFold(n_splits=5, shuffle=True, random_state=42)  # åˆå§‹åŒ– K-Fold äº¤å‰éªŒè¯
    mse_scores = []
    for train_index, val_index in kf.split(features):  # è¿›è¡Œäº¤å‰éªŒè¯
        X_train, X_val = features.iloc[train_index], features.iloc[val_index]
        y_train, y_val = target.iloc[train_index], target.iloc[val_index]
        dtrain = lgb.Dataset(X_train, label=y_train)  # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        dval = lgb.Dataset(X_val, label=y_val)  # åˆ›å»ºéªŒè¯æ•°æ®é›†
        callbacks = [
            lgb.early_stopping(stopping_rounds=50, first_metric_only=True, verbose=False),  # æ—©åœå›è°ƒ
            lgb.log_evaluation(period=0),  # ç¦ç”¨æ—¥å¿—è¾“å‡º
        ]
        model = lgb.train(best_params, dtrain, valid_sets=[dtrain, dval], valid_names=["train", "val"], num_boost_round=1000, callbacks=callbacks)  # è®­ç»ƒæ¨¡å‹
        y_pred = model.predict(X_val)  # é¢„æµ‹éªŒè¯é›†
        mse = mean_squared_error(y_val, y_pred)  # è®¡ç®—å‡æ–¹è¯¯å·®
        mse_scores.append(mse)
    avg_mse = np.mean(mse_scores)  # è®¡ç®—å¹³å‡ MSE
    print(f"Final Model Average MSE: {avg_mse}")  # æ‰“å°å¹³å‡ MSE
    return model

if __name__ == "__main__":
    features_path = "../data/selected_features.parquet"
    target_path = "../data/target.parquet"

    best_params = train_model(features_path, target_path, n_trials=100)  # è®­ç»ƒæ¨¡å‹å¹¶ä¼˜åŒ–è¶…å‚æ•°

    features = pd.read_parquet(features_path)  # è¯»å–ç‰¹å¾
    target = pd.read_parquet(target_path)  # è¯»å–ç›®æ ‡å˜é‡

    features = clean_feature_names(features)  # æ¸…ç†ç‰¹å¾åç§°

    final_model = train_final_model(best_params, features, target)  # è®­ç»ƒæœ€ç»ˆæ¨¡å‹

    final_model.save_model("final_model.txt")  # ä¿å­˜æ¨¡å‹

```

#### 3.4 ç­–ç•¥å›æµ‹ ( `strategy_backtesting.py` )

```python
# strategy_backtesting.py
import re
import lightgbm as lgb
import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

def clean_feature_names(df):
    df.columns = [re.sub(r"[^a-zA-Z0-9_]", "", col) for col in df.columns]  # æ¸…ç†ç‰¹å¾åç§°ï¼Œå»é™¤ç‰¹æ®Šå­—ç¬¦
    return df

def preprocess_data(features, target=None):
    features = features.fillna(features.mean())  # å¡«å……ç¼ºå¤±å€¼
    scaler = StandardScaler()  # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨
    features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)  # æ ‡å‡†åŒ–ç‰¹å¾

    if target is not None:
        return features, target
    else:
        return features

def load_model(model_path):
    model = lgb.Booster(model_file=model_path)  # åŠ è½½ LightGBM æ¨¡å‹
    return model

def backtest_strategy(features, target, model, threshold=0.5):
    features = preprocess_data(features)  # é¢„å¤„ç†ç‰¹å¾

    predictions = model.predict(features)  # é¢„æµ‹

    print(f"Predictions: Min={predictions.min()}, Max={predictions.max()}, Mean={predictions.mean()}")  # æ‰“å°é¢„æµ‹å€¼ç»Ÿè®¡ä¿¡æ¯

    mse = mean_squared_error(target, predictions)  # è®¡ç®—å‡æ–¹è¯¯å·®
    mae = mean_absolute_error(target, predictions)  # è®¡ç®—å¹³å‡ç»å¯¹è¯¯å·®
    r2 = r2_score(target, predictions)  # è®¡ç®— RÂ² åˆ†æ•°

    print(f"Mean Squared Error (MSE): {mse:.6f}")  # æ‰“å°å‡æ–¹è¯¯å·®
    print(f"Mean Absolute Error (MAE): {mae:.6f}")  # æ‰“å°å¹³å‡ç»å¯¹è¯¯å·®
    print(f"RÂ² Score: {r2:.4f}")  # æ‰“å° RÂ² åˆ†æ•°

    signals = (predictions > threshold).astype(int)  # ç”Ÿæˆäº¤æ˜“ä¿¡å·

    print(f"Signals: Sum={signals.sum()}, Count={len(signals)}, Non-zero count={(signals > 0).sum()}")  # æ‰“å°ä¿¡å·ç»Ÿè®¡ä¿¡æ¯

    daily_returns = target * signals  # è®¡ç®—æ¯æ—¥æ”¶ç›Š

    daily_returns_series = pd.Series(daily_returns)  # å°†æ¯æ—¥æ”¶ç›Šè½¬æ¢ä¸º Pandas Series

    cumulative_returns = (1 + daily_returns_series).cumprod() - 1  # è®¡ç®—ç´¯ç§¯æ”¶ç›Š

    total_return = cumulative_returns.iloc[-1]  # è®¡ç®—æ€»æ”¶ç›Š

    sharpe_ratio = np.sqrt(252) * (daily_returns_series.mean() / daily_returns_series.std())  # è®¡ç®—å¤æ™®æ¯”ç‡

    print(f"Total Return: {total_return:.4f}")  # æ‰“å°æ€»æ”¶ç›Š
    print(f"Sharpe Ratio: {sharpe_ratio:.4f}")  # æ‰“å°å¤æ™®æ¯”ç‡

    return cumulative_returns, total_return, sharpe_ratio

if __name__ == "__main__":
    features_path = "../data/selected_features.parquet"
    target_path = "../data/target.parquet"

    features = pd.read_parquet(features_path)  # è¯»å–ç‰¹å¾
    target = pd.read_parquet(target_path).values.flatten()  # è¯»å–ç›®æ ‡å˜é‡

    features = clean_feature_names(features)  # æ¸…ç†ç‰¹å¾åç§°

    model_path = "final_model.txt"
    model = load_model(model_path)  # åŠ è½½æ¨¡å‹

    cumulative_returns, total_return, sharpe_ratio = backtest_strategy(features, target, model, threshold=0.001)  # å›æµ‹ç­–ç•¥

    import matplotlib.pyplot as plt

    plt.figure(figsize=(14, 7))
    plt.plot(cumulative_returns, label="Cumulative Returns")  # ç»˜åˆ¶ç´¯ç§¯æ”¶ç›Šæ›²çº¿
    plt.title("Cumulative Returns Over Time")
    plt.xlabel("Time")
    plt.ylabel("Cumulative Returns")
    plt.legend()
    plt.show()

```

#### 3.5 è¾…åŠ©å‡½æ•° ( `utils.py` )

```python
# src/utils.py
import numpy as np
import pandas as pd
import talib


def calculate_future_returns(df, days_list=[3, 5, 7, 10]):
    """
    è®¡ç®—å¤šä¸ªæ—¶é—´çª—å£çš„æœªæ¥æ”¶ç›Šç‡ã€‚

    :param df: åŒ…å«æ”¶ç›˜ä»·çš„æ—¶é—´åºåˆ—æ•°æ®
    :param days_list: æœªæ¥å¤©æ•°åˆ—è¡¨
    :return: åŒ…å«æœªæ¥æ”¶ç›Šç‡çš„DataFrame
    """
    for days in days_list:
        target_variable = f"future_return_{days}d"
        df[target_variable] = df["close"].pct_change(periods=-days).shift(days)
    
    df.dropna(inplace=True)
    return df


def calculate_compound_returns(df, days=5):
    """
    è®¡ç®—å¤åˆæ”¶ç›Šç‡ã€‚
    :param df: åŒ…å«æ”¶ç›˜ä»·çš„æ—¶é—´åºåˆ—æ•°æ®
    :param days: æœªæ¥å¤©æ•°
    :return: åŒ…å«å¤åˆæ”¶ç›Šç‡çš„DataFrame
    """
    target_variable = f"compound_return_{days}d"
    df[target_variable] = (df["close"].shift(-days) / df["close"]) - 1
    df.dropna(subset=[target_variable], inplace=True)
    return df


def calculate_future_return_class(df, days=5):
    """
    è®¡ç®—æœªæ¥æ”¶ç›Šç‡å¹¶å°†å…¶è½¬æ¢ä¸ºåˆ†ç±»ç›®æ ‡ã€‚
    :param df: åŒ…å«æ”¶ç›˜ä»·çš„æ—¶é—´åºåˆ—æ•°æ®
    :param days: æœªæ¥å¤©æ•°
    :return: åŒ…å«åˆ†ç±»ç›®æ ‡çš„DataFrame
    """
    target_variable = f"future_return_{days}d"
    df[target_variable] = df["close"].pct_change(periods=-days).shift(days)

    # å°†æ”¶ç›Šç‡è½¬æ¢ä¸ºåˆ†ç±»ç›®æ ‡
    df[f"{target_variable}_class"] = pd.cut(
        df[target_variable],
        bins=[-np.inf, -0.01, 0.01, np.inf],
        labels=["down", "flat", "up"],
    )

    df.dropna(subset=[f"{target_variable}_class"], inplace=True)
    return df


def calculate_technical_indicator_changes(df, indicator="RSI", period=14, days=5):
    """
    è®¡ç®—æŠ€æœ¯æŒ‡æ ‡çš„å˜åŒ–ã€‚
    :param df: åŒ…å«æ”¶ç›˜ä»·çš„æ—¶é—´åºåˆ—æ•°æ®
    :param indicator: æŠ€æœ¯æŒ‡æ ‡åç§°
    :param period: æŠ€æœ¯æŒ‡æ ‡çš„æ—¶é—´å‘¨æœŸ
    :param days: æœªæ¥å¤©æ•°
    :return: åŒ…å«æŠ€æœ¯æŒ‡æ ‡å˜åŒ–çš„DataFrame
    """
    if indicator == "RSI":
        df[f"{indicator}_{period}"] = talib.RSI(df["close"], timeperiod=period)
    elif indicator == "MACD":
        macd, signal, _ = talib.MACD(
            df["close"], fastperiod=12, slowperiod=26, signalperiod=9
        )
        df[f"{indicator}_{period}"] = macd - signal
    else:
        raise ValueError(f"Unsupported indicator: {indicator}")

    target_variable = f"{indicator}_change_{days}d"
    df[target_variable] = (
        df[f"{indicator}_{period}"].pct_change(periods=-days).shift(days)
    )

    df.dropna(subset=[target_variable], inplace=True)
    return df

```

### 4. å…³é”®æŠ€æœ¯å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | è®¾ç½®å€¼ | æ³¨æ„äº‹é¡¹ |
| --- | --- | --- | --- |
| `num_leaves` | å¶å­èŠ‚ç‚¹æ•° | é€šå¸¸è®¾ç½®åœ¨ 32 åˆ° 256 ä¹‹é—´ | è¿‡å¤šä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè¿‡å°‘ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆ |
| `learning_rate` | å­¦ä¹ ç‡ | é€šå¸¸è®¾ç½®åœ¨ 0.01 åˆ° 0.3 ä¹‹é—´ | è¾ƒå°çš„å­¦ä¹ ç‡éœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°ï¼Œè¾ƒå¤§çš„å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´ä¸æ”¶æ•› |
| `max_depth` | æœ€å¤§æ·±åº¦ | é€šå¸¸è®¾ç½®åœ¨ 3 åˆ° 15 ä¹‹é—´ | è¿‡æ·±ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè¿‡æµ…ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆ |
| `min_child_samples` | æœ€å°æ ·æœ¬æ•° | é€šå¸¸è®¾ç½®åœ¨ 5 åˆ° 100 ä¹‹é—´ | è¿‡å°ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œè¿‡å¤§å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆ |
| `subsample` | å­é‡‡æ ·æ¯”ä¾‹ | é€šå¸¸è®¾ç½®åœ¨ 0.5 åˆ° 1.0 ä¹‹é—´ | è¾ƒå°çš„æ¯”ä¾‹å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼Œä½†ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ |
| `colsample_bytree` | åˆ—é‡‡æ ·æ¯”ä¾‹ | é€šå¸¸è®¾ç½®åœ¨ 0.5 åˆ° 1.0 ä¹‹é—´ | è¾ƒå°çš„æ¯”ä¾‹å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼Œä½†ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ |
| `reg_alpha` | L1 æ­£åˆ™åŒ–é¡¹ | é€šå¸¸è®¾ç½®åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´ | è¾ƒå¤§çš„å€¼å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼Œä½†å¯èƒ½é™ä½æ¨¡å‹æ€§èƒ½ |
| `reg_lambda` | L2 æ­£åˆ™åŒ–é¡¹ | é€šå¸¸è®¾ç½®åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´ | è¾ƒå¤§çš„å€¼å¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼Œä½†å¯èƒ½é™ä½æ¨¡å‹æ€§èƒ½ |

### 5. ç­–ç•¥è¯„ä»·æŒ‡æ ‡

åœ¨é‡åŒ–æŠ•èµ„ç­–ç•¥çš„è¯„ä¼°ä¸­ï¼Œè¯„ä»·æŒ‡æ ‡æ˜¯è¡¡é‡ç­–ç•¥æ€§èƒ½çš„å…³é”®å·¥å…·ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸ç”¨çš„è¯„ä»·æŒ‡æ ‡åŠå…¶æ„ä¹‰ï¼š

#### 5.1 é¢„æµ‹å‡†ç¡®æ€§æŒ‡æ ‡

* **å‡æ–¹è¯¯å·® (MSE)**
  ï¼šè¡¡é‡é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„å·®å¼‚ã€‚è¾ƒä½çš„ MSE è¡¨ç¤ºæ¨¡å‹é¢„æµ‹æ›´å‡†ç¡®ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

  MSE
  =
  1
  n
  âˆ‘
  i
  =
  1
  n
  (
  y
  i
  âˆ’
  y
  ^
  i
  )
  2
  \text{MSE} = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2






  MSE



  =
















  n











  1

  â€‹













  i

  =

  1





  âˆ‘






  n

  â€‹


  (


  y









  i

  â€‹




  âˆ’













  y





  ^

  â€‹










  i

  â€‹



  )









  2
    
  å…¶ä¸­ï¼Œ

  y
  i
  y_i






  y









  i

  â€‹

  æ˜¯å®é™…å€¼ï¼Œ

  y
  ^
  i
  \hat{y}_i













  y





  ^

  â€‹










  i

  â€‹

  æ˜¯é¢„æµ‹å€¼ï¼Œ

  n
  n





  n
  æ˜¯æ ·æœ¬æ•°é‡ã€‚
* **å¹³å‡ç»å¯¹è¯¯å·® (MAE)**
  ï¼šè¡¡é‡é¢„æµ‹å€¼ä¸å®é™…å€¼ä¹‹é—´çš„ç»å¯¹å·®å¼‚ã€‚è¾ƒä½çš„ MAE è¡¨ç¤ºæ¨¡å‹é¢„æµ‹æ›´å‡†ç¡®ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

  MAE
  =
  1
  n
  âˆ‘
  i
  =
  1
  n
  âˆ£
  y
  i
  âˆ’
  y
  ^
  i
  âˆ£
  \text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|






  MAE



  =
















  n











  1

  â€‹













  i

  =

  1





  âˆ‘






  n

  â€‹




  âˆ£


  y









  i

  â€‹




  âˆ’













  y





  ^

  â€‹










  i

  â€‹


  âˆ£
    
  å…¶ä¸­ï¼Œ

  y
  i
  y_i






  y









  i

  â€‹

  æ˜¯å®é™…å€¼ï¼Œ

  y
  ^
  i
  \hat{y}_i













  y





  ^

  â€‹










  i

  â€‹

  æ˜¯é¢„æµ‹å€¼ï¼Œ

  n
  n





  n
  æ˜¯æ ·æœ¬æ•°é‡ã€‚
* **RÂ² åˆ†æ•°**
  ï¼šè¡¡é‡æ¨¡å‹è§£é‡Šæ•°æ®å˜å¼‚æ€§çš„èƒ½åŠ›ã€‚è¾ƒé«˜çš„ RÂ² åˆ†æ•°è¡¨ç¤ºæ¨¡å‹è§£é‡Šèƒ½åŠ›å¼ºã€‚RÂ² çš„å–å€¼èŒƒå›´ä¸º [0, 1]ï¼Œå€¼è¶Šæ¥è¿‘ 1 è¡¨ç¤ºæ¨¡å‹æ‹Ÿåˆæ•ˆæœè¶Šå¥½ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

  R
  2
  =
  1
  âˆ’
  âˆ‘
  i
  =
  1
  n
  (
  y
  i
  âˆ’
  y
  ^
  i
  )
  2
  âˆ‘
  i
  =
  1
  n
  (
  y
  i
  âˆ’
  y
  Ë‰
  )
  2
  R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}






  R









  2



  =





  1



  âˆ’

















  âˆ‘










  i

  =

  1






  n

  â€‹


  (


  y









  i

  â€‹




  âˆ’










  y





  Ë‰

  â€‹



  )









  2












  âˆ‘










  i

  =

  1






  n

  â€‹


  (


  y









  i

  â€‹




  âˆ’











  y





  ^

  â€‹










  i

  â€‹



  )









  2

  â€‹

    
  å…¶ä¸­ï¼Œ

  y
  i
  y_i






  y









  i

  â€‹

  æ˜¯å®é™…å€¼ï¼Œ

  y
  ^
  i
  \hat{y}_i













  y





  ^

  â€‹










  i

  â€‹

  æ˜¯æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œ

  y
  Ë‰
  \bar{y}












  y





  Ë‰

  â€‹

  æ˜¯å®é™…å€¼çš„å‡å€¼ï¼Œ

  n
  n





  n
  æ˜¯æ ·æœ¬æ•°é‡ã€‚

#### 5.2 é£é™©ä¸æ”¶ç›ŠæŒ‡æ ‡

* **å¤æ™®æ¯”ç‡**
  ï¼šè¡¡é‡å•ä½é£é™©çš„è¶…é¢å›æŠ¥ï¼Œè¶Šé«˜è¶Šå¥½ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

  å¤æ™®æ¯”ç‡
  =
  R
  p
  âˆ’
  R
  f
  Ïƒ
  p
  \text{å¤æ™®æ¯”ç‡} = \frac{R_p - R_f}{\sigma_p}






  å¤æ™®æ¯”ç‡



  =

















  Ïƒ









  p

  â€‹













  R









  p

  â€‹




  âˆ’




  R









  f

  â€‹


  â€‹

    
  å…¶ä¸­ï¼Œ

  R
  p
  R_p






  R









  p

  â€‹

  æ˜¯ç­–ç•¥çš„é¢„æœŸæ”¶ç›Šç‡ï¼Œ

  R
  f
  R_f






  R









  f

  â€‹

  æ˜¯æ— é£é™©åˆ©ç‡ï¼Œ

  Ïƒ
  p
  \sigma_p






  Ïƒ









  p

  â€‹

  æ˜¯ç­–ç•¥æ”¶ç›Šç‡çš„æ ‡å‡†å·®ã€‚è¾ƒé«˜çš„å¤æ™®æ¯”ç‡è¡¨ç¤ºç­–ç•¥åœ¨æ‰¿æ‹…ç›¸åŒé£é™©çš„æƒ…å†µä¸‹è·å¾—äº†æ›´é«˜çš„å›æŠ¥ã€‚
* **ç´¯ç§¯æ”¶ç›Š**
  ï¼šåæ˜ ç­–ç•¥çš„æ•´ä½“è¡¨ç°ã€‚é€šè¿‡è®¡ç®—ç­–ç•¥åœ¨æ¯ä¸ªæ—¶é—´ç‚¹çš„å‡€å€¼å˜åŒ–ï¼Œç»˜åˆ¶ç´¯ç§¯æ”¶ç›Šæ›²çº¿ã€‚å¹³æ»‘ä¸Šå‡çš„ç´¯ç§¯æ”¶ç›Šæ›²çº¿è¡¨ç¤ºç­–ç•¥è¡¨ç°è‰¯å¥½ï¼Œä¸”æ³¢åŠ¨è¾ƒå°ã€‚
* **æ€»æ”¶ç›Š**
  ï¼šåæ˜ ç­–ç•¥çš„æ€»ä½“ç›ˆåˆ©èƒ½åŠ›ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š

  æ€»æ”¶ç›Š
  =
  æœ€ç»ˆèµ„äº§ä»·å€¼
  âˆ’
  åˆå§‹èµ„äº§ä»·å€¼
  åˆå§‹èµ„äº§ä»·å€¼
  Ã—
  100
  %
  \text{æ€»æ”¶ç›Š} = \frac{\text{æœ€ç»ˆèµ„äº§ä»·å€¼} - \text{åˆå§‹èµ„äº§ä»·å€¼}}{\text{åˆå§‹èµ„äº§ä»·å€¼}} \times 100\%






  æ€»æ”¶ç›Š



  =

















  åˆå§‹èµ„äº§ä»·å€¼












  æœ€ç»ˆèµ„äº§ä»·å€¼



  âˆ’




  åˆå§‹èµ„äº§ä»·å€¼

  â€‹




  Ã—





  100%
    
  è¾ƒé«˜çš„æ€»æ”¶ç›Šè¡¨ç¤ºç­–ç•¥ç›ˆåˆ©èƒ½åŠ›å¼ºã€‚
* **æœ€å¤§å›æ’¤ (MDD)**
  ï¼šè¡¡é‡ç­–ç•¥åœ¨é€‰å®šå‘¨æœŸå†…å¯èƒ½å‡ºç°çš„æœ€å¤§æŸå¤±ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

  MDD
  =
  max
  â¡
  (
  é«˜ç‚¹å‡€å€¼
  âˆ’
  ä½ç‚¹å‡€å€¼
  é«˜ç‚¹å‡€å€¼
  )
  \text{MDD} = \max\left(\frac{\text{é«˜ç‚¹å‡€å€¼} - \text{ä½ç‚¹å‡€å€¼}}{\text{é«˜ç‚¹å‡€å€¼}}\right)






  MDD



  =





  max





  (













  é«˜ç‚¹å‡€å€¼












  é«˜ç‚¹å‡€å€¼



  âˆ’




  ä½ç‚¹å‡€å€¼

  â€‹



  )
    
  è¾ƒä½çš„æœ€å¤§å›æ’¤è¡¨ç¤ºç­–ç•¥æŠ—é£é™©èƒ½åŠ›è¾ƒå¼ºã€‚
* **å¡ç›æ¯”ç‡ (Calmar Ratio)**
  ï¼šè¡¡é‡å•ä½æœ€å¤§å›æ’¤çš„å¤åˆå¹´åŒ–æ”¶ç›Šç‡ï¼Œè¶Šé«˜è¶Šå¥½ã€‚è®¡ç®—å…¬å¼ä¸ºï¼š

  å¡ç›æ¯”ç‡
  =
  å¤åˆå¹´åŒ–æ”¶ç›Šç‡
  æœ€å¤§å›æ’¤
  \text{å¡ç›æ¯”ç‡} = \frac{\text{å¤åˆå¹´åŒ–æ”¶ç›Šç‡}}{\text{æœ€å¤§å›æ’¤}}






  å¡ç›æ¯”ç‡



  =

















  æœ€å¤§å›æ’¤












  å¤åˆå¹´åŒ–æ”¶ç›Šç‡

  â€‹
* **èƒœç‡**
  ï¼šè¡¡é‡ç­–ç•¥åœ¨äº¤æ˜“ä¸­ç›ˆåˆ©çš„é¢‘ç‡ï¼Œè®¡ç®—å…¬å¼ä¸ºï¼š

  èƒœç‡
  =
  ç›ˆåˆ©äº¤æ˜“æ¬¡æ•°
  æ€»äº¤æ˜“æ¬¡æ•°
  Ã—
  100
  %
  \text{èƒœç‡} = \frac{\text{ç›ˆåˆ©äº¤æ˜“æ¬¡æ•°}}{\text{æ€»äº¤æ˜“æ¬¡æ•°}} \times 100\%






  èƒœç‡



  =

















  æ€»äº¤æ˜“æ¬¡æ•°












  ç›ˆåˆ©äº¤æ˜“æ¬¡æ•°

  â€‹




  Ã—





  100%
    
  è¾ƒé«˜çš„èƒœç‡è¡¨ç¤ºç­–ç•¥åœ¨å¤šæ•°äº¤æ˜“ä¸­èƒ½å¤Ÿè·åˆ©ã€‚

#### 5.3 å¦‚ä½•åˆ¤æ–­æ˜¯å¦è¾¾åˆ°äº†é¢„æœŸ

* **MSE å’Œ MAE**
  ï¼šå¦‚æœè¿™ä¸¤ä¸ªæŒ‡æ ‡ä½äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ MSE < 0.05ï¼ŒMAE < 0.1ï¼‰ï¼Œåˆ™è®¤ä¸ºæ¨¡å‹é¢„æµ‹å‡†ç¡®ã€‚
* **RÂ² åˆ†æ•°**
  ï¼šå¦‚æœ RÂ² åˆ†æ•°é«˜äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ 0.8 æˆ–æ›´é«˜ï¼‰ï¼Œåˆ™è®¤ä¸ºæ¨¡å‹è§£é‡Šèƒ½åŠ›å¼ºã€‚
* **å¤æ™®æ¯”ç‡**
  ï¼šå¦‚æœå¤æ™®æ¯”ç‡é«˜äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ 1.0 æˆ–æ›´é«˜ï¼‰ï¼Œåˆ™è®¤ä¸ºç­–ç•¥åœ¨æ‰¿æ‹…ç›¸åŒé£é™©çš„æƒ…å†µä¸‹è·å¾—äº†æ›´é«˜çš„å›æŠ¥ã€‚
* **ç´¯ç§¯æ”¶ç›Š**
  ï¼šå¦‚æœç´¯ç§¯æ”¶ç›Šæ›²çº¿å¹³æ»‘ä¸Šå‡ä¸”æ²¡æœ‰å¤§å¹…æ³¢åŠ¨ï¼Œåˆ™è®¤ä¸ºç­–ç•¥è¡¨ç°ç¨³å®šã€‚
* **æ€»æ”¶ç›Š**
  ï¼šå¦‚æœæ€»æ”¶ç›Šé«˜äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ 10% æˆ–æ›´é«˜ï¼‰ï¼Œåˆ™è®¤ä¸ºç­–ç•¥ç›ˆåˆ©èƒ½åŠ›å¼ºã€‚
* **æœ€å¤§å›æ’¤**
  ï¼šå¦‚æœæœ€å¤§å›æ’¤ä½äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ 10% æˆ–æ›´ä½ï¼‰ï¼Œåˆ™è®¤ä¸ºç­–ç•¥æŠ—é£é™©èƒ½åŠ›è¾ƒå¼ºã€‚
* **å¡ç›æ¯”ç‡**
  ï¼šå¦‚æœå¡ç›æ¯”ç‡é«˜äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ 1.0 æˆ–æ›´é«˜ï¼‰ï¼Œåˆ™è®¤ä¸ºç­–ç•¥åœ¨æ§åˆ¶å›æ’¤æ–¹é¢è¡¨ç°è‰¯å¥½ã€‚
* **èƒœç‡**
  ï¼šå¦‚æœèƒœç‡é«˜äºé¢„æœŸé˜ˆå€¼ï¼ˆå¦‚ 50% æˆ–æ›´é«˜ï¼‰ï¼Œåˆ™è®¤ä¸ºç­–ç•¥åœ¨å¤šæ•°äº¤æ˜“ä¸­èƒ½å¤Ÿè·åˆ©ã€‚

> **é£é™©æç¤ºä¸å…è´£å£°æ˜**
>   
> æœ¬æ–‡å†…å®¹åŸºäºå…¬å¼€ä¿¡æ¯ç ”ç©¶æ•´ç†ï¼Œä¸æ„æˆä»»ä½•å½¢å¼çš„æŠ•èµ„å»ºè®®ã€‚å†å²è¡¨ç°ä¸åº”ä½œä¸ºæœªæ¥æ”¶ç›Šä¿è¯ï¼Œå¸‚åœºå­˜åœ¨ä¸å¯é¢„è§çš„æ³¢åŠ¨é£é™©ã€‚æŠ•èµ„è€…éœ€ç»“åˆè‡ªèº«è´¢åŠ¡çŠ¶å†µåŠé£é™©æ‰¿å—èƒ½åŠ›ç‹¬ç«‹å†³ç­–ï¼Œå¹¶è‡ªè¡Œæ‰¿æ‹…äº¤æ˜“ç»“æœã€‚ä½œè€…åŠå‘å¸ƒæ–¹ä¸å¯¹ä»»ä½•ä¾æ®æœ¬æ–‡æ“ä½œå¯¼è‡´çš„æŸå¤±æ‰¿æ‹…æ³•å¾‹è´£ä»»ã€‚å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„é¡»è°¨æ…ã€‚