---
layout: post
title: "混元图生视频-腾讯混元开源的图生视频模型"
date: 2025-03-07 16:35:49 +0800
description: "混元图生视频是腾讯混元推出的开源图生视频模型，用户可以通过上传一张图片进行简短描述，让图片动起来生成5秒的短视频。输入图像首先经过预训练的多模态大型语言模型(MLLM)处理，生成语义图像token，然后与视频潜在token拼接，实现跨模态的全注意力计算。为解决用户提示词的语言风格和长度多变性问题，HunyuanVideo-12V引入了提示词重写模块，能将用户输入的提示词转换为模型更易理解的格式，提高生成效果。用户只需上传一张图片输入简短描述，模型可将静态图片转化为5秒的短视频，同时支持自动生成背景音效。"
keywords: "混元图生视频 低显存可用"
categories: ['Ai']
tags: ['音视频']
artid: "146099339"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146099339
    alt: "混元图生视频-腾讯混元开源的图生视频模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146099339
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146099339
cover: https://bing.ee123.net/img/rand?artid=146099339
image: https://bing.ee123.net/img/rand?artid=146099339
img: https://bing.ee123.net/img/rand?artid=146099339
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     混元图生视频-腾讯混元开源的图生视频模型
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h3>
     混元图生视频是什么
    </h3>
    <p>
     混元图生视频是腾讯混元推出的开源图生视频模型，用户可以通过上传一张图片进行简短描述，让图片动起来生成5秒的短视频。模型支持对口型、动作驱动和背景音效自动生成等功能。模型适用于写实、动漫和CGI等多种角色和场景，总参数量为130亿。腾讯混元图生视频模型已在腾讯云上线，用户可通过混元AI视频官网使用体验。混元图生视频模型在Github、Huggingface等主流开发者社区开源，包含权重、推理代码和LORA训练代码，开发者可以基于此训练专属LORA等衍生模型。
    </p>
    <p style="text-align:center">
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/d22fc1b4fac9007f4a8cef15fd8ff26c.webp?x-oss-process=image/format,png"/>
    </p>
    <h3>
     混元图生视频的主要功能
    </h3>
    <p>
     图生视频生成:
    </p>
    <p>
     用户只需上传一张图片输入简短描述，模型可将静态图片转化为5秒的短视频，同时支持自动生成背景音效。
    </p>
    <p>
     音频驱动功能:
    </p>
    <p>
     用户可以上传人物图片，输入文本或音频，模型能精准匹配嘴型，让图片中的人物“说话"或"唱歌”，呈现符合语气的面部表情。
     <br/>
     <br/>
     动作驱动功能:
    </p>
    <p>
     用户上传图片后，选择动作模板，模型可让图片中的人物完成跳舞、挥手、做体操等动作，适用于短视频创作、游戏角色动画和影视制作。
     <br/>
     <br/>
     高质量视频输出:支持2K高清画质，适用于写实、动漫和CGI等多种角色和场景。
    </p>
    <h3>
     混元图生视频的技术原理
    </h3>
    <p>
     图像到视频的生成框架:
     <br/>
     <br/>
     HunyuanVideo-12V通过图像潜在拼接技术，将参考图像的信息整合到视频生成过程中。输入图像首先经过预训练的多模态大型语言模型(MLLM)处理，生成语义图像token，然后与视频潜在token拼接，实现跨模态的全注意力计算。
    </p>
    <p>
     多模态大型语言模型(MLLM):
     <br/>
     <br/>
     模型采用具有Decoder-only结构的MLLM作为文本编码器，显著增强了对输入图像语义内容的理解能力。与传统的CLIP或T5模型相比，MLLM在图像细节描述和复杂推理方面表现更佳，能够更好地实现图像与文本描述信息的深度融合。
    </p>
    <p>
     3D变分自编码器(3D VAE):
     <br/>
     <br/>
     为了高效处理视频和图像数据，HunyuanVideo-12V使用CausalConv3D技术训练了-个3D VAE，将像素空间中的视频和图像压缩到紧凑的潜在空间。这种设计显著减少了后续模型中的token数量，能在原始分辨率和帧率下进行训练。
    </p>
    <p>
     双流转单流的混合模型设计:
     <br/>
     <br/>
     在双流阶段，视频和文本token通过多个Transformer块独立处理，避免相互干扰;在单流阶段，将视频和文本token连接起来，进行多模态信息融合。这种设计捕捉了视觉和语义信息之间的复杂交互，提升了生成视频的连贯性和语义一致性。
    </p>
    <p>
     渐进式训练策略:
     <br/>
     <br/>
     模型采用渐进式训练策略，从低分辨率、短视频逐步过渡到高分辨率、长视频。提高了模型的收敛速度，确保了生成视频在不同分辨率下的高质量。
    </p>
    <p>
     提示词重写模型:
     <br/>
     <br/>
     为解决用户提示词的语言风格和长度多变性问题，HunyuanVideo-12V引入了提示词重写模块，能将用户输入的提示词转换为模型更易理解的格式，提高生成效果。
    </p>
    <p>
     可定制化LoRA训练:
     <br/>
     <br/>
     模型支持LoRA(Low-RankAdaptation)训练，支持开发者通过少量数据训练出具有特定效果的视频生成模型，例如“头发生长“或”人物动作”等特效。
    </p>
    <h3>
     混元图生视频的项目地址
    </h3>
    <p>
     Github仓库:
     <a href="https://github.com/Tencent/HunyuanVideo-l2Vz" title="https://github.com/Tencent/HunyuanVideo-l2Vz">
      https://github.com/Tencent/HunyuanVideo-l2Vz
     </a>
     <br/>
     <br/>
     Huggingface模型库:
     <a href="https://huggingface.co/tencent/HunyuanVideo-l2Vz" rel="nofollow" title="https://huggingface.co/tencent/HunyuanVideo-l2Vz">
      https://huggingface.co/tencent/HunyuanVideo-l2Vz
     </a>
    </p>
    <p>
    </p>
    <p>
     来源：
     <a href="https://www.dcyzq.com/post/59.html" rel="nofollow" title="https://www.dcyzq.com/post/59.html">
      https://www.dcyzq.com/post/59.html
     </a>
    </p>
    <p>
    </p>
   </div>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f32323136333830332f:61727469636c652f64657461696c732f313436303939333339" class_="artid" style="display:none">
 </p>
</div>


