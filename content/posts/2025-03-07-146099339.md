---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f32323136333830332f:61727469636c652f64657461696c732f313436303939333339"
layout: post
title: "混元图生视频-腾讯混元开源的图生视频模型"
date: 2025-03-07 16:35:49 +0800
description: "混元图生视频是腾讯混元推出的开源图生视频模型，用户可以通过上传一张图片进行简短描述，让图片动起来生成5秒的短视频。输入图像首先经过预训练的多模态大型语言模型(MLLM)处理，生成语义图像token，然后与视频潜在token拼接，实现跨模态的全注意力计算。为解决用户提示词的语言风格和长度多变性问题，HunyuanVideo-12V引入了提示词重写模块，能将用户输入的提示词转换为模型更易理解的格式，提高生成效果。用户只需上传一张图片输入简短描述，模型可将静态图片转化为5秒的短视频，同时支持自动生成背景音效。"
keywords: "混元图生视频 低显存可用"
categories: ['Ai']
tags: ['音视频']
artid: "146099339"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146099339
    alt: "混元图生视频-腾讯混元开源的图生视频模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146099339
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146099339
cover: https://bing.ee123.net/img/rand?artid=146099339
image: https://bing.ee123.net/img/rand?artid=146099339
img: https://bing.ee123.net/img/rand?artid=146099339
---

# 混元图生视频-腾讯混元开源的图生视频模型

### 混元图生视频是什么

混元图生视频是腾讯混元推出的开源图生视频模型，用户可以通过上传一张图片进行简短描述，让图片动起来生成5秒的短视频。模型支持对口型、动作驱动和背景音效自动生成等功能。模型适用于写实、动漫和CGI等多种角色和场景，总参数量为130亿。腾讯混元图生视频模型已在腾讯云上线，用户可通过混元AI视频官网使用体验。混元图生视频模型在Github、Huggingface等主流开发者社区开源，包含权重、推理代码和LORA训练代码，开发者可以基于此训练专属LORA等衍生模型。

![](https://i-blog.csdnimg.cn/img_convert/d22fc1b4fac9007f4a8cef15fd8ff26c.webp?x-oss-process=image/format,png)

### 混元图生视频的主要功能

图生视频生成:

用户只需上传一张图片输入简短描述，模型可将静态图片转化为5秒的短视频，同时支持自动生成背景音效。

音频驱动功能:

用户可以上传人物图片，输入文本或音频，模型能精准匹配嘴型，让图片中的人物“说话"或"唱歌”，呈现符合语气的面部表情。
  
  
动作驱动功能:

用户上传图片后，选择动作模板，模型可让图片中的人物完成跳舞、挥手、做体操等动作，适用于短视频创作、游戏角色动画和影视制作。
  
  
高质量视频输出:支持2K高清画质，适用于写实、动漫和CGI等多种角色和场景。

### 混元图生视频的技术原理

图像到视频的生成框架:
  
  
HunyuanVideo-12V通过图像潜在拼接技术，将参考图像的信息整合到视频生成过程中。输入图像首先经过预训练的多模态大型语言模型(MLLM)处理，生成语义图像token，然后与视频潜在token拼接，实现跨模态的全注意力计算。

多模态大型语言模型(MLLM):
  
  
模型采用具有Decoder-only结构的MLLM作为文本编码器，显著增强了对输入图像语义内容的理解能力。与传统的CLIP或T5模型相比，MLLM在图像细节描述和复杂推理方面表现更佳，能够更好地实现图像与文本描述信息的深度融合。

3D变分自编码器(3D VAE):
  
  
为了高效处理视频和图像数据，HunyuanVideo-12V使用CausalConv3D技术训练了-个3D VAE，将像素空间中的视频和图像压缩到紧凑的潜在空间。这种设计显著减少了后续模型中的token数量，能在原始分辨率和帧率下进行训练。

双流转单流的混合模型设计:
  
  
在双流阶段，视频和文本token通过多个Transformer块独立处理，避免相互干扰;在单流阶段，将视频和文本token连接起来，进行多模态信息融合。这种设计捕捉了视觉和语义信息之间的复杂交互，提升了生成视频的连贯性和语义一致性。

渐进式训练策略:
  
  
模型采用渐进式训练策略，从低分辨率、短视频逐步过渡到高分辨率、长视频。提高了模型的收敛速度，确保了生成视频在不同分辨率下的高质量。

提示词重写模型:
  
  
为解决用户提示词的语言风格和长度多变性问题，HunyuanVideo-12V引入了提示词重写模块，能将用户输入的提示词转换为模型更易理解的格式，提高生成效果。

可定制化LoRA训练:
  
  
模型支持LoRA(Low-RankAdaptation)训练，支持开发者通过少量数据训练出具有特定效果的视频生成模型，例如“头发生长“或”人物动作”等特效。

### 混元图生视频的项目地址

Github仓库:
[https://github.com/Tencent/HunyuanVideo-l2Vz](https://github.com/Tencent/HunyuanVideo-l2Vz "https://github.com/Tencent/HunyuanVideo-l2Vz")
  
  
Huggingface模型库:
[https://huggingface.co/tencent/HunyuanVideo-l2Vz](https://huggingface.co/tencent/HunyuanVideo-l2Vz "https://huggingface.co/tencent/HunyuanVideo-l2Vz")

来源：
[https://www.dcyzq.com/post/59.html](https://www.dcyzq.com/post/59.html "https://www.dcyzq.com/post/59.html")