---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f7368616b656e2f:61727469636c652f64657461696c732f313436323631373333"
layout: post
title: "双3060Ubuntu22.04cuda12.8安装deepseek-32b-Q8"
date: 2025-03-14 17:05:56 +0800
description: "双3060、Ubuntu22.04、cuda12.8安装deepseek 32b-Q8"
keywords: "双3060、Ubuntu22.04、cuda12.8安装deepseek 32b-Q8"
categories: ['未分类']
tags: ['自然语言处理', '人工智能']
artid: "146261733"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146261733
    alt: "双3060Ubuntu22.04cuda12.8安装deepseek-32b-Q8"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146261733
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146261733
cover: https://bing.ee123.net/img/rand?artid=146261733
image: https://bing.ee123.net/img/rand?artid=146261733
img: https://bing.ee123.net/img/rand?artid=146261733
---

# 双3060、Ubuntu22.04、cuda12.8安装deepseek 32b-Q8

以下是针对双RTX 3060显卡（12GB显存）在Ubuntu 22.04系统部署DeepSeek-R1-32b-qwen-distill-q8模型的完整流程，结合最新技术规范与魔塔社区资源：

---

#### 一、驱动与CUDA环境配置

##### 1. 禁用开源驱动

bash

```
sudo tee /etc/modprobe.d/blacklist-nouveau.conf <<< "blacklist nouveau
options nouveau modeset=0"
sudo update-initramfs -u && sudo reboot  # 参考网页2的驱动管理方法
```

##### 2. 安装NVIDIA驱动570.57（适配CUDA 12.8）

bash

```
wget https://us.download.nvidia.com/XFree86/Linux-x86_64/570.57/NVIDIA-Linux-x86_64-570.57.run
sudo chmod +x NVIDIA-Linux-x86_64-570.57.run
sudo ./NVIDIA-Linux-x86_64-570.57.run --silent --no-opengl-files
sudo reboot
```

*验证驱动：
`nvidia-smi`
应显示驱动版本570.57且双卡在线*

##### 3. 安装CUDA 12.8

```
bash
```

```
wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_555.51_linux.run
sudo sh cuda_12.8.0_555.51_linux.run  # 取消勾选Driver选项
```

```
bash
```

```
echo 'export PATH=/usr/local/cuda-12.8/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

---

#### 二、Python环境与PyTorch安装

##### 1. 安装Python 3.10

```
bash
```

```
sudo apt update && sudo apt install python3.10 python3.10-venv
python3.10 -m venv ~/deepseek-env
source ~/deepseek-env/bin/activate
```

##### 2. 安装PyTorch 2.3.1（适配CUDA 12.8）(这里使用了官方nightly编译版本，方法到官网复制)

```
bash

```

```bash
pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

*版本对应关系：CUDA 12.8 + 驱动570.57 → PyTorch 2.3.1+cu121*

*2*

*3*

---

#### 三、llama.cpp服务端部署

##### 1. 获取源码（不使用git clone）

bash

```
wget https://github.com/ggerganov/llama.cpp/archive/refs/tags/b3117.tar.gz
tar -zxvf llama.cpp.tar.gz && cd llama.cpp-b3117
```

##### 2. CMake编译双显卡优化版本

bash

```
mkdir build && cd build
cmake .. -DLLAMA_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=86 -DLLAMA_NVCC_FLAGS="--use_fast_math" -DLLAMA_AVX2=ON
cmake --build . --config Release -j $(nproc)  # 参考网页3的编译优化思路
```

*关键参数说明：*

* `-DLLAMA_CUDA=ON`
  ：替代已弃用的CUBLAS参数
* `-DCMAKE_CUDA_ARCHITECTURES=86`
  ：适配RTX 3060的Ampere架构
* `-DLLAMA_NVCC_FLAGS`
  ：启用快速数学优化

---

#### 四、模型下载与部署

##### 1. 魔塔社区下载地址 [魔搭社区](https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/files "魔搭社区")

bash

```
wget https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/resolve/master/DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf
```

*注：需登录魔塔社区*

*1*

*3*

##### 2. 模型存放

bash

```
mkdir -p ~/models && mv deepseek-r1-32b-qwen-distill.Q8_0.gguf ~/models/
```

---

#### 五、双显卡API服务配置

##### 1. 启动命令

bash

```
./server -m ~/models/deepseek-r1-32b-qwen-distill.Q8_0.gguf \
  --host 0.0.0.0 --port 11434 \
  --n-gpu-layers 99 \             # 全量GPU计算层
  --tensor-split 11,11 \          # 显存分配（每卡11GB）
  --parallel 2 \                  # 双卡张量并行
  --main-gpu 0 \                  # 主卡ID
  --ctx-size 4096 \
  --mlock \
  --flash-attn \
  --batch-size 512
```

##### 2. 性能优化技巧

* ​
  **显存分配**
  ：根据网页3建议，实际可用显存=总显存-2GB系统保留
* ​
  **PCIe带宽优化**
  ：
  `sudo nvidia-smi -i 0,1 -pm 1`
  保持高性能模式

  2
* ​
  **冷启动加速**
  ：添加
  `--preload`
  参数预加载模型至显存

  1

---

#### 六、服务验证

bash

```
# GPU利用率监控
nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv -l 2

# API压力测试
curl http://localhost:11434/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "如何优化双显卡的深度学习推理性能？", "max_tokens": 500}'
```

---

#### 版本兼容性矩阵

| 组件 | 版本 | 适配说明 |
| --- | --- | --- |
| NVIDIA驱动 | 570.57 | CUDA 12.8最低要求 |
| CUDA | 12.8.0 | 需驱动≥570 |
| PyTorch | 2.3.1+cu121 | 通过cu121后缀兼容 |
| llama.cpp | b3117 | 支持张量并行 |

---

#### 常见问题处理

1. ​
   **显存不足**
   ：降低
   `--tensor-split`
   值（如10,10），或减少
   `--ctx-size`
   至2048
2. ​
   **模型加载失败**
   ：使用
   `md5sum`
   校验模型文件，魔塔社区提供完整校验码
3. ​
   **多卡未启用**
   ：检查PCIe连接状态，建议使用PCIe 4.0 x16插