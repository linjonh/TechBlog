---
layout: post
title: "双3060Ubuntu22.04cuda12.8安装deepseek-32b-Q8"
date: 2025-03-14 17:05:56 +0800
description: "双3060、Ubuntu22.04、cuda12.8安装deepseek 32b-Q8"
keywords: "双3060、Ubuntu22.04、cuda12.8安装deepseek 32b-Q8"
categories: ['未分类']
tags: ['自然语言处理', '人工智能']
artid: "146261733"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146261733
    alt: "双3060Ubuntu22.04cuda12.8安装deepseek-32b-Q8"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146261733
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146261733
cover: https://bing.ee123.net/img/rand?artid=146261733
image: https://bing.ee123.net/img/rand?artid=146261733
img: https://bing.ee123.net/img/rand?artid=146261733
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     双3060、Ubuntu22.04、cuda12.8安装deepseek 32b-Q8
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     以下是针对双RTX 3060显卡（12GB显存）在Ubuntu 22.04系统部署DeepSeek-R1-32b-qwen-distill-q8模型的完整流程，结合最新技术规范与魔塔社区资源：
    </p>
    <hr/>
    <h4>
     一、驱动与CUDA环境配置
    </h4>
    <h5>
     1. 禁用开源驱动
    </h5>
    <pre></pre>
    <p>
     bash
    </p>
    <pre><code>sudo tee /etc/modprobe.d/blacklist-nouveau.conf &lt;&lt;&lt; "blacklist nouveau
options nouveau modeset=0"
sudo update-initramfs -u &amp;&amp; sudo reboot  # 参考网页2的驱动管理方法</code></pre>
    <h5>
     2. 安装NVIDIA驱动570.57（适配CUDA 12.8）
    </h5>
    <pre></pre>
    <p>
     bash
    </p>
    <pre><code>wget https://us.download.nvidia.com/XFree86/Linux-x86_64/570.57/NVIDIA-Linux-x86_64-570.57.run
sudo chmod +x NVIDIA-Linux-x86_64-570.57.run
sudo ./NVIDIA-Linux-x86_64-570.57.run --silent --no-opengl-files
sudo reboot</code></pre>
    <p>
     <em>
      验证驱动：
      <code>
       nvidia-smi
      </code>
      应显示驱动版本570.57且双卡在线
     </em>
    </p>
    <h5>
     3. 安装CUDA 12.8
    </h5>
    <pre>bash</pre>
    <pre><code>wget https://developer.download.nvidia.com/compute/cuda/12.8.0/local_installers/cuda_12.8.0_555.51_linux.run
sudo sh cuda_12.8.0_555.51_linux.run  # 取消勾选Driver选项</code></pre>
    <pre>bash</pre>
    <pre><code>echo 'export PATH=/usr/local/cuda-12.8/bin:$PATH' &gt;&gt; ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH' &gt;&gt; ~/.bashrc
source ~/.bashrc</code></pre>
    <hr/>
    <h4>
     二、Python环境与PyTorch安装
    </h4>
    <h5>
     1. 安装Python 3.10
    </h5>
    <pre>bash</pre>
    <pre><code>sudo apt update &amp;&amp; sudo apt install python3.10 python3.10-venv
python3.10 -m venv ~/deepseek-env
source ~/deepseek-env/bin/activate</code></pre>
    <h5>
     2. 安装PyTorch 2.3.1（适配CUDA 12.8）(这里使用了官方nightly编译版本，方法到官网复制)
    </h5>
    <pre>bash
</pre>
    <pre><code class="language-bash">pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128</code></pre>
    <p>
     <em>
      版本对应关系：CUDA 12.8 + 驱动570.57 → PyTorch 2.3.1+cu121
     </em>
    </p>
    <p>
     <em>
      2
     </em>
    </p>
    <p>
     <em>
      3
     </em>
    </p>
    <hr/>
    <h4>
     三、llama.cpp服务端部署
    </h4>
    <h5>
     1. 获取源码（不使用git clone）
    </h5>
    <p>
     bash
    </p>
    <pre><code>wget https://github.com/ggerganov/llama.cpp/archive/refs/tags/b3117.tar.gz
tar -zxvf llama.cpp.tar.gz &amp;&amp; cd llama.cpp-b3117</code></pre>
    <h5>
     2. CMake编译双显卡优化版本
    </h5>
    <p>
     bash
    </p>
    <pre><code>mkdir build &amp;&amp; cd build
cmake .. -DLLAMA_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=86 -DLLAMA_NVCC_FLAGS="--use_fast_math" -DLLAMA_AVX2=ON
cmake --build . --config Release -j $(nproc)  # 参考网页3的编译优化思路</code></pre>
    <p>
     <em>
      关键参数说明：
     </em>
    </p>
    <ul>
     <li>
      <code>
       -DLLAMA_CUDA=ON
      </code>
      ：替代已弃用的CUBLAS参数
     </li>
     <li>
      <code>
       -DCMAKE_CUDA_ARCHITECTURES=86
      </code>
      ：适配RTX 3060的Ampere架构
     </li>
     <li>
      <code>
       -DLLAMA_NVCC_FLAGS
      </code>
      ：启用快速数学优化
     </li>
    </ul>
    <hr/>
    <h4>
     四、模型下载与部署
    </h4>
    <h5>
     1. 魔塔社区下载地址
     <br/>
     <a href="https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/files" rel="nofollow" title="魔搭社区">
      魔搭社区
     </a>
    </h5>
    <p>
     bash
    </p>
    <pre><code>wget https://www.modelscope.cn/models/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF/resolve/master/DeepSeek-R1-Distill-Qwen-32B-Q8_0.gguf</code></pre>
    <p>
     <em>
      注：需登录魔塔社区
     </em>
    </p>
    <p>
     <em>
      1
     </em>
    </p>
    <p>
     <em>
      3
     </em>
    </p>
    <h5>
     2. 模型存放
    </h5>
    <p>
     bash
    </p>
    <pre><code>mkdir -p ~/models &amp;&amp; mv deepseek-r1-32b-qwen-distill.Q8_0.gguf ~/models/</code></pre>
    <hr/>
    <h4>
     五、双显卡API服务配置
    </h4>
    <h5>
     1. 启动命令
    </h5>
    <p>
     bash
    </p>
    <pre><code>./server -m ~/models/deepseek-r1-32b-qwen-distill.Q8_0.gguf \
  --host 0.0.0.0 --port 11434 \
  --n-gpu-layers 99 \             # 全量GPU计算层
  --tensor-split 11,11 \          # 显存分配（每卡11GB）
  --parallel 2 \                  # 双卡张量并行
  --main-gpu 0 \                  # 主卡ID
  --ctx-size 4096 \
  --mlock \
  --flash-attn \
  --batch-size 512</code></pre>
    <h5>
     2. 性能优化技巧
    </h5>
    <ul>
     <li>
      ​
      <strong>
       显存分配
      </strong>
      ：根据网页3建议，实际可用显存=总显存-2GB系统保留
     </li>
     <li>
      ​
      <strong>
       PCIe带宽优化
      </strong>
      ：
      <code>
       sudo nvidia-smi -i 0,1 -pm 1
      </code>
      保持高性能模式
      <p>
       2
      </p>
     </li>
     <li>
      ​
      <strong>
       冷启动加速
      </strong>
      ：添加
      <code>
       --preload
      </code>
      参数预加载模型至显存
      <p>
       1
      </p>
     </li>
    </ul>
    <hr/>
    <h4>
     六、服务验证
    </h4>
    <p>
     bash
    </p>
    <pre><code># GPU利用率监控
nvidia-smi --query-gpu=utilization.gpu,memory.used --format=csv -l 2

# API压力测试
curl http://localhost:11434/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "如何优化双显卡的深度学习推理性能？", "max_tokens": 500}'</code></pre>
    <hr/>
    <h4>
     版本兼容性矩阵
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        组件
       </th>
       <th>
        版本
       </th>
       <th>
        适配说明
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        NVIDIA驱动
       </td>
       <td>
        570.57
       </td>
       <td>
        CUDA 12.8最低要求
       </td>
      </tr>
      <tr>
       <td>
        CUDA
       </td>
       <td>
        12.8.0
       </td>
       <td>
        需驱动≥570
       </td>
      </tr>
      <tr>
       <td>
        PyTorch
       </td>
       <td>
        2.3.1+cu121
       </td>
       <td>
        通过cu121后缀兼容
       </td>
      </tr>
      <tr>
       <td>
        llama.cpp
       </td>
       <td>
        b3117
       </td>
       <td>
        支持张量并行
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h4>
     常见问题处理
    </h4>
    <ol>
     <li>
      ​
      <strong>
       显存不足
      </strong>
      ：降低
      <code>
       --tensor-split
      </code>
      值（如10,10），或减少
      <code>
       --ctx-size
      </code>
      至2048
     </li>
     <li>
      ​
      <strong>
       模型加载失败
      </strong>
      ：使用
      <code>
       md5sum
      </code>
      校验模型文件，魔塔社区提供完整校验码
     </li>
     <li>
      ​
      <strong>
       多卡未启用
      </strong>
      ：检查PCIe连接状态，建议使用PCIe 4.0 x16插
      <p>
      </p>
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="68747470:733a2f2f626c6f672e6373646e2e6e65742f7368616b656e2f:61727469636c652f64657461696c732f313436323631373333" class_="artid" style="display:none">
 </p>
</div>


