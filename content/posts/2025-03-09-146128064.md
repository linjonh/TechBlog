---
layout: post
title: "大语言模型中的归一化技术LayerNorm与RMSNorm的深入研究"
date: 2025-03-09 10:03:06 +0800
description: "归一化（Normalization）是一种将数据映射到特定数值区间的数学变换技术，旨在提升计算稳定性并优化学习效率。归一化的本质在于调整数据的量纲规模而保持分布形态不变。归一化前后数据分布归一化前后按特征划分的数据分布通过对比归一化前后的数据分布可以明确观察到，尽管数据点的相对位置关系保持恒定，但数值范围发生了显著变化。如上图所示，横轴上原本分布在30至70区间的数据经归一化后映射至0到1区间，然而数据的拓扑分布结构保持不变。这构成了归一化技术的基本理论框架。"
keywords: "大语言模型中的归一化技术：LayerNorm与RMSNorm的深入研究"
categories: ['未分类']
tags: ['自然语言处理', '深度学习', '归一化', '大语言模型', '人工智能']
artid: "146128064"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146128064
    alt: "大语言模型中的归一化技术LayerNorm与RMSNorm的深入研究"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146128064
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146128064
cover: https://bing.ee123.net/img/rand?artid=146128064
image: https://bing.ee123.net/img/rand?artid=146128064
img: https://bing.ee123.net/img/rand?artid=146128064
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     大语言模型中的归一化技术：LayerNorm与RMSNorm的深入研究
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     在LLama等大规模Transformer架构的语言模型中，归一化模块是构建网络稳定性的关键组件。本文将系统分析归一化技术的必要性，并详细阐述为何原始Transformer架构中的LayerNorm在LLama模型中被RMSNorm所替代的技术原理。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/7cda200d567ad0f9db9006be34165fb3.jpeg"/>
    </p>
    <h3>
     <a id="_4">
     </a>
     归一化技术的基础原理
    </h3>
    <h4>
     <a id="_6">
     </a>
     归一化的核心定义
    </h4>
    <p>
     归一化（Normalization）是一种将数据映射到特定数值区间的数学变换技术，旨在提升计算稳定性并优化学习效率。归一化的本质在于调整
     <strong>
      数据的量纲规模而保持分布形态不变
     </strong>
     。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/0be4216fffe7cdd9bd81924ba67aeba0.jpeg"/>
    </p>
    <p>
     归一化前后数据分布
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/5e8b2850ecefd04cbc5a644fe88300f8.jpeg"/>
    </p>
    <p>
     归一化前后按特征划分的数据分布
    </p>
    <p>
     通过对比归一化前后的数据分布可以明确观察到，尽管数据点的相对位置关系保持恒定，但数值范围发生了显著变化。如上图所示，横轴上原本分布在30至70区间的数据经归一化后映射至0到1区间，然而数据的拓扑分布结构保持不变。这构成了归一化技术的基本理论框架。
    </p>
    <h4>
     <a id="_20">
     </a>
     归一化技术的技术必要性
    </h4>
    <p>
     归一化技术在深度学习中具有双重技术价值。首要方面是提高
     <strong>
      数值计算稳定性
     </strong>
     和
     <strong>
      优化收敛速度
     </strong>
     。当数据量级差异显著时，评估指标（如均方误差MSE）可能产生数量级上的波动。在MSE计算过程中，大值输入会导致梯度幅值过大，从而使优化过程不稳定。
    </p>
    <p>
     以均方误差计算为例，考虑两组输入值
    </p>
    <pre><code>a
</code></pre>
    <p>
     和
    </p>
    <pre><code>b
</code></pre>
    <p>
     的对比情况：
    </p>
    <ol>
     <li>
      未经归一化处理:-
      <code>
       a = [100, 200, 300]
      </code>
      ,
      <code>
       b = [110, 190, 310]
      </code>
      - MSE = (1/3) * [(100–110)² + (200–190)² + (300–310)²]- MSE = (1/3) * [100 + 100 + 100] = 100
     </li>
     <li>
      经归一化处理:- 归一化后的值：
      <code>
       a_normalized = [1.0, 2.0, 3.0]
      </code>
      ,
      <code>
       b_normalized = [1.1, 1.9, 3.1]
      </code>
      - MSE = (1/3) * [(1.0–1.1)² + (2.0–1.9)² + (3.0–3.1)²]- MSE = (1/3) * [0.01 + 0.01 + 0.01] = 0.01
     </li>
    </ol>
    <p>
     从计算结果可见，未归一化的数据由于数值较大，产生了较大的MSE值，这可能导致梯度幅值过大，使模型训练过程不稳定。而归一化处理后，MSE值显著降低，梯度幅值维持在合理区间，有助于参数更新过程的稳定性。
    </p>
    <p>
     通过将数据规范化到一致的数值范围，归一化能够有效控制梯度幅值，实现更为平稳的参数优化过程。
    </p>
    <p>
     归一化的另一技术价值在于潜在的内存效率提升。归一化通过缩减数据表示范围间接提高了内存利用效率。特别是在采用8位或16位浮点格式进行训练时，数据范围的收窄使得低精度表示更为精确。这种特性可通过量化技术或FP16计算进一步降低模型训练阶段的内存占用。
    </p>
    <p>
     需要说明的是，归一化本身并不直接减少内存占用，它需要与量化或低精度计算技术协同应用才能实现有效的内存优化。
    </p>
    <h4>
     <a id="_45">
     </a>
     归一化的实证效果分析
    </h4>
    <p>
     通过实际数据集分析可验证归一化的技术效果。以加州住房数据集为例，采用归一化技术的模型（蓝线）展现出快速且稳定的学习曲线，而未归一化的模型（红线）则表现出学习不稳定性。这种差异在训练过程中尤为明显，未归一化情况下可能出现数值溢出，导致损失函数出现异常峰值。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/04caab010e8a3dfb7bbb437d767928b6.jpeg"/>
    </p>
    <p>
     归一化前后MSE比较——加州住房数据集
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/19bccaa325e5ace6bfc710947030a7be.jpeg"/>
    </p>
    <p>
     归一化前后准确率性能比较——CIFAR-10
    </p>
    <p>
     在CIFAR-10图像分类任务中，类似的数值稳定性问题同样存在。应用归一化技术后，学习过程能够迅速达到稳定状态。因此，归一化已成为现代深度学习的基础技术组件，对于提升模型性能至关重要。
    </p>
    <h3>
     <a id="Layer_Normalization_59">
     </a>
     层归一化（Layer Normalization）
    </h3>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/c9410e20b92278f2125a4e0b95cc2792.jpeg"/>
    </p>
    <p>
     层归一化最初由Jimmy Lei Ba、Jamie Ryan Kiros和Geoffrey Hinton在2016年发表的论文"Layer Normalization"中提出。该研究提出LayerNorm作为批量归一化的替代方案，以解决其局限性，特别强调了其在序列数据和循环神经网络模型中的适用性。
    </p>
    <p>
     层归一化通过计算每个样本的均值（μ）和标准差（σ）来实现数据归一化。由于LayerNorm是在每个样本基础上独立应用的，不依赖于批量大小，因此在处理序列数据的模型中表现尤为出色。Transformer模型作为序列数据处理的核心架构，广泛采用了LayerNorm技术。
    </p>
    <p>
     在Transformer架构中，LayerNorm扮演着至关重要的角色。Transformer处理的序列数据由多个token组成，LayerNorm所归一化的"样本"实际上是每个token位置的嵌入向量或隐藏状态。
    </p>
    <p>
     例如，对于句子"The cat sat on the mat"，Transformer模型会为每个单词（token）生成特定的向量表示（嵌入）。LayerNorm在此场景下归一化的是该token位置的嵌入向量或隐藏状态。LayerNorm的核心机制是对每个token位置计算的嵌入或隐藏状态向量进行归一化，以维持特定范围内的数值稳定性。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/40763240d992b3488426d7ae4718eb38.jpeg"/>
    </p>
    <p>
     其中：
    </p>
    <ul>
     <li>
      <em>
       x
      </em>
      表示每个样本的输入值
     </li>
     <li>
      <em>
       μ
      </em>
      表示平均值
     </li>
     <li>
      <em>
       σ
      </em>
      表示标准差
     </li>
     <li>
      <em>
       ϵ
      </em>
      是一个微小常数，用于确保数值稳定性，防止除零错误，通常设定为0.00001或0.000001
     </li>
    </ul>
    <p>
     LayerNorm将数据归一化为均值为0，方差为1的分布。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/c5651914f57e176268eab2345cad8adf.jpeg"/>
    </p>
    <p>
     Transformer输入示例和LayerNorm计算过程
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/c49a4f863d7dc601b7f845712792928b.jpeg"/>
    </p>
    <p>
     LayerNorm应用结果
    </p>
    <p>
     从上图可观察到，每个token的嵌入维度为5，均值和方差计算在嵌入向量的5维值空间内进行。值得注意的是，这一计算过程独立于其他数据点或token。另一关键特性是数据分布向均值0对齐，这一特性与RMSNorm形成明显对比。
    </p>
    <h3>
     <a id="RMSRMS_Normalization_93">
     </a>
     RMS归一化（RMS Normalization）
    </h3>
    <p>
     RMS归一化（RMSNorm）是层归一化的变体，其特点是仅使用均方根（Root Mean Square）进行归一化，省略了均值计算环节。RMSNorm在降低计算成本的同时能够维持模型性能，同时提供更加稳定的学习过程。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/370b4c7e06633a25b6055965a8b95654.jpeg"/>
    </p>
    <p>
     RMS(x)的定义如下：
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/5eccb809373a75edbe4d805fdc96624e.jpeg"/>
    </p>
    <p>
     其中RMS代表特征值的均方根。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/f378ff6be4c24fdf7049ebe29c46aea6.jpeg"/>
    </p>
    <p>
     Transformer输入示例和RMSNorm计算过程
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/1bd3f5c6428c83b6db97f4ad17a5bd45.jpeg"/>
    </p>
    <p>
     从结果可以观察到，RMSNorm不执行均值对齐操作。由于它没有执行
     <em>
      x-μ
     </em>
     运算，因此不存在向0对齐的过程。与传统LayerNorm需要两次数据扫描（一次计算均值，一次计算方差）不同，RMSNorm仅需一次处理即可完成RMS计算。这种简化方法在保持性能的同时有效降低了计算资源需求。
    </p>
    <h3>
     <a id="LayerNormRMSNorm_113">
     </a>
     LayerNorm与RMSNorm的技术对比
    </h3>
    <h4>
     <a id="_115">
     </a>
     均值对齐特性分析
    </h4>
    <p>
     与LayerNorm显著不同的是，RMSNorm保持非零均值分布，不执行均值对零的对齐操作。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/dabc396653cc53ae4cd7e0092a8a79be.jpeg"/>
    </p>
    <p>
     通过对比LayerNorm和RMSNorm应用到前述Transformer输入示例后的均值分布结果，差异变得更为明显。尽管两种方法都将数据分布收敛到特定区域，LayerNorm呈现零均值特性，而RMSNorm则表现为非零均值分布。
    </p>
    <h3>
     <a id="_123">
     </a>
     从梯度传播视角的比较分析
    </h3>
    <p>
     RMSNorm通过保持非零均值而不强制均值对零的对齐，有效缓解了
     <strong>
      梯度消失问题
     </strong>
     。LayerNorm的零均值对齐可能导致梯度在反向传播过程中逐层衰减。
     <br/>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/8b4268396430adbd753a27d57a1ee3f7.jpeg"/>
    </p>
    <p>
     LayerNorm与RMSNorm：跨Epoch的梯度稳定性对比
    </p>
    <p>
     上图基于简化网络模型分析了LayerNorm和RMSNorm的梯度传播效果。左侧图表展示了按训练epoch划分的LayerNorm和RMSNorm的平均梯度范数变化。梯度范数是衡量网络权重变化速率的指标，具体表示反向传播过程中各层权重梯度的幅值。较大的梯度范数值表示权重变化幅度较大，而过大或过小的值可能导致学习不稳定。
    </p>
    <p>
     RMSNorm相比LayerNorm维持了更高的梯度范数，尤其在训练初期阶段。这一特性对于防止深度学习模型中的梯度消失问题至关重要。右侧图表显示了RMSNorm与LayerNorm之间的梯度比率，初期RMSNorm的梯度显著高于LayerNorm，随着学习进程的推进，这一差异逐渐减小。
    </p>
    <p>
     与LayerNorm相比，RMSNorm在计算效率和训练稳定性方面表现卓越。网络结构越深，RMSNorm通过有效防止梯度消失问题实现更稳定的学习过程，同时能够在减少计算资源消耗的条件下达成相似的性能水平。
    </p>
    <h3>
     <a id="_137">
     </a>
     结论
    </h3>
    <p>
     本文深入分析了原始Transformer架构中采用的层归一化技术与LLama架构中实现的RMS归一化技术之间的技术差异与特性。LayerNorm通过调整数据的均值和方差确保数值稳定性，在各类序列模型中发挥关键作用。而RMSNorm作为一种无需利用均值即可稳定数据尺度的技术，能够有效解决深层网络中的梯度消失问题，同时保持较快的训练收敛速度。
    </p>
    <p>
     这两种归一化技术在现代深度学习模型中均发挥着不可替代的作用，对于最大化提升模型性能（尤其是在Transformer架构中）至关重要。RMSNorm在计算效率和训练稳定性方面对LayerNorm形成了有效补充，为提升复杂深层模型性能做出了重要贡献。
    </p>
    <p>
     <a href="https://avoid.overfit.cn/post/224e11d8a7d84870b1a3d5e7ea410a35" rel="nofollow">
      https://avoid.overfit.cn/post/224e11d8a7d84870b1a3d5e7ea410a35
     </a>
    </p>
    <p>
     作者：Hugman Sangkeun Jung
    </p>
   </div>
   <link href="./../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="./../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f34363531303234352f:61727469636c652f64657461696c732f313436313238303634" class_="artid" style="display:none">
 </p>
</div>


