---
layout: post
title: "反无人机目标检测数据集空对空视觉检测微型无人机深度学习的实验评估"
date: 2025-03-11 19:06:44 +0800
description: "摘要—本文研究了利用单目摄像头进行空中对微型无人机（UAV）视觉检测的问题。该问题在许多应用中具有重要意义，例如基于视觉的无人机集群、恶意无人机检测以及无人机的“看见并避让”系统。尽管深度学习方法在许多目标检测任务中表现出色，但其在无人机检测中的潜力尚未得到充分探索。作为本文的第一个主要贡献，我们提出了一个名为Det-Fly的新数据集，该数据集包含由另一架飞行无人机获取的超过13,000张目标无人机飞行图像。"
keywords: "【反无人机目标检测数据集】空对空视觉检测微型无人机：深度学习的实验评估"
categories: ['目标检测']
tags: ['视觉检测', '目标检测', '无人机']
artid: "146186650"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146186650
    alt: "反无人机目标检测数据集空对空视觉检测微型无人机深度学习的实验评估"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146186650
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146186650
cover: https://bing.ee123.net/img/rand?artid=146186650
image: https://bing.ee123.net/img/rand?artid=146186650
img: https://bing.ee123.net/img/rand?artid=146186650
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【反无人机目标检测数据集】空对空视觉检测微型无人机：深度学习的实验评估
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     Air-to-Air Visual Detection of Micro-UAVs： An Experimental Evaluation of Deep Learning
     <br/>
     空对空视觉检测微型无人机：深度学习的实验评估
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7459bafc172447a795f0c8327646c726.png"/>
    </p>
    <h2>
     <a id="0_4">
     </a>
     0.论文摘要
    </h2>
    <p>
     摘要—本文研究了利用单目摄像头进行空中对微型无人机（UAV）视觉检测的问题。该问题在许多应用中具有重要意义，例如基于视觉的无人机集群、恶意无人机检测以及无人机的“看见并避让”系统。尽管深度学习方法在许多目标检测任务中表现出色，但其在无人机检测中的潜力尚未得到充分探索。作为本文的第一个主要贡献，我们提出了一个名为Det-Fly的新数据集，该数据集包含由另一架飞行无人机获取的超过13,000张目标无人机飞行图像。与现有数据集相比，该数据集更为全面，涵盖了不同背景场景、视角、相对距离、飞行高度和光照条件下的多种实际场景。本文的第二个主要贡献是基于该数据集对八种代表性深度学习算法进行了实验评估。据我们所知，这是迄今为止首次对视觉无人机检测任务中的深度学习算法进行全面实验评估。评估结果揭示了空中对无人机检测问题中的一些关键挑战，并为未来开发新算法提供了潜在方向。
    </p>
    <p>
     关键词—深度学习，无人机检测，视觉检测。
    </p>
    <p>
     <a href="https://github.com/Jake-WU/Det-Fly">
      数据集链接
     </a>
    </p>
    <h2>
     <a id="1_12">
     </a>
     1.引言
    </h2>
    <p>
     近年来，微型无人机（UAV）的视觉检测技术受到越来越多的关注，因为它是许多重要应用的核心技术。例如，在基于视觉的无人机集群系统中，视觉检测至关重要，每个无人机都需要使用机载摄像头来测量其邻近无人机的相对运动[1]。此外，如今微型无人机的恶意使用已成为公共安全和个人隐私的严重威胁。对恶意微型无人机的视觉检测[2][3]是开发民用无人机防御系统的关键技术。另一个应用是无人机之间的“看见并避让”[4]。特别是，随着越来越多的商用无人机占据低空领域，例如用于包裹递送，如何确保无人机能够及时检测到其他无人机以安全导航而不发生碰撞，是一个重要问题。
    </p>
    <p>
     无人机的检测可以分为两种应用场景。第一种是地对空场景，即在地面上部署摄像头以检测飞行的无人机。第二种场景是空对空场景，即飞行的无人机利用其机载摄像头检测其他飞行的无人机（例如，参见图1）。本文主要关注空对空场景。此外，尽管可以使用不同类型的传感器来检测微型无人机，如视觉、雷达[5]和声学传感器[6]，但由于微型无人机的机载载荷极为有限，视觉传感器是空对空场景中少数合适的选择之一。本文重点讨论最广泛使用的RGB单目摄像头。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d977252b102e469892eab41bb7fd2ecc.png"/>
    </p>
    <p>
     近年来，地空无人机检测逐渐吸引了越来越多的研究关注（详见第二节的综述），而更为复杂的空空无人机检测问题至今仍未得到很好的解决。在许多地空无人机检测任务中，地面摄像机通常是静止或缓慢移动的[7]，且目标无人机的图像背景为晴朗或多云的天空。相比之下，在空空无人机检测任务中，飞行中的无人机可能从顶部或侧面的视角观察目标无人机。因此，目标无人机图像的背景可能是极其复杂的场景，如城市和自然田野（见图2示例）。此外，由于机载摄像机处于动态飞行状态，目标无人机的外观（如形状、尺寸和颜色）可能会发生显著变化。由于微型无人机体积较小，其图像可能非常小（例如小于10×10像素），这进一步增加了检测的难度。
    </p>
    <p>
     现有的无人机检测方法可分为两类。第一类是传统方法，由两步操作组成。第一步是提取目标特征，例如使用方向梯度直方图（HOG）或尺度不变特征变换（SIFT）来表示。第二步是使用机器学习算法（如支持向量机（SVM）或Adaboost）对特征进行分类。第二类是基于深度学习的方法，它通过端到端的人工神经网络直接输出检测结果。与传统方法使用手工设计的特征不同，基于深度学习的方法依赖于深度卷积神经网络（DCNN）特征，因此具有更强的表示复杂目标的能力。然而，使用DCNN的缺点是计算需求高，并且需要大量数据集进行训练。现有方法的详细综述将在第二节中给出。
    </p>
    <p>
     尽管深度学习方法在许多目标检测任务中表现出色，但其在无人机（UAV）检测中的潜力至今尚未得到充分探索或评估（详见第II-B节的综述）。作为建立稳健的空对空无人机检测方法的第一步，本文提出了一种新的微型无人机图像数据集，并对八种具有代表性的深度学习算法进行了全面的实验评估。值得注意的是，我们关注的是目标无人机已知的情况，以便能够构建用于训练的数据集。这种情况适用于基于视觉的多无人机系统协同控制等任务，这也是我们进行无人机检测的主要动机。尽管这些算法在检测外观相似的未知无人机时表现出一定的泛化能力，但可能需要采取其他措施，例如构建多种类型无人机的数据集或目标运动感知[2]。
    </p>
    <p>
     本工作的新颖性和贡献详述如下。
    </p>
    <p>
     首先，本文提出了一个由另一架飞行无人机（DJI M210）获取的13,271张目标飞行无人机（DJI Mavic）图像的数据集。与现有的空对空数据集相比，该数据集在设计上更加系统且全面，涵盖了广泛的实用场景，包括不同的背景场景、视角、相对距离、飞行高度和光照条件。特别是，环境背景场景从简单的晴空到复杂的山地、田野和城市等均有涉及。目标无人机的相对距离从10米到100米不等，飞行高度从20米到110米不等。由于光照条件也是飞行无人机检测的重要因素，数据采集时间从早晨到傍晚，覆盖了不同的时间段。该数据集还涵盖了一些具有挑战性的场景，例如强光、运动模糊和目标部分遮挡等。
    </p>
    <p>
     其次，本文基于我们提出的数据集对八种代表性的深度学习算法进行了实验评估：RetinaNet [8]、SSD [9]、YOLOv3 [10]、FPN [11]、Faster R-CNN [12]、RefineDet [13]、Grid R-CNN [14] 和 Cascade R-CNN [15]。据我们所知，这是首次对无人机检测任务中的深度学习算法进行全面评估。评估结果表明，Cascade R-CNN 和 Grid R-CNN 的整体性能优于其他算法。我们还评估了背景场景复杂性、相对视角和目标尺度等关键因素对检测性能的影响。
    </p>
    <p>
     所提出的数据集可作为评估不同无人机检测算法（无论是传统方法还是基于深度学习的方法）的基准。评估结果突显了空对空无人机检测问题中的一些关键挑战，并为未来开发新算法提供了潜在的方向。
    </p>
    <h2>
     <a id="2_36">
     </a>
     2.相关工作
    </h2>
    <p>
     本节回顾了现有关于微型无人机视觉检测的研究。我们仅考虑使用单目相机的情况。
    </p>
    <h3>
     <a id="A_40">
     </a>
     A.传统方法
    </h3>
    <p>
     现有无人机检测工作所采用的传统技术可分为两类。第一类是使用特征提取方法获取目标特征，然后通过判别分类器确定目标位置。第二类是在图像中检测移动物体，然后通过生成分类器判断该移动物体是否为目标。
    </p>
    <p>
     特别是，文献[16]采用基于Haar小波的AdaBoost方法来检测无人机。该方法通过飞行实验证明在简单的多云天空背景下是有效的。文献[17]提出了一种基于Haar-like特征、局部二值模式和HOG的级联方法来检测无人机。由于该方法结合了不同的检测方法，其运行速度较低。文献[18]采用HOG特征来训练经典的级联检测器。尽管该方法通过应用非极大值抑制显著减少了重复检测的次数，但在部分遮挡情况下检测精度迅速下降。受“看与避”任务中运动目标检测的启发，文献[19]利用光流匹配来整合空间和时间信息以跟踪运动目标。该方法需要高精度的运动补偿。文献[20]也使用光流方法来定位运动物体，后续步骤是通过模板匹配来识别运动物体，但该方法对目标外观的变化不够鲁棒。文献[21]同样采用模板匹配以及形态学滤波进行无人机检测。文献[22]提出了一种实时检测与跟踪策略，其中通过计算每帧的背景连通性线索，可以在显著性图中自动检测到感兴趣的目标。文献[23]提出了一种金字塔Lucas-Kanade（PLK）算法来检测协作无人机团队中的运动目标。文献[24]通过提取分割图像中的几何特征和动态特征来检测运动目标，并通过从贝叶斯定理导出的判别函数对其进行分类。
    </p>
    <p>
     总的来说，尽管基于许多传统方法对无人机检测进行了研究，但这些方法仅在受限场景中有效，例如背景相对简单或目标外观变化不大的情况。
    </p>
    <h3>
     <a id="B_48">
     </a>
     B.深度学习方法
    </h3>
    <p>
     尽管基于深度学习的方法在通用目标检测领域取得了显著进展，但在无人机检测领域尚未得到充分探索。迄今为止，利用深度学习算法进行无人机视觉检测的研究仍然较少。例如，文献[25]提出了一种利用运动补偿检测飞行物体的方法，其中通过卷积神经网络对运动物体的特征进行分类。该方法实现了较高的平均检测精度，但运动补偿步骤需要对相机运动进行高精度测量。文献[26]将SegNet与底帽形态学处理相结合，用于检测空中大型飞机。该方法能够在长达2800米的范围内检测飞机，但精度低至13.4%。尽管其他一些研究如[27]、[28]也采用了YOLOv2等深度学习算法来检测无人机，但不同代表性深度学习算法在无人机检测中的性能尚未得到评估或比较。
    </p>
    <h3>
     <a id="C__52">
     </a>
     C. 现有无人机检测数据集
    </h3>
    <p>
     迄今为止，用于训练无人机检测深度学习算法的综合数据集非常少。文献[29]中的数据集包含20个视频序列，每个序列约有4000帧752×480的灰度图像。飞行目标无人机的图像由安装在另一架无人机上的摄像头在室内和室外环境中拍摄。文献[30]提出的数据集由两个子数据集组成。第一个是公共领域无人机数据集，包含30个视频序列，拍摄了不同型号的无人机在室内和室外环境中的图像。另一个是南加州大学（USC）无人机数据集，包含30个同一目标无人机的视频片段。该数据集在南加州大学校园内采集，大多数样本的背景是干净或多云的天空，与我们提出的数据集相比相对简单。为了增加数据集中的图像数量，USC数据集的作者开发了一种基于模型的自动数据增强方法，将裁剪的无人机模型图像粘贴到背景图像中。尽管通过这种方式可以扩展数据量，但文献[31]的研究表明，基于此类数据训练的网络可能不会因数据增强而显著提升性能。最近，文献[32]提出了一个名为MIDGARD的新数据集。该数据集包含不同类型的背景和变化的照明条件，并提出了一种基于其先前工作“紫外线方向与测距”[33]的自动标注新方法。第V节详细比较了MIDGARD数据集与我们的数据集。
    </p>
    <h2>
     <a id="3_59">
     </a>
     3.所提出的数据集
    </h2>
    <p>
     所提出的数据集名为Det-Fly，包含13,271张目标微型无人机（DJI Mavic）的图像。每张图像的分辨率为3840 × 2160像素。数据集中的部分图像是从视频中以5帧/秒的速率采样得到的，其余图像则是从所需的相对姿态中捕获的。所有图像均由专业人员手动标注。图2中展示了一些样本图像。该数据集可在https://github.com/Jake-WU/Det-Fly获取。
    </p>
    <p>
     Det-Fly涵盖了多种场景，包括不同的视角、背景环境、相对距离和光照条件。特别是，Det-Fly涉及四种环境背景：天空、城市、田野和山脉（见图2）。每种环境背景在整个数据集中所占比例大致相同（约20%–30%）。在相对视角方面，Det-Fly可以分为三类：正面视角、顶部视角和底部视角。这三种视角的数据比例分别为36.4%（正面视角）、32.5%（顶部视角）和31.1%（底部视角）。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/70085a8176d3400aa53fa605ff961a85.png">
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ff2d050da60e4911b9c8015e3231869f.png"/>
     </img>
    </p>
    <p>
     图2. 数据集中图像样本及八种算法对应的检测结果。数据集包含四种背景场景：天空、山脉、田野和城市。八种算法检测到的区域以彩色框标注在每张样本图像的右侧。如果对应区域为空白，则表示该算法未在该图像中检测到任何目标无人机。
    </p>
    <p>
     就目标无人机的图像尺寸而言，图3中的统计数据显示，数据集中大部分目标无人机图像较小。特别是，近一半的图像尺寸小于整个图像尺寸的5%。当目标无人机图像的高度和宽度小于整个图像的10%时，可被视为小目标，其检测是一项众所周知的具有挑战性的任务。此外，由于光照条件也是无人机飞行检测中的重要因素，图像采集时间从早晨到傍晚，覆盖了一天中的不同时段。该数据集还涵盖了一些具有挑战性的场景，如强光/弱光（10.8%）、运动模糊（11.2%）以及目标部分遮挡（0.8%）。
    </p>
    <p>
     以下是对所提出数据集的一些说明。首先，该数据集中的每张图像仅包含一个单一的目标无人机。然而，基于该数据集训练的算法自然可以检测多个无人机，这是基于视觉的无人机集群所必需的。其次，尽管该数据集涵盖了广泛的环境场景，但不可能覆盖所有可能的场景。建立该数据集的主要目的是评估不同的深度学习算法。如果某人希望在特定环境场景中实际实施深度学习方法，则应调整数据集以覆盖进行无人机检测的特定环境或更多环境场景，以增强泛化能力。第三，该数据集仅涵盖一种单一类型的无人机（DJI Mavic）。如果某人希望检测更多类型的无人机，可能需要采取其他措施，例如建立多种类型无人机的数据集或目标运动感知[2]。
    </p>
    <h2>
     <a id="4_78">
     </a>
     4.实验设置
    </h2>
    <p>
     在本文中，我们对八种经典的基于深度学习的目标检测方法进行了精细调优和评估：SSD [9]、RetinaNet [8]、YOLOv3 [10]、RefineDet [13]、Faster R-CNN [12]、FPN [11]、Cascade R-CNN [15] 和 Grid R-CNN [14]。这些方法在COCO数据集上对小尺度目标的检测性能相似，其中使用平均精度（mAP）作为评估指标。
    </p>
    <p>
     根据检测算法的类型，所选方法可以分为两类：单阶段网络和两阶段网络。单阶段网络直接在特征图上进行分类和回归，以实现快速的目标检测。在所选方法中，SSD、RetinaNet、YOLOv3和RefineDet属于单阶段网络。两阶段网络由区域提议网络（RPN）和分类回归网络组成，RPN提出若干候选框，分类回归网络实现对指定目标的识别和定位。在所选方法中，Faster R-CNN、FPN、Cascade R-CNN和Grid R-CNN属于两阶段网络。
    </p>
    <p>
     我们工作中实现的算法的主要超参数如表I所示。由于ResNet在ImageNet上达到了最先进的性能，我们在大多数算法中采用它作为骨干网络。通常，ResNet有两个常用版本，分别为ResNet-50和ResNet-101。在我们的工作中，我们选择ResNet-50而非ResNet-101，因为它更轻量，适合在微型无人机的嵌入式计算机上实现。由于DarkNet-53被广泛用作YOLOv3的骨干网络，并且其性能与ResNet-50相似[10], [34]，我们在实验中选择DarkNet-53作为YOLOv3的骨干网络。我们使用了原始的优化器。学习率（LR）、动量、权重衰减和迭代次数均基于大量测试进行了精细调整。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4c40984d8fb84eb68417da82d6deff71.png"/>
    </p>
    <p>
     我们的实验在一台配备Intel i7处理器、32 GB内存和Nvidia RTX 2080Ti显卡的计算机上实现，而非嵌入式计算机，以减少训练时间。我们基于70%的图像训练模型，其中10%用于验证，并在剩余的30%图像上进行测试。此外，我们使用非极大值抑制（NMS）来去除重叠的边界框，以确保一个物体仅包含在一个边界框中。作为NMS中评估预测边界框重叠率的重要参数，IoU定义为
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/85a4fdb526314992841fd6d37c1c4599.png"/>
    </p>
    <p>
     在我们的实验中，IoU阈值设置为0.5。
    </p>
    <p>
     在训练阶段，我们将训练轮数设置为八轮，并在每一轮保存模型参数。如果训练损失和验证损失保持稳定，我们则认为检测器已经训练良好。否则，我们会调整训练轮数并继续训练，直到模型训练良好为止。
    </p>
    <p>
     精确度是评估漏检的指标。本文中精确度的计算与一般视觉目标检测中的计算方法相同，即遍历所有预测框来计算精确度。如果无人机被成功检测到，则预测的边界框将被视为真正例（TP）；否则，将被视为假正例（FP）。精确度的定义如下：
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f80344e05e4d43c09dfcea936509a501.png">
      <br/>
      召回率是一种用于衡量误检的指标，其定义为
     </img>
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/08243d4ee64b4094b243ee2b48026adb.png"/>
    </p>
    <p>
     物体检测器的性能可以通过精确率×召回率（P-R）曲线来评估，该曲线考虑了不同阈值下的误检与漏检情况。然而，P-R曲线通常是上下波动的锯齿状曲线，并且经常相互交叉，因此在同一图中比较不同曲线（不同检测器）通常并不容易。相反，称为平均精度（AP）的数值指标可以帮助我们比较不同的检测器。AP是精确率×召回率曲线下的面积（AUC）。比较面积大小较为简单，因此我们使用AP作为评估指标。
    </p>
    <h2>
     <a id="5_107">
     </a>
     5.评估结论
    </h2>
    <h3>
     <a id="A_109">
     </a>
     A.平均精度
    </h3>
    <p>
     八种算法的AP值如表II所示。在所有检测器中，Grid R-CNN表现最佳（82.4%），而RefineDet表现最差（69.5%）。在两级网络中，Cascade R-CNN表现最佳（79.4%），而作为我们实验中两级网络主要框架的Faster R-CNN表现最差（70.5%）。在单级网络中，SSD512（78.7%）和RetinaNet（77.9%）均表现良好，而YOLOv3仅达到72.3%。尽管单级网络牺牲了检测性能以获得较高的实现效率，但SSD512的AP值与FPN相同，这表明SSD512可能是需要高计算效率任务的良好替代方案。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a06479801b9845669f2dadae624ea84e.png"/>
    </p>
    <p>
     为了进一步评估算法的性能，我们将测试集分为两个子集。其中一个子集名为Det-FlySimple，包含背景相对简单（如干净的天空）、传感器与目标距离较短且飞行速度较低的图像。另一个子集名为Det-Fly-Complex，包含背景较为复杂（如复杂的城市环境）且目标尺寸较小的图像。这两个数据集各占整个数据集约50%的图像。在Det-Fly-Simple上的评估结果表明，两阶段网络Cascade R-CNN和Grid R-CNN在所有八种网络中取得了最高的AP（超过82.0%）。在一阶段网络中，RetinaNet和SSD512表现最佳（接近81.0%）。除RefineDet和YOLOv3外，其他算法的性能均高于80.0%。与Det-Fly-Simple相比，大多数算法在Det-Fly-Complex上的检测性能显著下降，平均降幅接近5.0%，这主要是由于Det-Fly-Complex的高复杂性。算法的平均精度仅能达到74.4%。特别是，Grid R-CNN仍然表现最佳，也是唯一一个超过80.0%的算法。RetinaNet和SSD512表现相似，仍在一阶段网络中表现最好。总体而言，在此测试中，两阶段网络的性能略优于一阶段网络。
    </p>
    <p>
     总的来说，在所有评估场景中，Grid R-CNN和Cascade R-CNN表现出稳定且优越的性能。单阶段网络SSD512和RetinaNet也表现出稳定且良好的性能。由于它们能够实现更高的计算速度，SSD512和RetinaNet可能是计算资源有限任务的良好选择。
    </p>
    <h3>
     <a id="B__122">
     </a>
     B. 影响无人机检测的网络属性
    </h3>
    <p>
     算法的推理速度是实际应用中的一个重要方面，尤其是在嵌入式系统中。图4展示了我们实验中八种深度学习算法的平均推理时间。可以看出，单阶段网络的推理速度比两阶段网络更快。尽管Grid R-CNN在所有算法中取得了最佳的AP性能，但它也是最耗时的。YOLOv3的推理时间（32毫秒）几乎是Grid R-CNN（157毫秒）的五分之一。如果计算效率是应用中的优先考虑因素，则推荐使用YOLOv3，因为它是最快的，并且其性能优于其他两种算法（RefineDet和Faster R-CNN），如表II所示。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7b920ca1986a4cb090b8948f1c36619a.png"/>
    </p>
    <p>
     在我们的实验中，除了YOLOv3之外，所有对比模型均采用了ResNet-50作为骨干网络。尽管ResNet-50已经在一些嵌入式设备上实现了实时运行，但人们可能对使用更轻量级骨干网络的性能感兴趣。为此，我们在数据集上测试了以MobileNetv2为骨干网络的SSD512。结果显示，该模型在数据集上的AP为68.8%，比使用ResNet-50的SSD512低了近10%。然而，使用MobileNetv2的SSD512推理时间（53毫秒）远短于使用ResNet-50的SSD512（84毫秒）。因此，当板载计算资源极其有限时，可以考虑使用MobileNet等更轻量级的骨干网络。
    </p>
    <p>
     FPN和Faster R-CNN的不同表现表明，FPN网络结构可以显著提升无人机检测能力。由于Grid R-CNN和Cascade R-CNN的性能优于且比Faster R-CNN更稳健，这表明网格引导机制和多阶段结构可以生成回归效果更好的边界框。尽管使用多阶段结构会耗费更多时间，但网格机制在未来的检测器设计中是高度推荐的。此外，RefineDet的表现弱于SSD512，这可能表明高分辨率输入也能提升无人机检测能力。另外，RetinaNet在单阶段网络中表现出色且稳定，这表明焦点损失可能是解决类别不平衡问题的推荐方法。
    </p>
    <h3>
     <a id="C__132">
     </a>
     C. 影响无人机检测的图像属性
    </h3>
    <p>
     接下来，我们评估图像的一些关键方面，如环境背景、目标尺度、视角以及其他具有挑战性的条件对检测性能的影响。由于算法的性能可能受到训练不足和不同参数等多方面因素的影响，我们以这些算法的平均精度均值（mAP）作为公平评估的标准。
    </p>
    <ol>
     <li>
      环境背景：背景场景的复杂性对无人机检测性能有很大影响。表III展示了算法在不同类型环境背景下的平均精度（AP）。特别是，平均精度均值（mAP）表明，天空（88.3%）是最容易进行无人机检测的背景类型，而城市（62.0%）则是最困难的。这与我们的直觉一致，即复杂的城市背景使得视觉无人机检测极具挑战性。
     </li>
    </ol>
    <p>
     在算法性能方面，Grid R-CNN在不同类型的背景场景中表现出稳定且高精度的特点，而Faster R-CNN和RefineDet在背景复杂度增加时性能迅速下降。
    </p>
    <ol start="2">
     <li>
      <p>
       目标尺度：图像中目标无人机的大小对检测性能有很大影响。图5展示了所有算法在不同目标大小/比例下的平均精度（AP）。如图所示，随着目标尺度的增加，所有算法的AP以不同的速率提升。特别是，Grid R-CNN在不同目标尺度下表现出最佳性能，而RefineDet和Faster R-CNN在目标尺度变小时性能迅速下降。
      </p>
     </li>
     <li>
      <p>
       视角影响：通过实验我们注意到，目标无人机的视角也会对检测性能产生影响。图6展示了不同视角下的平均精度（AP）。可以看出，底部视角的精度最高，而正面视角的精度最低。原因在于，在底部视角的情况下，目标展示了丰富的几何信息，同时背景场景为蓝天或云层。然而，在正面视角的情况下，目标较为扁平，因此展示的几何信息较少，同时背景可能比底部视角的情况更为复杂。
      </p>
     </li>
    </ol>
    <p>
     图5展示了不同目标尺度下各算法的AP（平均精度）。如果标注边界框的宽度和高度分别小于整个图像宽度和高度的
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        x 
        
       
         （ 
        
       
         x 
        
       
         ∈ 
        
       
         { 
        
       
         1 
        
       
         / 
        
       
         40 
        
       
         , 
        
       
         1 
        
       
         / 
        
       
         20 
        
       
         , 
        
       
         1 
        
       
         / 
        
       
         10 
        
       
         } 
        
       
         ） 
        
       
      
        x（x ∈ \{1/40, 1/20, 1/10\}）
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.7224em; vertical-align: -0.0391em;">
         </span>
         <span class="mord mathnormal">
          x
         </span>
         <span class="mord cjk_fallback">
          （
         </span>
         <span class="mord mathnormal">
          x
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
         <span class="mrel">
          ∈
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1em; vertical-align: -0.25em;">
         </span>
         <span class="mopen">
          {
          <!-- -->
         </span>
         <span class="mord">
          1/40
         </span>
         <span class="mpunct">
          ,
         </span>
         <span class="mspace" style="margin-right: 0.1667em;">
         </span>
         <span class="mord">
          1/20
         </span>
         <span class="mpunct">
          ,
         </span>
         <span class="mspace" style="margin-right: 0.1667em;">
         </span>
         <span class="mord">
          1/10
         </span>
         <span class="mclose">
          }
         </span>
         <span class="mord cjk_fallback">
          ）
         </span>
        </span>
       </span>
      </span>
     </span>
     ，则将其分类为
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        &lt; 
        
       
         x 
        
       
         [ 
        
       
         W 
        
       
         , 
        
       
         H 
        
       
         ] 
        
       
      
        &lt; x[W, H]
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 0.5782em; vertical-align: -0.0391em;">
         </span>
         <span class="mrel">
          &lt;
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1em; vertical-align: -0.25em;">
         </span>
         <span class="mord mathnormal">
          x
         </span>
         <span class="mopen">
          [
         </span>
         <span class="mord mathnormal" style="margin-right: 0.1389em;">
          W
         </span>
         <span class="mpunct">
          ,
         </span>
         <span class="mspace" style="margin-right: 0.1667em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0813em;">
          H
         </span>
         <span class="mclose">
          ]
         </span>
        </span>
       </span>
      </span>
     </span>
     。AP由算法根据内部数据计算得出。mAP表示每个尺度区间内八种算法的平均AP。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ff2cb8934467483a8872ac41664e29a8.png"/>
    </p>
    <p>
     图6展示了不同视角下的AP（平均精度）。该图分为三个部分，分别是顶部视角（Top）、正面视角（Fro）和底部视角（Bot）。每个部分的纵轴表示算法的AP值，范围为0.5到1.0。每个部分的mAP（平均精度均值）分别约为0.78（Top）、0.72（Fro）和0.85（Bot）。每个部分中的标记代表算法的性能。
    </p>
    <ol start="4">
     <li>
      其他挑战性条件：该数据集涵盖了一些挑战性条件，如强光/弱光、运动模糊和部分遮挡。在我们的数据集中，这三种场景的图像比例分别为10.8%、11.2%和0.8%。其中，部分遮挡指的是目标无人机的部分区域不在视野范围内。这些情况下的所有图像都可以在我们的在线数据集中找到。
     </li>
    </ol>
    <p>
     表IV报告了八种算法在三种挑战性条件下的测试结果。值得注意的是，部分遮挡会导致AP显著降低。部分原因是部分遮挡目标检测确实是一项具有挑战性的任务，同时，这种情况下的图像在数据集中所占比例较小。另一方面，强/弱光照条件和运动模糊并未显著影响性能，这验证了深度学习算法的鲁棒性。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9a7991e060a94770a68ce0e1ff6fd7dc.png"/>
    </p>
    <h3>
     <a id="D__158">
     </a>
     D. 与最先进数据集的对比
    </h3>
    <p>
     据我们所知，MIDGARD 是迄今为止最新的、专为基于深度学习的微型无人机检测设计的综合数据集 [32]。与 MIDGARD 相比，Det-Fly 中每张图像的标注边界框更为紧凑，因为 Det-Fly 中的图像是由专业人员逐一手动标注的，而 MIDGARD 中的图像则是基于 UVDAR 和相对姿态估计自动标注的。此外，Det-Fly 涵盖了更广泛的相对目标距离范围。特别是，Det-Fly 中最长的相对目标距离超过 100 米，而 MIDGARD 中最长距离不到 20 米。由于相对距离范围广泛，Det-Fly 中目标无人机的尺度更加多样化。
    </p>
    <p>
     这八种算法已在MIDGARD上进行了训练和测试，测试结果如表V所示。可以看出，MIDGARD的结果比Det-Fly高出10%。这可能是由于Det-Fly样本的复杂性和多样性所致。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/98cfd7415388438484029eedde75fbac.png"/>
    </p>
    <h2>
     <a id="6_166">
     </a>
     6.讨论
    </h2>
    <p>
     这封信介绍了一个名为Det-Fly的新数据集，用于空对空无人机检测，并基于该数据集评估了八种代表性的深度学习算法。不仅对算法的整体性能进行了仔细评估和比较，还分析了环境背景、目标尺度、视角以及其他挑战性条件对检测性能的影响。根据实验结果，提出了未来如何设计算法以实现更好检测性能的建议。
    </p>
    <p>
     未来，为了在各种环境中检测未知无人机，应通过增加更多类型的无人机和背景场景来进一步丰富数据集。此外，有必要进行消融研究，以设计特别适用于无人机检测任务的深度学习算法，并实现其机载部署。同时，可以采用可解释性技术来解释为什么推荐的网络结构或方法能够提高检测性能。此外，能够处理高分辨率图像的算法也需要更多关注。
    </p>
    <h2>
     <a id="7_173">
     </a>
     7.引用文献
    </h2>
    <ul>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [1] Y. Tang et al., “Vision-aided multi-UAV autonomous flocking in GPSdenied environment,” IEEE Trans. Ind. Electron., vol. 66, no. 1, pp. 616–626, Jan. 2019.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [2] J. Xie, J. Yu, J. Wu, Z. Shi, and J. Chen, “Adaptive switching spatialtemporal fusion detection for remote flying drones,” IEEE Trans. Veh. Technol., vol. 69, no. 7, pp. 6964–6976, Jul. 2020.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [3] R. Mitchell and I. Chen, “Adaptive intrusion detection of malicious unmanned air vehicles using behavior rule specifications,” IEEE Trans. Syst., Man, Cybern. Syst., vol. 44, no. 5, pp. 593–604, May 2014.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [4] J. Zhang, C. Hu, R. G. Chadha, and S. Singh, “Maximum likelihood path planning for fast aerial maneuvers and collision avoidance,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2019, pp. 2805–2812.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [5] J. Ren and X. Jiang, “Regularized 2-d complex-log spectral analysis and subspace reliability analysis of micro-doppler signature for UAV detection,” Pattern Recognit., vol. 69, pp. 225–237, 2017.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [6] A. Bernardini, F. Mangiatordi, E. Pallotti, and L. Capodiferro, “Drone detection by acoustic signature identification,” Electron. Imag., vol. 2017, no. 10, pp. 60–64, 2017.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [7] R. Yoshihashi, T. T. Trinh, R. Kawakami, S. You, M. Iida, and T. Naemura, “Differentiating objects by motion: Joint detection and tracking of small flying objects,” Comput. Vis. Pattern Recognit., 2017, arXiv:1709.04666.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [8] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for dense object detection,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2017, pp. 2980–2988.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [9] W. Liu et al., “SSD: Single shot multibox detector,” in Proc. Eur. Conf. Comput. Vis., 2016, pp. 21–37.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [10] J. Redmon and A. Farhadi, “YOLOv3: An incremental improvement,” 2018, arXiv:1804.02767.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [11] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie, “Feature pyramid networks for object detection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 936–944.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [12] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards real-time object detection with region proposal networks,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [13] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Single-shot refinement neural network for object detection,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 4203–4212.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [14] X. Lu, B. Li, Y. Yue, Q. Li, and J. Yan, “Grid R-CNN,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp. 7363–7372.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [15] Z. Cai and N. Vasconcelos, “Cascade R-CNN: Delving into high quality object detection,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6154–6162.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [16] F. Lin, K. Peng, X. Dong, S. Zhao, and B. M. Chen, “Vision-based formation for UAVs,” in Proc. IEEE Int. Conf. Control Automat., 2014, pp. 1375–1380.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [17] F. Gökçe, G. Üçoluk, E. S ̧ ahin, and S. Kalkan, “Vision-based detection and distance estimation of micro unmanned aerial vehicles,” Sensors, vol. 15, no. 9, pp. 23805–23846, 2015.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [18] K. R. Sapkota et al., “Vision-based unmanned aerial vehicle detection and tracking for sense and avoid systems,” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2016, pp. 1556–1561.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [19] J. Li, D. H. Ye, T. Chung, M. Kolsch, J. Wachs, and C. Bouman, “Multitarget detection and tracking from a single camera in unmanned aerial vehicles (UAVs),” in Proc. IEEE/RSJ Int. Conf. Intell. Robot. Syst., 2016, pp. 4992–4997.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [20] S. Minaeian, J. Liu, and Y.-J. Son, “Effective and efficient detection of moving targets from a UAV’s camera,” IEEE Trans. Intell. Transp. Syst., vol. 19, no. 2, pp. 497–506, Feb. 2018.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [21] R. Opromolla, G. Fasano, and D. Accardo, “A vision-based approach to UAV detection and tracking in cooperative applications,” Sensors, vol. 18, no. 10, 2018.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [22] Y. Wu, Y. Sui, and G. Wang, “Vision-based real-time aerial object localization and tracking for UAV sensing system,” IEEE Access, vol. 5, pp. 23 969–23 978, 2017.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [23] S. Minaeian, J. Liu, and Y. Son, “Vision-based target detection and localization via a team of cooperative UAV and UGVs,” IEEE Trans. Syst., Man, Cybern. Syst., vol. 46, no. 7, pp. 1005–1016, Jul. 2016.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [24] F. Lin, X. Dong, B. M. Chen, K. Lum, and T. H. Lee, “A robust realtime embedded vision system on an unmanned rotorcraft for ground target following,” IEEE Trans. Ind. Electron., vol. 59, no. 2, pp. 1038–1049, Feb. 2012.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [25] A. Rozantsev, V. Lepetit, and P. Fua, “Detecting flying objects using a single moving camera,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 5, pp. 879–892, May 2017.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [26] J. James, J. J. Ford, and T. L. Molloy, “Learning to detect aircraft for long-range vision-based sense-and-avoid systems,” IEEE Robot. Automat. Lett., vol. 3, no. 4, pp. 4383–4390, Oct. 2018.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [27] C. Aker and S. Kalkan, “Using deep networks for drone detection,” in Proc. IEEE Int. Conf. Adv. Video Signal Based Surveill., 2017, pp. 1–6.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [28] A. Schumann, L. Sommer, J. Klatte, T. Schuchert, and J. Beyerer, “Deep cross-domain flying object classification for robust UAV detection,” in Proc. IEEE Int. Conf. Adv. Video Signal Based Surveill., 2017, pp. 1–6.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [29] A. Rozantsev, V. Lepetit, and P. Fua, “Flying objects detection from a single moving camera,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2015, pp. 4128–4136.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [30] Y. Chen, P. Aggarwal, J. Choi, and C. C. J. Kuo, “A deep learning approach to drone monitoring,” in Proc. Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit Conf., 2017, pp. 686–691.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [31] X. Peng, B. Sun, K. Ali, and K. Saenko, “Learning deep object detectors from 3D models,” in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 12781286.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [32] V. Walter, M. Vrba, and M. Saska, “On training datasets for machine learning-based visual relative localization of micro-scale UAVs,” in Proc. IEEE Int. Conf. Robot. Automat., 2020, pp. 10 674–10 680.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [33] V. Walter, M. Saska, and A. Franchi, “Fast mutual relative localization of UAVs using ultraviolet led markers,” in Proc. Int. Conf. Unmanned Aircr. Syst., 2018, pp. 1217–1226.
     </li>
     <li class="task-list-item">
      <input class="task-list-item-checkbox" disabled="disabled" type="checkbox"/>
      [34] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770–778.
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34343138343835322f:61727469636c652f64657461696c732f313436313836363530" class_="artid" style="display:none">
 </p>
</div>


