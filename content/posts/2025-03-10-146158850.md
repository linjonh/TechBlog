---
layout: post
title: "大语言模型-01-语言模型发展历程-02-从神经网络到ELMo"
date: 2025-03-10 17:11:12 +0800
description: "本博客内容是《大语言模型》一书的读书笔记，本文主要记录datawhale的活动学习笔记，本文为神经网络到ELMO。"
keywords: "大语言模型-01-语言模型发展历程-02-从神经网络到ELMo"
categories: ['大模型Llm']
tags: ['语言模型', '神经网络', '人工智能', 'Datawhale']
artid: "146158850"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146158850
    alt: "大语言模型-01-语言模型发展历程-02-从神经网络到ELMo"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146158850
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146158850
cover: https://bing.ee123.net/img/rand?artid=146158850
image: https://bing.ee123.net/img/rand?artid=146158850
img: https://bing.ee123.net/img/rand?artid=146158850
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     大语言模型-01-语言模型发展历程-02-从神经网络到ELMo
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h3>
     <a id="_0">
     </a>
     简介
    </h3>
    <p>
     本博客内容是《大语言模型》一书的读书笔记，该书是中国人民大学高瓴人工智能学院赵鑫教授团队出品，覆盖大语言模型训练与使用的全流程，从预训练到微调与对齐，从使用技术到评测应用，帮助学员全面掌握大语言模型的核心技术。并且，课程内容基于大量的代码实战与讲解，通过实际项目与案例，学员能将理论知识应用于真实场景，提升解决实际问题的能力。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/8462d74a8d764cc7a56c909eade4c15a.png"/>
    </p>
    <p>
     本文主要记录datawhale的活动学习笔记，
     <a href="https://www.datawhale.cn/activity/150" rel="nofollow">
      可点击活动连接
     </a>
    </p>
    <h3>
     <a id="11_5">
     </a>
     1.1语言模型发展历程
    </h3>
    <h5>
     <a id="Neural_Language_ModelsNLM_7">
     </a>
     神经语言模型（Neural Language Models,NLM）
    </h5>
    <blockquote>
     <p>
      在自然语言处理领域，NLM 指神经语言模型（Neural Language Models）。它利用神经网络来学习和表示语言的概率分布，能够更加精确地理解、处理和生成自然语言。通过深度学习和神经网络的结合，从大量的文本数据中学习语言的统计规律和上下文信息，从而捕捉到词语之间的关联和语义信息，提高对自然语言的理解能力。
      <br/>
      这种模型通常采用循环神经网络（RNN）、长短时记忆网络（LSTM）和变压器（Transformer）等结构，以建模文本序列并预测下一个单词或字符的概率分布。
      <br/>
      神经语言模型在自然语言处理领域的应用越来越广泛，涵盖了机器翻译、语音识别、文本生成等多个任务。
     </p>
    </blockquote>
    <h6>
     <a id="MLPNNLP_13">
     </a>
     早期工作（MLP或NNLP）原理
    </h6>
    <p>
     早期工作MLP（Multilayer Perceptron,MLP，多层感知机）：
     <br/>
     NNLM（Neural Network Language Model，神经网络语言模型），单词映射到词向量，再由神经网络预测当前时刻词汇。是一种通过神经网络进行语言建模的技术，通常用于预测序列中的下一个词。
    </p>
    <p>
     NNLM的核心思想是使用词嵌入（word embedding）将词转换为低维向量，并通过神经网络学习语言中的词序关系。
     <br/>
     NNLM的基本结构包括以下几个部分：
    </p>
    <blockquote>
     <p>
      输入层：输入一个固定长度的词窗口，例如 n 个词的上下文（前 n - 1 个词）作为输入。
      <br/>
      嵌入层：将每个输入词映射到一个低维空间，得到词向量。这一层的权重矩阵通常表示为词嵌入矩阵。
      <br/>
      隐藏层：一个或多个隐藏层可以捕获词之间的关系，一般是全连接层。
      <br/>
      输出层：用于预测下一个词的概率分布，通常使用softmax函数。
     </p>
    </blockquote>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7d6da8edfe8e4ef3ae218c6c8e9a6e21.png"/>
    </p>
    <blockquote>
     <p>
      A goal of statistical language modeling is to learn the joint probabilityfunction of sequences of words.
      <br/>
      统计语言建模的一个目标是学习单词序列的联合概率函数。
      <br/>
      This is intrinsically difficult because ofthe curse of dimensionality:we propose to fight it with its own weapons.
      <br/>
      这本质上是困难的，因为维度诅咒：我们建议用自己的武器来对抗它。
      <br/>
      In the proposed approach one learns simultaneously(1)a distributed rep-resentation for each word (i.e.a similarity between words)along with(2)the probability function for word sequences,expressed with these repre-sentations.
      <br/>
      在所提出的方法中，人们同时学习（1）每个单词的分布式表示（即单词之间的相似性）以及（2）用这些表示表示的单词序列的概率函数。
      <br/>
      Generalization is obtained because a sequence of words that has never been seen before gets high probability, if it is made of words that are similar to words forming an already seen sentence.
      <br/>
      之所以获得泛化，是因为如果一个以前从未见过的单词序列是由与已经见过的句子中的单词相似的单词组成的，那么它的概率很高。
      <br/>
      We report onexperiments using neural networks for the probability function,showingon two text corpora that the proposed approach very significantly im-proves on a state-of-the-art trigram model.
      <br/>
      我们报告了一项使用神经网络进行概率函数的实验，在两个文本语料库上表明，所提出的方法在最先进的三元组模型上得到了非常显著的改进。
     </p>
    </blockquote>
    <p>
     该模型主要任务是
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/518e9d7c44074326a91c442a665d9cf9.png"/>
    </p>
    <p>
     该模型的计算过程如下：
    </p>
    <blockquote>
     <p>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7f4993aa571b43de8c13617889baa74e.png">
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5d008c3c877e4579ab5de121595ac845.png">
        <br/>
        <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/36ae6632df25455096c8e013abebdf22.png"/>
       </img>
      </img>
     </p>
    </blockquote>
    <h6>
     <a id="MLPNNLP_48">
     </a>
     早期工作（MLP或NNLP）代码实现
    </h6>
    <p>
     参考：
     <br/>
     <a href="https://blog.csdn.net/weixin_62472350/article/details/143448417">
      https://blog.csdn.net/weixin_62472350/article/details/143448417
     </a>
     <br/>
     <a href="https://www.bilibili.com/video/BV1AT4y1J7bv/" rel="nofollow">
      https://www.bilibili.com/video/BV1AT4y1J7bv/
     </a>
     <br/>
     <a href="https://wmathor.com/index.php/archives/1442/" rel="nofollow">
      NNLM 的 PyTorch 实现
     </a>
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 1.导入必要的库</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimizer
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
 
<span class="token comment"># 2.数据准备</span>
sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"I like milk"</span><span class="token punctuation">,</span>
             <span class="token string">"I love hot-pot"</span><span class="token punctuation">,</span>
             <span class="token string">"I hate coffee"</span><span class="token punctuation">,</span>
             <span class="token string">"I want sing"</span><span class="token punctuation">,</span>
             <span class="token string">"I am sleep"</span><span class="token punctuation">,</span>
             <span class="token string">"I go home"</span><span class="token punctuation">,</span>
             <span class="token string">"Love you forever"</span><span class="token punctuation">]</span>
 
word_list <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># 获取个句子单词</span>
word_list <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">)</span>            <span class="token comment"># 获取单词列表</span>
 
word_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>w<span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">}</span>     <span class="token comment"># 单词-位置索引字典</span>
number_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>i<span class="token punctuation">:</span> w <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">}</span>   <span class="token comment"># 位置-单词索引字典</span>
 
vocab_size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span>         <span class="token comment"># 词汇表大小</span>
 
<span class="token comment"># 3.X-生成输入和输出数据</span>
<span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    output_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
 
    <span class="token keyword">for</span> sen <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        word <span class="token operator">=</span> sen<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
        input_temp <span class="token operator">=</span> <span class="token punctuation">[</span>word_dict<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> word<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        output_temp <span class="token operator">=</span> word_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
 
        input_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_temp<span class="token punctuation">)</span>
        output_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output_temp<span class="token punctuation">)</span>
 
    <span class="token keyword">return</span> input_data<span class="token punctuation">,</span> output_data
 
 
input_data<span class="token punctuation">,</span> output_data <span class="token operator">=</span> make_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
input_data<span class="token punctuation">,</span> output_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>output_data<span class="token punctuation">)</span>   <span class="token comment"># 数据转换：将 input_data 和 output_data 转换为 LongTensor，以便用于模型训练。</span>
dataset <span class="token operator">=</span> Data<span class="token punctuation">.</span>TensorDataset<span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> output_data<span class="token punctuation">)</span>       <span class="token comment"># 建数据集：Data.TensorDataset 将输入和输出配对为数据集</span>
loader <span class="token operator">=</span> Data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">)</span>                  <span class="token comment"># 数据加载器：DataLoader，使用批量大小为 2，随机打乱样本</span>
 
 
<span class="token comment"># 4.初始化参数</span>
m <span class="token operator">=</span> <span class="token number">2</span>
n_step <span class="token operator">=</span> <span class="token number">2</span>
n_hidden <span class="token operator">=</span> <span class="token number">10</span>
 
 
<span class="token comment"># 5.模型定义</span>
<span class="token keyword">class</span> <span class="token class-name">NNLM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>NNLM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>C <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> m<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>H <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_step <span class="token operator">*</span> m<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_step <span class="token operator">*</span> m<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>C<span class="token punctuation">(</span>X<span class="token punctuation">)</span>               <span class="token comment"># X = [batch_size, n_step, m]</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_step <span class="token operator">*</span> m<span class="token punctuation">)</span>  <span class="token comment"># 展平 X = [batch_size, n_step * m]</span>
        hidden_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d <span class="token operator">+</span> self<span class="token punctuation">.</span>H<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>b <span class="token operator">+</span> self<span class="token punctuation">.</span>W<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>U<span class="token punctuation">(</span>hidden_output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
 
<span class="token comment"># 6.定义训练过程</span>
model <span class="token operator">=</span> NNLM<span class="token punctuation">(</span><span class="token punctuation">)</span>
optim <span class="token operator">=</span> optimizer<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-3</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
<span class="token comment"># 7.模型训练</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_y <span class="token keyword">in</span> loader<span class="token punctuation">:</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        optim<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
 
<span class="token comment"># 8.模型测试</span>
pred <span class="token operator">=</span> model<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>number_dict<span class="token punctuation">[</span>idx<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> pred<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      代码解释
     </strong>
     <br/>
     <strong>
      1.最开始导入一些必要的库
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 1.导入必要的库</span>
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optimizer
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">as</span> Data
</code></pre>
    <p>
     <strong>
      2.首先需要准备一些数据，用于模型训练并测试
     </strong>
    </p>
    <pre><code class="prism language-python">word_list <span class="token operator">=</span> <span class="token string">" "</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>sentences<span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     ①对于文本数据，肯定要进行一个分词操作，先使用" ".join(sentences).split()来切分每一个句子的每个单词，这时候获得的word_list列表就是：
    </p>
    <blockquote>
     <p>
      [‘I’, ‘like’, ‘milk’, ‘I’, ‘love’, ‘hot-pot’, ‘I’, ‘hate’, ‘coffee’, ‘I’, ‘want’, ‘sing’, ‘I’, ‘am’, ‘sleep’, ‘I’, ‘go’, ‘home’, ‘Love’, ‘you’, ‘forever’]
     </p>
    </blockquote>
    <p>
     ②但是这里得到的单词可能会有重复的情况，我们需要得到不重复的单词列表，为后面的创建词典提供方便。
    </p>
    <pre><code class="prism language-python">word_list <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">)</span>  
</code></pre>
    <p>
     set (word_list) ：将 word_list 转换为集合（set），自动去除列表中的重复元素。
     <br/>
     list(set (word_list) )：再将集合转换回列表，这样可以保持原数据类型一致（即 word_list 仍然是一个列表）。
    </p>
    <blockquote>
     <p>
      [‘milk’, ‘coffee’, ‘sing’, ‘hot-pot’, ‘home’, ‘you’, ‘am’, ‘I’, ‘sleep’, ‘love’, ‘want’, ‘hate’, ‘go’, ‘forever’, ‘like’, ‘Love’]
     </p>
    </blockquote>
    <p>
     ③然后就是构造词典：单词到位置的索引字典、位置到单词的索引字典。
    </p>
    <pre><code class="prism language-python">word_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>w<span class="token punctuation">:</span> i <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">}</span>     <span class="token comment"># 单词-位置索引字典</span>
number_dict <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>i<span class="token punctuation">:</span> w <span class="token keyword">for</span> i<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>word_list<span class="token punctuation">)</span><span class="token punctuation">}</span>   <span class="token comment"># 位置-单词索引字典</span>
</code></pre>
    <blockquote>
     <p>
      word_dict： {‘sleep’: 0, ‘go’: 1, ‘home’: 2, ‘milk’: 3, ‘hate’: 4, ‘Love’: 5, ‘love’: 6, ‘am’: 7, ‘want’: 8, ‘sing’: 9, ‘forever’: 10, ‘hot-pot’: 11, ‘I’: 12, ‘like’: 13, ‘you’: 14, ‘coffee’: 15}
     </p>
    </blockquote>
    <blockquote>
     <p>
      number_dict：{0: ‘sleep’, 1: ‘go’, 2: ‘home’, 3: ‘milk’, 4: ‘hate’, 5: ‘Love’, 6: ‘love’, 7: ‘am’, 8: ‘want’, 9: ‘sing’, 10: ‘forever’, 11: ‘hot-pot’, 12: ‘I’, 13: ‘like’, 14: ‘you’, 15: ‘coffee’}
     </p>
    </blockquote>
    <pre><code>    这里使用enumerate是因为 enumerate 函数可以方便地同时获取列表元素的索引和对应的值。也就是我们想要的字典。
</code></pre>
    <p>
     ④然后就是获得词汇表大小，在模型搭建中需要用到。
    </p>
    <p>
     <strong>
      3.有了初始数据，我们需要构建出数据X，也就是输入数据和目标输出数据。
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">make_data</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    input_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    output_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
 
    <span class="token keyword">for</span> sen <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        word <span class="token operator">=</span> sen<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span>
        input_temp <span class="token operator">=</span> <span class="token punctuation">[</span>word_dict<span class="token punctuation">[</span>n<span class="token punctuation">]</span> <span class="token keyword">for</span> n <span class="token keyword">in</span> word<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
        output_temp <span class="token operator">=</span> word_dict<span class="token punctuation">[</span>word<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
 
        input_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>input_temp<span class="token punctuation">)</span>
        output_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>output_temp<span class="token punctuation">)</span>
 
    <span class="token keyword">return</span> input_data<span class="token punctuation">,</span> output_data
</code></pre>
    <p>
     ①先构建空的输入数据input_data和输出数据output_data。
    </p>
    <p>
     ②将每个句子的前 n-1个单词的位置添加到输入数据input_data中，第 n 个单词的位置添加到输入数据output_data中，得到每个输入 x 和输出 y 在词汇表中的顺序：
    </p>
    <blockquote>
     <p>
      input_data：[[12, 13], [12, 6], [12, 4], [12, 8], [12, 7], [12, 1], [5, 14]]
     </p>
    </blockquote>
    <blockquote>
     <p>
      output_data：[3, 11, 15, 9, 0, 2, 10]
     </p>
    </blockquote>
    <p>
     这是个二维的矩阵，行元素代表一个句子中用于训练输入/测试输出的单词在词汇表中的位置索引；列元素是不同的句子。
     <br/>
     每个句子都是3个单词，前2个作为前文信息作为输入，第3个作为预测输出，我们前面给的一共是7个句子。
    </p>
    <p>
     <strong>
      4.初始化参数
     </strong>
    </p>
    <p>
     这里的m指的是维度，也就是一个单词要嵌入到多少维度，由于这里的数据量比较小，每个句子也只有3个单词，所以这给出的维度选个很低的2。
    </p>
    <blockquote>
     <p>
      n_step=2，指的是用两个单词来预测下一个目标单词。
      <br/>
      n_hidden=10，指的是隐藏层的数量。
     </p>
    </blockquote>
    <p>
     <strong>
      5.前面的数据已经初步定义好了，这里就要搭建NNLM模型了。
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">NNLM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>NNLM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>C <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab_size<span class="token punctuation">,</span> m<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>H <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_step <span class="token operator">*</span> m<span class="token punctuation">,</span> n_hidden<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>d <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>n_hidden<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>U <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_hidden<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>n_step <span class="token operator">*</span> m<span class="token punctuation">,</span> vocab_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>vocab_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
 
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X <span class="token operator">=</span> self<span class="token punctuation">.</span>C<span class="token punctuation">(</span>X<span class="token punctuation">)</span>               <span class="token comment"># X = [batch_size, n_step, m]</span>
        X <span class="token operator">=</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_step <span class="token operator">*</span> m<span class="token punctuation">)</span>  <span class="token comment"># 展平 X = [batch_size, n_step * m]</span>
        hidden_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d <span class="token operator">+</span> self<span class="token punctuation">.</span>H<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
        output <span class="token operator">=</span> self<span class="token punctuation">.</span>b <span class="token operator">+</span> self<span class="token punctuation">.</span>W<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>U<span class="token punctuation">(</span>hidden_output<span class="token punctuation">)</span>
        <span class="token keyword">return</span> output
</code></pre>
    <p>
     ①def _init_(self)：定义各层和参数：
    </p>
    <blockquote>
     <p>
      self.C：词嵌入层，将输入词转换为词向量。
      <br/>
      vocab_size 定义词汇表的大小，m 是词嵌入的维度，表示每个词将被嵌入成 m 维的向量。
     </p>
    </blockquote>
    <blockquote>
     <p>
      self.H：线性层，将展平后的输入映射到隐藏层。
      <br/>
      n_step * m 是展平后的输入大小，n_hidden 是隐藏层的维度，用来控制隐藏层输出的特征数量。
     </p>
    </blockquote>
    <blockquote>
     <p>
      self.d：偏置向量，用于隐藏层的输出。
      <br/>
      n_hidden 是偏置项的维度，与隐藏层输出匹配，用于提升隐藏层的表达能力。
     </p>
    </blockquote>
    <blockquote>
     <p>
      self.U：线性层，将隐藏层输出映射到词汇表空间。
      <br/>
      n_hidden 是隐藏层输出的大小，vocab_size 是词汇表大小，用于将隐藏层的特征映射到每个词的预测空间。
     </p>
    </blockquote>
    <blockquote>
     <p>
      self.W：线性层，从输入直接映射到词汇表空间。
      <br/>
      n_step * m 是展平后的输入大小，vocab_size 是词汇表大小，用于将输入直接映射到词汇表的预测空间。
     </p>
    </blockquote>
    <blockquote>
     <p>
      self.b：偏置向量，用于最终输出层的分数调整。
      <br/>
      vocab_size 是词汇表的大小，用作最终输出层的偏置。
     </p>
    </blockquote>
    <p>
     这里分别用了Embedding、Linear和Parameter：
    </p>
    <blockquote>
     <p>
      Embedding：嵌入层，用于将离散的词汇索引（如单词的整数表示）映射到连续的稠密向量空间。
      <br/>
      Linear：全连接层（线性层），用于将输入的特征通过线性变换映射到输出空间。
      <br/>
      Parameter：可学习的参数。
     </p>
    </blockquote>
    <p>
     ②def forward(self,X)：定义神经网络在前向传播过程中的计算步骤
    </p>
    <pre><code class="prism language-python">X <span class="token operator">=</span> self<span class="token punctuation">.</span>C<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre>
    <blockquote>
     <p>
      首先通过嵌入层将输入的词索引（X）转换为词向量表示，这个时候得到是三维度的：[batch_size, n_step, m]。
     </p>
    </blockquote>
    <pre><code class="prism language-python">X <span class="token operator">=</span> X<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> n_step <span class="token operator">*</span> m<span class="token punctuation">)</span>
</code></pre>
    <blockquote>
     <p>
      然后将 X 从三维张量展平为二维张量 [batch_size, n_step * m]，方便输入到全连接层 self.H。
     </p>
    </blockquote>
    <pre><code class="prism language-python">hidden_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d <span class="token operator">+</span> self<span class="token punctuation">.</span>H<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <blockquote>
     <p>
      接着，利用公式
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         h 
         
        
          = 
         
        
          t 
         
        
          a 
         
        
          n 
         
        
          h 
         
        
          ( 
         
         
         
           W 
          
         
           h 
          
         
        
          X 
         
        
          + 
         
        
          d 
         
        
          ) 
         
        
       
         h=tanh(W_hX+d)
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.6944em;">
          </span>
          <span class="mord mathnormal">
           h
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
          <span class="mrel">
           =
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 1em; vertical-align: -0.25em;">
          </span>
          <span class="mord mathnormal">
           t
          </span>
          <span class="mord mathnormal">
           anh
          </span>
          <span class="mopen">
           (
          </span>
          <span class="mord">
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            W
           </span>
           <span class="msupsub">
            <span class="vlist-t vlist-t2">
             <span class="vlist-r">
              <span class="vlist" style="height: 0.3361em;">
               <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                <span class="pstrut" style="height: 2.7em;">
                </span>
                <span class="sizing reset-size6 size3 mtight">
                 <span class="mord mathnormal mtight">
                  h
                 </span>
                </span>
               </span>
              </span>
              <span class="vlist-s">
               ​
              </span>
             </span>
             <span class="vlist-r">
              <span class="vlist" style="height: 0.15em;">
               <span class="">
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
          <span class="mord mathnormal" style="margin-right: 0.0785em;">
           X
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
          <span class="mbin">
           +
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 1em; vertical-align: -0.25em;">
          </span>
          <span class="mord mathnormal">
           d
          </span>
          <span class="mclose">
           )
          </span>
         </span>
        </span>
       </span>
      </span>
      计算得到隐藏层输出。
     </p>
    </blockquote>
    <pre><code class="prism language-python">output <span class="token operator">=</span> self<span class="token punctuation">.</span>b <span class="token operator">+</span> self<span class="token punctuation">.</span>W<span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>U<span class="token punctuation">(</span>hidden_output<span class="token punctuation">)</span>
</code></pre>
    <blockquote>
     <p>
      最后，利用公式
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         o 
         
        
          u 
         
        
          t 
         
        
          p 
         
        
          u 
         
        
          t 
         
        
          = 
         
        
          b 
         
        
          + 
         
        
          W 
         
        
          X 
         
        
          + 
         
        
          U 
         
        
          h 
         
        
       
         output=b+WX+Uh
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.8095em; vertical-align: -0.1944em;">
          </span>
          <span class="mord mathnormal">
           o
          </span>
          <span class="mord mathnormal">
           u
          </span>
          <span class="mord mathnormal">
           tp
          </span>
          <span class="mord mathnormal">
           u
          </span>
          <span class="mord mathnormal">
           t
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
          <span class="mrel">
           =
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 0.7778em; vertical-align: -0.0833em;">
          </span>
          <span class="mord mathnormal">
           b
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
          <span class="mbin">
           +
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.1389em;">
           W
          </span>
          <span class="mord mathnormal" style="margin-right: 0.0785em;">
           X
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
          <span class="mbin">
           +
          </span>
          <span class="mspace" style="margin-right: 0.2222em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 0.6944em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.109em;">
           U
          </span>
          <span class="mord mathnormal">
           h
          </span>
         </span>
        </span>
       </span>
      </span>
      得到最终的输出。
     </p>
    </blockquote>
    <p>
     <strong>
      6.定义训练过程
     </strong>
    </p>
    <p>
     这里初始化model，并且设置优化器为Adam，并且使用了交叉熵损失。
    </p>
    <p>
     <strong>
      7.模型训练
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> batch_x<span class="token punctuation">,</span> batch_y <span class="token keyword">in</span> loader<span class="token punctuation">:</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>batch_x<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> batch_y<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">1000</span> <span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        optim<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optim<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     这里从数据加载器中加载数据，将当前批次的输入数据batch_x传入模型中得到预测结果，同时计算预测值与真实值的损失loss，每1000个epoch打印损失。然后梯度清零、反向传播计算梯度、更新模型参数。
    </p>
    <p>
     <strong>
      8.模型测试
     </strong>
    </p>
    <pre><code class="prism language-python">pred <span class="token operator">=</span> model<span class="token punctuation">(</span>input_data<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
</code></pre>
    <blockquote>
     <p>
      model(input_data)：将输入数据传递给模型，获取每个类别的得分（logits）。
      <br/>
      max(1, keepdim=True)[1]：
     </p>
     <blockquote>
      <p>
       max(1)：对每个样本找出最大得分的类别索引。
       <br/>
       keepdim=True：保持输出维度不变。
       <br/>
       [1]：提取每个样本的最大值索引（预测类别）。
      </p>
     </blockquote>
    </blockquote>
    <pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">[</span>number_dict<span class="token punctuation">[</span>idx<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">for</span> idx <span class="token keyword">in</span> pred<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <blockquote>
     <p>
      pred.squeeze()：移除维度为 1 的维度，得到一维张量。
      <br/>
      [idx.item() for idx in pred.squeeze()]：将每个索引转换为整数。
      <br/>
      number_dict[idx.item()]：通过索引查找可读标签。
      <br/>
      print([…])：打印出模型预测的类别标签。
     </p>
    </blockquote>
    <blockquote>
     <p>
      输出：
      <br/>
      1000 0.05966342240571976
      <br/>
      1000 0.034198883920907974
      <br/>
      2000 0.005526650696992874
      <br/>
      2000 0.009151813574135303
      <br/>
      3000 0.0021409429609775543
      <br/>
      3000 0.0015856553800404072
      <br/>
      4000 0.0006656644982285798
      <br/>
      4000 0.0005017295479774475
      <br/>
      5000 0.00018937562708742917
      <br/>
      5000 0.00020660058362409472
      <br/>
      [‘milk’, ‘hot-pot’, ‘coffee’, ‘sing’, ‘sleep’, ‘home’, ‘forever’]
     </p>
    </blockquote>
    <h6>
     <a id="_357">
     </a>
     神经网络的计算过程
    </h6>
    <p>
     BP神经网络：误差反向传播算法公式推导
     <br/>
     开端： BP算法提出
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/15dcb48d4b2e47408c4a28e8af629f4f.png"/>
    </p>
    <ol>
     <li>
      <p>
       BP神经网络参数符号及激活函数说明
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7f98c6ea29d54a509df443ff1457b7b0.png"/>
      </p>
     </li>
     <li>
      <p>
       网络输出误差（损失函数）定义
      </p>
     </li>
    </ol>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ab1a21ed994a4c0c9bbab27325f84545.png"/>
    </p>
    <ol start="3">
     <li>
      <p>
       隐藏层与输出层间的权重更新公式推导
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/be88cc604c8145d4afe3e77a623b423f.png"/>
      </p>
     </li>
     <li>
      <p>
       输入层与隐藏层间的权重更新公式推导
      </p>
     </li>
    </ol>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4e2cfd49ae9e437b898adef7e4fca4d8.png"/>
    </p>
    <h6>
     <a id="RNNRecurrent_Neural_Network_386">
     </a>
     循环神经网络（RNN，Recurrent Neural Network）
    </h6>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4c3f0f089b6d4610ab483e174f925775.png"/>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/54b81edb1c3747f294cc8903e56de02e.png"/>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/0dbd72f84e7341f4b10dea5bd122ef5c.png"/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a70d02887e164459aa9257c6aca7979d.png"/>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9304ab6c3a504ed08be2d61402b4dd80.png"/>
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/0afbaffaea7044e38448c602bda01cf9.png"/>
    </p>
    <p>
     <a href="https://zhuanlan.zhihu.com/p/689910526" rel="nofollow">
      RNN存在梯度消失和梯度爆炸的问题
     </a>
    </p>
    <h6>
     <a id="Word2Vec_399">
     </a>
     简化模型：Word2Vec
    </h6>
    <p>
     参考：
     <a href="https://blog.csdn.net/julac/article/details/127767053">
      一篇文章入门Word2Vec
     </a>
     <br/>
     <a href="https://www.cnblogs.com/stephen-goodboy/p/12574738.html" rel="nofollow">
      词向量模型word2vector详解
     </a>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c4d756efb63f467581190e1612945ef7.png"/>
    </p>
    <p>
     ➢ 基本功能
     <br/>
     ⁃ 给定文本数据，对于每个单词学习一个低维表示
     <br/>
     ➢ 基于分布式语义的思想进行设计
     <br/>
     ⁃ 词义=背景单词的语义
     <br/>
     ➢ 不考虑窗口内单词的顺序
     <br/>
     ⁃ 应用了简单的average pooling的策略
     <br/>
     ➢ 充分考虑实践和效果
     <br/>
     ⁃ 有很多的优化trick，速度快、效果稳定
    </p>
    <h6>
     <a id="__Word2VecELMo_414">
     </a>
     词向量 - 从Word2Vec到ELMo
    </h6>
    <p>
     参考：
     <a href="https://blog.csdn.net/Jackie_vip/article/details/141600366">
      【大模型系列篇】词向量 - 从Word2Vec到ELMo
     </a>
    </p>
    <blockquote>
     <p>
      词向量（又叫词嵌入）已经成为NLP领域各种任务的必备一步，而且随着BERT、GPT等预训练模型的发展，词向量演变为知识表示方法，但其本质思想不变。 生成词向量的方法有很多种，本文重点回顾Word2Vec到ELMo。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c78b6c2b3a7c40f28fa8d7784fc80bd6.png"/>
     </p>
    </blockquote>
    <p>
     one-hot 编码
    </p>
    <blockquote>
     <p>
      one-hot 编码，首先构造一个容量为 N 的词汇表，每个单词可以用一个 N 维的词向量表示，词向量中只有单词在词汇表的索引处取值为 1，其余为 0。
      <br/>
      one-hot 编码主要的缺点是：当词汇表的容量变大时，词向量的特征空间会变得很大；另外 one-hot 编码不能区分单词之间的相似度，同时具有强稀疏性。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/b8c70ea88b624734b56c02172cfb9c3d.png"/>
     </p>
    </blockquote>
    <p>
     共现矩阵（Co-Occurrence Matrix）
    </p>
    <blockquote>
     <p>
      通过统计一个事先指定大小的窗口内的单词共同出现的次数，此时将词划分为两种，中心词和其他词。假设现在语料库中只有三个句子 “I have a cat”、“cat eat fish”、“I have a apple”，则可以构造出单词间的共现矩阵 A。例如 “I” 和 “have” 在两个句子中共同出现过，因此在 A中的权重为 2；而 “I” 和 “cat“ 只在一个句子中共现， A 中权重为 1 。
      <br/>
      矩阵 A 的每一行就代表了一个单词的词向量，与 one-hot 编码类似，使用共现矩阵的词向量的维度也非常大。也可以使用 SVD (奇异值分解) 对 A进行分解，从而得到更低维度的词向量，但是 SVD 算法的时间复杂度较高，对 n×n 的矩阵进行 SVD 分解的复杂度为
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         O 
         
        
          ( 
         
         
         
           n 
          
         
           3 
          
         
        
          ) 
         
        
       
         O(n^3)
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 1.0641em; vertical-align: -0.25em;">
          </span>
          <span class="mord mathnormal" style="margin-right: 0.0278em;">
           O
          </span>
          <span class="mopen">
           (
          </span>
          <span class="mord">
           <span class="mord mathnormal">
            n
           </span>
           <span class="msupsub">
            <span class="vlist-t">
             <span class="vlist-r">
              <span class="vlist" style="height: 0.8141em;">
               <span class="" style="top: -3.063em; margin-right: 0.05em;">
                <span class="pstrut" style="height: 2.7em;">
                </span>
                <span class="sizing reset-size6 size3 mtight">
                 <span class="mord mtight">
                  3
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
          <span class="mclose">
           )
          </span>
         </span>
        </span>
       </span>
      </span>
      。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/db318d39bcd04143a7ea6dce12af8ba4.png"/>
     </p>
    </blockquote>
    <p>
     Word2Vec
    </p>
    <blockquote>
     <p>
      《Efficient Estimation of Word Representation in Vector Space》- Word2Vec(2013)
      <br/>
      Word2Vec词向量模型，可以用数值向量表示单词，且在向量空间中可以很好地衡量两个单词的相似性，它的核心思想是通过词的上下文得到词的向量化表示。
      <br/>
      Word2Vec是轻量级的神经网络，其模型仅仅包括输入层、隐藏层和输出层，模型框架根据输入输出的不同，主要包括CBOW和Skip-gram模型。
      <br/>
      两种模型的区别在于 CBOW 使用上下文词预测中心词，而 Skip-Gram 使用中心词预测其上下文单词。CBOW适合于数据集较小的情况，而Skip-Gram在大型语料中表现更好。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/1f82c8655eae480c8e9389d97d4f5b8e.png"/>
      <br/>
      Simple CBOW Model
      <br/>
      为了更好的了解模型深处的原理，我们先从Simple CBOW model（仅输入一个词，输出一个词）框架说起。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d463068c413746f3a26dfa68ca56add8.png"/>
      <br/>
      input layer输入的X是单词的one-hot representation（考虑一个词表V，里面的每一个词
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         w 
          
         
           i 
          
         
        
       
         w_i
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.5806em; vertical-align: -0.15em;">
          </span>
          <span class="mord">
           <span class="mord mathnormal" style="margin-right: 0.0269em;">
            w
           </span>
           <span class="msupsub">
            <span class="vlist-t vlist-t2">
             <span class="vlist-r">
              <span class="vlist" style="height: 0.3117em;">
               <span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;">
                <span class="pstrut" style="height: 2.7em;">
                </span>
                <span class="sizing reset-size6 size3 mtight">
                 <span class="mord mathnormal mtight">
                  i
                 </span>
                </span>
               </span>
              </span>
              <span class="vlist-s">
               ​
              </span>
             </span>
             <span class="vlist-r">
              <span class="vlist" style="height: 0.15em;">
               <span class="">
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
      都有一个编号
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         i 
         
        
          ϵ 
         
        
          { 
         
        
          1 
         
        
          , 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          , 
         
        
          ∣ 
         
        
          V 
         
        
          ∣ 
         
        
          } 
         
        
       
         i\epsilon \{1,...,|V|\}
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 1em; vertical-align: -0.25em;">
          </span>
          <span class="mord mathnormal">
           i
          </span>
          <span class="mord mathnormal">
           ϵ
          </span>
          <span class="mopen">
           {
           <!-- -->
          </span>
          <span class="mord">
           1
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord">
           ...
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord">
           ∣
          </span>
          <span class="mord mathnormal" style="margin-right: 0.2222em;">
           V
          </span>
          <span class="mord">
           ∣
          </span>
          <span class="mclose">
           }
          </span>
         </span>
        </span>
       </span>
      </span>
      ，那么词
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         w 
          
         
           i 
          
         
        
       
         w_i
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.5806em; vertical-align: -0.15em;">
          </span>
          <span class="mord">
           <span class="mord mathnormal" style="margin-right: 0.0269em;">
            w
           </span>
           <span class="msupsub">
            <span class="vlist-t vlist-t2">
             <span class="vlist-r">
              <span class="vlist" style="height: 0.3117em;">
               <span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;">
                <span class="pstrut" style="height: 2.7em;">
                </span>
                <span class="sizing reset-size6 size3 mtight">
                 <span class="mord mathnormal mtight">
                  i
                 </span>
                </span>
               </span>
              </span>
              <span class="vlist-s">
               ​
              </span>
             </span>
             <span class="vlist-r">
              <span class="vlist" style="height: 0.15em;">
               <span class="">
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
      的one-hot表示就是一个维度为
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         ∣ 
         
        
          V 
         
        
          ∣ 
         
        
       
         |V|
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 1em; vertical-align: -0.25em;">
          </span>
          <span class="mord">
           ∣
          </span>
          <span class="mord mathnormal" style="margin-right: 0.2222em;">
           V
          </span>
          <span class="mord">
           ∣
          </span>
         </span>
        </span>
       </span>
      </span>
      的向量，其中第i个元素值非零，其余元素全为0，例如：
      <span class="katex--inline">
       <span class="katex">
        <span class="katex-mathml">
         w 
          
         
           2 
          
         
        
          = 
         
        
          [ 
         
        
          0 
         
        
          , 
         
        
          1 
         
        
          , 
         
        
          0 
         
        
          , 
         
        
          . 
         
        
          . 
         
        
          . 
         
        
          , 
         
        
          0 
         
         
         
           ] 
          
         
           T 
          
         
        
       
         w_2=[0,1,0,...,0]^T
        </span>
        <span class="katex-html">
         <span class="base">
          <span class="strut" style="height: 0.5806em; vertical-align: -0.15em;">
          </span>
          <span class="mord">
           <span class="mord mathnormal" style="margin-right: 0.0269em;">
            w
           </span>
           <span class="msupsub">
            <span class="vlist-t vlist-t2">
             <span class="vlist-r">
              <span class="vlist" style="height: 0.3011em;">
               <span class="" style="top: -2.55em; margin-left: -0.0269em; margin-right: 0.05em;">
                <span class="pstrut" style="height: 2.7em;">
                </span>
                <span class="sizing reset-size6 size3 mtight">
                 <span class="mord mtight">
                  2
                 </span>
                </span>
               </span>
              </span>
              <span class="vlist-s">
               ​
              </span>
             </span>
             <span class="vlist-r">
              <span class="vlist" style="height: 0.15em;">
               <span class="">
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
          <span class="mrel">
           =
          </span>
          <span class="mspace" style="margin-right: 0.2778em;">
          </span>
         </span>
         <span class="base">
          <span class="strut" style="height: 1.0913em; vertical-align: -0.25em;">
          </span>
          <span class="mopen">
           [
          </span>
          <span class="mord">
           0
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord">
           1
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord">
           0
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord">
           ...
          </span>
          <span class="mpunct">
           ,
          </span>
          <span class="mspace" style="margin-right: 0.1667em;">
          </span>
          <span class="mord">
           0
          </span>
          <span class="mclose">
           <span class="mclose">
            ]
           </span>
           <span class="msupsub">
            <span class="vlist-t">
             <span class="vlist-r">
              <span class="vlist" style="height: 0.8413em;">
               <span class="" style="top: -3.063em; margin-right: 0.05em;">
                <span class="pstrut" style="height: 2.7em;">
                </span>
                <span class="sizing reset-size6 size3 mtight">
                 <span class="mord mathnormal mtight" style="margin-right: 0.1389em;">
                  T
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
      ；
      <br/>
      输入层到隐藏层之间有一个权重矩阵W，隐藏层得到的值是由输入X乘上权重矩阵得到的（细心的人会发现，0-1向量乘上一个矩阵，就相当于选择了权重矩阵的某一行，如图：输入的向量X是[0，0，1，0，0，0]，W的转置乘上X就相当于从矩阵中选择第3行[2,1,3]作为隐藏层的值）;
      <br/>
      隐藏层到输出层也有一个权重矩阵W’，因此，输出层向量y的每一个值，其实就是隐藏层的向量点乘权重向量W’的每一列，比如输出层的第一个数7，就是向量[2,1,3]和列向量[1,2,1]点乘之后的结果；
      <br/>
      最终的输出需要经过softmax函数，将输出向量中的每一个元素归一化到0-1之间的概率，概率最大的，就是预测的词。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/3659c3f4f1304c0cb0450b19a03772c6.png"/>
      <br/>
      输出层通过softmax归一化，u代表的是输出层的原始结果。通过下面公式，我们的目标函数可以转化为现在这个形式
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ab6c7e031a6a4a21843aacd65967b91b.png"/>
      <br/>
      CBOW Multi-Word Context Model
      <br/>
      了解了Simple CBOW model之后，扩展到CBOW就很容易了，只是把单个输入换成多个输入罢了（划红线部分）。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/2efb02b3ef84458888aee1410470b292.png"/>
      <br/>
      对比可以发现，和simple CBOW不同之处在于，输入由1个词变成了C个词，每个输入X_{ik}到达隐藏层都会经过相同的权重矩阵W，隐藏层h的值变成了多个词乘上权重矩阵之后加和求平均值。
      <br/>
      Skip-gram
      <br/>
      有了CBOW的介绍，对于Skip-gram model 的理解应该会更快一些。
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/01d5243d7fa4456689f8901190564e6e.png"/>
      <br/>
      如上图所示，Skip-gram model是通过输入一个词去预测多个词的概率。输入层到隐藏层的原理和simple CBOW一样，不同的是隐藏层到输出层，损失函数变成了C个词损失函数的总和，权重矩阵W’还是共享的。
     </p>
    </blockquote>
    <p>
     GloVe
    </p>
    <blockquote>
     <p>
      《GloVe : Global Vectors forWord Representation》全局向量的词嵌入，通常简称为GloVe，是一种用于将词语映射到连续向量空间的词嵌入方法。它旨在捕捉词语之间的语义关系和语法关系，以便在自然语言处理任务中能够更好地表示词语的语义信息。
      <br/>
      常见的词嵌入算法有基于矩阵分解的方法和基于浅层窗口的方法，Glove 结合了这两类方法的优点生成词向量。基于矩阵分解的方法可以有效地利用全局信息，但是在大数据集上速度慢；而基于浅层窗口的方法对于词汇类比任务效果较好，训练速度快，但是不能有效利用全局信息。
      <br/>
      词嵌入算法 Glove，结合两类词嵌入算法的优点。
      <br/>
      第一类是 Matrix Factorization (矩阵分解) 方法，例如 LSA；
      <br/>
      第二类是 Shallow Window-Based (基于浅层窗口) 方法，也称为基于预测的方法，例如 Word2Vec。
      <br/>
      GloVe模型将 LSA 和 Word2Vec 的优点结合在一起，既利用了语料库的全局统计信息，也利用了局部的上下文特征 (滑动窗口)。Glove 首先需要构造单词共现矩阵，并提出了共现概率矩阵 (Co-occurrence Probabilities Matrix)的概念，共现概率矩阵可以通过单词共现矩阵计算。
     </p>
    </blockquote>
    <p>
     ELMo
    </p>
    <blockquote>
     <p>
      ELMo来自于论文《Deep contextualized word representations》(2018)
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/1a865ea56d264f3b99b422375636cc6f.webp#pic_center"/>
      <br/>
      word2vec和glove这两种算法都是静态词向量算法，它们都存在一个问题，词在不同的语境下其实有不同的含义，而这两个模型词在不同语境下的向量表示是相同的；
      <br/>
      ELMo是一种动态词向量算法，就是针对这一点进行了优化，来自于语言模型的词向量表示，也是利用了深度上下文单词表征，该模型的优势： （1）能够处理单词用法中的复杂特性（比如句法和语义） （2）这些用法在不同的语言上下文中如何变化（比如为词的多义性建模）
      <br/>
      针对点1，作者是通过多层的stack LSTM去学习词的复杂用法，论文中的实验验证了作者的想法，不同层的output可以获得不同层次的词法特征。对于词义消歧有需求的任务，第2层会有较大的权重，对于词性、句法有需求的任务，对第1层会有比较大的权重。
      <br/>
      针对点2，作者通过pre-train+fine tuning的方式实现，先在大语料库上进行pre-train，再在下游任务的语料库上进行fine tuning。
      <br/>
      <strong>
       ELMo模型有三个优点：
      </strong>
      <br/>
      ELMo具有处理一词多义的能力。因为ELMo中每个单词的嵌入并不是固定的，在将这个单词的词嵌入输入双向LSTM之后，它的值会随着上下文内容的不同而改变，从而学到了和上下文相关的词嵌入。
      <br/>
      ELMo具有不同层次的表征能力。我们知道，对于一个多层的网络来说，不同的深度具有不同的表征能力，越接近输入层的网络层学到的特征越接近输入的原始特征，而越接近网络输出层的网络层学到的特征则具有很好的语义表征能力。ELMo使用了对多层LSTM的输出进行自适应加权的结构（attention），使用其可以根据下游任务自适应调整ELMo的输出，让其与下游任务相适应。
      <br/>
      ELMo具有非常强大的灵活性：除了ELMo本身的输入可以根据调整外，ELMo还可以以各种形式和下游任务进行结合。通过ELMo得到的仅是当前时间片的输入的编码结果，因此可以加入到输入层，隐层，输出层中。
      <br/>
      ELMo是最早的一批将深度学习应用到词向量学习的任务中，它的思想对后续的BERT等产生了巨大的影响。
     </p>
    </blockquote>
    <h4>
     <a id="_490">
     </a>
     参考：
    </h4>
    <p>
     <a href="https://blog.csdn.net/weixin_62472350/article/details/143448417">
      NNLM——预测下一个单词
     </a>
     <br/>
     <a href="https://www.bilibili.com/video/BV1AT4y1J7bv/" rel="nofollow">
      Neural Network Language Model PyTorch实现
     </a>
     <br/>
     <a href="https://wmathor.com/index.php/archives/1442/" rel="nofollow">
      NNLM 的 PyTorch 实现
     </a>
    </p>
    <p>
     <a href="https://www.cnblogs.com/tsingke/p/14826896.html" rel="nofollow">
      BP神经网络：误差反向传播算法公式推导图解
     </a>
     <br/>
     <a href="https://zhuanlan.zhihu.com/p/689910526" rel="nofollow">
      RNN存在梯度消失和梯度爆炸的问题
     </a>
    </p>
    <p>
     参考：
     <a href="https://blog.csdn.net/julac/article/details/127767053">
      一篇文章入门Word2Vec
     </a>
     <br/>
     <a href="https://www.cnblogs.com/stephen-goodboy/p/12574738.html" rel="nofollow">
      词向量模型word2vector详解
     </a>
    </p>
    <p>
     参考：
     <a href="https://blog.csdn.net/Jackie_vip/article/details/141600366">
      【大模型系列篇】词向量 - 从Word2Vec到ELMo
     </a>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f33383133393235302f:61727469636c652f64657461696c732f313436313538383530" class_="artid" style="display:none">
 </p>
</div>


