---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f71715f3235333333363831:2f61727469636c652f64657461696c732f3830303838313236"
layout: post
title: "音视频开发第一篇音视频基础概念"
date: 2025-01-18 22:20:08 +08:00
description: "研究音视频的数字化技术之前，音视频技术的本质就是声音和图像信息的"
keywords: "音视频流av帧"
categories: ['音视频']
tags: ['音视频编解码', '音视频基础', 'Pts', 'Ipb', 'Ffmpeg']
artid: "80088126"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=80088126
    alt: "音视频开发第一篇音视频基础概念"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=80088126
featuredImagePreview: https://bing.ee123.net/img/rand?artid=80088126
---

# 音视频开发第一篇——音视频基础概念。

研究音视频的数字化技术之前，必须对声音和图像的的物理性质有基本的了解。音视频技术的本质就是
**声音和图像信息的采集、存储和回放**
。学习音视频的数字化技术，不能上来就去编解码，这样有点本末倒置。

通过阅读本文，你将会回顾中学关于光学和声学部分知识，并了解音视频技术中的一些关键
**概念**
。

### 声音的物理特性

##### 声音的本质是波：

> 物体通过振动，对空气（传播介质）产生挤压，使空气有节奏的振动并产生疏密变化，从而形成疏密相间的纵波。

##### 声音的三要素

1、频率

> 频率决定音调的高低
>   
> 频率与波长成反比，频率越低，波长越长。低频波传递距离较长（可对比，寺庙的钟声和轮船的鸣笛）。

2、 振幅

> 振幅决定音量
>   
> 音量是能量大小的表现，通常用分贝来表示音量大小，超过一定分贝的声音对人体是有害的。

3.、波形

> 波形决定音色
>   
> 不同的声源，在同等频率，同等振幅的情况下，声波的形状是不同的，从而发出的声音也是有差异的。

### 声音的数字化

数字化音频技术，就是声音的采集、存储和回放。

1、采样

> 1. 采样就是在时间轴上数字化声音信号。
> 2. 为了提高采样质量，根据
>    **采样定理**
>    ，需要按比声音最高频率高2倍以上的频率对声音进行采样。人耳能听到的声音频率范围20Hz～20KHz，所有典型的采样率为44.1kHz（表示一秒钟采集44100次数据）。

2、量化

> 量化就是每个具体采样点的声音在振幅轴上的数字化表示。

3、编码

> 通常所说的音频裸数据就是PCM（Pulse Codec Modulation脉冲编码调制）

#### PCM

> PCM数据涉及四个概念：
>   
> 1、
> **sampleFormat（采样格式**
> ）：可理解为一多少字节存储声音，典型的量化格式为16bit。
>   
> 2、
> **sampleRate（采样率）**
> ：这就不用说了吧，典型的采样率为44.1KHz。
>   
> 3、
> **channel（声道数）**
> ：为了造成立体声效果，数字声音分为左、右两个声道。
>   
> 4、
> **比特率**
> ：对于数字音频而言，比特率是个关键概念。定义为：一秒时间内的比特数，用于衡量单位时间音频数据量的大小。

#### 编码的必要性

我们来计算一下一分钟未经编码的数据占有多少字节：
  
以CD音质为例：

> CD 音质参数：
>   
> **采样格式（sampleFormat）**
> 为16byte（2字节）
>   
> **采样率（sampleRate）**
> 为44.1KHz
>   
> **声道数（channel）**
> 为2
>   
> 所以，一分钟的数据大小为：

44100 * 16 * 2 * 60 / 8 / 1024 = 10.09 MB

也许按照现在的存储技术，这个大小勉强能够接受，倒是如果要实时传输的话，那就太勉强了。所以必须对数据进行编码。

#### 编码原理及其它

编码，也称为压缩编码。它的实质是通过特殊的算法压缩掉冗余信号。

> 冗余信号： 指不能被人耳感知的信号。

**压缩比**
： 是压缩编码的基本标准之一，小于1。压缩又分为有损压缩，和无损压缩。常用压缩格式中，常用有损压缩。压缩比越小，丢失信息越多，丢失的信息不可恢复。

#### 常见的压缩算法

##### 1、WAV编码

* WAV编码有多种实现方式，其中一种实现是：在PCM数据格式前加上44字节，用于表示PCM的采样率、声道数、数据格式等。也就是，并不会对PCM数据进行压缩（所有实现都不压缩）。
* 特点：音质好
* 场合：用于多媒体开发的中间件、或音效素材。

##### 2、MP3编码

* 同样MP3也有多种编码实现，其中LAME编码中的高码率文件，音效非常接近WAV。
* 特点：码率128Kbit/s以上的音频上压缩比较高，兼容性好。
* 场合：高比特率下，对兼容性有要求的音乐

##### 3、AAC编码

* 有损压缩技术，通过附加编码技术，有三种主要的版本：
    
  1. LC-AAC: 应用于中高码率场景（>= 80Kbit/s)
  2. HE-AAC： 应用于中低码率场景（<= 80Kbit/s)
  3. HE-AAC v2: 应用于低码率场景（<=48Kbit/s)
* 特点：在小于128Kbit/s码率下表现优异，常用于视频中的音频编码。
* 场景：128Kbit/s下的音频编码，用于视频中的音频编码

##### 4、Ogg编码

* 一种非常好的编码，在各种码率下表现都十分优异，特别是低码率下。
* 特点：可以用比MP3更小的码率实现比它更好的音质，中高码率编码表现也毫不逊色。但兼容性不好，不支持流媒体特性
* 场景：语音聊天

### 图像的物理特性

白色光能被分解为多种色光，实验证明，红绿蓝三种色光无法分解，这就是我们说的三原色。

另外，物体之所以能在我们的眼中呈像，是因为物体反射的光，进入了眼睛。但在视频技术中，却稍有不同。

因为手机或者电脑屏幕都是自发光原，并不需要反射光。

一块分辨率为1280 * 720的屏幕，水平方向有1280个像素点，竖直方向有720个像素点。每个像素点由三个子像素组成（有条件可以通过显微镜观察），分别表示RGB。屏幕显示图片时，将每一个像素点的RGB通道分别对应屏幕的子像素点，从而显示出照片。

### 视频的数字化

#### 图像的数字表示方式一： RGB

不管是通过常识，还是通过阅读本文，相信你已经知道任何图像都可以由
**RGB**
组成。
  
每个像素点的子像素有两种表示：

* 浮点表示：取值范围为0.0 ～ 1.0，常见于OpenGL中的子像素表示。
* 整数表示：取之范围为0 ～ 255或者00 ～ FF，8个bit表示一个子像素。常见的格式有
  **RGBA-8888**
  、Android平台上的
  **RGB-565**
  。

对于一般图像，通常使用整数表示。如计算一张分辨率为1280 * 720，格式为
**RGBA-8888**
的图像大小：
  
1280 * 720 * 4 = 3.516MB

> **RGBA-8888**
> 格式：一个字节表示透明度三个字节表示RGB分量。

以上计算出的大小，就是位图（bitmap）在内存中占据的大小。因为数据量较大，不利于网络传输。所以就有了各种压缩格式。

#### 图像的数字表示方式二：YUV

对于视频而言，它的裸数据更多的使用
**YUV**
格式表示。和
**RGB**
比较，最大的优点在于占用较少的频宽（
**RGB**
要求三个独立的视频数据分量同时传输），另外
**YUV**
可以很好的向黑白电视兼容。
  
其中：

* **Y**
  ：（拉丁文
  **Luminance或Luma**
  )表示亮度分量,通常称为亮度分量或者灰度。
* **U和V**
  ： 表示色度（
  **Chrominance或Chroma**
  ），作用是描述色彩和饱和度，用于指定颜色。

**Y**
亮度分量的建立，是通过叠加
**RGB**
输入信号的特定部分完成。
  
**U和V**
色度分量，定义了色调和饱和度两方面，分别用
**Cr**
和
**Cb**
表示。
**Cr**
反映
**RGB**
输入信号
**红色部分**
和亮度值之间的差异。
**Cb**
则反映
**RGB**
输入信号
**蓝色**
部分和亮度值之间的差异。

**YUV**
格式表示的数据，
**Y**
分量和
**U**
、
**V**
是分离的。只有
**Y**
分量的数据，表现出来就是黑白视频，这正是
**YUV**
格式能兼容黑白电视的原因。

**YUV**
常用的格式是
**4 ：2 ：0**
（关于
**YUV**
格式的种类和计算方式，以后单独开篇讲解）。
  
**Y、U、V**
都是使用8个bit表示。

#### 视频编码方式

和音频数据相似，视频的编码也是通过去除冗余数据实现。不同数据在于，视频数据在时间和空间上有较强的相关性。所以这些冗余信息包括
**时间冗余**
和
**空间冗余**
。

##### 帧间编码

> **帧内编码**
> 用于去除时间冗余。关于
> **帧间**
> 编码技术实现细节，可以先熟悉一下概念，暂时不用了解细节，这将在以后介绍。

**帧间编码**
技术，是去除
**时间冗余**
的方式，包括以下方面：
  
*
**运动补偿**
： 通过之前的图像来预测、补偿当前图像，是减少帧序列冗余信息的有效方法。
  
*
**运动表示**
： 不同区域的图像需要使用不同的运动适量来描述运动信息。
  
*
**运动估计**
： 是一中从视频序列中抽取运动信息的一整套技术。

##### 帧内编码

> **帧内编码**
> 用于去除空间冗余。关于
> **帧内**
> 编码技术实现细节，可以先熟悉一下概念，暂时不用了解细节，这将在以后介绍。

**帧内编码**
编码标准有很多，且都需要大量篇幅介绍，这里只作大致介绍。一类是
**MPEG**
,主要包括四个版本：1、
**Mpeg1(用于VCD）**
。2、
**Mpeg2(用于DVD）**
。3、
**Mpeg4(现在流行的流媒体）**
。第二类是
**H.26***
系列，包括
**H264**
。

##### 编码中的重要概念

1. **IPB帧**
     
   1. **I帧**
      ：
      **帧内编码帧（intra picture)**
      ,通常是每个
      **GOP**
      （MPEG使用的一种视频压缩技术）的第一帧，经过适当的压缩，作为随机访问的参考点，可以当作静态图像。
      **I帧**
      可以得到
      **6：1**
      的压缩比，而不造成图像模糊，可以去除空间冗余。
      **I帧**
      可理解为一张独立完整的视频画面，只是进行了空间冗余的压缩而已。
   2. **P帧**
      ：
      **前向预测帧（predictive-frame）**
      ，通过图像序列中，前面已编码帧的时间冗余信息的去除来压缩数据量的编码图像，也称为
      **预测帧**
      。
      **P帧**
      可理解为需要前一个
      **I帧**
      或
      **P帧**
      来解码才能得到一张完成视频画面。
   3. **B帧**
      ：
      **双向预测内插编码帧（bi-directional interpolated prediction frame)**
      , 即考虑图像序列前已编码帧，也参照图像序列后已编码帧的时间冗余信息，来压缩数据量，也称为
      **双向预测帧**
      。
      **B帧**
      可理解为需要曹考前一个
      **I帧**
      或
      **P帧**
      ，以及后一个
      **P帧**
      生成一张完整的视频画面。
   4. **IDR帧**
      ：（
      **instantaneous decoding refresh picture**
      ),在
      **H264**
      编码中出现的概念，类似
      **I帧**
      ，区别在于：
      **H264**
      采用多帧预测，
      **I帧**
      之后的
      **P帧**
      可能参考
      **I帧**
      之前的帧才能解析完整图像，所以在随机访问中，就不能以
      **I帧**
      作为参考条件。而
      **IDR帧**
      就是一种特殊的
      **I帧**
      ，这一帧后的所有帧只会参考它，而不会参考前面的帧。在编码器中，一旦接收到一个
      **IDR帧**
      ，就会立即清理参考帧缓冲区，并将这个
      **IDR帧**
      作为参考帧使用。
2. **PTS和DTS**
     
   **PTS**
   英文全称为
   **Presentation Time Stamp**
   ,
   **DTS**
   英文全称为
   **Decoding Time Stamp**
   ，都是时间戳的概念。
     
   简单起见，先来介绍一下
   **FFmpeg**
   中关于这两者的概念。
   **FFmpeg**
   使用
   **AVPacket**
   表示解码前或编码后的压缩数据，用
   **AVFrame**
   表示解码后或编码前的原始数据。对于视频而言，
   **AVFrame**
   就是以帧图像，至于什么时候显示给用户，取决于它的
   **PTS**
   。
   **DTS**
   是
   **AVPacket**
   的一个成员，表示该压缩包需要什么时候被解码，如果视频中的各帧是按顺序输入，那么解码时间和显示时间一致，但实际上大多数解码标准，编码顺序和输入顺序并不一致，于是就出现了两种不同的时间戳。
3. **GOP**
     
   英文全称为
   **Group Of Picture**
   ,意思是，两个
   **I帧**
   之间形成的一组图片。通常在为解码器设置参数时，需要指定
   **gop\_size**
   的值，因为
   **I帧**
   的压缩率是最低的，对一个视频源而言，
   **gop\_size**
   越大，相对来说
   **I帧**
   就越少，节约出来的空间就可以保存更多的
   **I帧**
   ，所以画质就会越好。所以，应该根据业务场景，选择适当的
   **gop\_size**
   值，从而提高视频质量。

   > 常见的压缩率：
   >   
   > **I帧**
   > ： 7
   >   
   > **P帧**
   > ： 20
   >   
   > **B帧**
   > ： 50

##### MUX和DEMUX

**mux**
的全称是
**multiplex**
，译为
**多路传输**
。在音视频中，其实就是
**混流**
。意思是将多路流包括音频、视频流混合/封装到一个流文件中。

**demux**
自然就是相反的意思，表示
**分流**
，意思是将经过混流后的流文件拆分开来，方便后续处理。

**muxer**
和
**demuxer**
，则可以分别表示为
**混流器**
、
**分流器**
。

**muxing**
与
**demuxing**
加上ing后缀，表示动作，可以分别理解为
**混流操作**
和
**分流操作**
。

> 编解码后的音频、视频、字幕等流文件通过混流器封装（打包）到一个文件中，同时作为数据传输，传输完毕后，又通过分流器对数据进行解封（拆包），将音频、视频、字幕等数据分流出来，进一步处理。

### 结语

如有任何有描述不清，或者有误之处，欢迎流言交流。更音视频相关内容将持续更新，敬请期待。