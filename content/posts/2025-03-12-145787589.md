---
layout: post
title: "论文笔记FLAREfeed-forwardposegeometry-estimateGS"
date: 2025-03-12 22:17:05 +0800
description: "我们提出了FLARE，一种前馈模型，旨在从未校准的稀疏视角图像（即少至2-8张输入）中推断出高质量的相机姿态和3D几何结构。这是一个在现实应用中具有挑战性但又非常实用的场景。我们的解决方案采用了一种级联学习范式，其中相机姿态作为关键桥梁，认识到其在将3D结构映射到2D图像平面中的重要作用。具体来说，FLARE从相机姿态估计开始，其结果作为后续几何结构和外观学习的条件，并通过几何重建和新视角合成的目标进行优化。"
keywords: "【论文笔记】FLARE：feed-forward+pose&geometry estimate+GS"
categories: ['未分类']
tags: ['论文阅读', '计算机视觉', '深度学习', '几何学', '3D']
artid: "145787589"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145787589
    alt: "论文笔记FLAREfeed-forwardposegeometry-estimateGS"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145787589
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145787589
cover: https://bing.ee123.net/img/rand?artid=145787589
image: https://bing.ee123.net/img/rand?artid=145787589
img: https://bing.ee123.net/img/rand?artid=145787589
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【论文笔记】FLARE：feed-forward+pose&amp;geometry estimate+GS
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     <strong>
      浙大周晓巍大神组的又一篇新作，CVPR2025，拖延两周了才看完，中间有其他事没顾上，本文使用了一个经验性的技巧方法，局部与全局学习结合，pose学习与几何学习结合，相辅相成，在速度与质量上做到了兼顾与高效，官网有在线demo，效果不错！！！并且虽然题目是稀疏视角，但是在显存足够的前提下，可以增加图片数量！
     </strong>
     <br/>
     文章地址：https://arxiv.org/pdf/2502.12138
     <br/>
     开源代码：https://github.com/ant-research/FLARE?tab=readme-ov-file
    </p>
    <p>
     <img alt="" height="1067" src="https://i-blog.csdnimg.cn/direct/b8f4115f970942d79a0aea598e5365e0.png" width="1252"/>
    </p>
    <h2 style="background-color:transparent">
     abstract
    </h2>
    <p>
     我们提出了FLARE，一种前馈模型，旨在从未校准的稀疏视角图像（即少至2-8张输入）中推断出高质量的相机姿态和3D几何结构。这是一个在现实应用中具有挑战性但又非常实用的场景。
     <br/>
     我们的解决方案采用了一种级联学习范式，其中相机姿态作为关键桥梁，认识到其在将3D结构映射到2D图像平面中的重要作用。具体来说，FLARE从相机姿态估计开始，其结果作为后续几何结构和外观学习的条件，并通过几何重建和新视角合成的目标进行优化。利用大规模公共数据集进行训练，我们的方法在姿态估计、几何重建和新视角合成任务中实现了最先进的性能，同时保持了推理效率（即少于0.5秒）。
    </p>
    <h2>
     1. Introduction
    </h2>
    <p>
     传统三维重建的pipeline是SfM-MVS，但是对于稀疏视角重建问题来说，这个路线存在局限性并且无法奏效，
     <strong>
      目前对于稀疏重建的解决方法主要依赖数据驱动，利用图像特征来完成重建任务，分为下面几类
     </strong>
     ：
     <strong>
      基于优化的方法
     </strong>
     （如BARF [36]和NeRF-- [81]）联合优化相机姿态和几何结构，但它们需要良好的初始化，并且在新场景中的泛化能力较差。
     <strong>
      深度相机估计方法
     </strong>
     [35, 50, 51, 60, 76, 99] 将稀疏视角SfM视为相机参数回归问题，但在准确性和泛化性方面表现不佳。VGGSfM [77] 通过引入多视角跟踪和可微分束调整改进了这一问题，但在提供密集几何结构方面仍有不足。
     <strong>
      基于预训练模型的方法
     </strong>
     ，DUSt3R [80] 和 MASt3R [32] 提出了生成具有像素级几何信息的双视角点图的方法，但它们依赖于后优化的全局配准，耗时且通常效果欠佳。PF-LRM [78] 提供了从四张图像进行前馈重建的能力，但其三平面表示 [5] 限制了其在大规模场景中的性能。因此
     <strong>
      稀疏重建领域仍然缺乏一种满足可扩展性、准确性和效率，同时优化相机、几何和外观估计的全面解决方案
     </strong>
     。
    </p>
    <p>
     <strong>
      我们提出了FLARE，一种新颖的前馈且可微分系统，能够从未校准的稀疏视角图像中推断出高质量的几何结构、外观和相机参数
     </strong>
     。我们在多个大型公共数据集上训练了我们的模型 [1, 34, 39, 49, 68, 83, 93, 95]。在
     <strong>
      相机姿态估计、点云估计和新视角合成
     </strong>
     任务中取得了最先进的成果。
     <strong>
      以未校准的图像作为输入，FLARE 能够在仅 0.5秒 内使用3DGS生成逼真的新视角合成效果
     </strong>
     ，这相比之前的基于优化的方法有了显著提升。如图1所示，我们的系统能够从少至2-8张输入图像中重建3D场景并估计相机姿态。本文的主要贡献如下：
    </p>
    <p>
     1)我们提出了一种高效、前馈且可微分的系统，用于
     <strong>
      从未校准的稀疏视角图像中实现高质量的3DGS场景重建，推理时间少于0.5秒
     </strong>
     。
     <br/>
     2)
     <strong>
      我们的方法证明利用相机姿态作为代理，能够有效简化复杂的3D学习任务
     </strong>
     。因此，我们引入了一种新颖的级联学习范式，该范式从相机姿态估计开始，其结果作为后续几何结构和外观学习的条件。
    </p>
    <p>
     3)我们提出了
     <strong>
      一种两阶段几何学习方法，首先学习以相机为中心的点图，然后构建一个全局几何投影器，将这些点图统一到全局坐标系中
     </strong>
     。
    </p>
    <h2 style="background-color:transparent">
     <br/>
     2. Method
    </h2>
    <p>
     第2.1节中介绍了用于稀疏视角姿态估计的神经姿态预测器。基于姿态估计结果，第2.2节是两阶段学习范式，用于几何估计。第2.3节介绍3D高斯重建头，用于实现高质量的外观建模与新视角合成。最后2.4节是框架的训练loss。
    </p>
    <p>
     <img alt="" height="495" src="https://i-blog.csdnimg.cn/direct/12af6b5201b94b01b673d6a960ec072f.png" width="1501"/>
    </p>
    <h3>
     2.1Neural Pose Predictor
    </h3>
    <p>
     考虑直接估计一个coarse camera pose，
     <strong>
      我们观察到，估计的姿态不需要非常精确——只需近似于真实分布即可
     </strong>
     。并且我们的核心观点是：
     <span style="color:#511b78">
      <strong>
       即使相机的pose不够精确，它也能提供关键的几何先验和空间初始化，从而显著降低几何和外观重建的优化复杂度
      </strong>
     </span>
     。
    </p>
    <p>
     <strong>
      给定一组图像
      <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/cd3dd62428fe42518af5caace207f084.png" width="106">
       ，首先切成
      </img>
     </strong>
     <span style="color:#000000">
      non-overlapping
     </span>
     <strong>
      patch输入神经网络得到tokens，然后初始化可学习相机隐变量
      <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/f840e86ade974fdaae96c10e40155bec.png" width="172">
       ，再把图像tokens与相机隐变量连接到一起送入Transformer decoder（称为“神经姿态预测器”
       <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/fa61c4d4202b40fdbb3f2e705ac790d0.png" width="55">
        ），输出coarse camera pose
       </img>
      </img>
     </strong>
    </p>
    <p>
     <img alt="" height="27" src="https://i-blog.csdnimg.cn/direct/b31bc67b561f4170bf22bea625a3739a.png" width="59">
      <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/a146157c50c141a6b162d923cd868b74.png" width="105">
       ：
      </img>
     </img>
    </p>
    <p class="img-center">
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/3a747c610e8142fda259987b8a9d2b72.png" width="569"/>
    </p>
    <p>
     其中，
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/7bb957c4a97d48a4bcf1274218bedbca.png" width="33"/>
     是一个7维向量，即表示旋转的四元数与平移，注意，这里是绝对旋转与绝对平移。
    </p>
    <p class="img-center">
     <img alt="" height="252" src="https://i-blog.csdnimg.cn/direct/395f641976284b779adb039f8160d78d.png" width="347"/>
    </p>
    <h3>
     2.2
     <span style="color:#000000">
      <strong>
       Multi-view Geometry Estimation
      </strong>
     </span>
    </h3>
    <p>
     <span style="color:#511b78">
      我们的核心思想是首先在局部坐标系（相机坐标系）中学习以相机为中心的几何，然后构建一个神经场景投影器，在估计姿态的指导下将其转换到全局世界坐标系中
     </span>
     。
    </p>
    <h4>
     1.局部几何与pose估计
    </h4>
    <p>
     <span style="color:#511b78">
      在相机空间中学习几何与图像形成过程一致，因为每个视图直接从其视角观察局部几何。这也简化了几何学习，因为只需关注每个视图中可见的局部结构，而不需要推理复杂的全局空间关系。在每一个局部，
      <strong>
       我们通过两阶段学习（局部几何估计和全局几何转换）提高了重建质量。
      </strong>
     </span>
    </p>
    <p>
     <span style="color:#511b78">
      <strong>
       首先，将图像分块为低分辨率 tokens，以及将前面估计的
       <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/7c525856ab444b9da90a1a2e42b857b2.png" width="30"/>
       送入网络得到coarse pose tokens，与前面的图像低分辨率tokens 拼接起来，再将拼接后的 tokens 输入到一个 Transformer
       <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/25cfd330f51943db80f35d805e5575a5.png" width="53"/>
       中，以估计低分辨率的 3D 点 tokens
      </strong>
     </span>
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/d143e755d90142b1a8fcb648b26fb82e.png" width="164"/>
     ​，Transformer 中的自注意力机制可以在不同视图之间建立关联，并利用相机的几何线索。
    </p>
    <p>
     最后，
     <strong>
      将
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/d143e755d90142b1a8fcb648b26fb82e.png" width="164"/>
      输入到 DPT 的解码器
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/3d28c645513e41b29c3164ea13dbfdd1.png" width="56"/>
      中，进行空间上采样，得到局部相机空间中的pointmap
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/a01f15005fb64eb0902c4afa2b58c012.png" width="148"/>
      和置信度图
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/cc220684c408413ca301d3d63020f14e.png" width="147"/>
      ​，dust3r之前是两个两个得到在第一个图坐标系中的这些信息。
     </strong>
    </p>
    <p>
     另外，
     <strong>
      在网络中引入额外的可学习pose tokens
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/a2adebd6eba745c49d73a42fa07c26b7.png" width="31"/>
      ，输出优化后的姿态估计
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/2499d98fd6a9404a88c18f10a7ad3978.png" width="33"/>
      ，同时进行几何预测
     </strong>
     。
     <span style="color:#511b78">
      主要的思想是：同时进行姿态优化和几何估计，两个任务之间可以相互提供补充的监督信号，从而提升各自的性能
     </span>
     。另外，为了在推理时处理可能不准确的姿态估计，对预测的相机姿态添加高斯噪声，使网络能够适应推理时的噪声姿态，增加了鲁棒性。
    </p>
    <p class="img-center">
     <img alt="" height="60" src="https://i-blog.csdnimg.cn/direct/529ef0eedc2e4669abde17b9923086fd.png" width="523"/>
    </p>
    <p>
     <img alt="" height="297" src="https://i-blog.csdnimg.cn/direct/4d39e4e03d6b4845b37cd7748629aae1.png" width="1098"/>
    </p>
    <p>
     DPT 即
     <strong>
      Dense Prediction Transformer
     </strong>
     ，基于 Transformer 进行密集预测，最初用于计算机视觉任务中的密集预测（如语义分割、深度估计等），能够将低分辨率的特征图上采样到高分辨率，同时保持全局上下文信息。在本文中，DPT 被用作解码器将低分辨率的 3D 点 tokens 上采样为高分辨率的点图置信度图​。
    </p>
    <p>
     <img alt="" height="940" src="https://i-blog.csdnimg.cn/direct/355a56f881304403bb9e4da319e612bc.png" width="1999"/>
    </p>
    <h4 style="background-color:transparent">
     2.全局估计
    </h4>
    <p>
     我们的目标是利用优化后的相机姿态
     <strong>
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/2499d98fd6a9404a88c18f10a7ad3978.png" width="33"/>
     </strong>
     ，将以相机为中心的几何
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/b6c73fb435a840d8826147340bce5398.png" width="32"/>
     预测转换为一致的全局几何。但是如果有不准确的的pose估计那么就会导致全局几何错误，所以使得直接的几何重投影（如三角测量或透视变换）是一个不可靠的方法。
    </p>
    <p>
     所以，
     <span style="color:#511b78">
      我们还是考虑使用前面的3D点tokens来完成，也就是构建一个
      <strong>
       可学习的几何投影器 Fg(⋅)，以
      </strong>
     </span>
     <strong>
      <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/2499d98fd6a9404a88c18f10a7ad3978.png" width="33"/>
      <span style="color:#511b78">
       为条件，以
       <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/d143e755d90142b1a8fcb648b26fb82e.png" width="164"/>
       为输入，得到全局tokens，然后再次使用DPT上采样得到pointmap与confidence map
      </span>
     </strong>
     ：
    </p>
    <p class="img-center">
     <img alt="" height="60" src="https://i-blog.csdnimg.cn/direct/54241ad182ce45b2b76f6187d6b89a1e.png" width="520"/>
    </p>
    <h3 style="background-color:transparent">
     <span style="color:#000000">
      <strong>
       2.3. 3D Gaussians for Appearance Modeling
      </strong>
     </span>
    </h3>
    <p>
     我们将上面得到的点云作为输入，进行3DGS的建模，高斯属性包括如下：
    </p>
    <p class="img-center">
     <img alt="" height="150" src="https://i-blog.csdnimg.cn/direct/8978b5efdf694ce39040b146c8c9d28c.png" width="618"/>
    </p>
    <p>
     为了高效地建模外观，
     <strong>
      <span style="color:#511b78">
       我们使用预训练的 VGG 网络从输入图像v中提取特征。并且在全局几何投影器 Fg上构建另一个 DPT 头，以生成外观特征
       <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/2055fc02573641d0ba2286edabe6c2c7.png" width="26"/>
       。最后，将
       <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/2055fc02573641d0ba2286edabe6c2c7.png" width="26"/>
       与 VGG 特征融合，输入到一个浅层 CNN 解码器 Fa(⋅)中，回归高斯参数
      </span>
     </strong>
     。
    </p>
    <p>
     为了解决估计几何与真实几何之间的尺度不一致问题，我们将两者归一化到一个统一的坐标空间中。我们从预测的点图
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/dce40c63bce645428576e50729c69aba.png" width="128"/>
     和真实点图
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/81f9716af7e14b0c96d61b26921c4d97.png" width="159"/>
     中计算平均尺度因子，在渲染时将场景归一化到单位空间。使用可微分的高斯光栅化器 R(⋅)来渲染归一化后的 3D 高斯分布：
    </p>
    <p>
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/e33283ca73b3411ab83b81ef5c3e2ef9.png" width="587"/>
    </p>
    <p>
    </p>
    <p>
     Ip′​ 是渲染的图像。
    </p>
    <h3>
     2.4 loss
    </h3>
    <p>
     loss分为三部分，分别是相机pose损失、几何损失和gs损失。
    </p>
    <p class="img-center">
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/3b889ce105024eee82e9a7b1a14f8573.png" width="565"/>
    </p>
    <p>
     <strong>
      相机姿态
     </strong>
     损失定义为旋转和平移损失的组合，遵循 VGGSFM 中的loss：
    </p>
    <p class="img-center">
     <img alt="" height="70" src="https://i-blog.csdnimg.cn/direct/8e506949e8a04418a6c6b3bad7a7613d.png" width="543"/>
    </p>
    <p>
     其中
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/8b42d1f995e34a7d8cc784eb971ae36f.png" width="36"/>
     ​ 是第 i 张图像的真实pose，
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/e1b0637b79d947699298fd236228a7e0.png" width="65"/>
     ​ 是姿态参数化之间的 Huber 损失。
    </p>
    <p>
     <strong>
      几何损失
     </strong>
     使用 DUSt3R 中的一个loss：
    </p>
    <p class="img-center">
     <img alt="" height="160" src="https://i-blog.csdnimg.cn/direct/264b814ab55d4974ad5461cbb1bf6cbd.png" width="543"/>
    </p>
    <p>
     gs损失为渲染图像
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/9b8a5ceaa83540098a5be56b4d4e9f8e.png" width="29"/>
     和真实图像
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/eb1cdde00fc9408eb7d9e7679cc5e6b0.png" width="26"/>
     ​ 之间的 L2 损失和 VGG 感知损失
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/a0e7437e9e9c4ca090842d345d27b5c0.png" width="48"/>
     ​ 的总和。此外，我们还加入了一个深度损失，用于监督渲染的深度图
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/5507e75fa8814528b5cdacf58939500f.png" width="40"/>
     ​ 与单目深度估计器
     <span style="color:#000000">
      Depth anything
     </span>
     的预测
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/5695061cea024b829f71f019cee07506.png" width="35"/>
     ​ 之间的loss：
    </p>
    <p class="img-center">
     <img alt="" height="90" src="https://i-blog.csdnimg.cn/direct/1bf8d56048d14a8888affadd27862fd6.png" width="514"/>
    </p>
    <h2>
     <span style="color:#000000">
      <strong>
       3. Experiment
      </strong>
     </span>
    </h2>
    <h3 style="background-color:transparent">
     3.1配置
    </h3>
    <p>
     <strong>
      数据与训练
     </strong>
     ：我们follow了
     <span style="color:#000000">
      DUSt3R 与 MASt3R的数据集，包括多种类型的场景MegaDepth [
     </span>
     <span style="color:#367dbd">
      34
     </span>
     <span style="color:#000000">
      ], ARKitScenes [
     </span>
     <span style="color:#367dbd">
      1
     </span>
     <span style="color:#000000">
      ], BlendedMVS [
     </span>
     <span style="color:#367dbd">
      93
     </span>
     <span style="color:#000000">
      ], ScanNet++ [
     </span>
     <span style="color:#367dbd">
      95
     </span>
     <span style="color:#000000">
      ], CO3D-v2 [
     </span>
     <span style="color:#367dbd">
      49
     </span>
     <span style="color:#000000">
      ], Waymo [
     </span>
     <span style="color:#367dbd">
      68
     </span>
     <span style="color:#000000">
      ], WildRGBD [
     </span>
     <span style="color:#367dbd">
      83
     </span>
     <span style="color:#000000">
      ], and DL3DV [
     </span>
     <span style="color:#367dbd">
      39
     </span>
     <span style="color:#000000">
      ].
     </span>
     使用 8 个视图作为输入进行训练。在
     <strong>
      64 个 NVIDIA A800 GPU 上训练 200 个 epoch，输入分辨率为 512×384，总训练时间约为 14 天
     </strong>
     。
    </p>
    <p>
     <strong>
      第一阶段
     </strong>
     ：先训练几何估计网络和相机姿态估计器（不包含高斯回归头），训练约 7 天，每个 GPU 的批量大小为 8。
    </p>
    <p>
     <strong>
      第二阶段
     </strong>
     ：引入高斯回归头，同时冻结粗略相机姿态估计器，训练约 7 天，每个 GPU 的批量大小减少为 4。
    </p>
    <h3 style="background-color:transparent">
     3.2 pose估计
    </h3>
    <p style="background-color:transparent">
     <strong>
      评估指标
     </strong>
     ：
    </p>
    <p>
     使用三个指标评估相机姿态估计的准确性：
    </p>
    <p>
     <strong>
      AUC
     </strong>
     ：计算不同角度阈值下的准确率曲线下面积，通过比较预测姿态与真实姿态的角度差异来确定准确率。
    </p>
    <p>
     <strong>
      RRA（相对旋转准确率）
     </strong>
     ：测量旋转角度差异。
    </p>
    <p>
     <strong>
      RTA（相对平移准确率）
     </strong>
     ：测量平移角度差异。
    </p>
    <p class="img-center">
     <img alt="" height="420" src="https://i-blog.csdnimg.cn/direct/7bde981443914357852e6366be158d37.png" width="619"/>
    </p>
    <h3>
     3.3 稀疏视角重建
    </h3>
    <p>
     通过两阶段几何学习范式，利用相机姿态作为几何先验，减少了噪声并提高了重建精度。
    </p>
    <p>
     <strong>
      评估指标
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       使用
       <strong>
        精度（accuracy）
       </strong>
       和
       <strong>
        完整性（completion）
       </strong>
       指标评估点云质量。
      </p>
     </li>
     <li>
      <p>
       这些指标用于衡量重建点云与真实点云之间的几何一致性和覆盖范围。
      </p>
     </li>
    </ul>
    <p>
     <img alt="" height="372" src="https://i-blog.csdnimg.cn/direct/8c81a14167d2416ba8c79477232d87ee.png" width="1495"/>
    </p>
    <p>
     <img alt="" height="623" src="https://i-blog.csdnimg.cn/direct/82dd6aee73324524ac66bfc45527e12b.png" width="1489"/>
    </p>
    <h3>
     <span style="color:#000000">
      <strong>
       3.4. NVS
      </strong>
     </span>
    </h3>
    <p>
     本文方法不依赖精确的相机姿态，使其在现实场景中更具适用性。通过联合优化几何和外观任务，本文方法在 PSNR、SSIM 和 LPIPS 指标上均优于基线方法。
    </p>
    <p style="background-color:transparent">
     <strong>
      评估指标
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       在
       <strong>
        RealEstate10K
       </strong>
       和
       <strong>
        DL3DV
       </strong>
       数据集上，使用以下指标评估渲染质量：
      </p>
      <ul>
       <li>
        <p>
         <strong>
          PSNR（峰值信噪比）
         </strong>
         ：衡量渲染图像与真实图像之间的像素级差异。
        </p>
       </li>
       <li>
        <p>
         <strong>
          SSIM（结构相似性）
         </strong>
         ：衡量渲染图像与真实图像在结构上的相似性。
        </p>
       </li>
       <li>
        <p>
         <strong>
          LPIPS（学习感知图像块相似性）
         </strong>
         ：通过深度学习模型衡量渲染图像与真实图像在感知上的差异。
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <p class="img-center">
     <img alt="" height="240" src="https://i-blog.csdnimg.cn/direct/b6b18b54f1814f3aa1bb86c5d404857b.png" width="503"/>
    </p>
    <p class="img-center">
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/0a183901b7e54d9eaf95d69b88689be9.png" width="443"/>
    </p>
    <p>
     <img alt="" height="1058" src="https://i-blog.csdnimg.cn/direct/750b35375e194041a9c31af5b53ecb1a.png" width="1507"/>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37343331303634362f:61727469636c652f64657461696c732f313435373837353839" class_="artid" style="display:none">
 </p>
</div>


