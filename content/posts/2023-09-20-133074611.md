---
layout: post
title: 分布式系统测试vdbench的使用教程裸盘测试和文件系统测试ceph
date: 2023-09-20 13:09:03 +0800
categories: ['Distributedsystem']
tags: ['分布式存储系统', '分布式']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=133074611
    alt: 分布式系统测试vdbench的使用教程裸盘测试和文件系统测试ceph
artid: 133074611
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=133074611
featuredImagePreview: https://bing.ee123.net/img/rand?artid=133074611
---
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【分布式系统测试】vdbench的使用教程——裸盘测试和文件系统测试ceph
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     即看即用
     <br/>
     1、下载 https://www.oracle.com/downloads/server-storage/vdbench-downloads.html
    </p>
    <p>
     (外网不一定能访问下载，可以直接在csdn下载栏搜索。下载)
    </p>
    <p>
     2、解压
    </p>
    <p>
     文件夹中的vdbench.bat是个windows用，vdbench 给linux用
    </p>
    <p>
     Linux
     <br/>
     3、进入文件夹
    </p>
    <p>
     ./vdbench -t   测试一下可用性
    </p>
    <p>
     运行测试模型
    </p>
    <p>
     ./vdbench -f {filename} -o {exportpath}
     <br/>
     #注：-f后接测试参数文件/名脚本名，-o后接导出测试结果路径
     <br/>
     脚本的编写
    </p>
    <p>
     这部分可以参考解压后的examples文件夹下的脚本，比如create_files；也可以参考output目录下的parmfile.html
    </p>
    <p>
     运行脚本
    </p>
    <p>
     root@node1:/home/vdbench50406# ./vdbench  -f  examples/filesys/create_files  -jn
    </p>
    <p>
     其中-jn是为了做数据校验，见文章末尾解释。
    </p>
    <p>
     输出结果
    </p>
    <p>
     windows
     <br/>
     3、进入文件夹
    </p>
    <p>
     可以执行./vdbench.bat -t  测试一下可用性(需要安装jre/jdk，要配置好环境变量)
    </p>
    <p>
     编写windows vdbench 脚本(或叫做配置文件) parmfile
    </p>
    <p>
     ./vdbench -f  parmfile
    </p>
    <p>
     这里的parmfile需要根据测试环境修改，可以参考output目录下的parmfile
     <br/>
     样例：
     <br/>
     dd if=/dev/zero of=/tmp/disk1 bs=4 count=1M
     <br/>
     vim parmfile
     <br/>
     sd=sd1,lun=/tmp/disk1 size=4m
     <br/>
     wd=wd1,sd=sd1,xfersize=4096,readpct=100
     <br/>
     rd=run1,wd=wd1,iorate=100,elapsed=10,interval=1
     <br/>
     [root@localhost vdbench504]# ./vdbench -f parmfile.html
    </p>
    <p>
     建立rsh通信
    </p>
    <p>
     ./vdbench rsh
    </p>
    <p>
     注：此命令是用于windows系统多主机联机跑vdbench时使用，因为windows操作系统不支持ssh，因此，vdbench提供了rsh的通信方式。在目标主机上执行此工具后，vdbench将会启动一个java socket用于vdbench slave与master之间通信
    </p>
    <p>
     详细说明
     <br/>
     vdbench简介
     <br/>
     vdbench 是一个磁盘I/O负载生成器，主要用于基准测试和存储产品的测试。
    </p>
    <p>
     vdbench 是由java语言编写的，由oracle公司开发。
    </p>
    <p>
     可以使用vdbench测试磁盘和文件系统的读写性能。
    </p>
    <p>
     这个工具有以下优点：
    </p>
    <p>
     1、能够每秒显示整个测试的io叠加，这样测试整个集群的io的时候，可以把所有虚机启动起来，然后进行io的压测，而不是去压单个rbd的iops，那个没有太大的意义，只能是一个数值，真正的环境大多也不是给一个业务使用的，也可以跑起一个业务以后，再看剩余的机器还能跑多少性能
     <br/>
     2、在测试输出报告里面会根据主机统计一次io，这个面向的业务场景就是，比如某台主机上面可能挂载多块云盘，那么可以根据主机进行统计
     <br/>
     3、在报告里面还会根据设备显示io个延时的信息，也就是只要是测试设备，每一个的性能指标都能查到，这个的好处就是检测集群里面的io是不是均匀的，如果做了qos，设备的测试性能值是不是跟设置限制一样
    </p>
    <p>
     本文链接：https://www.codercto.com/a/49559.html
    </p>
    <p>
     与fio的区别
    </p>
    <p>
     这个比fio强大的是，既能够测试块接口也能测试文件接口，文件接口是去模拟写入文件，这个又和mdtest类似，但是mdtest主要是去测试元数据能力，vdbench则比较综合。
    </p>
    <p>
     windows下载地址：
    </p>
    <p>
     https://www.oracle.com/downloads/server-storage/vdbench-downloads.html
    </p>
    <p>
     使用方法
     <br/>
     下载 https://www.oracle.com/downloads/server-storage/vdbench-downloads.html
    </p>
    <p>
     解压，文件夹中的vdbench.bat是个windows用，vdbench 给linux用。
    </p>
    <p>
     使用方法：
    </p>
    <p>
     1、编写脚本(或叫做配置文件)
    </p>
    <p>
     这部分可以参考解压后的examples⽂件夹下的脚本，⽐如create_files；也可以参考output⽬录下的parmfile.html
    </p>
    <p>
     fsd=fsd1,anchor=/dir,depth=1,width=1,files=10000,size=8k
     <br/>
     fwd=fwd1,fsd=fsd1,operation=read,threads=16
     <br/>
     rd=rd1,fwd=fwd*,fwdrate=100,format=yes,elapsed=5,interval=1
    </p>
    <p>
     注：(1)anchor=/dir 这个需要把路径换成你ceph挂载的路径，我的是anchor=/home/baymax/cephfs
    </p>
    <p>
     2、运行vdbench按脚本执行
     <br/>
     ./vdbench   -f   examples/filesys/create_files    -jn
     <br/>
     其中-jn是为了做数据校验，⽬前还不太懂。
    </p>
    <p>
     <br/>
     3、输出结果
    </p>
    <p>
     注意事项
     <br/>
     在联机测试时，客户端的系统时间需保持一致，否则会出现时钟同步告警(this can lead to heartbeat issues)
     <br/>
     客户端的防火墙要关闭(或者设置开放程序指定端口5570、5560访问)
     <br/>
     关闭系统日志服务rsyslog，避免运行时出现其他日志文件打印信息
     <br/>
     参数文件添加messagescan=no可以过滤掉多余的系统日志
     <br/>
     常用执行选项和参数文件说明
     <br/>
     vdbench 测试工具涉及到的参数非常多，这些参数可以分为 块存储 和 文件系统 两部分，每 部分又是按组进行划分的。 所有的参数都需要定义到一个参数文件当中，在被读取的时候都是按顺序进行的，所以在定义 时需要按指定的顺序进行定义。
    </p>
    <p>
     块存储参数文件的定义顺序： HD, SD, WD, RD
    </p>
    <p>
     文件系统参数文件的定义顺序： HD, FSD, FWD, RD
    </p>
    <p>
     HD, SD, WD, RD/HD, FSD, FWD, RD 配置文件中的关联依赖说明：
    </p>
    <p>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
    </p>
    <p>
     #有3个客户端主机，分别命名为hd1、hd2、hd3，IP/hostname分别是：node241、node242、node243
    </p>
    <p>
     hd=hd1,system=node241
     <br/>
     hd=hd2,system=node242
     <br/>
     hd=hd3,system=node243
    </p>
    <p>
     #待测试的存储分别命名为sd1、sd2、sd3，sd1指代的是lun=/dev/sdb，用客户端hd1测试，
    </p>
    <p>
     sd2指代的是lun=/dev/sdc，用客户端hd2测试，sd3指代的是lun=/dev/sdb，用客户端hd3测试
     <br/>
     sd=sd1,hd=hd1,lun=/dev/sdb,openflag=o_direct
     <br/>
     sd=sd2,hd=hd2,lun=/dev/sdc,openflag=o_direct
     <br/>
     sd=sd3,hd=hd3,lun=/dev/sdb,openflag=o_direct
    </p>
    <p>
     #定义我们的工作负载，名叫wd1，这个工作负载包括sd* (就是我们上面定义的sd1、sd2、sd3)
     <br/>
     wd=wd1,sd=sd*,seekpct=0,rdpct=0,xfersize=1M
    </p>
    <p>
     #定义我们的vdbench要运行的内容，命名为rd1,要做的工作是我们上面定义的wd1
     <br/>
     rd=rd1,wd=wd1,iorate=max,maxdata=100M,elapsed=64800,warmup=30,interval=5
    </p>
    <p>
     【存储测试】vdbench存储性能测试工具 - https://www.cnblogs.com/luxf0/p/13321077.html
    </p>
    <p>
     3.1 常用选项
    </p>
    <p>
     -t: 运行5秒的块设备测试，用于测试vdbench是否可用。
    </p>
    <p>
     -tf: 运行5秒的文件系统测试，用于测试vdbench是否可用。
    </p>
    <p>
     -f perfile: 加上参数文件prefile，进行实际测试。
    </p>
    <p>
     -v: 开启数据校验。
    </p>
    <p>
     -vr: 写入数据的同时也进行数据校验，通常用于测试时间比较长的情况。
    </p>
    <p>
     rsh: 运行rsh守护进程，用于windows联机运行测试。
    </p>
    <p>
     -o xxx: 将输出文件的路径另为其它路径，默认是保存在当前目录的output目录中。
    </p>
    <p>
     其它选项： -j , -jn , -jr
    </p>
    <p>
     1、文件系统
     <br/>
     文件系统参数文件定义顺序为：HD、FSD、FWD、RD
    </p>
    <p>
     1.1、HD(Host Define 主机定义)
    </p>
    <p>
     非必选项，单机运行时不需要配置HD参数，一般只有在多主机联机(给多个主机命名，以便在结果中区分)测试时才需要配置
    </p>
    <p>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node241
     <br/>
     hd=hd2,system=node242
     <br/>
     hd=hd3,system=node243
     <br/>
     hd= 标识主机定义的名称，多主机运行时，可以使用hd1、hd2、hd3...区分
     <br/>
     system= 主机IP地址或主机名
     <br/>
     vdbench= vdbench执行文件存放路径，当多主机存放路径不同时，可在hd定义时单独指定
     <br/>
     user= slave和master通信使用用户
     <br/>
     shell= 可选值为rsh、ssh或vdbench，默认值为rsh，多主机联机测试时，mater和slave主机间通信方式 当参数值为rsh时，需要配置master和slave主机rsh互信，考虑到rsh使用明文传输，安全级别不够，通常情况下不建议使用这种通信方式 当参数值为ssh时，需要配置master和slave主机ssh互信，通常Linux主机联机时使用此通信方式 当参数值为vdbench，需要在所有slave主机运行vdbench rsh启用vdbench本身的rsh守护进程，通常Window主机联机时使用此通信方式
     <br/>
     1.2、FSD(File System Define 文件系统定义)
    </p>
    <p>
     fsd=default,depth=2,width=3,files=2,size=128k
     <br/>
     fsd=fsd1,anchor=/mnt/client1
     <br/>
     fsd=fsd2,anchor=/mnt/client2
     <br/>
     fsd=fsd3,anchor=/mnt/client3
     <br/>
     fsd= 标识文件系统定义的名称，多文件系统时(fsd1、fsd2、fsd3...)，可以指定default(将相同的参数作为所有fsd的默认值)
     <br/>
     anchor= 文件写入根目录
     <br/>
     depth= 创建目录层级数(即目录深度)
     <br/>
     width= 每层文件夹的子文件夹数
     <br/>
     files= 测试文件个数(vdbench测试过程中会生成多层级目录结构，实际只有最后一层目录会生成测试文件)
     <br/>
     size= 每个测试文件大小
     <br/>
     **distribution= ** 可选值为bottom或all，默认为bottom --当参数值为bottom时，程序只在最后一层目录写入测试文件 --当参数值为all时，程序在每一层目录都写入测试文件
     <br/>
     shared= 可选值为yes或no，默认值为no，一般只有在多主机联机测试时指定 vdbench不允许不同的slave之间共享同一个目录结构下的所有文件，因为这样会带来很大的开销，但是它们允许共享同一个目录结构。加入设置了shared=yes，那么不同的slave可以平分一个目录下所有的文件来进行访问，相当于每个slave有各自等分的访问区域，因此不能测试多个客户的对同一个文件的读写 --当多主机联机测试时，写入的根目录anchor为同一个路径时，需要指定参数值为yes
     <br/>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node1
     <br/>
     hd=hd2,system=node2
     <br/>
     hd=hd3,system=node3
     <br/>
     fsd=fsd1,anchor=/client/,depth=2,width=100,files=100,size=4k,shared=yes
     <br/>
     计算公式如下：  最后一层生成文件夹个数=widthdepth  测试文件个数=(widthdepth)*files
    </p>
    <p>
     fsd=fsd1,anchor=/dir1,depth=2,width=3,files=2,size=128k
     <br/>
     以上述参数为例，生成目录结构及测试文件如下：
     <br/>
     最后一层文件夹数=3^2=9 最后一层文件数=9*2=18
     <br/>
     /dir1/
     <br/>
     ├── no_dismount.txt
     <br/>
     ├── vdb.1_1.dir
     <br/>
     │   ├── vdb.2_1.dir
     <br/>
     │   │   ├── vdb_f0001.file
     <br/>
     │   │   └── vdb_f0002.file
     <br/>
     │   ├── vdb.2_2.dir
     <br/>
     │   │   ├── vdb_f0001.file
     <br/>
     │   │   └── vdb_f0002.file
     <br/>
     │   └── vdb.2_3.dir
     <br/>
     │       ├── vdb_f0001.file
     <br/>
     │       └── vdb_f0002.file
     <br/>
     ├── vdb.1_2.dir
     <br/>
     │   ├── vdb.2_1.dir
     <br/>
     │   │   ├── vdb_f0001.file
     <br/>
     │   │   └── vdb_f0002.file
     <br/>
     │   ├── vdb.2_2.dir
     <br/>
     │   │   ├── vdb_f0001.file
     <br/>
     │   │   └── vdb_f0002.file
     <br/>
     │   └── vdb.2_3.dir
     <br/>
     │       ├── vdb_f0001.file
     <br/>
     │       └── vdb_f0002.file
     <br/>
     ├── vdb.1_3.dir
     <br/>
     │   ├── vdb.2_1.dir
     <br/>
     │   │   ├── vdb_f0001.file
     <br/>
     │   │   └── vdb_f0002.file
     <br/>
     │   ├── vdb.2_2.dir
     <br/>
     │   │   ├── vdb_f0001.file
     <br/>
     │   │   └── vdb_f0002.file
     <br/>
     │   └── vdb.2_3.dir
     <br/>
     │       ├── vdb_f0001.file
     <br/>
     │       └── vdb_f0002.file
     <br/>
     └── vdb_control.file
     <br/>
     <br/>
     12 directories, 20 files
    </p>
    <p>
     1.3、FWD(FileSystem Workload Defile 文件系统工作负载定义)
    </p>
    <p>
     fwd=default,operation=read,xfersize=4k,fileio=sequential,fileselect=random,threads=2
     <br/>
     fwd=fwd1,fsd=fsd1,host=hd1
     <br/>
     fwd=fwd2,fsd=fsd2,host=hd2
     <br/>
     fwd=fwd3,fsd=fsd3,host=hd3
     <br/>
     fwd= 标识文件系统工作负载定义的名称，多文件系统工作负载定义时，可以使用fwd1、fwd2、fwd3...区分
     <br/>
     fsd= 标识此工作负载使用文件存储定义的名称
     <br/>
     host= 标识此工作负载使用主机
     <br/>
     operation= 可选值为read或write,文件操作方式
     <br/>
     rdpct= 可选值为0~100，读操作占比百分比，一般混合读写时需要指定，当值为60时，则混合读写比为6：4
     <br/>
     fileio= 可选值为random或sequential，标识文件 I/O 将执行的方式
     <br/>
     fileselect= random或sequential，标识选择文件或目录的方式
     <br/>
     xfersizes= 数据传输(读取和写入操作)处理的数据大小(即单次IO大小)
     <br/>
     threads= 此工作负载的并发线程数量
     <br/>
     1.4、RD(Run Define 运行定义)
    </p>
    <p>
     rd=rd1,fwd=(fwd1-fwd3),fwdrate=max,format=restart,elapsed=604800,interval=10
     <br/>
     rd= 标识文件系统运行定义的名称。
     <br/>
     fwd= 标识文件系统工作负载定义的名称。
     <br/>
     fwdrate= 每秒执行的文件系统操作数量。设置为max，表示不做任何限制，按照最大强度自适应
     <br/>
     format= 可选值为yes、no或restart，标识预处理目录和文件结构的方式 --yes表示删除目录和文件结构再重新创建 --no表示不删除目录和文件结构 --restart表示只创建未生成的目录或文件，并且增大未达到实际大小的文件
     <br/>
     elapsed= 默认值为30，测试运行持续时间(单位为秒)
     <br/>
     interval= 结果输出打印时间间隔(单位为秒)
     <br/>
     2、块设备(裸盘测试)
     <br/>
     块设备参数文件定义顺序为：HD、SD、WD、RD
    </p>
    <p>
     1.1、HD(Host Define 主机定义)
    </p>
    <p>
     非必选项，单机运行时不需要配置HD参数，一般只有在多主机联机(给多个主机命名，以便在结果中区分)测试时才需要配置
    </p>
    <p>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node241
     <br/>
     hd=hd2,system=node242
     <br/>
     hd=hd3,system=node243
     <br/>
     hd= 标识主机定义的名称，多主机运行时，可以使用hd1、hd2、hd3...区分
     <br/>
     system= 主机IP地址或主机名
     <br/>
     vdbench= vdbench执行文件存放路径，当多主机存放路径不同时，可在hd定义时单独指定
     <br/>
     user= slave和master通信使用用户
     <br/>
     shell= 可选值为rsh、ssh或vdbench，默认值为rsh，多主机联机测试时，mater和slave主机间通信方式 当参数值为rsh时，需要配置master和slave主机rsh互信，考虑到rsh使用明文传输，安全级别不够，通常情况下不建议使用这种通信方式 当参数值为ssh时，需要配置master和slave主机ssh互信，通常Linux主机联机时使用此通信方式 当参数值为vdbench，需要在所有slave主机运行vdbench rsh启用vdbench本身的rsh守护进程，通常Window主机联机时使用此通信方式
     <br/>
     1.2、SD(Storage Define 存储定义)
    </p>
    <p>
     sd=sd1,hd=hd1,lun=/dev/sdb,openflags=o_direct,threads=6
     <br/>
     sd=sd3,hd=hd2,lun=/dev/sdb,openflags=o_direct,threads=6
     <br/>
     sd=sd6,hd=hd3,lun=/dev/sdb,openflags=o_direct,threads=6
     <br/>
     sd= 标识存储定义的名称
     <br/>
     hd= 标识主机定义的名称
     <br/>
     lun= 写入块设备，如：/dev/sdb, /dev/sdc...
     <br/>
     openflags= 通过设置为o_direct，以无缓冲缓存的方式进行读写操作
     <br/>
     threads= 对SD的最大并发I/O请求数量
     <br/>
     1.3、WD(Workload Define 工作负载定义)
    </p>
    <p>
     wd=wd1,sd=sd*,seekpct=100,rdpct=100,xfersize=8k,skew=40
     <br/>
     wd=wd2,sd=sd*,seekpct=100,rdpct=0,xfersize=8k,skew=10
     <br/>
     wd=wd3,sd=sd*,seekpct=100,rdpct=100,xfersize=1024k,skew=40
     <br/>
     wd=wd4,sd=sd*,seekpct=100,rdpct=0,xfersize=1024k,skew=10
     <br/>
     wd= 标识工作负载定义的名称
     <br/>
     sd= 标识存储定义的名称
     <br/>
     seekpct= 可选值为0或100(也可使用sequential或random表示)，默认值为100，随机寻道的百分比，设置为0时表示顺序，设置为100时表示随机。
     <br/>
     rdpct= 读取请求占请求总数的百分比，设置为0时表示写，设置为100时表示读
     <br/>
     xfersize= 要传输的数据大小。默认设置为4k
     <br/>
     skew= 非必选项，一般在多个工作负载时需要指定，表示该工作负载占总工作量百分比(skew总和为100)
     <br/>
     1.4、RD(Run Define 运行定义)
    </p>
    <p>
     rd=rd1,wd=wd*,iorate=max,maxdata=400GB,warmup=30,elapse=604800,interval=5
     <br/>
     rd= 标识运行定义的名称
     <br/>
     wd= 标识工作负载定义的名称
     <br/>
     iorate= 常用可选值为100、max，此工作负载的固定I/O速率 --当参数值为100时，以每秒100个I/Os的速度运行工作负载，当参数值设置为一个低于最大速率的值时，可以达到限制读写速度的效果 --当参数值为max时，以最大的I/O速率运行工作负载，一般测试读写最大性能时，该参数值均为max
     <br/>
     warmup= 预热时间(单位为秒)，默认情况下vdbench会将第一个时间间隔输出数据排除在外,程序在预热时间内的测试不纳入最终测试结果中(即预热结束后，才开始正式测试) --当interval为5、elapsed为600时，测试性能为2elapsed/interval(avg_2-120)时间间隔内的平均性能 --当interval为5、warmup为60、elapsed为600时，测试性能为1+(warmup/interval)(warmup+elapsed)/interval(avg_13-132)时间间隔内的平均性能
     <br/>
     maxdata= 读写数据大小，通常情况下，当运行elapsed时间后测试结束；当同时指定elapsed和maxdata参数值时，以最快运行完的参数为准(即maxdata测试时间小于elapsed时，程序写完elapsed数据量后结束) --当参数值为100以下时，表示读写数据量为总存储定义大小的倍数(如maxdata=2，2个存储定义(每个存储定义数据量为100G)，则实际读写数据大小为400G) --当参数值为100以上时，表示数据量为实际读写数据量(可以使用单位M、G、T等)
     <br/>
     elapsed= 默认值为30，测试运行持续时间(单位为秒)
     <br/>
     interval= 报告时间间隔(单位为秒)
     <br/>
     运行使用
     <br/>
     1、单机运行
     <br/>
     Linux  示例如下，单节点针对裸盘测试，1M顺序写，测试时间600s，预热时间60s，报告时间间隔2s
     <br/>
     [root@node241 vdbench50406]# cat Single-RawDisk.html
     <br/>
     sd=sd1,lun=/dev/sdb,openflag=o_direct
     <br/>
     wd=wd1,sd=sd1,seekpct=0,rdpct=0,xfersize=1M
     <br/>
     rd=rd1,wd=wd1,iorate=max,warmup=60,elapsed=600,interval=2
     <br/>
     [root@node241 vdbench50406]#
     <br/>
     [root@node241 vdbench50406]# ./vdbench -f Single-RawDisk.html
     <br/>
     Window  示例如下，单节点针对文件系统测试，1M顺序写，目录深度为2，每层目录数为3，每个目录文件数为10，每个文件大小为200M，测试时间为600s，报告时间时间2s
     <br/>
     E:\vdbench50406&gt;more "Single FileSystem.txt"
     <br/>
     fsd=fsd1,anchor=E:\Sigle-FileSystem,depth=2,width=3,files=10,size=200M
     <br/>
     fwd=fwd1,fsd=fsd1,operation=write,xfersize=1M,fileio=sequential,fileselect=rando
     <br/>
     m,threads=2
     <br/>
     rd=rd1,fwd=fwd1,fwdrate=max,format=yes,elapsed=600,interval=5
     <br/>
     E:\vdbench50406&gt;
     <br/>
     E:\vdbench50406&gt;vdbench -f "Single FileSystem.txt"
     <br/>
     2、联机运行
     <br/>
     配置免秘钥登录
     <br/>
     联机测试，需要配置此项
    </p>
    <p>
     示例使用三个客户端联机测试，使用客户端node241作为主节点
    </p>
    <p>
     客户端主机名    客户端IP
     <br/>
     node241    66.66.66.241
     <br/>
     node242    66.66.66.242
     <br/>
     node243    66.66.66.243
     <br/>
     将每个节点IP和主机名的映射关系写入到/etc/hosts配置文件内
    </p>
    <p>
     echo '66.66.66.241 node241' &gt;&gt; /etc/hosts
     <br/>
     echo '66.66.66.242 node242' &gt;&gt; /etc/hosts
     <br/>
     echo '66.66.66.243 node243' &gt;&gt; /etc/hosts
     <br/>
     主节点生成公钥文件，并拷贝到其他从节点(配置主节点到从节点免秘钥登录)
    </p>
    <p>
     ssh-keygen
     <br/>
     ssh-copy-id node242
     <br/>
     ssh-copy-id node243
     <br/>
     Linux 1、按照二、安装部署，配置多主机ssh互信 2、master主机运行测试参数文件即可 示例如下，三节点针对裸盘联机测试，1M顺序写，测试数据量为400G，预热时间30s，报告间隔5s
     <br/>
     [root@node241 vdbench50406]# cat Multi-RawDisk
     <br/>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node241
     <br/>
     hd=hd2,system=node242
     <br/>
     hd=hd3,system=node243
     <br/>
     sd=sd1,hd=hd1,lun=/dev/sdb,openflag=o_direct
     <br/>
     sd=sd2,hd=hd2,lun=/dev/sdb,openflag=o_direct
     <br/>
     sd=sd3,hd=hd3,lun=/dev/sdb,openflag=o_direct
     <br/>
     wd=wd1,sd=sd*,seekpct=0,rdpct=0,xfersize=1M
     <br/>
     rd=rd1,wd=wd1,iorate=max,maxdata=100M,elapsed=64800,warmup=30,interval=5
     <br/>
     [root@node241 vdbench50406]#
     <br/>
     [root@node241 vdbench50406]# ./vdbench -f Multi-RawDisk
     <br/>
     Window 1、所有slave主机运行vdbench本身rsh守护进程
     <br/>
     E:\vdbench50406&gt;vdbench rsh
     <br/>
     2、master主机运行测试参数文件即可 示例如下，三节点针对文件系统联机测试，1M顺序写，目录深度为2，每层目录数为3，每个目录文件数为10000，每个文件大小为200M，测试时间为600s，报告间隔1s
    </p>
    <p>
     E:\vdbench50406&gt;more "Multi FileSystem.txt"
     <br/>
     hd=default,vdbench=E:\vdbench50406,user=Micah,shell=vdbench
     <br/>
     hd=hd1,system=66.66.66.250
     <br/>
     hd=hd2,system=66.66.66.252
     <br/>
     fsd=fsd1,anchor=Z:\Sigle-FileSystem-01,depth=2,width=3,files=10000,size=200M
     <br/>
     fsd=fsd2,anchor=Z:\Sigle-FileSystem-02,depth=2,width=3,files=10000,size=200M
     <br/>
     fwd=default,operation=write,xfersize=1M,fileio=sequential,fileselect=random,threads=16
     <br/>
     fwd=fwd1,fsd=fsd1,host=hd1
     <br/>
     fwd=fwd2,fsd=fsd2,host=hd2
     <br/>
     rd=rd1,fwd=fwd*,fwdrate=max,format=yes,elapsed=600,interval=1
     <br/>
     E:\vdbench50406&gt;
     <br/>
     E:\vdbench50406&gt;vdbench -f "Multi FileSystem.txt"
     <br/>
     结果分析
     <br/>
     当vdbench运行完负载测试后，会在安装目录下生成output文件夹，里边包含测试结果文件
    </p>
    <p>
     1、输出文件
     <br/>
     errorlog.html 当运行测试启用数据校验时，它可能会包含一些错误信息，如：
     <br/>
     无效的密钥读取
     <br/>
     无效的 lba 读取(一个扇区的逻辑字节地址)
     <br/>
     无效的 SD 或 FSD 名称读取
     <br/>
     数据损坏
     <br/>
     坏扇区
     <br/>
     flatfile.html vdbench 生成的一种逐列的 ASCII 格式的信息，可以使用parseflat参数解析结果
     <br/>
     ./vdbench parseflat -i &lt;flatfile.html&gt; -o output.csv [-c col1 col2 ..] [-a] [-f col1 value1 col2 value2..]
     <br/>
     -i input flatfile, e.g. output/flatfile.html
     <br/>
     -o output CSV file name (default stdout)
     <br/>
     -c which column to write to CSV. Columns are written in the order specified
     <br/>
     -f filters: 'if (colX == valueX) ... ...' (Alphabetic compare)
     <br/>
     -a include only the 'avg' data. Default: include only non-avg data.
     <br/>
     -i是表示待分析的文件，这里写vdbench输出目录里的flatfile.html这个文件，写其它文件不能正常解析；
     <br/>
     -o是解析后的输出文件，可以手动指定存放目录。文件格式为CSV，此文件的列由-c参数指定，列的顺序为-c参数的顺序
     <br/>
     -a是表示csv文件中只记录测试过程中的avg值
     <br/>
     <br/>
     示例如下：
     <br/>
     .\vdbench.bat parseflat -i D:\vdbench50406\output\flatfile.html -c run rate MB/sec seekpct rdpct bytes/io threads resp -o d:\output.csv -a
     <br/>
     vdbench parseflat arguments:
     <br/>
     Argument 0: -i
     <br/>
     Argument 1: D:\vdbench50406\output\flatfile.html
     <br/>
     Argument 2: -c
     <br/>
     Argument 3: run
     <br/>
     Argument 4: rate
     <br/>
     Argument 5: MB/sec
     <br/>
     Argument 6: seekpct
     <br/>
     Argument 7: rdpct
     <br/>
     Argument 8: bytes/io
     <br/>
     Argument 9: threads
     <br/>
     Argument 10: resp
     <br/>
     Argument 11: -o
     <br/>
     Argument 12: D:\output.csv
     <br/>
     Argument 13: -a
     <br/>
     14:12:49.265 ParseFlat completed successfully.
     <br/>
     histogram.html 一种包含报告柱状图的响应时间、文本格式的文件
     <br/>
     logfile.html 包含 Java 代码写入控制台窗口的每行信息的副本。logfile.html 主要用于调试用途
     <br/>
     parmfile.html 包含测试运行配置参数信息
     <br/>
     summary.html 记录全部数据信息，显示每个报告间隔内总体性能情况及工作负载情况，以及除第一个间隔外的所有间隔的加权平均值
     <br/>
     totals.html 记录全部数据计算之后的平均值，一般测试结果从该文件取值，除第一个间隔外所有间隔的加权平均值
     <br/>
     2、结果分析
     <br/>
     2.1、文件系统
    </p>
    <p>
     #测试参数如下：
     <br/>
     hd=default,vdbench=E:\vdbench50406,user=Micah,shell=vdbench
     <br/>
     hd=hd1,system=66.66.66.250
     <br/>
     hd=hd2,system=66.66.66.252
     <br/>
     fsd=fsd1,anchor=Z:\Sigle-FileSystem-01,depth=2,width=3,files=10,size=4M
     <br/>
     fsd=fsd2,anchor=Z:\Sigle-FileSystem-02,depth=2,width=3,files=10,size=4M
     <br/>
     fwd=default,operation=write,xfersize=1M,fileio=sequential,fileselect=random,threads=2
     <br/>
     fwd=fwd1,fsd=fsd1,host=hd1
     <br/>
     fwd=fwd2,fsd=fsd2,host=hd2
     <br/>
     rd=rd1,fwd=fwd*,fwdrate=max,format=yes,elapsed=600,interval=5
     <br/>
     <br/>
     #测试结果如下：
     <br/>
     18:47:03.001 Starting RD=format_for_rd1
     <br/>
     <br/>
     六月 04, 2020 .Interval. .ReqstdOps... ...cpu%...  read ....read..... ....write.... ..mb/sec... mb/sec .xfer.. ...mkdir.... ...rmdir.... ...create... ....open.... ...close.... ...delete...
     <br/>
     rate   resp total  sys   pct   rate   resp   rate   resp  read write  total    size  rate   resp  rate   resp  rate   resp  rate   resp  rate   resp  rate   resp
     <br/>
     18:48:40.218   avg_2-20   57.6  6.244  13.4 2.99   0.0    0.0  0.000   57.6  6.244  0.00  7.20   7.20  131072   0.2 104.49   0.2 41.526   1.8 7527.0   1.8 192.01   1.8 7134.3   1.8 21.984
     <br/>
     <br/>
     18:48:42.000 Starting RD=rd1; elapsed=600; fwdrate=max. For loops: None
     <br/>
     <br/>
     18:58:42.205  avg_2-120    6.2  1.063  13.0 2.80   0.0    0.0  0.000    6.2  1.063  0.00  6.24   6.24 1048576   0.0  0.000   0.0  0.000   0.0  0.000   1.6 47.864   1.6 2401.1   0.0  0.000
     <br/>
     totals.html一般包括两个部分，第一部分为文件存储目录结构及数据填充的平均性能值，第二部分为执行测试过程中除第一个时间间隔外所有时间间隔平均性能值，主要看第二部分的内容
    </p>
    <p>
     Interval 报告间隔序号，测试结果一般为除第一个时间间隔外所有时间间隔加权平均值 如elapsed=600,interval=5，则性能结果为第2个间隔到第120个间隔的平均值(avg_2-120)
    </p>
    <p>
     ReqstdOps
    </p>
    <p>
     rate 每秒读写I/O个数(读写IOPS)，可以通过rd运行定义参数fwdrate控制 当fwdrate为max时，以最大I/O速率运行工作负载 当fwdrate为低于最大I/0速率的一个数值时，可以限制读写速度，以固定I/O速率运行工作负载
     <br/>
     resp 读写请求响应时间(读写时延)，单位为ms
     <br/>
     cpu%
    </p>
    <p>
     tatol 总的cpu占用率
     <br/>
     sys 系统cpu占用率
     <br/>
     read pct 读取请求占总请求数百分比占比，当为0时表示写，当为100时表示读
    </p>
    <p>
     read
    </p>
    <p>
     rate 每秒读I/O个数(读IOPS)
     <br/>
     resp 读请求响应时间(读时延)，单位为ms
     <br/>
     write
    </p>
    <p>
     rate 每秒写I/O个数(写IOPS)
     <br/>
     resp 写请求响应时间(写时延)，单位为ms
     <br/>
     mb/sec
    </p>
    <p>
     read 每秒读取速度
     <br/>
     write 每秒写入速度
     <br/>
     total 每秒读写速度总和
     <br/>
     xfersize 每个读写I/O传输数据量(即单个读写I/O大小)，单位为字节B
    </p>
    <p>
     2.2、块设备
    </p>
    <p>
     #测试参数如下：
     <br/>
     messagescan=no  #可以过滤掉多余的系统日志
     <br/>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node241
     <br/>
     hd=hd2,system=node242
     <br/>
     hd=hd3,system=node243
     <br/>
     sd=sd1,hd=hd1,lun=/dev/sdb,openflag=o_direct
     <br/>
     sd=sd2,hd=hd2,lun=/dev/sdb,openflag=o_direct
     <br/>
     sd=sd3,hd=hd3,lun=/dev/sdb,openflag=o_direct
     <br/>
     wd=wd1,sd=sd*,seekpct=0,rdpct=0,xfersize=1M
     <br/>
     rd=rd1,wd=wd1,iorate=max,elapsed=600,warmup=30,interval=5
     <br/>
     <br/>
     #测试结果如下：
     <br/>
     &lt;a name="_1143839598"&gt;&lt;/a&gt;&lt;i&gt;&lt;b&gt;19:02:15.001 Starting RD=rd1; I/O rate: Uncontrolled MAX; elapsed=600 warmup=30; For loops: None&lt;/b&gt;&lt;/i&gt;
     <br/>
     <br/>
     Jun 04, 2020  interval        i/o   MB/sec   bytes   read     resp     read    write     resp     resp queue  cpu%  cpu%
     <br/>
     rate  1024**2     i/o    pct     time     resp     resp      max   stddev depth sys+u   sys
     <br/>
     19:12:46.068 avg_7-126      82.74    82.74 1048576   0.00  289.158    0.000  289.158 2092.803  155.103  23.9  16.3  14.2
     <br/>
     interval 报告间隔序号，测试结果一般为除第一个时间时间外所有时间间隔加权平均值，如有设置预热时间，则这部分测试数据也需要排除在外 如elapsed=600,warmup=30，interval=5，则性能测试结果为第7个间隔到第126个间隔的平均值(avg_7-126)
    </p>
    <p>
     i/o rate 每秒读写I/O个数(读写IOPS)，可以通过rd运行定义参数iorate控制 当iorate为max时，以最大I/O速率运行工作负载 当iorate为低于最大I/0速率的一个数值时，可以限制读写速度，以固定I/O速率运行工作负载
    </p>
    <p>
     MB/sec 每秒读写速度(读写带宽) 注：按官方手册说明，
    </p>
    <p>
     bytes i/0 每个读写I/O传输数据量(即单个读写I/O大小)，单位为字节B，可以通过wd工作负载定义参数xfersize控制
    </p>
    <p>
     read pct 读取请求占请求总数的百分比，可以通过wd工作负载定义参数rdpct控制 当rdpct为0时，表示测试模型为写 当rdpct为100时，表示测试模型为读
    </p>
    <p>
     resp time 请求响应时间(读写时延)，单位为毫秒ms
    </p>
    <p>
     read resp 读取请求响应时间，单位为毫秒ms
    </p>
    <p>
     write resp 写入请求响应时间，单位为毫秒ms
    </p>
    <p>
     resp max 最大请求响应时间，单位为毫秒ms
    </p>
    <p>
     resp stddev 请求响应时间标准偏差，单位为毫秒ms
    </p>
    <p>
     queue depth 读写I/0队列深度
    </p>
    <p>
     cpu% sys+u 内核态空间及用户态空间CPU占用率
    </p>
    <p>
     cpu% sys 内核态空间CPU占用率 2020年05月29日 16:00:16 Clear
    </p>
    <p>
     测试说明
     <br/>
     1、测试总结
     <br/>
     线程数(thread)一般设置为客户端CPU线程数总大小 grep 'processor' /proc/cpuinfo | sort -u | wc -l
     <br/>
     测试总数据量需要为客户端内存大小两倍
     <br/>
     测试读模型时需要清理客户端缓存信息 sync；echo 3 &gt; /proc/sys/vm/drop
     <br/>
     2、常见测试参数
     <br/>
     4M顺序写 目录深度2、单级目录数100、单个目录文件数100、单文件大小4M、IO块大小1M、顺序写
     <br/>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node21
     <br/>
     hd=hd2,system=node22
     <br/>
     hd=hd3,system=node23
     <br/>
     hd=hd4,system=node24
     <br/>
     hd=hd5,system=node25
     <br/>
     hd=hd6,system=node26
     <br/>
     fsd=fsd1,anchor=/client/test01,depth=2,width=100,files=100,size=4M,shared=yes
     <br/>
     fwd=format,threads=24,xfersize=1m
     <br/>
     fwd=default,xfersize=1m,fileio=sequential,fileselect=sequential,operation=write,threads=24
     <br/>
     fwd=fwd1,fsd=fsd1,host=hd1
     <br/>
     fwd=fwd2,fsd=fsd1,host=hd2
     <br/>
     fwd=fwd3,fsd=fsd1,host=hd3
     <br/>
     fwd=fwd4,fsd=fsd1,host=hd4
     <br/>
     fwd=fwd5,fsd=fsd1,host=hd5
     <br/>
     fwd=fwd6,fsd=fsd1,host=hd6
     <br/>
     rd=rd1,fwd=fwd*,fwdrate=max,format=restart,elapsed=600,interval=1
     <br/>
     4M顺序读 目录深度2、单级目录数100、单个目录文件数100、单文件大小4M、IO块大小1M、顺序读
     <br/>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node21
     <br/>
     hd=hd2,system=node22
     <br/>
     hd=hd3,system=node23
     <br/>
     hd=hd4,system=node24
     <br/>
     hd=hd5,system=node25
     <br/>
     hd=hd6,system=node26
     <br/>
     fsd=fsd1,anchor=/client/test02,depth=2,width=100,files=100,size=4M,shared=yes
     <br/>
     fwd=format,threads=24,xfersize=1m
     <br/>
     fwd=default,xfersize=1m,fileio=sequential,fileselect=sequential,operation=read,threads=24
     <br/>
     fwd=fwd1,fsd=fsd1,host=hd1
     <br/>
     fwd=fwd2,fsd=fsd1,host=hd2
     <br/>
     fwd=fwd3,fsd=fsd1,host=hd3
     <br/>
     fwd=fwd4,fsd=fsd1,host=hd4
     <br/>
     fwd=fwd5,fsd=fsd1,host=hd5
     <br/>
     fwd=fwd6,fsd=fsd1,host=hd6
     <br/>
     rd=rd1,fwd=fwd*,fwdrate=max,format=restart,elapsed=600,interval=1
     <br/>
     8M混合读写 目录深度2、单级目录数100、单个目录文件数100、单文件大小8M、IO块大小1M、混合读写(读写比为6:4)
     <br/>
     hd=default,vdbench=/root/vdbench50406,user=root,shell=ssh
     <br/>
     hd=hd1,system=node21
     <br/>
     hd=hd2,system=node22
     <br/>
     hd=hd3,system=node23
     <br/>
     hd=hd4,system=node24
     <br/>
     hd=hd5,system=node25
     <br/>
     hd=hd6,system=node26
     <br/>
     fsd=fsd1,anchor=/client/test03,depth=2,width=100,files=100,size=8M,shared=yes
     <br/>
     fwd=format,threads=24,xfersize=1m
     <br/>
     fwd=default,xfersize=1m,fileio=random,fileselect=random,rdpct=60,threads=24
     <br/>
     fwd=fwd1,fsd=fsd1,host=hd1
     <br/>
     fwd=fwd2,fsd=fsd1,host=hd2
     <br/>
     fwd=fwd3,fsd=fsd1,host=hd3
     <br/>
     fwd=fwd4,fsd=fsd1,host=hd4
     <br/>
     fwd=fwd5,fsd=fsd1,host=hd5
     <br/>
     fwd=fwd6,fsd=fsd1,host=hd6
     <br/>
     rd=rd1,fwd=fwd*,fwdrate=max,format=restart,elapsed=600,interval=1
     <br/>
     测试和思考
     <br/>
     我们可以自己先问自己几个问题
    </p>
    <p>
     1、如果集群里面有一台虚拟机在跑大带宽的业务，你去测试iops，性能能到多少，这个对应的是真实场景里面一个备份业务和一个数据库业务混用的情况
     <br/>
     2、单机iops能到多少，如果几十台 服务器 都同时在跑的时候，单机的iops还能到多少？
     <br/>
     3、多机并发的时候，单个机器上面的io会不会受到其他的机器的io的影响
     <br/>
     4、性能在遇到scrub的时候，或者迁移的时候，能够还保留多少的性能，这个保留性能是否可控
     <br/>
     5、集群写入到70%的时候，性能是多少，是初始的百分之多少，还够覆盖业务IO不？
    </p>
    <p>
     本文来源：码农网
     <br/>
     本文链接：https://www.codercto.com/a/49559.html
    </p>
    <p>
     https://www.e-learn.cn/topic/3737957
    </p>
    <p>
     可视化
     <br/>
     vdbench测试实时可视化显示 | https://www.codercto.com/a/49559.html
    </p>
    <p>
     摘抄和参考：
     <br/>
     【存储测试】vdbench存储性能测试工具 https://www.cnblogs.com/luxf0/p/13321077.html
    </p>
    <p>
     data_errors=(1000000000,6000)
     <br/>
     data_errors：当发生IO错误时，如果error总数超过1000000000或者运行时间超过6000s则退出
    </p>
    <p>
     参数：-jn
     <br/>
     推荐参数：-jn
    </p>
    <p>
     打开vdbench校验数据的参数为-v或-j，这个过程会为每一次写操作记录日志用于后续校验。
    </p>
    <p>
     使用-v参数，则生成的校验日志直接保存于内存中，使用-j参数则生成一个校验日志的文件，第二次校验时，-jr即可进行日志恢复进行校验。-v直接记录于内存之中，速度更快，但如果存储系统出现重启或内存清理，那么-v参数记录的校验日志就丢失了；-j直接写到磁盘上，安全有保证但速度会慢一下，此时可选择-jn，异步写到磁盘上，速度和安全都有一定的保证。 数据校验原理
    </p>
    <p>
     数据校验的工作流程如下：每一个在存储系统中的第一次写操作记录在一个表中，假定写操作的块大小是1m，那么这个块大小中的每512字节中包含的两项–8字节的逻辑字节地址(LBA)和一个字节的数据校验key值(标记是第几次写，范围为0-125，00代表创建写，01代表第一次覆盖写，以此类推，当到达126后折返00，重新来一轮)会被记录，这个过程为生成校验日志；第二次重新运行脚本(使用参数-jr或者-vr)则根据第一次记录的日志进行数据校验 vdbench 过滤多余系统日志
     <br/>
     原文链接：https://blog.csdn.net/weixin_27758233/article/details/112920905
    </p>
    <p>
     错误记录
     <br/>
     vdbench  Waiting for configuration information from slave hd1
    </p>
    <p>
     解决：重启客户端
    </p>
    <p>
     https://community.oracle.com/tech/apps-infra/discussion/3938074/waiting-for-configuration-information-from-slave-hd2-waiting-for-configuration-information-from-s
     <br/>
    </p>
   </div>
  </div>
 </article>
</div>


