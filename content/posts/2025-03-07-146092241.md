---
layout: post
title: "深度学习笔记循环神经网络RNN"
date: 2025-03-07 12:04:37 +0800
description: "本文详细介绍面试过程中可能遇到的循环神经网络RNN知识点。"
keywords: "深度学习笔记——循环神经网络RNN"
categories: ['未分类']
tags: ['笔记', '深度学习', 'Rnn']
artid: "146092241"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146092241
    alt: "深度学习笔记循环神经网络RNN"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146092241
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146092241
cover: https://bing.ee123.net/img/rand?artid=146092241
image: https://bing.ee123.net/img/rand?artid=146092241
img: https://bing.ee123.net/img/rand?artid=146092241
---

# 深度学习笔记——循环神经网络RNN

> 大家好，这里是好评笔记，公主号：Goodnote，专栏文章私信限时Free。本文详细介绍面试过程中可能遇到的循环神经网络RNN知识点。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a5e1866da2d34cce8e434dc7df4801cb.png#pic_center)

## 热门专栏

### 机器学习

**[机器学习笔记合集](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk1NzgzMjY3OQ==&action=getalbum&album_id=3785370097922080771#wechat_redirect)**

### 深度学习

**[深度学习笔记合集](https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzk1NzgzMjY3OQ==&action=getalbum&album_id=3787967363480551430&from_itemidx=1&from_msgid=2247484821#wechat_redirect)**

* * *

> * * *

## 文本特征提取的方法

### 1\. 基础方法

#### 1.1 词袋模型（Bag of Words, BOW）

**词袋模型** 最简单的方法。它将文本表示为一个**词频向量** ，**不考虑词语的顺序或上下文关系** ，只统计每个词在文本中出现的频率。

##### 工作原理

  1. **构建词汇表** ：对整个语料库中的所有词汇建立一个词汇表（也称为词典）。每个文档中的每个词都与词汇表中的一个位置对应。
  2. **生成词频向量** ：对于每个文本（文档），生成一个**与词汇表长度相同的向量** 。向量中每个元素表示该词在文档中出现的次数（或者是否出现，用二进制表示）。

##### 举例

假设有两个句子：

  * 句子 1：`猫 喜欢 鱼`
  * 句子 2：`狗 不 喜欢 鱼`

词汇表 = [“猫”, “狗”, “喜欢”, “不”, “鱼”]

  * 句子 1 的词袋向量表示为：[1, 0, 1, 0, 1]
  * 句子 2 的词袋向量表示为：[0, 1, 1, 1, 1]

##### 优点

  * 简单直观，易于实现，有效地表示词频信息。

##### 缺点

  * **忽略词序** ：词袋模型无法捕捉词语的顺序，因此在语义表达上有局限。
  * **高维稀疏** ：对于大词汇表，词袋模型会生成非常长的特征向量，大多数元素为 0，容易导致**稀疏矩阵，影响计算效率** 。
  * **受到常见词的影响** ：常见词（如 “the”、“and” 等）可能在各类文档中频繁出现，但对语义贡献较少，词袋模型会受到这些高频词的影响，降低模型的效果。

#### 1.2 TF-IDF（Term Frequency-Inverse Document Frequency）

**TF-IDF** 是对词袋模型的改进，它**为词语赋予不同的权重** ，来**衡量每个词在文档中的重要性** 。与词袋模型相比，TF-IDF
不仅考虑词频，还考虑词的普遍性，**以避免常见词（如"the"、“and”）的影响** 。

##### 工作原理

  1. **TF（词频）** ：计算某个词在文档中出现的频率。  
T F ( t , d ) = 词 t 在文档 d 中的出现次数 文档 d 的总词数
TF(t,d)=\frac{词t在文档d中的出现次数}{文档d的总词数} TF(t,d)=文档d的总词数词t在文档d中的出现次数​

  2. **IDF（逆文档频率）** ：衡量词在整个语料库中的普遍性，出现频率越低的词权重越高。  
I D F ( t ) = log ⁡ ( N 1 \+ D F ( t ) ) IDF(t)=\log\left(\frac{N}{1 +
DF(t)}\right) IDF(t)=log(1+DF(t)N​)

     * 其中 N N N是文档总数， D F ( t ) DF(t) DF(t)是包含词 t t t的文档数。
  3. **TF - IDF** ：将 T F TF TF和 I D F IDF IDF相乘，得到词在特定文档中的权重：  
T F − I D F ( t , d ) = T F ( t , d ) × I D F ( t ) TF -
IDF(t,d)=TF(t,d)\times IDF(t) TF−IDF(t,d)=TF(t,d)×IDF(t)

##### 举例

对于句子“猫 喜欢 鱼”和“狗 不 喜欢 鱼”，假设 “喜欢” 出现在所有文档中，IDF 会给它较低的权重，而像 “猫”、“狗” 这样的词会有较高的 IDF
权重，因为它们只出现在一部分文档中。

##### 优点

  * **更准确地反映词的重要性，避免了词袋模型中常见词占主导地位的情况** 。尤其适用于文本分类任务。

##### 缺点

  * **稀疏矩阵** ：虽然词频的权重经过调整，但**词汇表的大小仍然很大，容易产生稀疏矩阵问题** 。
  * **忽略词序** ：仍然**无法捕捉词语之间的顺序和上下文关系** 。

#### 1.3 TF-IDF的改进——BM25

BM25**对TF和IDF进行加权** ，同时考虑文档长度对相关性的影响，使得**对较短和较长文档的评分更加合理** 。

BM25 的计算公式如下：

B M 25 ( q , d ) = ∑ t ∈ q I D F ( t ) ⋅ T F ( t , d ) ⋅ ( k 1 \+ 1 ) T F ( t
, d ) \+ k 1 ⋅ ( 1 − b \+ b ⋅ ∣ d ∣ a v g d l ) BM25(q,d)=\sum_{t\in
q}IDF(t)\cdot\frac{TF(t,d)\cdot(k_1 + 1)}{TF(t,d)+k_1\cdot(1 - b +
b\cdot\frac{|d|}{avgdl})}
BM25(q,d)=t∈q∑​IDF(t)⋅TF(t,d)+k1​⋅(1−b+b⋅avgdl∣d∣​)TF(t,d)⋅(k1​+1)​  
其中：

  * q q q 是查询， d d d 是文档， t t t 是查询中的词。
  * I D F IDF IDF是与 T F − I D F TF - IDF TF−IDF相似的逆文档频率。
  * T F TF TF是词频。
  * k 1 k_1 k1​ 是调节词频饱和度的参数，通常取值范围为 [ 1.2 , 2.0 ] [1.2,2.0] [1.2,2.0]。
  * b b b 是调节文档长度的参数，通常取值范围为 [ 0.0 , 1.0 ] [0.0,1.0] [0.0,1.0]， b = 0.75 b = 0.75 b=0.75是一个常用的设置。
  * ∣ d ∣ |d| ∣d∣是文档的长度（词数）， a v g d l avgdl avgdl是语料库中文档的平均长度。

TF-IDF 中的 IDF（逆文档频率）使用 log ⁡ N d f ( t ) \log\frac{N}{df(t)}
logdf(t)N​来衡量词的普遍性。然而这种计算方式可能会导致在某些极端情况下（如 df(t) = 0 ）出现不合理的结果。  
BM25 对 IDF 进行了小改进，以提高在极端情况下的稳定性：

I D F ( t ) = log ⁡ N − d f ( t ) \+ 0.5 d f ( t ) \+ 0.5 IDF(t)=\log\frac{N -
df(t)+ 0.5}{df(t)+ 0.5} IDF(t)=logdf(t)+0.5N−df(t)+0.5​

这种改进的 IDF 计算在文档数量较少或者某个词的出现频率极高时，能提供更合理的 IDF 值，增加了 BM25 的稳定性。

##### 优化

相比于 TF-IDF，BM25 主要做了以下改进：

  * **非线性词频** 缩放：**通过 k 1 k_1 k1​ 控制词频TF饱和** ，避免 TF 值无限增大导致的偏差。
  * **文档长度归一化** ：使用**参数 b b b调整文档长度对评分的影响**，防止长文档得分偏高。
  * **改进的 IDF 计算** ：**使用平滑** 后的 IDF 计算，保证在**极端情况下的稳定性** 。
  * **查询词频考虑** ：在评分中更合理地衡量查询中词频的影响，提高了对复杂查询的检索效果。

#### 1.4 N-Gram 模型

**N-Gram 模型** 是一种**基于词袋模型** 的**扩展** 方法，它通过**将词组作为特征，来捕捉词语的顺序信息** 。

##### 工作原理

  * **N-Gram** 是指在文本中**提取连续的 n 个词组成的词组作为特征** 。当 n=1 时，即为 unigram（单词级别特征）；当 n=2 时，即为 bigram（双词组特征）；当 n=3 时，即为 trigram（词三元组特征）。

  * 在提取 N-Gram 时，模型不仅关注单个词，还捕捉到词与词之间的顺序和依赖关系。例如，2-Gram 模型会将句子分解为相邻的两词组合。

##### 举例

对于句子“猫 喜欢 吃 鱼”，2-Gram 模型会提取出以下特征：

  * [“猫 喜欢”, “喜欢 吃”, “吃 鱼”]

##### 优点

  * 能**捕捉到顺序和依赖关系** ，比单词级别的特征表达更丰富。n 越大，模型捕捉的上下文信息越多。

##### 缺点

  * **维度膨胀** ：n 值越大，特征向量的维度会急剧增加，容易导致稀疏矩阵和计算复杂度升高。
  * 对长文本，N-Gram 模型可能会生成非常多的组合，**计算资源消耗较大** 。

### 2\. 词向量（Word Embeddings）

**词向量** 是现代 NLP 中的关键特征提取方法，能够捕捉词语的语义信息。常见的词向量方法包括 Word2Vec、GloVe、和
FastText。词向量的核心思想是**将每个词表示为一个低维的、密集的向量，词向量之间的相似性能够反映词语的语义相似性** 。

#### 2.1 Word2Vec

**Word2Vec** 是一种**使用浅层神经网络学习词向量的模型** ，由 Google 在 2013 年提出。它有两种模型架构：CBOW 和
Skip-gram。

##### 工作原理

  * **CBOW（Continuous Bag of Words）** ：**根据上下文中的词语来预测中心词** 。模型输入是上下文词语，输出是预测的中心词。
  * **Skip-gram** ：与 CBOW 相反，它是**根据中心词来预测上下文中的词语** 。

##### 举例

对于一个句子 “猫 喜欢 吃 鱼”，CBOW 会使用上下文 [“猫”, “吃”, “鱼”] 来预测 “喜欢”，而 Skip-gram 则会使用 “喜欢”
来预测上下文。

##### 优点

  * **语义相似性** ：Word2Vec 生成的词向量**能够捕捉词语之间的语义相似性** 。例如，“king” 和 “queen” 的词向量会非常相近。
  * **稠密向量** ：与词袋模型和 TF-IDF 生成的高维稀疏向量不同，Word2Vec 生成的词向量是**低维的密集向量** （如 100 维或 300 维），**更加高效** 。

##### 缺点

  * **无法处理 OOV（未登录词）** ：如果测试集中出现了训练集中未见过的词，Word2Vec 无法为其生成词向量。
  * **上下文无关** ：Word2Vec 生成的**词向量是固定的，无法根据上下文变化来调整词向量** 。

#### 2.2 FastText

**FastText** 是 Facebook 提出的词向量方法，它是 Word2Vec 的改进版。FastText 通过**将词分解为n-gram**
字符级别的子词，捕捉**词的形态信息** 。

##### 工作原理

  * FastText 将词分解为多个字符 n-gram，然后对每个 n-gram 生成词向量。通过这种方式，FastText 可以**捕捉到词语内部的形态信息** ，尤其对拼写错误或**未登录词** 有较好的处理能力。

##### 优点

  * **处理 OOV（未登录词）** ：因为 FastText 基于子词生成词向量，它**能够为未见过的词生成向量表示** 。
  * **考虑词形信息** ：能够捕捉词的形态变化，例如词根、前缀、后缀等。

##### 缺点

  * **计算复杂度较高** ：相比 Word2Vec，FastText 需要对每个词生成多个 n-gram，因此计算量更大。

### 3\. 预训练模型：BERT（Bidirectional Encoder Representations from Transformers）

**BERT** 是一种基于 Transformer 架构的预训练语言模型，由 Google 于 2018 年提出。与传统的词向量方法不同，BERT
通过双向的 Transformer 网络，能够生成上下文相关的动态词向量。

##### 工作原理

  * **双向Transformer** ：BERT 同时从词语的前后上下文学习词的表示，而不像传统的模型只从前向或后向学习。这样，BERT 能够捕捉到更丰富的语义信息。
  * **预训练任务** ： 
    1. **遮蔽语言模型（Masked Language Model, MLM）** ：在训练时，BERT 会随机遮蔽部分词语，并要求模型预测这些词，从而让模型学到上下文的双向依赖关系。
    2. **下一句预测（Next Sentence Prediction, NSP）** ：训练时，BERT 要预测两句话是否是连续的句子对，这让模型能够学习句子级别的关系。

##### 优点

  * **上下文相关词向量** ：BERT 生成的词向量是上下文相关的。例如，“bank” 在句子 “I went to the bank” 和 “The river bank” 中会有不同的向量表示。
  * **强大的语义理解能力** ：BERT 在问答、阅读理解、文本分类等任务中表现非常好，能够捕捉到复杂的语义关系。

##### 缺点

  * **计算资源需求大** ：BERT 是一个深层的 Transformer 模型，预训练和微调都需要大量的计算资源，训练时间较长。
  * **较慢的推理速度** ：由于模型较大，在实际应用中推理速度较慢，尤其在实时任务中。

> BERT详细参考历史/后续文章：[深度学习笔记——GPT、BERT、T5]

### 总结

**方法**| **工作原理**| **优点**| **缺点**| **适用场景**  
---|---|---|---|---  
**词袋模型（BOW）**|  将文本表示为**词频向量** ，**不考虑词序和上下文** 。| 简单直观，易实现，能够有效表示词频信息。|
忽略词序，生成高维稀疏向量。| 文本分类、信息检索  
**TF-IDF**| **基于词袋模型** ，**考虑词** 在文档中的频率以及整个语料库中的**普遍性** ，赋予不同词**权重** 。|
反映词的重要性，避免常见词主导影响，适用于文本分类。| 生成稀疏矩阵，**无法捕捉词序和上下文关系** 。| 文本分类、关键词提取  
**BM25**|  基于 TF-IDF 的改进，考虑词频、文档长度、词重要性等因素，以计算每个词对文档匹配的相关性得分。 **非线性词频** 缩放、
**文档长度归一化** 、 **改进的 IDF 计算**|  更好地反映词在文档中的相关性，更适合信息检索，适用于长文档，计算匹配更准确。|
对参数敏感，适用性依赖于超参数调优，不能捕捉上下文关系。| 信息检索、文档排名  
**N-Gram**|  捕捉连续 **n 个词作为特征** ，**考虑词序信息** 。| 能捕捉词语的顺序和依赖关系，n 越大捕捉的上下文信息越多。|
维度膨胀，计算资源消耗大。| 语言模型、短文本分类  
**Word2Vec**|  使用**浅层神经网** 络学习词向量，有 **CBOW** 和 **Skip-gram** 两种架构。|
词向量能**捕捉语义相似性** ，生成低维稠密向量，效率高。| **无法处理未登录词** （OOV），词向量上下文无关。| 词嵌入、相似度计算、文本分类  
**FastText**|  将**词分解为字符 n-gram** ，生成词向量，捕捉词的形态信息。| **能处理未登录词**
，捕捉词形信息，适合拼写错误和变形词。| 计算复杂度高于 Word2Vec。| 词嵌入、拼写纠错、文本分类  
**BERT**|  基于双向 Transformer，通过预训练生成上下文相关的词向量，支持 **Masked Language Model 和 Next
Sentence Prediction** 。| 生成上下文相关词向量，语义理解强，适用于复杂 NLP 任务。| 需要大量计算资源，训练和推理时间长。|
问答系统、文本分类、阅读理解  
  
  * **传统方法** ：如词袋模型、TF-IDF 和 N-Gram 易于实现，但无法捕捉语义和上下文信息。
  * **词向量方法** ：如 Word2Vec 和 FastText 通过词嵌入表示词语的语义关系，适合语义相似度计算、文本分类等任务。FastText 能够处理未登录词。
  * **预训练模型** ：如 BERT，能够生成上下文相关的动态词向量，适用于更复杂的自然语言处理任务，但对计算资源的要求更高。

* * *

## RNN

循环神经网络（RNN，Recurrent Neural Network）是一种用于**处理序列数据等具有顺序关系的数据**
的神经网络。与传统的前馈神经网络不同，RNN **具有循环连接** ，允许**信息通过隐藏状态在序列的不同时间步之间传播**
。这种结构使得RNN非常适合处理时间序列、文本数据、语音信号等**具有顺序关系的数据** 。

### RNN 参数

参数| 维度| 作用  
---|---|---  
输入权重矩阵 W x h W_{x h} Wxh​|  ( d h i d d e n × d i n p u t ) (d_{hidden}\times
d_{input}) (dhidden​×dinput​)| 将输入 x t x_t xt​映射到隐藏状态，确定当前输入对隐藏层状态的影响。  
隐藏状态权重矩阵 W h h W_{h h} Whh​|  ( d h i d d e n × d h i d d e n )
(d_{hidden}\times d_{hidden}) (dhidden​×dhidden​)| 将前一时间步的隐藏状态 h t − 1 h_{t -
1} ht−1​传递到当前时间步 h t h_t ht​，捕捉时间依赖关系。  
输出权重矩阵 W h y W_{h y} Why​|  ( d o u t p u t × d h i d d e n )
(d_{output}\times d_{hidden}) (doutput​×dhidden​)| 将隐藏状态 h t h_t ht​映射为输出 y t
y_t yt​。  
隐藏层偏置向量 b h b_{h} bh​|  ( d h i d d e n ) (d_{hidden}) (dhidden​)|
增强隐藏层的灵活性，通过加偏置调整隐藏层的激活函数输出。  
输出层偏置向量 b y b_{y} by​|  ( d o u t p u t ) (d_{output}) (doutput​)|
增强输出层的灵活性，通过加偏置调整输出层的结果。  
激活函数| —| 用于隐藏层状态的非线性变换，常用 t a n h tanh tanh或 R e L U ReLU ReLU。  
  
其他相关参数：

  * **时间步（Time Steps）** ：决定模型的循环次数，非可学习参数。
  * **损失函数（Loss Function）** ：指导模型参数更新的依据，非可学习参数。
  * **学习率（Learning Rate）** ：控制优化过程的步幅大小，超参数。

### RNN 的特点

  1. **顺序处理** ：RNN 可以**处理不同长度的输入序列** ，这是由于其内部结构允许将前一步的信息作为当前步的输入之一。
  2. **隐藏状态** ：RNN 具有隐藏状态，隐藏状态是前一个时间步的信息的压缩，并与当前输入一起决定下一时间步的输出。
  3. **权重共享** ：RNN 中**每个时间步之间共享相同的网络权重( W h h W_{hh} Whh​,  W x h W_{xh} Wxh​,  W h y W_{hy} Why​)**，减少了模型参数的数量，适合处理序列长度不同的问题。

### RNN 的局限性

  1. **梯度消失与爆炸问题** ：在长序列处理中，由于反向传播算法在计算梯度时，RNN 容易出现梯度消失（gradient vanishing）或梯度爆炸（gradient exploding）的现象，导致模型难以学习长期依赖关系。
  2. **长时间依赖问题** ：RNN 处理长序列时，无法有效捕捉到前后相距较远的依赖关系，导致模型的性能下降。
  3. **并行化困难** ：由于 RNN 是逐时间步处理序列的，因此不容易并行化处理，这使得其训练时间较长。

### 前向传播的核心计算

在 RNN 中，当前时间步的输出不仅依赖于**当前输入** ，还依赖于之前**时间步的隐状态** （hidden state）。隐状态是 RNN
中的一个内部存储器，它能够保存之前的时间步的信息，使得网络具备记忆能力。RNN 的计算公式如下：

#### 隐状态更新

h t = f ( W h h h t − 1 \+ W x h x t \+ b h ) h_t=f(W_{h h}h_{t - 1}+W_{x
h}x_t + b_{h}) ht​=f(Whh​ht−1​+Wxh​xt​+bh​)

  * h t h_t ht​：时间步 t t t的隐藏状态，是通过上一时间步 t − 1 t - 1 t−1的隐藏状态 h t − 1 h_{t - 1} ht−1​和当前的输入 x t x_t xt​计算得到的。
  * W h h W_{h h} Whh​：隐藏状态到隐藏状态的权重矩阵，用于表示时间步之间的状态传递。
  * W x h W_{x h} Wxh​：输入到隐藏状态的权重矩阵，负责将当前输入 x t x_t xt​映射到隐藏层。
  * b h b_{h} bh​：隐藏层的偏置向量。
  * f f f：激活函数，通常使用 t a n h tanh tanh或 R e L U ReLU ReLU。

#### 输出更新

y t = g ( W h y h t \+ b y ) y_t = g(W_{hy}h_t + b_y) yt​=g(Why​ht​+by​)

  * y t y_t yt​：时间步 t t t的输出。
  * W h y W_{hy} Why​：隐藏状态到输出的权重矩阵。
  * b y b_y by​：输出层的偏置。
  * g g g：输出层的激活函数，取决于具体的任务，如分类任务常用Softmax。

### RNN 的训练流程

RNN 的训练主要包括以下步骤：

#### 1\. 输入准备

  * 输入数据：RNN处理的是序列数据，输入可以是时间序列、文本、语音等。输入通常表示为 X = [ x 1 , x 2 , … , x T ] X = [x_1, x_2, \ldots, x_T] X=[x1​,x2​,…,xT​]，其中 x t x_t xt​代表时间步 t t t时的输入。
  * 标签数据（监督学习）：如果是监督学习任务，训练数据通常带有标签 Y = [ y 1 , y 2 , … , y T ] Y = [y_1, y_2, \ldots, y_T] Y=[y1​,y2​,…,yT​]，表示每个时间步 t t t的目标输出。

#### 2\. 前向传播（Forward Pass）

逐个时间步执行前向传播，将**输入数据逐步传递到隐藏层，计算每个时间步的隐藏状态和输出** （上面的核心计算）。

  * **初始化隐藏状态** ：在时间步 t = 0 时，隐藏状态  h 0 h_0 h0​通常**初始化为 0 或随机值** 。
  * **逐时间步的状态更新** ：对于每一个时间步 t ，计算当前时间步的隐藏状态和输出：

(1). **隐藏状态更新：**  
h t = f ( W h h h t − 1 \+ W x h x t \+ b h ) h_t = f(W_{hh}h_{t -
1}+W_{xh}x_t + b_h) ht​=f(Whh​ht−1​+Wxh​xt​+bh​)

  * h t h_t ht​：时间步 t t t的隐藏状态，是通过上一时间步 t − 1 t - 1 t−1的隐藏状态 h t − 1 h_{t - 1} ht−1​和当前的输入 x t x_t xt​计算得到的。
  * W h h W_{hh} Whh​：隐藏状态到隐藏状态的权重矩阵，用于表示时间步之间的状态传递。
  * W x h W_{xh} Wxh​：输入到隐藏状态的权重矩阵，负责将当前输入 x t x_t xt​映射到隐藏层。
  * b h b_h bh​：隐藏层的偏置向量。
  * f f f：激活函数，通常使用tanh或ReLU。

(2). **输出更新：**  
y t = g ( W h y h t \+ b y ) y_t = g(W_{hy}h_t + b_y) yt​=g(Why​ht​+by​)

  * y t y_t yt​：时间步 t t t的输出。
  * W h y W_{hy} Why​：隐藏状态到输出的权重矩阵。
  * b y b_y by​：输出层的偏置。
  * g g g：输出层的激活函数，取决于具体的任务，如分类任务常用Softmax。

#### 3\. 计算损失（Loss Calculation）

  * **损失函数** ：RNN 根据每个时间步的预测输出 y t y_t yt​ 和真实标签 y ^ t \hat{y}_t y^​t​计算损失。常见的损失函数包括： 
    * 分类任务使用 **交叉熵损失** 。
    * 回归任务使用 **均方误差（MSE）** 。

总的损失是各个时间步损失的累加：  
L = ∑ t = 1 T L o s s ( y t , y ^ t ) L=\sum_{t = 1}^{T}L_{oss}(y_t,
\hat{y}_t) L=t=1∑T​Loss​(yt​,y^​t​)

#### 4\. 反向传播（Backward Pass）

RNN 的反向传播主要通过**反向传播通过时间（Backpropagation Through Time, BPTT）**来更新权重。BPTT
是对标准反向传播算法的扩展，沿着时间轴进行梯度传递。

**反向传播的核心步骤** ：

  * 从时间步 T T T开始，逐步向前计算各个时间步的梯度，直至第一个时间步。
  * 在每个时间步，计算损失相对于输出 y t y_t yt​、隐藏状态 h t h_t ht​以及参数（权重和偏置）的梯度。 
    * 对损失函数 L L L进行求导，得到每个时间步的梯度，并依次更新参数：  
W x h ← W x h − α ∂ L ∂ W x h W_{xh}\leftarrow W_{xh}-\alpha\frac{\partial
L}{\partial W_{xh}} Wxh​←Wxh​−α∂Wxh​∂L​  
W h h ← W h h − α ∂ L ∂ W h h W_{hh}\leftarrow W_{hh}-\alpha\frac{\partial
L}{\partial W_{hh}} Whh​←Whh​−α∂Whh​∂L​  
W h y ← W h y − α ∂ L ∂ W h y W_{hy}\leftarrow W_{hy}-\alpha\frac{\partial
L}{\partial W_{hy}} Why​←Why​−α∂Why​∂L​

    * 其中 α \alpha α是学习率。
  * **梯度消失问题** ：BPTT 在长时间序列上可能出现梯度消失或爆炸问题，尤其是当梯度逐步传递时，这限制了 RNN 捕捉长时依赖的能力。

#### 5\. 参数更新

  * 根据反向传播得到的梯度更新网络中的权重参数,如  W x h W_{xh} Wxh​,  W h h W_{hh} Whh​,  W h y W_{hy} Why​，完成当前批次的训练。
  * 继续执行下一个批次的训练，直至完成所有训练数据的迭代。

### RNN 的推理流程

RNN 的**推理** （inference）过程**与训练过程中的前向传播类似** ，但**没有反向传播和参数更新** ，主要是用于生成输出或进行预测。

#### 1\. 输入准备

  * 输入序列数据  X = [ x 1 , x 2 , . . . , x T ] X = [x_1, x_2, ..., x_T] X=[x1​,x2​,...,xT​]。
  * 不需要提供标签数据，因为推理阶段是无监督的，RNN 根据输入数据生成输出。

#### 2\. 前向传播

推理阶段执行与训练相同的前向传播过程：

  1. **初始化隐藏状态** ： 
     * 与训练时一样，隐藏状态  h 0 h_0 h0​初始化为 0 或随机值。
  2. **逐时间步的状态更新** ：

  * 对每个时间步 t t t执行前向传播，计算隐藏状态 h t h_t ht​和输出 y t y_t yt​：  
h t = f ( W x h x t \+ W h h h t − 1 \+ b h ) h_t = f(W_{xh}x_t + W_{hh}h_{t -
1}+b_h) ht​=f(Wxh​xt​+Whh​ht−1​+bh​)  
y t = g ( W h y h t \+ b y ) y_t = g(W_{hy}h_t + b_y) yt​=g(Why​ht​+by​)

#### 3\. 生成输出

  * 推理过程中，RNN 根据输入数据在每个时间步生成相应的输出 y t y_t yt​。
  * 在序列生成任务（如文本生成）中，RNN 的输出  y t y_t yt​ 可以作为下一个时间步的输入  x t \+ 1 x_{t+1} xt+1​，从而生成一个完整的输出序列。

#### 4\. 推理结束

  * 推理结束时，RNN 已经生成了完整的输出序列或预测结果。

### RNN参数初始化

RNN 的参数初始化策略会影响训练过程的稳定性和收敛速度，以下是不同参数的初始化方法概述：

**参数**| **初始化方法**| **适用场景**  
---|---|---  
**输入权重 ( W x h W_{xh} Wxh​ )**| Xavier 初始化、He 初始化、标准正态分布| 用于**将输入映射到隐藏状态**
，适合不同激活函数场景  
**隐藏状态权重 ( W h h W_{hh} Whh​ )**| 正交初始化、Xavier 初始化、He 初始化| **隐藏状态循环权重**
，正交初始化适合处理梯度问题  
**输出权重 ( W h y W_{hy} Why​ )**| Xavier 初始化、He 初始化| 用于**将隐藏状态映射到输出层** ，取决于任务类型  
**偏置项 ( b h , b y b_h, b_y bh​,by​ )**| 零初始化、小随机数、遗忘门的偏置可初始化为较大正值（LSTM/GRU）|
**偏置项通常为零初始化** ，特殊情况下设定固定值  
  
通过使用**合适的初始化方法** ，可以显著提高 RNN 的收敛性和模型的训练效果，尤其在处理长序列时，**正交初始化** 和 **Xavier 初始化**
能帮助缓解梯度消失和梯度爆炸问题。

## 全连接层在各神经网络模型中的作用

全连接层（Fully Connected Layer, FC Layer）广泛应用于分类任务或回归任务的**最后阶段**
。全连接层的主要作用是**将上层提取的特征转换为具体的决策结果或输出**
。它在不同类型的神经网络模型中具有不同的作用。以下是全连接层在各类神经网络模型中的作用详细解释：

  1. 在**MLP** 中，全连接层是主要计算单元，**完成输入到输出的映射** 。
  2. 在**CNN** 中，全连接层主要用于**整合局部特征并生成分类结果** 。
  3. 在**RNN** 和**LSTM** 中，全连接层**将时间序列特征映射为输出结果** 。
  4. 在**Transformer** 模型中，全连接层**参与注意力机制和输出映射** 。
  5. 在**GAN** 中，全连接层**用于潜在空间和图像特征之间的映射** 。
  6. 在**自编码器** 中，全连接层**用于特征压缩和重构** 。
  7. 在**注意力机制** 中，全连接层用于**计算注意力得分并变换上下文向量** 。

无论在哪种神经网络中，全连接层的核心作用都是将前一层的特征进一步映射到目标空间，形成最后的输出或决策。



