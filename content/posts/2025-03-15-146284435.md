---
layout: post
title: "神经网络的基本知识"
date: 2025-03-15 22:50:06 +0800
description: "它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。：全连接层将前一层的所有输出与当前层的每个神经元连接，能够整合前一层的局部或全局特征，生成新的特征表示。Softmax 通过指数运算放大高分值的类别，抑制低分值的类别，使得高分值的类别概率更接近 1，低分值的类别概率更接近 0。RNN不同于前向神经网络，它的层内、层与层之间的信息可以双向传递，更高效地存储信息，通常用于处理信息序列的任务。"
keywords: "神经网络的基本知识"
categories: ['超级瓦利加加加']
tags: ['算法', '神经网络', '人工智能']
artid: "146284435"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146284435
    alt: "神经网络的基本知识"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146284435
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146284435
cover: https://bing.ee123.net/img/rand?artid=146284435
image: https://bing.ee123.net/img/rand?artid=146284435
img: https://bing.ee123.net/img/rand?artid=146284435
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     神经网络的基本知识
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     感知机
    </p>
    <p>
     <img alt="" height="343" src="https://i-blog.csdnimg.cn/direct/a7fea28b09bd41d49def6b6b796991cd.png" width="563"/>
    </p>
    <blockquote>
     <ul>
      <li>
       <p>
        输入：来自其他 n 个神经元传递过来的输入信号
       </p>
      </li>
      <li>
       <p>
        处理：输入信号通过带权重的连接进行传递, 神经元接受到总输入值将与神经元的阈值进行比较
       </p>
      </li>
      <li>
       <p>
        输出：通过激活函数的处理以得到输出
       </p>
      </li>
     </ul>
    </blockquote>
    <blockquote>
     <p>
      感知机由两层神经元组成, 输入层接受外界输入信号传递给输出层, 输出层是M-P神经元（阈值逻辑单元）
     </p>
    </blockquote>
    <p>
     <img alt="" height="160" src="https://i-blog.csdnimg.cn/direct/2bbbea5f8c354e63bce2d80b47192c88.png" width="691"/>
    </p>
    <p>
     <img alt="" height="290" src="https://i-blog.csdnimg.cn/direct/2941e2e9ed884cb18bf0c3a9d678a20d.png" width="401">
     </img>
    </p>
    <p>
     <img alt="" height="362" src="https://i-blog.csdnimg.cn/direct/d431349504be4f45bdbea9f93c3dbe0e.png" width="1150"/>
    </p>
    <p>
     <img alt="" height="389" src="https://i-blog.csdnimg.cn/direct/4e24c4197dd24bc694fed11d109d04fc.png" width="920"/>
    </p>
    <blockquote>
     <p>
      若感知机对训练样例 (x,y) 预测正确，则感知机不发生变化；否则根据错误程度进行权重的调整。
     </p>
    </blockquote>
    <blockquote>
     <p>
      若两类模式线性可分, 则感知机的学习过程一定会收敛；否则感知机的学习过程将会发生震荡，单层感知机的学习能力非常有限, 只能解决线性可分问题。
     </p>
    </blockquote>
    <hr/>
    <p>
     多层感知机
    </p>
    <p>
     <img alt="" height="497" src="https://i-blog.csdnimg.cn/direct/21983f517a6d4d22beb07b0af7be5210.png" width="1078"/>
    </p>
    <blockquote>
     <p>
      输出层与输入层之间的一层神经元, 被称之为隐层或隐含层, 隐含层和输出层神经元都是具有激活函数的功能神经元。
     </p>
    </blockquote>
    <p>
     感知机-多层前馈神经网络
    </p>
    <p>
     <img alt="" height="558" src="https://i-blog.csdnimg.cn/direct/f437de33f0e141e1b61229eca5cc2818.png" width="1051"/>
    </p>
    <hr/>
    <p>
     误差逆传播算法（Error BackPropagation, 简称BP）
    </p>
    <p>
     <img alt="" height="104" src="https://i-blog.csdnimg.cn/direct/2ace7cd5c7bb467d9b7f5a5b09cef8c0.png" width="783"/>
    </p>
    <p>
     <img alt="" height="255" src="https://i-blog.csdnimg.cn/direct/163d64ca24964e8fa78d1a2498edb553.png" width="781"/>
    </p>
    <p>
     <img alt="" height="308" src="https://i-blog.csdnimg.cn/direct/c9572834afb5481a96b95768b5144d0d.png" width="477"/>
    </p>
    <p>
     <img alt="" height="578" src="https://i-blog.csdnimg.cn/direct/bb21375dfeec4126952bdb7c5162df2f.png" width="1020"/>
    </p>
    <blockquote>
     <p>
      参数个数：d个输入神经元连接q个隐层神经元 d*q，q个隐层神经元连接l个输出神经元 q*l，再加 q+ l个阈值，最终结果是（d+l+q）*q+l。
     </p>
    </blockquote>
    <p>
     <img alt="" height="459" src="https://i-blog.csdnimg.cn/direct/a73cdf3976da432b9588cfe47efe940a.png" width="1049"/>
    </p>
    <p>
     <img alt="" height="471" src="https://i-blog.csdnimg.cn/direct/35af3348d4c5430087ee38d58f8d7f9f.png" width="1037"/>
    </p>
    <p>
     工作流程：
    </p>
    <blockquote>
     <p>
      （1）将输入示例提供给输入层神经元，逐层将信号前传，直到产生输出结果
     </p>
     <p>
      （2）计算输出层与真实值的误差，将误差使用BP算法传播到整个网络，对连接权重及阈值进行调整。
     </p>
     <p>
      （3） 该迭代过程循环进行，直到达到某些停止条件为止。（例如训练误差达到了很小的值，或者整个数据集运行了20轮）
     </p>
    </blockquote>
    <p>
     <img alt="" height="473" src="https://i-blog.csdnimg.cn/direct/ed2ed6376c974f3d9789cbf5c16fc9c6.png" width="1078"/>
    </p>
    <p>
     多层前馈网络表示能力 ：只需要一个包含足够多神经元的隐层, 多层前馈神经网络就能以任意精度逼近任意复杂度的连续函数 。
    </p>
    <p>
     多层前馈网络局限 ：神经网络由于强大的表示能力, 经常遭遇过拟合。表现为：训练误差持续降低, 但测试误差却可能上升 。如何设置隐层神经元的个数仍然是个未决问题.。
    </p>
    <p>
     实际应用中通常使用“试错法”调整 缓解过拟合的策略 。
    </p>
    <p>
     早停：在训练过程中, 若训练误差降低, 但验证误差升高, 则停止训练 。
    </p>
    <p>
     正则化：在误差目标函数中增加一项描述网络复杂程度的部分, 例如连接权值与阈值的平方和。
    </p>
    <hr/>
    <p>
     卷积神经网络
    </p>
    <p>
     <img alt="" height="549" src="https://i-blog.csdnimg.cn/direct/43607959ee224e10b0013f50c0e89aee.png" width="1103"/>
    </p>
    <blockquote>
     <p>
      卷积：平移不变模式，提取局部特征。
     </p>
     <p>
      池化：对图像进行缩放。
     </p>
    </blockquote>
    <p>
     <img alt="" height="508" src="https://i-blog.csdnimg.cn/direct/d085afb4c8c14ae680747fe092267b91.png" width="1096"/>
    </p>
    <blockquote>
     <p>
      卷积网络一般框架：卷积层+激活函数+池化层+全连接层。
     </p>
     <p>
      卷积+激活+池化：出现多次，用于提取特征。
     </p>
     <p>
      全连接层：最后一次出现，用于分类。
     </p>
    </blockquote>
    <p>
     <img alt="" height="480" src="https://i-blog.csdnimg.cn/direct/13d1940b5ba24a299f41fb3b7fa0af43.png" width="937"/>
    </p>
    <p>
     <img alt="" height="473" src="https://i-blog.csdnimg.cn/direct/f05acaba9f0c44089739e911a5553999.png" width="632"/>
    </p>
    <p>
     <strong>
      输入层
     </strong>
    </p>
    <p>
     <strong>
      输入层：
     </strong>
     输入层是对数据进行预处理的阶段，将输入的数据（图像/文字）转换成网络能够计算的数字。
    </p>
    <p>
     <img alt="" height="347" src="https://i-blog.csdnimg.cn/direct/45e25d71c81e4cada69a7ba1a71d381c.png" width="1055"/>
    </p>
    <p>
     <strong>
      卷积层
     </strong>
    </p>
    <p>
     <strong>
      卷积层（Convolutional layer）
     </strong>
     ，卷积神经网络中每层卷积层由若干卷积单元(卷积核)组成，每个卷积单元的参数都是通过反向传播算法优化得到的。
    </p>
    <p>
     <strong>
      卷积运算的目的是提取输入的不同特征
     </strong>
     ，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。
    </p>
    <blockquote>
     <ul>
      <li>
       卷积核（filter/kernel）：用于对输入图像进行共享权值的遍历
      </li>
      <li>
       步长（stride）：卷积核在图片上移动的大小
      </li>
      <li>
       填充（padding）：满足输出的图像的维度要求
      </li>
     </ul>
    </blockquote>
    <p>
     <img alt="" height="419" src="https://i-blog.csdnimg.cn/direct/8034e28a299b41c4b88e5031aff97328.png" width="601"/>
    </p>
    <p>
     <img alt="" height="566" src="https://i-blog.csdnimg.cn/direct/80e8026707714c5692f96f09bcf1809f.png" width="1122"/>
    </p>
    <p>
     <img alt="" height="548" src="https://i-blog.csdnimg.cn/direct/1c075cf9ddc044fdbd6e85fa37af07a2.png" width="1136"/>
    </p>
    <p>
     <img alt="" height="527" src="https://i-blog.csdnimg.cn/direct/4ca95669382f43d7a58641e6f3c60008.png" width="1157"/>
    </p>
    <p>
     <img alt="" height="537" src="https://i-blog.csdnimg.cn/direct/a5087d22abd24c99901494081d9c5f6f.png" width="1123"/>
    </p>
    <p>
     激活层
    </p>
    <blockquote>
     <p>
      往模型中加入非线性元素，可以更好地解决复杂的问题。
     </p>
    </blockquote>
    <p>
     <img alt="" height="548" src="https://i-blog.csdnimg.cn/direct/ff7d05833e204493988d6b0b05c13428.png" width="1119"/>
    </p>
    <p>
     池化层
    </p>
    <blockquote>
     <p>
      <strong>
       池化层的主要的作用是压缩数据和参数的量（保持最显著的特征）
      </strong>
      ，通过去掉上一层的输出中不重要的信息，进一步减少参数数量。Pooling的方法很多，常用方法有最大池化与均值池化。
     </p>
    </blockquote>
    <p>
     <img alt="" height="547" src="https://i-blog.csdnimg.cn/direct/2fb2d51e1de14844a3d67eec32aa2914.png" width="1075"/>
    </p>
    <p>
     全连接层（Fully Connected Layer，简称FC层）
    </p>
    <p>
     <img alt="" height="402" src="https://i-blog.csdnimg.cn/direct/45a97d7cf313437bb23a898489929033.png" width="640"/>
    </p>
    <blockquote>
     <ul>
      <li>
       <span style="color:#494949">
        <strong>
         特征整合
        </strong>
        ：全连接层将前一层的所有输出与当前层的每个神经元连接，能够整合前一层的局部或全局特征，生成新的特征表示。【将多层的特征映射成一个一维的向量】
       </span>
      </li>
      <li>
       <span style="color:#494949">
        <strong>
         非线性变换
        </strong>
        ：通过激活函数（如ReLU、Sigmoid等），全连接层引入非线性，增强模型的表达能力，使其能够拟合更复杂的函数。
       </span>
      </li>
      <li>
       <span style="color:#494949">
        <strong>
         输出转换
        </strong>
        ：在分类任务中，全连接层通常作为最后的输出层，将高维特征映射到类别空间。【对卷积层获得的不同的特征进行加权，最终目的是得到一个可以对不同类别进行区分的得分】【输出层就是获得对应每个类别的得分】
       </span>
      </li>
      <li>
       <span style="color:#494949">
        <strong>
         参数学习
        </strong>
        ：全连接层通过大量可训练参数（权重和偏置）学习数据的内在规律，提升模型的性能。
       </span>
      </li>
     </ul>
    </blockquote>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         输入
        </strong>
        ：前一层的所有输出。
       </p>
      </li>
      <li>
       <p>
        <strong>
         输出
        </strong>
        ：每个神经元的加权和经过激活函数后的结果。
       </p>
      </li>
      <li>
       <p>
        <strong>
         参数
        </strong>
        ：权重矩阵 WW和偏置向量 b，通过反向传播优化。
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <img alt="" height="130" src="https://i-blog.csdnimg.cn/direct/433b27b97cd54913b4d09db7f2c45a45.png" width="850"/>
    </p>
    <blockquote>
     <p>
      <strong>
       卷积网络在本质上是一种输入到输出的映射
      </strong>
      ，它能够学习大量的输入与输出之间的映射关系，而不需要任何输入和输出之间的精确的数学表达式，只要用已知的模式对卷积网络加以训练，网络就具有输入输出对之间的映射能力。卷积神经网络的训练过程与传统神经网络类似，也是参照了反向传播算法。
     </p>
    </blockquote>
    <p>
     <img alt="" height="367" src="https://i-blog.csdnimg.cn/direct/2c388628d6484013a6a73c224967c9db.png" width="747"/>
    </p>
    <hr/>
    <p>
    </p>
    <hr/>
    <p>
     循环神经网络
    </p>
    <blockquote>
     <p>
      循环神经网络是一种对序列数据建模的神经网络。
     </p>
     <p>
      RNN不同于前向神经网络，它的层内、层与层之间的信息可以双向传递，更高效地存储信息，通常用于处理信息序列的任务。
     </p>
    </blockquote>
    <p>
     RNN主要用来处理序列数据，在传统的神经网络模型中，每层内的节点之间无连接，但是
     <strong>
      循环神经网络中当前神经元的输出与前面的输出也有关
     </strong>
     ，网络会对前面的信息进行记忆并用于当前神经元的计算中。
    </p>
    <p>
     <img alt="" height="281" src="https://i-blog.csdnimg.cn/direct/09f9bb7eb1c44fe293bab13a9054de44.png" width="1116"/>
    </p>
    <p>
     <img alt="" height="525" src="https://i-blog.csdnimg.cn/direct/5239500c8c4847fca354716c63f6ae7d.png" width="1109"/>
    </p>
    <p>
     <img alt="" height="619" src="https://i-blog.csdnimg.cn/direct/ded1ea23da624be88eb9f417cfb617ca.png" width="1219"/>
    </p>
    <p>
    </p>
    <p>
     <img alt="" height="622" src="https://i-blog.csdnimg.cn/direct/3baed6e2372e440f97b6bc81fd8cb50a.png" width="1242"/>
    </p>
    <p>
     ...
    </p>
    <hr/>
    <p>
     LSTM
    </p>
    <p>
     <img alt="" height="405" src="https://i-blog.csdnimg.cn/direct/f1a1bf920359453f9a364d83ee6eb6fb.png" width="1223"/>
    </p>
    <p>
     <img alt="" height="309" src="https://i-blog.csdnimg.cn/direct/3539bac72a2e4d1cbb99da56d8338fdd.png" width="1185"/>
    </p>
    <p>
     <img alt="" height="523" src="https://i-blog.csdnimg.cn/direct/9a8d0cb32da04a95912064a520e8efae.png" width="1190"/>
    </p>
    <p>
     <img alt="" height="606" src="https://i-blog.csdnimg.cn/direct/5e727fc694f04a83bb3e74e4fdd547c6.png" width="1234"/>
    </p>
    <p>
     <img alt="" height="506" src="https://i-blog.csdnimg.cn/direct/60df7442a47149baaba6020d43073b3c.png" width="1158"/>
    </p>
    <p>
     <img alt="" height="511" src="https://i-blog.csdnimg.cn/direct/b12651b3c38a46d885a4343fc47f9004.png" width="1172"/>
    </p>
    <blockquote>
     <p>
      这里包含两个部分。首先，sigmoid 层称 “输入门层” 决定将要存放在细胞状态的信息量的大小。然后，一个 tanh 层创建一个新的候选值向量会被加入到状态中。下一步，我们会将这两个信息来产生对状态的更新。
     </p>
    </blockquote>
    <p>
     <img alt="" height="545" src="https://i-blog.csdnimg.cn/direct/3f13952065f646158673c0b908721c05.png" width="1113"/>
    </p>
    <p>
     <img alt="" height="488" src="https://i-blog.csdnimg.cn/direct/f7644ba476a04db39169057df29cce17.png" width="1157"/>
    </p>
    <p>
     <img alt="" height="518" src="https://i-blog.csdnimg.cn/direct/007df9074ef846c4b0b3e716887b8b11.png" width="1173"/>
    </p>
    <p>
     <img alt="" height="598" src="https://i-blog.csdnimg.cn/direct/f03caad885a34116ad1e99f09a58224f.png" width="1260"/>
    </p>
    <hr/>
    <p>
     激活函数
    </p>
    <p>
     <img alt="" height="506" src="https://i-blog.csdnimg.cn/direct/3cb1acf496b44be1970d00ac67ecd850.png" width="871"/>
    </p>
    <p>
     <img alt="" height="545" src="https://i-blog.csdnimg.cn/direct/3f13952065f646158673c0b908721c05.png" width="1113"/>
    </p>
    <hr/>
    <p>
     Softmax
    </p>
    <p>
     Softmax 是一种常用的激活函数，主要用于多分类任务中，
     <strong>
      将神经网络的输出转换为概率分布。
     </strong>
    </p>
    <p>
     它的核心作用是将一组任意实数转换为概率值，这些概率值的总和为 1，便于表示每个类别的预测概率。
    </p>
    <p>
     <img alt="" height="456" src="https://i-blog.csdnimg.cn/direct/cee1bfc509f3435db7f165fc8c55bf81.png" width="1112"/>
    </p>
    <p>
     Softmax 的特性：
    </p>
    <ol>
     <li>
      <p>
       <strong>
        输出为概率分布
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         Softmax 的输出是一个概率分布，每个类别的概率值在 [0,1] 之间，且所有类别的概率之和为 1。
        </p>
       </li>
       <li>
        <p>
         例如，对于 3 个类别，Softmax 的输出可能是[0.2,0.7,0.1]。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        放大差异
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         Softmax 通过指数运算放大高分值的类别，抑制低分值的类别，使得高分值的类别概率更接近 1，低分值的类别概率更接近 0。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        可导性
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         Softmax 是连续可导的，便于通过梯度下降法优化模型。
        </p>
       </li>
      </ul>
     </li>
    </ol>
    <p>
     Softmax 的应用场景
    </p>
    <ol>
     <li>
      <p>
       <strong>
        多分类任务
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         在神经网络的最后一层使用 Softmax，将输出转换为类别概率分布。
        </p>
       </li>
       <li>
        <p>
         例如，图像分类任务中，Softmax 可以将卷积神经网络的特征映射为每个类别的概率。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        损失函数结合
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         Softmax 通常与交叉熵损失函数（Cross-Entropy Loss）结合使用，用于衡量预测概率分布与真实标签之间的差异。
        </p>
       </li>
      </ul>
     </li>
    </ol>
    <p>
     它通过指数运算和归一化，将原始得分映射为概率值，便于模型训练和预测。
    </p>
    <blockquote>
     <p>
      我们希望模型输出y(j)可以视为属于类j的概率，然后选择具有最大输出值的类别argmaxy(j)作为我们的预测。softmax：最大概率的标签，能够将未规范化的预测变换为非负数，并且总和为1，同时能够让模型保持可导。
     </p>
    </blockquote>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36323839343637372f:61727469636c652f64657461696c732f313436323834343335" class_="artid" style="display:none">
 </p>
</div>


