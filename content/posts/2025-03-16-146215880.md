---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34303637323131352f:61727469636c652f64657461696c732f313436323135383830"
layout: post
title: "ä»Online-Softmaxåˆ°FlashAttention"
date: 2025-03-16 22:14:51 +08:00
description: "ä»Online Softmaxåˆ°FlashAttention"
keywords: "ä»Online Softmaxåˆ°FlashAttention"
categories: ['Llm']
tags: ['Softmax', 'Softmax', 'Softmax', 'Safe', 'Online', 'Flashattention', 'Attention']
artid: "146215880"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146215880
    alt: "ä»Online-Softmaxåˆ°FlashAttention"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146215880
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146215880
cover: https://bing.ee123.net/img/rand?artid=146215880
image: https://bing.ee123.net/img/rand?artid=146215880
img: https://bing.ee123.net/img/rand?artid=146215880
---

# ä»Online Softmaxåˆ°FlashAttention

### å‰è¨€

> æœ€è¿‘åœ¨å­¦ä¹  FlashAttentionï¼Œçœ‹åˆ°ä¸€ä»½ä¸é”™çš„æ‰‹ç¨¿åˆ†äº«ä¸‹ğŸ¤—
>
> manuscriptï¼š
> [From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf)

### 0. Abstract

FlashAttention
çš„å…³é”®åˆ›æ–°æ˜¯ä½¿ç”¨ç±»ä¼¼äº
Online Softmax
çš„æ€æƒ³æ¥ tile åˆ†å— self-attention çš„è®¡ç®—ï¼Œè¿™æ ·å¯ä»¥èåˆæ•´ä¸ªå¤šå¤´æ³¨æ„åŠ›å±‚ï¼Œè€Œæ— éœ€å¤šæ¬¡é‡å¤è®¿é—® GPU çš„å…¨å±€å†…å­˜æ¥è·å–ä¸´æ—¶å˜é‡å’Œæ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ

A
A





A
ã€‚æœ¬æ–‡å°†ç®€è¦è§£é‡Šå¦‚ä½• tiling åˆ†å— self-attention çš„è®¡ç®—ï¼Œä»¥åŠå¦‚ä½•ä» online softmax ä¸­æ¨å¯¼å‡º FlashAttention è®¡ç®—

### 1. The Self-Attention

Self-Attention çš„è®¡ç®—å¯ä»¥æè¿°ä¸ºï¼š

O
=
s
o
f
t
m
a
x
(
Q
K
T
)
V
\begin{equation} O = \mathrm{softmax}\left(QK^T\right)V \end{equation}














O



=




softmax





(

Q


K









T


)



V

â€‹













â€‹

å…¶ä¸­

Q
,
K
,
V
,
O
âˆˆ
R
L
Ã—
D
Q,K,V,O \in \mathbb{R}^{L\times D}





Q

,



K

,



V

,



O



âˆˆ






R










L

Ã—

D
ï¼Œ

L
L





L
æ˜¯åºåˆ—é•¿åº¦ï¼Œ

D
D





D
æ˜¯æ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œsoftmax å°†æŒ‰åˆ—åº”ç”¨äº

Q
K
T
QK^T





Q


K









T

**Note**
ï¼šè¿™é‡Œæˆ‘ä»¬å¿½ç•¥äº†å¤šå¤´ã€å¤š batchï¼Œå› ä¸ºåœ¨è¿™äº›ç»´åº¦ä¸Šçš„è®¡ç®—æ˜¯å®Œå…¨å¹¶è¡Œçš„ï¼Œä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åªå…³æ³¨å•å¤´ã€å• batch çš„è®¡ç®—å°±è¡Œã€‚å¦å¤–ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä¹Ÿå¿½ç•¥äº†æ³¨æ„åŠ›æ©ç ä»¥åŠç¼©æ”¾å› å­

1
D
\frac{1}{\sqrt{D}}

























D


â€‹













1

â€‹

è®¡ç®— self-attention çš„æ ‡å‡†æ–¹æ³•æ˜¯å°†å…¶åˆ†è§£ä¸ºä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

X
=
Q
K
T
A
=
s
o
f
t
m
a
x
(
X
)
O
=
A
V
\begin{align} X & = QK^T \\ A & = \mathrm{softmax}(X) \\ O & = AV \end{align}














X





A





O

â€‹














=



Q


K









T









=




softmax

(

X

)









=



A

V

â€‹

























â€‹

æˆ‘ä»¬ç§°

X
X





X
çŸ©é˜µä¸º pre-softmax logitsï¼Œ

A
A





A
çŸ©é˜µä¸ºæ³¨æ„åŠ›åˆ†æ•°ï¼ˆattention scoreï¼‰ï¼Œ

O
O





O
çŸ©é˜µä¸ºè¾“å‡º

FlashAttention çš„ä¸€ä¸ªæƒŠäººä¹‹å¤„åœ¨äºï¼Œæˆ‘ä»¬ä¸éœ€è¦åœ¨å…¨å±€å†…å­˜ï¼ˆglobal memoryï¼‰ä¸Šå®ç°

X
X





X
å’Œ

A
A





A
çŸ©é˜µï¼Œè€Œæ˜¯å°†å…¬å¼ (1) ä¸­çš„æ•´ä¸ªè®¡ç®—èåˆåˆ°å•ä¸ª CUDA kernel ä¸­ã€‚è¿™è¦æ±‚æˆ‘ä»¬è®¾è®¡ä¸€ç§èƒ½å¤Ÿç²¾å¿ƒç®¡ç†ç‰‡ä¸Šå†…å­˜ï¼ˆon-chip memoryï¼‰çš„ç®—æ³•ï¼Œå› ä¸º NVIDIA GPU çš„å…±äº«å†…å­˜ï¼ˆshared memoryï¼‰éå¸¸å°

å¯¹äºçŸ©é˜µä¹˜æ³•ç­‰ç»å…¸ç®—æ³•ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šä½¿ç”¨ tiling æŠ€æœ¯æ¥ç¡®ä¿ç‰‡ä¸Šå†…å­˜ä¸è¶…è¿‡ç¡¬ä»¶é™åˆ¶ã€‚ä¸‹å›¾æä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œåœ¨ kernel æ‰§è¡ŒæœŸé—´ï¼Œæ— è®ºçŸ©é˜µå½¢çŠ¶å¦‚ä½•ï¼Œåªæœ‰

3
T
2
3T^2





3


T









2
ä¸ªå…ƒç´ å­˜å‚¨åœ¨ç‰‡ä¸Šã€‚è¿™ç§ tiling æ–¹æ³•ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºåŠ æ³•æ˜¯å…³è”çš„ï¼Œå…è®¸å°†æ•´ä¸ªçŸ©é˜µä¹˜æ³•åˆ†è§£ä¸ºè®¸å¤š tile-wise çŸ©é˜µä¹˜æ³•çš„æ€»å’Œ

ç„¶è€Œï¼ŒSelf-Attention åŒ…å«ä¸€ä¸ªä¸ç›´æ¥å…³è”çš„ softmax è¿ç®—ç¬¦ï¼Œå› æ­¤å¾ˆéš¾åƒä¸‹å›¾é‚£æ ·ç®€å•åœ°å¯¹ Self-Attention è¿›è¡Œ tileï¼Œé‚£æœ‰æ²¡æœ‰åŠæ³•è®© softmax å…·æœ‰å…³è”æ€§å‘¢ï¼ŸğŸ¤”

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/158826e34d414323b046e9a21c2c339f.png#pic_center)

ä¸Šå›¾ç®€è¦è¯´æ˜äº†å¦‚ä½•å¯¹çŸ©é˜µä¹˜æ³•

C
=
A
Ã—
B
C=A\times B





C



=





A



Ã—





B
çš„è¾“å…¥å’Œè¾“å‡ºçŸ©é˜µè¿›è¡Œ tileï¼ˆåˆ†å—ï¼‰ï¼ŒçŸ©é˜µè¢«åˆ’åˆ†ä¸º

T
Ã—
T
T\times T





T



Ã—





T
ä¸ªå°å—ã€‚å¯¹äºæ¯ä¸ªè¾“å‡ºå—ï¼Œæˆ‘ä»¬ä»å·¦åˆ°å³éå†

A
A





A
ä¸­çš„ç›¸å…³å—ï¼Œä»ä¸Šåˆ°ä¸‹éå†

B
B





B
ä¸­çš„ç›¸å…³å—ï¼Œå¹¶å°†å®ƒä»¬çš„å€¼ä»å…¨å±€å†…å­˜ï¼ˆglobal memoryï¼‰åŠ è½½åˆ°ç‰‡ä¸Šå†…å­˜ï¼ˆon-chip memoryï¼‰ï¼ˆä»¥è“è‰²è¡¨ç¤ºï¼Œæ•´ä½“ç‰‡ä¸Šå†…å­˜å ç”¨ä¸º

O
(
T
2
)
O(T^2)





O

(


T









2

)
ï¼‰

æ¥ç€é€å—è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¯¹äºä½ç½®

(
i
,
j
)
(i,j)





(

i

,



j

)
ï¼Œæˆ‘ä»¬å…ˆä»ç‰‡ä¸Šå†…å­˜ä¸­åŠ è½½å—å†…æ‰€æœ‰

i
i





i
è¡Œçš„å…ƒç´ å³

A
[
i
,
k
]
A[i,k]





A

[

i

,



k

]
ä»¥åŠæ‰€æœ‰

j
j





j
åˆ—çš„å…ƒç´ å³

B
[
k
,
j
]
B[k,j]





B

[

k

,



j

]
ï¼ˆä»¥çº¢è‰²è¡¨ç¤ºï¼‰ï¼Œç„¶ååˆ©ç”¨

A
[
i
,
k
]
A[i,k]





A

[

i

,



k

]
å’Œ

B
[
k
,
j
]
B[k,j]





B

[

k

,



j

]
è®¡ç®—å¾—åˆ°ç‰‡ä¸Šå†…å­˜ä¸­çš„

C
[
i
,
j
]
C[i,j]





C

[

i

,



j

]
ã€‚ä»¥æ­¤å¾ªç¯ï¼Œç›´åˆ°å®Œæˆä¸€ä¸ªå—çš„è®¡ç®—åï¼Œæˆ‘ä»¬å†å°†ç‰‡ä¸Š

C
C





C
å—çš„ç»“æœå†™å›ä¸»å†…å­˜å¹¶ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå—

å®é™… tiling çš„åº”ç”¨è¦å¤æ‚å¾—å¤šï¼Œå¤§å®¶å¯ä»¥å‚è€ƒï¼š
[cutlass åœ¨ A100 ä¸Šå®ç°çŸ©é˜µä¹˜æ³•](https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21745-developing-cuda-kernels-to-push-tensor-cores-to-the-absolute-limit-on-nvidia-a100.pdf)

### 2. (Safe) Softmax

æˆ‘ä»¬å…ˆå›é¡¾ä¸€ä¸‹ softmax ç®—å­ï¼Œä¸‹é¢æ˜¯ softmax è®¡ç®—çš„é€šç”¨å…¬å¼ï¼š

s
o
f
t
m
a
x
(
{
x
1
,
â€¦
,
x
N
}
)
=
{
e
x
i
âˆ‘
j
=
1
N
e
x
j
}
i
=
1
N
\begin{equation} \mathrm{softmax}(\{x_1,\ldots,x_N\})=\left\{\frac{e^{x_i}}{\sum_{j=1}^Ne^{x_j}}\right\}_{i=1}^N \end{equation}















softmax

({


x









1

â€‹


,



â€¦



,




x









N

â€‹


})



=






{













âˆ‘










j

=

1





N

â€‹





e











x









j

â€‹













e











x









i

â€‹


â€‹



}










i

=

1





N

â€‹


â€‹













â€‹

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå½“

x
i
x_i






x









i

â€‹

æ¯”è¾ƒå¤§æ—¶

e
x
i
e^{x_i}






e











x









i

â€‹

å¾ˆå®¹æ˜“æº¢å‡ºï¼Œä¾‹å¦‚ float16 å¯ä»¥æ”¯æŒçš„æœ€å¤§å€¼æ˜¯ 65536ï¼Œè¿™æ„å‘³ç€å½“

x
â©¾
11
x \geqslant 11





x



â©¾





11
æ—¶ï¼Œ

e
x
e^x






e









x
å°†è¶…è¿‡ float16 çš„æœ‰æ•ˆèŒƒå›´

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåƒ pytorchã€tensorflow ç­‰æ¡†æ¶é€šå¸¸ä¼šä½¿ç”¨ä¸€ç§è¢«ç§°ä¸º â€œsafeâ€ çš„ softmax è®¡ç®—æ–¹æ³•ï¼š

e
x
i
âˆ‘
j
=
1
N
e
x
j
=
e
x
i
âˆ’
m
âˆ‘
j
=
1
N
e
x
j
âˆ’
m
\begin{equation} \frac{e^{x_{i}}}{\sum_{j=1}^{N}e^{x_{j}}}=\frac{e^{x_{i}-m}}{\sum_{j=1}^{N}e^{x_{j}-m}} \end{equation}


























âˆ‘










j

=

1






N

â€‹





e











x










j

â€‹













e











x










i

â€‹


â€‹




=















âˆ‘










j

=

1






N

â€‹





e











x










j

â€‹


âˆ’

m












e











x










i

â€‹


âˆ’

m

â€‹


â€‹













â€‹

å…¶ä¸­

m
=
max
â¡
j
=
1
N
(
x
j
)
m=\max_{j=1}^{N}(x_j)





m



=






max










j

=

1






N

â€‹


(


x









j

â€‹


)

æˆ‘ä»¬å°†

x
i
x_i






x









i

â€‹

å‡å»å®ƒä»¬ä¸­çš„æœ€å¤§å€¼ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿æ¯ä¸ªå…ƒç´ 

x
i
âˆ’
m
â©½
0
x_i-m \leqslant 0






x









i

â€‹




âˆ’





m



â©½





0
ï¼Œä»è€Œå†åšæŒ‡æ•°è¿ç®—æ—¶å¯ä»¥ç¡®ä¿å…¶å€¼ä¸ä¼šæº¢å‡º

æˆ‘ä»¬å¯ä»¥å°† safe softmax çš„è®¡ç®—æ€»ç»“ä¸ºä¸‹é¢çš„ 3-pass ç®—æ³•ï¼š

**Algorithm 3-pass safe softmax**

NOTATIONS

* {
  m
  i
  }
  \{m_i\}





  {


  m









  i

  â€‹


  }
  ï¼š

  max
  â¡
  j
  =
  1
  i
  {
  x
  j
  }
  \max_{j=1}^i\{x_j\}






  max










  j

  =

  1





  i

  â€‹


  {


  x









  j

  â€‹


  }
  ï¼Œåˆå§‹å€¼

  m
  0
  =
  âˆ’
  âˆ
  m_0=-\infty






  m









  0

  â€‹




  =





  âˆ’

  âˆ
* {
  d
  i
  }
  \{d_i\}





  {


  d









  i

  â€‹


  }
  ï¼š

  âˆ‘
  j
  =
  1
  i
  e
  x
  j
  âˆ’
  m
  N
  \sum_{j=1}^{i}e^{x_{j}-m_{N}}






  âˆ‘










  j

  =

  1






  i

  â€‹





  e











  x










  j

  â€‹


  âˆ’


  m










  N

  â€‹

  ï¼Œåˆå§‹å€¼

  d
  0
  =
  0
  d_0=0






  d









  0

  â€‹




  =





  0
  ï¼Œ

  d
  N
  d_N






  d









  N

  â€‹

  æ˜¯ safe softmax çš„åˆ†æ¯
* {
  a
  i
  }
  \{a_i\}





  {


  a









  i

  â€‹


  }
  ï¼šæœ€ç»ˆçš„ softmax å€¼

BODY

**for**



i
â†
1
,
N
i\leftarrow1,N





i



â†





1

,



N
**do**

m
i
â†
max
â¡
(
m
i
âˆ’
1
,
x
i
)
\begin{equation} m_i \leftarrow \max (m_{i-1},x_i) \end{equation}















m









i

â€‹




â†



max

(


m










i

âˆ’

1

â€‹


,




x









i

â€‹


)

â€‹













â€‹

**end**

**for**



i
â†
1
,
N
i\leftarrow1,N





i



â†





1

,



N
**do**

d
i
â†
d
i
âˆ’
1
+
e
x
i
âˆ’
m
N
\begin{equation} d_i \leftarrow d_{i-1} + e^{x_i-m_N} \end{equation}















d









i

â€‹




â†




d










i

âˆ’

1

â€‹




+




e











x









i

â€‹


âˆ’


m









N

â€‹


â€‹













â€‹

**end**

**for**



i
â†
1
,
N
i\leftarrow1,N





i



â†





1

,



N
**do**

a
i
â†
e
x
i
âˆ’
m
N
d
N
\begin{equation} a_i \leftarrow \frac{e^{x_i-m_N}}{d_N} \end{equation}















a









i

â€‹




â†















d









N

â€‹













e











x









i

â€‹


âˆ’


m









N

â€‹


â€‹


â€‹













â€‹

**end**

è¯¥ç®—æ³•è¦æ±‚æˆ‘ä»¬å¯¹

[
1
,
N
]
[1,N]





[

1

,



N

]
è¿›è¡Œ 3 æ¬¡è¿­ä»£ï¼Œåœ¨ Transformer çš„ self-attention ä¸­ï¼Œæ¯ä¸ª

x
i
x_i






x









i

â€‹

æ˜¯é€šè¿‡

Q
K
T
QK^T





Q


K









T
è®¡ç®—å¾—åˆ°çš„ã€‚å¦‚æœæˆ‘ä»¬æ— æ³•å°†æ‰€æœ‰çš„

x
i
x_i






x









i

â€‹

éƒ½ç¼“å­˜åˆ°ç‰‡ä¸Šå†…å­˜ï¼ˆSRAMï¼‰ä¸­ï¼ˆå®é™…ä¸Šæˆ‘ä»¬ä¹Ÿæ²¡æœ‰è¶³å¤Ÿå¤§çš„ç‰‡ä¸Šå†…å­˜ SRAM æ¥å®¹çº³æ‰€æœ‰çš„

x
i
x_i






x









i

â€‹

ï¼‰ï¼Œé‚£ä¹ˆåœ¨æ¯æ¬¡è¿­ä»£æ—¶ä¸å¾—ä¸è®¿é—®

Q
,
K
Q,K





Q

,



K
ä»¥åŠ¨æ€é‡æ–°è®¡ç®—

x
i
x_i






x









i

â€‹

ï¼Œè¿™ç§é¢‘ç¹çš„è®¿é—®ä¼šå¯¼è‡´å¤§é‡çš„ I/O æ“ä½œï¼Œä»è€Œé™ä½æ•´ä½“æ•ˆç‡

### 3. Online Softmax

å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿåœ¨ä¸€ä¸ªå¾ªç¯ä¸­èåˆå…¬å¼ (7)ã€(8)ã€(9)ï¼Œé‚£ä¹ˆå°±å¯ä»¥å°† global memory çš„è®¿é—®æ—¶é—´é™ä½ 3 å€ï¼Œä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½åœ¨åŒä¸€ä¸ªå¾ªç¯ä¸­èåˆå…¬å¼ (7)ã€(8)ï¼Œå› ä¸ºå…¬å¼ (8) çš„è®¡ç®—éœ€è¦ä¾èµ–

m
N
m_N






m









N

â€‹

ï¼Œè€Œ

m
N
m_N






m









N

â€‹

åªæœ‰åœ¨ç¬¬ä¸€ä¸ªå¾ªç¯å®Œæˆä¹‹åæ‰èƒ½å¤Ÿç¡®å®š

æ—¢ç„¶æ•°æ®ä¹‹é—´å­˜åœ¨ç€ä¾èµ–å…³ç³»ï¼Œé‚£æˆ‘ä»¬å¯ä»¥åˆ›å»ºå¦å¤–ä¸€ä¸ªåºåˆ—

d
i
â€²
:
=
âˆ‘
j
=
1
i
e
x
j
âˆ’
m
i
d_{i}^{ \prime} := \sum_{j=1}^{i}e^{x_{j}-m_{i}}






d










i






â€²

â€‹




:=






âˆ‘










j

=

1






i

â€‹





e











x










j

â€‹


âˆ’


m










i

â€‹

ä½œä¸ºåŸå§‹åºåˆ—

d
i
â€²
:
=
âˆ‘
j
=
1
i
e
x
j
âˆ’
m
N
d_{i}^{ \prime} := \sum_{j=1}^{i}e^{x_{j}-m_{N}}






d










i






â€²

â€‹




:=






âˆ‘










j

=

1






i

â€‹





e











x










j

â€‹


âˆ’


m










N

â€‹

çš„æ›¿ä»£ï¼Œä»¥æ¶ˆé™¤å¯¹

N
N





N
çš„ä¾èµ–ï¼Œå¹¶ä¸”è¿™ä¸¤ä¸ªåºåˆ—çš„ç¬¬

N
N





N
é¡¹æ˜¯ç›¸åŒçš„å³

d
N
=
d
N
â€²
d_N=d^{\prime}_N






d









N

â€‹




=






d









N






â€²

â€‹

ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°ç”¨

d
N
â€²
d^{\prime}_N






d









N






â€²

â€‹

æ›¿æ¢å…¬å¼ (9) ä¸­çš„

d
N
d_N






d









N

â€‹

**Note**
ï¼šåœ¨æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦ä¸­ï¼Œç¬¦å·

:
=
:=





:=
ç”¨äºè¡¨ç¤º â€œå®šä¹‰ä¸ºâ€ æˆ– â€œè¢«å®šä¹‰ä¸ºâ€ï¼Œä¾‹å¦‚å½“æˆ‘ä»¬å†™ä¸‹

x
:
=
y
x:=y





x



:=





y
æ—¶ï¼Œæˆ‘ä»¬æ˜¯åœ¨è¯´æ˜ â€œæˆ‘ä»¬å°†

x
x





x
å®šä¹‰ä¸º

y
y





y
â€ æˆ– â€œ

x
x





x
ç­‰äº

y
y





y
ï¼ˆè¿™æ˜¯å®ƒçš„å®šä¹‰ï¼‰â€

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ‰¾åˆ°

d
i
â€²
d^{\prime}_i






d









i






â€²

â€‹

å’Œ

d
i
âˆ’
1
â€²
d^{\prime}_{i-1}






d










i

âˆ’

1






â€²

â€‹

ä¹‹é—´çš„é€’å½’å…³ç³»ï¼š

d
i
â€²
=
âˆ‘
j
=
1
i
e
x
j
âˆ’
m
i
=
(
âˆ‘
j
=
1
i
âˆ’
1
e
x
j
âˆ’
m
i
)
+
e
x
i
âˆ’
m
i
=
(
âˆ‘
j
=
1
i
âˆ’
1
e
x
j
âˆ’
m
i
âˆ’
1
)
e
m
i
âˆ’
1
âˆ’
m
i
+
e
x
i
âˆ’
m
i
=
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
+
e
x
i
âˆ’
m
i
\begin{equation} \begin{aligned} d_{i}^{\prime} & =\sum_{j=1}^ie^{x_j-m_i} \\ & =\left(\sum_{j=1}^{i-1}e^{x_j-m_i}\right)+e^{x_i-m_i} \\ & =\left(\sum_{j=1}^{i-1}e^{x_j-m_{i-1}}\right)e^{m_{i-1}-m_i}+e^{x_i-m_i} \\ & =d_{i-1}^{\prime}e^{m_{i-1}-m_i}+e^{x_i-m_i} \end{aligned} \end{equation}

























d










i






â€²

â€‹


â€‹














=












j

=

1





âˆ‘





i

â€‹





e











x









j

â€‹


âˆ’


m









i

â€‹










=





(










j

=

1





âˆ‘






i

âˆ’

1

â€‹





e











x









j

â€‹


âˆ’


m









i

â€‹



)



+




e











x









i

â€‹


âˆ’


m









i

â€‹










=





(










j

=

1





âˆ‘






i

âˆ’

1

â€‹





e











x









j

â€‹


âˆ’


m










i

âˆ’

1

â€‹



)




e











m










i

âˆ’

1

â€‹


âˆ’


m









i

â€‹




+




e











x









i

â€‹


âˆ’


m









i

â€‹










=




d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m









i

â€‹




+




e











x









i

â€‹


âˆ’


m









i

â€‹


â€‹


â€‹













â€‹

è¿™ä¸ªé€’å½’å…¬å¼åªä¾èµ–äº

m
i
m_i






m









i

â€‹

å’Œ

m
i
âˆ’
1
m_{i-1}






m










i

âˆ’

1

â€‹

ï¼Œæ­¤å¤–æˆ‘ä»¬è¿˜å¯ä»¥åœ¨ä¸€ä¸ªå¾ªç¯ä¸­åŒæ—¶è®¡ç®—

m
j
m_j






m









j

â€‹

å’Œ

d
j
â€²
d^{\prime}_j






d









j






â€²

â€‹

**Algorithm 2-pass online softmax**

**for**



i
â†
1
,
N
i\leftarrow 1,N





i



â†





1

,



N
**do**

m
i
â†
max
â¡
(
m
i
âˆ’
1
,
x
i
)
d
i
â€²
â†
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
+
e
x
i
âˆ’
m
i
\begin{aligned} m_i & \leftarrow \max (m_{i-1}, x_i) \\ d^{\prime}_i & \leftarrow d^{\prime}_{i-1}e^{m_{i-1}-m_i}+e^{x_i-m_i} \end{aligned}
















m









i

â€‹







d









i






â€²

â€‹


â€‹














â†



max

(


m










i

âˆ’

1

â€‹


,




x









i

â€‹


)









â†




d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m









i

â€‹




+




e











x









i

â€‹


âˆ’


m









i

â€‹


â€‹

**end**

**for**



i
â†
1
,
N
i\leftarrow 1,N





i



â†





1

,



N
**do**

a
i
â†
e
x
i
âˆ’
m
N
d
N
â€²
a_i \leftarrow \frac{e^{x_i - m_N}}{d^{\prime}_N}






a









i

â€‹




â†

















d









N






â€²

â€‹













e











x









i

â€‹


âˆ’


m









N

â€‹


â€‹

**end**

è¿™æ˜¯
Online Softmax
è®ºæ–‡ä¸­æå‡ºçš„ç®—æ³•ï¼Œä½†å®ƒä»ç„¶éœ€è¦ä¸¤æ¬¡å¾ªç¯æ‰èƒ½å®Œæˆ softmax çš„è®¡ç®—ï¼Œæˆ‘ä»¬èƒ½å¦å°†å¾ªç¯æ¬¡æ•°å‡å°‘åˆ° 1 æ¬¡æ¥æœ€å°åŒ–å…¨å±€çš„ I/O å‘¢ï¼ŸğŸ¤”

### 4. FlashAttention

ä¸å¹¸çš„æ˜¯ï¼Œå¯¹äº softmax æ¥è¯´ï¼Œç­”æ¡ˆæ˜¯å¦å®šçš„ï¼Œä½†åœ¨ Self-Attention ä¸­ï¼Œæˆ‘ä»¬æœ€ç»ˆçš„ç›®æ ‡å¹¶ä¸æ˜¯æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ

A
=
s
o
f
t
m
a
x
(
Q
K
T
)
A=\mathrm{softmax}(QK^T)





A



=






softmax

(

Q


K









T

)
ï¼Œè€Œæ˜¯è¾“å‡ºçŸ©é˜µ

O
=
A
Ã—
V
O=A \times V





O



=





A



Ã—





V
ï¼Œæ—¢ç„¶æ— æ³•æ‰¾åˆ° softmax çš„ä¸€æ¬¡é€’å½’å½¢å¼ï¼Œé‚£æˆ‘ä»¬æ¢ä¸ªæ€è·¯æ€è€ƒä¸‹èƒ½å¦æ‰¾åˆ°çŸ©é˜µ

O
O





O
çš„ä¸€æ¬¡é€’å½’å½¢å¼å‘¢ï¼Ÿ

ä¸‹é¢æˆ‘ä»¬å°è¯•ä¸‹å…ˆå°† Self-Attention è®¡ç®—çš„ç¬¬

k
k





k
è¡Œå…¬å¼è½¬åŒ–ä¸ºé€’å½’ç®—æ³•ï¼š

**Note**
ï¼šç”±äºæ‰€æœ‰è¡Œçš„è®¡ç®—éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œæˆ‘ä»¬åªè§£é‡Šä¸€è¡Œçš„è®¡ç®—

**Algorithm Multi-pass Self-Attention**

NOTATIONS

* Q
  [
  k
  ,
  :
  ]
  Q[k,:]





  Q

  [

  k

  ,



  :




  ]
  ï¼šæŸ¥è¯¢çŸ©é˜µ

  Q
  Q





  Q
  çš„ç¬¬

  k
  k





  k
  è¡Œå‘é‡
* K
  T
  [
  :
  ,
  i
  ]
  K^T[:,i]






  K









  T

  [

  :




  ,



  i

  ]
  ï¼šé”®çŸ©é˜µ

  K
  T
  K^T






  K









  T
  çš„ç¬¬

  i
  i





  i
  åˆ—å‘é‡
* O
  [
  k
  ,
  :
  ]
  O[k,:]





  O

  [

  k

  ,



  :




  ]
  ï¼šè¾“å‡ºçŸ©é˜µ

  O
  O





  O
  çš„ç¬¬

  k
  k





  k
  è¡Œ
* V
  [
  i
  ,
  :
  ]
  V[i,:]





  V

  [

  i

  ,



  :




  ]
  ï¼šå€¼çŸ©é˜µ

  V
  V





  V
  çš„ç¬¬

  i
  i





  i
  è¡Œ
* o
  i
  \bm{o}_i








  o









  i

  â€‹

  ï¼š

  âˆ‘
  j
  =
  1
  i
  a
  j
  V
  [
  j
  ,
  :
  ]
  \sum_{j=1}^ia_jV[j,:]






  âˆ‘










  j

  =

  1





  i

  â€‹





  a









  j

  â€‹


  V

  [

  j

  ,



  :




  ]
  ï¼Œå­˜å‚¨

  A
  [
  k
  ,
  :
  i
  ]
  Ã—
  V
  [
  :
  i
  ,
  :
  ]
  A[k,:i]\times V[:i,:]





  A

  [

  k

  ,



  :





  i

  ]



  Ã—





  V

  [

  :





  i

  ,



  :




  ]
  ç»“æœçš„è¡Œå‘é‡

BODY

**for**



i
â†
1
,
N
i\leftarrow 1,N





i



â†





1

,



N
**do**

x
i
â†
Q
[
k
,
:
]
K
T
[
:
,
i
]
m
i
â†
max
â¡
(
m
i
âˆ’
1
,
x
i
)
d
i
â€²
â†
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
+
e
x
i
âˆ’
m
i
\begin{aligned} x_i & \leftarrow Q[k,:]K^T[:,i] \\ m_i & \leftarrow \max (m_{i-1},x_i) \\ d^{\prime}_i & \leftarrow d^{\prime}_{i-1} e^{m_{i-1}-m_i} + e^{x_i-m_i} \end{aligned}
















x









i

â€‹







m









i

â€‹







d









i






â€²

â€‹


â€‹














â†



Q

[

k

,



:

]


K









T

[

:

,



i

]









â†



max

(


m










i

âˆ’

1

â€‹


,




x









i

â€‹


)









â†




d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m









i

â€‹




+




e











x









i

â€‹


âˆ’


m









i

â€‹


â€‹

**end**

**for**



i
â†
1
,
N
i\leftarrow 1,N





i



â†





1

,



N
**do**

a
i
â†
e
x
i
âˆ’
m
N
d
N
â€²
o
i
â†
o
i
âˆ’
1
+
a
i
V
[
i
,
:
]
\begin{align} a_i & \leftarrow \frac{e^{x_i-m_N}}{d^{\prime}_N} \\ \bm{o}_i & \leftarrow \bm{o}_{i-1} + a_iV[i,:] \end{align}















a









i

â€‹









o









i

â€‹


â€‹














â†















d









N






â€²

â€‹













e











x









i

â€‹


âˆ’


m









N

â€‹


â€‹










â†






o










i

âˆ’

1

â€‹




+




a









i

â€‹


V

[

i

,



:

]

â€‹



















â€‹

**end**

O
[
k
,
:
]
â†
o
N
O[k,:] \leftarrow \bm{o}_N





O

[

k

,



:




]



â†








o









N

â€‹

æˆ‘ä»¬å°†å…¬å¼ (12) ä¸­çš„

a
i
a_i






a









i

â€‹

æ›¿æ¢å…¬å¼ (11) ä¸­çš„å®šä¹‰ï¼Œæœ‰ï¼š

o
i
:
=
âˆ‘
j
=
1
i
(
e
x
j
âˆ’
m
N
d
N
â€²
V
[
j
,
:
]
)
\begin{align} \bm{o}_i := \sum_{j=1}^i\left(\frac{e^{x_j-m_N}}{d_N^{\prime}}V[j,:]\right) \end{align}

















o









i

â€‹




:=












j

=

1





âˆ‘





i

â€‹






(













d









N






â€²

â€‹













e











x









j

â€‹


âˆ’


m









N

â€‹


â€‹


V

[

j

,



:

]


)

â€‹













â€‹

è¯¥å…¬å¼ä»ç„¶ä¾èµ–

m
N
m_N






m









N

â€‹

å’Œ

d
N
d_N






d









N

â€‹

å˜é‡ï¼Œè¿™ä¸¤ä¸ªå€¼åœ¨å‰ä¸€ä¸ªå¾ªç¯å®Œæˆä¹‹å‰æ— æ³•ç¡®å®šã€‚ä½†æˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨ç¬¬ 3 èŠ‚ä¸­çš„æ›¿ä»£æŠ€å·§ï¼Œå³åˆ›å»ºæ›¿ä»£åºåˆ—

o
â€²
\bm{o}^{\prime}








o










â€²
ï¼š

o
i
â€²
:
=
(
âˆ‘
j
=
1
i
e
x
j
âˆ’
m
i
d
i
â€²
V
[
j
,
:
]
)
\bm{o}_i^{\prime}:=\left(\sum_{j=1}^i\frac{e^{x_j-m_i}}{d_i^{\prime}}V[j,:]\right)








o









i






â€²

â€‹




:=







(










j

=

1





âˆ‘





i

â€‹
















d









i






â€²

â€‹













e











x









j

â€‹


âˆ’


m









i

â€‹


â€‹


V

[

j

,



:

]


)

o
\bm{o}







o
å’Œ

o
â€²
\bm{o}^{\prime}








o










â€²
çš„ç¬¬

N
N





N
ä¸ªå…ƒç´ ç›¸åŒå³

o
N
â€²
=
o
N
\bm{o}^{\prime}_N=\bm{o}_N








o









N






â€²

â€‹




=








o









N

â€‹

ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å‘ç°

o
i
â€²
\bm{o}^{\prime}_i








o









i






â€²

â€‹

å’Œ

o
i
âˆ’
1
â€²
\bm{o}^{\prime}_{i-1}








o










i

âˆ’

1






â€²

â€‹

ä¹‹é—´å­˜åœ¨å¦‚ä¸‹çš„é€’å½’å…³ç³»ï¼š

o
i
â€²
=
âˆ‘
j
=
1
i
e
x
j
âˆ’
m
i
d
i
â€²
V
[
j
,
:
]
=
(
âˆ‘
j
=
1
i
âˆ’
1
e
x
j
âˆ’
m
i
d
i
â€²
V
[
j
,
:
]
)
+
e
x
i
âˆ’
m
i
d
i
â€²
V
[
i
,
:
]
=
(
âˆ‘
j
=
1
i
âˆ’
1
e
x
j
âˆ’
m
i
âˆ’
1
d
i
âˆ’
1
â€²
e
x
j
âˆ’
m
i
e
x
j
âˆ’
m
i
âˆ’
1
d
i
âˆ’
1
â€²
d
i
â€²
V
[
j
,
:
]
)
+
e
x
i
âˆ’
m
i
d
i
â€²
V
[
i
,
:
]
=
(
âˆ‘
j
=
1
i
âˆ’
1
e
x
j
âˆ’
m
i
âˆ’
1
d
i
âˆ’
1
â€²
V
[
j
,
:
]
)
d
i
âˆ’
1
â€²
d
i
â€²
e
m
i
âˆ’
1
âˆ’
m
i
+
e
x
i
âˆ’
m
i
d
i
â€²
V
[
i
,
:
]
=
o
i
âˆ’
1
â€²
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
d
i
â€²
+
e
x
i
âˆ’
m
i
d
i
â€²
V
[
i
,
:
]
\begin{equation} \begin{aligned} \bm{o}^{\prime}_{i}& = \sum_{j=1}^{i} \frac{e^{x_{j}-m_{i}}}{d^{\prime}_{i}}V[j, : ]\\ & = \left(\sum_{j=1}^{i-1}\frac{e^{x_{j}-m_{i}}}{d^{\prime}_{i} }V[j, : ]\right) + \frac{e^{x_{i}-m_{i}}}{d^{\prime}_{i}}V[i, : ]\\ & = \left(\sum_{j=1}^{i-1}\frac{e^{x_{j}-m_{i-1}}}{d^{\prime}_{ i-1}}\frac{e^{x_{j}-m_{i}}}{e^{x_{j}-m_{i-1}}}\frac{d^{\prime}_{i-1}}{d^{ \prime}_{i}}V[j, : ]\right) + \frac{e^{x_{i}-m_{i}}}{d^{\prime}_{i}}V[i, : ]\\ & = \left(\sum_{j=1}^{i-1}\frac{e^{x_{j}-m_{i-1}}}{d^{\prime}_{ i-1}}V[j, : ]\right) \frac{d^{\prime}_{i-1}}{d^{\prime}_{i}}e^{m_{i-1}-m_{i}} + \frac{e^{x_{i}-m_{i}}}{d^{\prime}_{i}}V[i, : ]\\ & = \bm{o}^{\prime}_{i-1}\frac{d^{\prime}_{i-1} e^{m_{i-1}-m_{ i}}}{d^{\prime}_{i}} + \frac{e^{x_{i}-m_{i}}}{d^{\prime}_{i}}V[i, : ] \end{aligned} \end{equation}



























o










i






â€²

â€‹


â€‹














=












j

=

1





âˆ‘






i

â€‹
















d










i






â€²

â€‹













e











x










j

â€‹


âˆ’


m










i

â€‹


â€‹


V

[

j

,



:

]









=





(










j

=

1





âˆ‘






i

âˆ’

1

â€‹
















d










i






â€²

â€‹













e











x










j

â€‹


âˆ’


m










i

â€‹


â€‹


V

[

j

,



:

]


)



+















d










i






â€²

â€‹













e











x










i

â€‹


âˆ’


m










i

â€‹


â€‹


V

[

i

,



:

]









=





(










j

=

1





âˆ‘






i

âˆ’

1

â€‹
















d










i

âˆ’

1






â€²

â€‹













e











x










j

â€‹


âˆ’


m










i

âˆ’

1

â€‹


â€‹














e











x










j

â€‹


âˆ’


m










i

âˆ’

1

â€‹













e











x










j

â€‹


âˆ’


m










i

â€‹


â€‹














d










i






â€²

â€‹













d










i

âˆ’

1






â€²

â€‹


â€‹


V

[

j

,



:

]


)



+















d










i






â€²

â€‹













e











x










i

â€‹


âˆ’


m










i

â€‹


â€‹


V

[

i

,



:

]









=





(










j

=

1





âˆ‘






i

âˆ’

1

â€‹
















d










i

âˆ’

1






â€²

â€‹













e











x










j

â€‹


âˆ’


m










i

âˆ’

1

â€‹


â€‹


V

[

j

,



:

]


)















d










i






â€²

â€‹













d










i

âˆ’

1






â€²

â€‹


â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m










i

â€‹




+















d










i






â€²

â€‹













e











x










i

â€‹


âˆ’


m










i

â€‹


â€‹


V

[

i

,



:

]









=






o










i

âˆ’

1






â€²

â€‹














d










i






â€²

â€‹













d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m










i

â€‹


â€‹




+















d










i






â€²

â€‹













e











x










i

â€‹


âˆ’


m










i

â€‹


â€‹


V

[

i

,



:

]

â€‹


â€‹













â€‹

å®ƒä»…ä¾èµ–äº

d
i
â€²
,
Â 
d
i
âˆ’
1
â€²
,
Â 
m
i
,
Â 
m
i
âˆ’
1
,
Â 
x
i
d^{\prime}_i,\ d^{\prime}_{i-1}, \ m_i, \ m_{i-1}, \ x_i






d









i






â€²

â€‹


,






d










i

âˆ’

1






â€²

â€‹


,






m









i

â€‹


,






m










i

âˆ’

1

â€‹


,






x









i

â€‹

ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨å•ä¸ªå¾ªç¯ä¸­èåˆ Self-Attention ä¸­çš„æ‰€æœ‰è®¡ç®—

**Algorithm FlashAttention**

**for**



i
â†
1
,
N
i \leftarrow 1,N





i



â†





1

,



N
**do**

x
i
â†
Q
[
k
,
:
]
K
T
[
:
,
i
]
m
i
â†
max
â¡
(
m
i
âˆ’
1
,
x
i
)
d
i
â€²
â†
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
+
e
x
i
âˆ’
m
i
o
i
â€²
â†
o
i
âˆ’
1
â€²
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
d
i
â€²
+
e
x
i
âˆ’
m
i
d
i
â€²
V
[
i
,
:
]
\begin{aligned} {x_{i}} & \leftarrow {Q[k,:]} {K^{T}[:,i]}\\ {m_{i}} & \leftarrow \max\left( {m_{i-1},x_{i}} \right)\\ {d^{\prime}_{i}} & \leftarrow {d^{\prime}_{i-1}e^{m_{i-1}-m_{i}}}+ {e^{x_{i}-m_{i}}}\\ {o^{\prime}_{i}} & \leftarrow {o^{\prime}_{i-1}}\frac{ {d^{\prime}_{i-1}e^{m_{i-1}-m_{i}}}}{ { d^{\prime}_{i}}}+\frac{ {e^{x_{i}-m_{i}}}}{ {d^{\prime}_{i}}} {V[i,:]}\end{aligned}

















x










i

â€‹








m










i

â€‹








d










i






â€²

â€‹








o










i






â€²

â€‹


â€‹














â†




Q

[

k

,



:

]



K










T

[

:

,



i

]









â†



max




(



m










i

âˆ’

1

â€‹


,




x










i

â€‹


)









â†





d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m










i

â€‹




+





e











x










i

â€‹


âˆ’


m










i

â€‹










â†





o










i

âˆ’

1






â€²

â€‹















d










i






â€²

â€‹














d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m










i

â€‹


â€‹




+
















d










i






â€²

â€‹














e











x










i

â€‹


âˆ’


m










i

â€‹


â€‹



V

[

i

,



:

]

â€‹

**end**

O
[
k
,
:
]
â†
o
N
â€²
O[k,:] \leftarrow \bm{o}^{\prime}_N





O

[

k

,



:




]



â†








o









N






â€²

â€‹

çŠ¶æ€é‡

x
i
,
Â 
m
i
,
Â 
d
i
â€²
,
o
i
â€²
x_i, \ m_i, \ d^{\prime}_i, \bm{o}^{\prime}_i






x









i

â€‹


,






m









i

â€‹


,






d









i






â€²

â€‹


,






o









i






â€²

â€‹

å ç”¨çš„å†…å­˜éƒ½å¾ˆå°ï¼Œå¯ä»¥éå¸¸è½»æ¾çš„æ”¾å…¥ GPU çš„ shared memory ä¸­ã€‚å¦å¤–ç”±äºæ­¤ç®—æ³•ä¸­çš„æ‰€æœ‰æ“ä½œéƒ½æ˜¯å…³è”çš„ï¼Œå› æ­¤å®ƒä¸ tiling å…¼å®¹ï¼Œå¦‚æœæˆ‘ä»¬é€ä¸ª tiling è®¡ç®—ï¼Œåˆ™è¯¥ç®—æ³•å¯ä»¥è¡¨ç¤ºä¸ºï¼š

**Algorithm FlashAttentionï¼ˆTilingï¼‰**

NEW NOTATIONS

* b
  b





  b
  ï¼štile çš„ block size å¤§å°
* #tiles
  \text{\#tiles}






  #tiles
  ï¼šæ¯è¡Œ tile çš„æ•°é‡ï¼Œ

  N
  =
  b
  Ã—
  #tiles
  N= b \times \text{\#tiles}





  N



  =





  b



  Ã—






  #tiles
* x
  i
  \bm{x}_i








  x









  i

  â€‹

  ï¼šå­˜å‚¨ç¬¬

  i
  i





  i
  ä¸ª tile çš„

  Q
  [
  k
  ]
  K
  T
  Q[k]K^T





  Q

  [

  k

  ]


  K









  T
  å€¼çš„å‘é‡

  [
  (
  i
  âˆ’
  1
  )
  b
  :
  i
  b
  ]
  [(i-1)b:ib]





  [(

  i



  âˆ’





  1

  )

  b



  :





  ib

  ]
* m
  i
  (
  l
  o
  c
  a
  l
  )
  m_i^{\mathrm{(local)}}






  m









  i







  (

  local

  )

  â€‹

  ï¼š

  x
  i
  \bm{x}_i








  x









  i

  â€‹

  å†…éƒ¨çš„å±€éƒ¨æœ€å¤§å€¼

BODY

**for**



i
â†
1
,
#tile
i \leftarrow 1,\text{\#tile}





i



â†





1

,




#tile
**do**

x
i
Â 
â†
Â 
Q
[
k
,
:
]
K
T
[
:
,
(
i
âˆ’
1
)
b
:
i
b
]
m
i
(local)
Â 
=
Â 
max
â¡
j
=
1
b
(
x
i
[
j
]
)
m
i
Â 
â†
Â 
max
â¡
(
m
i
âˆ’
1
,
m
i
(local)
)
d
i
â€²
Â 
â†
Â 
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
+
âˆ‘
j
=
1
b
e
x
i
[
j
]
âˆ’
m
i
o
i
â€²
Â 
â†
Â 
o
i
âˆ’
1
â€²
d
i
âˆ’
1
â€²
e
m
i
âˆ’
1
âˆ’
m
i
d
i
â€²
+
âˆ‘
j
=
1
b
e
x
i
[
j
]
âˆ’
m
i
d
i
â€²
V
[
j
+
(
i
âˆ’
1
)
b
,
:
]
\begin{split}\bm{x}_{i}&\ \leftarrow\ Q[k, : ] K^{T}[:,(i-1) b : i b]\\ m_{i}^{\text{(local)}}&\ =\ \max_{j=1}^{b} (\bm{x}_{i} [j]) \\ m_{i}&\ \leftarrow\ \max\big(m_{i-1},m_{i}^{\text{(local)}} \big)\\ d_{i}^{\prime}&\ \leftarrow\ d_{i-1}^{\prime} e^{m_{i-1}-m_{i}} + \sum_{j=1}^{b} e^{\bm{x}_{i}[j]-m_{i}}\\ \bm{o}_{i}^{\prime}&\ \leftarrow\ \bm{o}_{i-1}^{\prime} \frac{d_{i-1}^{\prime} e^{m_{i-1}-m_{i}}}{d_{i}^{\prime}} + \sum_{j=1}^{b} \frac{e^{\bm{x}_{i}[j]-m_{i}}}{d_{i}^{\prime}}V[j+(i-1) b, : ]\end{split}


















x










i

â€‹







m










i







(local)

â€‹







m










i

â€‹







d










i






â€²

â€‹









o










i






â€²

â€‹


â€‹
















â†





Q

[

k

,



:

]


K










T

[

:

,



(

i



âˆ’



1

)

b



:



ib

]











=














j

=

1





max






b

â€‹


(




x










i

â€‹


[

j

])











â†





max




(


m










i

âˆ’

1

â€‹


,




m










i







(local)

â€‹



)











â†






d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m










i

â€‹




+












j

=

1





âˆ‘






b

â€‹





e













x










i

â€‹


[

j

]

âˆ’


m










i

â€‹












â†








o










i

âˆ’

1






â€²

â€‹














d










i






â€²

â€‹













d










i

âˆ’

1






â€²

â€‹



e











m










i

âˆ’

1

â€‹


âˆ’


m










i

â€‹


â€‹




+












j

=

1





âˆ‘






b

â€‹
















d










i






â€²

â€‹













e













x










i

â€‹


[

j

]

âˆ’


m










i

â€‹


â€‹


V

[

j



+



(

i



âˆ’



1

)

b

,



:

]

â€‹

**end**

O
[
k
,
:
]
â†
o
N
/
b
â€²
O[k,:] \leftarrow \bm{o}^{\prime}_{N/b}





O

[

k

,



:




]



â†








o










N

/

b






â€²

â€‹

ä¸‹å›¾è¯´æ˜äº†å¦‚ä½•å°†è¯¥ç®—æ³•åº”ç”¨åˆ°ç¡¬ä»¶ä¸Šï¼š

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/011cab53bdb4407ea1db49aee3982c94.png#pic_center)

ä¸Šå›¾è¯´æ˜äº† FlashAttention åœ¨ç¡¬ä»¶ä¸Šçš„è®¡ç®—æ–¹å¼ï¼Œå…¶ä¸­è“è‰²å—è¡¨ç¤ºåœ¨ SRAM ä¸­çš„ tileï¼Œè€Œçº¢è‰²å—å¯¹åº”äº tile ä¸­çš„ç¬¬

i
i





i
è¡Œï¼Œ

L
L





L
è¡¨ç¤ºåºåˆ—é•¿åº¦ï¼Œ

D
D





D
è¡¨ç¤ºæ¯ä¸ªå¤´çš„ç»´åº¦ï¼Œ

B
B





B
è¡¨ç¤º block size çš„å¤§å°

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ•´ä½“ SRAM çš„å†…å­˜å ç”¨ä»…ä»…å–å†³äº

B
B





B
å’Œ

D
D





D
ï¼Œä¸

L
L





L
æ— å…³ï¼Œå› æ­¤ï¼Œè¯¥ç®—æ³•å¯ä»¥æ‰©å±•åˆ°é•¿ä¸Šä¸‹æ–‡è€Œä¸ä¼šé‡åˆ°å†…å­˜é—®é¢˜ï¼ˆGPU çš„å…±äº«å†…å­˜å¾ˆå°ï¼Œä¾‹å¦‚ H100 æ¶æ„ä¸­ä¸º 228kb/SMï¼‰ã€‚åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä»å·¦åˆ°å³éå†

K
T
K^T






K









T
å’Œ

A
A





A
ï¼Œä»ä¸Šåˆ°ä¸‹éå†

V
V





V
ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–°

m
,
Â 
d
,
Â 
O
m,\ d, \ O





m

,





d

,





O
çš„çŠ¶æ€

### ç»“è¯­

> è¿™ç¯‡æ–‡ç« ä¸»è¦åˆ†äº«äº† FlashAttention çš„æ€æƒ³ï¼ŒFlashAttention æƒ³è¦åšçš„å°±æ˜¯åœ¨å•ä¸ªå¾ªç¯ä¸­å®Œæˆ self-attention çš„æ•´ä¸ªè®¡ç®—
>
> æˆ‘ä»¬å…ˆä» safe softmax å‡ºå‘åˆ†æäº† safe softmax ç®—æ³•éœ€è¦å¾ªç¯è¿­ä»£ 3 æ¬¡ï¼Œç¬¬ä¸€æ¬¡å¾ªç¯æ±‚å–åºåˆ—ä¸­çš„æœ€å¤§å€¼
>
> m
> N
> m_N
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> ï¼Œç¬¬äºŒæ¬¡å¾ªç¯æ±‚å– safe softmax çš„åˆ†æ¯
>
> d
> N
> d_N
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> ï¼Œç¬¬ä¸‰æ¬¡å¾ªç¯æ±‚å–æœ€ç»ˆçš„ softmax å€¼
>
> a
> i
> a_i
>
>
>
>
>
>
> a
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ã€‚ç”±äº SRAM ç‰‡ä¸Šå†…å­˜ç©ºé—´æœ‰é™ï¼Œæˆ‘ä»¬æ— æ³•å°†æ‰€æœ‰çš„
>
> x
> i
> x_i
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ç¼“å­˜ï¼Œå¯¼è‡´æˆ‘ä»¬åœ¨æ¯æ¬¡è¿­ä»£æ—¶éœ€è¦é‡æ–°è®¿é—®
>
> Q
> ,
> K
> Q,K
>
>
>
>
>
> Q
>
> ,
>
>
>
> K
> ä»¥åŠ¨æ€é‡æ–°è®¡ç®—
>
> x
> i
> x_i
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ï¼Œè¿™é€ æˆäº†å¤§é‡çš„ I/O æ“ä½œï¼Œæˆ‘ä»¬å¹¶ä¸å¸Œæœ›çœ‹åˆ°
>
> åœ¨ online softmax ä¸­æå‡ºäº†ä¸€ä¸ªæœ‰æ„æ€çš„æƒ³æ³•ï¼Œé‚£å°±æ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„åºåˆ—
>
> d
> i
> â€²
> d_i^{\prime}
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> i
>
>
>
>
>
>
> â€²
>
> â€‹
>
> æ¥æ›¿æ¢åŸå§‹çš„
>
> d
> i
> d_i
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ï¼Œä¸»è¦åŒºåˆ«åœ¨äºå°†åŸæ¥
>
> d
> i
> d_i
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> å®šä¹‰ä¸­çš„
>
> m
> N
> m_N
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> æ›¿æ¢ä¸ºäº†
>
> m
> i
> m_i
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ï¼Œä»¥æ¶ˆé™¤å¯¹
>
> N
> N
>
>
>
>
>
> N
> çš„ä¾èµ–ï¼Œå¹¶ä¸”æˆ‘ä»¬å‘ç°
>
> d
> N
> =
> d
> N
> â€²
> d_N=d_N^{\prime}
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
>
>
>
> =
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> N
>
>
>
>
>
>
> â€²
>
> â€‹
>
> ï¼Œé‚£ä¹ˆæ•´ä¸ª safe softmax çš„è®¡ç®—å°±åªéœ€è¦è¿­ä»£ 2 æ¬¡å³å¯ï¼Œç¬¬ä¸€æ¬¡å¾ªç¯è¿­ä»£å¯ä»¥åŒæ—¶è®¡ç®—
>
> m
> i
> m_i
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> å’Œ
>
> d
> i
> â€²
> d_i^{\prime}
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> i
>
>
>
>
>
>
> â€²
>
> â€‹
>
> FlashAttention æƒ³è¦æŠŠ self-attention ä¸­çš„å¾ªç¯æ¬¡æ•°é™ä½åˆ°ä¸€æ¬¡ï¼Œä½†æ˜¯å‘ç°å¯¹äºå…¶ä¸­çš„ softmax è€Œè¨€æ— æ³•å®ç°ï¼Œå› æ­¤å®ƒå¦è¾Ÿè¹Šå¾„è¯•å›¾å¯»æ‰¾è¾“å‡ºçŸ©é˜µ
>
> O
> O
>
>
>
>
>
> O
> çš„ä¸€æ¬¡è¿­ä»£å½¢å¼ã€‚ç»è¿‡æ¨å¯¼æˆ‘ä»¬å‘ç°è¾“å‡ºè¡Œå‘é‡
>
> o
> i
> \bm{o}_i
>
>
>
>
>
>
>
>
> o
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ä»ç„¶ä¾èµ–äº
>
> m
> N
> m_N
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> å’Œ
>
> d
> N
> d_N
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> å˜é‡ï¼Œè¿™ä¸¤ä¸ªå€¼åœ¨å‰ä¸€ä¸ªå¾ªç¯å®Œæˆä¹‹å‰æ— æ³•ç¡®å®šï¼Œå› æ­¤ä»éœ€è¦ä¸¤æ¬¡å¾ªç¯
>
> FlashAttention å€ŸåŠ© online softmax çš„æ›¿ä»£æ€æƒ³ï¼Œå°†åºåˆ—
>
> o
> \bm{o}
>
>
>
>
>
>
>
> o
> æ›¿æ¢ä¸º
>
> o
> â€²
> \bm{o}^{\prime}
>
>
>
>
>
>
>
>
> o
>
>
>
>
>
>
>
>
>
>
> â€²
> ï¼Œæ›¿æ¢ä¹‹åå‘ç°
>
> o
> i
> â€²
> \bm{o}_i^{\prime}
>
>
>
>
>
>
>
>
> o
>
>
>
>
>
>
>
>
>
> i
>
>
>
>
>
>
> â€²
>
> â€‹
>
> ä¸å†ä¾èµ–äº
>
> m
> N
> m_N
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> å’Œ
>
> d
> N
> d_N
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> N
>
> â€‹
>
> ç­‰å˜é‡ï¼Œè€Œä»…ä¾èµ–äº
>
> d
> i
> â€²
> ,
> Â 
> d
> i
> âˆ’
> 1
> â€²
> ,
> Â 
> m
> i
> ,
> Â 
> m
> i
> âˆ’
> 1
> ,
> Â 
> x
> i
> d^{\prime}_i,\ d^{\prime}_{i-1}, \ m_i, \ m_{i-1}, \ x_i
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
> i
>
>
>
>
>
>
> â€²
>
> â€‹
>
>
> ,
>
>
>
>
>
>
> d
>
>
>
>
>
>
>
>
>
>
> i
>
> âˆ’
>
> 1
>
>
>
>
>
>
> â€²
>
> â€‹
>
>
> ,
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
>
> ,
>
>
>
>
>
>
> m
>
>
>
>
>
>
>
>
>
>
> i
>
> âˆ’
>
> 1
>
> â€‹
>
>
> ,
>
>
>
>
>
>
> x
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> ï¼Œä»è€Œå¯ä»¥åœ¨å•ä¸ªå¾ªç¯ä¸­èåˆ self-attention ä¸­çš„æ‰€æœ‰è®¡ç®—
>
> æœ€åï¼Œç”±äº flashattention ç®—æ³•ä¸­çš„æ‰€æœ‰æ“ä½œéƒ½æ˜¯å…³è”çš„ï¼Œå› æ­¤å¯ä»¥é€ä¸ª tiling åˆ†å—è®¡ç®—ï¼Œå¹¶ä¸”æ•´ä½“ SRAM çš„å†…å­˜å ç”¨ä¸åºåˆ—é•¿åº¦æ— å…³ï¼Œå› æ­¤å¯ä»¥æ‰©å±•åˆ°é•¿ä¸Šä¸‹æ–‡è€Œä¸ç”¨æ‹…å¿ƒé‡åˆ°å†…å­˜é—®é¢˜
>
> OKï¼Œä»¥ä¸Šå°±æ˜¯å…³äºè¿™ç¯‡æ‰‹ç¨¿çš„å…¨éƒ¨å†…å®¹äº†ï¼Œå¤§å®¶æ„Ÿå…´è¶£çš„å¯ä»¥çœ‹çœ‹ï¼Œä½œä¸º FlashAttention çš„å…¥é—¨è¿˜æ˜¯æ²¡æœ‰é—®é¢˜çš„ğŸ˜„

### å‚è€ƒ

* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
* [Gtc 2020: developing cuda kernels to push tensor cores to the absolute limit on nvidia a100](https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21745-developing-cuda-kernels-to-push-tensor-cores-to-the-absolute-limit-on-nvidia-a100.pdf)
* [Online normalizer calculation for softmax](https://arxiv.org/abs/1805.02867)