---
layout: post
title: "张高兴的大模型开发实战一使用-Selenium-进行网页爬虫"
date: 2025-03-11 14:16:00 +0800
description: "目录什么是 Selenium环境搭建与配置安装 Selenium下载浏览器驱动基础操作启动浏览器并访问网页定位网页元素通过 ID 定位通过 CSS 选择器定位通过 XPath 定位与元素交互提取数据交互操作设置等待时间切换页面执行 JavaScript 代码关闭浏览器进阶技巧使用 ActionChains 模拟用户操作常用方法模拟鼠标悬停点击菜单项模拟键盘输入设置请求参数设置无头模式禁用浏览器弹..."
keywords: "张高兴的大模型开发实战：（一）使用 Selenium 进行网页爬虫"
categories: ['未分类']
tags: ['爬虫', '测试工具', 'Selenium']
artid: "146193968"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146193968
    alt: "张高兴的大模型开发实战一使用-Selenium-进行网页爬虫"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146193968
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146193968
cover: https://bing.ee123.net/img/rand?artid=146193968
image: https://bing.ee123.net/img/rand?artid=146193968
img: https://bing.ee123.net/img/rand?artid=146193968
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     张高兴的大模型开发实战：（一）使用 Selenium 进行网页爬虫
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <div class="blogpost-body cnblogs-markdown" id="cnblogs_post_body" style="font-size: 16px;">
     <h3 id="什么是-selenium">
      什么是 Selenium
     </h3>
     <p>
      Selenium 由 Jason Huggins 于 2004 年发起，最初名为 JavaScript Testing Framework，后因受到希腊神话中“月亮女神 Selene”的启发而更名为 Selenium。它最初是为了解决网页自动化测试需求而诞生的开源工具，但因其能模拟真实浏览器操作(如点击、输入、滚动等)，也被广泛用于网页数据爬取。爬虫工具有很多，例如 BeautifulSoup4，为什么选择自动化测试工具 Selenium 进行爬虫？目前绝大部分 Web 应用都使用 JavaScrip 动态加载数据，而 BeautifulSoup4 只能解析初始页面的 HTML 源码，对于动态加载的数据无法获取，因此使用 Selenium 模拟用户，完成数据加载的操作。
     </p>
     <table>
      <thead>
       <tr>
        <th>
         场景
        </th>
        <th>
         Selenium
        </th>
        <th>
         BeautifulSoup4
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         动态网页数据爬取
        </td>
        <td>
         ✅ 必须使用，如单页应用、JavaScript 渲染内容。
        </td>
        <td>
         ❌ 无法直接处理，需结合其他工具(如 Selenium 获取动态内容后解析)。
        </td>
       </tr>
       <tr>
        <td>
         静态网页数据提取
        </td>
        <td>
         ⚠️ 可用，但效率较低。
        </td>
        <td>
         ✅ 推荐，快速解析固定结构的 HTML/XML。
        </td>
       </tr>
       <tr>
        <td>
         登录验证/表单提交
        </td>
        <td>
         ✅ 直接模拟用户输入和点击。
        </td>
        <td>
         ❌ 需结合 Requests 会话保持，但无法处理 JavaScript 验证。
        </td>
       </tr>
       <tr>
        <td>
         跨浏览器兼容性测试
        </td>
        <td>
         ✅ 支持多浏览器并行测试(如通过 Selenium Grid)。
        </td>
        <td>
         ❌ 无此功能，仅解析 HTML。
        </td>
       </tr>
      </tbody>
     </table>
     <h3 id="环境搭建与配置">
      环境搭建与配置
     </h3>
     <h4 id="安装-selenium">
      安装 Selenium
     </h4>
     <p>
      创建 Python 虚拟环境后，执行命令安装
      <code>
       selenium
      </code>
      包。
     </p>
     <pre class="has"><code class="language-bash">pip install selenium</code></pre>
     <h4 id="下载浏览器驱动">
      下载浏览器驱动
     </h4>
     <p>
      下面以 Edge 浏览器为例：
     </p>
     <ol>
      <li>
       确认 Edge 版本：
       <ul>
        <li>
         设置 → 关于 Microsoft Edge。
         <img alt="" src="https://i-blog.csdnimg.cn/img_convert/de49e57414634dabe9d144d6d8a8d4a5.png" style="outline: none;"/>
        </li>
       </ul>
      </li>
      <li>
       下载对应版本的浏览器驱动：
       <ul>
        <li>
         官网：
         <a href="https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver" rel="noopener nofollow noopener noreferrer" target="_blank">
          https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver
         </a>
        </li>
        <li>
         注意：驱动版本需与浏览器版本一致(如浏览器版本为116，需选择116.x.x的驱动)。
         <img alt="" src="https://i-blog.csdnimg.cn/img_convert/8485eeae16ce0b1df66657f55eb07a21.png" style="outline: none;"/>
        </li>
       </ul>
      </li>
      <li>
       配置驱动路径：
       <ul>
        <li>
         方式一：将
         <code>
          edgedriver.exe
         </code>
         放置在任意
         <strong>
          已知的目录中
         </strong>
         或添加到系统
         <strong>
          环境变量
         </strong>
         中。
         <pre class="has"><code class="language-python">from selenium import webdriver
from selenium.webdriver.edge.service import Service

service = Service(executable_path='D:/msedgedriver.exe')
driver = webdriver.Edge(service=service)</code></pre>
        </li>
        <li>
         方式二：使用
         <code>
          WebDriverManager
         </code>
         自动管理驱动：
         <pre class="has"><code class="language-bash"># 安装 WebDriverManager
pip install webdriver-manager</code></pre>
         <pre class="has"><code class="language-python"># 使用代码自动下载驱动
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from webdriver_manager.microsoft import EdgeChromiumDriverManager

service = Service(executable_path=EdgeChromiumDriverManager().install())
driver = webdriver.Edge(service=service)</code></pre>
        </li>
       </ul>
      </li>
     </ol>
     <h3 id="基础操作">
      基础操作
     </h3>
     <h4 id="启动浏览器并访问网页">
      启动浏览器并访问网页
     </h4>
     <pre class="has"><code class="language-python">from selenium import webdriver
from selenium.webdriver.edge.service import Service

service = Service(executable_path='D:/msedgedriver.exe')
driver = webdriver.Edge(service=service)
driver.get("https://www.baidu.com/")</code></pre>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/img_convert/df0f62a39f11bc78141d398cc1759ccc.png" style="outline: none;"/>
     </p>
     <h4 id="定位网页元素">
      定位网页元素
     </h4>
     <p>
      不管是爬虫或者是测试，获取到“感兴趣”的元素才能进行后续的操作。常见的定位方法有这样几种方式：
     </p>
     <table>
      <thead>
       <tr>
        <th>
         定位方式
        </th>
        <th>
         语法示例
        </th>
        <th>
         适用场景
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         ID
        </td>
        <td>
         <code>
          driver.find_element(By.ID, "username")
         </code>
        </td>
        <td>
         通过元素的 id 属性定位，唯一性高，但需要目标元素有 id
        </td>
       </tr>
       <tr>
        <td>
         CSS 选择器
        </td>
        <td>
         <code>
          driver.find_element(By.CSS_SELECTOR, ".login")
         </code>
        </td>
        <td>
         通过 CSS 选择器定位，灵活，适合复杂结构
        </td>
       </tr>
       <tr>
        <td>
         XPath
        </td>
        <td>
         <code>
          driver.find_element(By.XPATH, "//button[@type='submit']")
         </code>
        </td>
        <td>
         通过 XPath 路径定位，精准，但路径易变，维护成本高
        </td>
       </tr>
      </tbody>
     </table>
     <h5 id="通过-id-定位">
      通过 ID 定位
     </h5>
     <pre class="has"><code class="language-python">from selenium.webdriver.common.by import By

element = driver.find_element(By.ID, "su")</code></pre>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/img_convert/d62dfc1f263fbca1429291eb9bf408f4.png" style="outline: none;"/>
     </p>
     <h5 id="通过-css-选择器定位">
      通过 CSS 选择器定位
     </h5>
     <pre class="has"><code class="language-python">element = driver.find_element(By.CSS_SELECTOR, ".s_btn")</code></pre>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/img_convert/fcd3022d604d8c96ae9e3e25c6b5cdf1.png" style="outline: none;"/>
     </p>
     <h5 id="通过-xpath-定位">
      通过 XPath 定位
     </h5>
     <pre class="has"><code class="language-python">element = driver.find_element(By.XPATH, "//*[@id="su"]")</code></pre>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/img_convert/ab2268d9c5f80fb6b9afd675abad94f7.png" style="outline: none;"/>
     </p>
     <h4 id="与元素交互">
      与元素交互
     </h4>
     <h5 id="提取数据">
      提取数据
     </h5>
     <pre class="has"><code class="language-python"># 获取页面标题
title = driver.title
# 获取元素文本、属性
text = element.text
href = element.get_attribute("href")</code></pre>
     <h5 id="交互操作">
      交互操作
     </h5>
     <pre class="has"><code class="language-python"># 输入文本
search_input = driver.find_element(By.ID, "kw")
search_input.clear()  # 清除原有内容
search_input.send_keys("电脑玩家张高兴")
# 点击按钮
search_btn = driver.find_element(By.ID, "su")
search_btn.click()</code></pre>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/img_convert/ef5e2cf23c3c4f14462de33d49d11594.png" style="outline: none;"/>
     </p>
     <h4 id="设置等待时间">
      设置等待时间
     </h4>
     <pre class="has"><code class="language-python">driver.implicitly_wait(10)  # 等待 10 秒</code></pre>
     <h4 id="切换页面">
      切换页面
     </h4>
     <p>
      Selenium 不会主动的切换窗口，要对新弹出的页面进行操作，需要先调用
      <code>
       switch_to
      </code>
      切换到该页面。
     </p>
     <pre class="has"><code class="language-python"># 获取所有窗口，将页面切换到最新的窗口
windows = driver.window_handles
driver.switch_to.window(windows[-1])</code></pre>
     <h4 id="执行-javascript-代码">
      执行 JavaScript 代码
     </h4>
     <p>
      有时网页数据是动态加载的，因此需要执行 JavaScript 代码，完成数据加载这一行为的触发。
     </p>
     <pre class="has"><code class="language-python"># 模拟滚动到底部加载更多内容
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")</code></pre>
     <h4 id="关闭浏览器">
      关闭浏览器
     </h4>
     <pre class="has"><code class="language-python">driver.quit()</code></pre>
     <h3 id="进阶技巧">
      进阶技巧
     </h3>
     <h4 id="使用-actionchains-模拟用户操作">
      使用 ActionChains 模拟用户操作
     </h4>
     <p>
      <code>
       ActionChains
      </code>
      是 Selenium 提供的一个低级用户交互模拟工具，用于执行复杂的鼠标和键盘操作。它通过构建一系列动作(如鼠标移动、点击、拖拽、键盘按键等)的队列，模拟真实用户的操作流程。
     </p>
     <h5 id="常用方法">
      常用方法
     </h5>
     <table>
      <thead>
       <tr>
        <th>
         方法
        </th>
        <th>
         功能
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         <code>
          move_to_element(element)
         </code>
        </td>
        <td>
         鼠标悬停在指定元素上
        </td>
       </tr>
       <tr>
        <td>
         <code>
          click(element)
         </code>
        </td>
        <td>
         点击元素
        </td>
       </tr>
       <tr>
        <td>
         <code>
          click_and_hold(element)
         </code>
        </td>
        <td>
         按住鼠标左键
        </td>
       </tr>
       <tr>
        <td>
         <code>
          context_click(element)
         </code>
        </td>
        <td>
         右键点击元素
        </td>
       </tr>
       <tr>
        <td>
         <code>
          double_click(element)
         </code>
        </td>
        <td>
         双击元素
        </td>
       </tr>
       <tr>
        <td>
         <code>
          drag_and_drop(source, target)
         </code>
        </td>
        <td>
         拖拽元素到目标位置
        </td>
       </tr>
       <tr>
        <td>
         <code>
          move_by_offset(x, y)
         </code>
        </td>
        <td>
         从当前位置移动鼠标到相对坐标(x,y)
        </td>
       </tr>
       <tr>
        <td>
         <code>
          key_down(key, element)
         </code>
        </td>
        <td>
         按下某个键(如
         <code>
          Keys.CONTROL
         </code>
         )，并保持按下状态
        </td>
       </tr>
       <tr>
        <td>
         <code>
          key_up(key, element)
         </code>
        </td>
        <td>
         松开之前按下的键
        </td>
       </tr>
       <tr>
        <td>
         <code>
          send_keys(keys)
         </code>
        </td>
        <td>
         向当前焦点元素发送按键
        </td>
       </tr>
      </tbody>
     </table>
     <h5 id="模拟鼠标悬停点击菜单项">
      模拟鼠标悬停点击菜单项
     </h5>
     <p>
      下面模拟用户用鼠标点击百度顶部菜单“更多”中的“翻译”条目。
     </p>
     <p>
      <img alt="" src="https://i-blog.csdnimg.cn/img_convert/fda781cea70123ffa477522c95ae4b39.png" style="outline: none;"/>
     </p>
     <pre class="has"><code class="language-python">from selenium.webdriver.common.action_chains import ActionChains
# 获取相关元素
menu = driver.find_element(By.CSS_SELECTOR, "#s-top-left &gt; div")
item = driver.find_element(By.CSS_SELECTOR, "#s-top-more &gt; div:nth-child(1) &gt; a")
# 定义动作链
actions = ActionChains(driver)
actions.move_to_element(menu).pause(1).click(item).perform()  # pause(1) 悬停1秒后点击</code></pre>
     <h5 id="模拟键盘输入">
      模拟键盘输入
     </h5>
     <pre class="has"><code class="language-python">from selenium.webdriver.common.keys import Keys
# 模拟 Ctrl+C
actions.key_down(Keys.CONTROL).send_keys('c').key_up(Keys.CONTROL).perform()</code></pre>
     <h4 id="设置请求参数">
      设置请求参数
     </h4>
     <p>
      在使用 Selenium 时，有些时候会因为自动化请求而遇到各种问题，或者想优化爬虫脚本，
      <code>
       Options
      </code>
      是一个非常重要的工具，用于定制浏览器的各种行为。
     </p>
     <h5 id="设置无头模式">
      设置无头模式
     </h5>
     <p>
      在服务器或无界面环境中运行爬虫，避免显式启动浏览器窗口，节省资源。
     </p>
     <pre class="has"><code class="language-python">from selenium.webdriver.edge.options import Options

options = Options()
options.add_argument("--headless=new")
options.add_argument("--disable-gpu")   # 部分系统需禁用 GPU 避免报错

service = Service(executable_path='D:/msedgedriver.exe')
driver = webdriver.Edge(service=service, options=options)</code></pre>
     <h5 id="禁用浏览器弹窗和通知">
      禁用浏览器弹窗和通知
     </h5>
     <p>
      网站存在弹窗广告、通知提示(如“允许通知”)，干扰自动化操作。
     </p>
     <pre class="has"><code class="language-python">options.add_argument("--disable-popup-blocking")  # 禁用弹窗拦截
options.add_argument("--disable-notifications")   # 禁用浏览器通知</code></pre>
     <h5 id="设置代理服务器">
      设置代理服务器
     </h5>
     <pre class="has"><code class="language-python">options.add_argument("--proxy-server=http://127.0.0.1:8080")</code></pre>
     <h5 id="忽略-https-证书错误">
      忽略 HTTPS 证书错误
     </h5>
     <p>
      访问的网站使用自签名证书或存在证书错误，导致浏览器报错。
     </p>
     <pre class="has"><code class="language-python">options.add_argument("--ignore-certificate-errors")</code></pre>
     <h5 id="禁用扩展程序">
      禁用扩展程序
     </h5>
     <p>
      浏览器默认安装的扩展(如广告拦截插件)干扰页面加载或元素定位。
     </p>
     <pre class="has"><code class="language-python">options.add_argument("--disable-extensions")  # 禁用所有扩展
options.add_argument("--disable-extensions-except=/path/to/extension")  # 通过扩展 ID 禁用某个扩展</code></pre>
     <h5 id="调整窗口大小和显示">
      调整窗口大小和显示
     </h5>
     <p>
      需要模拟特定分辨率或最大化窗口。
     </p>
     <pre class="has"><code class="language-python">options.add_argument("--start-maximized")      # 启动时最大化窗口
options.add_argument("window-size=1920x1080")  # 设置固定窗口大小</code></pre>
     <h5 id="设置反爬虫策略">
      设置反爬虫策略
     </h5>
     <pre class="has"><code class="language-python">options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36") # 设置User-Agent
options.add_argument("--disable-blink-features=AutomationControlled") # 禁用浏览器指纹</code></pre>
     <h3 id="实战案例爬取徐州工业职业技术学院网站新闻">
      实战案例：爬取徐州工业职业技术学院网站新闻
     </h3>
     <p>
      下面将使用
      <code>
       Selenium
      </code>
      库来爬取徐州工业职业技术学院网站上的新闻，并使用
      <code>
       jsonlines
      </code>
      库将爬取的数据输出为
      <code>
       .jsonl
      </code>
      文件。通过这个例子，可以学习到如何使用 Selenium 进行网页抓取、页面滚动控制、元素检查以及如何递归地遍历网站的所有链接。
     </p>
     <h4 id="核心功能函数">
      核心功能函数
     </h4>
     <ul>
      <li>
       <code>
        is_valid()
       </code>
       : 检查URL是否属于目标站点的一部分。
      </li>
      <li>
       <code>
        is_element()
       </code>
       : 判断指定元素是否存在。
      </li>
      <li>
       <code>
        smooth_scrol()
       </code>
       : 控制页面平滑滚动，帮助加载所有动态内容。
      </li>
      <li>
       <code>
        fetch_page_content(url)
       </code>
       : 获取并返回指定URL的内容。
      </li>
      <li>
       <code>
        parse_articles_and_links(url)
       </code>
       : 解析页面中的文章信息和内部链接。
      </li>
      <li>
       <code>
        crawl(url)
       </code>
       : 主爬虫逻辑，递归调用自身处理新发现的链接。
      </li>
     </ul>
     <p>
      通过这些函数，实现了从目标网站上自动提取新闻文章的功能，并将结果保存至本地文件。
     </p>
     <h4 id="数据提取与存储">
      数据提取与存储
     </h4>
     <p>
      对于每篇文章，脚本会提取标题、作者、日期和正文内容，并将其格式化后保存到
      <code>
       .jsonl
      </code>
      文件中。这种方法非常适合处理大量数据，因为
      <code>
       .jsonl
      </code>
      文件允许逐行添加记录，而不需要一次性加载整个文件。
     </p>
     <pre class="has"><code class="language-python">with jsonlines.open(os.path.join(file_path, 'articles.jsonl'), mode='a') as f:
    f.write(article)</code></pre>
     <h4 id="递归的遍历网站">
      递归的遍历网站
     </h4>
     <p>
      在这个脚本中，
      <code>
       crawl()
      </code>
      函数是核心部分，它负责从一个起始 URL 开始，递归地探索并抓取整个网站的内容，从一个页面开始，提取其所有链接，并对每个新发现的有效链接重复这一过程，直到没有新的链接为止。
     </p>
     <pre class="has"><code class="language-python">def crawl(url):
    """爬取页面"""
    html_content = fetch_page_content(url)
    if not html_content:
        return
    links = parse_articles_and_links(url)
    for link in links:
        crawl(link)  # 递归调用自身处理新链接</code></pre>
     <p>
      在
      <code>
       crawl
      </code>
      函数内部，首先获取当前页面的内容(
      <code>
       fetch_page_content
      </code>
      )，然后分析页面以提取文章信息和所有内部链接(
      <code>
       parse_articles_and_links
      </code>
      )。对于每一个新找到的有效链接，都会再次调用
      <code>
       crawl
      </code>
      函数进行处理。这样就形成了一个递归的过程，每次调用都进一步深入网站的一个新区域。当某个页面被访问过(存储在
      <code>
       visited_urls
      </code>
      列表中)，或者没有更多的有效链接可探索时，对应的
      <code>
       crawl
      </code>
      调用就会结束，程序返回到上一级调用继续执行，直到最初的调用也完成为止。
     </p>
     <h4 id="代码示例">
      代码示例
     </h4>
     <pre class="has"><code class="language-python">from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from urllib.parse import urljoin, urlparse
import os
import jsonlines
import re
import time

file_path = "output"
start_url = "https://www.xzcit.cn/"
visited_urls = []  # 已经访问过的URL
articles = []

service = Service(executable_path='D:/msedgedriver.exe')
driver = webdriver.Edge(service=service)

def is_valid(url):
    """检查URL是否属于目标站点"""
    parsed = urlparse(url)
    return bool(parsed.netloc) and 'www.xzcit.cn' in url

def is_element(by, value):
    """检查元素是否存在"""
    try:
        driver.find_element(by, value)
    except:
        return False
    return True

def smooth_scrol(scroll_step=350, wait_time=0.1):
    """
    按照指定步长和等待时间进行页面滚动。
    scroll_step: 每次滚动的像素数
    wait_time: 每次滚动间的等待时间
    """
    driver.execute_script("window.scrollTo({top: 0});")
    # 获取当前文档高度
    last_height = driver.execute_script("return document.body.scrollHeight")
    while True:
        for i in range(0, last_height, scroll_step):
            driver.execute_script(f"window.scrollTo(0, {i});")
            time.sleep(wait_time)  # 控制滚动速度  
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            break
        last_height = new_height

def fetch_page_content(url):
    """获取页面内容"""
    if url in visited_urls:
        return None

    print(f"Crawling: {url}")
    visited_urls.append(url)
    try:
        driver.get(url)
    except:
        return None

    smooth_scrol()

    html_content = driver.page_source
    return html_content if html_content else None

def parse_articles_and_links(url):
    """提取文章信息和内部链接""" 
    links = []

    try:
        if is_element(By.CLASS_NAME, 'article'):
            title = driver.find_element(By.CLASS_NAME, 'arti_title').text
            body = driver.find_element(By.CLASS_NAME, 'entry')
            author = driver.find_element(By.CLASS_NAME, 'arti_publisher').text.replace('作者：', '')
            dateStr = driver.find_element(By.CLASS_NAME, 'arti_update').text.replace('日期：', '')
            date = time.strftime('%Y年%#m月%#d日', time.strptime(dateStr, '%Y-%m-%d'))
            content = ''
            for p in body.find_elements(By.TAG_NAME, 'p'):
                if p.text != '':
                    content += p.text + '\n'
            content = "".join(re.split('\xa0| ', content))

            if content != '':
                article = {'date': date, 'author': author, 'title': title, 'content': content}
                articles.append(article)
                print(article)
                with jsonlines.open(os.path.join(file_path, 'articles.jsonl'), mode='a') as f:
                    f.write(article)
    except Exception as e:
        print("发生异常：", repr(e))

    # 提取页面中的所有链接
    for a in driver.find_elements(By.TAG_NAME, 'a'):
        link = a.get_attribute("href")
        full_link = urljoin(url, link)
        if is_valid(full_link) and full_link not in visited_urls and full_link not in links :
            links.append(full_link)
            print(full_link)

    return links

def crawl(url):
    """爬取页面"""
    html_content = fetch_page_content(url)
    if not html_content:
        return

    links = parse_articles_and_links(url)
    
    for link in links:
        crawl(link)  # 递归调用自身处理新链接

if __name__ == '__main__':
    if not os.path.exists(file_path):
        os.makedirs(file_path)

    crawl(start_url)
    driver.quit()</code></pre>
    </div>
   </div>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f7a353835393039352f:61727469636c652f64657461696c732f313436313933393638" class_="artid" style="display:none">
 </p>
</div>


