---
layout: post
title: "Tensorflow-2.0-GPU的使用与限制使用率及虚拟多GPU"
date: 2025-03-09 16:17:42 +0800
description: "做指定gpu计算时，如果gpu0虚拟成３个，那么在 /device:GPU:3 中的gpu序数中,物理gpu1序号为3，即依次往后推。仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）；上面的方式不仅可以设置显存的使用，还可以在只有单GPU的环境模拟多GPU进行调试。设置之后，当前程序只会使用自己可见的设备，不可见的设备不会被当前程序使用。限制消耗固定大小的显存（程序不会超出限定的显存大小，若超出的报错）。进一步说，在物理GPU0上虚拟，那么使用。"
keywords: "Tensorflow 2.0 GPU的使用与限制使用率及虚拟多GPU"
categories: ['未分类']
tags: ['人工智能', 'Tensorflow', 'Python']
artid: "146133606"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146133606
    alt: "Tensorflow-2.0-GPU的使用与限制使用率及虚拟多GPU"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146133606
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146133606
cover: https://bing.ee123.net/img/rand?artid=146133606
image: https://bing.ee123.net/img/rand?artid=146133606
img: https://bing.ee123.net/img/rand?artid=146133606
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Tensorflow 2.0 GPU的使用与限制使用率及虚拟多GPU
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <p>
    </p>
    <p>
     <strong>
      先插入一行简单代码，以下复制即可用来设置GPU使用率：
     </strong>
    </p>
    <pre><code class="prism language-c">import tensorflow as tf
import numpy as np

<span class="token function">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
import os

# 设置可使用的 gpu 序号
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token char">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token char">'0'</span>
# 用来设置是否在特殊情况下在cpu上进行计算
tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>set_soft_device_placement <span class="token operator">=</span> False
<span class="token macro property"><span class="token directive-hash">#</span> 
<span class="token directive keyword">tf</span><span class="token expression"><span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span>set_memory_growth <span class="token operator">=</span> True</span></span>
gpus <span class="token operator">=</span> tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">list_physical_devices</span><span class="token punctuation">(</span><span class="token char">'GPU'</span><span class="token punctuation">)</span>

<span class="token function">print</span><span class="token punctuation">(</span>gpus<span class="token punctuation">)</span>

<span class="token keyword">if</span> gpus<span class="token operator">:</span>
    tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">set_virtual_device_configuration</span><span class="token punctuation">(</span>gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                                           <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">VirtualDeviceConfiguration</span><span class="token punctuation">(</span>memory_limit<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    
    logical_gpus <span class="token operator">=</span> tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">list_logical_devices</span><span class="token punctuation">(</span><span class="token char">'GPU'</span><span class="token punctuation">)</span>
    <span class="token function">print</span><span class="token punctuation">(</span><span class="token function">len</span><span class="token punctuation">(</span>gpus<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">len</span><span class="token punctuation">(</span>logical_gpus<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token char">'Logical gpus'</span><span class="token punctuation">)</span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">tf</span><span class="token expression"><span class="token punctuation">.</span>debugging<span class="token punctuation">.</span><span class="token function">set_log_device_placement</span><span class="token punctuation">(</span>True<span class="token punctuation">)</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">loggpus</span> <span class="token expression"><span class="token operator">=</span> config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">list_logical_devices</span><span class="token punctuation">(</span><span class="token punctuation">)</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span> <span class="token directive keyword">strategy</span> <span class="token expression"><span class="token operator">=</span> tf<span class="token punctuation">.</span>distribute<span class="token punctuation">.</span><span class="token function">MirroredStrategy</span><span class="token punctuation">(</span><span class="token punctuation">)</span></span></span>
with tf<span class="token punctuation">.</span><span class="token function">device</span><span class="token punctuation">(</span><span class="token char">'/device:GPU:0'</span><span class="token punctuation">)</span><span class="token operator">:</span>
    w <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token function">constant</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3.4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    b <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token function">constant</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4.2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    x <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span><span class="token function">normal</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
    e <span class="token operator">=</span> tf<span class="token punctuation">.</span>random<span class="token punctuation">.</span><span class="token function">normal</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> stddev<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
    W <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token function">Variable</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span><span class="token function">constant</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    B <span class="token operator">=</span> tf<span class="token punctuation">.</span><span class="token function">Variable</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span><span class="token function">constant</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h2>
     <a id="1__39">
     </a>
     1. 获得当前主机上特定运算设备的列表
    </h2>
    <pre><code class="prism language-c">#　获取当前物理gpu
gpus <span class="token operator">=</span> tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">list_physical_devices</span><span class="token punctuation">(</span>device_type<span class="token operator">=</span><span class="token char">'GPU'</span><span class="token punctuation">)</span>
# 获取当前物理cpu
cpus <span class="token operator">=</span> tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">list_physical_devices</span><span class="token punctuation">(</span>device_type<span class="token operator">=</span><span class="token char">'CPU'</span><span class="token punctuation">)</span>
<span class="token function">print</span><span class="token punctuation">(</span>gpus<span class="token punctuation">,</span> cpus<span class="token punctuation">)</span>
# 获取当前虚拟gpu个数
logical_gpus <span class="token operator">=</span> tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">list_logical_devices</span><span class="token punctuation">(</span><span class="token char">'GPU'</span><span class="token punctuation">)</span>
</code></pre>
    <h2>
     <a id="2__50">
     </a>
     2. 设置当前程序可见的设备范围
    </h2>
    <p>
     默认情况下 TensorFlow 会使用其所能够使用的所有 GPU
    </p>
    <pre><code class="prism language-c">tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">set_visible_devices</span><span class="token punctuation">(</span>devices<span class="token operator">=</span>gpus<span class="token punctuation">[</span><span class="token number">2</span><span class="token operator">:</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device_type<span class="token operator">=</span><span class="token char">'GPU'</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     设置之后，当前程序只会使用自己可见的设备，不可见的设备不会被当前程序使用。
    </p>
    <p>
     另一种方式是使用环境变量 CUDA_VISIBLE_DEVICES 也可以控制程序所使用的 GPU。
     <br/>
     在终端输入
    </p>
    <pre><code class="prism language-c">export CUDA_VISIBLE_DEVICES<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span>
</code></pre>
    <p>
     或者在代码里加入
    </p>
    <pre><code class="prism language-c">import os
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token char">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"2,3"</span>
</code></pre>
    <h2>
     <a id="3__71">
     </a>
     3. 显存的使用
    </h2>
    <p>
     默认情况下，TensorFlow 将使用几乎所有可用的显存，以避免内存碎片化所带来的性能损失。
    </p>
    <p>
     但是TensorFlow 提供两种显存使用策略，让我们能够更灵活地控制程序的显存使用方式：
    </p>
    <ol>
     <li>
      <p>
       仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）；
      </p>
     </li>
     <li>
      <p>
       限制消耗固定大小的显存（程序不会超出限定的显存大小，若超出的报错）。
      </p>
     </li>
    </ol>
    <ul>
     <li>
      设置仅在需要时申请显存空间。
     </li>
    </ul>
    <pre><code class="prism language-c"><span class="token keyword">for</span> gpu in gpus<span class="token operator">:</span>
    tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">set_memory_growth</span><span class="token punctuation">(</span>gpu<span class="token punctuation">,</span> True<span class="token punctuation">)</span>
</code></pre>
    <ul>
     <li>
      下面的方式是设置Tensorflow固定消耗GPU:0的2GB显存。
     </li>
    </ul>
    <pre><code class="prism language-c">tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">set_virtual_device_configuration</span><span class="token punctuation">(</span>
    gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">VirtualDeviceConfiguration</span><span class="token punctuation">(</span>memory_limit<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
</code></pre>
    <h2>
     <a id="4_GPUGPU_93">
     </a>
     4. 单GPU模拟多GPU环境
    </h2>
    <p>
     上面的方式不仅可以设置显存的使用，还可以在只有单GPU的环境模拟多GPU进行调试。
    </p>
    <pre><code class="prism language-c">tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">set_virtual_device_configuration</span><span class="token punctuation">(</span>
    gpus<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">[</span>tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">VirtualDeviceConfiguration</span><span class="token punctuation">(</span>memory_limit<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     tf<span class="token punctuation">.</span>config<span class="token punctuation">.</span>experimental<span class="token punctuation">.</span><span class="token function">VirtualDeviceConfiguration</span><span class="token punctuation">(</span>memory_limit<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     上面的代码就在GPU:0上建立了两个显存均为 2GB 的虚拟 GPU。
     <br/>
     进一步说，在物理GPU0上虚拟，那么使用
    </p>
    <pre><code class="prism language-c">with tf<span class="token punctuation">.</span><span class="token function">device</span><span class="token punctuation">(</span><span class="token char">'/device:GPU:3'</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     做指定gpu计算时，如果gpu0虚拟成３个，那么在 /device:GPU:3 中的gpu序数中,物理gpu1序号为3，即依次往后推
    </p>
    <blockquote>
     <p>
      <a href="https://blog.csdn.net/suiyueruge1314/article/details/104037938">
       Tensorflow 2.0 GPU的使用与限制使用率及虚拟多GPU
      </a>
     </p>
    </blockquote>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f41676f6e696532322f:61727469636c652f64657461696c732f313436313333363036" class_="artid" style="display:none">
 </p>
</div>


