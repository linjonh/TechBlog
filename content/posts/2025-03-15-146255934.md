---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f6a786a6468646e642f:61727469636c652f64657461696c732f313436323535393334"
layout: post
title: "深入探讨RAID-5的性能与容错能力实验与分析磁盘阵列"
date: 2025-03-15 01:00:00 +08:00
description: "本实验旨在探讨 RAID 5 的性能和容错能力。通过创建 RAID 5 阵列并进行一系列读写性能测试及故障模拟，我们将观察 RAID 5 在数据冗余和故障恢复方面的表现，以验证其在实际应用中的可靠性和效率。首先说明：最少三块硬盘, 使用 4 块硬盘确实能获得更好的性能和更大的存储空间。让我们用 4 块硬盘（nvme0n2 到 nvme0n5）来构建 RAID 5。最少三块，我这边用四块做实验。你的硬盘类型可能和我的不一样 不过没有关系不影响操作实验。"
keywords: "深入探讨RAID 5的性能与容错能力：实验与分析(磁盘阵列)"
categories: ['Linux']
tags: ['键盘', '服务器', 'Python', 'Linux']
artid: "146255934"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146255934
    alt: "深入探讨RAID-5的性能与容错能力实验与分析磁盘阵列"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146255934
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146255934
cover: https://bing.ee123.net/img/rand?artid=146255934
image: https://bing.ee123.net/img/rand?artid=146255934
img: https://bing.ee123.net/img/rand?artid=146255934
---

# 深入探讨RAID 5的性能与容错能力：实验与分析(磁盘阵列)

#### 前言——

本实验旨在探讨 RAID 5 的性能和容错能力。通过创建 RAID 5 阵列并进行一系列读写性能测试及故障模拟，我们将观察 RAID 5 在数据冗余和故障恢复方面的表现，以验证其在实际应用中的可靠性和效率。

首先说明：最少三块硬盘, 使用 4 块硬盘确实能获得更好的性能和更大的存储空间。让我们用 4 块硬盘（nvme0n2 到 nvme0n5）来构建 RAID 5。

**最少三块，我这边用四块做实验。**

你的硬盘类型可能和我的不一样 不过没有关系不影响操作实验

#### 实验环境说明

* 系统盘：nvme0n1（不参与 RAID）
* 可用磁盘：

* + nvme0n2（20GB）
  + nvme0n3（20GB）
  + nvme0n4（20GB）
  + nvme0n5（20GB）

![](https://i-blog.csdnimg.cn/img_convert/ed8f60330a221e6a01639244c497b426.png)

#### 实验步骤

##### 1.确认安装 mdadm

```
sudo yum install mdadm -y
```

##### 2.清理现有分区

（因为已经有分区，需要先清理）清楚分区会删除数据 慎重操作

```
# 删除现有分区
sudo fdisk /dev/nvme0n2
# 输入 d 删除现有分区
# 输入 w 保存并退出

# 对其他磁盘重复相同操作
sudo fdisk /dev/nvme0n3
sudo fdisk /dev/nvme0n4
sudo fdisk /dev/nvme0n5
```

##### 3.创建新的 RAID 分区

对每个磁盘创建新分区：

```
sudo fdisk /dev/nvme0n2
```

**在 fdisk 中**
：

1. 输入
   `n`
   创建新分区
2. 选择
   `p`
   创建主分区
3. 分区号按默认（1）
4. 起始扇区按默认
5. 结束扇区输入
   `+15G`
   （给每个磁盘分配15GB用于RAID）
6. 输入
   `t`
   修改分区类型
7. 输入
   `fd`
   设置为 Linux RAID
8. 输入
   `w`
   保存并退出

**按照顺序来 具体分配多少空间根据你的需求**

对 nvme0n3、nvme0n4、nvme0n5 重复相同操作。

##### 4.创建 RAID 5 阵列

```
sudo mdadm -Cv /dev/md0 -l 5 -n 4 /dev/nvme0n2p1 /dev/nvme0n3p1 /dev/nvme0n4p1 /dev/nvme0n5p1
```

给大家介绍一下每一个参数的作用 （
**不理解的话做出来也没有用**
）

##### 命令分解

* `mdadm`
  : 用于管理Linux软件RAID设备的工具。
* `-C`
  : 创建一个新的RAID设备。
* `-v`
  : 显示详细输出，便于调试和查看进程。
* `/dev/md0`
  : 创建的RAID设备的名称，表示第一个RAID设备。
* `-l 5`
  :
  **指定RAID级别为5**
  。RAID 5使用条带化和奇偶校验，提供较好的读性能和容错能力。
* `-n 4`
  :
  **指定RAID阵列中的磁盘数量为4**
  。这意味着RAID 5将使用4个设备来存储数据和奇偶校验信息。
* `/dev/nvme0n2p1 /dev/nvme0n3p1 /dev/nvme0n4p1 /dev/nvme0n5p1`
  : 指
  **定参与RAID阵列的具体设备。这些是四个NVMe设备的分区。**

##### 5.查看 RAID 状态

```
cat /proc/mdstat
```

![](https://i-blog.csdnimg.cn/img_convert/56db1b948c97292430919686e6c975e8.png)

**可能会有疑惑 不是刚格式化创建好的分区，并没有创建什么文件数据之类的，为什么还要同步数据**
：

* 在RAID阵列创建过程中，系统会将所有参与的设备视为一个整体，并进行必要的操作（如奇偶校验的计算和写入），这就是为什么你看到有恢复进度。

创建文件系统

```
sudo mkfs.xfs /dev/md0    #创建文件系统

log stripe unit (524288 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/md0               isize=512    agcount=16, agsize=491008 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1    bigtime=0 inobtcount=0
data     =                       bsize=4096   blocks=7856128, imaxpct=25
         =                       sunit=128    swidth=384 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=3840, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
```

创建挂载点并挂载

```
sudo mkdir /raiddata
sudo mount /dev/md0 /raiddata
```

设置开机自动挂载

```
echo '/dev/md0 /raiddata xfs defaults 0 0' | sudo tee -a /etc/fstab
```

验证挂载

```
df -hT
```

对于四个10GB的磁盘，实际可用空间计算为： 可用空间=(磁盘数量−1)×每个磁盘的容量可用空间=(磁盘数量−1)×每个磁盘的容量 可用空间=(4−1)×10GB=30GB可用空间=(4−1)×10GB=30GB

![](https://i-blog.csdnimg.cn/img_convert/2a03ff79ad79f3f5a727643658f8bf49.png)

##### 6.如果需要删除 RAID

如果配置出错需要重来，可以使用以下命令：

```
bash
复制
# 停止 RAID
sudo mdadm --stop /dev/md0

# 清除超级块
sudo mdadm --zero-superblock /dev/nvme0n2p1
sudo mdadm --zero-superblock /dev/nvme0n3p1
sudo mdadm --zero-superblock /dev/nvme0n4p1
sudo mdadm --zero-superblock /dev/nvme0n5p1

# 从 fstab 中删除挂载项
sudo sed -i '/\/dev\/md0/d' /etc/fstab
```

#### 性能测试实验

设计一系列实验来测试 RAID 5 的性能和容错能力。这些测试将展示 RAID 5 的主要特性：读写性能、数据冗余和故障恢复能力

**这些测试将展示**
：

1. RAID 5 的读写性能
2. 数据冗余和容错能力
3. 在磁盘故障时的系统行为
4. 重建过程的自动化

#### 实验一：基础性能测试

首先创建一个测试文件

```
dd if=/dev/zero of=/raiddata/testfile bs=1M count=1024
```

这会创建一个 1GB 的测试文件

测试读取速度

```
dd if=/raiddata/testfile of=/dev/null bs=1M
```

测试写入速度

```
dd if=/dev/zero of=/raiddata/testfile2 bs=1M count=1024 conv=fdatasync
```

![](https://i-blog.csdnimg.cn/img_convert/372cbd48c9ab31d616983c284fd0341e.png)

#### 实验二：容错性测试（模拟磁盘故障）

1. 首先创建一些重要数据

```
# 创建测试目录
mkdir /raiddata/important_data

# 创建一些测试文件
for i in {1..5}; do
    dd if=/dev/urandom of=/raiddata/important_data/file$i bs=1M count=100
done

# 计算文件的 MD5 值以便后续验证
[root@Centos8 ~]# md5sum /raiddata/important_data/* > /root/checksums.txt
[root@Centos8 ~]# cat /root/checksums.txt 
6640f67939d03071c27bda6ba5cf3c3d  /raiddata/important_data/file1
f5d97e43cb0678fe4f07f2a2221912ac  /raiddata/important_data/file2
16c63489a2ab7a15b857d6a265f33cdd  /raiddata/important_data/file3
af4cde80c4affa9480a56cf7887466dc  /raiddata/important_data/file4
3cadc5fc4dbd6a21d5d0d4e73374a306  /raiddata/important_data/file5
```

![](https://i-blog.csdnimg.cn/img_convert/162cc12b4cca4f11634c809d932491e7.png)

模拟磁盘故障

```
# 查看当前 RAID 状态
cat /proc/mdstat

# 标记其中一个磁盘为故障
sudo mdadm /dev/md0 --fail /dev/nvme0n2p1

# 查看 RAID 状态
cat /proc/mdstat
mdadm --detail /dev/md0
```

![](https://i-blog.csdnimg.cn/img_convert/769e34f7e42ab63d4a91ff1e1100ffe5.png)

验证数据完整性

```
# 验证之前创建的文件是否仍然可以访问
ls -l /raiddata/important_data/

# 验证文件内容是否完整
md5sum -c /root/checksums.txt
```

![](https://i-blog.csdnimg.cn/img_convert/dc3e677c304b8b811d0a2b4ab5c52c2c.png)

#### 实验三：重建测试

移除故障磁盘

```
sudo mdadm /dev/md0 --remove /dev/nvme0n2p1
```

添加新磁盘（假设我们修复了原来的磁盘）

```
sudo mdadm /dev/md0 --add /dev/nvme0n2p1
```

观察重建过程

```
# 实时监控重建进度
watch -n 1 cat /proc/mdstat
```

#### 实验四：性能测试

1. 安装性能测试工具

```
bash
复制
sudo yum install fio -y
```

1. 运行综合性能测试

```
bash
复制
# 创建测试配置文件
cat > raid_test.fio << EOF
[global]
ioengine=libaio
direct=1
group_reporting
time_based
runtime=60

[sequential-read]
stonewall
rw=read
size=1g
directory=/raiddata
bs=1M

[sequential-write]
stonewall
rw=write
size=1g
directory=/raiddata
bs=1M

[random-read]
stonewall
rw=randread
size=1g
directory=/raiddata
bs=4k

[random-write]
stonewall
rw=randwrite
size=1g
directory=/raiddata
bs=4k
EOF

# 运行测试
fio raid_test.fio
```

执行完成脚本等待性能测试 然后输出各种参数和结果

![](https://i-blog.csdnimg.cn/img_convert/1c3f87f4e2e0291421ee9fa1fcb1ea0d.png)

#### 实验总结

通过本实验，我们成功验证了 RAID 5 的高效读写性能和良好的容错能力。在模拟磁盘故障的情况下，数据仍然保持完整，且重建过程顺利进行。这表明 RAID 5 是一种可靠的存储解决方案，适合需要高可用性和数据安全性的场景。