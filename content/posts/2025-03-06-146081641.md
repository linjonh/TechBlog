---
layout: post
title: "机器学习六"
date: 2025-03-06 22:24:57 +0800
description: "大一小白的机器学习笔记（六）"
keywords: "机器学习（六）"
categories: ['未分类']
tags: ['机器学习', '人工智能']
artid: "146081641"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146081641
    alt: "机器学习六"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146081641
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146081641
cover: https://bing.ee123.net/img/rand?artid=146081641
image: https://bing.ee123.net/img/rand?artid=146081641
img: https://bing.ee123.net/img/rand?artid=146081641
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     机器学习（六）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h5>
     一，决策树：
    </h5>
    <p>
     <img alt="" height="457" src="https://i-blog.csdnimg.cn/direct/0ad946d983cb4809b5068c19ea443566.png" width="744"/>
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     简介：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     决策树是一种通过构建类似树状的结构（颠倒的树），从根节点开始逐步对数据进行划分，最终在叶子节点做出预测结果的模型。
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     结构组成：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     根节点：初始的数据集全集，没有经过任何划分（最顶层的部分）
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     内部节点：代表对某个特征的测试，根据特征值将数据划分为子节点（中间部分）
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     叶子节点：代表最终的分类结果或回归值（最底层的部分）
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     学习过程：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①选择根节点：找到一个明显的特征，能将数据最纯净地分为两组
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②递归：对每个子节点重复上述操作，直到所有样本都同属于一类（完全纯净） or 没有更多的特征可用 or 达到的预设的树的深度（防止过拟合）
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     测量分类纯度（以区分猫狗为例）：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①信息熵：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="460" src="https://i-blog.csdnimg.cn/direct/b1965ef1957b45ee95ce0c84bd13e2cf.png" width="493"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     熵(H)的函数图像如上图所示，当区分后的样本全为猫或狗时，熵为0；当样本中猫和狗的比例越接近于1时，熵越大。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     熵的函数：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="158" src="https://i-blog.csdnimg.cn/direct/43d828666c044e4b9ca7e5242014dfc7.png" width="486"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Pi是数据集中第i类样本的比例
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     当所有样本属于同一类时（完全纯净），熵为0；当样本类别均匀分布时，熵最大。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②基尼指数：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     基尼指数衡量随机抽取的两个样本类别不一致的概率，数值越小纯度越高
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     公式：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="150" src="https://i-blog.csdnimg.cn/direct/83f4b47481764f24916602dfa042ec6d.png" width="364"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     当数据集完全纯净时，基尼指数为0；当数据集类别分布均匀时，基尼指数为0.5
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ③分类误差：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     分类误差表示使用多数类作为标签时误分类的概率
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     公式：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="78" src="https://i-blog.csdnimg.cn/direct/ba990bf0e2bb4d99a108c684cfade8e4.png" width="666"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     完全纯净时误差为0，二分类均匀分布时误差为0.5
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     三者比较：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="342" src="https://i-blog.csdnimg.cn/direct/33249e27db3244b3b402ebb4144b747a.png" width="875"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     计算效率：基尼指数=分类误差&gt;信息熵
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     敏感度：信息熵&gt;基尼指数&gt;分类误差
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     三者在决策树中的应用：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     信息增益（熵）：通过父节点减去子节点熵的加权平均值来选择增益最大的特征
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     基尼指数：类似信息增益，选择使基尼指数下降最多的分割
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     信息增益的实现：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     信息增益：在构建决策树过程中，使用某个特征对数据集实现分割后，学习熵的减少程度。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     核心：选择使信息增益最大的特征进行分割（即使学习熵减少程度最大），最大程度上纯化子节点
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="388" src="https://i-blog.csdnimg.cn/direct/b112a067c9b24e99ace2885e081da5cd.png" width="1107"/>
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     实现步骤：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①计算父节点的熵：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②按照特征分割，计算子节点的熵
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ③计算加权子节点的熵
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ④计算信息增益
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     举例：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     假设父节点中有10个样本，6个属于A类，4个属于B类
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①计算父节点的熵：代入信息熵的公式得到父节点的熵为0.971
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②按特征分割数据，计算子节点的熵：用特征将这10个样本分为两个子节点，每个子节点有5个样本，分别计算两个子节点的熵。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     子节点1（4A1B）：熵为0.722
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     子节点2（2A3B）：熵为0.971
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ③计算加权子节点熵：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     加权熵=（5/10）*子节点1熵+（5/10）*子节点2熵=0.8465
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ④计算信息增益：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     信息增益=父节点熵-加权子节点熵=0.1245
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     构建决策树：
    </h6>
    <p>
     ①数据准备和预处理：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     从决策树的根结点开始，对数据进行数据清洗和特征处理或标签编码
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②分割：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     通过选择该特征后提供的最高信息增益的特性进行分割
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ③创建树的分支：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     根据所选特性将数据集拆分成两个子集，并创建树的分支，后将数据集划分给分支
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ④递归：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     对每个子节点重复上述操作，直到满足停止条件
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     停止条件：
    </p>
    <ol>
     <li style="text-align:justify">
      节点纯度达标
     </li>
     <li style="text-align:justify">
      达到预设的最大树深
     </li>
     <li style="text-align:justify">
      继续分割的信息增益低于阈值（无显著信息增益）
     </li>
     <li style="text-align:justify">
      子节点的样本数小于阈值（样本数过少）
     </li>
    </ol>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     One-Hot编码：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     One-Hot编码是将包含K个类别的离散特征转换为K个二元特征，常用在构建决策树时，处理多类别特征。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     举例：原始特征“颜色”包含三个类别：红，蓝，绿
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     经过One-Hot编码后生成三个新特征：是不是红色，是不是蓝色，是不是绿色
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     优点：可以将多类别特征转换为二元特征，每个特征仅对应一个类别，模型可以更灵活的选择分割点；可以避免算法对多取值特征的偏好
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     缺点：
    </p>
    <p style="text-align:justify">
     1.增加特征数量，增加计算负担
    </p>
    <p style="text-align:justify">
     2.降低单个特征的重要性，使得信息分散
    </p>
    <p style="text-align:justify">
     3.过拟合风险
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     拆分连续值的决策树分支：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     连续值的分割目标是找到某个阈值，将数据集分为两个子集，使得分割后的子集纯度最高。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     实现步骤：
    </p>
    <ol>
     <li style="text-align:justify">
      排序连续特征值
     </li>
     <li style="text-align:justify">
      根据排完序的特征值点生成候选分割点
     </li>
     <li style="text-align:justify">
      计算每个分割点之间的纯度
     </li>
     <li style="text-align:justify">
      对比执行后分割
     </li>
    </ol>
    <p style="text-align:justify">
    </p>
    <h5 style="text-align:justify">
     <strong>
      二，回归树模型：
     </strong>
    </h5>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     回归树是一种用来预测连续数值的决策树模型，如用来预测房价，温度......。与分类树不同，分类树预测的是类别，而回归树预测的是连续的数值。
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     实现原理：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     从根节点到叶节点，分而治之，将数据集划分为多个小区域，每个区域内的样本数尽可能相似，直到每个小区域足够纯净
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     拆分步骤：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①遍历所有特征
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②遍历每个特征的所有可能分割点：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     若为连续特征：尝试相邻值的中间点；若为离散特征，尝试每个类别
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ③选择最优分割点
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     衡量指标：均方误差（EMS）
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     均方误差是衡量一个区域内样本的数值差异的程度，均方误差越小，该区域内样本越相似。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="446" src="https://i-blog.csdnimg.cn/direct/6a052c8f002b4f5d888c52f0cb5999e7.png" width="918"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     通过计算分割后的左右子区域的均方误差和，选择使总均方误差和最小的分割方式。
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     回归树的构建流程：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     从根节点开始，遍历所有特征---&gt; 遍历分割点，选择使总均方误差最小的分割方式---&gt;生成子节点，按分割条件将数据分为左右两个部分 ---&gt; 递归处理子节点，重复上述步骤 ---&gt; 达到停止条件（回归树达到预测树深||区域内样本数过少||均方误差下降不明显）
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     代码实现及运行结果：
    </h6>
    <p>
     <img alt="" height="569" src="https://i-blog.csdnimg.cn/direct/524ae0bb3d7a46ae983d5ed4d572e9d5.png" width="1106"/>
    </p>
    <pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor
import pandas as pd

# 示例数据：房屋面积（㎡）和价格（万元）
data = pd.DataFrame({
    '面积': [80, 120, 100, 90, 150],
    '价格': [300, 450, 400, 350, 500]
})

# 训练回归树模型
model = DecisionTreeRegressor(max_depth=2)  # 限制树深度为2
model.fit(data[['面积']], data['价格'])

# 预测新样本
new_house = pd.DataFrame({'面积': [110]})
predicted_price = model.predict(new_house)
print(f"预测价格：{predicted_price[0]:.1f}万元")  # 输出：预测价格：425.0万元</code></pre>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     决策树模型中的放回抽样：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     从原始的数据集中有放回地随机抽取样本的方法，每次抽取时，每个样本被选中的概率相同，而且可能被重复选中。每个训练子集的样本数与原始数据集相同
    </p>
    <h5 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
    </h5>
    <h5 style="text-align:justify">
     <strong>
      三，随机森林算法：
     </strong>
    </h5>
    <p>
     <strong>
     </strong>
     随机森林算法是一种通过组合多个决策树的算法（集成树）。
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     集成方法：
    </h6>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①Bagging：通过自助采样，从原始数据集中生成多个子集，每个子集训练一棵决策树
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②随机特征选择：每棵树节点分裂时，随机选择部分特征作为候选，降低树之间的相关性，防止过拟合
    </p>
    <h6 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     训练过程：
    </h6>
    <ol>
     <li style="text-align:justify">
      有放回地抽样，从原始数据集中随机抽取N个样本作为训练集
     </li>
     <li style="text-align:justify">
      用未参与训练的样本验证模型，无需额外的验证集
     </li>
     <li style="text-align:justify">
      特征抽样：每个节点分裂时，从全部特征中随机选取子集
     </li>
     <li style="text-align:justify">
      每棵树生长到最大深度||叶子节点纯度达标时停止
     </li>
    </ol>
    <h5 style="text-align:justify">
     <strong>
      四，XGBoost：（类似贪心算法）
     </strong>
    </h5>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     XGBoost是一种基于梯度提升树的集成算法，通过加法模型和前向分布算法逐步优化目标函数。每次训练后，把侧重点放在先前训练过但拟合效果不好的决策树上。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     第一次循环：通过已有的训练集开始训练决策树
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     第二-N次循环：根据前一次循环产生的决策树的训练效果，侧重于选择训练不好的决策树，来进行训练。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     XGBoost优点：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ①可以选择默认拆分标准和停止标准
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ②内置正则化防止过拟合
    </p>
    <h5 style="text-align:justify">
     五，决策树 VS 神经网络：
    </h5>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     决策树：适用于数据量小，特征明显，训练速度快的场景，例如分群，评分。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     神经网络：适用于数据量大，数据种类多样（图像，音频，文件），的场景。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="596" src="https://i-blog.csdnimg.cn/direct/be6aca09498d4ba1a1af444fb8cb22ea.png" width="1107"/>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f436f6d6d756e69737431392f:61727469636c652f64657461696c732f313436303831363431" class_="artid" style="display:none">
 </p>
</div>


