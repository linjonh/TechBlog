---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f436f6d6d756e69737431392f:61727469636c652f64657461696c732f313436303831363431"
layout: post
title: "机器学习六"
date: 2025-03-06 22:24:57 +08:00
description: "大一小白的机器学习笔记（六）"
keywords: "适用于数据量较小的经典机器学习算法"
categories: ['未分类']
tags: ['机器学习', '人工智能']
artid: "146081641"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146081641
    alt: "机器学习六"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146081641
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146081641
cover: https://bing.ee123.net/img/rand?artid=146081641
image: https://bing.ee123.net/img/rand?artid=146081641
img: https://bing.ee123.net/img/rand?artid=146081641
---

# 机器学习（六）

##### 一，决策树：

![](https://i-blog.csdnimg.cn/direct/0ad946d983cb4809b5068c19ea443566.png)

###### 简介：

决策树是一种通过构建类似树状的结构（颠倒的树），从根节点开始逐步对数据进行划分，最终在叶子节点做出预测结果的模型。

###### 结构组成：

根节点：初始的数据集全集，没有经过任何划分（最顶层的部分）

内部节点：代表对某个特征的测试，根据特征值将数据划分为子节点（中间部分）

叶子节点：代表最终的分类结果或回归值（最底层的部分）

###### 学习过程：

①选择根节点：找到一个明显的特征，能将数据最纯净地分为两组

②递归：对每个子节点重复上述操作，直到所有样本都同属于一类（完全纯净） or 没有更多的特征可用 or 达到的预设的树的深度（防止过拟合）

###### 测量分类纯度（以区分猫狗为例）：

①信息熵：

![](https://i-blog.csdnimg.cn/direct/b1965ef1957b45ee95ce0c84bd13e2cf.png)

熵(H)的函数图像如上图所示，当区分后的样本全为猫或狗时，熵为0；当样本中猫和狗的比例越接近于1时，熵越大。

熵的函数：

![](https://i-blog.csdnimg.cn/direct/43d828666c044e4b9ca7e5242014dfc7.png)

Pi是数据集中第i类样本的比例

当所有样本属于同一类时（完全纯净），熵为0；当样本类别均匀分布时，熵最大。

②基尼指数：

基尼指数衡量随机抽取的两个样本类别不一致的概率，数值越小纯度越高

公式：

![](https://i-blog.csdnimg.cn/direct/83f4b47481764f24916602dfa042ec6d.png)

当数据集完全纯净时，基尼指数为0；当数据集类别分布均匀时，基尼指数为0.5

③分类误差：

分类误差表示使用多数类作为标签时误分类的概率

公式：

![](https://i-blog.csdnimg.cn/direct/ba990bf0e2bb4d99a108c684cfade8e4.png)

完全纯净时误差为0，二分类均匀分布时误差为0.5

三者比较：

![](https://i-blog.csdnimg.cn/direct/33249e27db3244b3b402ebb4144b747a.png)

计算效率：基尼指数=分类误差>信息熵

敏感度：信息熵>基尼指数>分类误差

###### 三者在决策树中的应用：

信息增益（熵）：通过父节点减去子节点熵的加权平均值来选择增益最大的特征

基尼指数：类似信息增益，选择使基尼指数下降最多的分割

###### 信息增益的实现：

信息增益：在构建决策树过程中，使用某个特征对数据集实现分割后，学习熵的减少程度。

核心：选择使信息增益最大的特征进行分割（即使学习熵减少程度最大），最大程度上纯化子节点

![](https://i-blog.csdnimg.cn/direct/b112a067c9b24e99ace2885e081da5cd.png)

###### 实现步骤：

①计算父节点的熵：

②按照特征分割，计算子节点的熵

③计算加权子节点的熵

④计算信息增益

举例：

假设父节点中有10个样本，6个属于A类，4个属于B类

①计算父节点的熵：代入信息熵的公式得到父节点的熵为0.971

②按特征分割数据，计算子节点的熵：用特征将这10个样本分为两个子节点，每个子节点有5个样本，分别计算两个子节点的熵。

子节点1（4A1B）：熵为0.722

子节点2（2A3B）：熵为0.971

③计算加权子节点熵：

加权熵=（5/10）\*子节点1熵+（5/10）\*子节点2熵=0.8465

④计算信息增益：

信息增益=父节点熵-加权子节点熵=0.1245

###### 构建决策树：

①数据准备和预处理：

从决策树的根结点开始，对数据进行数据清洗和特征处理或标签编码

②分割：

通过选择该特征后提供的最高信息增益的特性进行分割

③创建树的分支：

根据所选特性将数据集拆分成两个子集，并创建树的分支，后将数据集划分给分支

④递归：

对每个子节点重复上述操作，直到满足停止条件

停止条件：

1. 节点纯度达标
2. 达到预设的最大树深
3. 继续分割的信息增益低于阈值（无显著信息增益）
4. 子节点的样本数小于阈值（样本数过少）

###### One-Hot编码：

One-Hot编码是将包含K个类别的离散特征转换为K个二元特征，常用在构建决策树时，处理多类别特征。

举例：原始特征“颜色”包含三个类别：红，蓝，绿

经过One-Hot编码后生成三个新特征：是不是红色，是不是蓝色，是不是绿色

优点：可以将多类别特征转换为二元特征，每个特征仅对应一个类别，模型可以更灵活的选择分割点；可以避免算法对多取值特征的偏好

缺点：

1.增加特征数量，增加计算负担

2.降低单个特征的重要性，使得信息分散

3.过拟合风险

###### 拆分连续值的决策树分支：

连续值的分割目标是找到某个阈值，将数据集分为两个子集，使得分割后的子集纯度最高。

实现步骤：

1. 排序连续特征值
2. 根据排完序的特征值点生成候选分割点
3. 计算每个分割点之间的纯度
4. 对比执行后分割

##### **二，回归树模型：**

回归树是一种用来预测连续数值的决策树模型，如用来预测房价，温度......。与分类树不同，分类树预测的是类别，而回归树预测的是连续的数值。

###### 实现原理：

从根节点到叶节点，分而治之，将数据集划分为多个小区域，每个区域内的样本数尽可能相似，直到每个小区域足够纯净

###### 拆分步骤：

①遍历所有特征

②遍历每个特征的所有可能分割点：

若为连续特征：尝试相邻值的中间点；若为离散特征，尝试每个类别

③选择最优分割点

###### 衡量指标：均方误差（EMS）

均方误差是衡量一个区域内样本的数值差异的程度，均方误差越小，该区域内样本越相似。

![](https://i-blog.csdnimg.cn/direct/6a052c8f002b4f5d888c52f0cb5999e7.png)

通过计算分割后的左右子区域的均方误差和，选择使总均方误差和最小的分割方式。

###### 回归树的构建流程：

从根节点开始，遍历所有特征---> 遍历分割点，选择使总均方误差最小的分割方式--->生成子节点，按分割条件将数据分为左右两个部分 ---> 递归处理子节点，重复上述步骤 ---> 达到停止条件（回归树达到预测树深||区域内样本数过少||均方误差下降不明显）

###### 代码实现及运行结果：

![](https://i-blog.csdnimg.cn/direct/524ae0bb3d7a46ae983d5ed4d572e9d5.png)

```python
from sklearn.tree import DecisionTreeRegressor
import pandas as pd

# 示例数据：房屋面积（㎡）和价格（万元）
data = pd.DataFrame({
    '面积': [80, 120, 100, 90, 150],
    '价格': [300, 450, 400, 350, 500]
})

# 训练回归树模型
model = DecisionTreeRegressor(max_depth=2)  # 限制树深度为2
model.fit(data[['面积']], data['价格'])

# 预测新样本
new_house = pd.DataFrame({'面积': [110]})
predicted_price = model.predict(new_house)
print(f"预测价格：{predicted_price[0]:.1f}万元")  # 输出：预测价格：425.0万元
```

###### 决策树模型中的放回抽样：

从原始的数据集中有放回地随机抽取样本的方法，每次抽取时，每个样本被选中的概率相同，而且可能被重复选中。每个训练子集的样本数与原始数据集相同

##### 

##### **三，随机森林算法：**

随机森林算法是一种通过组合多个决策树的算法（集成树）。

###### 集成方法：

①Bagging：通过自助采样，从原始数据集中生成多个子集，每个子集训练一棵决策树

②随机特征选择：每棵树节点分裂时，随机选择部分特征作为候选，降低树之间的相关性，防止过拟合

###### 训练过程：

1. 有放回地抽样，从原始数据集中随机抽取N个样本作为训练集
2. 用未参与训练的样本验证模型，无需额外的验证集
3. 特征抽样：每个节点分裂时，从全部特征中随机选取子集
4. 每棵树生长到最大深度||叶子节点纯度达标时停止

##### **四，XGBoost：（类似贪心算法）**

XGBoost是一种基于梯度提升树的集成算法，通过加法模型和前向分布算法逐步优化目标函数。每次训练后，把侧重点放在先前训练过但拟合效果不好的决策树上。

第一次循环：通过已有的训练集开始训练决策树

第二-N次循环：根据前一次循环产生的决策树的训练效果，侧重于选择训练不好的决策树，来进行训练。

XGBoost优点：

①可以选择默认拆分标准和停止标准

②内置正则化防止过拟合

##### 五，决策树 VS 神经网络：

决策树：适用于数据量小，特征明显，训练速度快的场景，例如分群，评分。

神经网络：适用于数据量大，数据种类多样（图像，音频，文件），的场景。

![](https://i-blog.csdnimg.cn/direct/be6aca09498d4ba1a1af444fb8cb22ea.png)