---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f79696e797531393935303831312f:61727469636c652f64657461696c732f313436303438363337"
layout: post
title: "Scaling-Laws-for-Neural-Language-Models"
date: 2025-03-07 10:36:01 +0800
description: "调查大模型与模型结构，模型大小，算力，数据之间的关系。这种关系可以被更严格地定义成 Scaling Law，这是一个可以描述 LLM 的测试损失随某个量（如训练计算量）的增长而降低的公式。Scaling Law 可帮助我们预测当投入更多资源进行更大规模训练时的效果，这能给我们提供继续投资 scaling 的必要信心。如何合理的分配资源来达到更好的训练效果。问题：模型的形状（即层的数量和大小）重要吗？使模型更大是否有助于其表现更好？训练这些更大的模型需要多少数据匹配？"
keywords: "Scaling Laws for Neural Language Models"
categories: ['Llm']
tags: ['语言模型', '深度学习', '人工智能']
artid: "146048637"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146048637
    alt: "Scaling-Laws-for-Neural-Language-Models"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146048637
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146048637
cover: https://bing.ee123.net/img/rand?artid=146048637
image: https://bing.ee123.net/img/rand?artid=146048637
img: https://bing.ee123.net/img/rand?artid=146048637
---

# Scaling Laws for Neural Language Models

### Scaling Laws for Neural Language Models

#### 摘要：

调查大模型与模型结构，模型大小，算力，数据之间的关系。这种关系可以被更严格地定义成 Scaling Law，这是一个可以描述 LLM 的测试损失随某个量（如训练计算量）的增长而降低的公式。Scaling Law 可帮助我们预测当投入更多资源进行更大规模训练时的效果，这能给我们提供继续投资 scaling 的必要信心。如何合理的分配资源来达到更好的训练效果。

问题：
  
模型的形状（即层的数量和大小）重要吗？
  
使模型更大是否有助于其表现更好？
  
训练这些更大的模型需要多少数据匹配？
  
可以提前预测loss early stop吗？

结论：

* 模型性能主要与模型参数量N，数据集大小D，算力C相关，与模型参数（例如深度or宽度）无关。
* 当不受另外两个参数限制时，模型性能与N，D，C成幂律的关系。单独改变一个参数，性能变化是平缓的。
* 性能变化是可预测的，只要同时放大N与D的规模，如果N与D保持不变，另外一个增加，则会进入收益递减的状态。性能损失可预见的取决于该比例：

  N
  0.74
  /
  D
  N^{0.74}/D






  N










  0.74

  /

  D
  ， 如果增加模型大小8倍，需要同时增加数据大小5倍来避免惩罚（避免过拟合）。
* 训练曲线遵循可预测的幂律，其参数大致与模型大小无关，通过外推早期预测曲线，后期loss是大致可预测的。
* sample-efficient ：大模型比小模型更加sample-efficient，使用更少的train step和数据可以达到相关的效果。

也就是说模型的测试损失和模型参数量之间存在可测量的关系。其中一个量的变化将导致另一个量发生相对的、无关尺度的变化。换句话说，我们可基于这种关系了解到：增加模型参数量（假设已满足其他条件，比如训练数据充足）将导致测试损失降低某个可预测的程度。

* 当拥有更多计算资源时，应该如何分配：
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b0541573f49e402babe0733e740f368e.png)

### Test loss预测

在其他条件固定且充足的条件下，test loss是可预测的：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ee7ed7b7fead457195d3d90167494a53.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5494d120e194468aa764b8a23ecc3317.png)
  
LLM 的性能（就其在 WebText2 上的测试损失而言）会随着参数、数据和计算量的增加而稳步提高。这些趋势在计算量方面跨越了八个数量级，在模型大小方面跨越了六个数量级，在数据集大小方面跨越了两个数量级(横坐标)。上图提供了确切的幂律关系和拟合每个幂律关系的方程。

随着我们扩大参数

N
N





N
,

D
D





D
,

C
m
i
n
C\_{min}






C










min

​

，模型性能改变的程度成

α
N
\alpha\_{N}






α










N

​

，

α
D
\alpha\_{D}






α










D

​

,

α
C
m
i
n
\alpha\_{Cmin}






α










C

min

​

幂律变化。例如，将N增大2倍，那么loss缩小

2
−
α
N
2^{-\alpha\_N}






2










−


α









N

​

=0.95倍。而模型的性能与model shape，tansformer超参数是弱相关的。

#### 更大的模型具有更高的sample efficent

较大的 LLM 往往具有更高的样本效率，达到相同的测试效果相比小模型需要更少的数据。因此，对 LLM 进行预训练以使其收敛（可以说）不是最优的。相反，我们可以在较少的数据上训练更大的模型，在收敛之前停止训练过程。这种方法在训练计算使用量方面是最优的，但它没有考虑到推理成本。实际上通常会在更多数据上训练较小的模型，因为较小的模型推理成本较低。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/cfcd34861c204aada66aae353b82fb0b.png)
  
将等式1.1和1.2结合，可以得到单个等式（N与D，N与S）来预测loss走势，指导过拟合的程度（loss曲线平缓了就可以early stop 见图4）：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9278fc9fb4f440c9bd382db4f15cf4a2.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/af6cb0fbaeb845cb8530e7c002b7ffa2.png)

S：number of train steps：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/74de507e0d83449ea524519b3fdc93bb.png)
  
当使用固定的 compute budget

C
C





C
, 可以预测最优的

N
N





N
,

B
B





B
,

S
S





S
,

D
D





D
:
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a8a8ecbc674b4e8d8205b485dd9fb4cb.png)
  
可以得到：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/61f481843ce9435eb5431b2d5d09e990.png)

#### 性能与模型参数之间的关系：

1. 性能与模型shape 参数有较微弱的关系：
     
   ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/723bd417a4c34323b7931cea85532182.png)
2. 性能与模型参数之间的关系：

* 包含embedding参数：相同的参数下，效果与层数相关性巨大。
* 不包含embedding参数：相同的参数下，效果呈相同的趋势（与模型shape相关参数关系微弱）。
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/64be268084404f29b8de8669f14f9677.png)

3. 与LSTM比较：

* 比较LSTM和Transformer的性能与non-embedding参数计数N的关系。LSTM使用相同的数据集和上下文长度进行训练。从这些图中我们可以看出，LSTM在上下文早期出现的令牌中的性能与Transformer一样好，但无法与后期的Transformer性能相匹配。性能和上下文位置之间的幂律关系：其中较大模型的幂函数越来越大，表明快速识别模式的能力得到了提高。
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/17798cc8b3974ff591cc09fbcf48577b.png)

4. 训练时总共的non-embedding参数为：

   C
   =
   6
   N
   B
   S
   C=6NBS





   C



   =





   6

   NBS
   , 6是前向和反向传播的加和统计。所以对于给定C值，我们可以扫描不同的N值来找到最佳的性能。

#### Charting the Infinite Data Limit and Overfitting

* 我们将实证证明，经过最佳训练的测试损耗符合方程式（1.5）定律。这为我们提供了指导，告诉我们需要多少数据来训练越来越大的模型，同时控制过度拟合。
    
  -
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f6e71fb2077c49d5b1a839e315fbd08f.png)
  - 该公式的3个原则

1. 词汇量或标记化的变化预计将按总体因素重新调整损失。L（N，D）（以及所有损失模型）的参数化必须自然地允许这种重新缩放。
2. 固定D，放大N到无穷，整体loss接近

   O
   (
   D
   )
   O(D)





   O

   (

   D

   )
   。同理，固定N，放大D到无穷，整体loss接近

   O
   (
   N
   )
   O(N)





   O

   (

   N

   )
   。

#### Results for L ( N , S m i n ) L(N,S\_{min}) L ( N , S min ​ ) and Performance with Model Size and Compute

不同预算值都有最佳的模型大小；
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8949a145f08a4020a5600e8984410f3f.png)

#### 实际意义

大规模预训练非常好，但这一事实却带来了一些困境。为了得到最好的模型，需要大量数据进行大规模模型训练。然而，这些训练成本很高，这意味着它们也会带来很大的风险。如果我们花费了 1000 万美元，结果训练了一个不符合我们期望的模型，这可如何是好？考虑到预训练的费用，我们无法执行任何特定于模型的调整，我们必须确保我们训练的模型表现良好。我们需要制定一个策略来调整这些模型并预测它们的性能，同时无需花费太多钱。

参考：

* 2020- Scaling Laws for Neural Language Models
* 万字长文解读Scaling Law的一切，洞见LLM的未来： https://baijiahao.baidu.com/s?id=1822931723327422180&wfr=spider&for=pc