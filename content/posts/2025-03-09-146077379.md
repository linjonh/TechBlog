---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f736f6172696e675f63617369612f:61727469636c652f64657461696c732f313436303737333739"
layout: post
title: "碰撞率降低57.4VLM-AD显著提升自动驾驶规划准确性,无需VLM实时推理"
date: 2025-03-09 19:23:08 +0800
description: "端到端自动驾驶（AD）将感知、预测和规划整合到一个框架中，旨在协调检测、跟踪等复杂任务。近期方法通过传感器数据生成自我轨迹，但面对复杂场景时性能下降。人类驾驶员通过推理环境有效应对挑战，而现有模型依赖于轨迹点序列监督，缺乏推理信息。手动标注推理信息成本高且耗时，难以获得高质量标注。大型基础模型如视觉-语言模型（VLMs）提供了替代方案，增强了驾驶系统的推理能力。然而，直接整合这些模型需要大量微调，并增加训练和推理时间，使其不适用于实际应用。"
keywords: "碰撞率降低57.4%！VLM-AD显著提升自动驾驶规划准确性，无需VLM实时推理"
categories: ['未分类']
tags: ['自动驾驶']
artid: "146077379"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146077379
    alt: "碰撞率降低57.4VLM-AD显著提升自动驾驶规划准确性,无需VLM实时推理"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146077379
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146077379
cover: https://bing.ee123.net/img/rand?artid=146077379
image: https://bing.ee123.net/img/rand?artid=146077379
img: https://bing.ee123.net/img/rand?artid=146077379
---

# 碰撞率降低57.4%！VLM-AD显著提升自动驾驶规划准确性，无需VLM实时推理

> 人类驾驶员靠常识推理来导航多样且动态的现实世界场景。现有端到端（E2E）自动驾驶（AD）模型通常被优化以模仿数据中观察到的驾驶模式，没有捕捉到背后的推理过程。这一限制阻碍了它们处理具有挑战性的驾驶场景的能力。为了缩小差距，本文提出了VLM-AD，一种利用视觉语言模型（VLMs）作为教师的方法，通过提供包含非结构化推理信息和结构化动作标签的额外监督来增强训练。这种监督增强了模型学习更丰富的特征表示的能力，捕捉到驾驶模式背后的基本原理。 本文的方法在推理过程中不需要使用VLM，使其适用于实时部署。当与最先进的方法结合时，VLM-AD在nuScenes数据集上实现规划准确性的显著提高，降低了碰撞率。

©️【深蓝AI】编译

*论文标题：VLM-AD: End-to-End Autonomous Driving through Vision-Language Model Supervision*

*论文作者：Yi Xu, Yuxin Hu, Zaiwei Zhang, Gregory P. Meyer, Siva Karthik Mustikovela, Siddhartha Srinivasa, Eric M. Wolff, Xin Huang*

*论文地址：https://arxiv.org/pdf/2412.14446*

*作者单位：Cruise LLC ，美国东北大学*

## 01 介绍

端到端自动驾驶（AD）将感知、预测和规划整合到一个框架中，旨在协调检测、跟踪等复杂任务。近期方法通过传感器数据生成自我轨迹，但面对复杂场景时性能下降。人类驾驶员通过推理环境有效应对挑战，而现有模型依赖于轨迹点序列监督，缺乏推理信息。

手动标注推理信息成本高且耗时，难以获得高质量标注。大型基础模型如视觉-语言模型（VLMs）提供了替代方案，增强了驾驶系统的推理能力。然而，直接整合这些模型需要大量微调，并增加训练和推理时间，使其不适用于实际应用。

为解决这些问题，本文提出VLM-AD，利用VLMs自动生成基于推理的文本标注作为额外监督信号，丰富特征表示。具体来说，给定多视角图像和未来轨迹，本文将未来轨迹投影到前视图上，并使用VLM模型生成自由形式和结构化响应，融入关键知识。

这种方法使本文能够创建富含VLM生成注释的数据集，解决了现有数据集中推理信息不足的问题。本文设计了辅助任务，无缝集成到现有的端到端模型中，提升模型学习更丰富特征的能力，而无需在推理时使用VLM。本文的贡献包括从VLMs中提炼驾驶推理知识进入AD管道，并设计两个即插即用的辅助任务，通过非结构化的自由文本和结构化动作标签进行监督，提升规划性能，无需VLM微调或推理时使用。 本文的主要贡献在于以下几个方面：

* 本文提出了VLM-AD，一种简单而有效的方法，通过基于推理的行为文本注释的高质量数据集，从VLMs中提炼出驾驶推理知识进入端到端AD管道。
* 本文设计了两种即插即用的辅助任务，通过非结构化的自由文本和结构化动作标签对现有的端到端AD管道进行监督。这些任务实现了VLM知识的有效提炼，引导模型学习更丰富的特征表示以提升规划性能，而无需VLM微调或推理时使用。
* 在nuScenes数据集上的广泛实验验证了本文提出方法的有效性，显示L2规划误差分别提高了14.6%和33.3%，UniAD和VAD的碰撞率分别降低了38.7%和57.4%。

![](https://i-blog.csdnimg.cn/img_convert/d092208176037b519f5087bab585a7ff.png)

▲图1｜ VLM-AD在训练过程中使用辅助文本预测任务来增强任意端到端驾驶模型。这些任务从VLM中提炼驾驶推理知识，以鼓励模型学习更丰富的表示，而无需在训练时对VLM进行微调或在推理时需要使用VLM©️【深蓝AI】编译

## 02 相关工作

### 2.1. 端到端自动驾驶

端到端自动驾驶系统将所有模块联合训练以实现统一目标，从而在整个流程中减少信息损失。统一框架如ST-P3和UniAD提出了基于视觉的端到端AD系统，统一了感知、预测和规划。这些模型在开放循环nuScenes数据集上取得了最先进的结果。后续工作如VAD和VADv2引入了矢量化编码方法以实现高效的场景表示，并扩展到CARLA上的闭环模拟。最近的方法如Ego-MLP、BEV-Planner和PARA-Drive探索了自我状态和模块堆栈中的新设计空间，以进一步提升驾驶性能。尽管端到端驾驶模型展示了有希望的结果，但它们主要优化为模仿数据中的驾驶模式，而未捕捉背后的推理过程。这一局限主要是由于现有数据集中缺乏推理信息，导致这些方法难以获取深层次的推理知识，限制了其在挑战性场景中的表现。

### 2.2. 自动驾驶的基础模型

基础模型，包括大型语言模型（LLMs）和视觉-语言模型（VLMs），正越来越多地应用于自动驾驶，以利用其先进的推理能力。GPT-Driver和Driving-with-LLMs使用LLMs提供带有解释的动作建议，增强决策透明度。最近的方法利用LLMs评估车道占用和安全性，实现了更类似人类的直观场景理解。然而，基于LLM的方法主要依赖于语言输入，这限制了其整合驾驶所需丰富视觉特征的潜力。VLMs通过整合语言和视觉进行多模态推理来解决这一问题，支持场景理解和数据生成等任务。VLMs也被用于统一导航和规划以及端到端自动驾驶。然而，现有的基于VLM的方法通常需要广泛的领域特定微调，显著增加了计算成本和推理延迟。与本文的方法密切相关的端到端自动驾驶中的VLP 将地面真实轨迹和边界框标签转换为文本特征进行对比学习，但它没有引入超出现有监督标签的信息。相比之下，本文的方法利用VLMs提供额外的推理信息以进一步增强驾驶性能。

### 2.3. 多任务学习

多任务学习通过共享表示同时执行多个相关任务，通常通过单独的分支或头部实现。这种方法利用共享的领域知识，增强了特征的鲁棒性和泛化能力，使其非常适合端到端自动驾驶。在AD系统中，常见的辅助任务包括语义分割、深度估计、高清地图和BEV分割，以提取有意义的感知表示供后续对象使用。除了视觉任务外，其他方法还预测额外的交通灯状态或控制信号，以提高驾驶性能。受多任务学习成功的启发，本文设计了新的辅助任务，通过高质量的VLM推理注释鼓励模型学习更丰富的特征表示，最终实现更可靠的规划性能。

## 03 本文方法

![](https://i-blog.csdnimg.cn/img_convert/bd36c00669feddf20416985ff53a07e2.png)

▲图2｜ 本文提出的VLM-AD框架©️【深蓝AI】编译

图2展示了本文提出的VLM-AD框架的概述，该框架由两个主要组件组成。第一个是注释分支，本文在这里利用VLM生成额外的信息，创建一个作为监督的补充数据集。第二个组件是设计的辅助头，旨在与这一额外的监督对齐，并可以有效地集成到任何跟随规划模块的端到端模型中。

### 3.1. VLM文本注释

![](https://i-blog.csdnimg.cn/img_convert/5274462c92ce23e42005d9803b7003b2.png)

▲图3｜ 本文提出的VLM-AD框架©️【深蓝AI】编译

图3展示了注释过程，本文在这里利用一个VLM作为教师，通过其从视觉输入中获取的推理能力为数据集增加额外信息，从而加深端到端模型对驾驶行为的理解。注释过程可以定义为：

![\mathcal{A} = \mathcal{M}(\mathcal{P}, \mathcal{V}),](https://latex.csdn.net/eq?%5Cmathcal%7BA%7D%20%3D%20%5Cmathcal%7BM%7D%28%5Cmathcal%7BP%7D%2C%20%5Cmathcal%7BV%7D%29%2C)

其中
![\mathcal{M}(\cdot)](https://latex.csdn.net/eq?%5Cmathcal%7BM%7D%28%5Ccdot%29)
表示VLM模型，
![\mathcal{P}](https://latex.csdn.net/eq?%5Cmathcal%7BP%7D)
表示语言提示，
![\mathcal{V}](https://latex.csdn.net/eq?%5Cmathcal%7BV%7D)
是模型的自然语言输出，作为数据集的注释。本文的目标是提供从自车摄像头捕获的图像，以及专门设计的提示，以从VLM获得详细的信息性响应，利用其广泛的世界知识。 在本文的工作中，本文使用了GPT-4o，这是一个在互联网规模数据上训练的高性能VLM，用于自动注释本文的数据集。GPT-4o能够解释场景，生成基于推理的适当响应，并准确识别复杂场景中自车的动作。

视觉输入。在确定视觉输入时，本文遇到了两个挑战。第一个挑战是从提供360度覆盖的多个摄像头中选择适当的图像。本文探索了两种方法：将所有视图合成一个大图像，或仅使用通常包含大多数驾驶任务所需最多相关信息的前视图图像。本文的注释结果显示，这两种方法产生的输出质量相当，因此本文选择仅使用前视图图像以减少整体复杂性。

第二个挑战涉及整合时间信息，这对于有效的规划和决策至关重要。本文也考虑了两种方法。一种直接的方法是将几个连续帧作为序列输入，并提示未来的时刻。然而，本文观察到VLM在处理时间连续性方面存在困难，常常混淆自车的身份，可能是由于时间定位的限制。相反，本文将自车的未来轨迹投影到单个前视图图像上，利用相机的内参和外参参数以及传感器规格。本文在提示中指明投影轨迹反映了车辆的未来路径。这种经济高效的设计允许VLM比使用图像序列更可靠地解释时间信息。

自由形式推理注释。作为VLM的关键输入，设计良好的问题是增强推理能力并提高VLM响应可解释性的关键。在本文的方法中，本文专注于规划任务，通过设计特定的提示从VLM获取推理。本文创建了两种类型的提问，首先是开放式问题，旨在生成自由形式、非结构化的响应，这些响应包含丰富的高维语言信息。本文将这些响应称为非结构化推理注释。

为了最大化VLM的推理能力，本文在提出具体问题之前提供了详细的上下文描述作为初步指令。具体的上下文和问题定义如下：

C1: 这是自车的前视图图像。红线表示未来轨迹，没有线表示停车或减速。在解释推理时，请关注相机图像和周围环境，而不是引用绘制的轨迹。

Q1−1: 请描述自车当前的动作。

Q1−2: 请预测自车未来的动作。

Q1−3: 请解释当前和未来动作的推理。完整的输入提示定义为P1 = [C1, Q1]，其中Q1代表问题集，Q1 = {Q1−1, Q1−2, Q1−3}。这些开放式问题产生了描述自车当前状态、预期未来动作及VLM知识背后推理的自由形式文本注释。

结构化动作注释。为了检验本文方法的灵活性，本文定义了第二种类型的问题，采用结构化格式。具体来说，本文创建了三个不同的动作集，并提示VLM从这些预定义选项中选择答案。这使本文能够为每个问题获得单一的动作注释。具体的上下文和问题定义如下：

C2: 这是自车的前视图图像。红线表示未来轨迹，没有线表示停车或减速。

Q2−1: 请从控制动作列表中描述自车的动作：{直行，慢速移动，停车，倒车}。

Q2−2: 请从转弯动作列表中描述自车的动作：{左转，右转，掉头，无}。

Q2−3: 请从车道动作列表中描述自车的动作：{向左变道，向右变道，合并至左车道，合并至右车道，无}。

完整的输入提示定义为P2 = [C2, Q2]，其中Q2代表结构化动作问题集，Q2 = {Q2−1, Q2−2, Q2−3}。通过这种方式，本文可以从VLM获得三个具体动作。与自由形式文本注释相比，结构化注释的一个主要优点是可以用于监督端到端驾驶模型，以预测人类可解释的动作，如第4节实验结果所示。

### 3.2. 辅助头

通常，数据驱动的端到端自动驾驶方法专注于总结一个可学习的自车特征
![f_{ego}](https://latex.csdn.net/eq?f_%7Bego%7D)
以生成规划结果，这对于生成可靠和准确的规划轨迹至关重要。这个可学习的自车特征通过不同的网络从上游模块汇总了关于自车的所有相关信息。在本文的方法中，本文开发了使用这个自车特征作为输入的辅助头，使模型能够从VLM的响应中提炼知识。

使用
![Q_1](https://latex.csdn.net/eq?Q_1)
问题，本文获得三个文本嵌入，记为
![A_1 = \{A_c, A_f, A_r\}](https://latex.csdn.net/eq?A_1%20%3D%20%5C%7BA_c%2C%20A_f%2C%20A_r%5C%7D)
，分别代表当前前缀。未表示动作的推理描述。使用
![Q_2](https://latex.csdn.net/eq?Q_2)
问题，本文从预设的集合中获得两个嵌入，记为
![A_2 = \{A_{control}, A_{turn}, A_{lane}\}](https://latex.csdn.net/eq?A_2%20%3D%20%5C%7BA_%7Bcontrol%7D%2C%20A_%7Bturn%7D%2C%20A_%7Blane%7D%5C%7D)
对应于控制动作，转弯动作和车道动作。为了将注意力转换到端的自动驾驶流中作监控。

对于
![Q_1](https://latex.csdn.net/eq?Q_1)
的自由文本注释，本文利用现有的语言模型将文本转换为特征表示。对于结构化回答，每个动作都被编码为一个 one-hot 标志。形式上：

![y_1 = \text{CLIP}(A_1)](https://latex.csdn.net/eq?y_1%20%3D%20%5Ctext%7BCLIP%7D%28A_1%29)

![y_2 = \text{One-Hot}(A_2)](https://latex.csdn.net/eq?y_2%20%3D%20%5Ctext%7BOne-Hot%7D%28A_2%29)

其中
![y_1](https://latex.csdn.net/eq?y_1)
和
![y_2](https://latex.csdn.net/eq?y_2)
含有三个组成部分：
![y_1 = \{y_{c}, y_{f}, y_{r}\}](https://latex.csdn.net/eq?y_1%20%3D%20%5C%7By_%7Bc%7D%2C%20y_%7Bf%7D%2C%20y_%7Br%7D%5C%7D)
和
![y_2 = \{y_{control}, y_{turn}, y_{lane}\}](https://latex.csdn.net/eq?y_2%20%3D%20%5C%7By_%7Bcontrol%7D%2C%20y_%7Bturn%7D%2C%20y_%7Blane%7D%5C%7D)
。这里，
![y_c, y_f, y_r](https://latex.csdn.net/eq?y_c%2C%20y_f%2C%20y_r)
是小的特征向量，其中
![c](https://latex.csdn.net/eq?c)
是文本嵌入的维度，而
![control, y_{turn}, y_{lane}](https://latex.csdn.net/eq?control%2C%20y_%7Bturn%7D%2C%20y_%7Blane%7D)
是大小为
![N](https://latex.csdn.net/eq?N)
的三个 one-hot 动作标志，其中
![N_{control} = 4, N_{turn} = 4, N_{lane} = 5](https://latex.csdn.net/eq?N_%7Bcontrol%7D%20%3D%204%2C%20N_%7Bturn%7D%20%3D%204%2C%20N_%7Blane%7D%20%3D%205)
。

使用一个文本特征
![y = \{y_c, y_f, y_r\}](https://latex.csdn.net/eq?y%20%3D%20%5C%7By_c%2C%20y_f%2C%20y_r%5C%7D)
作为监控，本文开辟了一个特征对齐头，该头以自车特征
![f_{ego}](https://latex.csdn.net/eq?f_%7Bego%7D)
作为输入。这种设置类似于知识蒸馏，其中特征对齐头学习与教师 VLM 提供的文本特征对齐。

在查询头中，文本初始化为可学习的文本查询
![q_1 = \{q_c, q_f, q_r\}](https://latex.csdn.net/eq?q_1%20%3D%20%5C%7Bq_c%2C%20q_f%2C%20q_r%5C%7D)
。每个查询通过多交叉注意力（MHCA）块与自车特征
![f_{ego}](https://latex.csdn.net/eq?f_%7Bego%7D)
交互，其中文本查询作为查询，自车特征作为键和值，产生更新的文本查询。这些更新的查询随后与自车特征连接，形成文本本身的特征输出。这个过程被表达为：

![q'_1 = \text{MHCA}(q, k, v)](https://latex.csdn.net/eq?q%27_1%20%3D%20%5Ctext%7BMHCA%7D%28q%2C%20k%2C%20v%29)

![q = q_1, k = v = f_{ego}](https://latex.csdn.net/eq?q%20%3D%20q_1%2C%20k%20%3D%20v%20%3D%20f_%7Bego%7D)

![f_1 = \text{MLP}(q'_1 \odot f_{ego})](https://latex.csdn.net/eq?f_1%20%3D%20%5Ctext%7BMLP%7D%28q%27_1%20%5Codot%20f_%7Bego%7D%29)

其中 ⊙ 表示连接，f 表示三个输入特征，与相应的 VLM 文本特征对齐。注意意图使用
![f_{ego}](https://latex.csdn.net/eq?f_%7Bego%7D)
独占的 MHCA 头，每个组件会使得文本查询能够更关注于自车特征的特征表示，这些方面可以用于文本形式表示。

受到知识蒸馏方法的启发，该方法控制特征向量的平滑度和锐度，本文采用类比的策略，使用不同的温度参数来归一化文本和输出特征，生成特征分布而不是原始特征值，如下所示：

![P(y_1) = \frac{\exp(y_1 / \tau_t)}{\sum_{k=1}^{C} \exp(y^{(k)}_1 / \tau_t)}](https://latex.csdn.net/eq?P%28y_1%29%20%3D%20%5Cfrac%7B%5Cexp%28y_1%20/%20%5Ctau_t%29%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BC%7D%20%5Cexp%28y%5E%7B%28k%29%7D_1%20/%20%5Ctau_t%29%7D)

![P(\hat{f}_1) = \frac{\exp(\hat{f}_1 / \tau_s)}{\sum_{k=1}^{C} \exp(\hat{f}^{(k)}_1 / \tau_s)}](https://latex.csdn.net/eq?P%28%5Chat%7Bf%7D_1%29%20%3D%20%5Cfrac%7B%5Cexp%28%5Chat%7Bf%7D_1%20/%20%5Ctau_s%29%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BC%7D%20%5Cexp%28%5Chat%7Bf%7D%5E%7B%28k%29%7D_1%20/%20%5Ctau_s%29%7D)

其中
![\tau_t](https://latex.csdn.net/eq?%5Ctau_t)
和
![\tau_s](https://latex.csdn.net/eq?%5Ctau_s)
是控制这些分布锐度的温度参数。这种调整使得输出特征和监控标签之间的对齐更好，增强了知识蒸馏的对齐质量。请注意，本文不应用中心化操作，因为文本认为监控是实际值。

结构化动作分类。本文以 VLM 使用问题 Q2 获得结构化动作标签
![y_2 = \{y_{control}, y_{turn}, y_{lane}\}](https://latex.csdn.net/eq?y_2%20%3D%20%5C%7By_%7Bcontrol%7D%2C%20y_%7Bturn%7D%2C%20y_%7Blane%7D%5C%7D)
。然后本文构建另一个动作分类头，该头以自车特征作为输入。类比于之前的特征对齐阶段，本文初始化三个可学习的动作查询
![q_{control}, q_{turn}, q_{lane}](https://latex.csdn.net/eq?q_%7Bcontrol%7D%2C%20q_%7Bturn%7D%2C%20q_%7Blane%7D)
，它们通过三个 MHCA 块与
![f_{ego}](https://latex.csdn.net/eq?f_%7Bego%7D)
交互。在这种设置中，每个动作查询作为查询，自车特征同时作为键和值，产生更新的动作查询。然后本文将这些更新的查询与自车特征连接，以创建动作分类头的特征表示，通过 MLP 层后跟 Softmax 函数生成动作预测。这个过程被表达为：

![q'_2 = \text{MHCA}(q, k, v)](https://latex.csdn.net/eq?q%27_2%20%3D%20%5Ctext%7BMHCA%7D%28q%2C%20k%2C%20v%29)

![q = q_2, \quad k = v = f_{ego}](https://latex.csdn.net/eq?q%20%3D%20q_2%2C%20%5Cquad%20k%20%3D%20v%20%3D%20f_%7Bego%7D)

![\hat{f}_2 = \text{Softmax}(\text{MLP}(q'_2 \oplus f_{ego}))](https://latex.csdn.net/eq?%5Chat%7Bf%7D_2%20%3D%20%5Ctext%7BSoftmax%7D%28%5Ctext%7BMLP%7D%28q%27_2%20%5Coplus%20f_%7Bego%7D%29%29)

其中
![\hat{f}_2 = \{\hat{f}_{control}, \hat{f}_{turn}, \hat{f}_{lane}\}](https://latex.csdn.net/eq?%5Chat%7Bf%7D_2%20%3D%20%5C%7B%5Chat%7Bf%7D_%7Bcontrol%7D%2C%20%5Chat%7Bf%7D_%7Bturn%7D%2C%20%5Chat%7Bf%7D_%7Blane%7D%5C%7D)
表示预测的动作。转弯动作和车道动作
![f_e](https://latex.csdn.net/eq?f_e)
。本文为每个动作查询使用独立的 MHCA 块来生成不同的动作特征。

### 3.3. 辅助损失

本文在规划模块之后定义了两个并行的辅助任务，以使模型能够从 VLM 中提炼知识，整体训练损失被定义为两个组成部分的加权和：

![\mathcal{L} = \lambda_1 \mathcal{L}_{align} + \lambda_2 \mathcal{L}_{action}](https://latex.csdn.net/eq?%5Cmathcal%7BL%7D%20%3D%20%5Clambda_1%20%5Cmathcal%7BL%7D_%7Balign%7D%20&plus;%20%5Clambda_2%20%5Cmathcal%7BL%7D_%7Baction%7D)

其中每个组成部分对应于一个不同的辅助文本头，在目标区域提供监控：

![\mathcal{L}_{align} = - P(y_c) \log(P(\hat{f}_c)) - P(y_f) \log(P(\hat{f}_f)) - P(y_r) \log(P(\hat{f}_r))](https://latex.csdn.net/eq?%5Cmathcal%7BL%7D_%7Balign%7D%20%3D%20-%20P%28y_c%29%20%5Clog%28P%28%5Chat%7Bf%7D_c%29%29%20-%20P%28y_f%29%20%5Clog%28P%28%5Chat%7Bf%7D_f%29%29%20-%20P%28y_r%29%20%5Clog%28P%28%5Chat%7Bf%7D_r%29%29)

对于特征对齐，本文使用交叉熵损失来对监控特征和输出特征，以捕捉文本传达的关键性信息。对于动作分类任务，本文也应用交叉熵损失以确保分类的准确性：

![\mathcal{L}_{action} = - \sum_{i=1}^{N_{control}} y^i_{control} \log(\hat{f}^i_{control}) - \sum_{i=1}^{N_{turn}} y^i_{turn} \log(\hat{f}^i_{turn}) - \sum_{i=1}^{N_{lane}} y^i_{lane} \log(\hat{f}^i_{lane})](https://latex.csdn.net/eq?%5Cmathcal%7BL%7D_%7Baction%7D%20%3D%20-%20%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bcontrol%7D%7D%20y%5Ei_%7Bcontrol%7D%20%5Clog%28%5Chat%7Bf%7D%5Ei_%7Bcontrol%7D%29%20-%20%5Csum_%7Bi%3D1%7D%5E%7BN_%7Bturn%7D%7D%20y%5Ei_%7Bturn%7D%20%5Clog%28%5Chat%7Bf%7D%5Ei_%7Bturn%7D%29%20-%20%5Csum_%7Bi%3D1%7D%5E%7BN_%7Blane%7D%7D%20y%5Ei_%7Blane%7D%20%5Clog%28%5Chat%7Bf%7D%5Ei_%7Blane%7D%29)

## 04 实验

### 4.1. 设置

基线。本文提出的方法是一个通用框架，兼容各种端到端自动驾驶方法。本文通过将其应用于两个广泛认可的开源方法UniAD和VAD来验证其有效性。此外，本文还将其与VLP进行比较，VLP通过CLIP将自车的真实标签投影到文本特征空间中进行对比学习。

数据集。本文使用nuScenes数据集进行开环规划评估。nuScenes是一个大规模的自动驾驶数据集，包含1000个场景，每个场景持续约20秒，并以2Hz的频率标注。该数据集包括详细的注释，使其成为端到端自动驾驶研究的流行基准。

评估协议。本文专注于规划任务，并使用标准指标（如L2位移误差和碰撞率）来评估性能。

实现细节。本文使用UniAD和VAD的官方代码，并遵循其官方实现中指定的超参数。对于本文的VLM-AD，本文定义了两个辅助任务头，每个头包含一个带有8个头和3层交叉注意力的MHCA块，并为Q1和Q2各自设置了3个文本查询。在训练过程中，本文将温度参数τs = 0.1和τt = 0.04设置为控制特征的锐度，并将λ1 = 1和λ2 = 0.1设置为平衡Lalign和Laction。所有模型都在8个NVIDIA H100 GPU上使用PyTorch框架进行训练。

### 4.2. 主要结果

表1展示了将本文提出的VLM-AD应用于两个基线（UniAD和VAD）的结果，以及与VLP的比较。通过比较方法ID 0和1，本文使用作者官方训练的检查点实现了几乎相同的规划结果。对于方法ID 6和7，以及ID 12和13，本文观察到重现结果与报告值之间存在一些差异，这归因于官方代码库中的图像配置修正。

从表格的第一部分可以看出，通过引入Q1和Q2，VLM-AD在平均L2规划误差和平均碰撞率方面显著优于UniAD，并且在这两个指标上也超过了最先进的基线VLP。关于VAD，本文的VLM-AD始终优于VAD-Base和VAD-Tiny，特别是在L2规划误差指标上，并且在VAD-Base中相比VLP表现更好。这些结果证明了本文VLM-AD方法的有效性和优势。此外，Q1比Q2产生了更好的结果，验证了通过丰富的推理信息监督驾驶模型的价值。

### 4.3. 消融研究

![](https://i-blog.csdnimg.cn/img_convert/e5d12a8bbfc57808b73a0531bc301a4c.jpeg)

▲表2｜ Q1−1、Q1−2和Q1−3的贡献的消融研究。最佳结果以粗体显示，次佳结果以下划线表示©️【深蓝AI】编译

子问题贡献。本文进一步分析了Q1中的每个子问题Q1−1、Q1−2和Q1−3的贡献。每个子问题提供了与自车当前状态、预测未来动作和推理相关的具体文本信息。表2展示了这三个子问题的消融研究。结果表明，每个子问题对整体性能都有积极影响，表明本文设计的问题为规划任务提供了有价值的信息。值得注意的是，推理特征对减少L2规划误差的贡献最大，突显了其在提升驾驶性能方面的重要性。

![](https://i-blog.csdnimg.cn/img_convert/ede00fba58387bf4bf1b6c59b7912cd2.jpeg)

▲表3｜ 不同VLM-KD变体的结果©️【深蓝AI】编译

特征对齐损失。本文还研究了特征对齐的替代方案，包括最小化CLIP中的对比学习损失、MSE损失、KL散度损失[30]或最大化负余弦相似性以对齐Q1中的三个特征。表3显示的结果表明，MSE损失通过最小化特征之间的欧几里得距离，略微优于UniAD，但在训练过程中会导致信息丢失。CLIP损失、KL散度和余弦相似性均优于UniAD，但不如本文的对齐损失。这突显了通过不同温度进行归一化以平衡教师-学生特征的平滑度和锐度的重要性。

![](https://i-blog.csdnimg.cn/img_convert/53b6d8d342715e0534e059f27c693c3b.jpeg)

▲表4｜ 不同设计的VLM-KD结果。MLP表示使用MLP层替换Q2中的MHCA块。T5和MPNet表示使用不同的语言模型将Q1中的推理注释转换为推理特征，作为监督标签©️【深蓝AI】编译

模型设计。本文研究了方法中的替代设计方案。首先，本文在结构化动作分类头中使用MLP层而不是MHCA块。其次，本文研究了不同的语言模型，如T5和MPNet，除了CLIP用于编码Q1中的文本注释。从表4可以看出，使用T5和MPNet的表现略差于L2性能，但碰撞率相同。此外，本文观察到T5和MPNet都优于UniAD基线，但略逊于CLIP。

![](https://i-blog.csdnimg.cn/img_convert/5076cbdbf5cba62ff062534ad4859601.jpeg)

▲表5｜ 使用不同超参数λ1和λ2来控制Lalign和Laction权重的结果©️【深蓝AI】编译

超参数研究。在多任务学习中，平衡不同任务的损失是一个关键挑战。本文在UniAD的背景下研究了超参数λ1和λ2。表5显示的结果表明，所有三种变体均优于UniAD。在这些变体中，当λ1 = 0.1和λ2 = 1时性能最差，因为Q1的注释包含比Q2的注释更有价值的信息。

### 4.4. 可视化

![](https://i-blog.csdnimg.cn/direct/6db084a83f9a4486861d578954c6453a.png)

▲图4｜ UniAD与本文方法的定性比较。黄色箭头突出显示了VLM-AD优于UniAD的区域。红色框表示UniAD的失败规划指令，紫色框表示本文VLM-AD辅助文本头预测的三个动作输出©️【深蓝AI】编译

本文从nuScenes数据集中提供了四个可视化示例，如图4所示，以展示本文提出方法的有效性。在第一、第三和最后一行的案例中，UniAD生成的规划轨迹曲折且缺乏平滑性，而本文的方法生成了更平滑的轨迹，能够准确地跟随道路。此外，在第二、第三和最后一行的案例中，基线计算的指令错误地建议了转弯意图，而实际上这些场景中的自车是直行的。本文的动作文本头在这三种情况下正确输出了控制动作“直行”，不仅验证了VLM监督的有效性，还提供了模型决策的可解释性。

## 05 结论和未来工作

在本研究中，本文提出了VLM-AD，这是一种通过利用视觉-语言模型（VLMs）作为辅助教师来增强端到端自动驾驶模型的新方法。通过针对包含非结构化推理文本和结构化动作标签的目标问题集成基于VLM的注释，本文在训练过程中增加了额外的推理和动作监督。本文的方法在nuScenes数据集上展示了显著的规划精度提升，并减少了碰撞率，同时通过动作预测提供了输出轨迹的可解释性。重要的是，VLM-AD在推理时不需要使用VLM，使其可以即插即用，无需额外的推理成本即可部署到实际应用中。