---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33333433313336382f:61727469636c652f64657461696c732f313330323131393736"
layout: post
title: "时空序列全球气象预测大模型-OpenCastKit-正式开源"
date: 2025-01-13 09:58:02 +0800
description: "OpenCastKit 是幻方 AI 开源的 AI 气象大模型工具包，包含了两大气象 SOTA 模型"
keywords: "opencastkit"
categories: ['未分类']
tags: ['深度学习', '人工智能']
artid: "130211976"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=130211976
  alt: "时空序列全球气象预测大模型-OpenCastKit-正式开源"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=130211976
featuredImagePreview: https://bing.ee123.net/img/rand?artid=130211976
---

# 【时空序列】全球气象预测大模型 OpenCastKit 正式开源

![e8f7ee70066a45d0367f0f83707b079f.jpeg](https://i-blog.csdnimg.cn/blog_migrate/9a1aff1c3789c9624d1909ff61ec1929.jpeg)

OpenCastKit 是幻方 AI 开源的 AI 气象大模型工具包，包含了两大气象 SOTA 模型 FourCastNet 和 GraphCast，并提供完整参数开源。用户可以轻松得通过该大模型输出全球高分辨率的气象预测结果。

**Open**

**CastKit**

用 AI 方法对
**现**
**代数值天气预报**
（numerical weather prediction, NWP）进行改进提升近两年受到了广泛关注，如 Nvidia 发布的 FourCastNet、 DeepMind 发布的 GraphCast 和华为发布的盘古气象大模型，在与欧洲中期天气预报中心（ECMWF）的高分辨率综合预测系统（IFS）对比中都获得了不错的效果。

基于此，我们最近复现整合了这些工作，并将这些成果贡献给开源社区。我们基于 FourCastNet 和 GraphCast 论文构建了一个新的全球AI气象预测项目——
**OpenCastKit**
。

![c8404d7c18231b92a41ad595bffa21af.png](https://i-blog.csdnimg.cn/blog_migrate/afd9f7e8dafad9dc5a41b0cb2b875e6b.png)

项目地址：https://github.com/HFAiLab/OpenCastKit

这个项目提供了一个强大的、基于ERA5数据训练的开源气象模型和参数，可以生成全球高分辨率的气象预测结果。具体来说，它包含：

* 一个统一的数据处理工具，抽取ERA5数据和特征并整理成高性能训练数据格式
  ffrecord
  ；
* 基于
  hfai 算子
  和
  hfreduce 并行通信
  优化的 FourCastNet 模型源码和 GraphCast 模型源码，供社区研究优化；
* 基于1979年到2022年15TB的ERA5数据，在萤火高性能集群上训练的模型参数，可以进行微调，获得高精度预测结果。

同时，我们上线了一个每日更新的
**HF-Earth**
，展示气象大模型输出的全球预测效果：

![4971a93cee196a8c09c693790236134a.png](https://i-blog.csdnimg.cn/blog_migrate/12b6d5c271f53ea8d4b88cf5356c8668.png)

Demo地址：https://www.high-flyer.cn/hf-earth/

经过一段时间的测试来看，AI气象大模型对台风、极端降水等事件的预测上效果明显，在长期气候变化的分析中可以起到一定作用。希望在此开源项目的基础上，构建出更加强大的 AI 气象应用。

**数据集**

欧洲中期天气预报中心（ECMWF）提供了一个公开可用的综合数据集 ERA5，其将物理模型数据与来自世界各地的观测数据结合起来，形成一个全球完整的、一致的数据集，以小时级到天级不等，提供包括温度、风量、降水、水文、气压等多项全球气象指标数据，供各种气象预报模型学习。

官方地址：https://www.ecmwf.int/en/forecasts/datasets/reanalysis-datasets/era5

FourCastNet 与 GraphCast 使用了不同规模的 ERA5 数据来训练，产生的预测效果各自不同。前者仅使用了 20 个相关气象指标，包括 4 个不同位势高度下的温度、风速、相对湿度和一些近地表变量，其旨对极端天气、自然灾害进行预警；而后者使用了更加全面的数据，其包含 37 个不同位势高度下的气象指标和 5 个地表气象指标，总计 227 个指标，其旨在对气象变化进行更加全面的评估和预报。

对此，我们将这些数据进行了归纳整理，通过
hfai.datasets
工具进行管理优化。原始数据通过特征处理，转化成 “
*X
t-1
, X
t
→ X
t+1*
” 的模式，通过高性能训练样本格式 ffrecord 进行保存，从而可以在萤火集群中进行高效的并行训练。更多信息可以浏览 hfai 数据集仓库。

**模型构建和优化**

为了进行 0.25° 分辨率下的全球气象预测，FourCastNet 采用自适应傅里叶神经算子 AFNO，而 GraphCast 采用了图神经网络。前者计算效率高效，可以灵活且可扩展地建模跨空间和不同指标之间的依赖关系；后者通过构建节点之间的联系，更加详细捕捉如“蝴蝶效应”般的气候因子影响。前者在小 batchsize 上可以进行数据并行以加速训练，而后者球体节点之间的 message passing 参数规模更大，需要进行流水线并行（或称模型并行）的改造，以实现模型的完整训练。

![2c43a0024a8cb901ef880eef894322a4.png](https://i-blog.csdnimg.cn/blog_migrate/3a74446c4124862f53718987c9194927.png)

FourCastNet 模型结构

![367f9663e42dbc268464b15de7b58778.png](https://i-blog.csdnimg.cn/blog_migrate/cdda1b4d4d90c62e75b178fb9e57633d.png)

GraphCast 模型结构

这里我们采用自研的
haiscale 高性能并行训练工具库
对两种模型进行复现优化。对于 FourCastNet，我们使用
*haiscale.ddp*
或者
*haiscale.fsdp*
进行数据并行优化，实验中我们采用小 batchsize 即实现了论文效果的复现；对于 GraphCast，完整参数基本无法塞入单张显卡，因此对于不同的环节，如球体中 grid 节点与 mesh 节点进行message passing，我们需要对其拆分，让其分布在不同的显卡上，通过
*haiscale.pipeline*
对不同环节进行串联，实现模型并行训练。具体如下：

**FourCastNet 数据并行**

FourCastNet 模型的训练包括
*pretrian*
、
*finetune*
和
*precipitation*
三个部分。模型采用递进式，即以
*X
t*
作为输入，预测下一步
*X
t+1*
。一次训练输出多步，与真值对比计算 loss。如下伪代码所示：

```python
from hfai.datasets import ERA5
from haiscale.ddp import DistributedDataParallel
from torch.utils.data.distributed import DistributedSampler

model = FourCastNet(args).cuda()
model = DistributedDataParallel(model)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

data = ERA5(split='train')
sampler = DistributedSamper(data, shuffle=True)
dataloader = data.loader(args.batch_szie, sampler=sampler, num_workers=8, pin_memory=True, drop_last=True)

# training ...

for step, (xt0, xt1, xt2, pt2) in enumerate(dataloader):
   xt1_pred = model(xt0)            # pretrain
   xt2_pred = model(xt1_pred)          # finetune
   pt2_pred = model(xt2_pred, precip=True)    # preciptation

pretrain_loss = criterion(xt1_pred, xt1)
   finttune_loss = criterion(xt2_pred, xt2)
   precip_loss = criterion(pt2_pred, pt2)

# optim ...

# stop hfreduce

model.reducer.stop()
```

*haiscale.ddp*
默认采用 hfreduce 进行通信优化，我们还可以使用优化算子，加入一行
*model = hfai.nn.to\_hfai(model)*
代码进行进一步加速。在萤火集群上我们使用 96 张 A100 进行数据并行加速，耗时 16~17 个小时左右基本可以完成 FourCastNet 的整体训练。

**GraphCast 数据并行**

不同于 FourCastNet，GraphCast 只有主干模型一个，其也是采用递进式，不过以
*X
t-1
, X
t
, T, C, G*
作为输入，预测下一步
*X
t+1*
。这里
*T, C*
代表了时间戳信息和地理位置信息，
*G*
代表所构建的球体 Graph 信息。如下伪代码所示：

```python
from hfai.datasets import ERA5
from haiscale.ddp import DistributedDataParallel
from haiscale.pipeline import PipeDream, make_subgroups, partition
from torch.utils.data.distributed import DistributedSampler

dist.init_process_group(...)
torch.cuda.set_device(local_rank)
rank, world_size = dist.get_rank(), dist.get_world_size()

dp_group, pp_group = make_subgroups(pp_size=pp_size)
dp_rank, dp_size = dp_group.rank(), dp_group.size()
pp_rank, pp_size = pp_group.rank(), pp_group.size()

model = GraphCast_sequentail(args)
model = partition(model, pp_group.rank(), pp_group.size(), balance=[1,1,1,1,1,1,1,1])
model = DistributedDataParallel(model.cuda(), process_group=dp_group)
model = PipeDream(model, args.chunks, process_group=pp_group)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

data = ERA5(split='train')
sampler = DistributedSamper(data, num_replicas=dp_size, rank=dp_rank, shuffle=True)
dataloader = data.loader(args.batch_szie, sampler=sampler, num_workers=8, pin_memory=True, drop_last=True)
earth_graph = generate_graph(args)

# training ...

for step, (xt0, xt1, xt2) in enumerate(dataloader):
loss = model.forward_backward(xt0, xt1, earth_graph, criterion=criterion, labels=(xt2,))

    # optim ...

# synchronize all processes

model.module.reducer.stop()
dist.barrier()
```

在使用
*haiscale.pipeline*
进行流水线并行训练时，需要我们提前将模型进行拆分，通过
*haiscale.SequentialModel*
进行模型的串联。同时 haiscale 提供了一个统一的 forward\_backward 接口，进行样本和标签的统一输入和结果输出。在萤火集群上我们使用 256 张 A100 进行模型并行加速（单节点 8 卡做流水并行，32 节点做数据并行），耗时 3 天左右基本可以完成 GraphCast 的整体训练。

关于代码的更多细节可以访问项目地址阅读源码。

**预测结果**

参照论文中的评估方式，我们采用递归输出未来多天的预测结果，与真实值对比，通过误差增长曲线来比较不同 AI 气象大模型的预测效果。如下图所示：

![8ba76c86f9fd3640fe167fb2a86197ea.png](https://i-blog.csdnimg.cn/blog_migrate/33dbfe12be4321d43e4bc0956db563c4.png)

可以看到，在进行 14 天的中期天气预报测试中，无论是 GraphCast 还是 FourCastNet，递归预测导致误差随时间逐步增长。整体误差上看 GraphCast 考虑了地理时间和地理位置的因素，预测误差比 FourCastNet 要小。受此启发，我们将时间和地理信息加入 FourCastNet 进行模型训练（FourCastNet+），发现最终模型输出的预测误差几乎与 GraphCast 一致。

下面我们以 2022 年 6 月 22 日开始连续输出 14 天的预测，展示 OpenCastKit 的预测效果：

![5794b01a96db618fd9462c95f6652f3d.gif](https://i-blog.csdnimg.cn/blog_migrate/1bb4ca4c29d9a3a4758b7db6b5c17830.gif)

FourCastNet 温度预测

![d45d607ecb751e65a6a66f77d110e88c.gif](https://i-blog.csdnimg.cn/blog_migrate/dfc74d7f5b9b67373c350444bc38e863.gif)

GraphCast 温度预测

![605571539ea2017a8f84e6fdeeda05a2.gif](https://i-blog.csdnimg.cn/blog_migrate/e892a756633fa17adc242045cf81fda5.gif)

FourCastNet 风力预测

![314c330631d2010cc4a6c8c2726c51b1.gif](https://i-blog.csdnimg.cn/blog_migrate/9999b593022be3a40149001d14810e99.gif)

GraphCast  风力预测

![6d030ba80179aa8cf0ec10110e342d7a.gif](https://i-blog.csdnimg.cn/blog_migrate/2493b6c55d666f974c8492a0e8d92fbc.gif)

真实温度

![a74b52b8dbc3d57b9a5fdee929478cfd.gif](https://i-blog.csdnimg.cn/blog_migrate/2b9517c789a3aecf24cdf2f7f767aaf1.gif)

真实风力

可以看到 FourCastNet 和 GraphCast 都可以对风力和温度的衍变进行比较准确的预测。其中 GraphCast 相对来说更加接近真实情况，包括气象指标的细节纹理更丰富和一致，还有在 6 月 30 号开始在我国东南沿海的两次台风路径的预测。

```go
推荐阅读：

我的 2022 届互联网校招分享

我的 2021 总结

浅谈算法岗和开发岗的区别

互联网校招研发薪资汇总
2022 届互联网求职现状，金 9 银 10 快变成铜 9 铁 10！！

公众号：AI 蜗牛车

保持谦逊、保持自律、保持进步

发送【蜗牛】获取一份《手把手 AI 项目》（AI 蜗牛车著）
发送【1222】获取一份不错的 leetcode 刷题笔记

发送【AI 四大名著】获取四本经典 AI 电子书
```