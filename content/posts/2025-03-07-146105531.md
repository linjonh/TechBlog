---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35393930373339342f:61727469636c652f64657461696c732f313436313035353331"
layout: post
title: "ç¬¬TR3å‘¨Pytorchå¤çŽ°Transformer"
date: 2025-03-07 21:41:14 +0800
description: "Transformeré€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¹å˜äº†åºåˆ—å»ºæ¨¡çš„æ–¹å¼ï¼Œæˆä¸ºAIé¢†åŸŸçš„åŸºç¡€æž¶æž„ç¼–ç å™¨ï¼šç†è§£è¾“å…¥ï¼Œæå–ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚è§£ç å™¨ï¼šåŸºäºŽç¼–ç ç‰¹å¾ï¼ŒæŒ‰é¡ºåºç”Ÿæˆè¾“å‡ºã€‚"
keywords: "ç¬¬TR3å‘¨ï¼šPytorchå¤çŽ°Transformer"
categories: ['æœªåˆ†ç±»']
tags: ['äººå·¥æ™ºèƒ½', 'Transformer', 'Pytorch']
artid: "146105531"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146105531
    alt: "ç¬¬TR3å‘¨Pytorchå¤çŽ°Transformer"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146105531
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146105531
cover: https://bing.ee123.net/img/rand?artid=146105531
image: https://bing.ee123.net/img/rand?artid=146105531
img: https://bing.ee123.net/img/rand?artid=146105531
---

# ç¬¬TR3å‘¨ï¼šPytorchå¤çŽ°Transformer

* **ðŸ¨**
  **æœ¬æ–‡ä¸º**
  [ðŸ”—365å¤©æ·±åº¦å­¦ä¹ è®­ç»ƒè¥](https://mp.weixin.qq.com/s/kV8ZsJv6cPNzJLEuhPfvXg "ðŸ”—365å¤©æ·±åº¦å­¦ä¹ è®­ç»ƒè¥")
  **ä¸­çš„å­¦ä¹ è®°å½•åšå®¢**
* **ðŸ–**
  **åŽŸä½œè€…ï¼š**
  [KåŒå­¦å•Š](https://mtyjkh.blog.csdn.net/ "KåŒå­¦å•Š")

Transformeré€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¹å˜äº†åºåˆ—å»ºæ¨¡çš„æ–¹å¼ï¼Œæˆä¸ºAIé¢†åŸŸçš„åŸºç¡€æž¶æž„

ç¼–ç å™¨ï¼šç†è§£è¾“å…¥ï¼Œæå–ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚

è§£ç å™¨ï¼šåŸºäºŽç¼–ç ç‰¹å¾ï¼ŒæŒ‰é¡ºåºç”Ÿæˆè¾“å‡ºã€‚

```
![](https://i-blog.csdnimg.cn/direct/89075e29873a473a9149354632f8dacf.png)

```

### 1.å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶

> ```
> import math
> import torch
> import torch.nn as nn
>
> device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
>
> class MultiHeadAttention(nn.Module):
>     # n_heads:å¤šå¤´æ³¨æ„åŠ›çš„æ•°é‡
>     # hid_dim:æ¯ä¸ªè¯è¾“å‡ºçš„å‘é‡ç»´åº¦
>     def __init__(self,hid_dim,n_heads):
>         super(MultiHeadAttention,self).__init__()
>         self.hid_dim=hid_dim
>         self.n_heads=n_heads
>
>         #å¼ºåˆ¶hid_dimå¿…é¡»æ•´é™¤ h
>         assert hid_dim % n_heads == 0
>         #å®šä¹‰W_qçŸ©é˜µce
>         self.w_q=nn.Linear(hid_dim,hid_dim)
>         #å®šä¹‰W_kçŸ©é˜µ
>         self.w_k=nn.Linear(hid_dim,hid_dim)
>         #å®šä¹‰W_vçŸ©é˜µ
>         self.w_v=nn.Linear(hid_dim,hid_dim)
>         self.fc =nn.Linear(hid_dim,hid_dim)
>         #ç¼©æ”¾
>         self.scale=torch.sqrt(torch.FloatTensor([hid_dim//n_heads]))
>
>     def forward(self,query,key,value,mask=None):
>         #Q,K,Vçš„åœ¨å¥å­è¿™é•¿åº¦è¿™ä¸€ä¸ªç»´åº¦çš„æ•°å€¼å¯ä»¥ä¸ä¸€æ ·ï¼Œå¯ä»¥ä¸€æ ·
>         #K:[64,10,300],å‡è®¾batch_sizeä¸º64ï¼Œæœ‰10ä¸ªè¯ï¼Œæ¯ä¸ªè¯çš„Queryå‘é‡æ˜¯300ç»´
>         bsz=query.shape[0]
>         Q  =self.w_q(query)
>         K  =self.w_k(key)
>         V  =self.w_v(value)
>         #è¿™é‡ŒæŠŠK Q V çŸ©é˜µæ‹†åˆ†ä¸ºå¤šç»„æ³¨æ„åŠ›
>         #æœ€åŽä¸€ç»´å°±æ˜¯æ˜¯ç”¨self.hid_dim // self.n_heads æ¥å¾—åˆ°çš„ï¼Œè¡¨ç¤ºæ¯ç»„æ³¨æ„åŠ›çš„å‘é‡é•¿åº¦ï¼Œæ¯ä¸ªheadçš„å‘é‡é•¿åº¦æ˜¯:300/6=50
>         #64è¡¨ç¤ºbatch size,6è¡¨ç¤ºæœ‰6ç»„æ³¨æ„åŠ›ï¼Œ10è¡¨ç¤ºæœ‰10è¯ï¼Œ50è¡¨ç¤ºæ¯ç»„æ³¨æ„åŠ›çš„è¯çš„å‘é‡é•¿åº¦
>         #K: [64,10,300] æ‹†åˆ†å¤šç»„æ³¨æ„åŠ› -> [64,10,6,50] è½¬ç½®å¾—åˆ° -> [64,6,10,50]
>         #è½¬ç½®æ˜¯ä¸ºäº†æŠŠæ³¨æ„åŠ›çš„æ•°é‡6æ”¾åœ¨å‰é¢ï¼ŒæŠŠ10å’Œ50æ”¾åœ¨åŽé¢ï¼Œæ–¹ä¾¿ä¸‹é¢è®¡ç®—
>         Q=Q.view(bsz,-1,self.n_heads,self.hid_dim//
>                  self.n_heads).permute(0,2,1,3)
>         K=K.view(bsz,-1,self.n_heads,self.hid_dim//
>                  self.n_heads).permute(0,2,1,3)
>         V=V.view(bsz,-1,self.n_heads,self.hid_dim//
>                  self.n_heads).permute(0,2,1,3)
>         #Qä¹˜ä»¥Kçš„è½¬ç½®ï¼Œé™¤ä»¥scale
>         #[64,6,12,50]*[64,6,50,10]=[64,6,12,10]
>         #attention:[64,6,12,10]
>         attention=torch.matmul(Q,K.permute(0,1,3,2)) / self.scale
>
>         #å¦‚æžœmaskä¸ä¸ºç©ºï¼Œé‚£ä¹ˆå°±æŠŠmaskä¸º0çš„ä½ç½®çš„attentionåˆ†æ•°è®¾ç½®ä¸º-1e10,è¿™é‡Œç”¨â€˜0â€™æ¥æŒ‡ç¤ºå“ªäº›ä½ç½®çš„è¯å‘é‡ä¸èƒ½è¢«attentionåˆ°,æ¯”å¦‚paddingä½ç½®
>         if mask is not None:
>             attention=attention.masked_fill(mask==0,-1e10)
>
>             #ç¬¬äºŒæ­¥:è®¡ç®—ä¸Šä¸€æ­¥ç»“æžœçš„softmaxï¼Œå†ç»è¿‡dropout,å¾—åˆ°attention
>             #æ³¨æ„ï¼Œè¿™é‡Œæ˜¯å¯¹æœ€åŽä¸€ç»´åšsoftmaxï¼Œä¹Ÿå°±æ˜¯åœ¨è¾“å…¥åºåˆ—çš„ç»´åº¦åšsoftmax
>             #attention: [64,6,12,10]
>         attention=torch.softmax(attention,dim=-1)
>
>         #ç¬¬ä¸‰æ­¥,attentionç»“æžœä¸ŽVç›¸ä¹˜ï¼Œå¾—åˆ°å¤šå¤´æ³¨æ„åŠ›çš„ç»“æžœ
>         #[64,6,12,10] * [64,6,10,50] =[64,6,12,50]
>         # x: [64,6,12,50]
>         x=torch.matmul(attention,V)
>
>         #å› ä¸ºqueryæœ‰12ä¸ªè¯ï¼Œæ‰€ä»¥æŠŠ12æ”¾åœ¨å‰é¢ï¼ŒæŠŠ50å’Œ6æ”¾åœ¨åŽé¢ï¼Œæ–¹ä¾¿ä¸‹é¢æ‹¼æŽ¥å¤šç»„çš„ç»“æžœ
>         #x: [64,6,12,50] è½¬ç½® -> [64,12,6,50]
>         x=x.permute(0,2,1,3).contiguous()
>         #è¿™é‡Œçš„çŸ©é˜µè½¬æ¢å°±æ˜¯ï¼šæŠŠå¤šå¤´æ³¨æ„åŠ›çš„ç»“æžœæ‹¼æŽ¥èµ·æ¥
>         #æœ€åŽç»“æžœå°±æ˜¯[64,12,300]
>         # x:[64,12,6,50] -> [64,12,300]
>         x=x.view(bsz,-1,self.n_heads*(self.hid_dim//self.n_heads))
>         x=self.fc(x)
>         return x
>
> ```

### 2.å‰é¦ˆä¼ æ’­

> ```
> class Feedforward(nn.Module):
>     def __init__(self,d_model,d_ff,dropout=0.1):
>         super(Feedforward,self).__init__()
>         #ä¸¤å±‚çº¿æ€§æ˜ å°„å’Œæ¿€æ´»å‡½æ•°
>         self.linear1=nn.Linear(d_model,d_ff)
>         self.dropout=nn.Dropout(dropout)
>         self.linear2=nn.Linear(d_ff,d_model)
>
>     def forward(self,x):
>         x=torch.nn.functional.relu(self.linear1(x))
>         x=self.dropout(x)
>         x=self.linear2(x)
>         return x
> ```

### 3.ä½ç½®ç¼–ç 

> ```
> class PositionalEncoding(nn.Module):
>     "å®žçŽ°ä½ç½®ç¼–ç "
>     def __init__(self, d_model, dropout=0.1, max_len=5000):
>         super(PositionalEncoding, self).__init__()
>         self.dropout = nn.Dropout(p=dropout)
>         # åˆå§‹åŒ–Shapeä¸º(max_len, d_model)çš„PE (positional encoding)
>         pe = torch.zeros(max_len, d_model).to(device)
>
>         # åˆå§‹åŒ–ä¸€ä¸ªtensor [[0, 1, 2, 3, ...]]
>         position = torch.arange(0, max_len).unsqueeze(1)
>         # è¿™é‡Œå°±æ˜¯sinå’Œcosæ‹¬å·ä¸­çš„å†…å®¹ï¼Œé€šè¿‡eå’Œlnè¿›è¡Œäº†å˜æ¢
>         div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
>
>         pe[:, 0::2] = torch.sin(position * div_term) # è®¡ç®—PE(pos, 2i)
>         pe[:, 1::2] = torch.cos(position * div_term) # è®¡ç®—PE(pos, 2i+1)
>
>         pe = pe.unsqueeze(0) # ä¸ºäº†æ–¹ä¾¿è®¡ç®—ï¼Œåœ¨æœ€å¤–é¢åœ¨unsqueezeå‡ºä¸€ä¸ªbatch
>
>         # å¦‚æžœä¸€ä¸ªå‚æ•°ä¸å‚ä¸Žæ¢¯åº¦ä¸‹é™ï¼Œä½†åˆå¸Œæœ›ä¿å­˜modelçš„æ—¶å€™å°†å…¶ä¿å­˜ä¸‹æ¥
>         # è¿™ä¸ªæ—¶å€™å°±å¯ä»¥ç”¨register_buffer
>         self.register_buffer("pe", pe)
>
>     def forward(self, x):
>         """
>         x ä¸ºembeddingåŽçš„inputsï¼Œä¾‹å¦‚(1,7, 128)ï¼Œbatch sizeä¸º1,7ä¸ªå•è¯ï¼Œå•è¯ç»´åº¦ä¸º128
>         """
>         # å°†xå’Œpositional encodingç›¸åŠ ã€‚
>         x = x + self.pe[:, :x.size(1)].requires_grad_(False)
>
>         return self.dropout(x)
> ```

### 4.ç¼–ç å±‚

> ```
> class EncoderLayer(nn.Module):
>     def __init__(self,d_model,n_heads,d_ff,dropout=0.1):
>         super(EncoderLayer,self).__init__()
>         #ç¼–ç å™¨å±‚åŒ…å«è‡ªæ³¨æ„æœºåˆ¶å’Œå‰é¦ˆç¥žç»ç½‘ç»œ
>         self.self_attn=MultiHeadAttention(d_model,n_heads)
>         self.feedforward=Feedforward(d_model,d_ff,dropout)
>         self.norm1=nn.LayerNorm(d_model)
>         self.norm2=nn.LayerNorm(d_model)
>         self.dropout=nn.Dropout(dropout)
>
>     def forward(self,x,mask):
>         #è‡ªæ³¨æ„åŠ›æœºåˆ¶
>         atten_output=self.self_attn(x,x,x,mask)
>         x=x+self.dropout(atten_output)
>         x=self.norm1(x)
>
>         #å‰é¦ˆç¥žç»ç½‘ç»œ
>         ff_output=self.feedforward(x)
>         x=x+self.dropout(ff_output)
>         x=self.norm2(x)
>
>         return x
> ```

### 5.è§£ç å±‚

> ```
> class DecoderLayer(nn.Module):
>     def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
>         super(DecoderLayer, self).__init__()
>         # è§£ç å™¨å±‚åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥žç»ç½‘ç»œ
>         self.self_attn   = MultiHeadAttention(d_model, n_heads)
>         self.enc_attn    = MultiHeadAttention(d_model, n_heads)
>         self.feedforward = Feedforward(d_model, d_ff, dropout)
>         self.norm1   = nn.LayerNorm(d_model)
>         self.norm2   = nn.LayerNorm(d_model)
>         self.norm3   = nn.LayerNorm(d_model)
>         self.dropout = nn.Dropout(dropout)
>
>     def forward(self, x, enc_output, self_mask, context_mask):
>         # è‡ªæ³¨æ„åŠ›æœºåˆ¶
>         attn_output = self.self_attn(x, x, x, self_mask)
>         x           = x + self.dropout(attn_output)
>         x           = self.norm1(x)
>
>         # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›æœºåˆ¶
>         attn_output = self.enc_attn(x, enc_output, enc_output, context_mask)
>         x           = x + self.dropout(attn_output)
>         x           = self.norm2(x)
>
>         # å‰é¦ˆç¥žç»ç½‘ç»œ
>         ff_output = self.feedforward(x)
>         x = x + self.dropout(ff_output)
>         x = self.norm3(x)
>
>         return x
> ```

### 6.Transformeræ¨¡åž‹æž„å»º

> ```
> class Transformer(nn.Module):
>     def __init__(self, vocab_size, d_model, n_heads, n_encoder_layers, n_decoder_layers, d_ff, dropout=0.1):
>         super(Transformer, self).__init__()
>         # Transformer æ¨¡åž‹åŒ…å«è¯åµŒå…¥ã€ä½ç½®ç¼–ç ã€ç¼–ç å™¨å’Œè§£ç å™¨
>         self.embedding           = nn.Embedding(vocab_size, d_model)
>         self.positional_encoding = PositionalEncoding(d_model)
>         self.encoder_layers      = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_encoder_layers)])
>         self.decoder_layers      = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_decoder_layers)])
>         self.fc_out              = nn.Linear(d_model, vocab_size)
>         self.dropout             = nn.Dropout(dropout)
>
>     def forward(self, src, trg, src_mask, trg_mask):
>         # è¯åµŒå…¥å’Œä½ç½®ç¼–ç 
>         src = self.embedding(src)
>         src = self.positional_encoding(src)
>         trg = self.embedding(trg)
>         trg = self.positional_encoding(trg)
>
>         # ç¼–ç å™¨
>         for layer in self.encoder_layers:
>             src = layer(src, src_mask)
>
>         # è§£ç å™¨
>         for layer in self.decoder_layers:
>             trg = layer(trg, src, trg_mask, src_mask)
>
>         # è¾“å‡ºå±‚
>         output = self.fc_out(trg)
>
>         return output
> ```

> ```
> vocab_size = 10000
> d_model    = 128
> n_heads    = 8
> n_encoder_layers = 6
> n_decoder_layers = 6
> d_ff             = 2048
> dropout          = 0.1
>
> device = torch.device('cpu')
>
> transformer_model = Transformer(vocab_size, d_model, n_heads, n_encoder_layers, n_decoder_layers, d_ff, dropout)
>
> # å®šä¹‰è¾“å…¥
> src = torch.randint(0, vocab_size, (32, 10))  # æºè¯­è¨€å¥å­
> trg = torch.randint(0, vocab_size, (32, 20))  # ç›®æ ‡è¯­è¨€å¥å­
> src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # æŽ©ç ï¼Œç”¨äºŽå±è”½å¡«å……çš„ä½ç½®
> trg_mask = (trg != 0).unsqueeze(1).unsqueeze(2)  # æŽ©ç ï¼Œç”¨äºŽå±è”½å¡«å……çš„ä½ç½®
>
> # æ¨¡åž‹å‰å‘ä¼ æ’­
> output = transformer_model(src, trg, src_mask, trg_mask)
> print(output.shape)
> ```

![](https://i-blog.csdnimg.cn/direct/771f6373d7b14b05a33d23a36baf2cca.png)