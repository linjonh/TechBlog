---
layout: post
title: "强化学习赵世钰版-学习笔记7.时序差分学习"
date: 2025-03-13 23:42:49 +0800
description: "Q-Learning是Off-policy，而Saras和MC都是On-policy，因为需要计算的策略，用到的数据都是相同的策略生成的，同时也是个策略问题，都是通过迭代找到最优策略的。TD算法更新状态值的公式，可以展开来看。TD算法适用于计算状态值的算法，对应计算行为值的类似算法叫做Saras（state-action-reward-state-action的缩写），其表达式为。本章是课程算法与方法中的第四章，介绍的时序差分学习算法是基于随机近似方法设计的强化学习方法，也是model-free的方法。"
keywords: "强化学习（赵世钰版）-学习笔记（7.时序差分学习）"
categories: ['深度学习', '数学学习', '人工智能']
tags: ['笔记', '学习', '人工智能']
artid: "146245152"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146245152
    alt: "强化学习赵世钰版-学习笔记7.时序差分学习"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146245152
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146245152
cover: https://bing.ee123.net/img/rand?artid=146245152
image: https://bing.ee123.net/img/rand?artid=146245152
img: https://bing.ee123.net/img/rand?artid=146245152
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     强化学习（赵世钰版）-学习笔记（7.时序差分学习）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     本章是课程算法与方法中的第四章，介绍的时序差分学习算法是基于随机近似方法设计的强化学习方法，也是model-free的方法。
    </p>
    <p>
     <img alt="" height="1035" src="https://i-blog.csdnimg.cn/direct/4f2f6bff8d5942219dac7cc663b5c3aa.png" width="1242"/>
    </p>
    <p>
     时序差分算法是一种近似估计策略状态值的算法，具体的形式如下：
    </p>
    <p>
     <img alt="" height="694" src="https://i-blog.csdnimg.cn/direct/fcbfdf57d28e484399519db44b5c453a.png" width="1382"/>
    </p>
    <p>
     本质上是在当前t时刻，被访问到的状态采用近似迭代的策略（即上一章讲的RM算法）估计出一个状态值，没被访问到的维持不变。TD算法更新状态值的公式，可以展开来看。
    </p>
    <p>
     <img alt="" height="887" src="https://i-blog.csdnimg.cn/direct/2aaea5b8e704425495561777b9ac0aa3.png" width="1378"/>
    </p>
    <p>
     TD Target是状态值的定义，当前回报加上打折后的后续状态值。TD Error是相关估计误差，类似于SGD里面的梯度。这里证明了随着迭代的持续，估计值会慢慢想真实值靠拢。
    </p>
    <p>
     <img alt="" height="981" src="https://i-blog.csdnimg.cn/direct/cdc0aef02e694b95b63b9b8f2deea0c8.png" width="1168"/>
    </p>
    <p>
     而TD Error是用于衡量t时刻的状态值与真实的状态值，如果相等皆大欢喜，如果不相等，则提供了策略修改的信息。
    </p>
    <p>
     <img alt="" height="923" src="https://i-blog.csdnimg.cn/direct/8b7be5d033794d48b9df47188fb380b2.png" width="1251"/>
    </p>
    <p>
     这里引入了一个贝尔曼方程的另一种形式-贝尔曼期望方程。
    </p>
    <p>
     <img alt="" height="831" src="https://i-blog.csdnimg.cn/direct/97a0f8cab29b4e788548e5276bae5dec.png" width="1373"/>
    </p>
    <p>
     因为是Model-free，所以只能获取到相关的采样输入，代进贝尔曼期望方程，则为
    </p>
    <p>
     <img alt="" height="348" src="https://i-blog.csdnimg.cn/direct/793680daff274f19998fba43214b088a.png" width="1314"/>
    </p>
    <p>
     所以，用RM算法计算状态值，可以表现为一下的一个迭代公式
    </p>
    <p>
     <img alt="" height="391" src="https://i-blog.csdnimg.cn/direct/08ae1ae98a004f029f3eecb611232e7f.png" width="1361"/>
    </p>
    <p>
     TD算法的公式再次贴过了，方便两者的对比（即公式3和公式6）。
    </p>
    <p>
     <img alt="" height="342" src="https://i-blog.csdnimg.cn/direct/9bd32171ab4b46bab3d0ded9b8d22d64.png" width="1385"/>
    </p>
    <p>
     两个公式有一下差异，对RM算法的表达形式（即公式三）进行对应的修改，就变成了TD算法（公式六）。
    </p>
    <p>
     <img alt="" height="579" src="https://i-blog.csdnimg.cn/direct/8cd6ae2153514064ac89afefaddc9da5.png" width="1307"/>
    </p>
    <p>
     这里对比了MC算法和TD算法，TD的算法快一些，不需要等所有抽样结束才开始算。
    </p>
    <p>
     <img alt="" height="400" src="https://i-blog.csdnimg.cn/direct/77a2932d2fa441b48d47e1f5945f6ced.png" width="1359"/>
    </p>
    <p>
     <img alt="" height="558" src="https://i-blog.csdnimg.cn/direct/d0059b6b57614e84b4dc92e25e8ae373.png" width="1377"/>
    </p>
    <p>
     TD算法适用于计算状态值的算法，对应计算行为值的类似算法叫做Saras（state-action-reward-state-action的缩写），其表达式为
    </p>
    <p>
     <img alt="" height="199" src="https://i-blog.csdnimg.cn/direct/9cd4c5c6bdbf42c98851bbe940e46584.png" width="1496"/>
    </p>
    <p>
     通过Saras算法，可以计算出行为值的期望值，并进一步找到最优策略，具体的方式如下伪代码所示；
    </p>
    <p>
     <img alt="" height="911" src="https://i-blog.csdnimg.cn/direct/9dcf8618068741ffa961c214e81aa05f.png" width="1510"/>
    </p>
    <p>
     介绍完Saras算法，后续是n-step Saras算法，这是Saras和MC算法的结合体。
    </p>
    <p>
     <img alt="" height="846" src="https://i-blog.csdnimg.cn/direct/411bb20520cd437c8c141002f3f7a4f6.png" width="1476"/>
    </p>
    <p>
     n是这个算法的一个超参数，设为1变成原版的Saras算法，设为无穷则变成了MC算法。
    </p>
    <p>
     <img alt="" height="393" src="https://i-blog.csdnimg.cn/direct/e5f87816fc4644e2a063d033fb858580.png" width="1552"/>
    </p>
    <p>
     n-step Saras结合了Saras和MC算法的特点，通过n来调整算法的倾向性。
    </p>
    <p>
     <img alt="" height="580" src="https://i-blog.csdnimg.cn/direct/32e4f44ba099462eb61871272f49c36c.png" width="1529"/>
    </p>
    <p>
     然后是大名鼎鼎的Q-Learning，Saras的思路是估算出一个策略的行为值，并结合策略改进找到最优策略。而Q-Learning的策略是一步到位。
    </p>
    <p>
     <img alt="" height="580" src="https://i-blog.csdnimg.cn/direct/40406f3d1bde473eb753acd3538720b6.png" width="1529"/>
    </p>
    <p>
     Q-Learning算法的数学模型如下所示，与Saras算法的形式类似，唯一区别就是TD Target（红框部分）
    </p>
    <p>
     <img alt="" height="311" src="https://i-blog.csdnimg.cn/direct/bd53e8d4d67e4b3da5f5cedcaef863dd.png" width="1503"/>
    </p>
    <p>
     Q-Learning本质上就是用贝尔曼最优方程计算最优行为值。后面提到了On-policy和Off-policy，如果行为策略和目标策略一致，就是On-policy，否则就是Off-policy。
    </p>
    <p>
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/cfb293ae9ec94a358c29381f3ad5768c.png" width="1461"/>
    </p>
    <p>
     Off-policy的优点就在于，可以通过另一个策略的采样结果，来找到目标策略的最优情况。
    </p>
    <p>
     <img alt="" height="127" src="https://i-blog.csdnimg.cn/direct/bffa66efb02d44a6bb3abe61adf98c35.png" width="1490"/>
    </p>
    <p>
     那么怎么判断一个TD算法是On-policy还是Off-policy呢？第一个是看要解决的数学问题，第二个是看算法对实验样本的要求。
    </p>
    <p>
     <img alt="" height="234" src="https://i-blog.csdnimg.cn/direct/3a952fddafc1472ea12869c716f965e7.png" width="1296"/>
    </p>
    <p>
     Q-Learning是Off-policy，而Saras和MC都是On-policy，因为需要计算的策略，用到的数据都是相同的策略生成的，同时也是个策略问题，都是通过迭代找到最优策略的。
    </p>
    <p>
     <img alt="" height="363" src="https://i-blog.csdnimg.cn/direct/66faeb7d0ca744bbb2769b8e9956b1c1.png" width="1531"/>
    </p>
    <p>
     Q-Learning完全不一样
    </p>
    <p>
     <img alt="" height="554" src="https://i-blog.csdnimg.cn/direct/ff11d2b5c4214ddbb03f436a4c20bc47.png" width="1566"/>
    </p>
    <p>
     Q-Learning分别可以用On-policy和Off-policy实现，下面是两种方法的伪代码
    </p>
    <p>
     <img alt="" height="776" src="https://i-blog.csdnimg.cn/direct/1fa19a3d84794ad6a258ebba65680b5f.png" width="1550"/>
    </p>
    <p>
     <img alt="" height="764" src="https://i-blog.csdnimg.cn/direct/d9c9af5f3ca94d56b6702155739ab43f.png" width="1546"/>
    </p>
    <p>
     本章介绍的几个算法，数学模型的架构都是一样的，唯一的区别就是TD Target不一样。
    </p>
    <p>
     <img alt="" height="867" src="https://i-blog.csdnimg.cn/direct/bf34747ce5f84e67803b3af8104b067b.png" width="1588"/>
    </p>
    <p>
     这几个算法都是随机近似法来解决贝尔曼方程或贝尔曼最优方程（Q-Learning）。
    </p>
    <p>
     <img alt="" height="467" src="https://i-blog.csdnimg.cn/direct/0254d9a8dff441f0b117a7ca02d736ca.png" width="1627"/>
    </p>
    <p>
     个人感觉Monte Carlo、Saras和n-step Saras，分别类似于随机梯度下降、梯度下降、小批量梯度下降。
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f707a6231393834313131362f:61727469636c652f64657461696c732f313436323435313532" class_="artid" style="display:none">
 </p>
</div>


