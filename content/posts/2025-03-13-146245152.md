---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f707a6231393834313131362f:61727469636c652f64657461696c732f313436323435313532"
layout: post
title: "强化学习赵世钰版-学习笔记7.时序差分学习"
date: 2025-03-13 23:42:49 +0800
description: "Q-Learning是Off-policy，而Saras和MC都是On-policy，因为需要计算的策略，用到的数据都是相同的策略生成的，同时也是个策略问题，都是通过迭代找到最优策略的。TD算法更新状态值的公式，可以展开来看。TD算法适用于计算状态值的算法，对应计算行为值的类似算法叫做Saras（state-action-reward-state-action的缩写），其表达式为。本章是课程算法与方法中的第四章，介绍的时序差分学习算法是基于随机近似方法设计的强化学习方法，也是model-free的方法。"
keywords: "强化学习（赵世钰版）-学习笔记（7.时序差分学习）"
categories: ['深度学习', '数学学习', '人工智能']
tags: ['笔记', '学习', '人工智能']
artid: "146245152"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146245152
    alt: "强化学习赵世钰版-学习笔记7.时序差分学习"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146245152
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146245152
cover: https://bing.ee123.net/img/rand?artid=146245152
image: https://bing.ee123.net/img/rand?artid=146245152
img: https://bing.ee123.net/img/rand?artid=146245152
---

# 强化学习（赵世钰版）-学习笔记（7.时序差分学习）

本章是课程算法与方法中的第四章，介绍的时序差分学习算法是基于随机近似方法设计的强化学习方法，也是model-free的方法。

![](https://i-blog.csdnimg.cn/direct/4f2f6bff8d5942219dac7cc663b5c3aa.png)

时序差分算法是一种近似估计策略状态值的算法，具体的形式如下：

![](https://i-blog.csdnimg.cn/direct/fcbfdf57d28e484399519db44b5c453a.png)

本质上是在当前t时刻，被访问到的状态采用近似迭代的策略（即上一章讲的RM算法）估计出一个状态值，没被访问到的维持不变。TD算法更新状态值的公式，可以展开来看。

![](https://i-blog.csdnimg.cn/direct/2aaea5b8e704425495561777b9ac0aa3.png)

TD Target是状态值的定义，当前回报加上打折后的后续状态值。TD Error是相关估计误差，类似于SGD里面的梯度。这里证明了随着迭代的持续，估计值会慢慢想真实值靠拢。

![](https://i-blog.csdnimg.cn/direct/cdc0aef02e694b95b63b9b8f2deea0c8.png)

而TD Error是用于衡量t时刻的状态值与真实的状态值，如果相等皆大欢喜，如果不相等，则提供了策略修改的信息。

![](https://i-blog.csdnimg.cn/direct/8b7be5d033794d48b9df47188fb380b2.png)

这里引入了一个贝尔曼方程的另一种形式-贝尔曼期望方程。

![](https://i-blog.csdnimg.cn/direct/97a0f8cab29b4e788548e5276bae5dec.png)

因为是Model-free，所以只能获取到相关的采样输入，代进贝尔曼期望方程，则为

![](https://i-blog.csdnimg.cn/direct/793680daff274f19998fba43214b088a.png)

所以，用RM算法计算状态值，可以表现为一下的一个迭代公式

![](https://i-blog.csdnimg.cn/direct/08ae1ae98a004f029f3eecb611232e7f.png)

TD算法的公式再次贴过了，方便两者的对比（即公式3和公式6）。

![](https://i-blog.csdnimg.cn/direct/9bd32171ab4b46bab3d0ded9b8d22d64.png)

两个公式有一下差异，对RM算法的表达形式（即公式三）进行对应的修改，就变成了TD算法（公式六）。

![](https://i-blog.csdnimg.cn/direct/8cd6ae2153514064ac89afefaddc9da5.png)

这里对比了MC算法和TD算法，TD的算法快一些，不需要等所有抽样结束才开始算。

![](https://i-blog.csdnimg.cn/direct/77a2932d2fa441b48d47e1f5945f6ced.png)

![](https://i-blog.csdnimg.cn/direct/d0059b6b57614e84b4dc92e25e8ae373.png)

TD算法适用于计算状态值的算法，对应计算行为值的类似算法叫做Saras（state-action-reward-state-action的缩写），其表达式为

![](https://i-blog.csdnimg.cn/direct/9cd4c5c6bdbf42c98851bbe940e46584.png)

通过Saras算法，可以计算出行为值的期望值，并进一步找到最优策略，具体的方式如下伪代码所示；

![](https://i-blog.csdnimg.cn/direct/9dcf8618068741ffa961c214e81aa05f.png)

介绍完Saras算法，后续是n-step Saras算法，这是Saras和MC算法的结合体。

![](https://i-blog.csdnimg.cn/direct/411bb20520cd437c8c141002f3f7a4f6.png)

n是这个算法的一个超参数，设为1变成原版的Saras算法，设为无穷则变成了MC算法。

![](https://i-blog.csdnimg.cn/direct/e5f87816fc4644e2a063d033fb858580.png)

n-step Saras结合了Saras和MC算法的特点，通过n来调整算法的倾向性。

![](https://i-blog.csdnimg.cn/direct/32e4f44ba099462eb61871272f49c36c.png)

然后是大名鼎鼎的Q-Learning，Saras的思路是估算出一个策略的行为值，并结合策略改进找到最优策略。而Q-Learning的策略是一步到位。

![](https://i-blog.csdnimg.cn/direct/40406f3d1bde473eb753acd3538720b6.png)

Q-Learning算法的数学模型如下所示，与Saras算法的形式类似，唯一区别就是TD Target（红框部分）

![](https://i-blog.csdnimg.cn/direct/bd53e8d4d67e4b3da5f5cedcaef863dd.png)

Q-Learning本质上就是用贝尔曼最优方程计算最优行为值。后面提到了On-policy和Off-policy，如果行为策略和目标策略一致，就是On-policy，否则就是Off-policy。

![](https://i-blog.csdnimg.cn/direct/cfb293ae9ec94a358c29381f3ad5768c.png)

Off-policy的优点就在于，可以通过另一个策略的采样结果，来找到目标策略的最优情况。

![](https://i-blog.csdnimg.cn/direct/bffa66efb02d44a6bb3abe61adf98c35.png)

那么怎么判断一个TD算法是On-policy还是Off-policy呢？第一个是看要解决的数学问题，第二个是看算法对实验样本的要求。

![](https://i-blog.csdnimg.cn/direct/3a952fddafc1472ea12869c716f965e7.png)

Q-Learning是Off-policy，而Saras和MC都是On-policy，因为需要计算的策略，用到的数据都是相同的策略生成的，同时也是个策略问题，都是通过迭代找到最优策略的。

![](https://i-blog.csdnimg.cn/direct/66faeb7d0ca744bbb2769b8e9956b1c1.png)

Q-Learning完全不一样

![](https://i-blog.csdnimg.cn/direct/ff11d2b5c4214ddbb03f436a4c20bc47.png)

Q-Learning分别可以用On-policy和Off-policy实现，下面是两种方法的伪代码

![](https://i-blog.csdnimg.cn/direct/1fa19a3d84794ad6a258ebba65680b5f.png)

![](https://i-blog.csdnimg.cn/direct/d9c9af5f3ca94d56b6702155739ab43f.png)

本章介绍的几个算法，数学模型的架构都是一样的，唯一的区别就是TD Target不一样。

![](https://i-blog.csdnimg.cn/direct/bf34747ce5f84e67803b3af8104b067b.png)

这几个算法都是随机近似法来解决贝尔曼方程或贝尔曼最优方程（Q-Learning）。

![](https://i-blog.csdnimg.cn/direct/0254d9a8dff441f0b117a7ca02d736ca.png)

个人感觉Monte Carlo、Saras和n-step Saras，分别类似于随机梯度下降、梯度下降、小批量梯度下降。