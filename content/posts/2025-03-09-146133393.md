---
layout: post
title: "基于PyTorch的深度学习5神经网络工具箱"
date: 2025-03-09 21:59:55 +0800
description: "多个层链接在一起构成一个模型或网络，输入数据通过这个模型转换为预测值，然后损失函数把预测值与真实值进行比较，得到损失值（损失值可以是距离、概率值等）​，该损失值用于衡量预测值与目标结果的匹配或相似程度，优化器利用损失值更新权重参数，从而使损失值越来越小。像卷积层、全连接层、Dropout层等因含有可学习参数，一般使用nn.Module，而激活函数、池化层不含可学习参数，可以使用nn.functional中对应的函数。3)损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。"
keywords: "基于PyTorch的深度学习5——神经网络工具箱"
categories: ['未分类']
tags: ['神经网络', '深度学习', 'Pytorch']
artid: "146133393"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146133393
    alt: "基于PyTorch的深度学习5神经网络工具箱"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146133393
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146133393
cover: https://bing.ee123.net/img/rand?artid=146133393
image: https://bing.ee123.net/img/rand?artid=146133393
img: https://bing.ee123.net/img/rand?artid=146133393
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     基于PyTorch的深度学习5——神经网络工具箱
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     可以学习如下内容：
    </p>
    <p>
    </p>
    <p>
     • 介绍神经网络核心组件。
    </p>
    <p>
     • 如何构建一个神经网络。
    </p>
    <p>
     • 详细介绍如何构建一个神经网络。
    </p>
    <p>
     • 如何使用nn模块中Module及functional。
    </p>
    <p>
     • 如何选择优化器。
    </p>
    <p>
     • 动态修改学习率参数。
    </p>
    <p>
    </p>
    <p>
     5.1 核心组件
    </p>
    <p>
     神经网络核心组件不多，把这些组件确定后，这个神经网络基本就确定了。这些核心组件包括：
    </p>
    <p>
     1)层：神经网络的基本结构，将输入张量转换为输出张量。
    </p>
    <p>
     2)模型：层构成的网络。
    </p>
    <p>
     3)损失函数：参数学习的目标函数，通过最小化损失函数来学习各种参数。
    </p>
    <p>
     4)优化器：如何使损失函数最小，这就涉及优化器。
    </p>
    <p>
     多个层链接在一起构成一个模型或网络，输入数据通过这个模型转换为预测值，然后损失函数把预测值与真实值进行比较，得到损失值（损失值可以是距离、概率值等）​，该损失值用于衡量预测值与目标结果的匹配或相似程度，优化器利用损失值更新权重参数，从而使损失值越来越小。这是一个循环过程，当损失值达到一个阀值或循环次数到达指定次数，循环结束。接下来利用PyTorch的nn工具箱，构建一个神经网络实例。nn中对这些组件都有现成包或类，可以直接使用，非常方便。
    </p>
    <p>
     ——————————实现神经网络实例
    </p>
    <p>
     构建网络层可以基于Module类，或函数(nn.functional)。
    </p>
    <p>
     nn.Module中的大多数层(Layer)在functional中都有与之对应的函数。
    </p>
    <p>
     nn.functional中函数与nn.Module中的Layer的主要区别是后者继承Module类，会自动提取可学习的参数。
    </p>
    <p>
     而nn.functional更像是纯函数。
    </p>
    <p>
     两者功能相同，且性能也没有很大区别，那么如何选择呢？像卷积层、全连接层、Dropout层等因含有可学习参数，一般使用nn.Module，而激活函数、池化层不含可学习参数，可以使用nn.functional中对应的函数。
    </p>
    <p>
     下面通过实例来说明如何使用nn构建一个网络模型。
    </p>
    <p>
     <img alt="" height="1225" src="https://i-blog.csdnimg.cn/direct/da8827f82cfc4082bf62d26678ac9ced.png" width="2025">
      这节将利用神经网络完成对手写数字进行识别的实例，来说明如何借助nn工具箱来实现一个神经网络，并对神经网络有个直观了解。在这个基础上，后续我们将对nn的各模块进行详细介绍。实例环境使用PyTorch1.0+，GPU或CPU，源数据集为MNIST。
     </img>
    </p>
    <p>
     主要步骤：
    </p>
    <p>
     1)利用PyTorch内置函数mnist下载数据。
    </p>
    <p>
     2)利用torchvision对数据进行预处理，调用torch.utils建立一个数据迭代器。
    </p>
    <p>
     3)可视化源数据。
    </p>
    <p>
     4)利用nn工具箱构建神经网络模型。
    </p>
    <p>
     5)实例化模型，并定义损失函数及优化器。
    </p>
    <p>
     6)训练模型。
    </p>
    <p>
     7)可视化结果。
    </p>
    <p>
     <img alt="" height="560" src="https://i-blog.csdnimg.cn/direct/61d7865d133048e99d0e66778e6cb1c0.png" width="2290"/>
    </p>
    <pre><code class="hljs">import numpy as np
import torch

from torchvision.datasets import mnist

#导入预处理模块
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

#导入nn及优化器
import torch.nn.functional as F
import torch.optim as optim
from torch import nn


</code></pre>
    <p>
     接下来，定义一些超参数
    </p>
    <pre><code class="hljs">train_batch_size=64
test_batch_size=128
learning_rate=0.01
num_epoches=20
lr=0.01
momentum=0.5</code></pre>
    <p>
     下载数据并对数据进行预处理
    </p>
    <pre><code class="hljs">#定义预处理函数，这些预处理依次放在Compose函数中。
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize([0.5], [0.5])])
#下载数据，并对数据进行预处理
train_dataset = mnist.MNIST('./data', train=True, transform=transform, download=True)
test_dataset = mnist.MNIST('./data', train=False, transform=transform)

#dataloader是一个可迭代对象，可以使用迭代器一样使用。
train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)</code></pre>
    <ul>
     <li>
      <p>
       <strong>
        <code>
         transforms.Compose
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         Compose
        </code>
        是 PyTorch 中的一个工具，用于将多个预处理操作组合在一起，依次执行。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        <code>
         transforms.ToTensor()
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        将 PIL 图像或 NumPy 数组转换为 PyTorch 张量（Tensor）。
       </li>
       <li>
        同时会将图像的像素值从
        <code>
         [0, 255]
        </code>
        转换到
        <code>
         [0, 1]
        </code>
        的范围。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        <code>
         transforms.Normalize([0.5], [0.5])
        </code>
       </strong>
       :
      </p>
      <ul>
       <li>
        对张量进行归一化处理。
       </li>
       <li>
        归一化的公式是：
        <code>
         output = (input - mean) / std
        </code>
        。
       </li>
       <li>
        这里的参数
        <code>
         [0.5]
        </code>
        表示均值（mean），
        <code>
         [0.5]
        </code>
        表示标准差（std）。
       </li>
       <li>
        因此，归一化后数据的范围会从
        <code>
         [0, 1]
        </code>
        变为
        <code>
         [-1, 1]
        </code>
       </li>
      </ul>
     </li>
    </ul>
    <ul>
     <li>
      <ul>
       <li>
        <code>
         DataLoader
        </code>
        是 PyTorch 提供的一个工具，用于将数据集分批加载，并支持多线程、打乱顺序等功能。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       参数解释
      </strong>
      :
      <ul>
       <li>
        <code>
         train_dataset
        </code>
        和
        <code>
         test_dataset
        </code>
        : 分别指定训练集和测试集。
       </li>
       <li>
        <code>
         batch_size
        </code>
        : 每次加载的数据批量大小。
        <code>
         train_batch_size
        </code>
        和
        <code>
         test_batch_size
        </code>
        应该在代码其他地方定义。
       </li>
       <li>
        <code>
         shuffle=True
        </code>
        : 是否在每个 epoch 开始时打乱数据顺序。通常在训练集上设置为
        <code>
         True
        </code>
        ，以增加模型的泛化能力。
       </li>
       <li>
        <code>
         shuffle=False
        </code>
        : 测试集一般不需要打乱顺序。
       </li>
      </ul>
     </li>
    </ul>
    <p>
     ——————————————可视化源数据
    </p>
    <pre><code class="hljs">import matplotlib.pyplot as plt

examples=enumerate(test_loader)
batch_idx,(example_data,example_targets)=next(examples)

fig=plt.figure()
for i in range(6):
    plt.subplot(2,3,i+1)
    plt.tight_layout()
    plt.imshow(example_data[i][0],cmap="gray",interpolation='none')
    plt.title("Ground Truth:{}".format(example_targets[i]))
    plt.xticks([])
    plt.yticks([])
</code></pre>
    <p>
     _______________________构建模型
    </p>
    <p>
     数据预处理结束后，开始构建模型
    </p>
    <p>
     1）构建网络的类Net
    </p>
    <pre><code class="hljs">class Net(nn.Module):
    
    """
    使用sequential构建网络，Sequential()函数的功能是将网络的层组合到一起
    """
    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):
        super(Net, self).__init__()

        self.layer1 = nn.Sequential(nn.Linear(in_dim,
n_hidden_1),nn.BatchNorm1d(n_hidden_1))
        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2),nn.BatchNorm1d (n_hidden_2))
        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = F.relu(self.layer2(x))
        x = self.layer3(x)
        return x</code></pre>
    <p>
     2）创造实例
    </p>
    <pre><code class="hljs">#检测是否有可用的GPU，有则使用，否则使用CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#实例化网络
model = Net(28 * 28, 300, 100, 10)
model.to(device)

# 定义损失函数criterion和优化器optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</code></pre>
    <p>
     以下内容为大语言模型生成：
    </p>
    <p>
     这段代码实现了一个使用
     <code>
      nn.Sequential
     </code>
     构建的三层全连接神经网络，包含批归一化和ReLU激活函数。以下是对代码的分析及优化建议：
    </p>
    <hr/>
    <h4>
     <strong>
      代码结构分析
     </strong>
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        网络定义
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         Layer1
        </strong>
        ：线性层（
        <code>
         in_dim
        </code>
        →
        <code>
         n_hidden_1
        </code>
        ） + 批归一化。
       </li>
       <li>
        <strong>
         Layer2
        </strong>
        ：线性层（
        <code>
         n_hidden_1
        </code>
        →
        <code>
         n_hidden_2
        </code>
        ） + 批归一化。
       </li>
       <li>
        <strong>
         Layer3
        </strong>
        ：线性层（
        <code>
         n_hidden_2
        </code>
        →
        <code>
         out_dim
        </code>
        ），无批归一化和激活函数。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        前向传播
       </strong>
       ：
      </p>
      <ul>
       <li>
        前两层使用ReLU激活，最后一层输出原始 logits（适用于分类任务结合
        <code>
         CrossEntropyLoss
        </code>
        或回归任务）。
       </li>
      </ul>
     </li>
    </ol>
    <p>
     ————————训练模型
    </p>
    <p>
     这里使用for循环进行迭代。其中包括对训练数据的训练模型，然后用测试数据的验证模型。
    </p>
    <p>
     1.训练模型
    </p>
    <pre><code class="hljs">import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm

# 假设已定义模型、数据集、损失函数和优化器
# model = Net(...)
# train_dataset = ...
# test_dataset = ...

# 数据预处理（展平）
class FlattenTransform:
    def __call__(self, img):
        return img.view(-1)

transform = transforms.Compose([
    transforms.ToTensor(),
    FlattenTransform()
])

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64)

# 初始化
losses = []
acces = []
eval_losses = []
eval_acces = []

for epoch in range(num_epoches):
    # 动态调整学习率
    if epoch &gt; 0 and epoch % 5 == 0:
        for param_group in optimizer.param_groups:
            new_lr = max(param_group['lr'] * 0.1, 1e-6)
            param_group['lr'] = new_lr
            print(f"Learning rate updated to {new_lr}")

    # 训练阶段
    model.train()
    train_loss = 0
    train_correct = 0
    train_total = 0
    for img, label in tqdm(train_loader, desc=f"Epoch {epoch+1} Training"):
        img, label = img.to(device), label.to(device)
        
        # 前向传播
        output = model(img)
        loss = criterion(output, label)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # 记录指标
        train_loss += loss.item()
        _, pred = output.max(1)
        train_correct += (pred == label).sum().item()
        train_total += label.size(0)
    
    # 计算平均指标
    avg_train_loss = train_loss / len(train_loader)
    avg_train_acc = train_correct / train_total
    losses.append(avg_train_loss)
    acces.append(avg_train_acc)

    # 测试阶段
    model.eval()
    eval_loss, eval_acc = calculate_metrics(test_loader, model, criterion, device)
    eval_losses.append(eval_loss)
    eval_acces.append(eval_acc)
    
    # 打印日志
    print(f'Epoch {epoch+1}/{num_epoches}: '
          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}, '
          f'Test Loss: {eval_loss:.4f}, Test Acc: {eval_acc:.4f}')</code></pre>
    <p>
     以下内容由大语言模型生成
    </p>
    <h4>
     <strong>
      为什么需要
      <code>
       model.eval()
      </code>
      ？
     </strong>
    </h4>
    <p>
     在神经网络中，某些层（如
     <code>
      BatchNorm
     </code>
     、
     <code>
      Dropout
     </code>
     ）在训练（training）和推理（evaluation）阶段的行为不同：
    </p>
    <ol>
     <li>
      <p>
       <strong>
        BatchNorm 层
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         训练阶段
        </strong>
        ：使用当前 batch 的均值和方差进行标准化，并更新运行统计量（running mean/variance）。
       </li>
       <li>
        <strong>
         推理阶段
        </strong>
        ：使用训练阶段累积的运行统计量，而非当前 batch 的统计量。
       </li>
       <li>
        若不调用
        <code>
         model.eval()
        </code>
        ，推理时会继续更新统计量，导致结果不稳定。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        Dropout 层
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         训练阶段
        </strong>
        ：随机丢弃部分神经元，防止过拟合。
       </li>
       <li>
        <strong>
         推理阶段
        </strong>
        ：关闭 Dropout，使用所有神经元的输出（权重按概率缩放）。
       </li>
       <li>
        若不调用
        <code>
         model.eval()
        </code>
        ，推理时会继续随机丢弃神经元，导致结果随机。
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h4>
     <strong>
      功能详解
     </strong>
    </h4>
    <p>
     调用
     <code>
      model.eval()
     </code>
     后：
    </p>
    <ol>
     <li>
      <strong>
       关闭训练相关行为
      </strong>
      ：
      <ul>
       <li>
        <code>
         BatchNorm
        </code>
        层停止计算均值/方差，使用累积的统计量。
       </li>
       <li>
        <code>
         Dropout
        </code>
        层停止随机丢弃神经元。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       不影响梯度计算
      </strong>
      ：
      <ul>
       <li>
        <code>
         model.eval()
        </code>
        仅改变层的行为，不涉及梯度计算。如果需要禁用梯度，需配合
        <code>
         torch.no_grad()
        </code>
        。
       </li>
      </ul>
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="687474:70733a2f2f626c6f672e6373646e2e6e65742f57697334652f:61727469636c652f64657461696c732f313436313333333933" class_="artid" style="display:none">
 </p>
</div>


