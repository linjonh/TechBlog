---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37343832343439362f:61727469636c652f64657461696c732f313434333937313231"
layout: post
title: "LLMOllama本地大模型-WebAPI-调用"
date: 2024-12-11 12:09:33 +08:00
description: "从下载并安装。"
keywords: "ollama api调用"
categories: ['未分类']
tags: ['Java']
artid: "144397121"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=144397121
    alt: "LLMOllama本地大模型-WebAPI-调用"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=144397121
featuredImagePreview: https://bing.ee123.net/img/rand?artid=144397121
cover: https://bing.ee123.net/img/rand?artid=144397121
image: https://bing.ee123.net/img/rand?artid=144397121
img: https://bing.ee123.net/img/rand?artid=144397121
---

# 【LLM】Ollama：本地大模型 WebAPI 调用

#### Ollama 快速部署

* **安装 Docker**
  ：从
  [Docker 官网](https://www.docker.com/get-started)
  下载并安装。
* **部署 Ollama**
  ：

  + 使用以下命令进行部署：

    docker run -d -p 11434:11434 --name ollama --restart always ollama/ollama:latest
* **进入容器并下载 qwen2.5:0.5b 模型**
  ：

  + 进入 Ollama 容器：

    docker exec -it ollama bash
  + 在容器内下载模型：

    ollama pull qwen2.5:0.5b

#### Python 环境准备

在开始之前，请确保您已安装
`requests`
模块。可以通过以下命令安装：

```
pip install requests

```

我们还将初始化基本的 API 相关配置，如下所示：

```
import requests

# 基础初始化设置
base_url = "http://localhost:11434/api"
headers = {
    "Content-Type": "application/json"
}

```

#### 对话方式

##### 生成文本补全 (Generate a Completion)

* **API**
  :
  `/generate`
* **功能**
  : 生成指定模型的文本补全。输入提示词后，模型根据提示生成文本结果。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `model`
    （必填）：模型名称，用于指定生成模型，例如
    `qwen2.5:0.5b`
    。
  + `prompt`
    （必填）：生成文本所用的提示词。
  + `suffix`
    （可选）：生成的补全文本之后附加的文本。
  + `stream`
    （可选）：是否流式传输响应，默认
    `true`
    ，设置为
    `false`
    时返回完整文本。
  + `system`
    （可选）：覆盖模型系统信息的字段，影响生成文本风格。
  + `temperature`
    （可选）：控制文本生成的随机性，默认值为
    `1`
    。

  def generate_completion(prompt, model=“qwen2.5:0.5b”):
    
  url = f"{base_url}/generate"
    
  data = {
    
  “model”: model,
    
  “prompt”: prompt,
    
  “stream”: False
    
  }
    
  response = requests.post(url, headers=headers, json=data)
    
  return response.json().get(‘response’, ‘’)

  ## 示例调用

  completion = generate_completion(“介绍一下人工智能。”)
    
  print(“生成文本补全:”, completion)

##### 流式生成文本补全 (Streaming Completion)

* **API**
  :
  `/generate`
* **功能**
  : 流式生成文本补全，模型会逐步返回生成的结果，适用于长文本。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `model`
    （必填）：模型名称。
  + `prompt`
    （必填）：生成文本所用的提示词。
  + `stream`
    （必填）：设置为
    `true`
    ，启用流式传输。

  def generate_completion_stream(prompt, model=“qwen2.5:0.5b”):
    
  url = f"{base_url}/generate"
    
  data = {
    
  “model”: model,
    
  “prompt”: prompt,
    
  “stream”: True
    
  }
    
  response = requests.post(url, headers=headers, json=data, stream=True)
    
  result = “”
    
  for line in response.iter_lines():
    
  if line:
    
  result += line.decode(‘utf-8’)
    
  return result

  ## 示例调用

  stream_completion = generate_completion_stream(“讲解机器学习的应用。”)
    
  print(“流式生成文本补全:”, stream_completion)

##### 生成对话补全 (Chat Completion)

* **API**
  :
  `/chat`
* **功能**
  : 模拟对话补全，支持多轮交互，适用于聊天机器人等场景。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `model`
    （必填）：模型名称。
  + `messages`
    （必填）：对话的消息列表，按顺序包含历史对话，每条消息包含
    `role`
    和
    `content`
    。
    - `role`
      :
      `user`
      （用户）、
      `assistant`
      （助手）或
      `system`
      （系统）。
    - `content`
      : 消息内容。
  + `stream`
    （可选）：是否流式传输响应，默认
    `true`
    。

  def generate_chat_completion(messages, model=“qwen2.5:0.5b”):
    
  url = f"{base_url}/chat"
    
  data = {
    
  “model”: model,
    
  “messages”: messages,
    
  “stream”: False
    
  }
    
  response = requests.post(url, headers=headers, json=data)
    
  return response.json().get(‘message’, {}).get(‘content’, ‘’)

  ## 示例调用

  messages = [
    
  {“role”: “user”, “content”: “什么是自然语言处理？”},
    
  {“role”: “assistant”, “content”: “自然语言处理是人工智能的一个领域，专注于人与计算机之间的自然语言交互。”}
    
  ]
    
  chat_response = generate_chat_completion(messages)
    
  print(“生成对话补全:”, chat_response)

##### 生成文本嵌入 (Generate Embeddings)

* **API**
  :
  `/embed`
* **功能**
  : 为输入的文本生成嵌入向量，常用于语义搜索或分类等任务。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `model`
    （必填）：生成嵌入的模型名称。
  + `input`
    （必填）：文本或文本列表，用于生成嵌入。
  + `truncate`
    （可选）：是否在文本超出上下文长度时进行截断，默认
    `true`
    。
  + `stream`
    （可选）：是否流式传输响应，默认
    `true`
    。

  def generate_embeddings(text, model=“qwen2.5:0.5b”):
    
  url = f"{base_url}/embed"
    
  data = {
    
  “model”: model,
    
  “input”: text
    
  }
    
  response = requests.post(url, headers=headers, json=data)
    
  return response.json()

  ## 示例调用

  embeddings = generate_embeddings(“什么是深度学习？”)
    
  print(“生成文本嵌入:”, embeddings)

#### 模型的增删改查

##### 列出本地模型 (List Local Models)

* **API**
  :
  `/tags`
* **功能**
  : 列出本地已加载的所有模型。
* **请求方法**
  :
  `GET`
* **Ollama API 参数**
  : 无。

  def list_local_models():
    
  url = f"{base_url}/tags"
    
  response = requests.get(url, headers=headers)
    
  return response.json().get(‘models’, [])

  ## 示例调用

  local_models = list_local_models()
    
  print(“列出本地模型:”, local_models)

##### 查看模型信息 (Show Model Information)

* **API**
  :
  `/show`
* **功能**
  : 查看特定模型的详细信息，如模型的参数、版本、系统信息等。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `name`
    （必填）：需要查看信息的模型名称。
  + `verbose`
    （可选）：设置为
    `true`
    时返回更详细的信息。

  def show_model_info(model=“qwen2.5:0.5b”):
    
  url = f"{base_url}/show"
    
  data = {“name”: model}
    
  response = requests.post(url, headers=headers, json=data)
    
  return response.json()

  ## 示例调用

  model_info = show_model_info()
    
  print(“模型信息:”, model_info)

##### 创建模型 (Create a Model)

* **API**
  :
  `/create`
* **功能**
  : 根据指定的
  `Modelfile`
  创建一个新模型，可以在已有模型的基础上定制。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `name`
    （必填）：新创建的模型名称。
  + `modelfile`
    （可选）：模型文件的内容，定义模型的基础信息。
  + `stream`
    （可选）：是否流式传输响应。

  def create_model(model_name=“qwen2.5_custom”, base_model=“qwen2.5:0.5b”):
    
  url = f"{base_url}/create"
    
  data = {
    
  “name”: model_name,
    
  “modelfile”: f"FROM {base_model}
    
  SYSTEM You are a helpful assistant."
    
  }
    
  response = requests.post(url, headers=headers, json=data)
    
  return response.json()

  ## 示例调用

  create_response = create_model()
    
  print(“创建模型:”, create_response)

##### 拉取模型 (Pull a Model)

* **API**
  :
  `/api/pull`
* **功能**
  : 从 Ollama 库下载模型。取消的拉取将从上次中断的位置继续，并且多个调用将共享相同的下载进度。
* **请求方法**
  :
  `POST`
* **Ollama API 参数**
  :

  + `name`
    （必填）：要拉取的模型名称。
  + `insecure`
    （可选）：允许与库建立不安全的连接，仅在开发过程中从自己的库中提取时使用。
  + `stream`
    （可选）：如果为
    `false`
    ，响应将作为单个响应对象返回，而不是对象流。

  def pull_model(model_name=“qwen2.5:0.5b”):
    
  url = f"{base_url}/api/pull"
    
  data = {“name”: model_name}
    
  response = requests.post(url, headers=headers, json=data)
    
  return response.json()

  ## 示例调用

  pull_response = pull_model()
    
  print(“拉取模型:”, pull_response)

##### 删除模型 (Delete a Model)

* **API**
  :
  `/delete`
* **功能**
  : 删除本地模型以及其相关的数据。
* **请求方法**
  :
  `DELETE`
* **Ollama API 参数**
  :

  + `name`
    （必填）：需要删除的模型名称。

  def delete_model(model_name=“qwen2.5_custom”):
    
  url = f"{base_url}/delete"
    
  data = {“name”: model_name}
    
  response = requests.delete(url, headers=headers, json=data)
    
  return response.json()

  ## 示例调用

  delete_response = delete_model()
    
  print(“删除模型:”, delete_response)