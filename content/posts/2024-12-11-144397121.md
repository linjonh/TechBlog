---
layout: post
title: "LLMOllama本地大模型-WebAPI-调用"
date: 2024-12-11 12:09:33 +0800
description: "从下载并安装。"
keywords: "ollama api调用"
categories: ['未分类']
tags: ['Java']
artid: "144397121"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=144397121
    alt: "LLMOllama本地大模型-WebAPI-调用"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=144397121
featuredImagePreview: https://bing.ee123.net/img/rand?artid=144397121
cover: https://bing.ee123.net/img/rand?artid=144397121
image: https://bing.ee123.net/img/rand?artid=144397121
img: https://bing.ee123.net/img/rand?artid=144397121
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【LLM】Ollama：本地大模型 WebAPI 调用
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h4>
     <a id="Ollama__4">
     </a>
     Ollama 快速部署
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        安装 Docker
       </strong>
       ：从
       <a href="https://www.docker.com/get-started" rel="nofollow">
        Docker 官网
       </a>
       下载并安装。
      </p>
     </li>
     <li>
      <p>
       <strong>
        部署 Ollama
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         使用以下命令进行部署：
        </p>
        <p>
         docker run -d -p 11434:11434 --name ollama --restart always ollama/ollama:latest
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        进入容器并下载 qwen2.5:0.5b 模型
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         进入 Ollama 容器：
        </p>
        <p>
         docker exec -it ollama bash
        </p>
       </li>
       <li>
        <p>
         在容器内下载模型：
        </p>
        <p>
         ollama pull qwen2.5:0.5b
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="Python__28">
     </a>
     Python 环境准备
    </h4>
    <p>
     在开始之前，请确保您已安装
     <code>
      requests
     </code>
     模块。可以通过以下命令安装：
    </p>
    <pre><code>pip install requests
</code></pre>
    <p>
     我们还将初始化基本的 API 相关配置，如下所示：
    </p>
    <pre><code>import requests

# 基础初始化设置
base_url = "http://localhost:11434/api"
headers = {
    "Content-Type": "application/json"
}
</code></pre>
    <h4>
     <a id="_46">
     </a>
     对话方式
    </h4>
    <h5>
     <a id="_Generate_a_Completion_48">
     </a>
     生成文本补全 (Generate a Completion)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /generate
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 生成指定模型的文本补全。输入提示词后，模型根据提示生成文本结果。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         model
        </code>
        （必填）：模型名称，用于指定生成模型，例如
        <code>
         qwen2.5:0.5b
        </code>
        。
       </li>
       <li>
        <code>
         prompt
        </code>
        （必填）：生成文本所用的提示词。
       </li>
       <li>
        <code>
         suffix
        </code>
        （可选）：生成的补全文本之后附加的文本。
       </li>
       <li>
        <code>
         stream
        </code>
        （可选）：是否流式传输响应，默认
        <code>
         true
        </code>
        ，设置为
        <code>
         false
        </code>
        时返回完整文本。
       </li>
       <li>
        <code>
         system
        </code>
        （可选）：覆盖模型系统信息的字段，影响生成文本风格。
       </li>
       <li>
        <code>
         temperature
        </code>
        （可选）：控制文本生成的随机性，默认值为
        <code>
         1
        </code>
        。
       </li>
      </ul>
      <p>
       def generate_completion(prompt, model=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/generate"
       <br/>
       data = {
       <!-- -->
       <br/>
       “model”: model,
       <br/>
       “prompt”: prompt,
       <br/>
       “stream”: False
       <br/>
       }
       <br/>
       response = requests.post(url, headers=headers, json=data)
       <br/>
       return response.json().get(‘response’, ‘’)
      </p>
      <h2>
       <a id="_71">
       </a>
       示例调用
      </h2>
      <p>
       completion = generate_completion(“介绍一下人工智能。”)
       <br/>
       print(“生成文本补全:”, completion)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Streaming_Completion_76">
     </a>
     流式生成文本补全 (Streaming Completion)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /generate
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 流式生成文本补全，模型会逐步返回生成的结果，适用于长文本。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         model
        </code>
        （必填）：模型名称。
       </li>
       <li>
        <code>
         prompt
        </code>
        （必填）：生成文本所用的提示词。
       </li>
       <li>
        <code>
         stream
        </code>
        （必填）：设置为
        <code>
         true
        </code>
        ，启用流式传输。
       </li>
      </ul>
      <p>
       def generate_completion_stream(prompt, model=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/generate"
       <br/>
       data = {
       <!-- -->
       <br/>
       “model”: model,
       <br/>
       “prompt”: prompt,
       <br/>
       “stream”: True
       <br/>
       }
       <br/>
       response = requests.post(url, headers=headers, json=data, stream=True)
       <br/>
       result = “”
       <br/>
       for line in response.iter_lines():
       <br/>
       if line:
       <br/>
       result += line.decode(‘utf-8’)
       <br/>
       return result
      </p>
      <h2>
       <a id="_100">
       </a>
       示例调用
      </h2>
      <p>
       stream_completion = generate_completion_stream(“讲解机器学习的应用。”)
       <br/>
       print(“流式生成文本补全:”, stream_completion)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Chat_Completion_105">
     </a>
     生成对话补全 (Chat Completion)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /chat
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 模拟对话补全，支持多轮交互，适用于聊天机器人等场景。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         model
        </code>
        （必填）：模型名称。
       </li>
       <li>
        <code>
         messages
        </code>
        （必填）：对话的消息列表，按顺序包含历史对话，每条消息包含
        <code>
         role
        </code>
        和
        <code>
         content
        </code>
        。
        <ul>
         <li>
          <code>
           role
          </code>
          :
          <code>
           user
          </code>
          （用户）、
          <code>
           assistant
          </code>
          （助手）或
          <code>
           system
          </code>
          （系统）。
         </li>
         <li>
          <code>
           content
          </code>
          : 消息内容。
         </li>
        </ul>
       </li>
       <li>
        <code>
         stream
        </code>
        （可选）：是否流式传输响应，默认
        <code>
         true
        </code>
        。
       </li>
      </ul>
      <p>
       def generate_chat_completion(messages, model=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/chat"
       <br/>
       data = {
       <!-- -->
       <br/>
       “model”: model,
       <br/>
       “messages”: messages,
       <br/>
       “stream”: False
       <br/>
       }
       <br/>
       response = requests.post(url, headers=headers, json=data)
       <br/>
       return response.json().get(‘message’, {}).get(‘content’, ‘’)
      </p>
      <h2>
       <a id="_127">
       </a>
       示例调用
      </h2>
      <p>
       messages = [
       <br/>
       {“role”: “user”, “content”: “什么是自然语言处理？”},
       <br/>
       {“role”: “assistant”, “content”: “自然语言处理是人工智能的一个领域，专注于人与计算机之间的自然语言交互。”}
       <br/>
       ]
       <br/>
       chat_response = generate_chat_completion(messages)
       <br/>
       print(“生成对话补全:”, chat_response)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Generate_Embeddings_136">
     </a>
     生成文本嵌入 (Generate Embeddings)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /embed
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 为输入的文本生成嵌入向量，常用于语义搜索或分类等任务。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         model
        </code>
        （必填）：生成嵌入的模型名称。
       </li>
       <li>
        <code>
         input
        </code>
        （必填）：文本或文本列表，用于生成嵌入。
       </li>
       <li>
        <code>
         truncate
        </code>
        （可选）：是否在文本超出上下文长度时进行截断，默认
        <code>
         true
        </code>
        。
       </li>
       <li>
        <code>
         stream
        </code>
        （可选）：是否流式传输响应，默认
        <code>
         true
        </code>
        。
       </li>
      </ul>
      <p>
       def generate_embeddings(text, model=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/embed"
       <br/>
       data = {
       <!-- -->
       <br/>
       “model”: model,
       <br/>
       “input”: text
       <br/>
       }
       <br/>
       response = requests.post(url, headers=headers, json=data)
       <br/>
       return response.json()
      </p>
      <h2>
       <a id="_156">
       </a>
       示例调用
      </h2>
      <p>
       embeddings = generate_embeddings(“什么是深度学习？”)
       <br/>
       print(“生成文本嵌入:”, embeddings)
      </p>
     </li>
    </ul>
    <h4>
     <a id="_161">
     </a>
     模型的增删改查
    </h4>
    <h5>
     <a id="_List_Local_Models_163">
     </a>
     列出本地模型 (List Local Models)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /tags
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 列出本地已加载的所有模型。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        GET
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       : 无。
      </p>
      <p>
       def list_local_models():
       <br/>
       url = f"{base_url}/tags"
       <br/>
       response = requests.get(url, headers=headers)
       <br/>
       return response.json().get(‘models’, [])
      </p>
      <h2>
       <a id="_175">
       </a>
       示例调用
      </h2>
      <p>
       local_models = list_local_models()
       <br/>
       print(“列出本地模型:”, local_models)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Show_Model_Information_180">
     </a>
     查看模型信息 (Show Model Information)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /show
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 查看特定模型的详细信息，如模型的参数、版本、系统信息等。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         name
        </code>
        （必填）：需要查看信息的模型名称。
       </li>
       <li>
        <code>
         verbose
        </code>
        （可选）：设置为
        <code>
         true
        </code>
        时返回更详细的信息。
       </li>
      </ul>
      <p>
       def show_model_info(model=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/show"
       <br/>
       data = {“name”: model}
       <br/>
       response = requests.post(url, headers=headers, json=data)
       <br/>
       return response.json()
      </p>
      <h2>
       <a id="_195">
       </a>
       示例调用
      </h2>
      <p>
       model_info = show_model_info()
       <br/>
       print(“模型信息:”, model_info)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Create_a_Model_200">
     </a>
     创建模型 (Create a Model)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /create
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 根据指定的
       <code>
        Modelfile
       </code>
       创建一个新模型，可以在已有模型的基础上定制。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         name
        </code>
        （必填）：新创建的模型名称。
       </li>
       <li>
        <code>
         modelfile
        </code>
        （可选）：模型文件的内容，定义模型的基础信息。
       </li>
       <li>
        <code>
         stream
        </code>
        （可选）：是否流式传输响应。
       </li>
      </ul>
      <p>
       def create_model(model_name=“qwen2.5_custom”, base_model=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/create"
       <br/>
       data = {
       <!-- -->
       <br/>
       “name”: model_name,
       <br/>
       “modelfile”: f"FROM {base_model}
       <br/>
       SYSTEM You are a helpful assistant."
       <br/>
       }
       <br/>
       response = requests.post(url, headers=headers, json=data)
       <br/>
       return response.json()
      </p>
      <h2>
       <a id="_220">
       </a>
       示例调用
      </h2>
      <p>
       create_response = create_model()
       <br/>
       print(“创建模型:”, create_response)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Pull_a_Model_225">
     </a>
     拉取模型 (Pull a Model)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /api/pull
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 从 Ollama 库下载模型。取消的拉取将从上次中断的位置继续，并且多个调用将共享相同的下载进度。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        POST
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         name
        </code>
        （必填）：要拉取的模型名称。
       </li>
       <li>
        <code>
         insecure
        </code>
        （可选）：允许与库建立不安全的连接，仅在开发过程中从自己的库中提取时使用。
       </li>
       <li>
        <code>
         stream
        </code>
        （可选）：如果为
        <code>
         false
        </code>
        ，响应将作为单个响应对象返回，而不是对象流。
       </li>
      </ul>
      <p>
       def pull_model(model_name=“qwen2.5:0.5b”):
       <br/>
       url = f"{base_url}/api/pull"
       <br/>
       data = {“name”: model_name}
       <br/>
       response = requests.post(url, headers=headers, json=data)
       <br/>
       return response.json()
      </p>
      <h2>
       <a id="_241">
       </a>
       示例调用
      </h2>
      <p>
       pull_response = pull_model()
       <br/>
       print(“拉取模型:”, pull_response)
      </p>
     </li>
    </ul>
    <h5>
     <a id="_Delete_a_Model_246">
     </a>
     删除模型 (Delete a Model)
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        API
       </strong>
       :
       <code>
        /delete
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        功能
       </strong>
       : 删除本地模型以及其相关的数据。
      </p>
     </li>
     <li>
      <p>
       <strong>
        请求方法
       </strong>
       :
       <code>
        DELETE
       </code>
      </p>
     </li>
     <li>
      <p>
       <strong>
        Ollama API 参数
       </strong>
       :
      </p>
      <ul>
       <li>
        <code>
         name
        </code>
        （必填）：需要删除的模型名称。
       </li>
      </ul>
      <p>
       def delete_model(model_name=“qwen2.5_custom”):
       <br/>
       url = f"{base_url}/delete"
       <br/>
       data = {“name”: model_name}
       <br/>
       response = requests.delete(url, headers=headers, json=data)
       <br/>
       return response.json()
      </p>
      <h2>
       <a id="_260">
       </a>
       示例调用
      </h2>
      <p>
       delete_response = delete_model()
       <br/>
       print(“删除模型:”, delete_response)
      </p>
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37343832343439362f:61727469636c652f64657461696c732f313434333937313231" class_="artid" style="display:none">
 </p>
</div>


