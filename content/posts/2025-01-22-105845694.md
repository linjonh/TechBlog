---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34323436333837312f:61727469636c652f64657461696c732f313035383435363934"
layout: post
title: "华为Atlas人工智能计算平台概述"
date: 2025-01-22 09:08:55 +08:00
description: "人工智能的四大要素：数据、算力、算法、场景AI芯片：也被称为AI加速器，即专门用于处理人工智能应用中"
keywords: "华为atlas csdn"
categories: ['智能计算']
tags: ['运维', '深度学习', 'Tensorflow']
artid: "105845694"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=105845694
    alt: "华为Atlas人工智能计算平台概述"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=105845694
featuredImagePreview: https://bing.ee123.net/img/rand?artid=105845694
---

# 华为Atlas人工智能计算平台概述

**人工智能的四大要素：数据、算力、算法、场景**

**AI芯片**
：也被称为AI加速器，即专门用于处理人工智能应用中的大量计算任务的功能模块。

### AI芯片分类

从技术架构来看，大致分为四个类型：CPU、GPU、ASIC、FPGA。
*之前的博客中都有提到，这里不做介绍。*

**从业务应用来看，可以分为Training（训练）和Inference（推理）两个类型：**

**训练环节**
通常需要通过大量的数据输入，或者采取增强学习等非监督学习方法，训练出复杂的深度神经网络模型，训练过程中，涉及到海量的训练数据和复杂的深度神经网络结构，运算量巨大，需要庞大的计算规模，对于处理器的计算能力，精度、可扩展性等性能要求很高，常用的例如：英伟达的GPU集群、Google的TPU（张量处理单元）等。

**推理环节**
指利用训练好的模型，使用新的数据去推理出各种结论，比如视频监控设备通过后台的深度神经网络模型，判断一张抓拍的人脸是否属于特定的目标，虽然推理的计算量相比于训练环节要少很多，但是仍然会设计到很多的运算，在推理环节，GPU、FPGA和ASIC都有很多的应用价值。

所以说对于人工智能来说，对
**算力**
的要求很高。

### AI芯片的现状

**CPU**
：早期的计算机性能会随着摩尔定律逐年稳步提升；后来摩尔定律失效，就通过增加核数提升性能，但是功耗和成本会增加；后来就通过修改架构的方式来提升AI性能，比如在复杂指令集架构中加入AVX512等指令，在ALU计算模块加入矢量运算模块（FMA）,或者是在精简指令集架构中加入Cortex A等指令集并计算持续升级；然后通过提高频率来提升性能，但是提升空间有限，高主频会导致芯片出现功耗过大和过热的问题。

**GPU**
：在矩阵计算和并行计算上具有突出的性能，异构计算的主力，早期被作为深度学习的加速芯片进入AI淋雨，并且，生态成熟（最早是为了游戏）

英伟达沿用了GPU的架构，在这个基础上，在蛇毒学习的两个方向上发力，一个方向是丰富生态，在这个方向上推出了cuDNN针对神经网络的优化库，提升易用性并优化GPU底层架构；另一个方向就是提升定制性，增加多数据类型支持，不再是只坚持float32了，还增加了int8等数据类型，还添加了深度学习专用模块，（如引入并配置了张量核的改进型架构，V100的TensorCore）

对于GPU来说，当前存在的主要问题就是成本高、能耗比低、延迟高。

**TPU**
（Tensor Processing Unit）

谷歌从2006年起致力于将ASIC的设计理念应用到神经网络领域，发布了支持深度学习开源框架TensorFlow的人工智能定制芯片TPU(Tensor Processing Unit)。

利用大规模脉动阵列结合大容量片上存储来高效加速深度神经网络中最为常见的卷积运算：脉动阵列可用来优化矩阵乘法和卷积运算，以达到提供更高算力和更低能耗的作用。

**FPGA**

采用HDL可编程方式，灵活性高，可以进行重构，可深度定制。

可以通过多片FPGA联合将DNN模型加载到片上进行低时延计算，计算性能优于GPU，但由于需要考虑不断的擦写的过程，性能达不到最优（冗余晶体管和连线，相同功能逻辑电路占芯片面积更大）。

由于可以重构，供货风险和研发风险较低，成本取决于购买的数量，相对自由。

设计、流片过程解耦，开发周期较长（通常半年），门槛高。

昇腾AI处理器

NPU（Neural-Network Processing Unit，神经网络处理器）：在电路层模拟人类神经元和突触，并且用深度学习指令集直接处理大规模的神经元和突触，一条指令完成一组神经元的处理。

NPU的典型代表-----华为昇腾AI芯片（Ascend）、寒武纪芯片、IBM的TrueNorth。

对于华为公司的昇腾AI芯片主要有两款，昇腾310和昇腾910.

对于昇腾310来说，主要用于推理应用，昇腾910主要用于训练，二者的功耗也有一定的差异，如下图所示，
![](https://i-blog.csdnimg.cn/blog_migrate/a838fb79a95d172d28eb27e8af64daa1.png)

### 昇腾芯片硬件架构

昇腾芯片硬件架构由昇腾AI处理器逻辑架构和以及达芬奇架构。

#### 昇腾处理器逻辑架构

昇腾AI处理器的主要架构组成：

1.芯片系统控制CPU

2.AI计算引擎（包括AI Core和AI CPU）

3.多层级的片上系统缓存（Cache）或缓冲区（Buffer）

4.数字数据预处理模块（Digital Vision Pre-Processing，DVPP）等

如下图所示是昇腾AI处理器的逻辑架构

![](https://i-blog.csdnimg.cn/blog_migrate/f595c231160b3662f2e3523e74e12ea1.png)

#### 昇腾AI计算引擎---达芬奇架构

昇腾AI处理器的四大主要架构之一是AI计算引擎，AI计算引擎有包含了AI core （达芬奇架构）和AI CPU。因此达芬奇架构这一专门为AI算力提升所研发的架构 也就是昇腾AI计算引擎和AI处理器的核心所在。

达芬奇架构主要部分：

1.计算单元： 包含三种基础计算资源（矩阵计算单元、向量计算单元、标量计算单元）

*计算单元会跟传统的CPU和GPU有一定的区别，它包含了三种基础计算资源，将不同的数据类型放到不同的计算资源里面，最大化的提升计算力度。*

2. 存储系统： AI Core的片上存储单元和相应的数据通路构成了存储系统。

3. 控制单元：整个计算过程提供了指令控制，相当于AI Core的司令部，负责整个AI Core的运行。

#### 达芬奇架构（AI Core）— 计算单元

计算单元里包含三种基础计算资源，分别是：Cube Unit、Vector Unit和Scalar Unit，分别对应矩阵、向量和标量三种常见的计算模式。

1. 矩阵计算单元：矩阵计算单元和累加器主要完成矩阵相关运 算。一拍完成一个fp16的 16x16与16x16矩阵乘（4096）； 如果是int8输入，则一拍完成 16\*32 与 32\*16 矩阵乘 （8192）；如图像数据。

2.向量计算单元：实现向量和标量，或双向量之间的计算，功 能覆盖各种基本的计算类型和许多定制的计算类型，主要包 括FP16/FP32/Int32/Int8等数据类型的计算； 如一维。

3. 标量计算单元：相当于一个微型CPU，控制整个AI Core的运 行，完成整个程序的循环控制、分支判断，可以为 Cube/Vector提供数据地址和相关参数的计算，以及基本的 算术运算，如个位的数字。

#### 达芬奇架构（AI Core）— 存储系统

1.存储系统由存储单元和相应的数据通路，构成了AICore的存储系统
  
2.存储单元由存储控制单元、缓冲区和寄存器组成

3. 数据通路：是指AICore在完成一次计算任务时，数据在AICore中的流通路径。

*达芬奇架构数据通路的特点是多进单出，主要是考虑到神经网络在计算过程中，输入的数据种类繁多并且数量巨大， 可以通过并行输入的方式来提高数据流入的效率。与此相反，将多种输入数据处理完成后往往只生成输出特征矩阵， 数据种类相对单一，单输出的数据通路，可以节约芯片硬件资源。*

#### 昇腾芯片的软件架构，

#### 

昇腾芯片的软件架构，包括：昇腾AI处理器软件的逻辑架构以及昇腾AI 处理器神经网络软件流。

昇腾AI处理器软件的逻辑架构可以分为四层，分别是L3应用使能层、 L2执行框架层、 L1芯片使能层和 L0计算资源层，如下图所示，
![](https://i-blog.csdnimg.cn/blog_migrate/e2f285f2ad0fd640460dc99ba7d20a08.png)

**L3应用使能层：是应用级封装，面向特定的应用领域，提供不同的处理算法。**
为各种领域提供具有计算和处 理能力的引擎可以直接使用下一层L2执行框架提供的框架调度能力，通过通用框架来生成相应的神经网络而 实现具体的引擎功能。 （ 通用引擎：提供通用的神经网络推理能力； 计算机视觉引擎：提供视频或图像处理的算法封装；  语言文字引擎：提供语音、文本等数据的基础处理算法封装）

**L2执行框架层：是框架调用能力和离线模型生成能力的封装。**
*L3层将应用算法开发完并封装成引 擎后，L2层会根据相关算法的特点进行合适深度学习框架的调用(如Caffe或TensorFlow)，来得到 相应功能的神经网络，再通过框架管理器生成离线模型。L2层将神经网络的原始模型转化成可在 昇腾AI芯片上运行的离线模型后，离线模型执行器将离线模型传送给L1芯片使能层进行任务分配。*

**L1芯片使能层：是离线模型通向昇腾AI芯片的桥梁。**
针对不同的计算任务，L1层通过加速库 (Library)给离线模型计算提供加速功能。L1层是最接近底层计算资源的一层，负责给硬件输出算 子层面的任务。

**L0计算资源层：是昇腾AI芯片的硬件算力基础，提供计算资源，执行具体的计算任务。**

#### 昇腾AI处理器神经网络软件流

昇腾AI处理器神经网络软件流，是深度学习框架到昇腾AI芯片之间的一座桥梁，完成 一个神经网络应用的实现和执行，并聚集了如下几个功能模块。
  
1.流程编排器：负责完成神经网络在昇腾AI芯片上的落地与实现，统筹了整个神经网络生 效的过程，控制离线模型的加载和执行过程；
  
2. 数字视觉预处理模块：在输入之前进行一次数据处理和修饰，来满足计算的格式需求；
  
3. 张量加速引擎：作为神经网络算子兵工厂，为神经网络模型源源不断提供功能强大的 计算算子；
  
4. 框架管理器：专门将原始神经网络模型打造成昇腾AI芯片支持的形态，并且将塑造后的 模型与昇腾AI芯片相融合，引导神经网络运行并高效发挥出性能；
  
5. 运行管理器：为神经网络的任务下发和分配提供了各种资源管理通道；

6. 任务调度器：作为一个硬件执行的任务驱动者，为昇腾AI芯片提供具体的目标任务；运 行管理器和任务调度器联合互动，共同组成了神经网络任务流通向硬件资源的大坝系统， 实时监控和有效分发不同类型的执行任务。

它们之间的逻辑关系如下图所示

![](https://i-blog.csdnimg.cn/blog_migrate/cd923fbbecfabd635c11a2906be9d28b.png)

数字通过数字视觉预处理模块进入到框架管理器，框架管理器将塑造好的魔心和芯片融合，接着通过运行管理器来下发人物，分配一些资源管理通道，进入任务调度器，任务调度器分发不同类型的执行任务，最后交给昇腾AI处理器。

昇腾AI处理器数据流程图的一个典型应用就是人脸识别推理应用，

首先是Camera数据采集和处理 ，从摄像头传入压缩视频流,，通过PCIE存储值DDR内存中。 DVPP将压缩视频流读入缓存。  DVPP经过预处理，将解压缩的帧写入DDR内存。

接着对数据进行推理，通过 任务调度器（TS）向直接存储访问引擎（DMA）发送指令，将AI资源从DDR预加载到片上缓冲区。 任务调度器（TS） 配置AI Core以执行任务。 AI Core工作时，它将读取特征图和权重并将结果写入DDR或片上缓冲区。

最后 进行人脸识别结果输出 ， AI Core完成处理后，发送信号给任务调度器（TS），任务调度器检查结果，如果需要会分配另一个任务， 并返回之前的步骤 。 当最后一个AI任务完成，任务调度器（TS）会将结果报告给Host。

### Atlas人工智能计算平台全景

芯片和硬件是最底层的部分，像芯片，和一些其他的部件。

CANN：基于昇腾AI处理器的芯片算子库和高度自动化算子开发工具，兼具最优开发效率和算子最佳匹配昇腾芯片性能。

框架：各种开发框架，需要被使用，华为的开发框架是MindDpore

应用使能：主要是智能边缘平台、深度学习的平台，还包含了一些集群的管理等。

对于Atals加速AI推理模块方面，华为推出了以下几款基于昇腾310AI处理器的产品，

![](https://i-blog.csdnimg.cn/blog_migrate/76822595bc464d67ae5774cc9c43d0bc.png)

Atlas 200DK和Atlas 200是比较初级的设备，用于一些小型的推理应用，可以在一台笔记本上搭建开发环境，本地独立环境成本比较低，对于一些研究者，可以进行一些本地开发和云端训练，对于创业者来说，可以提供一些代码级Demo，可以基于一些参考架构，完成算法的功能，对于一些商品产品进行无缝迁移；Atlas 300是业界最高密度的64路的视频推理加速卡，可应用的场景非常多，比如高清视频实时分析，语音识别，医疗影像分析；Atlas 500 是智能小站，可以完成一些实时的数据流的处理，Atlas 800是基于鲲鹏处理器的超强算例的AI推理平台。

对于Atals加速AI训练模块方面，华为推出了以下几款基于昇腾910AI处理器的产品，

![](https://i-blog.csdnimg.cn/blog_migrate/5530c37b4eba640e590f4d1d4c51ed7f.png)

Atlas 900 AI集群是全球最快的AI训练集群，算力是业界领先，拥有极致散热系统。

Atlas深度学习系统加速AI模型训练，构建丰富应用，端边云协同，打造极致开发、使用体验。