---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34343232333838332f:61727469636c652f64657461696c732f313436313634323933"
layout: post
title: "神经网络与深度学习第一章课后习题"
date: 2025-03-10 22:06:02 +08:00
description: "1. 梯度问题：当预测值远离真实值时，梯度反而变小，导致训练效率太低。以二分类问题为例解释平方损失函数的梯度问题：平方损失函数：L=(y−y^)2L = (y - \\hat{y})^2L=(y−y^​)2其梯度：∂L∂y^=2(y−y^)\\frac{\\partial L}{\\partial \\hat{y}} = 2(y - \\hat{y})∂y^​∂L​=2(y−y^​)当真实标签y=1y=1y=1时：如果预测值y^=0.9\\hat{y}=0.9y^​=0.9，梯度为0.2如果预测值y^=0.1"
keywords: "神经网络与深度学习第一章课后习题"
categories: ['未分类']
tags: ['神经网络', '深度学习', '人工智能']
artid: "146164293"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146164293
    alt: "神经网络与深度学习第一章课后习题"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146164293
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146164293
cover: https://bing.ee123.net/img/rand?artid=146164293
image: https://bing.ee123.net/img/rand?artid=146164293
img: https://bing.ee123.net/img/rand?artid=146164293
---

# 神经网络与深度学习第一章课后习题

## 2-1分析为什么平方损失函数不适用于分类问题。

**1. 梯度问题：当预测值远离真实值时，梯度反而变小，导致训练效率太低。**
  
以二分类问题为例解释平方损失函数的梯度问题：
  
平方损失函数：

L
=
(
y
−
y
^
)
2
L = (y - \hat{y})^2





L



=





(

y



−












y





^

​



)









2
  
其梯度：

∂
L
∂
y
^
=
2
(
y
−
y
^
)
\frac{\partial L}{\partial \hat{y}} = 2(y - \hat{y})

















∂








y





^

​













∂

L

​




=





2

(

y



−












y





^

​


)
  
当真实标签

y
=
1
y=1





y



=





1
时：
  
如果预测值

y
^
=
0.9
\hat{y}=0.9












y





^

​




=





0.9
，梯度为0.2
  
如果预测值

y
^
=
0.1
\hat{y}=0.1












y





^

​




=





0.1
，梯度为1.8
  
如果预测值

y
^
=
3.0
\hat{y}=3.0












y





^

​




=





3.0
，梯度为-4.0
  
可以看出，当预测值偏离得很远时(如

y
^
=
3.0
\hat{y}=3.0












y





^

​




=





3.0
)，反而得到更大的负梯度，导致模型更新方向不合理。这违背了我们的直觉：预测越准确，梯度应该越小；预测越不准确，梯度应该越大。
  
**2. 非凸性：在分类场景下会形成非凸函数，容易陷入局部最优。**
  
平方损失函数在分类问题中的非凸性分析：
  
对于二分类问题，假设真实标签

y
∈
{
0
,
1
}
y \in \{0,1\}





y



∈





{

0

,



1

}
，模型输出

y
^
\hat{y}












y





^

​

可以是任意实数。平方损失函数为：
  



L
=
(
y
−
y
^
)
2
L = (y - \hat{y})^2





L



=





(

y



−












y





^

​



)









2
  
当

y
=
1
y=1





y



=





1
时，损失函数为：

L
=
(
1
−
y
^
)
2
L = (1 - \hat{y})^2





L



=





(

1



−












y





^

​



)









2
  
当

y
=
0
y=0





y



=





0
时，损失函数为：

L
=
(
0
−
y
^
)
2
=
y
^
2
L = (0 - \hat{y})^2 = \hat{y}^2





L



=





(

0



−












y





^

​



)









2



=













y





^

​










2
  
这形成了一个非凸的损失曲面，意味着：
  
存在多个局部最小值点
  
优化算法容易陷入局部最优解
  
难以保证找到全局最优解
  
相比之下，交叉熵损失函数是凸函数，更容易优化。
  
**3. 概率解释：输出难以解释为概率值，不适合分类任务的概率预测。**

## 2-2在线性回归中，如果我们给每个样本 ( x ( n ) , y ( n ) ) (x^{(n)}, y^{(n)}) ( x ( n ) , y ( n ) ) 赋予一个权重 r ( n ) r^{(n)} r ( n ) ，经验风险函数为： R ( w ) = 1 2 ∑ n = 1 N r ( n ) ( y ( n ) − w T x ( n ) ) 2 \mathcal{R}(w) = \frac{1}{2} \sum\_{n=1}^N r^{(n)}(y^{(n)} - w^T x^{(n)})^2 R ( w ) = 2 1 ​ ∑ n = 1 N ​ r ( n ) ( y ( n ) − w T x ( n ) ) 2 计算其最优参数 w ∗ w^* w ∗ ，并分析权重的作用 r ( n ) r^{(n)} r ( n ) 的作用。

1. 首先，为了求最优参数

   w
   ∗
   w^*






   w









   ∗
   ，我们需要对

   R
   (
   w
   )
   \mathcal{R}(w)





   R

   (

   w

   )
   求导并令其等于0：

∂
R
(
w
)
∂
w
=
1
2
∑
n
=
1
N
2
r
(
n
)
(
y
(
n
)
−
w
T
x
(
n
)
)
(
−
x
(
n
)
)
=
0
\frac{\partial \mathcal{R}(w)}{\partial w} = \frac{1}{2} \sum\_{n=1}^N 2r^{(n)}(y^{(n)} - w^T x^{(n)})(-x^{(n)}) = 0

















∂

w












∂

R

(

w

)

​




=

















2












1

​





∑










n

=

1





N

​




2


r










(

n

)

(


y










(

n

)



−






w









T


x










(

n

)

)

(

−


x










(

n

)

)



=





0

简化后：
  



∑
n
=
1
N
r
(
n
)
(
w
T
x
(
n
)
−
y
(
n
)
)
x
(
n
)
=
0
\sum\_{n=1}^N r^{(n)}(w^T x^{(n)} - y^{(n)})x^{(n)} = 0






∑










n

=

1





N

​





r










(

n

)

(


w









T


x










(

n

)



−






y










(

n

)

)


x










(

n

)



=





0

2. 重新整理方程：

∑
n
=
1
N
r
(
n
)
x
(
n
)
(
x
(
n
)
)
T
w
=
∑
n
=
1
N
r
(
n
)
x
(
n
)
y
(
n
)
\sum\_{n=1}^N r^{(n)}x^{(n)}(x^{(n)})^T w = \sum\_{n=1}^N r^{(n)}x^{(n)}y^{(n)}






∑










n

=

1





N

​





r










(

n

)


x










(

n

)

(


x










(

n

)


)









T

w



=






∑










n

=

1





N

​





r










(

n

)


x










(

n

)


y










(

n

)

3. 用矩阵形式表示：

(
∑
n
=
1
N
r
(
n
)
x
(
n
)
(
x
(
n
)
)
T
)
w
=
∑
n
=
1
N
r
(
n
)
x
(
n
)
y
(
n
)
(\sum\_{n=1}^N r^{(n)}x^{(n)}(x^{(n)})^T)w = \sum\_{n=1}^N r^{(n)}x^{(n)}y^{(n)}





(


∑










n

=

1





N

​





r










(

n

)


x










(

n

)

(


x










(

n

)


)









T

)

w



=






∑










n

=

1





N

​





r










(

n

)


x










(

n

)


y










(

n

)

4. 最优解为：
     



   w
   ∗
   =
   (
   ∑
   n
   =
   1
   N
   r
   (
   n
   )
   x
   (
   n
   )
   (
   x
   (
   n
   )
   )
   T
   )
   −
   1
   (
   ∑
   n
   =
   1
   N
   r
   (
   n
   )
   x
   (
   n
   )
   y
   (
   n
   )
   )
   w^* = (\sum\_{n=1}^N r^{(n)}x^{(n)}(x^{(n)})^T)^{-1}(\sum\_{n=1}^N r^{(n)}x^{(n)}y^{(n)})






   w









   ∗



   =





   (


   ∑










   n

   =

   1





   N

   ​





   r










   (

   n

   )


   x










   (

   n

   )

   (


   x










   (

   n

   )


   )









   T


   )










   −

   1

   (


   ∑










   n

   =

   1





   N

   ​





   r










   (

   n

   )


   x










   (

   n

   )


   y










   (

   n

   )

   )
5. 权重

   r
   (
   n
   )
   r^{(n)}






   r










   (

   n

   )
   的作用分析：
     
   当

   r
   (
   n
   )
   r^{(n)}






   r










   (

   n

   )
   较大时，对应样本点在优化过程中的影响更大
     
   当

   r
   (
   n
   )
   r^{(n)}






   r










   (

   n

   )
   较小时，对应样本点的影响较小
     
   可以用来处理：
     
   不平衡数据
     
   异常点处理（降低异常点的权重）
     
   基于样本重要性的加权
     
   这实际上是加权最小二乘法(Weighted Least Squares, WLS)的推导过程。

你好！ 这是你第一次使用
**Markdown编辑器**
所展示的欢迎页。如果你想学习如何使用Markdown编辑器, 可以仔细阅读这篇文章，了解一下Markdown的基本语法知识。

## 2-3证明在线性回归中，如果样本数量N小于特征数量D+1，则 X X T XX^T X X T 的秩最大为N。

1. 首先明确矩阵维度：

   * 设

     X
     X





     X
     的维度为

     N
     ×
     D
     N \times D





     N



     ×





     D
     ，其中N是样本数，D是特征维度
   * 则

     X
     T
     X^T






     X









     T
     的维度为

     D
     ×
     N
     D \times N





     D



     ×





     N
   * X
     X
     T
     XX^T





     X


     X









     T
     的维度为

     N
     ×
     N
     N \times N





     N



     ×





     N
2. 根据矩阵乘法的性质：

   * r
     a
     n
     k
     (
     X
     X
     T
     )
     ≤
     m
     i
     n
     (
     r
     a
     n
     k
     (
     X
     )
     ,
     r
     a
     n
     k
     (
     X
     T
     )
     )
     rank(XX^T) \leq min(rank(X), rank(X^T))





     r

     ank

     (

     X


     X









     T

     )



     ≤





     min

     (

     r

     ank

     (

     X

     )

     ,



     r

     ank

     (


     X









     T

     ))
   * r
     a
     n
     k
     (
     X
     )
     =
     r
     a
     n
     k
     (
     X
     T
     )
     rank(X) = rank(X^T)





     r

     ank

     (

     X

     )



     =





     r

     ank

     (


     X









     T

     )
3. 对于矩阵的秩，有以下性质：

   * 矩阵的秩不会超过其行数和列数的最小值
   * 即

     r
     a
     n
     k
     (
     X
     )
     ≤
     m
     i
     n
     (
     N
     ,
     D
     )
     rank(X) \leq min(N, D)





     r

     ank

     (

     X

     )



     ≤





     min

     (

     N

     ,



     D

     )
4. 在本题条件下：

   * 已知

     N
     <
     D
     +
     1
     N < D+1





     N



     <





     D



     +





     1
     ，即

     N
     ≤
     D
     N \leq D





     N



     ≤





     D
   * 因此

     r
     a
     n
     k
     (
     X
     )
     ≤
     N
     rank(X) \leq N





     r

     ank

     (

     X

     )



     ≤





     N
5. 结合上述性质：

   * r
     a
     n
     k
     (
     X
     X
     T
     )
     ≤
     r
     a
     n
     k
     (
     X
     )
     ≤
     N
     rank(XX^T) \leq rank(X) \leq N





     r

     ank

     (

     X


     X









     T

     )



     ≤





     r

     ank

     (

     X

     )



     ≤





     N
   * 而

     X
     X
     T
     XX^T





     X


     X









     T
     是一个

     N
     ×
     N
     N \times N





     N



     ×





     N
     的矩阵
   * 因此

     r
     a
     n
     k
     (
     X
     X
     T
     )
     rank(XX^T)





     r

     ank

     (

     X


     X









     T

     )
     的最大值为N

## 2-4在线性回归中，验证岭回归的解为结构风险最小化准则下的最小二乘估计。

1. 首先写出岭回归的目标函数：

J
(
w
)
=
1
2
∑
n
=
1
N
(
y
(
n
)
−
w
T
x
(
n
)
)
2
+
λ
2
∣
∣
w
∣
∣
2
J(w) = \frac{1}{2}\sum\_{n=1}^N(y^{(n)} - w^Tx^{(n)})^2 + \frac{\lambda}{2}||w||^2





J

(

w

)



=

















2












1

​





∑










n

=

1





N

​


(


y










(

n

)



−






w









T


x










(

n

)


)









2



+

















2












λ

​


∣∣

w

∣


∣









2

其中第一项是经验风险（均方误差），第二项是结构风险（正则化项）。

2. 为求最优解，对

   J
   (
   w
   )
   J(w)





   J

   (

   w

   )
   求导并令其为0：

   ∂
   J
   (
   w
   )
   ∂
   w
   =
   −
   ∑
   n
   =
   1
   N
   (
   y
   (
   n
   )
   −
   w
   T
   x
   (
   n
   )
   )
   x
   (
   n
   )
   +
   λ
   w
   =
   0
   \frac{\partial J(w)}{\partial w} = -\sum\_{n=1}^N(y^{(n)} - w^Tx^{(n)})x^{(n)} + \lambda w = 0

















   ∂

   w












   ∂

   J

   (

   w

   )

   ​




   =





   −




   ∑










   n

   =

   1





   N

   ​


   (


   y










   (

   n

   )



   −






   w









   T


   x










   (

   n

   )

   )


   x










   (

   n

   )



   +





   λ

   w



   =





   0
3. 用矩阵形式重写：

   −
   X
   T
   (
   y
   −
   X
   w
   )
   +
   λ
   w
   =
   0
   -X^T(y - Xw) + \lambda w = 0





   −


   X









   T

   (

   y



   −





   Xw

   )



   +





   λ

   w



   =





   0

   −
   X
   T
   y
   +
   X
   T
   X
   w
   +
   λ
   w
   =
   0
   -X^Ty + X^TXw + \lambda w = 0





   −


   X









   T

   y



   +






   X









   T

   Xw



   +





   λ

   w



   =





   0

   (
   X
   T
   X
   +
   λ
   I
   )
   w
   =
   X
   T
   y
   (X^TX + \lambda I)w = X^Ty





   (


   X









   T

   X



   +





   λ

   I

   )

   w



   =






   X









   T

   y
4. 因此岭回归的解为：

   w
   ∗
   =
   (
   X
   T
   X
   +
   λ
   I
   )
   −
   1
   X
   T
   y
   w^* = (X^TX + \lambda I)^{-1}X^Ty






   w









   ∗



   =





   (


   X









   T

   X



   +





   λ

   I


   )










   −

   1


   X









   T

   y
5. 这正是结构风险最小化准则下的解：

   * λ
     =
     0
     \lambda = 0





     λ



     =





     0
     时退化为普通最小二乘
   * λ
     >
     0
     \lambda > 0





     λ



     >





     0
     时引入了对参数的惩罚项
   * 正则化项

     λ
     2
     ∣
     ∣
     w
     ∣
     ∣
     2
     \frac{\lambda}{2}||w||^2

















     2












     λ

     ​


     ∣∣

     w

     ∣


     ∣









     2
     可以：
     + 防止过拟合
     + 提高模型泛化能力
     + 处理特征共线性问题
6. 从贝叶斯角度理解：

   * 相当于对参数引入了均值为0、协方差为

     1
     λ
     I
     \frac{1}{\lambda}I

















     λ












     1

     ​


     I
     的高斯先验
   * 最终解为后验概率最大化的结果

## 2-5在线性回归中，若假设标签 y ∼ N ( w T x , β ) y \sim \mathcal{N}(w^Tx, \beta) y ∼ N ( w T x , β ) ，并用最大似然估计求最优化参数时，验证最优参数为公式2.50的解。

1. 首先写出似然函数：
     
   对于单个样本，其概率密度函数为：

p
(
y
(
n
)
∣
x
(
n
)
,
w
)
=
1
2
π
β
exp
⁡
(
−
(
y
(
n
)
−
w
T
x
(
n
)
)
2
2
β
)
p(y^{(n)}|x^{(n)},w) = \frac{1}{\sqrt{2\pi\beta}}\exp(-\frac{(y^{(n)}-w^Tx^{(n)})^2}{2\beta})





p

(


y










(

n

)

∣


x










(

n

)

,



w

)



=

























2

π

β


​













1

​




exp

(

−













2

β












(


y










(

n

)

−


w









T


x










(

n

)


)









2

​


)

2. 对于所有N个样本，似然函数为：

   L
   (
   w
   )
   =
   ∏
   n
   =
   1
   N
   1
   2
   π
   β
   exp
   ⁡
   (
   −
   (
   y
   (
   n
   )
   −
   w
   T
   x
   (
   n
   )
   )
   2
   2
   β
   )
   L(w) = \prod\_{n=1}^N \frac{1}{\sqrt{2\pi\beta}}\exp(-\frac{(y^{(n)}-w^Tx^{(n)})^2}{2\beta})





   L

   (

   w

   )



   =






   ∏










   n

   =

   1





   N

   ​
























   2

   π

   β


   ​













   1

   ​




   exp

   (

   −













   2

   β












   (


   y










   (

   n

   )

   −


   w









   T


   x










   (

   n

   )


   )









   2

   ​


   )
3. 取对数似然：

   ln
   ⁡
   L
   (
   w
   )
   =
   −
   N
   2
   ln
   ⁡
   (
   2
   π
   β
   )
   −
   1
   2
   β
   ∑
   n
   =
   1
   N
   (
   y
   (
   n
   )
   −
   w
   T
   x
   (
   n
   )
   )
   2
   \ln L(w) = -\frac{N}{2}\ln(2\pi\beta) - \frac{1}{2\beta}\sum\_{n=1}^N(y^{(n)}-w^Tx^{(n)})^2





   ln



   L

   (

   w

   )



   =





   −













   2












   N

   ​




   ln

   (

   2

   π

   β

   )



   −

















   2

   β












   1

   ​





   ∑










   n

   =

   1





   N

   ​


   (


   y










   (

   n

   )



   −






   w









   T


   x










   (

   n

   )


   )









   2
4. 对

   w
   w





   w
   求导并令其为0：

   ∂
   ln
   ⁡
   L
   (
   w
   )
   ∂
   w
   =
   1
   β
   ∑
   n
   =
   1
   N
   (
   y
   (
   n
   )
   −
   w
   T
   x
   (
   n
   )
   )
   x
   (
   n
   )
   =
   0
   \frac{\partial \ln L(w)}{\partial w} = \frac{1}{\beta}\sum\_{n=1}^N(y^{(n)}-w^Tx^{(n)})x^{(n)} = 0

















   ∂

   w












   ∂




   l

   n



   L

   (

   w

   )

   ​




   =

















   β












   1

   ​





   ∑










   n

   =

   1





   N

   ​


   (


   y










   (

   n

   )



   −






   w









   T


   x










   (

   n

   )

   )


   x










   (

   n

   )



   =





   0
5. 用矩阵形式重写：

   X
   T
   (
   y
   −
   X
   w
   )
   =
   0
   X^T(y-Xw) = 0






   X









   T

   (

   y



   −





   Xw

   )



   =





   0

   X
   T
   y
   −
   X
   T
   X
   w
   =
   0
   X^Ty - X^TXw = 0






   X









   T

   y



   −






   X









   T

   Xw



   =





   0

   X
   T
   X
   w
   =
   X
   T
   y
   X^TXw = X^Ty






   X









   T

   Xw



   =






   X









   T

   y
6. 因此最优解为：

   w
   ∗
   =
   (
   X
   T
   X
   )
   −
   1
   X
   T
   y
   w^* = (X^TX)^{-1}X^Ty






   w









   ∗



   =





   (


   X









   T

   X


   )










   −

   1


   X









   T

   y

## 2-6假设有N个样本 x ( 1 ) , x ( 2 ) , … , x ( N ) x^{(1)}, x^{(2)}, \ldots, x^{(N)} x ( 1 ) , x ( 2 ) , … , x ( N ) 服从正态分布 N ( μ , σ 2 ) \mathcal{N}(\mu, \sigma^2) N ( μ , σ 2 ) ，其中 μ \mu μ 未知。使用最大似然估计求解最优参数 μ M L E \mu^{MLE} μ M L E 。若参数 μ \mu μ 为随机变量，并服从正态分布 N ( μ 0 , σ 0 2 ) \mathcal{N}(\mu\_0, \sigma\_0^2) N ( μ 0 ​ , σ 0 2 ​ ) ，使用最大后验估计求解最优参数 μ M A P \mu^{MAP} μ M A P 。

(1) 最大似然估计(MLE)：

1. 写出似然函数：
     



   L
   (
   μ
   )
   =
   ∏
   n
   =
   1
   N
   1
   2
   π
   σ
   2
   exp
   ⁡
   (
   −
   (
   x
   (
   n
   )
   −
   μ
   )
   2
   2
   σ
   2
   )
   L(\mu) = \prod\_{n=1}^N \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(x^{(n)}-\mu)^2}{2\sigma^2})





   L

   (

   μ

   )



   =






   ∏










   n

   =

   1





   N

   ​
























   2

   π


   σ









   2


   ​













   1

   ​




   exp

   (

   −













   2


   σ









   2












   (


   x










   (

   n

   )

   −

   μ


   )









   2

   ​


   )
2. 取对数：
     



   ln
   ⁡
   L
   (
   μ
   )
   =
   −
   N
   2
   ln
   ⁡
   (
   2
   π
   σ
   2
   )
   −
   1
   2
   σ
   2
   ∑
   n
   =
   1
   N
   (
   x
   (
   n
   )
   −
   μ
   )
   2
   \ln L(\mu) = -\frac{N}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum\_{n=1}^N(x^{(n)}-\mu)^2





   ln



   L

   (

   μ

   )



   =





   −













   2












   N

   ​




   ln

   (

   2

   π


   σ









   2

   )



   −

















   2


   σ









   2












   1

   ​





   ∑










   n

   =

   1





   N

   ​


   (


   x










   (

   n

   )



   −





   μ


   )









   2
3. 求导并令其为0：
     



   ∂
   ln
   ⁡
   L
   (
   μ
   )
   ∂
   μ
   =
   1
   σ
   2
   ∑
   n
   =
   1
   N
   (
   x
   (
   n
   )
   −
   μ
   )
   =
   0
   \frac{\partial \ln L(\mu)}{\partial \mu} = \frac{1}{\sigma^2}\sum\_{n=1}^N(x^{(n)}-\mu) = 0

















   ∂

   μ












   ∂




   l

   n



   L

   (

   μ

   )

   ​




   =


















   σ









   2












   1

   ​





   ∑










   n

   =

   1





   N

   ​


   (


   x










   (

   n

   )



   −





   μ

   )



   =





   0
4. 解得：
     



   μ
   M
   L
   E
   =
   1
   N
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   \mu^{MLE} = \frac{1}{N}\sum\_{n=1}^N x^{(n)}






   μ










   M

   L

   E



   =

















   N












   1

   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )

(2) 最大后验估计(MAP)：

1. 根据贝叶斯定理，后验概率正比于似然函数与先验概率的乘积：
     



   p
   (
   μ
   ∣
   X
   )
   ∝
   p
   (
   X
   ∣
   μ
   )
   p
   (
   μ
   )
   p(\mu|X) \propto p(X|\mu)p(\mu)





   p

   (

   μ

   ∣

   X

   )



   ∝





   p

   (

   X

   ∣

   μ

   )

   p

   (

   μ

   )
2. 取对数：
     



   ln
   ⁡
   p
   (
   μ
   ∣
   X
   )
   =
   ln
   ⁡
   p
   (
   X
   ∣
   μ
   )
   +
   ln
   ⁡
   p
   (
   μ
   )
   +
   C
   \ln p(\mu|X) = \ln p(X|\mu) + \ln p(\mu) + C





   ln



   p

   (

   μ

   ∣

   X

   )



   =





   ln



   p

   (

   X

   ∣

   μ

   )



   +





   ln



   p

   (

   μ

   )



   +





   C
3. 代入具体表达式：
     



   ln
   ⁡
   p
   (
   μ
   ∣
   X
   )
   =
   −
   1
   2
   σ
   2
   ∑
   n
   =
   1
   N
   (
   x
   (
   n
   )
   −
   μ
   )
   2
   −
   (
   μ
   −
   μ
   0
   )
   2
   2
   σ
   0
   2
   +
   C
   \ln p(\mu|X) = -\frac{1}{2\sigma^2}\sum\_{n=1}^N(x^{(n)}-\mu)^2 - \frac{(\mu-\mu\_0)^2}{2\sigma\_0^2} + C





   ln



   p

   (

   μ

   ∣

   X

   )



   =





   −













   2


   σ









   2












   1

   ​





   ∑










   n

   =

   1





   N

   ​


   (


   x










   (

   n

   )



   −





   μ


   )









   2



   −

















   2


   σ









   0





   2

   ​













   (

   μ

   −


   μ









   0

   ​



   )









   2

   ​




   +





   C
4. 求导并令其为0：
     



   ∂
   ln
   ⁡
   p
   (
   μ
   ∣
   X
   )
   ∂
   μ
   =
   1
   σ
   2
   ∑
   n
   =
   1
   N
   (
   x
   (
   n
   )
   −
   μ
   )
   −
   μ
   −
   μ
   0
   σ
   0
   2
   =
   0
   \frac{\partial \ln p(\mu|X)}{\partial \mu} = \frac{1}{\sigma^2}\sum\_{n=1}^N(x^{(n)}-\mu) - \frac{\mu-\mu\_0}{\sigma\_0^2} = 0

















   ∂

   μ












   ∂




   l

   n



   p

   (

   μ

   ∣

   X

   )

   ​




   =


















   σ









   2












   1

   ​





   ∑










   n

   =

   1





   N

   ​


   (


   x










   (

   n

   )



   −





   μ

   )



   −


















   σ









   0





   2

   ​













   μ

   −


   μ









   0

   ​


   ​




   =





   0
5. 解得：
     



   μ
   M
   A
   P
   =
   σ
   0
   2
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   +
   σ
   2
   μ
   0
   N
   σ
   0
   2
   +
   σ
   2
   \mu^{MAP} = \frac{\sigma\_0^2\sum\_{n=1}^N x^{(n)} + \sigma^2\mu\_0}{N\sigma\_0^2 + \sigma^2}






   μ










   M

   A

   P



   =

















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   0





   2

   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )

   +


   σ









   2


   μ









   0

   ​


   ​

## 2-7证明在上一题中，当N趋近于无穷时，最大后验估计(MAP)趋向于最大似然估计(MLE)。

1. 回顾两个估计的结果：

   MLE:

   μ
   M
   L
   E
   =
   1
   N
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   \mu^{MLE} = \frac{1}{N}\sum\_{n=1}^N x^{(n)}






   μ










   M

   L

   E



   =

















   N












   1

   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )

   MAP:

   μ
   M
   A
   P
   =
   σ
   0
   2
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   +
   σ
   2
   μ
   0
   N
   σ
   0
   2
   +
   σ
   2
   \mu^{MAP} = \frac{\sigma\_0^2\sum\_{n=1}^N x^{(n)} + \sigma^2\mu\_0}{N\sigma\_0^2 + \sigma^2}






   μ










   M

   A

   P



   =

















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   0





   2

   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )

   +


   σ









   2


   μ









   0

   ​


   ​
2. 对MAP估计进行变形：

   μ
   M
   A
   P
   =
   σ
   0
   2
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   +
   σ
   2
   μ
   0
   N
   σ
   0
   2
   +
   σ
   2
   \mu^{MAP} = \frac{\sigma\_0^2\sum\_{n=1}^N x^{(n)} + \sigma^2\mu\_0}{N\sigma\_0^2 + \sigma^2}






   μ










   M

   A

   P



   =

















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   0





   2

   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )

   +


   σ









   2


   μ









   0

   ​


   ​

   =
   σ
   0
   2
   N
   σ
   0
   2
   +
   σ
   2
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   +
   σ
   2
   N
   σ
   0
   2
   +
   σ
   2
   μ
   0
   = \frac{\sigma\_0^2}{N\sigma\_0^2 + \sigma^2}\sum\_{n=1}^N x^{(n)} + \frac{\sigma^2}{N\sigma\_0^2 + \sigma^2}\mu\_0





   =

















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   0





   2

   ​


   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )



   +

















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   2

   ​



   μ









   0

   ​
3. 当N趋近于无穷时，分析各项：

   lim
   ⁡
   N
   →
   ∞
   σ
   0
   2
   N
   σ
   0
   2
   +
   σ
   2
   =
   lim
   ⁡
   N
   →
   ∞
   1
   N
   +
   σ
   2
   σ
   0
   2
   =
   1
   \lim\_{N \to \infty} \frac{\sigma\_0^2}{N\sigma\_0^2 + \sigma^2} = \lim\_{N \to \infty} \frac{1}{N + \frac{\sigma^2}{\sigma\_0^2}} = 1






   lim










   N

   →

   ∞

   ​
















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   0





   2

   ​


   ​




   =






   lim










   N

   →

   ∞

   ​
















   N

   +














   σ








   0




   2

   ​














   σ








   2

   ​













   1

   ​




   =





   1

   lim
   ⁡
   N
   →
   ∞
   σ
   2
   N
   σ
   0
   2
   +
   σ
   2
   =
   lim
   ⁡
   N
   →
   ∞
   σ
   2
   σ
   0
   2
   N
   +
   σ
   2
   σ
   0
   2
   =
   0
   \lim\_{N \to \infty} \frac{\sigma^2}{N\sigma\_0^2 + \sigma^2} = \lim\_{N \to \infty} \frac{\frac{\sigma^2}{\sigma\_0^2}}{N + \frac{\sigma^2}{\sigma\_0^2}} = 0






   lim










   N

   →

   ∞

   ​
















   N


   σ









   0





   2

   ​


   +


   σ









   2













   σ









   2

   ​




   =






   lim










   N

   →

   ∞

   ​
















   N

   +














   σ








   0




   2

   ​














   σ








   2

   ​


























   σ








   0




   2

   ​














   σ








   2

   ​


   ​




   =





   0
4. 因此：

   lim
   ⁡
   N
   →
   ∞
   μ
   M
   A
   P
   =
   lim
   ⁡
   N
   →
   ∞
   [
   1
   N
   ∑
   n
   =
   1
   N
   x
   (
   n
   )
   ]
   =
   μ
   M
   L
   E
   \lim\_{N \to \infty} \mu^{MAP} = \lim\_{N \to \infty} [\frac{1}{N}\sum\_{n=1}^N x^{(n)}] = \mu^{MLE}






   lim










   N

   →

   ∞

   ​





   μ










   M

   A

   P



   =






   lim










   N

   →

   ∞

   ​


   [













   N












   1

   ​





   ∑










   n

   =

   1





   N

   ​





   x










   (

   n

   )

   ]



   =






   μ










   M

   L

   E
5. 直观解释：

   * 当样本量N很大时，数据提供的信息远多于先验信息
   * 先验分布的影响（第二项）趋近于0
   * 样本信息的权重（第一项）趋近于1
   * 因此MAP估计渐近地等价于MLE估计

## 2-8验证公式2.61，即证明在平方损失函数下，最优模型为条件期望 f ∗ ( x ) = E y ∼ p r ( y ∣ x ) [ y ] f^*(x) = \mathbb{E}\_{y\sim p\_r(y|x)}[y] f ∗ ( x ) = E y ∼ p r ​ ( y ∣ x ) ​ [ y ] 。

1. 首先回顾期望误差的定义（公式2.60）：

   R
   (
   f
   )
   =
   E
   (
   x
   ,
   y
   )
   ∼
   p
   r
   (
   x
   ,
   y
   )
   [
   (
   y
   −
   f
   (
   x
   )
   )
   2
   ]
   \mathcal{R}(f) = \mathbb{E}\_{(x,y)\sim p\_r(x,y)}[(y - f(x))^2]





   R

   (

   f

   )



   =






   E










   (

   x

   ,

   y

   )

   ∼


   p









   r

   ​


   (

   x

   ,

   y

   )

   ​


   [(

   y



   −





   f

   (

   x

   )


   )









   2

   ]
2. 对于固定的x，考虑条件期望：

   R
   (
   f
   ∣
   x
   )
   =
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   (
   y
   −
   f
   (
   x
   )
   )
   2
   ]
   \mathcal{R}(f|x) = \mathbb{E}\_{y\sim p\_r(y|x)}[(y - f(x))^2]





   R

   (

   f

   ∣

   x

   )



   =






   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [(

   y



   −





   f

   (

   x

   )


   )









   2

   ]
3. 展开平方项：

   R
   (
   f
   ∣
   x
   )
   =
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   y
   2
   −
   2
   y
   f
   (
   x
   )
   +
   f
   (
   x
   )
   2
   ]
   \mathcal{R}(f|x) = \mathbb{E}\_{y\sim p\_r(y|x)}[y^2 - 2yf(x) + f(x)^2]





   R

   (

   f

   ∣

   x

   )



   =






   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [


   y









   2



   −





   2

   y

   f

   (

   x

   )



   +





   f

   (

   x


   )









   2

   ]

   =
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   y
   2
   ]
   −
   2
   f
   (
   x
   )
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   y
   ]
   +
   f
   (
   x
   )
   2
   = \mathbb{E}\_{y\sim p\_r(y|x)}[y^2] - 2f(x)\mathbb{E}\_{y\sim p\_r(y|x)}[y] + f(x)^2





   =






   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [


   y









   2

   ]



   −





   2

   f

   (

   x

   )


   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [

   y

   ]



   +





   f

   (

   x


   )









   2
4. 为求最优模型，对

   f
   (
   x
   )
   f(x)





   f

   (

   x

   )
   求导并令其为0：

   ∂
   R
   (
   f
   ∣
   x
   )
   ∂
   f
   (
   x
   )
   =
   −
   2
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   y
   ]
   +
   2
   f
   (
   x
   )
   =
   0
   \frac{\partial \mathcal{R}(f|x)}{\partial f(x)} = -2\mathbb{E}\_{y\sim p\_r(y|x)}[y] + 2f(x) = 0

















   ∂

   f

   (

   x

   )












   ∂

   R

   (

   f

   ∣

   x

   )

   ​




   =





   −

   2


   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [

   y

   ]



   +





   2

   f

   (

   x

   )



   =





   0
5. 解得：

   f
   ∗
   (
   x
   )
   =
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   y
   ]
   f^*(x) = \mathbb{E}\_{y\sim p\_r(y|x)}[y]






   f









   ∗

   (

   x

   )



   =






   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [

   y

   ]
6. 验证这是最小值而非最大值：

   ∂
   2
   R
   (
   f
   ∣
   x
   )
   ∂
   f
   (
   x
   )
   2
   =
   2
   >
   0
   \frac{\partial^2 \mathcal{R}(f|x)}{\partial f(x)^2} = 2 > 0

















   ∂

   f

   (

   x


   )









   2













   ∂









   2

   R

   (

   f

   ∣

   x

   )

   ​




   =





   2



   >





   0
7. 因此对每个x，最优预测为：

   f
   ∗
   (
   x
   )
   =
   E
   y
   ∼
   p
   r
   (
   y
   ∣
   x
   )
   [
   y
   ]
   f^*(x) = \mathbb{E}\_{y\sim p\_r(y|x)}[y]






   f









   ∗

   (

   x

   )



   =






   E










   y

   ∼


   p









   r

   ​


   (

   y

   ∣

   x

   )

   ​


   [

   y

   ]

## 2-11分别用一元、二元和三元特征的词袋模型表示文本"我打了张三"和"张三打了我"，并分析不同模型的优缺点。

1. 一元特征(Unigram)表示：

文本分词后：

* 句子1：“我”，“打”，“了”，“张三”
* 句子2：“张三”，“打”，“了”，“我”

特征词典：{“我”, “打”, “了”, “张三”}

向量表示：

* 句子1：[1, 1, 1, 1]
* 句子2：[1, 1, 1, 1]

2. 二元特征(Bigram)表示：

特征词典：{“我打”, “打了”, “了张三”, “张三打”, “打了”, “了我”}

向量表示：

* 句子1：[1, 1, 1, 0, 0, 0]
* 句子2：[0, 0, 0, 1, 1, 1]

3. 三元特征(Trigram)表示：

特征词典：{“我打了”, “打了张三”, “了张三”, “张三打了”, “三打了我”}

向量表示：

* 句子1：[1, 1, 0, 0, 0]
* 句子2：[0, 0, 0, 1, 1]

1. 一元特征模型：
     
   优点：

* 特征空间较小，计算效率高
* 对数据稀疏性不敏感
* 词序无关的场景下表现好

缺点：

* 完全丢失词序信息
* 无法区分语义相反的句子
* 如本例中无法区分主谓关系

2. 二元特征模型：
     
   优点：

* 保留了部分相邻词的顺序信息
* 能够捕捉一定的短语特征
* 可以区分简单的语序差异

缺点：

* 特征空间显著增大
* 数据稀疏性问题加重
* 只能捕获局部的序列关系

3. 三元特征模型：
     
   优点：

* 保留了更多的上下文信息
* 能够捕捉更长的短语特征
* 语序表达更准确

缺点：

* 特征空间急剧膨胀
* 严重的数据稀疏问题
* 计算复杂度高
* 泛化能力下降

## 2-12 计算三分类问题的各项评估指标。

真实标签：1,1,2,2,2,3,3,3,3
  
预测标签：1,2,2,2,3,3,3,1,2
  
分别计算模型的精确率、召回率、F1值以及它们的宏平均和微平均。

1. 首先构建混淆矩阵：
     
   预测\真实 类别1 类别2 类别3
     
   类别1 1 0 1
     
   类别2 1 3 1
     
   类别3 0 0 2
2. 计算每个类别的精确率(Precision)：

   * P1 = 1/(1+0+1) = 1/2 = 0.5
   * P2 = 3/(1+3+1) = 3/5 = 0.6
   * P3 = 2/(0+0+2) = 2/2 = 1.0
3. 计算每个类别的召回率(Recall)：

   * R1 = 1/(1+1+0) = 1/2 = 0.5
   * R2 = 3/(0+3+0) = 3/3 = 1.0
   * R3 = 2/(1+1+2) = 2/4 = 0.5
4. 计算每个类别的F1值：

   * F1\_1 = 2×0.5×0.5/(0.5+0.5) = 0.5
   * F1\_2 = 2×0.6×1.0/(0.6+1.0) = 0.75
   * F1\_3 = 2×1.0×0.5/(1.0+0.5) = 0.67
5. 计算宏平均(Macro-average)：

   * Macro-P = (0.5 + 0.6 + 1.0)/3 = 0.70
   * Macro-R = (0.5 + 1.0 + 0.5)/3 = 0.67
   * Macro-F1 = (0.5 + 0.75 + 0.67)/3 = 0.64
6. 计算微平均(Micro-average)：

   * 总TP = 1 + 3 + 2 = 6
   * 总样本数 = 9
   * Micro-P = 6/9 = 0.67
   * Micro-R = 6/9 = 0.67
   * Micro-F1 = 2×0.67×0.67/(0.67+0.67) = 0.67

类别1：

* Precision = 0.50
* Recall = 0.50
* F1 = 0.50

类别2：

* Precision = 0.60
* Recall = 1.00
* F1 = 0.75

类别3：

* Precision = 1.00
* Recall = 0.50
* F1 = 0.67

宏平均：

* Macro-Precision = 0.70
* Macro-Recall = 0.67
* Macro-F1 = 0.64

微平均：

* Micro-Precision = 0.67
* Micro-Recall = 0.67
* Micro-F1 = 0.67