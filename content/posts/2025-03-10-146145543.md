---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f33383133393235302f:61727469636c652f64657461696c732f313436313435353433"
layout: post
title: "å¤§è¯­è¨€æ¨¡å‹-å…¨æ–‡"
date: 2025-03-10 15:36:37 +0800
description: "æœ¬åšå®¢å†…å®¹æ˜¯ã€Šå¤§è¯­è¨€æ¨¡å‹ã€‹ä¸€ä¹¦çš„è¯»ä¹¦ç¬”è®°ï¼Œæœ¬æ–‡ä¸»è¦è®°å½•datawhaleçš„æ´»åŠ¨å­¦ä¹ ç¬”è®°ï¼Œæ˜¯ç³»åˆ—åšå®¢çš„æ±‡æ€»ã€‚"
keywords: "å¤§è¯­è¨€æ¨¡å‹-å…¨æ–‡"
categories: ['æœºå™¨å­¦ä¹ 2025', 'å¤§æ¨¡å‹Llm']
tags: ['è¯­è¨€æ¨¡å‹', 'è‡ªç„¶è¯­è¨€å¤„ç†', 'äººå·¥æ™ºèƒ½', 'Datawhale']
artid: "146145543"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146145543
    alt: "å¤§è¯­è¨€æ¨¡å‹-å…¨æ–‡"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146145543
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146145543
cover: https://bing.ee123.net/img/rand?artid=146145543
image: https://bing.ee123.net/img/rand?artid=146145543
img: https://bing.ee123.net/img/rand?artid=146145543
---

# å¤§è¯­è¨€æ¨¡å‹-å…¨æ–‡

### ç®€ä»‹

æœ¬åšå®¢å†…å®¹æ˜¯ã€Šå¤§è¯­è¨€æ¨¡å‹ã€‹ä¸€ä¹¦çš„è¯»ä¹¦ç¬”è®°ï¼Œè¯¥ä¹¦æ˜¯ä¸­å›½äººæ°‘å¤§å­¦é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢èµµé‘«æ•™æˆå›¢é˜Ÿå‡ºå“ï¼Œè¦†ç›–å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸ä½¿ç”¨çš„å…¨æµç¨‹ï¼Œä»é¢„è®­ç»ƒåˆ°å¾®è°ƒä¸å¯¹é½ï¼Œä»ä½¿ç”¨æŠ€æœ¯åˆ°è¯„æµ‹åº”ç”¨ï¼Œå¸®åŠ©å­¦å‘˜å…¨é¢æŒæ¡å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæŠ€æœ¯ã€‚å¹¶ä¸”ï¼Œè¯¾ç¨‹å†…å®¹åŸºäºå¤§é‡çš„ä»£ç å®æˆ˜ä¸è®²è§£ï¼Œé€šè¿‡å®é™…é¡¹ç›®ä¸æ¡ˆä¾‹ï¼Œå­¦å‘˜èƒ½å°†ç†è®ºçŸ¥è¯†åº”ç”¨äºçœŸå®åœºæ™¯ï¼Œæå‡è§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ã€‚
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/8462d74a8d764cc7a56c909eade4c15a.png)

æœ¬æ–‡ä¸»è¦è®°å½•datawhaleçš„æ´»åŠ¨å­¦ä¹ ç¬”è®°ï¼Œ
[å¯ç‚¹å‡»æ´»åŠ¨è¿æ¥](https://www.datawhale.cn/activity/150)

### 1.1è¯­è¨€æ¨¡å‹å‘å±•å†ç¨‹

#### å¤§æ¨¡å‹çš„èƒ½åŠ›

> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/5f036bbc0bc146cd84af5510a5cafd13.png)

> **èŒƒå›´å¹¿æ³›çš„ä¸–ç•ŒçŸ¥è¯†**
> ï¼š
>   
> ä¾‹å¦‚ï¼Œäº†è§£å†å²äº‹ä»¶å¦‚2024å¹´å·´é»å¥¥è¿ä¼šçš„è¯¦ç»†æƒ…å†µï¼ŒåŒ…æ‹¬å‚èµ›å›½å®¶ã€ä¸»è¦æ¯”èµ›é¡¹ç›®å’Œè·å¥–è€…çš„ä¿¡æ¯ï¼›æˆ–è€…è§£é‡Šç§‘å­¦ç†è®ºï¼Œæ¯”å¦‚é‡å­è®¡ç®—çš„åŸºæœ¬åŸç†åŠå…¶å¯¹æœªæ¥è®¡ç®—æœºæŠ€æœ¯çš„å½±å“ã€‚
> **è¿™é‡Œä¸ºçŸ¥é“è°æ˜¯è€å­ï¼Œä»¥åŠè€å­çš„æ€æƒ³ã€‚**
>   
> **è¾ƒå¼ºçš„äººç±»æŒ‡ä»¤éµå¾ªèƒ½åŠ›**
> ï¼š
>   
> å¦‚æœç”¨æˆ·è¯¢é—®ï¼šâ€œè¯·å¸®æˆ‘æŸ¥æ‰¾ä¸€ä¸‹å¦‚ä½•åœ¨å®¶åˆ¶ä½œä¸€æ¯æ‹¿é“å’–å•¡çš„æ–¹æ³•â€ï¼Œç³»ç»Ÿåº”è¯¥èƒ½å¤Ÿæä¾›è¯¦ç»†çš„æ­¥éª¤æŒ‡å¯¼ï¼Œä»å‡†å¤‡æ‰€éœ€ææ–™åˆ°å…·ä½“çš„åˆ¶ä½œæµç¨‹ï¼Œç¡®ä¿ç”¨æˆ·å¯ä»¥è½»æ¾è·Ÿéšæ“ä½œã€‚
> **è¿™é‡Œä¸ºç»™å‡ºæœ¬é¢˜çš„ç­”æ¡ˆB**
>   
> **æ”¹è¿›çš„å¤æ‚ä»»åŠ¡æ¨ç†èƒ½åŠ›**
> ï¼š
>   
> æ¯”å¦‚åˆ†æå¤æ‚çš„ç»æµæ¨¡å‹ï¼Œé¢„æµ‹æŸä¸€æ”¿ç­–å˜åŠ¨å¯¹ä¸åŒè¡Œä¸šçš„å½±å“ã€‚è¿™æ¶‰åŠåˆ°ç†è§£å®è§‚ç»æµæŒ‡æ ‡ã€å¸‚åœºè¶‹åŠ¿ä»¥åŠå„è¡Œä¸šçš„ç›¸äº’å…³ç³»ï¼Œå¹¶åŸºäºè¿™äº›ä¿¡æ¯åšå‡ºåˆç†çš„æ¨æµ‹ã€‚
> **è¿™é‡Œä¸ºåŸºäºè€è€…çš„è¯æ¨å‡ºè€è€…çš„æ€æƒ³ï¼Œå¹¶ç»™å‡ºä¸ä¹‹åŒ¹é…çš„æ”¿æ²»ç†å¿µæå‡ºè€…**
>   
> **è¾ƒå¼ºçš„é€šç”¨ä»»åŠ¡è§£å†³èƒ½åŠ›**
> ï¼šä¾‹å¦‚ï¼Œå¸®åŠ©ç”¨æˆ·è§„åˆ’ä¸€æ¬¡å›½é™…æ—…è¡Œï¼Œä¸ä»…åŒ…æ‹¬é¢„è®¢æœºç¥¨å’Œé…’åº—ï¼Œè¿˜éœ€è¦è€ƒè™‘ç­¾è¯ç”³è¯·ã€å½“åœ°äº¤é€šå®‰æ’ã€æ—…æ¸¸æ™¯ç‚¹æ¨èç­‰å¤šä¸ªæ–¹é¢çš„é—®é¢˜ï¼Œç¡®ä¿æ•´ä¸ªæ—…ç¨‹é¡ºåˆ©ã€‚
>   
> **è¾ƒå¥½çš„äººç±»å¯¹é½èƒ½åŠ›**
> ï¼š åœ¨ä¸ç”¨æˆ·çš„å¯¹è¯ä¸­ï¼Œèƒ½å¤Ÿç†è§£å¹¶å›åº”ç”¨æˆ·çš„æƒ…æ„ŸçŠ¶æ€ã€‚å¦‚æœç”¨æˆ·è¡¨è¾¾å‡ºå¤±æœ›æˆ–å›°æƒ‘çš„æƒ…ç»ªï¼Œç³»ç»Ÿåº”èƒ½ä»¥åŒç†å¿ƒå›åº”ï¼Œå¹¶æä¾›é€‚å½“çš„æ”¯æŒæˆ–å»ºè®®ï¼Œæ¯”å¦‚å½“ç”¨æˆ·æåˆ°å› ä¸ºé”™è¿‡ä¸€åœºé‡è¦ä¼šè®®è€Œæ„Ÿåˆ°æ²®ä¸§æ—¶ï¼Œç³»ç»Ÿå¯ä»¥æä¾›ä¸€äº›æ—¶é—´ç®¡ç†æŠ€å·§æˆ–é¼“åŠ±çš„è¯è¯­ã€‚
>   
> **è¾ƒå¼ºçš„å¤šè½®å¯¹è¯äº¤äº’èƒ½åŠ›**
> ï¼š
>   
> æ”¯æŒæ·±å…¥ä¸”è¿è´¯çš„å¯¹è¯ã€‚ä¾‹å¦‚ï¼Œåœ¨è®¨è®ºä¸€ä¸ªé¡¹ç›®çš„è®¡åˆ’æ—¶ï¼Œç”¨æˆ·å¯ä»¥é€æ­¥æå‡ºä¸åŒçš„é—®é¢˜æˆ–è¦æ±‚æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œç³»ç»Ÿèƒ½å¤Ÿä¿æŒä¸Šä¸‹æ–‡çš„ç†è§£ï¼ŒæŒç»­ç»™å‡ºç›¸å…³çš„å›ç­”å’Œå»ºè®®ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½éœ€è¦é‡æ–°è¾“å…¥èƒŒæ™¯ä¿¡æ¯ã€‚
> **è¿™é‡Œæ²¡æœ‰ç»§ç»­é—®ï¼Œä¸€é—®ä¸€ä¸ªä¸å±å£°**

#### å¤§è¯­è¨€æ¨¡å‹çš„ç™¾èŠ±é½æ”¾æ—¶ä»£

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/977f911920464b4480a83ef77c40c181.png)
  
è®ºæ–‡åœ°å€:
  
<https://arxiv.org/abs/2303.18223>
  
<https://hub.baai.ac.cn/view/27667>

#### è¯­è¨€æ¨¡å‹å‘å±•å†ç¨‹

è¯­è¨€æ¨¡å‹é€šå¸¸æ˜¯æŒ‡èƒ½å¤Ÿå»ºæ¨¡è‡ªç„¶è¯­è¨€æ–‡æœ¬ç”Ÿæˆæ¦‚ç‡çš„æ¨¡å‹ã€‚
  
ä»è¯­è¨€å»ºæ¨¡åˆ°ä»»åŠ¡æ±‚è§£ï¼Œè¿™æ˜¯ç§‘å­¦æ€ç»´çš„ä¸€æ¬¡é‡è¦è·ƒå‡ã€‚
  
è¯­è¨€æ¨¡å‹çš„å‘å±•å†ç¨‹å¦‚ä¸‹ï¼š
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/d504b2c9da7240d6ad260b25c998dd61.png)

##### ç»Ÿè®¡è¯­è¨€æ¨¡å‹ï¼ˆStatistical Language models,SLMï¼‰

â¢ ä¸»è¦å»ºç«‹åœ¨ç»Ÿè®¡å­¦ä¹ ç†è®ºæ¡†æ¶ï¼Œé€šå¸¸ä½¿ç”¨é“¾å¼æ³•åˆ™å»ºæ¨¡å¥å­åºåˆ—
  
â¢ ä¾‹å¦‚ï¼š
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b6df20b8b2db4af39e5c8cd40779692e.png)

###### n-gram è¯­è¨€æ¨¡å‹ï¼š

n-gram è¯­è¨€æ¨¡å‹ï¼šåŸºäºé©¬å°”ç§‘å¤«å‡è®¾ï¼Œå½“å‰è¯æ¦‚ç‡ä»…ä¸å‰ğ‘› âˆ’ 1ä¸ªè¯æœ‰å…³
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/90d6a97add194cb8acc4fd1806356998.png)

â¢ å¦‚æœä½¿ç”¨äºŒå…ƒè¯­è¨€æ¨¡å‹ï¼Œåˆ™ä¸Šè¿°ç¤ºä¾‹æ¦‚ç‡è®¡ç®—å˜ä¸º
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/6e80f32f918145e5ae54aa4c99c84d82.png)

> è¿™é‡Œçš„p(I,am,fime)è¡¨ç¤ºè¿™ä¸ªå¥å­åœ¨è¯­æ–™åº“ä¸­å‡ºç°çš„æ¦‚ç‡ï¼Œæ¦‚ç‡è¶Šå¤§ï¼Œè¯´æ˜è¿™ä¸ªå¥å­çº¦åˆç†ã€‚p(fine|am)è¡¨ç¤ºè¯­æ–™åº“ä¸­ï¼Œamåé¢å‡ºç°fineçš„æ¦‚ç‡ï¼Œæ¦‚ç‡è¶Šå¤§ï¼Œè¯´ä¹ˆè¿™ä¸ªamåé¢è¶Šå¯èƒ½åŠ fineã€‚

è¯­æ–™åº“çš„æ ·ä¾‹å¦‚ä¸‹ï¼š
  
å‚è€ƒ:
<https://bcc.blcu.edu.cn/zh/search/0/%E6%88%91%E5%BE%88>
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/14fede33a5af4942945d876ffc49cb26.png)

###### åŸºäºé¢‘ç‡çš„ä¼°è®¡æ–¹æ³• (æœ€å¤§ä¼¼ç„¶ä¼°è®¡)

â¢ å››å…ƒè¯­è¨€æ¨¡å‹ä¼°è®¡ç¤ºä¾‹
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1da6e4ec8e17406dab9fe5db4ab6b04e.png)
  
â¢ ä¸»è¦é—®é¢˜
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b506fa1104904dd3ae66c379dd7835bd.png)

â¢ è§£å†³åŠæ³•-åŠ ä¸€å¹³æ»‘ (åˆç§°ä¸º Laplace smoothing )
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e95c797d66484f37bd09f0e4e0521967.png)

â¢ å›é€€ (back-off)
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1e9eeb991a2746659b41962b8244204f.png)
  
â¢ æ’å€¼ (interpolation)
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/f675e6782592471a8c5392fb9da06c64.png)

> å¯ä»¥è¯æ˜ï¼Œä»ç„¶èƒ½å¤Ÿä¿è¯è¯­è¨€æ¨¡å‹çš„æ¦‚ç‡æ€§è´¨
>   
> é€šå¸¸è¿™ç§æ–¹å¼å¯ä»¥ç»“åˆä¸åŒé˜¶æ•°ä¼°è®¡æ–¹æ³•çš„ä¼˜åŠ¿
>   
> ä½†ä»ç„¶ä¸èƒ½ä»æ ¹æœ¬è§£å†³æ•°æ®ç¨€ç–æ€§é—®é¢˜

##### ç¥ç»è¯­è¨€æ¨¡å‹ï¼ˆNeural Language Models,NLMï¼‰

> åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ŒNLM æŒ‡ç¥ç»è¯­è¨€æ¨¡å‹ï¼ˆNeural Language Modelsï¼‰ã€‚å®ƒåˆ©ç”¨ç¥ç»ç½‘ç»œæ¥å­¦ä¹ å’Œè¡¨ç¤ºè¯­è¨€çš„æ¦‚ç‡åˆ†å¸ƒï¼Œèƒ½å¤Ÿæ›´åŠ ç²¾ç¡®åœ°ç†è§£ã€å¤„ç†å’Œç”Ÿæˆè‡ªç„¶è¯­è¨€ã€‚é€šè¿‡æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œçš„ç»“åˆï¼Œä»å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸­å­¦ä¹ è¯­è¨€çš„ç»Ÿè®¡è§„å¾‹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä»è€Œæ•æ‰åˆ°è¯è¯­ä¹‹é—´çš„å…³è”å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œæé«˜å¯¹è‡ªç„¶è¯­è¨€çš„ç†è§£èƒ½åŠ›ã€‚
>   
> è¿™ç§æ¨¡å‹é€šå¸¸é‡‡ç”¨å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ã€é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰å’Œå˜å‹å™¨ï¼ˆTransformerï¼‰ç­‰ç»“æ„ï¼Œä»¥å»ºæ¨¡æ–‡æœ¬åºåˆ—å¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯æˆ–å­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€‚
>   
> ç¥ç»è¯­è¨€æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ï¼Œæ¶µç›–äº†æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬ç”Ÿæˆç­‰å¤šä¸ªä»»åŠ¡ã€‚

###### æ—©æœŸå·¥ä½œï¼ˆMLPæˆ–NNLPï¼‰åŸç†

æ—©æœŸå·¥ä½œMLPï¼ˆMultilayer Perceptron,MLPï¼Œå¤šå±‚æ„ŸçŸ¥æœºï¼‰ï¼š
  
NNLMï¼ˆNeural Network Language Modelï¼Œç¥ç»ç½‘ç»œè¯­è¨€æ¨¡å‹ï¼‰ï¼Œå•è¯æ˜ å°„åˆ°è¯å‘é‡ï¼Œå†ç”±ç¥ç»ç½‘ç»œé¢„æµ‹å½“å‰æ—¶åˆ»è¯æ±‡ã€‚æ˜¯ä¸€ç§é€šè¿‡ç¥ç»ç½‘ç»œè¿›è¡Œè¯­è¨€å»ºæ¨¡çš„æŠ€æœ¯ï¼Œé€šå¸¸ç”¨äºé¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚

NNLMçš„æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨è¯åµŒå…¥ï¼ˆword embeddingï¼‰å°†è¯è½¬æ¢ä¸ºä½ç»´å‘é‡ï¼Œå¹¶é€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ è¯­è¨€ä¸­çš„è¯åºå…³ç³»ã€‚
  
NNLMçš„åŸºæœ¬ç»“æ„åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š

> è¾“å…¥å±‚ï¼šè¾“å…¥ä¸€ä¸ªå›ºå®šé•¿åº¦çš„è¯çª—å£ï¼Œä¾‹å¦‚ n ä¸ªè¯çš„ä¸Šä¸‹æ–‡ï¼ˆå‰ n - 1 ä¸ªè¯ï¼‰ä½œä¸ºè¾“å…¥ã€‚
>   
> åµŒå…¥å±‚ï¼šå°†æ¯ä¸ªè¾“å…¥è¯æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ï¼Œå¾—åˆ°è¯å‘é‡ã€‚è¿™ä¸€å±‚çš„æƒé‡çŸ©é˜µé€šå¸¸è¡¨ç¤ºä¸ºè¯åµŒå…¥çŸ©é˜µã€‚
>   
> éšè—å±‚ï¼šä¸€ä¸ªæˆ–å¤šä¸ªéšè—å±‚å¯ä»¥æ•è·è¯ä¹‹é—´çš„å…³ç³»ï¼Œä¸€èˆ¬æ˜¯å…¨è¿æ¥å±‚ã€‚
>   
> è¾“å‡ºå±‚ï¼šç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œé€šå¸¸ä½¿ç”¨softmaxå‡½æ•°ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/7d6da8edfe8e4ef3ae218c6c8e9a6e21.png)

> A goal of statistical language modeling is to learn the joint probabilityfunction of sequences of words.
>   
> ç»Ÿè®¡è¯­è¨€å»ºæ¨¡çš„ä¸€ä¸ªç›®æ ‡æ˜¯å­¦ä¹ å•è¯åºåˆ—çš„è”åˆæ¦‚ç‡å‡½æ•°ã€‚
>   
> This is intrinsically difficult because ofthe curse of dimensionality:we propose to fight it with its own weapons.
>   
> è¿™æœ¬è´¨ä¸Šæ˜¯å›°éš¾çš„ï¼Œå› ä¸ºç»´åº¦è¯…å’’ï¼šæˆ‘ä»¬å»ºè®®ç”¨è‡ªå·±çš„æ­¦å™¨æ¥å¯¹æŠ—å®ƒã€‚
>   
> In the proposed approach one learns simultaneously(1)a distributed rep-resentation for each word (i.e.a similarity between words)along with(2)the probability function for word sequences,expressed with these repre-sentations.
>   
> åœ¨æ‰€æå‡ºçš„æ–¹æ³•ä¸­ï¼Œäººä»¬åŒæ—¶å­¦ä¹ ï¼ˆ1ï¼‰æ¯ä¸ªå•è¯çš„åˆ†å¸ƒå¼è¡¨ç¤ºï¼ˆå³å•è¯ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼‰ä»¥åŠï¼ˆ2ï¼‰ç”¨è¿™äº›è¡¨ç¤ºè¡¨ç¤ºçš„å•è¯åºåˆ—çš„æ¦‚ç‡å‡½æ•°ã€‚
>   
> Generalization is obtained because a sequence of words that has never been seen before gets high probability, if it is made of words that are similar to words forming an already seen sentence.
>   
> ä¹‹æ‰€ä»¥è·å¾—æ³›åŒ–ï¼Œæ˜¯å› ä¸ºå¦‚æœä¸€ä¸ªä»¥å‰ä»æœªè§è¿‡çš„å•è¯åºåˆ—æ˜¯ç”±ä¸å·²ç»è§è¿‡çš„å¥å­ä¸­çš„å•è¯ç›¸ä¼¼çš„å•è¯ç»„æˆçš„ï¼Œé‚£ä¹ˆå®ƒçš„æ¦‚ç‡å¾ˆé«˜ã€‚
>   
> We report onexperiments using neural networks for the probability function,showingon two text corpora that the proposed approach very significantly im-proves on a state-of-the-art trigram model.
>   
> æˆ‘ä»¬æŠ¥å‘Šäº†ä¸€é¡¹ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œæ¦‚ç‡å‡½æ•°çš„å®éªŒï¼Œåœ¨ä¸¤ä¸ªæ–‡æœ¬è¯­æ–™åº“ä¸Šè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨æœ€å…ˆè¿›çš„ä¸‰å…ƒç»„æ¨¡å‹ä¸Šå¾—åˆ°äº†éå¸¸æ˜¾è‘—çš„æ”¹è¿›ã€‚

è¯¥æ¨¡å‹ä¸»è¦ä»»åŠ¡æ˜¯
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/518e9d7c44074326a91c442a665d9cf9.png)

è¯¥æ¨¡å‹çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š

> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/7f4993aa571b43de8c13617889baa74e.png)
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/5d008c3c877e4579ab5de121595ac845.png)
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/36ae6632df25455096c8e013abebdf22.png)

###### æ—©æœŸå·¥ä½œï¼ˆMLPæˆ–NNLPï¼‰ä»£ç å®ç°

å‚è€ƒï¼š
  
<https://blog.csdn.net/weixin_62472350/article/details/143448417>
  
<https://www.bilibili.com/video/BV1AT4y1J7bv/>
  
[NNLM çš„ PyTorch å®ç°](https://wmathor.com/index.php/archives/1442/)

```python
# 1.å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optimizer
import torch.utils.data as Data
 
# 2.æ•°æ®å‡†å¤‡
sentences = ["I like milk",
             "I love hot-pot",
             "I hate coffee",
             "I want sing",
             "I am sleep",
             "I go home",
             "Love you forever"]
 
word_list = " ".join(sentences).split()     # è·å–ä¸ªå¥å­å•è¯
word_list = list(set(word_list))            # è·å–å•è¯åˆ—è¡¨
 
word_dict = {w: i for i, w in enumerate(word_list)}     # å•è¯-ä½ç½®ç´¢å¼•å­—å…¸
number_dict = {i: w for i, w in enumerate(word_list)}   # ä½ç½®-å•è¯ç´¢å¼•å­—å…¸
 
vocab_size = len(word_list)         # è¯æ±‡è¡¨å¤§å°
 
# 3.X-ç”Ÿæˆè¾“å…¥å’Œè¾“å‡ºæ•°æ®
def make_data():
    input_data = []
    output_data = []
 
    for sen in sentences:
        word = sen.split()
        input_temp = [word_dict[n] for n in word[:-1]]
        output_temp = word_dict[word[-1]]
 
        input_data.append(input_temp)
        output_data.append(output_temp)
 
    return input_data, output_data
 
 
input_data, output_data = make_data()
input_data, output_data = torch.LongTensor(input_data), torch.LongTensor(output_data)   # æ•°æ®è½¬æ¢ï¼šå°† input_data å’Œ output_data è½¬æ¢ä¸º LongTensorï¼Œä»¥ä¾¿ç”¨äºæ¨¡å‹è®­ç»ƒã€‚
dataset = Data.TensorDataset(input_data, output_data)       # å»ºæ•°æ®é›†ï¼šData.TensorDataset å°†è¾“å…¥å’Œè¾“å‡ºé…å¯¹ä¸ºæ•°æ®é›†
loader = Data.DataLoader(dataset, 4, True)                  # æ•°æ®åŠ è½½å™¨ï¼šDataLoaderï¼Œä½¿ç”¨æ‰¹é‡å¤§å°ä¸º 2ï¼Œéšæœºæ‰“ä¹±æ ·æœ¬
 
 
# 4.åˆå§‹åŒ–å‚æ•°
m = 2
n_step = 2
n_hidden = 10
 
 
# 5.æ¨¡å‹å®šä¹‰
class NNLM(nn.Module):
    def __init__(self):
        super(NNLM, self).__init__()
        self.C = nn.Embedding(vocab_size, m)
        self.H = nn.Linear(n_step * m, n_hidden, bias=False)
        self.d = nn.Parameter(torch.ones(n_hidden))
        self.U = nn.Linear(n_hidden, vocab_size, bias=False)
        self.W = nn.Linear(n_step * m, vocab_size, bias=False)
        self.b = nn.Parameter(torch.ones(vocab_size))
 
    def forward(self, X):
        X = self.C(X)               # X = [batch_size, n_step, m]
        X = X.view(-1, n_step * m)  # å±•å¹³ X = [batch_size, n_step * m]
        hidden_output = torch.tanh(self.d + self.H(X))
        output = self.b + self.W(X) + self.U(hidden_output)
        return output
 
# 6.å®šä¹‰è®­ç»ƒè¿‡ç¨‹
model = NNLM()
optim = optimizer.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()
 
# 7.æ¨¡å‹è®­ç»ƒ
for epoch in range(5000):
    for batch_x, batch_y in loader:
        pred = model(batch_x)
        loss = criterion(pred, batch_y)
        if (epoch + 1) % 1000 ==0:
            print(epoch+1, loss.item())
        optim.zero_grad()
        loss.backward()
        optim.step()
 
# 8.æ¨¡å‹æµ‹è¯•
pred = model(input_data).max(1, keepdim=True)[1]
print([number_dict[idx.item()] for idx in pred.squeeze()])

```

**ä»£ç è§£é‡Š**
  
**1.æœ€å¼€å§‹å¯¼å…¥ä¸€äº›å¿…è¦çš„åº“**

```python
# 1.å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.optim as optimizer
import torch.utils.data as Data

```

**2.é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€äº›æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒå¹¶æµ‹è¯•**

```python
word_list = " ".join(sentences).split()

```

â‘ å¯¹äºæ–‡æœ¬æ•°æ®ï¼Œè‚¯å®šè¦è¿›è¡Œä¸€ä¸ªåˆ†è¯æ“ä½œï¼Œå…ˆä½¿ç”¨" ".join(sentences).split()æ¥åˆ‡åˆ†æ¯ä¸€ä¸ªå¥å­çš„æ¯ä¸ªå•è¯ï¼Œè¿™æ—¶å€™è·å¾—çš„word\_liståˆ—è¡¨å°±æ˜¯ï¼š

> [â€˜Iâ€™, â€˜likeâ€™, â€˜milkâ€™, â€˜Iâ€™, â€˜loveâ€™, â€˜hot-potâ€™, â€˜Iâ€™, â€˜hateâ€™, â€˜coffeeâ€™, â€˜Iâ€™, â€˜wantâ€™, â€˜singâ€™, â€˜Iâ€™, â€˜amâ€™, â€˜sleepâ€™, â€˜Iâ€™, â€˜goâ€™, â€˜homeâ€™, â€˜Loveâ€™, â€˜youâ€™, â€˜foreverâ€™]

â‘¡ä½†æ˜¯è¿™é‡Œå¾—åˆ°çš„å•è¯å¯èƒ½ä¼šæœ‰é‡å¤çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å¾—åˆ°ä¸é‡å¤çš„å•è¯åˆ—è¡¨ï¼Œä¸ºåé¢çš„åˆ›å»ºè¯å…¸æä¾›æ–¹ä¾¿ã€‚

```python
word_list = list(set(word_list))  

```

set (word\_list) ï¼šå°† word\_list è½¬æ¢ä¸ºé›†åˆï¼ˆsetï¼‰ï¼Œè‡ªåŠ¨å»é™¤åˆ—è¡¨ä¸­çš„é‡å¤å…ƒç´ ã€‚
  
list(set (word\_list) )ï¼šå†å°†é›†åˆè½¬æ¢å›åˆ—è¡¨ï¼Œè¿™æ ·å¯ä»¥ä¿æŒåŸæ•°æ®ç±»å‹ä¸€è‡´ï¼ˆå³ word\_list ä»ç„¶æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼‰ã€‚

> [â€˜milkâ€™, â€˜coffeeâ€™, â€˜singâ€™, â€˜hot-potâ€™, â€˜homeâ€™, â€˜youâ€™, â€˜amâ€™, â€˜Iâ€™, â€˜sleepâ€™, â€˜loveâ€™, â€˜wantâ€™, â€˜hateâ€™, â€˜goâ€™, â€˜foreverâ€™, â€˜likeâ€™, â€˜Loveâ€™]

â‘¢ç„¶åå°±æ˜¯æ„é€ è¯å…¸ï¼šå•è¯åˆ°ä½ç½®çš„ç´¢å¼•å­—å…¸ã€ä½ç½®åˆ°å•è¯çš„ç´¢å¼•å­—å…¸ã€‚

```python
word_dict = {w: i for i, w in enumerate(word_list)}     # å•è¯-ä½ç½®ç´¢å¼•å­—å…¸
number_dict = {i: w for i, w in enumerate(word_list)}   # ä½ç½®-å•è¯ç´¢å¼•å­—å…¸

```

> word\_dictï¼š {â€˜sleepâ€™: 0, â€˜goâ€™: 1, â€˜homeâ€™: 2, â€˜milkâ€™: 3, â€˜hateâ€™: 4, â€˜Loveâ€™: 5, â€˜loveâ€™: 6, â€˜amâ€™: 7, â€˜wantâ€™: 8, â€˜singâ€™: 9, â€˜foreverâ€™: 10, â€˜hot-potâ€™: 11, â€˜Iâ€™: 12, â€˜likeâ€™: 13, â€˜youâ€™: 14, â€˜coffeeâ€™: 15}

> number\_dictï¼š{0: â€˜sleepâ€™, 1: â€˜goâ€™, 2: â€˜homeâ€™, 3: â€˜milkâ€™, 4: â€˜hateâ€™, 5: â€˜Loveâ€™, 6: â€˜loveâ€™, 7: â€˜amâ€™, 8: â€˜wantâ€™, 9: â€˜singâ€™, 10: â€˜foreverâ€™, 11: â€˜hot-potâ€™, 12: â€˜Iâ€™, 13: â€˜likeâ€™, 14: â€˜youâ€™, 15: â€˜coffeeâ€™}

```
    è¿™é‡Œä½¿ç”¨enumerateæ˜¯å› ä¸º enumerate å‡½æ•°å¯ä»¥æ–¹ä¾¿åœ°åŒæ—¶è·å–åˆ—è¡¨å…ƒç´ çš„ç´¢å¼•å’Œå¯¹åº”çš„å€¼ã€‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„å­—å…¸ã€‚

```

â‘£ç„¶åå°±æ˜¯è·å¾—è¯æ±‡è¡¨å¤§å°ï¼Œåœ¨æ¨¡å‹æ­å»ºä¸­éœ€è¦ç”¨åˆ°ã€‚

**3.æœ‰äº†åˆå§‹æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºå‡ºæ•°æ®Xï¼Œä¹Ÿå°±æ˜¯è¾“å…¥æ•°æ®å’Œç›®æ ‡è¾“å‡ºæ•°æ®ã€‚**

```python
def make_data():
    input_data = []
    output_data = []
 
    for sen in sentences:
        word = sen.split()
        input_temp = [word_dict[n] for n in word[:-1]]
        output_temp = word_dict[word[-1]]
 
        input_data.append(input_temp)
        output_data.append(output_temp)
 
    return input_data, output_data

```

â‘ å…ˆæ„å»ºç©ºçš„è¾“å…¥æ•°æ®input\_dataå’Œè¾“å‡ºæ•°æ®output\_dataã€‚

â‘¡å°†æ¯ä¸ªå¥å­çš„å‰ n-1ä¸ªå•è¯çš„ä½ç½®æ·»åŠ åˆ°è¾“å…¥æ•°æ®input\_dataä¸­ï¼Œç¬¬ n ä¸ªå•è¯çš„ä½ç½®æ·»åŠ åˆ°è¾“å…¥æ•°æ®output\_dataä¸­ï¼Œå¾—åˆ°æ¯ä¸ªè¾“å…¥ x å’Œè¾“å‡º y åœ¨è¯æ±‡è¡¨ä¸­çš„é¡ºåºï¼š

> input\_dataï¼š[[12, 13], [12, 6], [12, 4], [12, 8], [12, 7], [12, 1], [5, 14]]

> output\_dataï¼š[3, 11, 15, 9, 0, 2, 10]

è¿™æ˜¯ä¸ªäºŒç»´çš„çŸ©é˜µï¼Œè¡Œå…ƒç´ ä»£è¡¨ä¸€ä¸ªå¥å­ä¸­ç”¨äºè®­ç»ƒè¾“å…¥/æµ‹è¯•è¾“å‡ºçš„å•è¯åœ¨è¯æ±‡è¡¨ä¸­çš„ä½ç½®ç´¢å¼•ï¼›åˆ—å…ƒç´ æ˜¯ä¸åŒçš„å¥å­ã€‚
  
æ¯ä¸ªå¥å­éƒ½æ˜¯3ä¸ªå•è¯ï¼Œå‰2ä¸ªä½œä¸ºå‰æ–‡ä¿¡æ¯ä½œä¸ºè¾“å…¥ï¼Œç¬¬3ä¸ªä½œä¸ºé¢„æµ‹è¾“å‡ºï¼Œæˆ‘ä»¬å‰é¢ç»™çš„ä¸€å…±æ˜¯7ä¸ªå¥å­ã€‚

**4.åˆå§‹åŒ–å‚æ•°**

è¿™é‡Œçš„mæŒ‡çš„æ˜¯ç»´åº¦ï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªå•è¯è¦åµŒå…¥åˆ°å¤šå°‘ç»´åº¦ï¼Œç”±äºè¿™é‡Œçš„æ•°æ®é‡æ¯”è¾ƒå°ï¼Œæ¯ä¸ªå¥å­ä¹Ÿåªæœ‰3ä¸ªå•è¯ï¼Œæ‰€ä»¥è¿™ç»™å‡ºçš„ç»´åº¦é€‰ä¸ªå¾ˆä½çš„2ã€‚

> n\_step=2ï¼ŒæŒ‡çš„æ˜¯ç”¨ä¸¤ä¸ªå•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªç›®æ ‡å•è¯ã€‚
>   
> n\_hidden=10ï¼ŒæŒ‡çš„æ˜¯éšè—å±‚çš„æ•°é‡ã€‚

**5.å‰é¢çš„æ•°æ®å·²ç»åˆæ­¥å®šä¹‰å¥½äº†ï¼Œè¿™é‡Œå°±è¦æ­å»ºNNLMæ¨¡å‹äº†ã€‚**

```python
class NNLM(nn.Module):
    def __init__(self):
        super(NNLM, self).__init__()
        self.C = nn.Embedding(vocab_size, m)
        self.H = nn.Linear(n_step * m, n_hidden, bias=False)
        self.d = nn.Parameter(torch.ones(n_hidden))
        self.U = nn.Linear(n_hidden, vocab_size, bias=False)
        self.W = nn.Linear(n_step * m, vocab_size, bias=False)
        self.b = nn.Parameter(torch.ones(vocab_size))
 
    def forward(self, X):
        X = self.C(X)               # X = [batch_size, n_step, m]
        X = X.view(-1, n_step * m)  # å±•å¹³ X = [batch_size, n_step * m]
        hidden_output = torch.tanh(self.d + self.H(X))
        output = self.b + self.W(X) + self.U(hidden_output)
        return output

```

â‘ def \_init\_(self)ï¼šå®šä¹‰å„å±‚å’Œå‚æ•°ï¼š

> self.Cï¼šè¯åµŒå…¥å±‚ï¼Œå°†è¾“å…¥è¯è½¬æ¢ä¸ºè¯å‘é‡ã€‚
>   
> vocab\_size å®šä¹‰è¯æ±‡è¡¨çš„å¤§å°ï¼Œm æ˜¯è¯åµŒå…¥çš„ç»´åº¦ï¼Œè¡¨ç¤ºæ¯ä¸ªè¯å°†è¢«åµŒå…¥æˆ m ç»´çš„å‘é‡ã€‚

> self.Hï¼šçº¿æ€§å±‚ï¼Œå°†å±•å¹³åçš„è¾“å…¥æ˜ å°„åˆ°éšè—å±‚ã€‚
>   
> n\_step \* m æ˜¯å±•å¹³åçš„è¾“å…¥å¤§å°ï¼Œn\_hidden æ˜¯éšè—å±‚çš„ç»´åº¦ï¼Œç”¨æ¥æ§åˆ¶éšè—å±‚è¾“å‡ºçš„ç‰¹å¾æ•°é‡ã€‚

> self.dï¼šåç½®å‘é‡ï¼Œç”¨äºéšè—å±‚çš„è¾“å‡ºã€‚
>   
> n\_hidden æ˜¯åç½®é¡¹çš„ç»´åº¦ï¼Œä¸éšè—å±‚è¾“å‡ºåŒ¹é…ï¼Œç”¨äºæå‡éšè—å±‚çš„è¡¨è¾¾èƒ½åŠ›ã€‚

> self.Uï¼šçº¿æ€§å±‚ï¼Œå°†éšè—å±‚è¾“å‡ºæ˜ å°„åˆ°è¯æ±‡è¡¨ç©ºé—´ã€‚
>   
> n\_hidden æ˜¯éšè—å±‚è¾“å‡ºçš„å¤§å°ï¼Œvocab\_size æ˜¯è¯æ±‡è¡¨å¤§å°ï¼Œç”¨äºå°†éšè—å±‚çš„ç‰¹å¾æ˜ å°„åˆ°æ¯ä¸ªè¯çš„é¢„æµ‹ç©ºé—´ã€‚

> self.Wï¼šçº¿æ€§å±‚ï¼Œä»è¾“å…¥ç›´æ¥æ˜ å°„åˆ°è¯æ±‡è¡¨ç©ºé—´ã€‚
>   
> n\_step \* m æ˜¯å±•å¹³åçš„è¾“å…¥å¤§å°ï¼Œvocab\_size æ˜¯è¯æ±‡è¡¨å¤§å°ï¼Œç”¨äºå°†è¾“å…¥ç›´æ¥æ˜ å°„åˆ°è¯æ±‡è¡¨çš„é¢„æµ‹ç©ºé—´ã€‚

> self.bï¼šåç½®å‘é‡ï¼Œç”¨äºæœ€ç»ˆè¾“å‡ºå±‚çš„åˆ†æ•°è°ƒæ•´ã€‚
>   
> vocab\_size æ˜¯è¯æ±‡è¡¨çš„å¤§å°ï¼Œç”¨ä½œæœ€ç»ˆè¾“å‡ºå±‚çš„åç½®ã€‚

è¿™é‡Œåˆ†åˆ«ç”¨äº†Embeddingã€Linearå’ŒParameterï¼š

> Embeddingï¼šåµŒå…¥å±‚ï¼Œç”¨äºå°†ç¦»æ•£çš„è¯æ±‡ç´¢å¼•ï¼ˆå¦‚å•è¯çš„æ•´æ•°è¡¨ç¤ºï¼‰æ˜ å°„åˆ°è¿ç»­çš„ç¨ å¯†å‘é‡ç©ºé—´ã€‚
>   
> Linearï¼šå…¨è¿æ¥å±‚ï¼ˆçº¿æ€§å±‚ï¼‰ï¼Œç”¨äºå°†è¾“å…¥çš„ç‰¹å¾é€šè¿‡çº¿æ€§å˜æ¢æ˜ å°„åˆ°è¾“å‡ºç©ºé—´ã€‚
>   
> Parameterï¼šå¯å­¦ä¹ çš„å‚æ•°ã€‚

â‘¡def forward(self,X)ï¼šå®šä¹‰ç¥ç»ç½‘ç»œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„è®¡ç®—æ­¥éª¤

```python
X = self.C(X)

```

> é¦–å…ˆé€šè¿‡åµŒå…¥å±‚å°†è¾“å…¥çš„è¯ç´¢å¼•ï¼ˆXï¼‰è½¬æ¢ä¸ºè¯å‘é‡è¡¨ç¤ºï¼Œè¿™ä¸ªæ—¶å€™å¾—åˆ°æ˜¯ä¸‰ç»´åº¦çš„ï¼š[batch\_size, n\_step, m]ã€‚

```python
X = X.view(-1, n_step * m)

```

> ç„¶åå°† X ä»ä¸‰ç»´å¼ é‡å±•å¹³ä¸ºäºŒç»´å¼ é‡ [batch\_size, n\_step \* m]ï¼Œæ–¹ä¾¿è¾“å…¥åˆ°å…¨è¿æ¥å±‚ self.Hã€‚

```python
hidden_output = torch.tanh(self.d + self.H(X))

```

> æ¥ç€ï¼Œåˆ©ç”¨å…¬å¼
>
> h
> =
> t
> a
> n
> h
> (
> W
> h
> X
> +
> d
> )
> h=tanh(W\_hX+d)
>
>
>
>
>
> h
>
>
>
> =
>
>
>
>
>
> t
>
> anh
>
> (
>
>
> W
>
>
>
>
>
>
>
>
>
> h
>
> â€‹
>
>
> X
>
>
>
> +
>
>
>
>
>
> d
>
> )
> è®¡ç®—å¾—åˆ°éšè—å±‚è¾“å‡ºã€‚

```python
output = self.b + self.W(X) + self.U(hidden_output)

```

> æœ€åï¼Œåˆ©ç”¨å…¬å¼
>
> o
> u
> t
> p
> u
> t
> =
> b
> +
> W
> X
> +
> U
> h
> output=b+WX+Uh
>
>
>
>
>
> o
>
> u
>
> tp
>
> u
>
> t
>
>
>
> =
>
>
>
>
>
> b
>
>
>
> +
>
>
>
>
>
> W
>
> X
>
>
>
> +
>
>
>
>
>
> U
>
> h
> å¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

**6.å®šä¹‰è®­ç»ƒè¿‡ç¨‹**

è¿™é‡Œåˆå§‹åŒ–modelï¼Œå¹¶ä¸”è®¾ç½®ä¼˜åŒ–å™¨ä¸ºAdamï¼Œå¹¶ä¸”ä½¿ç”¨äº†äº¤å‰ç†µæŸå¤±ã€‚

**7.æ¨¡å‹è®­ç»ƒ**

```python
for epoch in range(5000):
    for batch_x, batch_y in loader:
        pred = model(batch_x)
        loss = criterion(pred, batch_y)
        if (epoch + 1) % 1000 ==0:
            print(epoch+1, loss.item())
        optim.zero_grad()
        loss.backward()
        optim.step()

```

è¿™é‡Œä»æ•°æ®åŠ è½½å™¨ä¸­åŠ è½½æ•°æ®ï¼Œå°†å½“å‰æ‰¹æ¬¡çš„è¾“å…¥æ•°æ®batch\_xä¼ å…¥æ¨¡å‹ä¸­å¾—åˆ°é¢„æµ‹ç»“æœï¼ŒåŒæ—¶è®¡ç®—é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„æŸå¤±lossï¼Œæ¯1000ä¸ªepochæ‰“å°æŸå¤±ã€‚ç„¶åæ¢¯åº¦æ¸…é›¶ã€åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ã€æ›´æ–°æ¨¡å‹å‚æ•°ã€‚

**8.æ¨¡å‹æµ‹è¯•**

```python
pred = model(input_data).max(1, keepdim=True)[1]

```

> model(input\_data)ï¼šå°†è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œè·å–æ¯ä¸ªç±»åˆ«çš„å¾—åˆ†ï¼ˆlogitsï¼‰ã€‚
>   
> max(1, keepdim=True)[1]ï¼š
>
> > max(1)ï¼šå¯¹æ¯ä¸ªæ ·æœ¬æ‰¾å‡ºæœ€å¤§å¾—åˆ†çš„ç±»åˆ«ç´¢å¼•ã€‚
> >   
> > keepdim=Trueï¼šä¿æŒè¾“å‡ºç»´åº¦ä¸å˜ã€‚
> >   
> > [1]ï¼šæå–æ¯ä¸ªæ ·æœ¬çš„æœ€å¤§å€¼ç´¢å¼•ï¼ˆé¢„æµ‹ç±»åˆ«ï¼‰ã€‚

```python
print([number_dict[idx.item()] for idx in pred.squeeze()])

```

> pred.squeeze()ï¼šç§»é™¤ç»´åº¦ä¸º 1 çš„ç»´åº¦ï¼Œå¾—åˆ°ä¸€ç»´å¼ é‡ã€‚
>   
> [idx.item() for idx in pred.squeeze()]ï¼šå°†æ¯ä¸ªç´¢å¼•è½¬æ¢ä¸ºæ•´æ•°ã€‚
>   
> number\_dict[idx.item()]ï¼šé€šè¿‡ç´¢å¼•æŸ¥æ‰¾å¯è¯»æ ‡ç­¾ã€‚
>   
> print([â€¦])ï¼šæ‰“å°å‡ºæ¨¡å‹é¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾ã€‚

> è¾“å‡ºï¼š
>   
> 1000 0.05966342240571976
>   
> 1000 0.034198883920907974
>   
> 2000 0.005526650696992874
>   
> 2000 0.009151813574135303
>   
> 3000 0.0021409429609775543
>   
> 3000 0.0015856553800404072
>   
> 4000 0.0006656644982285798
>   
> 4000 0.0005017295479774475
>   
> 5000 0.00018937562708742917
>   
> 5000 0.00020660058362409472
>   
> [â€˜milkâ€™, â€˜hot-potâ€™, â€˜coffeeâ€™, â€˜singâ€™, â€˜sleepâ€™, â€˜homeâ€™, â€˜foreverâ€™]

###### ç¥ç»ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹

BPç¥ç»ç½‘ç»œï¼šè¯¯å·®åå‘ä¼ æ’­ç®—æ³•å…¬å¼æ¨å¯¼
  
å¼€ç«¯ï¼š BPç®—æ³•æå‡º
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/15dcb48d4b2e47408c4a28e8af629f4f.png)

1. BPç¥ç»ç½‘ç»œå‚æ•°ç¬¦å·åŠæ¿€æ´»å‡½æ•°è¯´æ˜
     
   ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/7f98c6ea29d54a509df443ff1457b7b0.png)
2. ç½‘ç»œè¾“å‡ºè¯¯å·®ï¼ˆæŸå¤±å‡½æ•°ï¼‰å®šä¹‰

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/ab1a21ed994a4c0c9bbab27325f84545.png)

3. éšè—å±‚ä¸è¾“å‡ºå±‚é—´çš„æƒé‡æ›´æ–°å…¬å¼æ¨å¯¼
     
   ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/be88cc604c8145d4afe3e77a623b423f.png)
4. è¾“å…¥å±‚ä¸éšè—å±‚é—´çš„æƒé‡æ›´æ–°å…¬å¼æ¨å¯¼

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/4e2cfd49ae9e437b898adef7e4fca4d8.png)

###### å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼ŒRecurrent Neural Networkï¼‰

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/4c3f0f089b6d4610ab483e174f925775.png)
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/54b81edb1c3747f294cc8903e56de02e.png)
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/0dbd72f84e7341f4b10dea5bd122ef5c.png)
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/a70d02887e164459aa9257c6aca7979d.png)
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/9304ab6c3a504ed08be2d61402b4dd80.png)

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/0afbaffaea7044e38448c602bda01cf9.png)

[RNNå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜](https://zhuanlan.zhihu.com/p/689910526)

###### ç®€åŒ–æ¨¡å‹ï¼šWord2Vec

å‚è€ƒï¼š
[ä¸€ç¯‡æ–‡ç« å…¥é—¨Word2Vec](https://blog.csdn.net/julac/article/details/127767053)
  
[è¯å‘é‡æ¨¡å‹word2vectorè¯¦è§£](https://www.cnblogs.com/stephen-goodboy/p/12574738.html)
  
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/c4d756efb63f467581190e1612945ef7.png)

â¢ åŸºæœ¬åŠŸèƒ½
  
âƒ ç»™å®šæ–‡æœ¬æ•°æ®ï¼Œå¯¹äºæ¯ä¸ªå•è¯å­¦ä¹ ä¸€ä¸ªä½ç»´è¡¨ç¤º
  
â¢ åŸºäºåˆ†å¸ƒå¼è¯­ä¹‰çš„æ€æƒ³è¿›è¡Œè®¾è®¡
  
âƒ è¯ä¹‰=èƒŒæ™¯å•è¯çš„è¯­ä¹‰
  
â¢ ä¸è€ƒè™‘çª—å£å†…å•è¯çš„é¡ºåº
  
âƒ åº”ç”¨äº†ç®€å•çš„average poolingçš„ç­–ç•¥
  
â¢ å……åˆ†è€ƒè™‘å®è·µå’Œæ•ˆæœ
  
âƒ æœ‰å¾ˆå¤šçš„ä¼˜åŒ–trickï¼Œé€Ÿåº¦å¿«ã€æ•ˆæœç¨³å®š

###### è¯å‘é‡ - ä»Word2Vecåˆ°ELMo

å‚è€ƒï¼š
[ã€å¤§æ¨¡å‹ç³»åˆ—ç¯‡ã€‘è¯å‘é‡ - ä»Word2Vecåˆ°ELMo](https://blog.csdn.net/Jackie_vip/article/details/141600366)

> è¯å‘é‡ï¼ˆåˆå«è¯åµŒå…¥ï¼‰å·²ç»æˆä¸ºNLPé¢†åŸŸå„ç§ä»»åŠ¡çš„å¿…å¤‡ä¸€æ­¥ï¼Œè€Œä¸”éšç€BERTã€GPTç­‰é¢„è®­ç»ƒæ¨¡å‹çš„å‘å±•ï¼Œè¯å‘é‡æ¼”å˜ä¸ºçŸ¥è¯†è¡¨ç¤ºæ–¹æ³•ï¼Œä½†å…¶æœ¬è´¨æ€æƒ³ä¸å˜ã€‚ ç”Ÿæˆè¯å‘é‡çš„æ–¹æ³•æœ‰å¾ˆå¤šç§ï¼Œæœ¬æ–‡é‡ç‚¹å›é¡¾Word2Vecåˆ°ELMoã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/c78b6c2b3a7c40f28fa8d7784fc80bd6.png)

one-hot ç¼–ç 

> one-hot ç¼–ç ï¼Œé¦–å…ˆæ„é€ ä¸€ä¸ªå®¹é‡ä¸º N çš„è¯æ±‡è¡¨ï¼Œæ¯ä¸ªå•è¯å¯ä»¥ç”¨ä¸€ä¸ª N ç»´çš„è¯å‘é‡è¡¨ç¤ºï¼Œè¯å‘é‡ä¸­åªæœ‰å•è¯åœ¨è¯æ±‡è¡¨çš„ç´¢å¼•å¤„å–å€¼ä¸º 1ï¼Œå…¶ä½™ä¸º 0ã€‚
>   
> one-hot ç¼–ç ä¸»è¦çš„ç¼ºç‚¹æ˜¯ï¼šå½“è¯æ±‡è¡¨çš„å®¹é‡å˜å¤§æ—¶ï¼Œè¯å‘é‡çš„ç‰¹å¾ç©ºé—´ä¼šå˜å¾—å¾ˆå¤§ï¼›å¦å¤– one-hot ç¼–ç ä¸èƒ½åŒºåˆ†å•è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ŒåŒæ—¶å…·æœ‰å¼ºç¨€ç–æ€§ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b8c70ea88b624734b56c02172cfb9c3d.png)

å…±ç°çŸ©é˜µï¼ˆCo-Occurrence Matrixï¼‰

> é€šè¿‡ç»Ÿè®¡ä¸€ä¸ªäº‹å…ˆæŒ‡å®šå¤§å°çš„çª—å£å†…çš„å•è¯å…±åŒå‡ºç°çš„æ¬¡æ•°ï¼Œæ­¤æ—¶å°†è¯åˆ’åˆ†ä¸ºä¸¤ç§ï¼Œä¸­å¿ƒè¯å’Œå…¶ä»–è¯ã€‚å‡è®¾ç°åœ¨è¯­æ–™åº“ä¸­åªæœ‰ä¸‰ä¸ªå¥å­ â€œI have a catâ€ã€â€œcat eat fishâ€ã€â€œI have a appleâ€ï¼Œåˆ™å¯ä»¥æ„é€ å‡ºå•è¯é—´çš„å…±ç°çŸ©é˜µ Aã€‚ä¾‹å¦‚ â€œIâ€ å’Œ â€œhaveâ€ åœ¨ä¸¤ä¸ªå¥å­ä¸­å…±åŒå‡ºç°è¿‡ï¼Œå› æ­¤åœ¨ Aä¸­çš„æƒé‡ä¸º 2ï¼›è€Œ â€œIâ€ å’Œ â€œcatâ€œ åªåœ¨ä¸€ä¸ªå¥å­ä¸­å…±ç°ï¼Œ A ä¸­æƒé‡ä¸º 1 ã€‚
>   
> çŸ©é˜µ A çš„æ¯ä¸€è¡Œå°±ä»£è¡¨äº†ä¸€ä¸ªå•è¯çš„è¯å‘é‡ï¼Œä¸ one-hot ç¼–ç ç±»ä¼¼ï¼Œä½¿ç”¨å…±ç°çŸ©é˜µçš„è¯å‘é‡çš„ç»´åº¦ä¹Ÿéå¸¸å¤§ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ SVD (å¥‡å¼‚å€¼åˆ†è§£) å¯¹ Aè¿›è¡Œåˆ†è§£ï¼Œä»è€Œå¾—åˆ°æ›´ä½ç»´åº¦çš„è¯å‘é‡ï¼Œä½†æ˜¯ SVD ç®—æ³•çš„æ—¶é—´å¤æ‚åº¦è¾ƒé«˜ï¼Œå¯¹ nÃ—n çš„çŸ©é˜µè¿›è¡Œ SVD åˆ†è§£çš„å¤æ‚åº¦ä¸º
>
> O
> (
> n
> 3
> )
> O(n^3)
>
>
>
>
>
> O
>
> (
>
>
> n
>
>
>
>
>
>
>
>
>
> 3
>
> )
> ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/db318d39bcd04143a7ea6dce12af8ba4.png)

Word2Vec

> ã€ŠEfficient Estimation of Word Representation in Vector Spaceã€‹- Word2Vec(2013)
>   
> Word2Vecè¯å‘é‡æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ•°å€¼å‘é‡è¡¨ç¤ºå•è¯ï¼Œä¸”åœ¨å‘é‡ç©ºé—´ä¸­å¯ä»¥å¾ˆå¥½åœ°è¡¡é‡ä¸¤ä¸ªå•è¯çš„ç›¸ä¼¼æ€§ï¼Œå®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è¯çš„ä¸Šä¸‹æ–‡å¾—åˆ°è¯çš„å‘é‡åŒ–è¡¨ç¤ºã€‚
>   
> Word2Vecæ˜¯è½»é‡çº§çš„ç¥ç»ç½‘ç»œï¼Œå…¶æ¨¡å‹ä»…ä»…åŒ…æ‹¬è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ï¼Œæ¨¡å‹æ¡†æ¶æ ¹æ®è¾“å…¥è¾“å‡ºçš„ä¸åŒï¼Œä¸»è¦åŒ…æ‹¬CBOWå’ŒSkip-gramæ¨¡å‹ã€‚
>   
> ä¸¤ç§æ¨¡å‹çš„åŒºåˆ«åœ¨äº CBOW ä½¿ç”¨ä¸Šä¸‹æ–‡è¯é¢„æµ‹ä¸­å¿ƒè¯ï¼Œè€Œ Skip-Gram ä½¿ç”¨ä¸­å¿ƒè¯é¢„æµ‹å…¶ä¸Šä¸‹æ–‡å•è¯ã€‚CBOWé€‚åˆäºæ•°æ®é›†è¾ƒå°çš„æƒ…å†µï¼Œè€ŒSkip-Gramåœ¨å¤§å‹è¯­æ–™ä¸­è¡¨ç°æ›´å¥½ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1f82c8655eae480c8e9389d97d4f5b8e.png)
>   
> Simple CBOW Model
>   
> ä¸ºäº†æ›´å¥½çš„äº†è§£æ¨¡å‹æ·±å¤„çš„åŸç†ï¼Œæˆ‘ä»¬å…ˆä»Simple CBOW modelï¼ˆä»…è¾“å…¥ä¸€ä¸ªè¯ï¼Œè¾“å‡ºä¸€ä¸ªè¯ï¼‰æ¡†æ¶è¯´èµ·ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/d463068c413746f3a26dfa68ca56add8.png)
>   
> input layerè¾“å…¥çš„Xæ˜¯å•è¯çš„one-hot representationï¼ˆè€ƒè™‘ä¸€ä¸ªè¯è¡¨Vï¼Œé‡Œé¢çš„æ¯ä¸€ä¸ªè¯
>
> w
> i
> w\_i
>
>
>
>
>
>
> w
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> éƒ½æœ‰ä¸€ä¸ªç¼–å·
>
> i
> Ïµ
> {
> 1
> ,
> .
> .
> .
> ,
> âˆ£
> V
> âˆ£
> }
> i\epsilon \{1,...,|V|\}
>
>
>
>
>
> i
>
> Ïµ
>
> {
>
> 1
>
> ,
>
>
>
> ...
>
> ,
>
>
>
> âˆ£
>
> V
>
> âˆ£
>
> }
> ï¼Œé‚£ä¹ˆè¯
>
> w
> i
> w\_i
>
>
>
>
>
>
> w
>
>
>
>
>
>
>
>
>
> i
>
> â€‹
>
> çš„one-hotè¡¨ç¤ºå°±æ˜¯ä¸€ä¸ªç»´åº¦ä¸º
>
> âˆ£
> V
> âˆ£
> |V|
>
>
>
>
>
> âˆ£
>
> V
>
> âˆ£
> çš„å‘é‡ï¼Œå…¶ä¸­ç¬¬iä¸ªå…ƒç´ å€¼éé›¶ï¼Œå…¶ä½™å…ƒç´ å…¨ä¸º0ï¼Œä¾‹å¦‚ï¼š
>
> w
> 2
> =
> [
> 0
> ,
> 1
> ,
> 0
> ,
> .
> .
> .
> ,
> 0
> ]
> T
> w\_2=[0,1,0,...,0]^T
>
>
>
>
>
>
> w
>
>
>
>
>
>
>
>
>
> 2
>
> â€‹
>
>
>
>
> =
>
>
>
>
>
> [
>
> 0
>
> ,
>
>
>
> 1
>
> ,
>
>
>
> 0
>
> ,
>
>
>
> ...
>
> ,
>
>
>
> 0
>
>
> ]
>
>
>
>
>
>
>
>
>
> T
> ï¼›
>   
> è¾“å…¥å±‚åˆ°éšè—å±‚ä¹‹é—´æœ‰ä¸€ä¸ªæƒé‡çŸ©é˜µWï¼Œéšè—å±‚å¾—åˆ°çš„å€¼æ˜¯ç”±è¾“å…¥Xä¹˜ä¸Šæƒé‡çŸ©é˜µå¾—åˆ°çš„ï¼ˆç»†å¿ƒçš„äººä¼šå‘ç°ï¼Œ0-1å‘é‡ä¹˜ä¸Šä¸€ä¸ªçŸ©é˜µï¼Œå°±ç›¸å½“äºé€‰æ‹©äº†æƒé‡çŸ©é˜µçš„æŸä¸€è¡Œï¼Œå¦‚å›¾ï¼šè¾“å…¥çš„å‘é‡Xæ˜¯[0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ0]ï¼ŒWçš„è½¬ç½®ä¹˜ä¸ŠXå°±ç›¸å½“äºä»çŸ©é˜µä¸­é€‰æ‹©ç¬¬3è¡Œ[2,1,3]ä½œä¸ºéšè—å±‚çš„å€¼ï¼‰;
>   
> éšè—å±‚åˆ°è¾“å‡ºå±‚ä¹Ÿæœ‰ä¸€ä¸ªæƒé‡çŸ©é˜µWâ€™ï¼Œå› æ­¤ï¼Œè¾“å‡ºå±‚å‘é‡yçš„æ¯ä¸€ä¸ªå€¼ï¼Œå…¶å®å°±æ˜¯éšè—å±‚çš„å‘é‡ç‚¹ä¹˜æƒé‡å‘é‡Wâ€™çš„æ¯ä¸€åˆ—ï¼Œæ¯”å¦‚è¾“å‡ºå±‚çš„ç¬¬ä¸€ä¸ªæ•°7ï¼Œå°±æ˜¯å‘é‡[2,1,3]å’Œåˆ—å‘é‡[1,2,1]ç‚¹ä¹˜ä¹‹åçš„ç»“æœï¼›
>   
> æœ€ç»ˆçš„è¾“å‡ºéœ€è¦ç»è¿‡softmaxå‡½æ•°ï¼Œå°†è¾“å‡ºå‘é‡ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ å½’ä¸€åŒ–åˆ°0-1ä¹‹é—´çš„æ¦‚ç‡ï¼Œæ¦‚ç‡æœ€å¤§çš„ï¼Œå°±æ˜¯é¢„æµ‹çš„è¯ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/3659c3f4f1304c0cb0450b19a03772c6.png)
>   
> è¾“å‡ºå±‚é€šè¿‡softmaxå½’ä¸€åŒ–ï¼Œuä»£è¡¨çš„æ˜¯è¾“å‡ºå±‚çš„åŸå§‹ç»“æœã€‚é€šè¿‡ä¸‹é¢å…¬å¼ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°å¯ä»¥è½¬åŒ–ä¸ºç°åœ¨è¿™ä¸ªå½¢å¼
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/ab6c7e031a6a4a21843aacd65967b91b.png)
>   
> CBOW Multi-Word Context Model
>   
> äº†è§£äº†Simple CBOW modelä¹‹åï¼Œæ‰©å±•åˆ°CBOWå°±å¾ˆå®¹æ˜“äº†ï¼Œåªæ˜¯æŠŠå•ä¸ªè¾“å…¥æ¢æˆå¤šä¸ªè¾“å…¥ç½¢äº†ï¼ˆåˆ’çº¢çº¿éƒ¨åˆ†ï¼‰ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/2efb02b3ef84458888aee1410470b292.png)
>   
> å¯¹æ¯”å¯ä»¥å‘ç°ï¼Œå’Œsimple CBOWä¸åŒä¹‹å¤„åœ¨äºï¼Œè¾“å…¥ç”±1ä¸ªè¯å˜æˆäº†Cä¸ªè¯ï¼Œæ¯ä¸ªè¾“å…¥X\_{ik}åˆ°è¾¾éšè—å±‚éƒ½ä¼šç»è¿‡ç›¸åŒçš„æƒé‡çŸ©é˜µWï¼Œéšè—å±‚hçš„å€¼å˜æˆäº†å¤šä¸ªè¯ä¹˜ä¸Šæƒé‡çŸ©é˜µä¹‹ååŠ å’Œæ±‚å¹³å‡å€¼ã€‚
>   
> Skip-gram
>   
> æœ‰äº†CBOWçš„ä»‹ç»ï¼Œå¯¹äºSkip-gram model çš„ç†è§£åº”è¯¥ä¼šæ›´å¿«ä¸€äº›ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/01d5243d7fa4456689f8901190564e6e.png)
>   
> å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒSkip-gram modelæ˜¯é€šè¿‡è¾“å…¥ä¸€ä¸ªè¯å»é¢„æµ‹å¤šä¸ªè¯çš„æ¦‚ç‡ã€‚è¾“å…¥å±‚åˆ°éšè—å±‚çš„åŸç†å’Œsimple CBOWä¸€æ ·ï¼Œä¸åŒçš„æ˜¯éšè—å±‚åˆ°è¾“å‡ºå±‚ï¼ŒæŸå¤±å‡½æ•°å˜æˆäº†Cä¸ªè¯æŸå¤±å‡½æ•°çš„æ€»å’Œï¼Œæƒé‡çŸ©é˜µWâ€™è¿˜æ˜¯å…±äº«çš„ã€‚

GloVe

> ã€ŠGloVe : Global Vectors forWord Representationã€‹å…¨å±€å‘é‡çš„è¯åµŒå…¥ï¼Œé€šå¸¸ç®€ç§°ä¸ºGloVeï¼Œæ˜¯ä¸€ç§ç”¨äºå°†è¯è¯­æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´çš„è¯åµŒå…¥æ–¹æ³•ã€‚å®ƒæ—¨åœ¨æ•æ‰è¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»å’Œè¯­æ³•å…³ç³»ï¼Œä»¥ä¾¿åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­èƒ½å¤Ÿæ›´å¥½åœ°è¡¨ç¤ºè¯è¯­çš„è¯­ä¹‰ä¿¡æ¯ã€‚
>   
> å¸¸è§çš„è¯åµŒå…¥ç®—æ³•æœ‰åŸºäºçŸ©é˜µåˆ†è§£çš„æ–¹æ³•å’ŒåŸºäºæµ…å±‚çª—å£çš„æ–¹æ³•ï¼ŒGlove ç»“åˆäº†è¿™ä¸¤ç±»æ–¹æ³•çš„ä¼˜ç‚¹ç”Ÿæˆè¯å‘é‡ã€‚åŸºäºçŸ©é˜µåˆ†è§£çš„æ–¹æ³•å¯ä»¥æœ‰æ•ˆåœ°åˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œä½†æ˜¯åœ¨å¤§æ•°æ®é›†ä¸Šé€Ÿåº¦æ…¢ï¼›è€ŒåŸºäºæµ…å±‚çª—å£çš„æ–¹æ³•å¯¹äºè¯æ±‡ç±»æ¯”ä»»åŠ¡æ•ˆæœè¾ƒå¥½ï¼Œè®­ç»ƒé€Ÿåº¦å¿«ï¼Œä½†æ˜¯ä¸èƒ½æœ‰æ•ˆåˆ©ç”¨å…¨å±€ä¿¡æ¯ã€‚
>   
> è¯åµŒå…¥ç®—æ³• Gloveï¼Œç»“åˆä¸¤ç±»è¯åµŒå…¥ç®—æ³•çš„ä¼˜ç‚¹ã€‚
>   
> ç¬¬ä¸€ç±»æ˜¯ Matrix Factorization (çŸ©é˜µåˆ†è§£) æ–¹æ³•ï¼Œä¾‹å¦‚ LSAï¼›
>   
> ç¬¬äºŒç±»æ˜¯ Shallow Window-Based (åŸºäºæµ…å±‚çª—å£) æ–¹æ³•ï¼Œä¹Ÿç§°ä¸ºåŸºäºé¢„æµ‹çš„æ–¹æ³•ï¼Œä¾‹å¦‚ Word2Vecã€‚
>   
> GloVeæ¨¡å‹å°† LSA å’Œ Word2Vec çš„ä¼˜ç‚¹ç»“åˆåœ¨ä¸€èµ·ï¼Œæ—¢åˆ©ç”¨äº†è¯­æ–™åº“çš„å…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼Œä¹Ÿåˆ©ç”¨äº†å±€éƒ¨çš„ä¸Šä¸‹æ–‡ç‰¹å¾ (æ»‘åŠ¨çª—å£)ã€‚Glove é¦–å…ˆéœ€è¦æ„é€ å•è¯å…±ç°çŸ©é˜µï¼Œå¹¶æå‡ºäº†å…±ç°æ¦‚ç‡çŸ©é˜µ (Co-occurrence Probabilities Matrix)çš„æ¦‚å¿µï¼Œå…±ç°æ¦‚ç‡çŸ©é˜µå¯ä»¥é€šè¿‡å•è¯å…±ç°çŸ©é˜µè®¡ç®—ã€‚

ELMo

> ELMoæ¥è‡ªäºè®ºæ–‡ã€ŠDeep contextualized word representationsã€‹(2018)
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/1a865ea56d264f3b99b422375636cc6f.webp#pic_center)
>   
> word2vecå’Œgloveè¿™ä¸¤ç§ç®—æ³•éƒ½æ˜¯é™æ€è¯å‘é‡ç®—æ³•ï¼Œå®ƒä»¬éƒ½å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œè¯åœ¨ä¸åŒçš„è¯­å¢ƒä¸‹å…¶å®æœ‰ä¸åŒçš„å«ä¹‰ï¼Œè€Œè¿™ä¸¤ä¸ªæ¨¡å‹è¯åœ¨ä¸åŒè¯­å¢ƒä¸‹çš„å‘é‡è¡¨ç¤ºæ˜¯ç›¸åŒçš„ï¼›
>   
> ELMoæ˜¯ä¸€ç§åŠ¨æ€è¯å‘é‡ç®—æ³•ï¼Œå°±æ˜¯é’ˆå¯¹è¿™ä¸€ç‚¹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œæ¥è‡ªäºè¯­è¨€æ¨¡å‹çš„è¯å‘é‡è¡¨ç¤ºï¼Œä¹Ÿæ˜¯åˆ©ç”¨äº†æ·±åº¦ä¸Šä¸‹æ–‡å•è¯è¡¨å¾ï¼Œè¯¥æ¨¡å‹çš„ä¼˜åŠ¿ï¼š ï¼ˆ1ï¼‰èƒ½å¤Ÿå¤„ç†å•è¯ç”¨æ³•ä¸­çš„å¤æ‚ç‰¹æ€§ï¼ˆæ¯”å¦‚å¥æ³•å’Œè¯­ä¹‰ï¼‰ ï¼ˆ2ï¼‰è¿™äº›ç”¨æ³•åœ¨ä¸åŒçš„è¯­è¨€ä¸Šä¸‹æ–‡ä¸­å¦‚ä½•å˜åŒ–ï¼ˆæ¯”å¦‚ä¸ºè¯çš„å¤šä¹‰æ€§å»ºæ¨¡ï¼‰
>   
> é’ˆå¯¹ç‚¹1ï¼Œä½œè€…æ˜¯é€šè¿‡å¤šå±‚çš„stack LSTMå»å­¦ä¹ è¯çš„å¤æ‚ç”¨æ³•ï¼Œè®ºæ–‡ä¸­çš„å®éªŒéªŒè¯äº†ä½œè€…çš„æƒ³æ³•ï¼Œä¸åŒå±‚çš„outputå¯ä»¥è·å¾—ä¸åŒå±‚æ¬¡çš„è¯æ³•ç‰¹å¾ã€‚å¯¹äºè¯ä¹‰æ¶ˆæ­§æœ‰éœ€æ±‚çš„ä»»åŠ¡ï¼Œç¬¬2å±‚ä¼šæœ‰è¾ƒå¤§çš„æƒé‡ï¼Œå¯¹äºè¯æ€§ã€å¥æ³•æœ‰éœ€æ±‚çš„ä»»åŠ¡ï¼Œå¯¹ç¬¬1å±‚ä¼šæœ‰æ¯”è¾ƒå¤§çš„æƒé‡ã€‚
>   
> é’ˆå¯¹ç‚¹2ï¼Œä½œè€…é€šè¿‡pre-train+fine tuningçš„æ–¹å¼å®ç°ï¼Œå…ˆåœ¨å¤§è¯­æ–™åº“ä¸Šè¿›è¡Œpre-trainï¼Œå†åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„è¯­æ–™åº“ä¸Šè¿›è¡Œfine tuningã€‚
>   
> **ELMoæ¨¡å‹æœ‰ä¸‰ä¸ªä¼˜ç‚¹ï¼š**
>   
> ELMoå…·æœ‰å¤„ç†ä¸€è¯å¤šä¹‰çš„èƒ½åŠ›ã€‚å› ä¸ºELMoä¸­æ¯ä¸ªå•è¯çš„åµŒå…¥å¹¶ä¸æ˜¯å›ºå®šçš„ï¼Œåœ¨å°†è¿™ä¸ªå•è¯çš„è¯åµŒå…¥è¾“å…¥åŒå‘LSTMä¹‹åï¼Œå®ƒçš„å€¼ä¼šéšç€ä¸Šä¸‹æ–‡å†…å®¹çš„ä¸åŒè€Œæ”¹å˜ï¼Œä»è€Œå­¦åˆ°äº†å’Œä¸Šä¸‹æ–‡ç›¸å…³çš„è¯åµŒå…¥ã€‚
>   
> ELMoå…·æœ‰ä¸åŒå±‚æ¬¡çš„è¡¨å¾èƒ½åŠ›ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œå¯¹äºä¸€ä¸ªå¤šå±‚çš„ç½‘ç»œæ¥è¯´ï¼Œä¸åŒçš„æ·±åº¦å…·æœ‰ä¸åŒçš„è¡¨å¾èƒ½åŠ›ï¼Œè¶Šæ¥è¿‘è¾“å…¥å±‚çš„ç½‘ç»œå±‚å­¦åˆ°çš„ç‰¹å¾è¶Šæ¥è¿‘è¾“å…¥çš„åŸå§‹ç‰¹å¾ï¼Œè€Œè¶Šæ¥è¿‘ç½‘ç»œè¾“å‡ºå±‚çš„ç½‘ç»œå±‚å­¦åˆ°çš„ç‰¹å¾åˆ™å…·æœ‰å¾ˆå¥½çš„è¯­ä¹‰è¡¨å¾èƒ½åŠ›ã€‚ELMoä½¿ç”¨äº†å¯¹å¤šå±‚LSTMçš„è¾“å‡ºè¿›è¡Œè‡ªé€‚åº”åŠ æƒçš„ç»“æ„ï¼ˆattentionï¼‰ï¼Œä½¿ç”¨å…¶å¯ä»¥æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡è‡ªé€‚åº”è°ƒæ•´ELMoçš„è¾“å‡ºï¼Œè®©å…¶ä¸ä¸‹æ¸¸ä»»åŠ¡ç›¸é€‚åº”ã€‚
>   
> ELMoå…·æœ‰éå¸¸å¼ºå¤§çš„çµæ´»æ€§ï¼šé™¤äº†ELMoæœ¬èº«çš„è¾“å…¥å¯ä»¥æ ¹æ®è°ƒæ•´å¤–ï¼ŒELMoè¿˜å¯ä»¥ä»¥å„ç§å½¢å¼å’Œä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œç»“åˆã€‚é€šè¿‡ELMoå¾—åˆ°çš„ä»…æ˜¯å½“å‰æ—¶é—´ç‰‡çš„è¾“å…¥çš„ç¼–ç ç»“æœï¼Œå› æ­¤å¯ä»¥åŠ å…¥åˆ°è¾“å…¥å±‚ï¼Œéšå±‚ï¼Œè¾“å‡ºå±‚ä¸­ã€‚
>   
> ELMoæ˜¯æœ€æ—©çš„ä¸€æ‰¹å°†æ·±åº¦å­¦ä¹ åº”ç”¨åˆ°è¯å‘é‡å­¦ä¹ çš„ä»»åŠ¡ä¸­ï¼Œå®ƒçš„æ€æƒ³å¯¹åç»­çš„BERTç­‰äº§ç”Ÿäº†å·¨å¤§çš„å½±å“ã€‚

#### å‚è€ƒï¼š

[NNLMâ€”â€”é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯](https://blog.csdn.net/weixin_62472350/article/details/143448417)
  
[Neural Network Language Model PyTorchå®ç°](https://www.bilibili.com/video/BV1AT4y1J7bv/)
  
[NNLM çš„ PyTorch å®ç°](https://wmathor.com/index.php/archives/1442/)

[BPç¥ç»ç½‘ç»œï¼šè¯¯å·®åå‘ä¼ æ’­ç®—æ³•å…¬å¼æ¨å¯¼å›¾è§£](https://www.cnblogs.com/tsingke/p/14826896.html)
  
[RNNå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜](https://zhuanlan.zhihu.com/p/689910526)

å‚è€ƒï¼š
[ä¸€ç¯‡æ–‡ç« å…¥é—¨Word2Vec](https://blog.csdn.net/julac/article/details/127767053)
  
[è¯å‘é‡æ¨¡å‹word2vectorè¯¦è§£](https://www.cnblogs.com/stephen-goodboy/p/12574738.html)

å‚è€ƒï¼š
[ã€å¤§æ¨¡å‹ç³»åˆ—ç¯‡ã€‘è¯å‘é‡ - ä»Word2Vecåˆ°ELMo](https://blog.csdn.net/Jackie_vip/article/details/141600366)