---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f546f6b795f6d696e2f:61727469636c652f64657461696c732f313436303238313434"
layout: post
title: "文献阅读Efficient-Prompting-Methods-for-Large-Language-Models-A-Survey"
date: 2025-03-05 20:28:46 +08:00
description: "从本质上讲，摘要是一种语义级别的压缩，可能会改变语言表达，但会保留原始含义。为确保压缩Prompt的大语言模型性能与原始Prompt没有显著偏差，以下将介绍有训练和无训练的两种方法。第一种方法通常将原始输出作为监督信号来训练摘要器。在RAG场景中，RECOMP（Retrieve, Compress, Prepend）[106] 在上下文集成之前，将检索到的文档压缩为文本摘要。有两个以查询为重点的压缩器对多文档进行摘要，以提高大语言模型的性能。"
keywords: "【文献阅读】Efficient Prompting Methods for Large Language Models: A Survey"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "146028144"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146028144
    alt: "文献阅读Efficient-Prompting-Methods-for-Large-Language-Models-A-Survey"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146028144
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146028144
cover: https://bing.ee123.net/img/rand?artid=146028144
image: https://bing.ee123.net/img/rand?artid=146028144
img: https://bing.ee123.net/img/rand?artid=146028144
---

# 【文献阅读】Efficient Prompting Methods for Large Language Models: A Survey

arXiv:2404.01077v2 [cs.CL] 2 Dec 2024

Prompt是一种主流范式，用于在
**不修改内部参数**
的情况下，使大语言模型适应特定的自然语言处理任务。因此，需要将
**详细的
补充知识**
整合到
**外部Prompt**
中，这在实际应用中不可避免地会带来额外的人力和计算负担。作为一种
**减轻资源消耗**
的有效解决方案，高效Prompt方法受到了广泛关注。我们从
**高层次提供数学表达式**
，深入讨论针对不同Prompt组件的自动Prompt工程，以及在连续和离散空间中的Prompt压缩。最后，我们强调了有前景的未来研究方向，以启发对该领域感兴趣的研究人员。
  
CCS概念：
  
- 通用与参考→综述与概述；
  
- 计算方法→自然语言处理。
  
其他关键词和短语：大语言模型；高效Prompt方法；自动Prompt工程；Prompt压缩

## 1 引言

随着大语言模型（LLMs）在参数规模上取得数千亿的突破，它们获得了涌现能力[103]，尤其是上下文学习能力[6]，这推动了Prompt技术的快速发展。Prompt作为一种
**轻量级且前景广阔的解决方案**
，可在不调整参数的情况下控制大语言模型，在自然语言处理（NLP）领域受到了广泛关注。Prompt有两种类型，其中
**硬Prompt**
是离散的自然语言描述，而
**软Prompt**
是连续的向量表示。特别地，硬Prompt凭借其相较于软Prompt更高的可解释性、可控性和灵活性，成为了人机交互的关键桥梁。因此，我们主要关注硬Prompt，它涵盖了大语言模型输入的所有组件，从
**简洁的指令到带有示例的长上下文（思维链（CoT）[104]、角色扮演系统Prompt等）**
。

目前，通过Prompt来释放大语言模型在特定领域的潜力已很常见。例如，CoT系列研究[5, 13, 111]通过逐步思考，逐步增强了大语言模型的推理能力。此外，OpenAI最近推出了经过强化学习训练的推理大语言模型
**o1**
[62, 63]，它可以分解更复杂的问题，并在回答前生成较长的内部思维链。

这些方法的出色表现推动了对优化Prompt方法的研究不断增加，这在自然语言处理领域逐渐形成了一个全新的领域。与此同时，与应用效率相关的一些挑战也接踵而至：越来越复杂的Prompt设计使得手动优化Prompt既耗时又费力；越来越详细的Prompt内容在应用于大规模模型时，不可避免地会消耗大量计算资源。如此高昂的开销已成为大语言模型实际部署的主要障碍，因此在本文中，我们将“高效Prompt方法”定义为使用更少的人力或计算资源，使语言模型实现可比甚至更好性能的Prompt方法。

![](https://i-blog.csdnimg.cn/direct/19e7f455fd0a465eb58a739112091692.png)

我们将本综述聚焦于大语言模型时代的
**高效Prompt方法**
。据我们所知，这是首次从“高效”的角度总结大语言模型Prompt方法的综述。值得注意的是，我们在2.3节从数学角度对各类资源节约方法的核心概念进行了建模。

随后，我们提出了一种新颖的分类法，如图1所示，以便基于一致的优化策略全面回顾高效Prompt方法。为避免人力资源的浪费，我们在第3节介绍了基于大语言模型赋能的自动Prompt工程方法，包括对不同Prompt组件的迭代设计和优化。

为节省计算资源，我们在第4节根据Prompt类型将Prompt压缩方法分为两类：
**连续空间中的文本到向量（T2V）压缩和离散空间中的文本到文本（T2T）压缩**
。此外，我们提供了足够的示意图来描述每类方法的基本流程，以及展示同类方法细节差异的表格。

附录A.1中还列出了开源项目，方便自然语言处理从业者在科研和商业部署中高效地对大语言模型进行Prompt。最后，我们在第5节分析了现有方法的挑战，并讨论了有前景的未来研究方向。我们希望本综述能清晰呈现高效Prompt这一主题，并为
**通用人工智能（AGI）**
发展过程中的便捷人机交互做出贡献。

## 2 预备知识

### 2.1 Prompt的起源

在自然语言处理领域，从监督学习到微调再到Prompt，已经发生了两次重大的范式转变，这里的“范式”指的是训练语言模型（LM）并将其应用于特定下游任务的方式。目前，Prompt已成为大语言模型的主导范式。下面，我们在语言模型发展的背景下引出Prompt的起源。

随着基于Transformer[94]架构的预训练语言模型（PLMs）的兴起，第一次重大转变是从
**单阶段（完全监督学习）转变为两阶段过程（预训练 + 微调）**
。我们迎来了
**预训练语言模型时代**
，避免了从头开始训练的计算成本。第一阶段是使用大量无标签数据预训练语言模型，第二阶段是使用少量有标签数据对预训练语言模型进行微调，以适应特定任务。

具有1750亿参数的著名生成式预训练语言模型GPT-3[6]展现出了令人印象深刻的上下文学习能力，能够从少量示例中生成预期的响应，这引发了第二次重大转变（预训练、Prompt和预测）。通过适当的Prompt引导单个通用模型执行多个任务成为可能，而无需更新参数，从而节省了为每个任务微调单独模型所需的大量计算成本。结果，语言模型的参数规模不断扩大到数十亿级别，开启了大语言模型时代，为Prompt的进一步发展奠定了坚实基础。总之，Prompt主导了人机交互方式，具有开启通用人工智能大门的巨大潜力。

### 2.2 Prompt类型

Prompt作为描述人类意图的模型输入，可以是文本、图像、音频等多种模态，其质量直接决定了特定任务的准确性。本文仅讨论自然语言处理领域中文本形式的Prompt，即“
**硬Prompt”**
。当文本被神经网络处理为向量时，向量形式的Prompt也被称为“软Prompt”。考虑到Prompt类型与模型架构之间的密切关系，以下将分别从预训练语言模型和大语言模型时代的角度进行介绍。

#### 2.2.1 硬Prompt

- 预训练语言模型时代。基于Transformer的预训练语言模型根据其架构可分为三类：仅编码器、仅解码器和编码器 - 解码器。每个模型适用的输入格式在很大程度上取决于预训练任务的选择，因此针对特定架构设计了不同的Prompt形式。例如，通过掩码语言建模（MLM）任务预训练的仅编码器模型[22, 43, 56]擅长预测Prompt模板中的[MASK]标记，如表1 - a所示。而通过自回归语言建模任务预训练的仅解码器模型[6, 76, 77]则擅长从左到右生成前缀Prompt后的下一个标记，如表1 - b所示。
  
- 大语言模型时代。目前，大多数大语言模型采用仅解码器架构，并通过指令微调使其符合人类偏好，因此它们可以根据自然语言Prompt生成所需的响应。此外，随着大语言模型在各种通用任务中表现出色，Prompt的设计也变得更加详细和全面。Prompt的基本组件可以总结为四个部分，如表1 - c所示。

![](https://i-blog.csdnimg.cn/direct/5d0662f9f9974f73880c2f38cc606535.png)

表1. 硬Prompt的三种形式，其中绿色代表硬Prompt，突出显示的部分代表思维链。(a) 完形填空Prompt通常是一个带有待填充空格的Prompt模板。(b)
**前缀Prompt**
使用简洁的指令和输入 - 输出示例，在预训练期间激活大语言模型的记忆。(c) 详细描述包含特定任务的指令、上下文、输入和输出指示，能更好地利用大语言模型在各种下游任务中的涌现能力。

#### 2.2.2 软Prompt

- 预训练语言模型时代。早期的工作研究了使用软Prompt进行参数高效微调，包括Prompt调整[44]、前缀调整[49]等。如表2 - a和b所示，这些方法冻结原始参数，仅微调少量额外的软Prompt，以实现与全参数微调相当的性能。随着模型规模的增加，这种现象更加明显。然而，随着大语言模型时代的到来，许多大语言模型是闭源的，参数无法访问，导致软Prompt的后续研究相对停滞。
  
- 大语言模型时代。在本综述中，我们也将语言模型内部硬Prompt的向量表示视为软Prompt。软Prompt仍然至关重要的原因是，它们可以与语言模型一起训练，以实现有效的模型调整。大语言模型时代的软Prompt（如表2 - c所示）通常比硬Prompt更紧凑，能够以更少的空间存储硬Prompt中的有效信息，从而加速推理。

![](https://i-blog.csdnimg.cn/direct/314f7dacc56c44469c64c36e1269c5f1.png)

表2. 软Prompt的
**三种形式**
，其中圆形代表连续向量，方形代表离散标记，橙色代表
**软Prompt**
，灰色代表
**冻结参数**
，绿色代表
**硬Prompt**
。(a) 嵌入指添加到Prompt模板中的可训练参数。(b) 前缀被添加到神经网络的每一层，以指示下游任务类型。(c) 紧凑向量是由开源大语言模型压缩的硬Prompt。

### 2.3 数学建模

为了充分发挥大语言模型在各种研究和应用中的广泛优势，我们从减少实际应用中资源消耗的角度回顾高效Prompt方法。我们将主要资源分为两类：人力资源和计算资源。我们的分析表明，尽管每类优化目标有许多共性，但它们之间缺乏联系。

为了弥合这一差距，我们尝试将各类高效Prompt方法的优化过程建模为数学公式，以揭示它们作为本综述理论基础的作用。通过这种方式，初学者不仅可以一目了然地了解高效Prompt方法的本质，研究人员也可以受到启发，合理地将这两种策略整合为双赢的解决方案，同时减少两个领域的资源消耗。

- 自动Prompt工程（第3节）：优化对象主要针对Prompt的指令\(P\_{ins}\)和示例\(p\_{dem}\)。优化目标是在离散Prompt空间\(P\_{Hard}\)中搜索最优的自然语言Prompt\(p^{}\)，以使目标模型\(M\)的性能最大化：
  
\[\large p^{}=\underset{p \in P\_{Hard }}{arg max } \mathcal{P}\left[f\left(p\_{ins }, p\_{dem } ; M\right)\right]\]

其中\(f(\cdot ; M)\)表示目标模型的输出，\(\mathcal{P}\)表示通过将输出与真实标签进行比较的评估指标来衡量的性能，例如自然语言理解或数学推理任务中的准确率。

- Prompt压缩（第4节）：有两种Prompt压缩方法在不同的优化空间中执行。连续空间中的文本到向量（T2V）压缩通过微调大语言模型，将长文本\(p\_{o}^{h}\)（原始硬Prompt）压缩为短向量\(p\_{c}^{s}\)（压缩后的软Prompt）。离散空间中的文本到文本（T2T）压缩通过过滤冗余信息或进行总结，直到信息足够，将长文本\(p\_{o}^{h}\)压缩为短文本\(p\_{c}^{h}\)（压缩后的硬Prompt）。信息性\(I(p)\)（例如自信息[50]、困惑度[34, 35]等）可以由小语言模型（SLM）或大语言模型来衡量。优化目标是最小化Prompt压缩前后模型输出分布的差异：
  
\[\large p\_{c}^{s }=\underset{p^{s} \sim\left\{\theta\_{M}, \theta\_{soft}\right\}}{arg min } \mathcal{D}\left[q\left(y\_{M} | p\_{o}^{h}\right) \| q\left(y\_{M} | p\_{c}^{s}\right)\right]\]
  
\[\large p\_{c}^{h }=\underset{I\left(p^{h}\right) \geq \lambda}{arg min } \mathcal{D}\left[q\left(y\_{M} | p\_{o}^{h}\right) \| q\left(y\_{M} | p\_{c}^{h}\right)\right]\]
  
其中\(\theta\)表示可训练参数，\(\lambda\)表示信息性的最小阈值，\(q(y\_{M} | \cdot)\)表示目标模型\(M\)的输出分布，\(\mathcal{D}\)表示输出分布之间的距离度量，例如KL散度。

基于上述理论基础，我们在以下内容中详细介绍
**两类代表性的高效Prompt方法**
，这将进一步加深对这些数学公式的理解。图2展示了本综述的概述。左侧表示自动Prompt工程（第3节）通过元Prompt利用大语言模型强大的生成能力，迭代开发高质量指令（第3.1节）并优化思维链Prompt框架（第3.2节）以获得更好的性能，从而替代了人类Prompt工程师的重复性劳动。右侧讨论了在连续和离散空间中的Prompt压缩（第4节），以缩短Prompt长度，获得更多输入空间，并节省计算资源以加速推理。T2V压缩（第4.1节）通过训练开源大语言模型，利用向量缓存硬Prompt的关键信息。T2T压缩（第4.2节）仅保留硬Prompt中的有效信息，而不会降低性能。

![](https://i-blog.csdnimg.cn/direct/62ef610fd851496d968fc66034461314.png)

图2. 高效Prompt方法概述。自动Prompt工程（第3节）利用大语言模型的强大生成能力，通过元Prompt迭代开发
**高质量指令**
（第3.1节）并优化
**思维链Prompt框架**
（第3.2节），以替代人类Prompt工程师的重复性劳动，减少人力资源消耗。Prompt压缩（第4节）在连续和离散空间中减少Prompt长度，节省计算资源以加速推理。T2V压缩（第4.1节）利用向量缓存硬Prompt的关键信息，T2T压缩（第4.2节）仅保留硬Prompt中的有效信息，而不会降低性能。

## 3 自动Prompt工程

Prompt工程是指设计有效的Prompt，以便在复杂场景中更好地利用特定语言模型。

挑战：手动设计精心的Prompt，使语言模型充分理解人类意图并模仿人类行为，这直观上是Prompt工程的目标。然而，由于早期语言模型的理解能力有限，Prompt通常是简洁的指令，手动改进的空间很小。

后来，随着语言模型在通用能力上取得革命性突破，人们开始频繁使用更详细的自然语言Prompt与大语言模型进行交互。大量实验发现，大语言模型Prompt的上限变得模糊，因此人类专业知识无法快速准确地应对新任务。

我们总结了手动设计大语言模型Prompt面临的几个挑战：(1) 敏感性：特别是在零样本设置中，Prompt内容的细微差异可能导致模型性能的显著差异[36, 82, 124]，因此不同的模型和任务需要定制Prompt。(2) 次优性：尽管精心设计的Prompt能有效提高任务准确率，但大语言模型与特定Prompt的内部兼容性仍不确定。缺乏优化方向意味着人类工程师只能依靠试错来探索更广泛的Prompt空间，以寻找相对最优的解决方案。(3) 差异：大语言模型对自然语言的解释可能与人类不同[21]，有时语法和句法不一致的无意义Prompt可能更有效，这显然超出了人类设计Prompt的范围。总之，在大语言模型时代，设计自然语言Prompt是一门艺术，需要大量的时间和经验，这不可避免地过度消耗了人力资源。

解决方案：大语言模型可能确切知道它们想要什么样的Prompt，因此人们提出了多种由大语言模型驱动的Prompt优化方法，以减少人类偏见，并自适应地搜索高质量Prompt，通过公式1最大化大语言模型的性能。这一系列工作统称为自动Prompt工程，其本质是模仿离散空间中的搜索算法，由三个迭代步骤组成，如图3所示。

![](https://i-blog.csdnimg.cn/direct/0792b8822f8f4656a75891fd32d269a9.png)

图3. 自动Prompt工程的基本流程。步骤1：根据定制的优化方向扩展离散Prompt空间。步骤2：根据目标模型性能合理评估候选Prompt。步骤3：使用适当的采样策略从Prompt池中选择最优Prompt。

根据Prompt的不同组件，自动Prompt工程分为两部分：第3.1节的指令设计和第3.2节的思维链优化。这两种方法主要采用搜索算法原理来确定最优解，元Prompt在引导大语言模型（称为优化器）进行生成、评估和选择方面起着至关重要的作用，而不是由人类工程师来完成，从而以简单高效的方式实现Prompt设计的自动化。它们的区别在于，前者侧重于扩展指令空间，而后者更倾向于构建全面的问题解决框架。

### 3.1 指令设计

指令作为人机交互最直接的方式，是上下文学习的首选。通常，指令是简洁明了的任务描述，用于引导大语言模型执行特定任务，类似于激活特定领域的某些知识。作为Prompt的核心组件，指令设计一直是自动Prompt工程领域众多讨论的热门话题。模仿搜索算法的前提是创建足够的搜索空间，这对人类工程师来说可能是一个巨大的挑战。然而，对于在预训练中积累了广泛语言知识，并在监督微调中掌握了出色生成能力的大语言模型来说，这轻而易举，其创造力甚至优于人类。下面，我们将从三个角度梳理现有工作。

#### 3.1.1 基于采样的方法

Self-instruct[101]表明，大语言模型可以从
**少量人类编写的种子指令**
中自动生成适用的指令。为了充分利用大语言模型在构建多样化Prompt空间方面的生成能力，最直接的方法是在推理过程中进行
**迭代采样**
。但是大语言模型如何生成指令呢？它可以根据Prompt中的输入 - 输出对匹配最合适的指令，或者模仿精心设计的Prompt。随后，元Prompt引导大语言模型执行自动Prompt工程中的每一步。

APE（自动Prompt工程师）[126]是一项开创性的工作，它将人类难以处理的问题视为由大语言模型引导的黑盒优化过程。所有Prompt优化过程都可以由一个符合人类偏好的指令生成大语言模型来执行。大语言模型分别充当不同的Prompt工程师来优化Prompt中的指令。

步骤1：推理大语言模型根据示例（输入 - 输出对）生成指令。步骤2：评分大语言模型通过预测下一个标记的概率来评估这些候选指令，并选择得分超过特定阈值的高质量指令。步骤3（可选）：重采样大语言模型围绕最佳候选指令进行蒙特卡罗搜索，以寻找语义相似的指令。

这样的优化流程相当于将示例压缩为单个指令，在零样本/少样本学习和零样本思维链设置中，大大优于先前的大语言模型基线。此外，APE在24个指令归纳任务和21个精选的Big - Bench[90]任务中的17个上，实现了与人类注释者生成的指令相当或更好的性能。然而，Prompt优化方法在数据分布变化时面临显著的性能差距挑战。GPO框架[47]首先利用元Prompt让大语言模型为单个无标签输入生成多个相应的输出。然后，Prompt集成标记策略选择一致性最高的输出作为标签。最后，混合不同分布的示例来运行APE进行联合Prompt优化，从而获得具有高鲁棒性和泛化性的最终优化Prompt。

基于探索与利用的概念，OPRO[109]使用元Prompt分别指示大语言模型作为优化器来探索大量指令，并作为评分器来评估它们的分数。然后，指令与其分数迭代形成优化轨迹，以指导优化器探索新的指令，从而提高任务准确率。由于上述方法缺乏人类试错过程和更深入的领域专业知识，PromptAgent[99]基于大语言模型的自我反思能力，通过蒙特卡罗树搜索（MCTS）自动设计富含领域特定细节和结构化指导的专家Prompt。它以树结构系统地扩展Prompt空间，并选择最优路径中具有最高奖励的最佳节点。受专家混合（MoE）的启发，混合Prompt（MoP）[97]通过对不同专家的示例进行聚类来扩展指令优化空间，然后通过基于区域的联合搜索（RBJS）算法为每个聚类确定最佳指令。

GPS[107]验证了基于进化算法（EA）自动搜索高性能Prompt的可行性。进化算法是一类高度鲁棒且广泛适用的优化算法，它模拟自然选择的 “适者生存” 原理，包括遗传算法（GA）、差分进化（DE）等。在Prompt优化领域，一些工作遵循进化算法的概念来增强离散Prompt空间的多样性。

大语言模型作为进化算子，在元Prompt的指导下执行四个关键操作。步骤1：使用适当的种子初始化Prompt种群。步骤2：通过选择、变异和交叉将初始Prompt进化为候选Prompt。步骤3：根据适当的指标评估候选Prompt。步骤4：用最适合的样本更新种群。不同方法的种群初始化可能不同，但迭代进化过程基本相同。

EvoPrompting[8]首次引入大语言模型作为进化算子，避免了手动设计离散搜索空间带来的资源浪费和人类偏见。经过代码预训练的大语言模型用于执行神经架构搜索（NAS）任务的端到端元学习算法。通过少数精心设计的程序种子初始化父样本，以启动迭代。

EvoPrompting在模型性能和计算效率方面均优于手动设计和简单的少样本Prompt方法。EvoPrompt[28]是首次尝试通过自然语言描述将大语言模型与进化算法相结合，在离散Prompt优化中实现了探索与利用的平衡。初始种群既包括利用人类先验知识的手动Prompt，也包括通过轮盘赌方法选择的多样化生成Prompt，以避免局部最优。实验表明，当已经存在几个高质量Prompt时，选择遗传算法；否则选择差分进化算法。

Promptbreeder[24]是一种通用的自引用自我改进机制，用于针对给定领域同时进化任务Prompt（原始Prompt）和变异Prompt（元Prompt）。AELP[30]通过带有束搜索的简单贪心算法优化长Prompt，并利用搜索历史来增强基于大语言模型的变异的有效性。Cui等人[20]引入了一种多阶段探索 - 利用策略PhaseEvo，以实现指令和示例的联合优化。首先，利用可用的输入 - 输出对和人类设计的Prompt来探索一个庞大的联合初始化种群。然后，一个大语言模型改进器根据大语言模型检查器提供的反馈生成候选Prompt。值得注意的是，依次应用基于大语言模型的全局和局部变异，以实现进化的多样性和快速收敛。

#### 3.1.2 基于反馈的方法

准确的反馈是监督优化对象迭代更新为更好版本的关键因素，它可以采取多种形式，如图4所示，例如强化学习（RL）中的奖励信号、梯度下降中的梯度，甚至是人类偏好。这一系列工作与前面的方法的区别在于，反馈信号指示了更明确的优化方向，因此搜索空间相对较小。

![](https://i-blog.csdnimg.cn/direct/55de580772a248aa87000136887bb8d1.png)

图4. 各种反馈信号有助于明确自动提示工程的优化空间，其中橙色虚线表示原始的提示搜索空间，灰色实线代表因优化方向更明确而缩小的搜索空间。

早期，黑盒大语言模型的离散Prompt优化可以被表述为强化学习问题。通常，策略将冻结的预训练语言模型与具有相对少量参数的可训练神经网络相结合，奖励反映了将目标大语言模型的预测与真实情况进行比较的综合反馈。

RLPrompt[21]由一个冻结的预训练语言模型（RoBERTa - large[55]）和一个可学习的特定任务多层感知器（MLP）组成，逐个生成前缀Prompt的标记，以最大化奖励，但这会导致Prompt在语义上不连贯。DSP[51]仅使用极少的标记数据对一个小的策略语言模型（T5[79]）进行微调，为原始输入生成细致的、特定实例的定向刺激Prompt，例如在总结指令后添加关键词。PACE[23]利用大语言模型同时作为行动者和评论家的双重角色进行迭代Prompt优化。

PRewrite[41]训练一个Prompt重写大语言模型（PaLM[3]）通过强化学习优化与输入无关的指令，并设计了两种重写策略：PRewrite - I采用贪心解码，PRewrite - S采用搜索算法。SCULPT[42]是一个新颖的框架，通过强化学习的行动者 - 评论家机制以层次树的形式系统地优化长Prompt。它有两种互补的反馈机制：初步评估在执行前评估Prompt结构，错误评估在执行后诊断模型错误。

实际上，梯度下降是在连续空间中探索最优解最常用的算法，其中梯度是一种负反馈形式。在封闭资源的大语言模型广泛流行的背景下，元Prompt通常用于指导大语言模型自行执行自动Prompt优化的关键步骤，特别是梯度生成可以被视为一种自我反馈。

ProTeGi[72]借助基于API的大语言模型提出了一种简单的非参数Prompt优化解决方案。其非参数性质体现在以下几个方面：(1) 文本梯度：大语言模型根据训练数据的小批量总结初始Prompt的缺陷，作为文本梯度。(2) 反向传播：另一个大语言模型通过在文本梯度的相反语义方向上编辑初始Prompt，将其转换为候选Prompt。(3) 扩展：释义大语言模型进行蒙特卡罗搜索，在与候选Prompt语义相似的范围内探索Prompt空间。(4) 束搜索：多臂赌博机优化中的最佳臂识别算法选择最有前途的候选Prompt用于下一次迭代。

有趣的是，由于元Prompt在操纵大语言模型工程师方面起着重要作用，PE2[113]专注于按照基于梯度的优化中常用的概念对元Prompt进行Prompt工程。为了优化ProTeGi的搜索空间，PREFER[116]设计了一个用于Prompt集成学习的反馈 - 反思 - 改进框架，其中反馈反映了每个弱Prompt的不足，并协同工作以在增强过程中覆盖多个子空间。

还有一种有效的集成方法来增强Prompt效果评估的稳定性，优于常规的束搜索或蒙特卡罗搜索。基于梯度累积，AutoHint[88]为每个输出错误的样本（Prompt - 输入对）推导一个Prompt，并从每个样本的Prompt中总结梯度。UniPrompt[38]通过对具有相似任务方面的样本进行聚类，并结合每个聚类的反馈（如小批量），生成捕捉任务多个方面的优化Prompt。GPO[92]不总结梯度信息，而是直接从包含任务Prompt、错误示例和Prompt - 分数轨迹的元Prompt中引导大语言模型实现更新方向，并使用编辑距离执行更新方法。为了有效适应不同的数据分布，AMPO[110]自动构建一个多分支Prompt，以高效处理复杂任务中的多种模式。大语言模型分析器识别每个失败案例的原因，大语言模型总结器迭代地将所有原因总结为不同的模式，以减少重复和冗余的分支。大语言模型修订器最终在深度（通过添加更多细节）或广度（通过添加更多分支）上优化Prompt。

人类偏好对齐有助于大语言模型学习人类智能，对齐的效果在很大程度上取决于偏好数据的质量。与基于人类反馈的强化学习（RLHF）[64]类似，APOHF[53]通过训练一个神经网络，根据人类偏好预测最佳Prompt，实现了Prompt优化的自动化。BPO[15]基于人类偏好反馈构建成对的原始Prompt和优化Prompt，训练一个70亿参数的序列到序列模型作为Prompt偏好优化器。

优化后的Prompt适应了大语言模型的理解，适用于各种大语言模型，与基于近端策略优化（PPO）[81]和直接偏好优化（DPO）[78]的强化学习相比，进一步缩小了人与大语言模型之间的对齐差距。APEAR[37]利用大语言模型迭代优化Prompt，特别是针对基于自我反馈和高质量Prompt偏好的段落相关性排名任务。FIPO[57]引入了第一个大规模Prompt优化偏好数据集（POP），该数据集收集了次优的GPT - 3.5的被拒绝数据和最优的GPT - 4的选择数据，并经过人类专家和分析模型的严格交叉验证。POP用于离线微调基于本地大语言模型的优化器，以改进自由形式指令导向的Prompt。然而，大语言模型是否是一个好的Prompt优化器是一个值得探索的问题[59]。

#### 3.1.3 基于编辑的方法

除了上述方法外，通过在特定词汇单元级别使用删除、交换、释义、添加等操作直接编辑Prompt，在不偏离原始Prompt的情况下扩展Prompt空间也是可行的。GrIPS[70]是一种无梯度、基于编辑的搜索方法，由三个步骤组成。步骤1包括两个操作：切片和编辑。首先，使用基于条件随机字段（CRF）的成分分析器[115]将基础指令分割为更短的词汇单元（即单词、短语或句子），实践证明短语级别的切片最有帮助。然后，通过四种操作（删除、交换、释义、添加）之一对特定切片进行编辑，以扩展指令空间。步骤2在一个分数集上评估候选指令，并基于贪心搜索、束搜索和模拟退火（SA）算法选择最佳指令。步骤3进行多次搜索迭代，直到分数不再提高或达到最大阈值。GrIPS已被证明适用于许多Prompt模式（有无示例均可），尤其是在经过指令微调的大语言模型中。

Plum[65]作为一种通用的Prompt学习方法，同时满足自动、离散、黑盒、无梯度和可解释的特点，它将GrIPS的编辑操作与各种元启发式算法（包括爬山法、模拟退火、带/不带交叉的遗传算法、禁忌搜索和和声搜索）相结合，以扩展离散Prompt空间。SPRIG[117]特别使用GrIPS编辑操作来优化系统Prompt。

编辑操作也可以在强化学习过程中用于探索优化空间。TEMPERA[120]在测试时顺序编辑与查询相关的Prompt。Prompt的编辑操作包括对指令的交换、添加和删除；对示例的置换和交换；对语言表达的更改。根据RLPrompt[21]中提出的步骤奖励，TEMPERA使用连续编辑之间的分数差异作为即时奖励，并采用基于注意力的策略架构来选择可能的操作。与先前的工作不同，这种方法在人类先验知识和Prompt性能之间取得了良好的平衡。

### 3.2 思维链优化

鉴于指令的简洁性，其包含的信息可预见地不足。为了更好地激发大语言模型在解决复杂任务方面的潜力，可以添加与任务相关的示例来丰富Prompt的上下文。在这些技术中，思维链（CoT）[104]作为一种关键的Prompt策略被引入，它显著增强了大语言模型的推理能力。思维链的核心思想是 “分而治之”，将复杂问题分解为更细粒度的子问题，并在一个完整的思维框架中逐步进行明确推导。由于推理是人工智能系统最有意义的任务之一，因此出现了许多以思维链为代表的Prompt框架优化工作。我们将它们纳入自动Prompt工程中的思维链优化，并根据与第3.1节指令设计分类标准类似的方式提供三类方法。最后一类涉及使用外部工具与更广泛的知识库进行交互。

#### 3.2.1 基于采样的方法

Zero - Shot - CoT[40]首次验证了大语言模型可以使用指令性Prompt “让我们一步一步思考” 采样多样的思维链示例（推理 - 答案对），这大大降低了手动设计少样本思维链的人力资源需求。随后，一种新的解码策略——自一致性[100]被提出，以取代思维链采样中使用的简单贪心解码。具体来说，它首先通过调整温度系数采样多样的推理 - 答案对，然后通过多数投票选择最一致的答案。这两项研究显著释放了大语言模型的推理能力，为思维链Prompt优化奠定了可靠的基础。

起初，LMSI[32]证明了大语言模型可以通过使用思维链Prompt和自一致性，对带有推理增强答案的混合格式进行训练来实现自我提升。许多工作不再侧重于增强训练数据，而是开始基于大语言模型自身的输出改进Prompt输入。

为了减轻Zero - Shot - CoTPrompt中大语言模型的幻觉问题，Auto - CoT[122]利用句子BERT[80]对问题进行多样性聚类，并生成相应的思维链示例，以实现更灵活、适应任务的少样本Prompt。Boosted Prompting[69]鼓励大语言模型为不同问题采样多个推理。将准确率或一致性最低的问题及其正确推理视为困难示例，形成有信息的Prompt，迭代组成一个增强的集合，从而增加问题空间的整体覆盖范围。COSP[95]专门设计了一个评分函数，该函数结合了一致性、多样性和重复性，用于在第一阶段选择由Zero - Shot - CoTPrompt大语言模型生成的优秀输出，并在第二阶段通过结果熵自适应地确定示例的数量。

最后，对两个阶段的输出进行多数投票，形成最终预测。USP[96]是COSP[95]的改进通用版本，不限于推理任务。它采用相同的候选Prompt生成方法，但在使用任务特定选择器方面有所不同，该选择器针对三种经典自然语言处理任务类型（分类、短文本生成和长文本生成）设计评分函数，以选择合适的伪示例。最后，使用贪心解码代替多数投票。面对混合任务场景，Meta - CoT[127]采用路由机制将问题类型与现成的示例池进行匹配。如果不匹配，则使用Zero - Shot - CoTPrompt生成思维链示例，并基于密度像Auto - CoT[122]一样自动更新具有相似问题聚类的数据缓存。Reprompting[108]通过吉布斯采样生成逐步的思维链示例。它从训练数据的条件分布中采样，推导出思维链示例的联合分布，这也可以看作是进化算法的一种变体，在给定少量问答对的情况下，无需人工干预，为每个模型迭代找到有效的思维链。

#### 3.2.2 基于反馈的方法

Self - refine[60]验证了大语言模型具有基于自我反馈在各种任务中进行迭代优化的能力，其中元Prompt指示同一个大语言模型进行初始生成、反馈和优化。因此，许多优化的Prompt框架都包括迭代反馈，这通常是大语言模型对自身行为的深刻反思，其行为可能是完全错误的或次优的。我们将涉及逐步推理性质的相关工作归为这一类。

强化学习在推理任务中仍然有效，PromptPG[58]在冻结的预训练语言模型（BERT[80]）之上训练一个小的线性层，根据大语言模型预测与标签之间的一致性选择上下文示例，以确保Prompt的完整性。Prompt - OIRL[87]通过基于代理奖励模型（XGBoost[10]）的离线逆强化学习，在实例级别而不是分布级别优化与查询相关的Prompt。用查询 - Prompt对训练代理，以逼近由黑盒大语言模型和黄金标签计算的真实在线奖励。Reflexion框架[85]设计了一个特殊的记忆模块，将标量奖励转换为自然语言反馈。行动者与环境之间的交互历史（称为轨迹）作为短期记忆，而从轨迹及其奖励分数中总结的语言经验反馈存储在长期记忆中。Reflexion智能体在两个记忆组件的协同作用下，性能优于其他大语言模型。与Reflexion为了在线动态反馈多次执行任务不同，PROMST框架[14]中的TaskLLM一次性执行任务，根据人类设计的反馈规则自动合成关于错误的离线反馈。SumLLM将反馈总结为上下文，供GenLLM生成新的Prompt候选。为了降低评估成本，对分数预测模型进行微调，以启发式地采样候选子集进行评估。

负反馈可能对大语言模型的思考更有帮助。DTG[45]设计模板，促使大语言模型检测不相关系统输出中的错误类型，并从负反馈中触发大语言模型的思考能力。Prompt模板只需进行最小的调整，就能引导大语言模型执行适用于广泛文本生成任务的简单单步推理。Reprompt[12]在各种推理任务中优化思维链的逐步指令。如果没有这部分内容，额外的检查器会将思维链转换为带有逐步指令的形式。受梯度下降的启发，损失是从目标大语言模型与反馈生成器之间的一批交互历史中总结出来的重点。然后，Prompt优化器主要根据损失更新通用Prompt部分。这个行动循环迭代直到Prompt收敛。

#### 3.2.3 基于交互的方法

大语言模型在处理推理任务时，经常会出现幻觉和错误传播等问题。为此，大语言模型与外部资源进行交互以提升其内部能力的做法越来越普遍。例如，在Prompt框架的中间步骤中借助外部工具进行错误检查已被证明非常有效。

改进后的ReAct[112]Prompt轨迹由稀疏协同的思维链（仅推理）Prompt和仅行动Prompt构成，它引导大语言模型以交错的方式生成推理痕迹和特定任务的行动，并通过与简单的维基百科API交互来克服常见的推理问题。Verify-and-Edit框架[123]生成验证问题来处理一致性较低的思维链示例，并利用外部知识对推理进行后期编辑，以使预测更符合事实，这种方式更加自然和具有对话性。ART框架[67]从任务库中检索相似任务，构建少样本Prompt，将任务分解为子步骤，并在必要时调用工具库中提供的搜索引擎和代码生成等工具。

在Self-ask框架[71]中，大语言模型不断明确地提出后续问题，并使用搜索引擎进行回答，直到有足够的信息来解决原始任务。ToolLLM[74]是一个通用的工具使用框架，包括由API集合、指令生成和解决方案注释构建的ToolBench，由ToolBench进行监督微调的ToolLLaMA，以及用于多轮交互评估的ToolEval。ATC[84]表明，大语言模型不仅可以通过编程使用一系列工具来解决复杂任务（多工具用户），还可以通过黑盒探测自动掌握快速更新的新工具（多工具学习者）。

## 4 Prompt压缩

Prompt压缩指的是在不牺牲大语言模型性能的前提下，将长文本Prompt提炼为尽可能短的文本或向量。

挑战：随着大语言模型生成能力的不断提高，研究人员发现，在少样本设置中使用更具体的上下文进行Prompt可以提高任务准确率。与此同时，大语言模型在长上下文场景中的应用，如多轮对话、检索增强生成（RAG）、多文档摘要等，也在积极探索中。理论上，上下文中涵盖的与任务相关的知识越全面，大语言模型的表现就越好。

然而，无限的输入长度会给实际部署带来困难：(1) 有限的上下文窗口：每个大语言模型在预训练时都有一个固定的输入长度限制，任何超过此长度的文本都会被截断，从而丢失这部分上下文语义。(2) 灾难性遗忘：当没有足够的缓存空间时，大语言模型在对长序列进行建模时可能会忘记之前学到的知识。(3) 推理速度慢：大语言模型的大规模是一把双刃剑，虽然具有大量参数的大语言模型可以存储丰富的知识，通用性强，但在推理过程中不可避免地会过度消耗计算资源。

解决方案：为了缓解上下文窗口的限制，可以将
**可重复使用的系统Prompt封装在模型内部**
，从而为用户输入节省更多空间。继早期参数高效Prompt调整工作[44, 49]之后，少量可学习参数（软Prompt）可以携带特定任务信息，这样冗长硬Prompt中的任务描述就可以转换为更紧凑的软Prompt，以解决灾难性遗忘问题。这两种方法都通过公式2利用开源大语言模型进行训练；对于闭源大语言模型，可以借助小语言模型进行信息测量或大语言模型进行总结，按照公式3提高硬Prompt的信息密度，以加快推理速度。

根据压缩Prompt的类型，我们将Prompt压缩分为
**连续空间中的文本到向量（T2V）压缩和离散空间中的文本到文本（T2T）压缩**
。图5描述了四种主要的方法类型，T2V压缩将大语言模型视为一个压缩器，把关键文本信息压缩为更紧凑的向量表示。T2T压缩则直接测量标记的信息性，并在可接受的性能损失范围内尽可能压缩冗余信息。

![](https://i-blog.csdnimg.cn/direct/cc570b5863b746fe8b43f059f2ccdd43.png)

图5.
**连续和离散空间中的Prompt压缩**
。T2V压缩包括基于
**知识蒸馏**
将系统或用户Prompt内化到模型参数中，以及以
**迭代或一次性的方式**
将
**硬Prompt**
的关键信息编码为
**软Prompt**
。T2T压缩分别包含提取式和抽象式方法，即不同粒度的剪枝和为获取足够信息性的摘要。

### 4.1 文本到向量压缩

当语言模型处理文本Prompt时，将其转换为向量是必要步骤。由于向量比文本的表示更密集，因此自然会考虑将模型外部的离散文本Prompt压缩为模型内部的连续向量。在这种情况下，连续向量可以是模型的内部参数，称为内化；也可以是额外的软Prompt，称为编码。这种Prompt压缩不仅扩大了上下文窗口，而且在频繁重用相同Prompt时还能加速推理。

#### 4.1.1 内化

这里，我们引入知识蒸馏（KD）[29]的概念，即把来自较大教师模型的知识转移到较小的学生模型中。受此启发，可以通过
**原始Prompt和压缩Prompt的输出分布之间的KL散度**
对语言模型进行
**微调**
，将文本Prompt中的知识提炼到模型参数中，从而实现Prompt功能的内化。

最初的研究尝试
**内化系统Prompt**
。为了开发一个符合人类价值观的通用人工智能系统，Context Distillation[4]通过在带有上下文和输入的原始模型与仅带有输入的蒸馏模型之间进行知识蒸馏，将HHH（有用、诚实和无害）Prompt内化到语言模型中。Prompt Injection（PI）[18]提出了伪输入生成（PING）方法来内化系统Prompt，避免了推理过程中对重用Prompt的重复计算。第一阶段使用在特定任务数据上训练的输入生成器根据Prompt生成伪输入。第二阶段在带有Prompt和伪输入的教师模型与仅带有伪输入的学生模型之间进行知识蒸馏。

后来，研究人员开始在模型内部压缩用户Prompt的上下文。Snell等人[86]提出提炼Prompt中的三种特定类型的上下文：抽象任务指令、复杂推理和具体训练示例。基于教师和学生Prompt模板，将原始输入转换为两种具有显著分布差异的Prompt。训练语言模型对不同Prompt产生相同的响应，以实现内化。

测试了三种蒸馏方法，结果表明，一旦模型学习了特定任务Prompt的知识，它就可以在没有明确Prompt的情况下执行相应任务。Instruction Distillation[89]内化了三种复杂的指令技术：逐点排序、成对排序和列表排序。其目标是通过使大语言模型能够用更简单的指令进行高效排序，解决零样本相关性排序的低效率问题。

特别是在推理方面，Distilling Step-by-Step[31]不仅使学生小语言模型能够从教师大语言模型中学习标签，还提取推理过程作为额外的监督信号，在多任务框架下将大语言模型的推理能力内化到学生小语言模型中，在较小的模型规模和较少的训练数据下实现了比标准微调更好的性能。

对于
**检索增强生成任务**
，受提取模态特征的模态编码器CLIP[75]的启发，xRAG[16]将密集检索中的文档嵌入重新定义为来自检索模态的特征，并使用即插即用的投影仪（模态融合桥）将其无缝集成到语言模型的表示空间中。xRAG有两个训练阶段：首先，大语言模型与投影仪提供的压缩表示内部兼容，并使用释义指令进行训练；然后，冻结的大语言模型与可训练的投影仪一起进行指令微调，以基于压缩表示执行特定的下游任务。还使用自蒸馏来增强xRAG在嘈杂环境中的适应性。

#### 4.1.2 编码

在上述内化方法中，压缩后的硬Prompt缺乏明确的载体，而编码方法则预定义适合大语言模型的软Prompt来表示硬Prompt，如多任务指令、冗长的上下文等。通常，有一个压缩器将硬Prompt编码为软Prompt，这些软Prompt可以有效地应用于各种下游任务。

Wingate等人[105]首先提出将硬Prompt压缩为软Prompt，硬Prompt被添加到输入序列的开头，而软Prompt被添加到输入嵌入的开头。通过最小化两个输出分布，将复杂硬Prompt中的重要信息提炼为简洁的软Prompt。虽然这个训练过程成本较高，但在重用相同Prompt时可以降低推理成本。

有一系列基于
**Gisting**
[61]的工作，如图6所示，其目标是在
**不同Prompt之间实现更好的泛化**
。

Gisting[61]的目标是将多任务指令编码为要点标记，这些要点标记可以缓存并重复使用，以提高计算效率。值得注意的是，它利用超调（HyperTuning）[68]中的元学习来预测要点标记，而不是使用梯度下降来更新软Prompt。

![](https://i-blog.csdnimg.cn/direct/5deb8fe8e1fe4d06acfb91dab3906d3b.png)

图6. Gisting系列工作基于经过特殊注意力机制训练的编码器或解码器，将硬Prompt的关键信息压缩为要点标记。上半部分的矩阵表示掩码策略，其中灰色框表示标准掩码，黄色框表示要点掩码。

具体来说，
**要点标记**
（Gist Tokens）首先被添加到词汇表和嵌入矩阵中，然后连接在指令之后，最后根据掩码进行微调，如图6 - (a)所示。Gisting可应用于仅解码器和编码器 - 解码器架构，能够实现高达26倍的Prompt压缩，减少40%的浮点运算量，加速4.2%的运行时间，并节省存储空间，同时输出质量的损失最小。

受信息论中最小描述长度（MDL）原理[27]的启发，Gist - COCO（Gist Conditioned Coding）[48]使用编码器 - 解码器架构将原始Prompt压缩为更短的要点Prompt。编码器经过微调，将要点标记压缩为要点表示，通过KL散度模拟原始Prompt的输出分布。解码器将这些要点表示转化为要点Prompt，以在不同的大语言模型之间实现高压缩率的泛化。除了压缩短指令，Gisting还可以压缩长上下文。

顾名思义，UltraGist[118]使用要点标记压缩超长上下文。它采用具有优化交叉注意力机制的仅解码器架构，如图6 - (c)所示，逐步将细粒度的上下文片段压缩为UltraGist标记，这些标记跟随下一个片段进行后续压缩。它显著允许随机采样压缩比来处理动态上下文，同时保持近乎无损的性能。

接下来，我们介绍一些专门用于压缩Prompt中长上下文的研究，如图7所示，这些研究在加速推理和减少GPU内存使用方面都具有优势。

AutoCompressor[17]采用循环记忆Transformer（RMT）[7]架构，使用摘要标记将任意长度的上下文片段迭代压缩为摘要向量。在后续迭代中，前一个片段的摘要向量连接在下一个片段的前面，摘要标记连接在末尾。此外，通过时间反向传播（BPTT）方法[11]在压缩两步后停止对缓存的摘要向量进行梯度更新，进一步降低计算负载和GPU内存需求。在RAG系统方面，LLoCO（Learning Long Contexts Offline）[91]使用AutoCompressor将长文档压缩为多个存储在向量数据库中用于检索的摘要嵌入。

另一类工作一次性压缩完整的上下文，而不是逐段迭代。通常有一个用于编码的可训练大语言模型（称为压缩器）和一个用于解码的冻结大语言模型（称为生成器）。In - context Autoencoder（ICAE）[26]对适应LoRA的大语言模型进行微调，基于PWC（Prompt - with - Context）数据集，使用记忆标记将上下文编码到记忆槽中。带有记忆槽的生成器通过自动编码和文本续写任务进行预训练，以重建原始上下文，并应用于各种下游任务。500xCompressor[52]同样对适应LoRA的大语言模型进行微调以压缩Prompt。不同之处在于，生成器通过再生任务进行预训练，并通过问答任务进行微调，学习将压缩Prompt的KV值作为输入并基于此生成响应。500xCompressor可以压缩任何文本，回答各种类型的问题，并实现6倍到480倍的压缩率。在基于大语言模型的推荐方面，Prompt Distillation（POD）[46]主要用于三种典型任务：序列推荐、Top - N推荐和可解释推荐。骨干模型采用编码器 - 解码器架构，其中编码器使用额外的全词嵌入将离散的Prompt模板提炼为多个连续的Prompt向量，以确保项目ID的完整性，解码器根据这些Prompt向量生成推荐。随后，RDRec[98]框架由两个阶段组成，用于合成训练数据并将推理过程内化到较小的模型中。在交互推理蒸馏阶段，大语言模型生成包括用户偏好和项目属性的推理过程作为训练标签。在推理感知推荐阶段，由POD[46]压缩的Prompt模板向量与用户和项目ID连接作为训练输入。RDRec在序列推荐和Top - N推荐中均实现了最先进的性能。

为了平衡训练成本、推理效率和生成质量，SelfCP（Self - Compressor）[25]仅压缩超出限制的Prompt，以降低压缩和生成的难度。SelfCP有三个组件：压缩器和生成器是冻结的大语言模型，它们之间的可训练连接器是一个1700万参数的线性层。与训练过程效率较低的AutoCompressor和输入长度有限的ICAE不同，SelfCP采用仅解码器的压缩器异步压缩每个超出限制的片段，并像AutoCompressor一样将它们连接起来。然后，对连接器进行微调，将记忆标签顶部的隐藏状态投影为大语言模型可接受的记忆标记。对于上下文学习，直接提取从目标示例压缩得到的记忆标记更为高效。

这里，我们提供了不同编码方法之间的比较，如表3所示。我们发现，在仅解码器架构的压缩器中，软Prompt通常添加到输入序列的前面，而在编码器架构中则添加到后面，这与2.2节中提到的训练任务有关。

![](https://i-blog.csdnimg.cn/direct/acdf19762236404884045f9ebd7d7805.png)

图7. 专门用于长上下文的代表性编码方法的差异。AutoCompressor使用摘要标记迭代压缩上下文片段。ICAE一次性压缩完整的上下文。SelfCP仅基于连接器压缩超出限制的上下文片段。

![](https://i-blog.csdnimg.cn/direct/69d430f994544c96a5a911a436871790.png)

表3. 不同编码方法的比较。

### 4.2 文本到文本压缩

连续空间中的文本到向量压缩侧重于使语言模型适应自然语言。然而，压缩后的软Prompt通常缺乏人类可读性和可解释性。为了进一步促进大语言模型与人类之间的无缝交互，研究人员从人类理解自然语言的角度出发，考虑仅在离散空间中压缩Prompt。常见的做法是利用可访问的语言模型来测量硬Prompt的信息密度，然后在保持大语言模型性能稳定的前提下，尽可能缩短Prompt长度。我们将方法总结为两类，如图5右侧所示，剪枝可定义为提取式压缩方法，而摘要则是抽象式压缩方法。

#### 4.2.1 剪枝

剪枝指的是直接去除不同粒度（即标记、短语、句子）中信息含量较低的词汇单元，而不改变原始Prompt的语言表达。它通过利用较小的语言模型计算信息熵，或训练判别器模型来确定是否丢弃Prompt中的某些标记，以此来衡量信息量。根据不同的过滤单元，有三种剪枝方案：粗粒度、粗到细粒度和细粒度。
  
粗粒度：
  
示例级别：为了实现黑盒大语言模型的性能 - 效率权衡，DynaICL[125]训练一个元控制器，根据输入复杂度和计算预算动态分配上下文示例的数量。
  
句子级别：FilCo[102]采用三种词汇和信息论方法——字符串包含、词汇重叠和条件交叉互信息（CXMI），将检索到的文档提炼为有用的上下文，用于训练上下文过滤模型和RAG任务的生成模型。为了保持上下文的语义完整性，CPC[54]训练一个上下文感知句子编码器，测量给定问题与上下文中每个句子之间的嵌入相似度（余弦距离），然后去除不相关的句子进行压缩。
  
文档级别：AdaComp[119]训练一个压缩率预测器，根据查询复杂度和检索质量动态选择最优文档。
  
粗到细粒度：微软提出了LLMLingua系列研究，用于压缩Prompt以加速大语言模型推理。LLMLingua[34]没有将Prompt视为一个整体，而是首次以粗到细的方式分别压缩Prompt的不同组件（即指令、问题和示例）。首先，准备一个通过指令微调与大语言模型对齐的小语言模型，基于困惑度（PPL）来测量标记的信息性。有一个预算控制器根据不同组件的重要性动态分配压缩率，其中指令和问题被预定义较低的压缩率，而示例则被分配较高的压缩率。粗粒度压缩减少示例的数量，细粒度压缩则在剩余示例中迭代过滤PPL较低的标记。顾名思义，LongLLMLingua[35]在LLMLingua框架的基础上进一步压缩长文档。有一个线性调度器根据粗粒度压缩率自适应地控制细粒度压缩率。显著的区别在于，LongLLMLingua测量与给定问题相关的信息量，其中使用重新排序机制来识别更多与问题相关的文档，并提出对比困惑度来保留更多与问题相关的标记。最后，子序列恢复方法确保大语言模型响应中关键信息（如时间和地点）的完整性。通过这些技巧，压缩后的Prompt可以以较低的成本获得更高的性能，同时减少端到端系统延迟。CoT - Influx[33]是一个即插即用的粗到细剪枝器，专门用于压缩由GPT - 4[1]生成的思维链Prompt。示例剪枝器减少无用的思维链示例，标记剪枝器过滤冗余标记。这两个剪枝器都是具有GELU激活函数的两层前馈网络（MLP），通过多目标奖励的强化学习进行训练，而不是反向传播。
  
细粒度：Selective Context[50]使用由因果语言模型计算的自信息[83]来评估词汇单元的信息性，并通过基于百分位数的方法过滤冗余内容。值得一提的是，标记自信息通过预测下一个标记的概率来计算，在词汇单元范围内累加，然后取平均值以获得短语和句子级别的自信息。实践证明，短语是最有效的过滤单元，它可以将推理内存使用减少36%，推理时间减少32%，同时性能下降可忽略不计，在效率和性能之间取得了良好的平衡。为了保留句法和语义结构，PROMPT - SAW[2]通过关系感知图提取具有关键信息的标记，并将它们恢复到压缩Prompt中，从而实现更好的可读性和可解释性。除了通过显式计算信息量来修剪冗余标记外，还可以训练语言模型自行决定是否修剪某些标记。PCRL[39] 将Prompt压缩视为一个二元分类任务，由一个冻结的预训练策略语言模型和基于强化学习的可训练多层感知器层来执行。该模型为Prompt中的每个标记分配包含或排除标签，奖励函数同时考虑压缩Prompt的忠实度和缩短的长度。与使用单向仅解码器模型计算信息熵不同，LLMLinga - 2 [66] 采用带有线性分类层的双向仅编码器模型作为压缩器，来决定每个标记是保留还是丢弃。这个小型但通用的压缩器在由GPT - 4 [1] 合成的蒸馏数据上训练10个epoch，显著提升了性能，压缩加速比提高了3 - 6倍，端到端延迟加速了1.6 - 2.9倍。

这里，我们在经典基准测试中对不同压缩粒度的剪枝方法进行了全面的实证分析，如表4所示。我们观察到，粗到细粒度的压缩似乎对复杂推理任务更有益，而细粒度压缩更适合长上下文任务。

![](https://i-blog.csdnimg.cn/direct/b6853af66ee34bf6815ff6a6f6ae3dd3.png)

表4. 各种剪枝方法在常用推理任务和长上下文任务中的大语言模型性能和Prompt压缩率（shot表示示例数量，×表示原始Prompt与压缩Prompt的比例）。值得注意的是，由于不同方法的实验设置存在差异，它们的性能可能无法直接比较。

#### 4.2.2 摘要

从本质上讲，摘要是一种语义级别的压缩，可能会改变语言表达，但会保留原始含义。为确保压缩Prompt的大语言模型性能与原始Prompt没有显著偏差，以下将介绍有训练和无训练的两种方法。

第一种方法通常将原始输出作为监督信号来训练摘要器。在RAG场景中，RECOMP（Retrieve, Compress, Prepend）[106] 在上下文集成之前，将检索到的文档压缩为文本摘要。有两个以查询为重点的压缩器对多文档进行摘要，以提高大语言模型的性能。提取式压缩器是一个1.1亿参数的双编码器模型，通过对比学习训练，用于选择与输入查询语义相似度高的句子。抽象式压缩器是一个7.75亿参数的编码器 - 解码器模型，通过知识蒸馏学习大语言模型的摘要能力，实现选择性增强，而不是添加不相关的文档。Nano - Capsulator [19] 是一个压缩器大语言模型，通过摘要指令对自然语言格式的Prompt进行语义压缩。原始Prompt和压缩Prompt之间的响应差异可视为奖励反馈，用于监控Nano - Capsulator的优化过程。还有一个严格的截断机制来限制压缩Prompt的长度。

第二种方法实时监控摘要Prompt的信息是否足以正确响应。MEMWALKER [9] 通过交互式Prompt大语言模型来压缩长上下文，而不是进行微调。它由两个阶段组成，第一阶段迭代总结上下文片段以构建记忆树，第二阶段从根节点导航，搜索足够的相关信息来回答给定的查询。CompAct [114] 主要解决长上下文场景中的问答任务。压缩器大语言模型依次将检索到的文档片段总结为压缩Prompt，评估其信息是否完整以回答给定问题。如果不完整，压缩Prompt将与后续片段连接进行下一次迭代总结。CompAct可以作为现成的检索器和阅读器之间高效且灵活的插件式压缩器，实现高达47倍的压缩率。Style - Compress [73] 可以看作是一种结合了自动Prompt工程和Prompt压缩技巧的高效少样本Prompt方法。一个较小的大语言模型（LLaMA - 2 - 7B [93]）通过依次使用多样化的人类编写风格进行Prompt，并结合任务特定的高性能示例进行上下文学习，迭代压缩原始Prompt。然后，由较大的大语言模型（LLaMA - 2 - 13B / GPT - 3.5）评估得到的最佳压缩Prompt及其比较优势被添加到示例池中，指导压缩器进行特定任务的Prompt压缩。

## 5 未来方向

尽管在大语言模型Prompt领域已经取得了显著进展，但仍面临一些重大挑战，这些挑战也为未来研究提供了令人兴奋的机会。我们在综述的最后对未来有前景的研究方向进行了推测，特别强调了与两种资源节约型Prompt策略的交叉点。同时，我们指出了未来研究方向中的一些开放性问题。
  
将两种资源节约型Prompt策略结合为双赢解决方案：实际上，Prompt工程和Prompt压缩的优化目标之间存在差距。前者倾向于优化更短的指令，而后者主要处理更长的示例。思维链（CoT）代表了这两个领域的共同关注点，尤其是在推理中起着关键作用，而推理是追求通用人工智能（AGI）的关键研究焦点。未来的研究可以基于公式1和公式3改进思维链Prompt框架。一种可行的选择是考虑直接嵌套这两种策略。例如，首先迭代地自我改进思维过程，然后从冗长的思维链中压缩无用信息。或者，这两种策略可以在单个迭代中同步执行，可能使用强化学习，将大语言模型的性能作为奖励信号来监督压缩过程。此外，设计平衡因子来整合公式1和公式3中的目标函数，有助于进行分析工作，例如讨论优化和压缩水平的相互约束，以及如何平衡模型性能、计算成本和推理速度等各种指标。
  
探讨大语言模型与人类在理解自然语言方面的差异：虽然Prompt压缩加速了大语言模型的推理，并且通常能保持或略微提高性能，但往往难以确保压缩Prompt的可读性和可解释性。因此，有机会在通用人工智能的背景下探索无意义压缩Prompt与人类可解释压缩Prompt的意义。从人类理解角度定义的信息，是否是衡量为大语言模型提供的有效信息的合理标准？研究这种差异可能会为优化Prompt以更好地与人类对齐提供有价值的见解。
  
提高高效Prompt的鲁棒性：优化后的指令通常适用于特定的下游任务，因此值得研究如何在自动优化过程中提高Prompt在不同数据分布之间的可转移性。例如，混合标记数据和未标记数据可能有助于学习更通用的Prompt。同样，T2V压缩Prompt通常是为特定语言模型定制的，在不进行微调的情况下难以适应其他模型。一种潜在的解决方案是将软Prompt封装在一个即插即用的适配器中，增强编码表示的可转移性，实现高性能和通用性。
  
未来反思的开放性问题：受小语言模型推理能力的限制，自动Prompt工程在很大程度上依赖大语言模型来优化Prompt [121]。因此，需要同时考虑模型能力和计算成本。虽然自动Prompt工程显著减轻了人力劳动，但仍然无法完全避免人工干预。具体来说，在指令优化过程中，初始迭代需要用户提供输入 - 输出示例或高质量的初始Prompt；在示例（尤其是思维链）优化过程中，需要精心设计元Prompt内容或框架，以正确引导大语言模型生成合适的响应。

## 6 结论

本综述从减少资源消耗的角度，全面介绍了大语言模型高效Prompt方法的最新进展。我们全面回顾了现有的两类高效Prompt方法：避免人力资源消耗的自动Prompt工程和节省计算资源的Prompt压缩。值得注意的是，我们从数学角度提炼了每类方法的优化目标，并期望在未来将这两种资源节约策略结合起来，用于轻量级大语言模型应用。我们进一步总结了每类方法的代表性方法，特别强调了效率。最后，我们概述了未来研究的潜在方向，并提供了附录A.1中列出的开源高效Prompt项目。