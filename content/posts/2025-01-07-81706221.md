---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f636173736965507974686f6e:2f61727469636c652f64657461696c732f3831373036323231"
layout: post
title: "跨模态检索带你领略图文检索的魅力"
date: 2025-01-07 18:44:01 +0800
description: "跨模态检索：带你领略图文检索的魅力引子作为经常“百度一下”的众多网民中的一份子，我们经常使用百度，输"
keywords: "跨模态图文检索"
categories: ['机器学习']
tags: ['Machine', 'Learning']
artid: "81706221"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=81706221
    alt: "跨模态检索带你领略图文检索的魅力"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=81706221
featuredImagePreview: https://bing.ee123.net/img/rand?artid=81706221
---

# 跨模态检索：带你领略图文检索的魅力

## 跨模态检索：带你领略图文检索的魅力

### 引子

作为经常“百度一下”的众多网民中的一份子，我们经常使用百度，输入要搜索的关键词，来检索要想的文本信息，此时是使用文本来检索文本；有时我们又会使用百度图片检索的功能，上传图片来寻找相似的图片，此时是以图来检索图；但我们也经常使用文本来搜索相应的图片，此时我们输入信息的类型和获得的信息的类型就不同了，我们称之为“跨模态”。

### 简 介

来下个专业点的定义：跨模态检索就是寻找不同模态样本之间的关系,实现利用某一种模态样本,搜索近似语义的其他模态样本。比如：利用图像来检索相应的文本，或者利用文本来检索想要的图像。当然这里模态也不仅仅限于图像和文本，如语音，生理信号，视频都可以作为跨模态检索的组成成分。

我们知道，网络的发展，让我们多媒体数据种类急速增多，比如：文本，图像，视频等，每种数据可以看作时一种模态，在同一模态中进行检索相对是简单的。如何利用一种模态，对另一种模态进行检索，就比较复杂了。

### 方法

想要寻求解决的方法，首先要知晓其特点。对于跨模态检索，不同的模态之间呈现出底层特征异构，高层语义相关的特点。听起来好难理解，举个例子：我们现在有个网页的主题是“老虎“，网页中文本上描述是“猫科动物，身上有条纹，爱吃肉肉blablabla”；而网页上的图片直接给出了“老虎”。这两者从语义上讲，都在描述同一主题——老虎；但是从特征上讲，文本通过字典向量来描述，而图片可以通过SIFT特征、LBP特征等等来表示，从特征描述上讲两者的类型是完全不同的。

于是，跨模态检索的主要方法就是：寻找不同模态之间的关系，将不同模态映射到同一子空间中，在此空间中我们就可以度量不同模态间的相似性了。具体而言：

1. 首先，我们有一些训练的实列，每个实例都是一个有着标签的image-text对；
2. 然后，将这些实列划分为source set（即训练集）和target set（即测试集），每个set中有着不相关的image-text对，但都有着同样的类别（即训练集和测试集不存在相同的image-text对，但类别是相同的）；
3. 接着，在训练时，我们从source set中学习一个公共的语义空间；
4. 进而，将该公共的语义空间应用到target set中，从而对target set中的实列产生公共的特征表示；
5. 最后，利用公共的特征表示就可以比对两种模态的样本的相似程度，从而进行匹配。

### 实例

我们以ICMR2018的一篇文章“Modal-adversarial Semantic Learning Network for Extendable Cross-modal Retrieval” 为例来具体的看利用深度学习是如何实现跨模态检索的。

在上面的方法中，source set的类别包含target set中的类别，如图1中的(a)所示：

source set中的类别是：艺术，生物，运动，战争等等，包含target set中的类别（艺术，生物，运动，战争等等）。我们称解决此问题的方法为非可扩展的跨模态检索。但是实际中往往会出现这种尴尬的情况，source set的种类并不能完全覆盖target set中的种类。我们称解决此问题的方法为可扩展的跨模态检索方法，如图1中的(b)所示。

![这里写图片描述](https://i-blog.csdnimg.cn/blog_migrate/25716c7810b3e444d357cd4438948cd4.png)

本文旨在解决可扩展的跨模态检索问题。提出的网络结构如图2所示：

![这里写图片描述](https://i-blog.csdnimg.cn/blog_migrate/2037b6846e73696d6de324520e308e40.png)

网络的输入是成对的Image-Text。然后是编码器，其中隐藏层的Code Layer用于关联两个网络的分支，意在挖掘不同模态间的关系。换言之，对于成对的输入样例，两个子网络学到的特征应该是相似的，通过一个相似度损失(Similarity loss)来实现：

f





c

o

r

r


(




v



i


,




t



i


)

=





∥





f



v


(




v



i


;




Θ



r


)

−




f



t


(




t



i


;




Θ



r


)

∥



2



2


f
c
o
r
r
(
v
i
,
t
i
)
=
‖
f
v
(
v
i
;
Θ
r
)
−
f
t
(
t
i
;
Θ
r
)
‖
2
2

  
则，对于所有source set样例有：
  



L





c

o

r

r


=




∑





i

=

1








N



s





f





c

o

r

r


(




v



i


,




t



i


)


L
c
o
r
r
=
∑
i
=
1
N
s
f
c
o
r
r
(
v
i
,
t
i
)

  
通过优化该损失，可以减少不同模态对之间的差异。接下来我们看到了Class Embedding：将输入对的类别信息进行编码，输入网络，与Code Layer的输出特征进行拼接，构成新的特征，输入到后面的解码器中。该Class Embedding是本文解决可扩展跨模态检索问题的关键，受启发于零样本学习（Zero-shot Learning）。零样本学习的一个重要理论基础是：利用样本的高维的语义特征，使得训练得到的模型具有可迁移性。比如：一个物体的高维语义是“四条腿，喵喵叫，宠物的一种”，我们可以轻松地断定这是猫咪。实际上，作者的想法很类似与CVPR2017的一篇文章(Semantic Autoencoder for Zero-Shot Learning)，使用Class Embedding作为高维的语义特征，对生成的低维的特征进行约束，使低维特征能够保留高维的语义特征。

前面拼接构成的新的特征，有两种类型，即从文本中构造的特征以及从图像中构造的特征，因为经过Code Layer层，该两种特征趋向于同一子空间，所以可以分别输入后面不同的解码器，进行重构。换言之，每个进行重构的解码器都有两种模态的输入。重构的损失为：

L





r

e

c


(




v



i


,




t



i


,




Θ



r


)

=




L



I


(




v



i


,




t



i


;




Θ



r


)

+




L



T


(




v



i


,




t



i


;




Θ



r


)


L
r
e
c
(
v
i
,
t
i
,
Θ
r
)
=
L
I
(
v
i
,
t
i
;
Θ
r
)
+
L
T
(
v
i
,
t
i
;
Θ
r
)

  
其中，
  



L



I


(




v



i


,




t



i


;




Θ



r


)

=







∥


∥






v



i


−









v



^



I



i




∥


∥



2



2


+







∥


∥






t



i


−









t



^



I



i




∥


∥



2



2


,








L



T


(




v



i


,




t



i


;




Θ



r


)

=







∥


∥






v



i


−









v



^



T



i




∥


∥



2



2


+







∥


∥






t



i


−









t



^



T



i




∥


∥



2



2


L
I
(
v
i
,
t
i
;
Θ
r
)
=
‖
v
i
−
v
^
i
I
‖
2
2
+
‖
t
i
−
t
^
i
I
‖
2
2
,
L
T
(
v
i
,
t
i
;
Θ
r
)
=
‖
v
i
−
v
^
i
T
‖
2
2
+
‖
t
i
−
t
^
i
T
‖
2
2

分别表示两个重构子网络对于两种模态的损失。实际上即约束重构的样本与原先的样本相似 ，使用欧式距离来衡量。

最后我们来看如图2所示网络结构中红色的部分。首先我们使用对抗学习的思想，引入一个简单的只有全连接层的识别器，使其无法区分输入的特征是属于哪一种模态的，从而进一步减少模态间的差异。在训练时，给每一个实例附上一个one-hot编码的标签来表示它属于哪一种模态。对抗损失为：

L





a

d


v


=




1






N



s





∑





i

=

1








N



s





f





c

r

o

s

s


(




z



j


,

p

(




z



j


)

,




Θ



a


)


L
a
d
v
=
1
N
s
∑
i
=
1
N
s
f
c
r
o
s
s
(
z
j
,
p
(
z
j
)
,
Θ
a
)

  
其中







p

(

⋅

)


p
(
⋅
)

表示标签指示器，










f





c

r

o

s

s


(

x

,

p

,

θ

)


f
c
r
o
s
s
(
x
,
p
,
θ
)

：是sigmoid交叉熵损失函数
  



f





c

r

o

s

s


(

x

,

p

,

θ

)

=

p

(

x

)

l

o

g


(






p



^


(

x

,

θ

)

)

+


[


1

−

p

(

x

)

]

l

o

g



[


1

−






p



^


(

x

,

θ

)

]


f
c
r
o
s
s
(
x
,
p
,
θ
)
=
p
(
x
)
l
o
g
(
p
^
(
x
,
θ
)
)
+
[
1
−
p
(
x
)
]
l
o
g
[
1
−
p
^
(
x
,
θ
)
]

通过最大化该对抗损失即可进一步促使两种模特的低维特征无法区分，趋向于同一子空间。

然后对于Code Layer得到的低维特征，还是要保留语义的，即是可以用来进行分类的，与通常的多分类任务一样。使用一个带有softmax层的全连接层作为一个公共的分类层。其分类损失为：

L





d


i

s


=




1






N



s





∑





i

=

1








N



s


(




f





s

o

f


t

m

a

x


(




v



i


,




y



i


;




Θ



d


)

+




f





s

o

f


t

m

a

x


(




t



i


,




y



i


;




Θ



d


)

)






(5)


(5)
L
d
i
s
=
1
N
s
∑
i
=
1
N
s
(
f
s
o
f
t
m
a
x
(
v
i
,
y
i
;
Θ
d
)
+
f
s
o
f
t
m
a
x
(
t
i
,
y
i
;
Θ
d
)
)

  
其中：
  



f





s

o

f


t

m

a

x


(

x

,

y


;

θ

)

=




∑





k

=

1





c


1


{


y


=

k

}

l

o

g



[







p



^


(

x

,

k

,

θ

)

]






(6)


(6)
f
s
o
f
t
m
a
x
(
x
,
y
;
θ
)
=
∑
k
=
1
c
1
{
y
=
k
}
l
o
g
[
p
^
(
x
,
k
,
θ
)
]

博主觉得，如何将类别的属性信息进行编码嵌入网络是这篇文章的亮点。关于零样本学习，是个诱人深入学习的领域呢。

大家如果感兴趣可以结合论文进一步理解，传送门：
  
<https://dl.acm.org/citation.cfm?id=3206033>