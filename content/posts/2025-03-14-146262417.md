---
arturl_encode: "6874747073:3a2f2f626c6f672e6373646e2e6e65742f737072696e67352f:61727469636c652f64657461696c732f313436323632343137"
layout: post
title: "DataWhale-大语言模型-长上下文模型和新型架构"
date: 2025-03-14 17:31:26 +08:00
description: "全局和局部自注意力（Global and Local Self-Attention）：Longformer结合了全局自注意力和局部自注意力，全局注意力用于处理序列中的关键部分，而局部注意力则用于处理更广泛的上下文。相对位置编码（Relative Positional Encoding）：相对于原始Transformer的绝对位置编码，Transformer-XL使用相对位置编码，这使得模型能够更好地处理长序列。随着研究的深入，未来可能会有更多创新的模型架构被提出，以更好地解决长文本处理的问题。"
keywords: "DataWhale 大语言模型 - 长上下文模型和新型架构"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "146262417"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146262417
    alt: "DataWhale-大语言模型-长上下文模型和新型架构"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146262417
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146262417
cover: https://bing.ee123.net/img/rand?artid=146262417
image: https://bing.ee123.net/img/rand?artid=146262417
img: https://bing.ee123.net/img/rand?artid=146262417
---

# DataWhale 大语言模型 - 长上下文模型和新型架构

本课程围绕中国人民大学高瓴人工智能学院赵鑫教授团队出品的《大语言模型》书籍展开，覆盖大语言模型训练与使用的全流程，从预训练到微调与对齐，从使用技术到评测应用，帮助学员全面掌握大语言模型的核心技术。并且，课程内容基于大量的代码实战与讲解，通过实际项目与案例，学员能将理论知识应用于真实场景，提升解决实际问题的能力。

课程地址：https://www.datawhale.cn/learn/summary/107

赵鑫教授团队：http://aibox.ruc.edu.cn/

视频课程地址：
[《大语言模型》2.3 长上下文模型和新型架构\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1LxRKYQEqE?vd_source=9f2d252c81af9097ef70bb0c9b830406&spm_id_from=333.788.videopod.sections "《大语言模型》2.3 长上下文模型和新型架构_哔哩哔哩_bilibili")

![](https://i-blog.csdnimg.cn/direct/f1bca55ba035421f8ed3dc2e16c03073.png)

长上下文模型是指能够处理和理解长序列文本的神经网络模型。由于传统的循环神经网络（RNN）和长短时记忆网络（LSTM）等模型在处理长序列时存在梯度消失和内存限制等问题，研究人员开发了多种新型架构来克服这些挑战。以下是一些新型长上下文模型的架构：

**1. Transformer模型**

自注意力机制（Self-Attention）：Transformer模型通过自注意力机制来同时处理序列中的所有单词，而不是像RNN那样依次处理。这有助于模型捕捉长距离依赖关系。

位置编码（Positional Encoding）：为了保留序列中单词的位置信息，Transformer模型引入了位置编码。

**2. Transformer-XL**

段级重复注意力（Segment-Level Recurrence）：Transformer-XL通过利用相邻段落之间的重复注意力来处理长文本，从而减少了计算量和内存需求。

相对位置编码（Relative Positional Encoding）：相对于原始Transformer的绝对位置编码，Transformer-XL使用相对位置编码，这使得模型能够更好地处理长序列。

**3. Compressive Transformer**

记忆压缩（Memory Compression）：Compressive Transformer通过压缩过去的上下文信息来减少内存使用，同时保留关键信息。

**4. Longformer**

全局和局部自注意力（Global and Local Self-Attention）：Longformer结合了全局自注意力和局部自注意力，全局注意力用于处理序列中的关键部分，而局部注意力则用于处理更广泛的上下文。

**5. Big Bird**

稀疏注意力（Sparse Attention）：Big Bird使用了多种类型的注意力机制，包括稀疏注意力，以处理长序列而不会显著增加计算复杂度。

**6. Reformer**

可逆层（Reversible Layers）：Reformer通过使用可逆层来减少内存使用，因为它不需要存储中间激活。

局部敏感哈希（Locality Sensitive Hashing, LSH）：Reformer使用LSH来高效地计算长序列中的自注意力。

**7. Performer**

随机特征方法（Random Feature Methods）：Performer利用随机特征方法来近似自注意力，从而将计算复杂度从二次降低到线性。

**8. GPT-3和Switch Transformer**

稀疏激活（Sparse Activation）：GPT-3和Switch Transformer等模型通过仅在序列的子集上激活注意力机制来处理长文本，这有助于减少计算和内存需求。

这些新型架构各有特点，它们通过不同的技术手段来提高模型处理长上下文的能力。随着研究的深入，未来可能会有更多创新的模型架构被提出，以更好地解决长文本处理的问题。