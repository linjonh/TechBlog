---
arturl_encode: "68747470733a2f2f626c:6f672e6373646e2e6e65742f6b6f646f7368696e696368692f:61727469636c652f64657461696c732f313039383930323236"
layout: post
title: "最优化一维搜索技术"
date: 2023-11-25 14:10:54 +08:00
description: "讲述了最优化领域中的一维搜索技术：二分法、区间等分法(两点、三点)、Fibonacci搜索、黄金分割"
keywords: "一维搜索二分法"
categories: ['数学']
tags: ['牛顿法', '最优化', '数学', '区间等分法', '二分法']
artid: "109890226"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=109890226
    alt: "最优化一维搜索技术"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=109890226
featuredImagePreview: https://bing.ee123.net/img/rand?artid=109890226
---

# 【最优化】一维搜索技术

## 一维搜索技术

> 在第二篇博文
> [《【最优化】最优化的相关条件》](https://blog.csdn.net/kodoshinichi/article/details/109839911)
> 中，我们讲述了函数局部(全局)极值点所具有的充分条件、必要条件以及充分必要条件；但在现实复杂问题的背景下，我们很难使用“一阶导数为0”这个条件进行求解。
>   
>   
> 本节我们就要学习如何利用计算机进行最优值的求解，先从最基础的一维函数讲起。
>   
> 后续我们会学习到，多元函数的优化过内含了一元函数的优化过程。

---

### 各类优化算法的基本思路

**1. 问题抽象——盲人爬山**

简单来说，想象有一个盲人拄着拐杖想要爬到山顶(或回到山脚)，他只能通过拐杖去探测当前他所在的位置的局部信息，从而决定当前最优的行走方向，循环往复，直到达到他的目的地。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1db773a160fa29cfbe80ad1661c2f9c3.png#pic_center)

> * 盲人与拐杖：因为很多时候即使我们知道目标函数的解析式，也无法清晰地获知图像的性状与全局性质；所以对于图像的全局性质来说，我们是“不可见”的。
> * 用拐杖探测局部信息：我们知道当前点的某些邻域内的函数值，知道该邻域内的某阶梯度值，利用这些信息进行决策。

**2. 逐次下降法(Iterative Descent)**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a422b08ea965e52cf0f8570fac4e7544.png#pic_center)

* 从一个初始点x
  0
  出发，计算该点的局部信息
* 通过这些信息进行决策，选择一个合适的搜索方向
* 沿着该搜索方向找到下一个点x
  1
  ，并围绕新的点计算局部信息
* 重复上述过程，找到一系列点x
  i
  (i = 1,2,…n)，保证每次f(x)都能减小(如果目标是最小化函数)

**3. 考虑的问题**

①怎么选择最优方向？
  
②沿着最优方向走多远？
  
③怎样保证算法一定能收敛到最优点？
  
④算法收敛到最优点的速度怎样？

---

### 单峰函数

**1. 单峰函数的定义**

假设函数f(x)在区间[a,b]内的x
*
处有一个极小点：

那么说f(x)
**在区间[a,b]上是单峰的**
，即意味着对于区间[a,b]上的任意一个点x，
**当x趋向于x
*
的时候，f(x)的函数值都是减小的**
。

> 通俗地来说，也就是在区间[a,b]内只能有一个极小值点。
>   
>   
> p.s. 本节我们对于一维搜索技术的讨论都是围绕单峰函数而言的。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c286c24a452441efbdbb619485980008.png#pic_center)

**2. 重要结论**
  
假设已知f(x)是区间[a,b]上的单峰函数，而且在x
*
∈[a,b]处取得最小值；
  
为了可以取到一个包含极小值点x
*
的更小的子区间（该子区间在[a,b]内），至少需要再
**计算两次函数值**
。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/3e13be362028a93ac1dd49095d97b399.png#pic_center)
  
p.s. 该结论在后续的搜索技术应用中十分重要。

---

### （一） 二分法

**1. 基本思想**

> 根据上文提到的有关需要计算两个新的点，由此来决定新的区间的范围；
>   
> 二分法就是通过区间中值来确定新区间的两个端点值。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/25fbb6321d41e706e43c82060b7d0a59.png#pic_center)
  
**2. 示例**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/770bc94f3488a022a71c5176e90448f7.png#pic_center)

> 【ε的含义】
>   
> 设第n次迭代得到的区间长度为L
> n
> ,则根据上述的推导可知L
> n+1
> = (L
> n
> +ε）/2，根据收敛性推导可知，最终得到的区间长度不会超过ε，因此
> **ε应该设定为可以接受的误差上界**
> 。

p.s.上述迭代过程思路十分清晰，可以
**很容易地使用计算机语言进行实现**
。

---

### （二）区间等分法

> 因为在上述二分法的思想中，新区间的端点值的确定要经历求中值点→中值点左右取邻域→两邻域点函数值比较的过程，过于繁琐；所以我们想到直接对区间进行划分。

**1. 两点等分法**

> 既然要在区间内找到两个点，那么就需要对原区间进行三等分。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9b57a1ccde8ead5ecd828b67bb3d9125.png#pic_center)
  
**2. 三点等分法**

> 基于两点等分法“计算速率较慢”这一缺点，我们进一步提出了三点等分法，每一次将区间进行四等分，从而得
> **出了三个四等分点作为候选点**
> 。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/80f704e175ff422935f777bc9658eb1e.png#pic_center)
  
**3. 计算量的比较**

> 上述【迭代速率】的比较只是基于区间缩短的快慢而言的，但这并不代表算法快慢的全部，否则读者很容易会产生一个误区——那是不是只要将区间进行越多的等分，那么得到的子区间自然就会缩减的越多？
>   
> ——但同时也别忘记了，进行更多的等分，需要计算更多的等分点以及该等分点位置上的函数值。
>   
>   
> 以下我们定性地分析一下两点等分和三点等分整体的算法效率

对于两点等分而言，按照2/3的比率缩减区间，且每轮缩减需要计算两个点；
  
对于三等分点而言，按照1/2的比率缩减区间，且
**除了第一轮需要计算三个点以外，其余每轮只需要计算两个点**
。

综上，我们可知【三点等分法】的效率是要高于【两点等分法】的。

---

【例】三点等分法进行最优点的搜索
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/fc39fb8c0427e286eae68496e84ad2d2.png#pic_center)

---

### （三）斐波拉契搜索法

> 从减少计算量的角度出发，为了确定新的区间，每一轮必须有两个候选点（前面给出的结论），有没有可能做到每一轮只需要重新计算一个点，另外一个点可以利用前面已经计算过的结果来实现呢？

**1. “Fibonacci”的由来**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1ea4eaa8f650b3e84e58d03c8778f1b9.png#pic_center)
  
因为采用的是二分搜索，所以区间长度的对应关系应该为：L
n
= (L
n-1
+ε)/2；也就是，L
n-1
= 2L
n
+ε。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/cd9b5b475a9712c483808f22bfe18660.png#pic_center)
  
从而得到区间长度收敛的“Fibonacci”关系
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9ba756a45300d7f0b63efb0ac88c333a.png#pic_center)
  
**2. 算法过程**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/f719faa4b3378a9b8d219b066755122a.png#pic_center)
  
**3. 示例**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/af7fa7b69d9379fde2ff7420ace7a503.png#pic_center)
  
**4. 评价**

（1）优点
  
①在同样的轮数下，它可以将搜索区间缩小到一个最小的长度
  
②没有使用到梯度相关的计算
  
③方便计算机实现
  
④每一轮迭代只需要保留四个点
  
⑤可以事先计算出搜索的误差和不确定性

（2）缺点
  
计算前迭代次数n和区间摇摆微小量ε需要事先确定
  
p.s. 这个缺点也可以通过【直接进行迭代计算，当搜索区间“充分小”时就停止迭代】这一机制进行克服；当然这样肯定会损失部分效率。

---

### （四）黄金分割法（0.618法）

**1. 基本思想**
  
**新区间与旧区间的区间长度保持一个常数不变**
(在Fibonacci Search中新旧区间的长度并不是一个常数)

> 比较来看，前面的二分法和区间等分法所得到的新旧区间的长度确实是一个常数，但他们每一次迭代计算新的候选点的数目较多；我们期望的黄金分割法可以在保留“
> **尽可能地少计算新的候选点**
> ”的优点下，保持新旧区间的长度不变。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/258fa11efb99bfead9864650bcc711c3.png#pic_center)
  
基于上面的数学关系，在每一轮迭代时都应该满足该等式。经过整理（数学过程略去），可以得到关于比例系数λ的一元二次方程：
**λ
2
+λ-1 = 0**
，求解后保留正根，则可得到λ = 0.618（这也是【0.618法】这个名字的来源）

**2. 计算效率**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1585daf3e7e5e33d7274909f758487ea.png#pic_center)
  
**3. 示例**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a12f1cfcb9e67ff1c426984250cd06cd.png#pic_center)

> 黄金分割法的效率其实也要比三点等分法的效率更高：
>   
> 看起来三点等分法，每一轮迭代都是把区间缩小至原来的一半，但是
> **在计算点数相同的情况下，黄金分割法的效率更高**
> （评价指标主要是区间缩小的程度）。

**4. 现有的搜索算法的总结**

①目前来说讨论的所有一维搜索技术都可以很容易地编程实现
  
②但是为了达到一个需要的较高精度，这些算法都需要经过较多轮次的迭代

> 在实际应用中，我们其实
> **并不需要一维搜索得到的结果的精度有多高**
> 。比如在多元优化问题中，我们常常把一个多元优化问题分解成若干个一元优化问题，我们只需要得到各一元优化问题粗略的近似解即可。
>   
>   
> 多项式类（尤其是二元函数）函数的最优化点是很好求解的，因此我们想——是否可以
> **把我们所需优化求解的任意函数都先用多项式函数近似，通过求解多项式函数的最优解，从而得到原函数的近似最优解**
> 。

---

### （五）二插法(Quadratic Interpolation)

**1. 算法思想与原型**

> 下面的讲述以最小化问题为例

对于当前待优化函数f(x)，我们在给定的初始点x
0
处，根据该点的下降梯度v，要找到一个点x
1
= x
0
+λ
1
v，其中参数λ
1
必须要保证λ
1
使得函数f(x
0
+λv)的值关于λ参数最小。

根据上面的描述，我们可以把这个最小化问题等价于对标量函数g(λ) = f(x
0
+λv)关于自变量λ的最小化问题。

> 【参数规定】
>   
> 因为给定的方向v已经确定是函数f(x)在x
> 0
> 点的下降梯度方向，所以要求λ≥0
>   
> 同时为了防止在搜索的时候跨步过大导致发散等问题的出现，我们制定了参数λ的上界m。

综上，原来给定的关于函数f(x)的一维优化搜索的问题就转换成了
  
**对于函数g(λ) = f(x
0
+λv)，当0≤λ≤m时的最小化问题**
。

**2. 二插法的算法框架**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/3dc3fa6bf06ecf42affc41afdb969390.png#pic_center)

* 不论是按照2
  λ
  的规律向外拓展，还是按照(1/2)
  λ
  向内拓展，这些操作的实质都是为了可以找到三个点，保证拟合出来的二次曲线具有“先下降后上升”的趋势，从而保证了该二次曲线最小值的存在性。
* 两种情况下，计算除了a,b,c之后，按照公式可以得到候选最优点λ
  hat
  的值，还要讲该点的函数值和b点的函数值进行比较，才能确定最终的近似解取二者中的哪一个。
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ac6b7f51d0e6fd0ae93ed7bc77b260b4.png#pic_center)
    
  **3. 示例**
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/b800b40423a6ac321e486f188d40a7ae.png#pic_center)

> 观察上面两个例子的求解过程，即可知道其求解效率较高，但是精度较低；但这对于未来进行的多函数优化问题并不是大的缺点。

---

### （六）牛顿法

> 前面讲过的所有算法都没有涉及到【导数】或者【梯度】的概念，牛顿法对此会有此涉及。

**1. 算法思路**

* 该算法建立在所求函数必须是
  **至少二阶连续可微**
  的前提下
* 牛顿法在数值分析中被用于求解方程f(x) = 0的近似解，通过前面的讲解，我们知道一个函数的候选最优点往往会落在其驻点（一阶导数为0的点）上，因此我们可以
  **利用牛顿法来求解一阶导数的零点**
  。
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/fa1a663960fa2beac0e662f90f2814d7.png#pic_center)
* 之前的方法是从一个区间往后搜素下一个区间，而牛顿法是从一个点迭代搜索下一个点

**2. 相关结论**
  
①当H(x)是正定阵时(函数f(x)对应就是严格凸函数)，算法的收敛速度很快，且与算法的初始点无关。

> 有关凸函数定义、凸函数的凸性及相关定理的讨论可以参考上一篇博文
> [《【最优化】最优化理论的基本概念》](https://blog.csdn.net/kodoshinichi/article/details/109736488)

②但是牛顿法的一个缺点就在于其每一轮迭代都需要计算二阶导数，计算量过大。

> 但是对于一个
> **二次函数f(x) = (1/2)x
> T
> Gx+b
> T
> x+c**
> 来说，它的二阶导对应的Hessian矩阵是一个常矩阵(与x无关)；
>   
> 很显然，根据矩阵的求导运算可以得到：
>   
> **▽f(x) = Gx+b
>   
> ▽
> 2
> f(x) = G**
>   
> 其二阶导数的确是一个常数矩阵G，那么代入到上图中的迭代公式中，在每一轮迭代只需要求解函数在x
> n
> 点处的导数即可。

③基于第二点，我们在使用牛顿法的时候，常常把函数近似成二次函数的形式：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4f2345510146199892ce76c93b3c080c.png#pic_center)
  
**3. 示例**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/493f89e4e80dd497a9925313a10b96e9.png#pic_center)