---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f6a696e6565712f:61727469636c652f64657461696c732f313232323830373836"
layout: post
title: "Python爬虫抓取东方财富网股票数据并实现MySQL数据库存储转"
date: 2024-11-30 14:47:58 +0800
description: "本文介绍了如何使用Python爬虫从东方财富网抓取股票数据，然后将数据保存到本地CSV文件，接着将数"
keywords: "东方财富网 爬取"
categories: ['未分类']
tags: ['爬虫', '数据库', 'Python']
artid: "122280786"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=122280786
  alt: "Python爬虫抓取东方财富网股票数据并实现MySQL数据库存储转"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=122280786
featuredImagePreview: https://bing.ee123.net/img/rand?artid=122280786
---

# Python爬虫抓取东方财富网股票数据并实现MySQL数据库存储（转）

> Python爬虫可以说是好玩又好用了。现想利用Python爬取网页股票数据保存到本地csv数据文件中，同时想把股票数据保存到MySQL数据库中。需求有了，剩下的就是实现了。

在开始之前，保证已经安装好了MySQL并需要启动本地MySQL数据库服务。提到安装MySQL数据库，前两天在一台电脑上安装MySQL5.7时，死活装不上，总是提示缺少Visual Studio 2013 Redistributable，但是很疑惑，明明已经安装了呀，原来问题出在版本上，更换一个版本后就可以了。小问题大苦恼，不知道有没有人像我一样悲催。

言归正传，启动本地数据库服务：
  
用管理员身份打开“命令提示符（管理员）”，然后输入“net start mysql57”（我把数据库服务名定义为mysql57了，安装MySQL时可以修改）就可以开启服务了。注意使用管理员身份打开小黑框，如果不是管理员身份，我这里会提示没有权限，大家可以试试。
  
启动服务之后，我们可以选择打开“MySQL 5.7 Command Line Client”小黑框，需要先输入你的数据库的密码，安装的时候定义过，在这里可以进行数据库操作。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/70a57319c756e2fa906e47a37a96bc2b.gif#pic_center)
  
下面开始上正餐。

### 一、Python爬虫抓取网页数据并保存到本地数据文件中

首先导入需要的数据模块，定义函数：

```python
#导入需要使用到的模块
import urllib
import re
import pandas as pd
import pymysql
import os

#爬虫抓取网页函数
def getHtml(url):
html = urllib.request.urlopen(url).read()
html = html.decode('gbk')
return html

#抓取网页股票代码函数
def getStackCode(html):
s = r'<li><a target="_blank" href="http://quote.eastmoney.com/\S\S(.*?).html">'
pat = re.compile(s)
code = pat.findall(html)
return code

```

真正干活的代码块：

```python
Url = 'http://quote.eastmoney.com/stocklist.html'#东方财富网股票数据连接地址
filepath = 'D:\\data\\'#定义数据文件保存路径
#实施抓取
code = getStackCode(getHtml(Url))
#获取所有股票代码（以 6 开头的，应该是沪市数据）集合
CodeList = []
for item in code:
if item[0]=='6':
CodeList.append(item)
#抓取数据并保存到本地 csv 文件
for code in CodeList:
print('正在获取股票%s 数据'%code)
url = 'http://quotes.money.163.com/service/chddata.html?code=0'+code+\
 '&end=20161231&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP'
urllib.request.urlretrieve(url, filepath+code+'.csv')

```

以上代码实现了爬虫网页抓取股票数据，并保存到本地文件中。关于爬虫的东西，有很多资料可以参考，大都是一个套路，不再多说。同时，本文实现过程中也参考了很多的网页资源，在此对所有原创者表示感谢！

先看下抓取的结果。CodeList 是抓取到的所有股票代码的集合，我们看到它共包含 1416 条元素，即 1416 支股票数据。因为股票太多，所以抓取的是以 6 开头的，貌似是沪市股票数据（原谅我不懂金融）。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/bb11e8bcb5145c9da8177ee13c323cb7.png)

抓取到的股票数据会分别存储到 csv 文件中，一只股票数据一个文件。理论上会有 1416 个 csv 文件，和股票代码数一致。但原谅我的渣网速，下载一个都费劲，也是呵呵了。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/f7ef5584d389e34d5352f199c9615f9c.gif#pic_center)
  
打开一个本地数据文件看一下抓取的数据长什么样子：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/dc134dd8f040d0f8d84f42ee0d3db66b.gif#pic_center)
  
其实和人工手动下载也没什么区别了，硬要说区别，那就是解放了劳动力，提高了生产力（怎么听起来像政治？）。

### 二、将数据存储到 MySQL 数据库

首先建立本地数据库连接：

```python
#数据库名称和密码
name = 'xxxx'
password = 'xxxx' #替换为自己的用户名和密码
#建立本地数据库连接(需要先开启数据库服务)
db = pymysql.connect('localhost', name, password, charset='utf8')
cursor = db.cursor()

```

其中，数据库名称(name)和密码(password)是安装 MySQL 时设置的。

创建数据库，专门用来存储本次股票数据：

```python
#创建数据库 stockDataBase，如果存在则跳过
sqlSentence1 = "create database if not exists stockDataBase"
cursor.execute(sqlSentence1)#选择使用当前数据库
sqlSentence2 = "use stockDataBase;"
cursor.execute(sqlSentence2)

```

在首次运行的时候一般都会正常创建数据库，但如果再次运行，因数据库已经存在，那么跳过创建，继续往下执行。创建好数据库后，选择使用刚刚创建的数据库，在该数据库中存储数据表。

下面看具体的存储代码：

```python
#获取本地文件列 fileList = os.listdir(filepath)#依次对每个数据文件进行存储
for fileName in fileList:
data = pd.read_csv(filepath+fileName, encoding="gbk")
#创建数据表，如果数据表已经存在，会跳过继续执行下面的步骤 print('创建数据表 stock_%s'% fileName[0:6])
sqlSentence3 = "create table if not exists stock*%s" % fileName[0:6] + "(日期 date, 股票代码 VARCHAR(10), 名称 VARCHAR(10), 收盘价 float,\ 最高价 float, 最低价 float, 开盘价 float, 前收盘 float, 涨跌额 float, 涨跌幅 float, 换手率 float,\ 成交量 bigint, 成交金额 bigint, 总市值 bigint, 流通市值 bigint)"
cursor.execute(sqlSentence3)#迭代读取表中每行数据，依次存储（整表存储还没尝试过）
print('正在存储 stock*%s'% fileName[0:6])
length = len(data)
for i in range(0, length):
record = tuple(data.loc[i])
#插入数据语句
try:
sqlSentence4 = "insert into stock\_%s" % fileName[0:6] + "(日期, 股票代码, 名称, 收盘价, 最高价, 最低价, 开盘价,\ 前收盘, 涨跌额, 涨跌幅, 换手率, 成交量, 成交金额, 总市值, 流通市值) \ values ('%s',%s','%s',%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)" % record
#获取的表中数据很乱，包含缺失值、Nnone、none 等，插入数据库需要处理成空值
sqlSentence4 = sqlSentence4.replace('nan','null').replace('None','null').replace('none','null')
cursor.execute(sqlSentence4)
except:#如果以上插入过程出错，跳过这条数据记录，继续往下进行
break

```

代码并不复杂，只要注意其中几个点就好了。

**1.逻辑层次：**
  
包含两层循环，外层循环是对股票代码的循环，内层循环是对当前股票的每一条记录的循环。说白了就是按照股票一支一支的存储，对于每一支股票，按照它每日的记录一条一条的存储。是不是很简单很暴力？是的！完全没有考虑更加优化的方式。

**2.读取本地数据文件的编码方式：**

```
使用'gbk'编码，默认应该是'utf8'，但好像不支持中文。

```

**3.创建数据表：**
  
同样的，如果数据表已经存在（判断是否存在 if not exists），则跳过创建，继续执行下面的步骤（会继续存储）。有个问题是，有可能数据重复存储，可以选择跳过存储或者只存储最新数据。我在这里没有考虑太多额外的处理。其次，指定字段格式，后边几个字段成交量、成交金额、总市值、流通市值，因为数据较大，选择使用 bigint 类型。

**4.没有指定数据表的主键：**
  
最初是打算使用日期作为主键的，后来发现获取到的数据中竟然包含重复日期的数据，这就打破了主键的唯一性，会出 bug 的，然后我也没有多去思考数据文件的内容，也不会进一步使用这些个数据，也就图省事直接不设置主键了。

**5.构造 sql 语句 sqlSentence4：**
  
该过程实现中，直接把股票数据记录 tuple 了，然后使用字符串格式化（%操作符）。造成的精度问题没有多考虑，不知道会不会产生什么样的影响。%s 有的上边带着’ ‘，是为了在 sql 语句中表示字符串。其中有一个%s’，只有右边有单引号，匹配的是股票代码，只有一边单引号，这是因为从数据文件中读取到的字符串已经包含了左边的单引号，左边不需要再添加了。这是数据文件格式的问题，为了表示文本形式预先使用了单引号。

**6.异常值处理：**

```
文本文件中，包含有空值、None、none 等不标准化数据，这里全部替换为 null 了，即数据库的空值。

```

完成 MySQL 数据库数据存储后，需要关闭数据库连接：

```python
#关闭游标，提交，关闭数据库连接
cursor.close()
db.commit()
db.close()

```

不关闭数据库连接，就无法在 MySQL 端进行数据库的查询等操作，相当于数据库被占用。

### 三、MySQL 数据库查询

```python
#重新建立数据库连接
db = pymysql.connect('localhost', name, password, 'stockDataBase')
cursor = db.cursor()
#查询数据库并打印内容
cursor.execute('select _ from stock_600000')
results = cursor.fetchall()
for row in results:
print(row)
#关闭
cursor.close()
db.commit()
db.close()

```

以上逐条打印，会凌乱到死的。也可以在 MySQL 端查看，先选中数据库：use stockDatabase;，然后查询：select \_ from stock\_600000;，结果大概就是下面这个样子了：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7d599002027c314ab8a9b66eafbaa8b2.gif#pic_center)

### 四、完整代码

.
  
实际上，整个事情完成了两个相对独立的过程：1.爬虫获取网页股票数据并保存到本地文件；2.将本地文件数据储存到 MySQL 数据库。并没有直接的考虑把从网页上抓取到的数据实时（或者通过一个临时文件）扔进数据库，跳过本地数据文件这个过程。这里只是尝试着去实现了一下这件事情，代码没有做任何的优化考虑。本身不实际去使用，只是乐趣而已，差不多先这样。哈哈~~

```python
#导入需要使用到的模块
import urllib
import re
import pandas as pd
import pymysql
import os

#爬虫抓取网页函数
def getHtml(url):
html = urllib.request.urlopen(url).read()
html = html.decode('gbk')
return html

#抓取网页股票代码函数
def getStackCode(html):
s = r'<li><a target="\_blank" href="http://quote.eastmoney.com/\S\S(.*?).html">'
pat = re.compile(s)
code = pat.findall(html)
return code

#########################开始干活############################
Url = 'http://quote.eastmoney.com/stocklist.html'#东方财富网股票数据连接地址
filepath = 'C:\\Users\\Lenovo\\Desktop\\data\\'#定义数据文件保存路径
#实施抓取
code = getStackCode(getHtml(Url))
#获取所有股票代码（以 6 开头的，应该是沪市数据）集合
CodeList = []
for item in code:
if item[0]=='6':
CodeList.append(item)
#抓取数据并保存到本地 csv 文件
for code in CodeList:
print('正在获取股票%s 数据'%code)
url = 'http://quotes.money.163.com/service/chddata.html?code=0'+code+\
 '&end=20161231&fields=TCLOSE;HIGH;LOW;TOPEN;LCLOSE;CHG;PCHG;TURNOVER;VOTURNOVER;VATURNOVER;TCAP;MCAP'
urllib.request.urlretrieve(url, filepath+code+'.csv')

##########################将股票数据存入数据库###########################

#数据库名称和密码
name = 'xxxx'
password = 'xxxx' #替换为自己的账户名和密码
#建立本地数据库连接(需要先开启数据库服务)
db = pymysql.connect('localhost', name, password, charset='utf8')
cursor = db.cursor()
#创建数据库 stockDataBase
sqlSentence1 = "create database stockDataBase"
cursor.execute(sqlSentence1)#选择使用当前数据库
sqlSentence2 = "use stockDataBase;"
cursor.execute(sqlSentence2)

#获取本地文件列表
fileList = os.listdir(filepath)
#依次对每个数据文件进行存储
for fileName in fileList:
data = pd.read*csv(filepath+fileName, encoding="gbk")
#创建数据表，如果数据表已经存在，会跳过继续执行下面的步骤 print('创建数据表 stock*%s'% fileName[0:6])
sqlSentence3 = "create table stock\_%s" % fileName[0:6] + "(日期 date, 股票代码 VARCHAR(10), 名称 VARCHAR(10),\
 收盘价 float, 最高价 float, 最低价 float, 开盘价 float, 前收盘 float, 涨跌额 float, \
 涨跌幅 float, 换手率 float, 成交量 bigint, 成交金额 bigint, 总市值 bigint, 流通市值 bigint)"
cursor.execute(sqlSentence3)
except:
print('数据表已存在！')

    #迭代读取表中每行数据，依次存储（整表存储还没尝试过）
    print('正在存储stock_%s'% fileName[0:6])
    length = len(data)
    for i in range(0, length):
        record = tuple(data.loc[i])
        #插入数据语句
        try:
            sqlSentence4 = "insert into stock_%s" % fileName[0:6] + "(日期, 股票代码, 名称, 收盘价, 最高价, 最低价, 开盘价, 前收盘, 涨跌额, 涨跌幅, 换手率, \
            成交量, 成交金额, 总市值, 流通市值) values ('%s',%s','%s',%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)" % record
            #获取的表中数据很乱，包含缺失值、Nnone、none等，插入数据库需要处理成空值
            sqlSentence4 = sqlSentence4.replace('nan','null').replace('None','null').replace('none','null')
            cursor.execute(sqlSentence4)
        except:
            #如果以上插入过程出错，跳过这条数据记录，继续往下进行
            break

#关闭游标，提交，关闭数据库连接
cursor.close()
db.commit()
db.close()

###########################查询刚才操作的成果##################################

#重新建立数据库连接
db = pymysql.connect('localhost', name, password, 'stockDataBase')
cursor = db.cursor()
#查询数据库并打印内容
cursor.execute('select \* from stock_600000')
results = cursor.fetchall()
for row in results:
print(row)
#关闭
cursor.close()
db.commit()
db.close()

```

转载于：https://www.cnblogs.com/dennis-liucd/p/7669161.html