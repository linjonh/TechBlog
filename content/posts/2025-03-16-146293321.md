---
arturl_encode: "68747470733a2f2f626c6f67:2e6373646e2e6e65742f4c6f756973655f5472656e6465722f:61727469636c652f64657461696c732f313436323933333231"
layout: post
title: "llama-factory笔记"
date: 2025-03-16 12:22:55 +08:00
description: "RoPE 插值方法能提升长文本效果，如果没有特殊需求可以不使用（yarn最好，dynamic能动态变化，linear适合所有文本长度相近的情况且耗时最短）加速方法：内置了flash_attention，auto即为flash_attention，unsloth更适合显存低的情况；flash_attention和unsloth主要用在训练过程中，vllm主要用在推理过程中Qlora框架下的具体量化方法：bitsandbytes直接内置比较方便，hqq在显存极低情况下更好用（显存占用下降更多），但需要安装。"
keywords: "llama-factory笔记"
categories: ['未分类']
tags: ['笔记', 'Llama']
artid: "146293321"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146293321
    alt: "llama-factory笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146293321
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146293321
cover: https://bing.ee123.net/img/rand?artid=146293321
image: https://bing.ee123.net/img/rand?artid=146293321
img: https://bing.ee123.net/img/rand?artid=146293321
---

# llama-factory笔记

## llama-factory笔记

RoPE 插值方法能提升长文本效果，如果没有特殊需求可以不使用（yarn最好，dynamic能动态变化，linear适合所有文本长度相近的情况且耗时最短）

加速方法：内置了flash_attention，auto即为flash_attention，unsloth更适合显存低的情况；flash_attention和unsloth主要用在训练过程中，vllm主要用在推理过程中

Qlora框架下的具体量化方法：bitsandbytes直接内置比较方便，hqq在显存极低情况下更好用（显存占用下降更多），但需要安装

Qlora最低容忍度为4bit量化，损失在4-8%（8bit损失＜1%），但3比特及以下都有两位数的损失

Qlora只是在微调过程中牺牲精度来量化微调，模型本身依然是全精度的

微调后量化使用的导出量校准数据集格式为[{“text”:“abc”},…]

导出设备选择auto，cpu可能报错；大模型文件导出可能报错，需要修改最大分块大小，即单个模型文件的最大大小（GB）。

evaluate主要比的是BLEU相似度，不是准确率

截断长度对显存占用影响较大，若数据集长度固定，可以降低到max_length甚至低于max_length(抛弃长文本数据)

### 记录

**Device**
: AMD 7940HX + 4060(8GB) + 64GB RAM
  
**数据量**
：692240条

#### 1epoch Qwen2.5-1.5B-Instruct 模型本身2.87GB

batch_size=1 86530item 截断长度1024 时长43h 显存3.7GB
  
QLoRA未启用 加速方式flashattn

batch_size=4 21632item 截断长度256 时长11.5h 显存3.9GB
  
QLoRA未启用 加速方式flashattn

batch_size=4 21632item 截断长度256 时长15.5h 显存2.1GB
  
QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn

batch_size=4 21632item 截断长度256 时长32h 显存2.7GB
  
QLoRA启用(8bit) 量化方法bitsandbytes 加速方式flashattn

#### 1epoch Qwen2.5-7B-Instruct 模型本身14.1GB

batch_size=1 86530item 截断长度256 时长700h 显存14.5GB
  
QLoRA未启用 量化方法bitsandbytes 加速方式flashattn

batch_size=1 86530item 截断长度256 时长174h 显存6.2GB
  
QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn

#### 1epoch Qwen2.5-3B-Instruct 模型本身5.74GB

batch_size=1 86530item 截断长度256 时长58h 显存6.7GB
  
QLoRA未启用 量化方法bitsandbytes 加速方式flashattn

batch_size=1 86530item 截断长度256 时长84h 显存2.6GB
  
QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn

batch_size=4 21632item 截断长度256 时长27h 显存3.1GB
  
QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn

batch_size=16 5409item 截断长度256 时长16h 显存5.1GB
  
QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn

batch_size=32 2704item 截断长度256 时长16h 显存7.1GB
  
QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn