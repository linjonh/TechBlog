---
layout: post
title: "llama-factory笔记"
date: 2025-03-16 12:22:55 +0800
description: "RoPE 插值方法能提升长文本效果，如果没有特殊需求可以不使用（yarn最好，dynamic能动态变化，linear适合所有文本长度相近的情况且耗时最短）加速方法：内置了flash_attention，auto即为flash_attention，unsloth更适合显存低的情况；flash_attention和unsloth主要用在训练过程中，vllm主要用在推理过程中Qlora框架下的具体量化方法：bitsandbytes直接内置比较方便，hqq在显存极低情况下更好用（显存占用下降更多），但需要安装。"
keywords: "llama-factory笔记"
categories: ['未分类']
tags: ['笔记', 'Llama']
artid: "146293321"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146293321
    alt: "llama-factory笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146293321
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146293321
cover: https://bing.ee123.net/img/rand?artid=146293321
image: https://bing.ee123.net/img/rand?artid=146293321
img: https://bing.ee123.net/img/rand?artid=146293321
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     llama-factory笔记
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-github-gist" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="llamafactory_0">
     </a>
     llama-factory笔记
    </h2>
    <p>
     RoPE 插值方法能提升长文本效果，如果没有特殊需求可以不使用（yarn最好，dynamic能动态变化，linear适合所有文本长度相近的情况且耗时最短）
    </p>
    <p>
     加速方法：内置了flash_attention，auto即为flash_attention，unsloth更适合显存低的情况；flash_attention和unsloth主要用在训练过程中，vllm主要用在推理过程中
    </p>
    <p>
     Qlora框架下的具体量化方法：bitsandbytes直接内置比较方便，hqq在显存极低情况下更好用（显存占用下降更多），但需要安装
    </p>
    <p>
     Qlora最低容忍度为4bit量化，损失在4-8%（8bit损失＜1%），但3比特及以下都有两位数的损失
    </p>
    <p>
     Qlora只是在微调过程中牺牲精度来量化微调，模型本身依然是全精度的
    </p>
    <p>
     微调后量化使用的导出量校准数据集格式为[{“text”:“abc”},…]
    </p>
    <p>
     导出设备选择auto，cpu可能报错；大模型文件导出可能报错，需要修改最大分块大小，即单个模型文件的最大大小（GB）。
    </p>
    <p>
     evaluate主要比的是BLEU相似度，不是准确率
    </p>
    <p>
     截断长度对显存占用影响较大，若数据集长度固定，可以降低到max_length甚至低于max_length(抛弃长文本数据)
    </p>
    <h3>
     <a id="_20">
     </a>
     记录
    </h3>
    <p>
     <strong>
      Device
     </strong>
     : AMD 7940HX + 4060(8GB) + 64GB RAM
     <br/>
     <strong>
      数据量
     </strong>
     ：692240条
    </p>
    <h4>
     <a id="1epoch_Qwen2515BInstruct_287GB_24">
     </a>
     1epoch Qwen2.5-1.5B-Instruct 模型本身2.87GB
    </h4>
    <p>
     batch_size=1 86530item 截断长度1024 时长43h 显存3.7GB
     <br/>
     QLoRA未启用 加速方式flashattn
    </p>
    <p>
     batch_size=4 21632item 截断长度256 时长11.5h 显存3.9GB
     <br/>
     QLoRA未启用 加速方式flashattn
    </p>
    <p>
     batch_size=4 21632item 截断长度256 时长15.5h 显存2.1GB
     <br/>
     QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
    <p>
     batch_size=4 21632item 截断长度256 时长32h 显存2.7GB
     <br/>
     QLoRA启用(8bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
    <h4>
     <a id="1epoch_Qwen257BInstruct_141GB_38">
     </a>
     1epoch Qwen2.5-7B-Instruct 模型本身14.1GB
    </h4>
    <p>
     batch_size=1 86530item 截断长度256 时长700h 显存14.5GB
     <br/>
     QLoRA未启用 量化方法bitsandbytes 加速方式flashattn
    </p>
    <p>
     batch_size=1 86530item 截断长度256 时长174h 显存6.2GB
     <br/>
     QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
    <h4>
     <a id="1epoch_Qwen253BInstruct_574GB_46">
     </a>
     1epoch Qwen2.5-3B-Instruct 模型本身5.74GB
    </h4>
    <p>
     batch_size=1 86530item 截断长度256 时长58h 显存6.7GB
     <br/>
     QLoRA未启用 量化方法bitsandbytes 加速方式flashattn
    </p>
    <p>
     batch_size=1 86530item 截断长度256 时长84h 显存2.6GB
     <br/>
     QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
    <p>
     batch_size=4 21632item 截断长度256 时长27h 显存3.1GB
     <br/>
     QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
    <p>
     batch_size=16 5409item 截断长度256 时长16h 显存5.1GB
     <br/>
     QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
    <p>
     batch_size=32 2704item 截断长度256 时长16h 显存7.1GB
     <br/>
     QLoRA启用(4bit) 量化方法bitsandbytes 加速方式flashattn
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f67:2e6373646e2e6e65742f4c6f756973655f5472656e6465722f:61727469636c652f64657461696c732f313436323933333231" class_="artid" style="display:none">
 </p>
</div>


