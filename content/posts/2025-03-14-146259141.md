---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35323438323634302f:61727469636c652f64657461696c732f313436323539313431"
layout: post
title: "深度学习大模型补充知识点"
date: 2025-03-14 18:00:04 +08:00
description: "了解大语言模型的预训练，指令微调，强化学习的概念和典型例子。以及大模型的不同架构实例，分为only-encoder,only-decoder,encoder-decoder；"
keywords: "深度学习大模型补充知识点"
categories: ['未分类']
tags: ['自然语言处理', '深度学习', '机器学习', '人工智能']
artid: "146259141"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146259141
    alt: "深度学习大模型补充知识点"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146259141
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146259141
cover: https://bing.ee123.net/img/rand?artid=146259141
image: https://bing.ee123.net/img/rand?artid=146259141
img: https://bing.ee123.net/img/rand?artid=146259141
---

# 深度学习大模型补充知识点

---

## VIT

ViT（Vision Transformer） 首次将 Transformer架构成功应用于计算机视觉领域（尤其是图像分类任务）。传统视觉任务主要依赖卷积神经网络（CNN），而ViT通过将图像视为序列化的
**图像块（Patch），利用Transformer的全局注意力机制捕捉图像的长距离依赖关系，突破了CNN的局部感受野限制。**

### 用途

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d27a1de08bc943d8bf70a0e1141b792c.png)

### 处理方法

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/57b40789b8e5409686f74deef8fe94a3.png)
  
将图片划分为多个patch，转换为离散的向量，作为encoder输入,进行交互提取特征然后经过分类头输出。

### 与CNN区别

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fff17df514e847f79441000774a18129.png)

---

`

## 多模态

![示例：pandas 是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建的。](https://i-blog.csdnimg.cn/direct/eeb64aee22174a48b71af3e951b05eb0.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fbf629e5a45b46d0877fc3f3b593bc1a.png)
  
transformer架构天然为多模态而生。
  
Bert就常用于多模态训练：无论输入是文字，图片，还是声音，都让他们进入self_attention进行交互。
  
如：ViltBert就是一个多模态模型，用于从图片和文字中提取特征

## LLM：大语言模型

基于transformer架构的大模型
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c74956ed7daa4ba8a8158ba457add8f5.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1c753e49d12549538cf2462480beabc3.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8ae2cabbc9a44811904d021931e48720.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3f2bf9bf2bbe45769cf9a457cfec2943.png)

以gpt为例，only-decoder架构的大模型

#### 预训练

gpt采用自回归预训练，通过预测下一个字的生成，与翻译任务不同，预训练采用的是teach force.
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5ffed16ba02541adb109ee4024e13f6c.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8c56b241bcd041f9980698eee427ca26.png)

#### 指令微调

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d77b9886a9384334a933f094110c60be.png)
  
SFT 是 Supervised Fine-Tuning（监督微调）的缩写，是大语言模型（LLM）训练流程中的一个关键阶段。它的核心思想是：通过人工标注的高质量数据，进一步调整预训练模型的参数，使其更符合特定任务的需求（例如对话生成、指令遵循等）

#### 强化学习

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1180916693564a3e998badf747e6efe2.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6c46252ddfbb4b58b91a77e9aff3589a.png)
  
PPO 近端策略优化，选择某个操作如果正确奖励就越高，梯度就越大，朝着越好的方向更新，选择正确操作的概率越大。

### 总结

了解大语言模型的预训练，指令微调，强化学习的概念和典型例子。
  
以及大模型的不同架构实例，分为only-encoder,only-decoder,encoder-decoder；