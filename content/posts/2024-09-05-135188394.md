---
title: 2024-09-05-Pythonè·å–Yandexæœç´¢å¼•æ“æœç´¢ç»“æœè¯¦è§£
date: 2024-09-05 11:26:07 +08:00
categories: ['Python']
tags: ['Python', 'æœç´¢å¼•æ“', 'å¼€å‘è¯­è¨€']
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=135188394
  alt: Pythonè·å–Yandexæœç´¢å¼•æ“æœç´¢ç»“æœè¯¦è§£
artid: 135188394
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=135188394
featuredImagePreview: https://bing.ee123.net/img/rand?artid=135188394
---

# Pythonè·å–Yandexæœç´¢å¼•æ“æœç´¢ç»“æœè¯¦è§£

![](https://i-blog.csdnimg.cn/blog_migrate/87d69cf88a3e176b2a292c3f058b56cb.png)

## æ›´å¤šèµ„æ–™è·å–

ğŸ“š ä¸ªäººç½‘ç«™ï¼š
[ipengtao.com](http://ipengtao.com/)

---

åœ¨ç½‘ç»œæœç´¢é¢†åŸŸï¼ŒYandexæ˜¯ä¸€ä¸ªå¤‡å—æ¬¢è¿çš„æœç´¢å¼•æ“ï¼Œç‰¹åˆ«åœ¨ä¿„ç½—æ–¯å’Œå‘¨è¾¹åœ°åŒºä½¿ç”¨å¹¿æ³›ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨Pythonè·å–Yandexæœç´¢å¼•æ“çš„æœç´¢ç»“æœï¼Œä»¥ä¾¿åœ¨é¡¹ç›®ä¸­è¿›è¡Œæœç´¢ç»“æœåˆ†æå’Œæ•°æ®æŒ–æ˜ã€‚

### ä½¿ç”¨Requestsåº“è¿›è¡ŒHTTPè¯·æ±‚

ä½¿ç”¨
`requests`
åº“å‘Yandexæœç´¢å¼•æ“å‘é€HTTPè¯·æ±‚ã€‚

ç¡®ä¿å·²ç»å®‰è£…äº†è¯¥åº“ï¼š

```bash
pip install requests

```

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ä»£ç ï¼Œæ¼”ç¤ºå¦‚ä½•å‘Yandexæœç´¢å¼•æ“å‘èµ·æœç´¢è¯·æ±‚å¹¶è·å–ç»“æœï¼š

```python
import requests

def yandex_search(query):
    base_url = "https://yandex.com/search/"
    params = {'text': query}
    
    response = requests.get(base_url, params=params)
    
    if response.status_code == 200:
        return response.text
    else:
        return None

# ç¤ºä¾‹æœç´¢
query = "Python web scraping"
search_results = yandex_search(query)

print(search_results)

```

è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªå‡½æ•°
`yandex_search`
ï¼Œæ¥å—ä¸€ä¸ªæœç´¢æŸ¥è¯¢ä½œä¸ºå‚æ•°ï¼Œå¹¶è¿”å›Yandexæœç´¢ç»“æœçš„HTMLæ–‡æœ¬ã€‚è¯·æ³¨æ„ï¼Œå®é™…é¡¹ç›®ä¸­ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨æ›´å¤æ‚çš„è¯·æ±‚å¤´å’Œå¤„ç†å¯èƒ½çš„åçˆ¬è™«æœºåˆ¶ã€‚

### ä½¿ç”¨Beautiful Soupè§£æHTML

ä½¿ç”¨
`Beautiful Soup`
åº“è§£æYandexæœç´¢ç»“æœçš„HTMLæ–‡æœ¬ã€‚

ç¡®ä¿å·²ç»å®‰è£…äº†è¯¥åº“ï¼š

```bash
pip install beautifulsoup4

```

ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨Beautiful Soupæå–æœç´¢ç»“æœä¸­çš„æ ‡é¢˜å’Œé“¾æ¥ï¼š

```python
from bs4 import BeautifulSoup

def parse_search_results(html):
    soup = BeautifulSoup(html, 'html.parser')
    
    results = []
    for result in soup.find_all('li', class_='serp-item'):
        title = result.find('a', class_='organic__url-text').text
        link = result.find('a', class_='organic__url')['href']
        results.append({'title': title, 'link': link})
    
    return results

# è§£ææœç´¢ç»“æœ
parsed_results = parse_search_results(search_results)

# æ‰“å°ç»“æœ
for result in parsed_results:
    print(result)

```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå®šä¹‰äº†ä¸€ä¸ªå‡½æ•°
`parse_search_results`
ï¼Œè¯¥å‡½æ•°æ¥å—Yandexæœç´¢ç»“æœçš„HTMLæ–‡æœ¬ï¼Œä½¿ç”¨Beautiful Soupè§£æHTMLå¹¶æå–æœç´¢ç»“æœçš„æ ‡é¢˜å’Œé“¾æ¥ã€‚

### å®Œæ•´ç¤ºä¾‹

ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ä»£ç ï¼Œæ¼”ç¤ºå¦‚ä½•ä¸€æ¬¡æ€§è¿›è¡ŒYandexæœç´¢ã€è§£æHTMLå¹¶è¾“å‡ºç»“æœï¼š

```python
import requests
from bs4 import BeautifulSoup

def yandex_search(query):
    base_url = "https://yandex.com/search/"
    params = {'text': query}
    
    response = requests.get(base_url, params=params)
    
    if response.status_code == 200:
        return response.text
    else:
        return None

def parse_search_results(html):
    soup = BeautifulSoup(html, 'html.parser')
    
    results = []
    for result in soup.find_all('li', class_='serp-item'):
        title = result.find('a', class_='organic__url-text').text
        link = result.find('a', class_='organic__url')['href']
        results.append({'title': title, 'link': link})
    
    return results

# ç¤ºä¾‹æœç´¢
query = "Python web scraping"
search_results = yandex_search(query)

# è§£ææœç´¢ç»“æœ
parsed_results = parse_search_results(search_results)

# æ‰“å°ç»“æœ
for result in parsed_results:
    print(result)

```

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼Œå¯ä»¥å°†è¿™äº›ä»£ç é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­ï¼Œä»¥ä¾¿è·å–å¹¶åˆ†æYandexæœç´¢å¼•æ“çš„æœç´¢ç»“æœã€‚

### æ·»åŠ ç”¨æˆ·ä»£ç†å’Œåçˆ¬è™«æœºåˆ¶

ä¸ºäº†æé«˜è¯·æ±‚çš„å¯é æ€§å’Œé¿å…è¢«è¯†åˆ«ä¸ºçˆ¬è™«ï¼Œå¯ä»¥è®¾ç½®ç”¨æˆ·ä»£ç†å’Œå¤„ç†åçˆ¬è™«æœºåˆ¶ã€‚

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œä½¿ç”¨
`fake_useragent`
åº“ç”Ÿæˆéšæœºçš„ç”¨æˆ·ä»£ç†ï¼š

```bash
pip install fake_useragent

```

ç„¶åï¼Œä¿®æ”¹
`yandex_search`
å‡½æ•°ï¼š

```python
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

def yandex_search(query):
    base_url = "https://yandex.com/search/"
    params = {'text': query}
    headers = {'User-Agent': UserAgent().random}
    
    response = requests.get(base_url, params=params, headers=headers)
    
    if response.status_code == 200:
        return response.text
    else:
        return None

```

è¿™æ ·ï¼Œæ¯æ¬¡è¯·æ±‚æ—¶ï¼Œéƒ½ä¼šä½¿ç”¨ä¸€ä¸ªéšæœºçš„ç”¨æˆ·ä»£ç†ï¼Œå¢åŠ äº†åçˆ¬è™«çš„éš¾åº¦ã€‚

### å¤šé¡µæœç´¢ç»“æœ

é€šå¸¸ï¼Œæœç´¢ç»“æœä¼šåˆ†ä¸ºå¤šé¡µï¼Œå¯èƒ½éœ€è¦è·å–å¤šä¸ªé¡µé¢çš„ç»“æœã€‚

ä¸‹é¢æ˜¯ä¿®æ”¹ä»£ç ä»¥è·å–å¤šé¡µç»“æœçš„ç¤ºä¾‹ï¼š

```python
def yandex_search(query, num_pages=3):
    base_url = "https://yandex.com/search/"
    results = []
    
    for page in range(0, num_pages):
        params = {'text': query, 'p': page}
        headers = {'User-Agent': UserAgent().random}
        response = requests.get(base_url, params=params, headers=headers)
        
        if response.status_code == 200:
            results.append(response.text)
        else:
            return None
    
    return results

```

ç„¶åï¼Œå¯ä»¥ä¿®æ”¹è§£æå‡½æ•°ä»¥å¤„ç†å¤šä¸ªé¡µé¢çš„HTMLæ–‡æœ¬ã€‚

```python
def parse_search_results(html_pages):
    all_results = []
    
    for html in html_pages:
        soup = BeautifulSoup(html, 'html.parser')
        for result in soup.find_all('li', class_='serp-item'):
            title = result.find('a', class_='organic__url-text').text
            link = result.find('a', class_='organic__url')['href']
            all_results.append({'title': title, 'link': link})
    
    return all_results

```

### å¢åŠ å¼‚å¸¸å¤„ç†æœºåˆ¶

åœ¨çœŸå®çš„ç½‘ç»œçˆ¬è™«é¡¹ç›®ä¸­ï¼Œç»å¸¸éœ€è¦æ·»åŠ å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼Œä»¥å¤„ç†ç½‘ç»œè¯·æ±‚å¯èƒ½é‡åˆ°çš„é—®é¢˜ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„ä¿®æ”¹ï¼Œä»¥å¤„ç†å¯èƒ½çš„å¼‚å¸¸ï¼š

```python
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent

def yandex_search(query, num_pages=3):
    base_url = "https://yandex.com/search/"
    results = []
    
    for page in range(0, num_pages):
        params = {'text': query, 'p': page}
        headers = {'User-Agent': UserAgent().random}
        
        try:
            response = requests.get(base_url, params=params, headers=headers)
            response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
        except requests.exceptions.RequestException as e:
            print(f"Error in page {page + 1}: {e}")
            continue
        
        results.append(response.text)
    
    return results

```

è¿™ä¸ªä¿®æ”¹ä½¿ç”¨äº†
`try-except`
å—æ¥æ•è·
`requests`
åº“å¯èƒ½æŠ›å‡ºçš„å¼‚å¸¸ï¼Œå¹¶åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶æ‰“å°é”™è¯¯ä¿¡æ¯ã€‚è¿™æœ‰åŠ©äºåœ¨ç½‘ç»œè¯·æ±‚å¤±è´¥æ—¶è¿›è¡Œé€‚å½“çš„å¤„ç†ï¼Œé¿å…ç¨‹åºå´©æºƒã€‚

### å­˜å‚¨æœç´¢ç»“æœ

åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œå¯èƒ½éœ€è¦å°†æœç´¢ç»“æœä¿å­˜åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“ä¸­ï¼Œä»¥å¤‡åç»­åˆ†æã€‚

ä»¥ä¸‹æ˜¯å°†æœç´¢ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶çš„ç®€å•ç¤ºä¾‹ï¼š

```python
import json

def save_results_to_json(results, filename):
    with open(filename, 'w', encoding='utf-8') as file:
        json.dump(results, file, ensure_ascii=False, indent=2)

# ç¤ºä¾‹è°ƒç”¨
search_results = yandex_search("Python web scraping", num_pages=2)
save_results_to_json(search_results, "yandex_search_results.json")

```

è¿™ä¸ªç¤ºä¾‹å®šä¹‰äº†ä¸€ä¸ª
`save_results_to_json`
å‡½æ•°ï¼Œæ¥å—æœç´¢ç»“æœå’Œæ–‡ä»¶åä½œä¸ºå‚æ•°ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°JSONæ–‡ä»¶ä¸­ã€‚å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹æ­¤å‡½æ•°ï¼Œä»¥é€‚åº”ä¸åŒçš„æ•°æ®å­˜å‚¨éœ€æ±‚ã€‚

### æ€»ç»“

åœ¨æœ¬æ–‡ä¸­ï¼Œæ·±å…¥æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨Pythonä»Yandexæœç´¢å¼•æ“è·å–æœç´¢ç»“æœã€‚é€šè¿‡ä½¿ç”¨
`requests`
åº“æ„å»ºHTTPè¯·æ±‚ï¼Œ
`Beautiful Soup`
åº“è§£æHTMLæ–‡æœ¬ï¼Œä»¥åŠ
`fake_useragent`
åº“ç”Ÿæˆéšæœºç”¨æˆ·ä»£ç†ï¼Œå®ç°äº†ä¸€ä¸ªå¼ºå¤§è€Œçµæ´»çš„æœç´¢å¼•æ“çˆ¬è™«ã€‚ç¤ºä¾‹ä»£ç ä¸­è€ƒè™‘äº†å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼Œç¡®ä¿äº†ç¨‹åºçš„ç¨³å®šæ€§ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•å°†æœç´¢ç»“æœå­˜å‚¨åˆ°JSONæ–‡ä»¶ä¸­ã€‚

åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œè¿™äº›ç¤ºä¾‹ä»£ç å¯ä»¥ä½œä¸ºä¸€ä¸ªåŸºç¡€æ¡†æ¶ï¼Œå¸®åŠ©å¼€å‘è€…å®šåˆ¶é€‚åº”ç‰¹å®šéœ€æ±‚çš„ç½‘ç»œçˆ¬è™«ã€‚é€šè¿‡äº†è§£å¼‚å¸¸å¤„ç†ã€ç”¨æˆ·ä»£ç†è®¾ç½®ã€HTMLè§£æç­‰å…³é”®æ¦‚å¿µï¼Œè¯»è€…å°†æ›´å¥½åœ°ç†è§£æ„å»ºå¥å£®ç½‘ç»œçˆ¬è™«çš„åŸºæœ¬æ­¥éª¤ã€‚æ­¤å¤–ï¼Œç¤ºä¾‹ä»£ç è¿˜æ¼”ç¤ºäº†å¦‚ä½•å¤„ç†å¤šé¡µæœç´¢ç»“æœï¼Œä½¿å…¶æ›´å…·å®ç”¨æ€§ã€‚é€šè¿‡åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨è¿™äº›æ¦‚å¿µï¼Œå¼€å‘è€…å¯ä»¥è½»æ¾åœ°å®šåˆ¶è‡ªå·±çš„ç½‘ç»œçˆ¬è™«ï¼Œç”¨äºè·å–ã€åˆ†æå’Œå­˜å‚¨Yandexæœç´¢å¼•æ“çš„ä¸°å¯Œä¿¡æ¯ã€‚

---

## Pythonå­¦ä¹ è·¯çº¿

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/blog_migrate/0b9967d3437b99c90e6e948411ec96a1.png)

## æ›´å¤šèµ„æ–™è·å–

ğŸ“š ä¸ªäººç½‘ç«™ï¼š
[ipengtao.com](http://ipengtao.com/)

å¦‚æœè¿˜æƒ³è¦é¢†å–æ›´å¤šæ›´ä¸°å¯Œçš„èµ„æ–™ï¼Œå¯ä»¥ç‚¹å‡»æ–‡ç« ä¸‹æ–¹åç‰‡ï¼Œå›å¤ã€
**ä¼˜è´¨èµ„æ–™**
ã€‘ï¼Œå³å¯è·å– å…¨æ–¹ä½å­¦ä¹ èµ„æ–™åŒ…ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/blog_migrate/d844690f5d767b2e027b4cc0354198bb.png)
  
ç‚¹å‡»æ–‡ç« ä¸‹æ–¹é“¾æ¥å¡ç‰‡ï¼Œå›å¤ã€
**ä¼˜è´¨èµ„æ–™**
ã€‘ï¼Œå¯ç›´æ¥é¢†å–èµ„æ–™å¤§ç¤¼åŒ…ã€‚

68747470733a2f2f626c:6f672e6373646e2e6e65742f77755368694a696e675a756f2f:61727469636c652f64657461696c732f313335313838333934