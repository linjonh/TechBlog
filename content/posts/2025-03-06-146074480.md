---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f534a4a535f312f:61727469636c652f64657461696c732f313436303734343830"
layout: post
title: "YOLOv12本地部署教程42速度提升,让高效目标检测触手可及"
date: 2025-03-06 16:48:58 +0800
description: "YOLOv12 是“你只看一次”（You Only Look Once, YOLO）系列的最新版本，于 2025 年 2 月发布。它引入了注意力机制，提升了检测精度，同时保持了高效的实时性能。在保持速度的同时，显著提升了检测精度。例如，YOLOv12-N 在 T4 GPU 上的推理延迟为 1.64 毫秒，平均精度（mAP）达到 40.6%，相比 YOLOv10-N 和 YOLOv11-N 分别提升了 2.1% 和 1.2%"
keywords: "yolov12"
categories: ['模型构建']
tags: ['人工智能']
artid: "146074480"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146074480
    alt: "YOLOv12本地部署教程42速度提升,让高效目标检测触手可及"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146074480
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146074480
cover: https://bing.ee123.net/img/rand?artid=146074480
image: https://bing.ee123.net/img/rand?artid=146074480
img: https://bing.ee123.net/img/rand?artid=146074480
---

# YOLOv12本地部署教程——42%速度提升，让高效目标检测触手可及

YOLOv12 是“你只看一次”（You Only Look Once, YOLO）系列的最新版本，于 2025 年 2 月发布。它引入了注意力机制，提升了检测精度，同时保持了高效的实时性能。在保持速度的同时，显著提升了检测精度。例如，YOLOv12-N 在 T4 GPU 上的推理延迟为 1.64 毫秒，平均精度（mAP）达到 40.6%，相比 YOLOv10-N 和 YOLOv11-N 分别提升了 2.1% 和 1.2%

![1741228236413_20250306101338.png](https://i-blog.csdnimg.cn/img_convert/798d6c33cdabd6abf47926ff6e9d1260.jpeg)

YOLOv12 作为 YOLO 系列的最新迭代，首次将注意力机制深度融入单阶段检测框架，通过三大关键技术实现性能飞跃：

* 区域注意力模块（A2）：通过特征图分块与重塑操作，将全局注意力的计算复杂度从二次方降至线性，同时保留大感受野，兼顾效率与精度。
* 残差高效层聚合网络（R-ELAN）：引入块级残差连接与动态缩放技术，解决传统ELAN的梯度阻塞问题，提升训练稳定性与特征融合能力。
* 极简架构设计：移除位置编码、降低MLP扩展比、减少堆叠块深度，结合FlashAttention优化内存访问效率，推理速度较YOLOv9提升42%。

#### 在性能方面更是全面碾压前代模型，在 MS COCO 基准测试中，YOLOv12展现出显著优势：

* 小型模型（YOLOv12-N）：以40.6% mAP超越YOLOv10-N（38.5%），延迟仅1.64毫秒/图像。
* 中型模型（YOLOv12-S）：48.0% mAP，较YOLOv8-S提升3.0%，计算量降低至21.4G FLOPs。
* 跨任务兼容性：支持实例分割、姿态估计等扩展任务，在复杂场景（如遮挡、低光照）中检测精度提升15%以上。

与基于 Transformer 的 RT-DETR 相比，YOLOv12-S 快42%、仅需36%计算资源，在实时性与部署成本上占据绝对优势。

接下来就为大家奉上详细的 YOLOv12 本地部署教程，手把手教你如何将模型部署到你的项目中，轻松享受高性能AI带来的便利。

## 二、部署流程

**环境推荐配置**

**系统：Ubuntu22.04，**

**显卡：4090，**

**显存：24G，cuda11.8**

### 1. 基础环境

**查看系统是否有Miniconda3的虚拟环境**

```
conda -V

```

如果输入命令没有显示Conda版本号，则需要安装。

![1733121521985_image.png](https://i-blog.csdnimg.cn/img_convert/ade63aa77dae593dfe3795eae1ee321f.png)

### 2.更新系统命令

**输入下列命令将系统更新及系统下载**

```
apt-get update && apt-get install ffmpeg libsm6 libxext6  -y

```

![1733121540798_image.png](https://i-blog.csdnimg.cn/img_convert/b18fad351bb502729ccfe95128fcf113.png)

### 3.创建虚拟环境

**创建名称为“yolov12”的虚拟环境并激活**

```
conda create -n yolov12 python=3.11 -y
conda activate yolov12

```

![1741228987875_20250224102029.png](https://i-blog.csdnimg.cn/img_convert/400bd75c04afbdc08269afecd8b7d4c9.png)

### **4.下载模型**

**输入下列命令下载yolov12模型同时进入项目中**

```
git clone https://gitclone.com/github.com/sunsmarterjie/yolov12.git 
cd yolov12/

```

![1741229001963_20250224102138.png](https://i-blog.csdnimg.cn/img_convert/db905f702be7ad26e612f6547ffc1808.png)

### 5.下载模型依赖包

输入下列命令：

```
pip install -r requirements.txt

```

![1741229257788_20250306104714.png](https://i-blog.csdnimg.cn/img_convert/59092bbd59194d276db903fc5c7a1208.png)

**出现报错，重新使用命令下载包同时换源加速：**

```
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu11torch2.2cxx11abiFALSE-cp311-cp311-linux_x86_64.whl

```

![1741229869835_20250224101700.png](https://i-blog.csdnimg.cn/img_convert/f850d7d0da83942830016536d3ebe166.png)

```
pip install -r requirements.txt  -i https://pypi.tuna.tsinghua.edu.cn/simple

```

![1741229398884_20250224102518.png](https://i-blog.csdnimg.cn/img_convert/836305d04ed58ec78c88c8cb307f0db5.png)

**继续下载其他依赖包**

```
pip install -e .

```

![1741229462635_20250224112924.png](https://i-blog.csdnimg.cn/img_convert/5371435fb2a5b0e7d3a2ed3201e0c67a.png)

## 三、网页演示

**在本地运行Gradio应用程序，使用下列命令运行项目呈现模型的成功界面**

```
python app.py

```

![1741229154861_20250224113202.png](https://i-blog.csdnimg.cn/img_convert/106566dbda5668e157c6bd4dc30b501f.png)

**打开网址：**

![1741228669675_20250306101322.png](https://i-blog.csdnimg.cn/img_convert/667883e7bf32e788ed34241b77a1db75.png)