---
layout: post
title: "LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS-论文阅读"
date: 2025-03-16 21:57:28 +0800
description: "1、为什么要这么做？预训练模型越来越大，比如GPT-3 175B训练独立变得越来越不可行2、方法：冻结预训练模型的权重，在Transformer架构的每一层中注入可训练的低秩分解矩阵3、效果：训练参数量减少10000x，GPU显存减少3x，且不像adapter引入额外的推理延迟"
keywords: "LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 论文阅读"
categories: ['基础模型训练专题']
tags: ['预训练', '语言模型', '人工智能', 'Mllms', 'Lora', 'Llm']
artid: "146166829"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146166829
    alt: "LORA-LOW-RANK-ADAPTATION-OF-LARGE-LANGUAGE-MODELS-论文阅读"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146166829
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146166829
cover: https://bing.ee123.net/img/rand?artid=146166829
image: https://bing.ee123.net/img/rand?artid=146166829
img: https://bing.ee123.net/img/rand?artid=146166829
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 论文阅读
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     一、TL；DR
    </h2>
    <ol>
     <li>
      <span style="color:#fe2c24">
       为什么要这么做
      </span>
      ？预训练模型越来越大，比如GPT-3 175B训练独立变得越来越不可行
     </li>
     <li>
      <span style="color:#fe2c24">
       方法
      </span>
      ：冻结预训练模型的权重，在Transformer架构的每一层中注入可训练的低秩分解矩阵
     </li>
     <li>
      <span style="color:#fe2c24">
       效果
      </span>
      ：训练参数量减少
      <strong>
       <span style="color:#fe2c24">
        10000x
       </span>
      </strong>
      ，GPU显存减少
      <strong>
       <span style="color:#fe2c24">
        3x
       </span>
      </strong>
      ，且不像adapter引入额外的推理延迟
     </li>
    </ol>
    <h2>
     二、方法
    </h2>
    <h3>
     2.1 整体介绍
    </h3>
    <p>
     <span style="color:#0d0016">
      LoRA允许我们通过优化
     </span>
     dense layers
     <span style="color:#0d0016">
      在微调过程中变化的秩分解矩阵，间接地训练神经网络
     </span>
     <span style="color:#0d0016">
      ，同时保持预训练权重冻结，
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="280" src="https://i-blog.csdnimg.cn/direct/53a3eeee5c80417a8a1308073831a011.png" width="247"/>
    </p>
    <p>
     <span style="color:#0d0016">
      如图1所示。以GPT-3 175B为例，我们表明即使完整秩（即d）高达12,288，一个非常低的秩（即图1中的r可以是一或二）也足够了，这使得LoRA在存储和计算方面都非常高效。
     </span>
    </p>
    <p>
     LoRA具有4个关键优势：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        <span style="color:#fe2c24">
         多lora部署模型
        </span>
       </strong>
       ：部署不同的图1中的矩阵A和B高效地切换任务，显著减少存储需求和任务切换开销。
      </p>
     </li>
     <li>
      <p>
       <strong>
        <span style="color:#fe2c24">
         训练门槛低：
        </span>
       </strong>
       LoRA\将显存需求减少了3x，因为不需要为大多数参数计算梯度或维护优化器状态。只需要优化注入的、小得多的低秩矩阵。
      </p>
     </li>
     <li>
      <p>
       <strong>
        <span style="color:#fe2c24">
         无推理延迟成本
        </span>
       </strong>
       ：简单的线性设计允许在部署时将lora参数和预训练权重合并，与完全微调的模型相比，LoRA不会引入任何推理延迟。
      </p>
     </li>
     <li>
      <p>
       方案灵活可叠加：LoRA可与（prefix-tuning）等方法叠加使用
      </p>
     </li>
    </ul>
    <h3>
     2.2 问题描述
    </h3>
    <p>
     首先定义一下大型预训练模型如何训练下游任务的问题。
    </p>
    <p>
     全量微调一个自回归语言模型，模型从预训练权重
     <strong>
      Φ0​
     </strong>
     开始初始化，并通过反复沿着梯度更新，
     <span style="color:#fe2c24">
      最大化条件语言模型概率
     </span>
     ：
    </p>
    <p>
     <img alt="" height="83" src="https://i-blog.csdnimg.cn/direct/f7f09443b24c480897efa0a7f57fec3d.png" width="738"/>
    </p>
    <p>
     <span style="color:#fe2c24">
      全参的缺点
     </span>
     ：
    </p>
    <p>
     对每个下游任务，都需要学习一组不同的参数增量
     <span style="color:#fe2c24">
      <strong>
       ∆Φ
      </strong>
     </span>
     ，其维度|∆Φ|等于预训练模型的参数维度|Φ₀|。因此，如果预训练模型规模庞大（例如GPT-3，其|Φ₀|约为1750亿），存储和部署众多独立的微调模型实例可能会非常困难，甚至根本不可行
    </p>
    <p>
     <span style="color:#fe2c24">
      lora的优点
     </span>
     ：
    </p>
    <p>
     是一种更具参数效率的方法，其中针对特定任务的
     <span style="color:#fe2c24">
      <strong>
       参数增量∆Φ = ∆Φ(Θ)
      </strong>
     </span>
     进一步由一组小得多的参数Θ编码，且
     <span style="color:#fe2c24">
      <strong>
       |Θ|
      </strong>
     </span>
     远小于
     <span style="color:#fe2c24">
      <strong>
       |Φ₀|
      </strong>
     </span>
     。因此，
     <span style="color:#fe2c24">
      寻找∆Φ的任务就变成了对Θ进行优化
     </span>
     ：
    </p>
    <p>
     <img alt="" height="93" src="https://i-blog.csdnimg.cn/direct/f3f0eb3b43a048668ea1554c1fc69c0f.png" width="835"/>
    </p>
    <p>
     使用一种低秩表示来编码∆Φ，更为高效。举例在GPT-3 175B训练时，可训练参数的数量|Θ|可以小至|Φ₀|的0.01%。
    </p>
    <p>
     <span style="color:#fe2c24">
      讲人话：
     </span>
    </p>
    <p>
     <span style="color:#0d0016">
      原来是全参训练更新参数，现在是相较预训练模型增量的那部分变化用lora的低秩组成的矩阵来代替，从而达到同样的效果
     </span>
    </p>
    <h3>
     <span style="color:#0d0016">
      2.3 现在的解决方案（翻译原文）
     </span>
    </h3>
    <p>
     添加adapter或优化输入层激活的某些形式，局限性是会引入延迟（细节略，直接翻译见下文）：
    </p>
    <p>
     <strong>
      适配器层会引入推理延迟
     </strong>
     <br/>
     适配器有多种变体。我们关注Houlsby等人（2019）最初的设计，该设计在每个Transformer块中包含两个适配器层，以及Lin等人（2020）最近提出的设计，后者每个块中只有一个适配器层，但额外增加了一个LayerNorm（Ba等人，2016）。尽管可以通过剪枝层或利用多任务设置来减少整体延迟（Rücklé等人，2020；Pfeiffer等人，2021），但没有直接的方法可以绕过适配器层中的额外计算。这似乎不是问题，因为适配器层被设计为只有少量参数（有时不到原始模型的1%），通过设置较小的瓶颈维度来限制它们增加的浮点运算量（FLOPs）。然而，大型神经网络依赖硬件并行化来保持低延迟，而适配器层必须按顺序处理。这在在线推理场景中（通常批量大小仅为1）会产生差异。在没有模型并行化的通用场景中，例如在单个GPU上运行GPT-2（Radford等人，b）中等模型时，即使瓶颈维度非常小，使用适配器时也会明显增加延迟（见表1）。
    </p>
    <p>
     当需要对模型进行分片时（如Shoeybi等人，2020；Lepikhin等人，2020），这个问题会变得更糟，因为额外的深度需要更多的同步GPU操作，如AllReduce和Broadcast，除非我们将适配器参数冗余存储多次。
    </p>
    <p>
     <strong>
      直接优化提示词是困难的
     </strong>
     <br/>
     另一个方向，以前缀调整（Li和Liang，2021）为例，面临着不同的挑战。我们观察到前缀调整难以优化，并且其性能会随着可训练参数的变化而非单调变化，这证实了原始论文中的类似观察。更根本的是，为适应保留一部分序列长度，必然会减少可用于处理下游任务的序列长度，我们怀疑这使得提示词调整的性能不如其他方法。关于任务性能的研究将在第5节中进行讨论。
    </p>
    <h2>
     4 本文的方法
    </h2>
    <h3>
     4.1 LORA原理
    </h3>
    <p>
     假设在适应过程中，权重的更新具有较低的“内在秩”。对于一个预训练的权重矩阵 W0​∈Rd×k，我们通过低秩分解来约束其更新：W0​+ΔW=W0​+BA，其中 B∈Rd×r，A∈Rr×k，且秩 r≪min(d,k)。
    </p>
    <p>
     在训练过程中，W0​ 是固定的，不会接收梯度更新，而 A 和 B 包含可训练参数。注意，W0​ 和 ΔW=BA 都与相同的输入相乘，并且它们的输出向量是逐坐标相加的。修改后的前向传播结果为：
     <br/>
     <img alt="" height="241" src="https://i-blog.csdnimg.cn/direct/0ac223488fa54e789d2fa3043725b5a6.png" width="857">
      <br/>
      我们在图1中展示了这种重新参数化。
     </img>
    </p>
    <p>
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/09b83152334c487abd24e29e11567bd0.png" width="277"/>
    </p>
    <p>
     具体来说：
    </p>
    <ol>
     <li>
      随机高斯初始化 A，并将 B 初始化为零，在训练开始时，ΔW=BA 为零。然后我们将 ΔWx 缩放 rα​，其中 α 是一个与 r 相关的常数。当使用Adam优化时，调整 α 大致等同于调整学习率，只要我们适当地缩放初始化即可。因此，我们只需将 α 设置为我们尝试的第一个 r，而无需对其进行调整。这种缩放有助于减少在变化 r 时重新调整超参数的需求（Yang &amp; Hu, 2021）。
     </li>
    </ol>
    <p>
     上述原理可以支持多lora进行部署，即
     <span style="color:#fe2c24">
      共用一套预训练模型，但是不同下游的A和B矩阵进行merge计算并部署
     </span>
    </p>
    <h4>
     4.2 将LoRA应用于Transformer哪个模块
    </h4>
    <p>
     在Transformer架构中，自注意力模块中有四个权重矩阵（Wq​,Wk​,Wv​,Wo​），在gpt3.5的模型训练中，
     <span style="color:#fe2c24">
      发现同时调整Wq和Wv效果是比较好的，因此更应该倾向于用不同的秩微调不同的参数
     </span>
     （这个地方我有一些疑问啊，上面不是明显4个权重一起调整效果会更好吗？）
    </p>
    <p>
     <img alt="" height="408" src="https://i-blog.csdnimg.cn/direct/59edca27cc084e88a279d5edd960a58d.png" width="851"/>
    </p>
    <h3>
     4.3 experiments
    </h3>
    <p>
     核心结论：lora比其他的参数微调方法都要稳定和效果好
    </p>
    <p>
     <img alt="" height="364" src="https://i-blog.csdnimg.cn/direct/924414cd0b0f4c4081e2720708340199.png" width="836"/>
    </p>
    <p>
     核心结论：r设置多少和更新哪几个权重比较好？明显是r=2/4且更新多个权重比较好
    </p>
    <p>
     <img alt="" height="373" src="https://i-blog.csdnimg.cn/direct/4a4844e4e6bc4bb0b20b34344f61b0ed.png" width="848"/>
    </p>
    <p>
     核心结论：top奇异向量的作用最大，其他的奇异可能会引入更多的噪声。这证明了
     <strong>
      更新参数矩阵
     </strong>
     ΔW
     <strong>
      存在极小的‘内在秩’（这个图我没看懂，下周在更新怎么理解这个图）
     </strong>
     。
    </p>
    <p>
     <img alt="" height="369" src="https://i-blog.csdnimg.cn/direct/4786e08419514533a82040ddc12139a3.png" width="881"/>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470:733a2f2f626c6f672e6373646e2e6e65742f6c6f766570312f:61727469636c652f64657461696c732f313436313636383239" class_="artid" style="display:none">
 </p>
</div>


