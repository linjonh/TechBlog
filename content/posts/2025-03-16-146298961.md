---
layout: post
title: "PyTorch-深度学习实战15Twin-Delayed-DDPG-TD3-算法"
date: 2025-03-16 18:35:51 +08:00
description: "本文介绍了 TD3 算法的基本原理，并使用 PyTorch 实现了一个简单的 TD3 模型来解决 Pendulum 问题。通过这个例子，我们学习了如何使用 TD3 算法进行连续动作空间的策略优化。在下一篇文章中，我们将探讨更高级的强化学习算法，如 Soft Actor-Critic (SAC)。敬请期待！代码实例说明本文代码可以直接在 Jupyter Notebook 或 Python 脚本中运行。如果你有 GPU，代码会自动检测并使用 GPU 加速。希望这篇文章能帮助你更好地理解 TD3 算法！"
keywords: "PyTorch 深度学习实战（15）：Twin Delayed DDPG (TD3) 算法"
categories: ['深度学习实战', 'Pytorch']
tags: ['算法', '深度学习', 'Pytorch']
artid: "146298961"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146298961
    alt: "PyTorch-深度学习实战15Twin-Delayed-DDPG-TD3-算法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146298961
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146298961
cover: https://bing.ee123.net/img/rand?artid=146298961
image: https://bing.ee123.net/img/rand?artid=146298961
img: https://bing.ee123.net/img/rand?artid=146298961
---

# PyTorch 深度学习实战（15）：Twin Delayed DDPG (TD3) 算法
## 在上一篇文章中，我们介绍了 Deep Deterministic Policy Gradient (DDPG) 算法，并使用它解决了 Pendulum
问题。本文将深入探讨 \*\*Twin Delayed DDPG (TD3)\*\* 算法，这是一种改进的 DDPG 算法，能够有效解决 DDPG
中的过估计问题。我们将使用 PyTorch 实现 TD3 算法，并应用于经典的 Pendulum 问题。
\* \* \*
### 一、TD3 算法基础
TD3 是 DDPG 的改进版本，通过引入以下三个关键技术来解决 DDPG 中的过估计问题：
1. \*\*双重 Critic 网络\*\* ：
使用两个 Critic 网络来估计 Q 值，从而减少过估计问题。
2. \*\*延迟更新\*\* ：
延迟 Actor 网络的更新，确保 Critic 网络更稳定地收敛。
3. \*\*目标策略平滑\*\* ：
在目标动作中加入噪声，从而减少 Critic 网络的过拟合。
#### 1\. TD3 的核心思想
\* \*\*双重 Critic 网络\*\* ：
\* 使用两个 Critic 网络来估计 Q 值，取两者中的较小值作为目标 Q 值，从而减少过估计。
\* \*\*延迟更新\*\* ：
\* 每更新 Critic 网络多次，才更新一次 Actor 网络，确保 Critic 网络更稳定地收敛。
\* \*\*目标策略平滑\*\* ：
\* 在目标动作中加入噪声，从而减少 Critic 网络的过拟合。
#### 2\. TD3 的优势
\* \*\*减少过估计\*\* ：
\* 通过双重 Critic 网络和目标策略平滑，TD3 能够有效减少 Q 值的过估计。
\* \*\*训练稳定\*\* ：
\* 延迟更新策略确保 Critic 网络更稳定地收敛。
\* \*\*适用于连续动作空间\*\* ：
\* TD3 能够直接输出连续动作，适用于机器人控制、自动驾驶等任务。
#### 3\. TD3 的算法流程
1. 使用当前策略采样一批数据。
2. 使用目标网络计算目标 Q 值。
3. 更新 Critic 网络以最小化 Q 值的误差。
4. 延迟更新 Actor 网络以最大化 Q 值。
5. 更新目标网络。
6. 重复上述过程，直到策略收敛。
\* \* \*
### 二、Pendulum 问题实战
我们将使用 PyTorch 实现 TD3 算法，并应用于 Pendulum 问题。目标是控制摆杆使其保持直立。
#### 1\. 问题描述
Pendulum 环境的状态空间包括摆杆的角度和角速度。动作空间是一个连续的扭矩值，范围在 −2,2
之间。智能体每保持摆杆直立一步，就会获得一个负的奖励，目标是最大化累积奖励。
#### 2\. 实现步骤
1. 安装并导入必要的库。
2. 定义 Actor 网络和 Critic 网络。
3. 定义 TD3 训练过程。
4. 测试模型并评估性能。
#### 3\. 代码实现
以下是完整的代码实现：
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import random
from collections import deque
import matplotlib.pyplot as plt
​
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode\_minus'] = False
​
device = torch.device("cuda" if torch.cuda.is\_available() else "cpu")
print(f"使用设备: {device}")
​
env = gym.make('Pendulum-v1')
state\_dim = env.observation\_space.shape[0]
action\_dim = env.action\_space.shape[0]
max\_action = float(env.action\_space.high[0])
​
SEED = 42
torch.manual\_seed(SEED)
np.random.seed(SEED)
random.seed(SEED)
​
​
# 改进的 Actor 网络（增加层归一化）
class Actor(nn.Module):
  def \_\_init\_\_(self, state\_dim, action\_dim, max\_action):
      super(Actor, self).\_\_init\_\_()
      self.l1 = nn.Linear(state\_dim, 256)
      self.ln1 = nn.LayerNorm(256)
      self.l2 = nn.Linear(256, 256)
      self.ln2 = nn.LayerNorm(256)
      self.l3 = nn.Linear(256, action\_dim)
      self.max\_action = max\_action
​
  def forward(self, x):
      x = F.relu(self.ln1(self.l1(x)))
      x = F.relu(self.ln2(self.l2(x)))
      x = torch.tanh(self.l3(x)) \* self.max\_action
      return x
​
​
# 改进的 Critic 网络（增加层归一化）
class Critic(nn.Module):
  def \_\_init\_\_(self, state\_dim, action\_dim):
      super(Critic, self).\_\_init\_\_()
      self.l1 = nn.Linear(state\_dim + action\_dim, 256)
      self.ln1 = nn.LayerNorm(256)
      self.l2 = nn.Linear(256, 256)
      self.ln2 = nn.LayerNorm(256)
      self.l3 = nn.Linear(256, 1)
​
  def forward(self, x, u):
      x = F.relu(self.ln1(self.l1(torch.cat([x, u], 1))))
      x = F.relu(self.ln2(self.l2(x)))
      x = self.l3(x)
      return x
​
​
class TD3:
  def \_\_init\_\_(self, state\_dim, action\_dim, max\_action):
      self.actor = Actor(state\_dim, action\_dim, max\_action).to(device)
      self.actor\_target = Actor(state\_dim, action\_dim, max\_action).to(device)
      self.actor\_target.load\_state\_dict(self.actor.state\_dict())
      self.actor\_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)
​
      self.critic1 = Critic(state\_dim, action\_dim).to(device)
      self.critic2 = Critic(state\_dim, action\_dim).to(device)
      self.critic1\_target = Critic(state\_dim, action\_dim).to(device)
      self.critic2\_target = Critic(state\_dim, action\_dim).to(device)
      self.critic1\_target.load\_state\_dict(self.critic1.state\_dict())
      self.critic2\_target.load\_state\_dict(self.critic2.state\_dict())
      self.critic1\_optimizer = optim.Adam(self.critic1.parameters(), lr=3e-4)
      self.critic2\_optimizer = optim.Adam(self.critic2.parameters(), lr=3e-4)
​
      self.max\_action = max\_action
      self.replay\_buffer = deque(maxlen=1000000)
      self.batch\_size = 256
      self.gamma = 0.99
      self.tau = 0.005
      self.policy\_noise = 0.2
      self.noise\_clip = 0.5
      self.policy\_freq = 2
      self.total\_it = 0
      self.exploration\_noise = 0.1 # 新增探索噪声
​
  def select\_action(self, state, add\_noise=True):
      state = torch.FloatTensor(state.reshape(1, -1)).to(device)
      action = self.actor(state).cpu().data.numpy().flatten()
      if add\_noise:
          noise = np.random.normal(0, self.exploration\_noise, size=action\_dim)
          action = (action + noise).clip(-self.max\_action, self.max\_action)
      return action
​
  def train(self):
      if len(self.replay\_buffer) < self.batch\_size:
          return
​
      self.total\_it += 1
​
      batch = random.sample(self.replay\_buffer, self.batch\_size)
      state = torch.FloatTensor(np.array([t[0] for t in batch])).to(device)
      action = torch.FloatTensor(np.array([t[1] for t in batch])).to(device)
      reward = torch.FloatTensor(np.array([t[2] for t in batch])).reshape(-1, 1).to(device) / 10.0 # 奖励缩放
      next\_state = torch.FloatTensor(np.array([t[3] for t in batch])).to(device)
      done = torch.FloatTensor(np.array([t[4] for t in batch])).reshape(-1, 1).to(device)
​
      with torch.no\_grad():
          noise = (torch.randn\_like(action) \* self.policy\_noise).clamp(-self.noise\_clip, self.noise\_clip)
          next\_action = (self.actor\_target(next\_state) + noise).clamp(-self.max\_action, self.max\_action)
          target\_Q1 = self.critic1\_target(next\_state, next\_action)
          target\_Q2 = self.critic2\_target(next\_state, next\_action)
          target\_Q = torch.min(target\_Q1, target\_Q2)
          target\_Q = reward + (1 - done) \* self.gamma \* target\_Q
​
      current\_Q1 = self.critic1(state, action)
      current\_Q2 = self.critic2(state, action)
      critic1\_loss = F.mse\_loss(current\_Q1, target\_Q)
      critic2\_loss = F.mse\_loss(current\_Q2, target\_Q)
      self.critic1\_optimizer.zero\_grad()
      critic1\_loss.backward()
      torch.nn.utils.clip\_grad\_norm\_(self.critic1.parameters(), 1.0) # 梯度裁剪
      self.critic1\_optimizer.step()
      self.critic2\_optimizer.zero\_grad()
      critic2\_loss.backward()
      torch.nn.utils.clip\_grad\_norm\_(self.critic2.parameters(), 1.0)
      self.critic2\_optimizer.step()
​
      if self.total\_it % self.policy\_freq == 0:
          actor\_loss = -self.critic1(state, self.actor(state)).mean()
          self.actor\_optimizer.zero\_grad()
          actor\_loss.backward()
          torch.nn.utils.clip\_grad\_norm\_(self.actor.parameters(), 1.0)
          self.actor\_optimizer.step()
​
          for param, target\_param in zip(self.critic1.parameters(), self.critic1\_target.parameters()):
              target\_param.data.copy\_(self.tau \* param.data + (1 - self.tau) \* target\_param.data)
          for param, target\_param in zip(self.critic2.parameters(), self.critic2\_target.parameters()):
              target\_param.data.copy\_(self.tau \* param.data + (1 - self.tau) \* target\_param.data)
          for param, target\_param in zip(self.actor.parameters(), self.actor\_target.parameters()):
              target\_param.data.copy\_(self.tau \* param.data + (1 - self.tau) \* target\_param.data)
​
  def save(self, filename):
      torch.save(self.actor.state\_dict(), filename + "\_actor.pth")
​
​
def train\_td3(env, agent, episodes=2000, early\_stop\_threshold=-150):
  rewards\_history = []
  moving\_avg = []
  best\_avg = -np.inf
​
  for ep in range(episodes):
      state,\_ = env.reset()
      episode\_reward = 0
      done = False
      step = 0
​
      while not done:
          # 线性衰减探索噪声
          if ep < 300:
              agent.exploration\_noise = max(0.5 \* (1 - ep / 300), 0.1)
          else:
              agent.exploration\_noise = 0.1
​
          action = agent.select\_action(state, add\_noise=(ep < 100)) # 前100轮强制探索
          next\_state, reward, done, \_, \_ = env.step(action)
          agent.replay\_buffer.append((state, action, reward, next\_state, done))
          state = next\_state
          episode\_reward += reward
          agent.train()
          step += 1
​
      rewards\_history.append(episode\_reward)
      current\_avg = np.mean(rewards\_history[-50:])
      moving\_avg.append(current\_avg)
​
      if current\_avg > best\_avg:
          best\_avg = current\_avg
          agent.save("td3\_pendulum\_best")
​
      if (ep + 1) % 50 == 0:
          print(f"Episode: {ep + 1}, Avg Reward: {current\_avg:.2f}")
​
      # 早停机制
      if current\_avg >= early\_stop\_threshold:
          print(f"早停触发，平均奖励达到 {current\_avg:.2f}")
          break
​
  return moving\_avg, rewards\_history
​
​
# 训练并可视化
td3\_agent = TD3(state\_dim, action\_dim, max\_action)
moving\_avg, rewards\_history = train\_td3(env, td3\_agent, episodes=2000)
​
# 可视化结果
plt.figure(figsize=(12, 6))
plt.plot(rewards\_history, alpha=0.6, label='single round reward')
plt.plot(moving\_avg, 'r-', linewidth=2, label='moving average (50 rounds)')
plt.xlabel('episodes')
plt.ylabel('reward')
plt.title('TD3 training performance on Pendulum-v1')
plt.legend()
plt.grid(True)
plt.show()
\* \* \*
### 三、代码解析
1. \*\*Actor 和 Critic 网络\*\* ：
\* Actor 网络输出连续动作，通过 `tanh` 函数将动作限制在 −max\_action,max\_action 范围内。
\* Critic 网络输出状态-动作对的 Q 值。
2. \*\*TD3 训练过程\*\* ：
\* 使用当前策略采样一批数据。
\* 使用目标网络计算目标 Q 值。
\* 更新 Critic 网络以最小化 Q 值的误差。
\* 延迟更新 Actor 网络以最大化 Q 值。
\* 更新目标网络。
3. \*\*训练过程\*\* ：
\* 在训练过程中，每 50 个 episode 打印一次平均奖励。
\* 训练结束后，绘制训练过程中的总奖励曲线。
\* \* \*
### 四、运行结果
运行上述代码后，你将看到以下输出：
\* 训练过程中每 50 个 episode 打印一次平均奖励。
\* 训练结束后，绘制训练过程中的总奖励曲线。
![](https://i-blog.csdnimg.cn/direct/56c5768550984b9e9a83d301da1889ff.png)
\* \* \*
### 五、总结
本文介绍了 TD3 算法的基本原理，并使用 PyTorch 实现了一个简单的 TD3 模型来解决 Pendulum 问题。通过这个例子，我们学习了如何使用
TD3 算法进行连续动作空间的策略优化。
在下一篇文章中，我们将探讨更高级的强化学习算法，如 Soft Actor-Critic (SAC)。敬请期待！
\*\*代码实例说明\*\* ：
\* 本文代码可以直接在 Jupyter Notebook 或 Python 脚本中运行。
\* 如果你有 GPU，代码会自动检测并使用 GPU 加速。
希望这篇文章能帮助你更好地理解 TD3 算法！如果有任何问题，欢迎在评论区留言讨论。