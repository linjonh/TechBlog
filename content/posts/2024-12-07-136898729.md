---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f73756e6261696775692f:61727469636c652f64657461696c732f313336383938373239"
layout: post
title: "视觉AIGC元年技术大爆炸Dalle-3SoraStable-Diffusion-3-掀起AIGC新浪潮,究竟有哪些模块值得借鉴"
date: 2024-12-07 20:46:34 +08:00
description: "随着科技的飞速发展，我们迎来了视觉AIGC高光时刻，一个充满无限可能与机遇的新时代。在这个时代里，三"
keywords: "dalle - 3"
categories: ['深度', '学习论文与相关应用']
tags: ['大模型', 'Stable', 'Sora', 'Diffusion', 'Aigc', '3']
artid: "136898729"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=136898729
    alt: "视觉AIGC元年技术大爆炸Dalle-3SoraStable-Diffusion-3-掀起AIGC新浪潮,究竟有哪些模块值得借鉴"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=136898729
featuredImagePreview: https://bing.ee123.net/img/rand?artid=136898729
---

# 视觉AIGC元年：技术大爆炸！Dalle-3、Sora、Stable Diffusion 3 掀起AIGC新浪潮，究竟有哪些模块值得借鉴！

随着科技的飞速发展，我们迎来了视觉AIGC高光时刻，一个充满无限可能与机遇的新时代。在这个时代里，
**三大里程碑Dalle-3、Sora和Stable Diffusion 3以其炸裂式的技术发展，引领着AIGC领域的新浪潮**
。文章首先做相应简要介绍，后半部分着重做新兴技术拆解分析，看看究竟哪些模块值得借鉴！

**Dalle-3**
在2023年9月以其强大的图像生成能力惊艳了世人。它从精细化的文本描述入手，融合进了GPT-4的丰富caption能力，另外还引入了早已验证成功了latent空间替代逐pixel的预测，大幅提高了生成图像的质量和多样性。Dalle-3的出现，极大地推动了视觉AIGC领域的发展，为后续的效果创新奠定了坚实的基础。以下是官方样例：

![](https://i-blog.csdnimg.cn/blog_migrate/3a97ef19a78e13214de029f60c83b85c.png)

紧随其后的是2024年2月15的
**Sora**
，它在通用视频生成领域取得了突破性的进展。Sora引入了先进的DiTs模块替换掉了UNET，并且将视频处理成了Transformer结构中的visual patch，极大的提升了视频生成的性能，引发了业界广泛的关注。

压轴的是2024年2月22的
**Stable Diffusion 3**
，即图像生成领域的最新SOTA。它引入了更为先进的扩散过程和噪声估计技术Flow Matching，精细化的文本描述配合多模DiTs，使得生成的图像更具指令跟随能力，尤其在图像中文本控制的表现上刷新了新高度。Stable Diffusion 3的出现，不仅提升了扩散模型在图像生成任务中的性能，还为其他领域如自然语言处理、语音识别等提供了可借鉴的技术思路。以下是官方样例：

![](https://i-blog.csdnimg.cn/blog_migrate/283e5daa57dd9b5c1e02406e6dbf8ee2.png)

**这三大技术的崛起，标志着视觉AIGC元年技术大爆炸的到来。它们不仅在各自领域内取得了卓越的成就，还在相互融合中催生出更多新的应用场景和商业模式。未来，随着技术的不断进步和创新，我们有理由相信，视觉AIGC将会为人类带来更多的惊喜和可能**
。

## 一、Dalle-3

论文题目：Improving Image Generation with Better Captions，
[https://cdn.openai.com/papers/dall-e-3.pdf](https://cdn.openai.com/papers/dall-e-3.pdf "https://cdn.openai.com/papers/dall-e-3.pdf")

体验入口：
[Bing AI - 搜索](https://bing.com/chat "Bing AI - 搜索")
，
[https://cn.bing.com/create](https://cn.bing.com/create "https://cn.bing.com/create")

发布时间：2023.9

### 亮点：

#### 1.）精细化caption

论文着重宣传部分。caption生成模块使用了CLIP（Contrastive Language-Image Pretraining）图像编码器和GPT语言模型（GPT-4），可为每张图像生成细致的文字描述。以下是用GPT-4生成更加精细化caption的例子：
![](https://i-blog.csdnimg.cn/blog_migrate/f657d2ba1b8ddb1e3623e04bfaaa580c.png)

#### 2.）LDM diffusion

图像生成模块先用VAE将高分辨率图像压缩为低维向量，降低学习难度，然后使用T5 Transformer将文本编码为向量，并通过GroupNorm层将其注入LDM diffusion模型而且像素级diffusion，指导图像生成方向。
**与SDXL类似在潜空间进行diffusion是DALL-E 3比前两代生成的图片质量更好的核心原因之一**
。

![](https://i-blog.csdnimg.cn/blog_migrate/cd337015595c4d113529392471d029e8.png)
![](https://i-blog.csdnimg.cn/blog_migrate/721d15bd290f46016f84eee8d92b5642.png)

## 二、Sora

论文题目：Video generation models as world simulators，https://openai.com/research/video-generation-models-as-world-simulators

体验入口：
[Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators "Video generation models as world simulators")

发布时间：2024.2.15

### 亮点：

#### 1.）visual patch

Sora将视频数据转换成Transformer大模型可以使用的tokens。这个过程涉及到将视频中的多帧图像进行深度学习压缩，并加上第三维的时间信息，形成patches。这些patches作为tokens，可以应用于Transformer模型中。这种数据转换方式使得Sora能够处理和理解视频数据，为后续的视频生成和处理打下基础。：

![](https://i-blog.csdnimg.cn/blog_migrate/e7093b4bf475a3889b58f5cb2871baee.png)

#### 2.）diffusion transformer（DiTs）

Sora运用扩散模型来处理视频生成的连续性和细节刻画问题，而Transformer则用于理解并整合复杂的时空上下文信息。通过这样的组合方式，Sora能够高效且创造性地生成高质量的视频内容。具体来讲，
使用Transformers替换扩散模型中U-Net主干网络，分析发现，这种Diffusion Transformers（DiTs）不仅速度更快（更高的Gflops），而且在ImageNet 512×512和256×256的类别条件图片生成任务上，取得了更好的效果，256×256上实现了SOTA的FID指标（2.27）
。DiTs论文：Scalable Diffusion Models with Transformers，
[https://arxiv.org/abs/2212.09748](https://arxiv.org/abs/2212.09748 "https://arxiv.org/abs/2212.09748")
。

![](https://i-blog.csdnimg.cn/blog_migrate/cd48e1ec9077bff601c77a5fe8007f67.png)

其中DiTs结构如下：

![](https://i-blog.csdnimg.cn/blog_migrate/f4dd1aaa9e304e4b66fa606706b81d35.png)

## 三、SD-3

论文题目：Scaling Rectified Flow Transformers for High-Resolution Image Synthesis，https://arxiv.org/pdf/2403.03206.pdf

体验入口：https://stability.ai/stablediffusion3

发布时间：2024.02.22

![](https://i-blog.csdnimg.cn/blog_migrate/1f128ad1d3c31db6b90b9be942eb9dbc.png)

该图表以 SD3 为基准，基于人类偏好评估，展示了 SD3 在视觉美学、提示遵循和排版等方面相对于其他竞争模型的优势。

### 亮点：

#### 1.）diffusion transformer（DiTs）

与Sora类似用Latent Diffusion Transformer（DiTs） 换掉扩散模型中的 U-Net 结构。SD 3架构图如下所示：

![](https://i-blog.csdnimg.cn/blog_migrate/3572b32dd3b1d8168b97a5f7cc0e8232.png)

具体来讲，多模态扩散是基座，该架构是建立在 DiT基础上。原始DiT 只考虑类别条件下的图像生成，并使用调制机制来对扩散过程的时间步和类别标签进行条件约束。MM-DiT有如下特点。

1、输入侧：简单文本特征+timestep，丰富文本特征，带噪latent特征+位置编码。

2、多模态DiT：如图 2b 所示，为文本和图像两种模态使用两组独立的权重，然后将两种模态转化后的特征连接起来进行attention后继续分拆出来文本与图像分支，如此嵌套。最终达到文本控制的最大化。

#### 2.）Flow Matching

Flow Matching是一个新的生成模型框架，这项研究为基于连续归一化流（CNF）的生成建模引入了一种新范式，实现了以前所未有的规模训练 CNF。这个框架不依赖复杂的模拟或对数似然估计，而是直接处理生成目标概率路径的向量场。简单来说，Flow Matching给我们提供了一张地图（向量场）和一条路线（概率路径），让我们能够更清晰地了解数据是如何生成的。通过这张地图和路线，我们可以更轻松地训练生成模型，让它学习从噪声中生成出我们想要的数据。Flow Matching还提出了一个叫做条件Flow Matching (CFM)的损失函数，这个函数让模型的训练变得更容易。同时，它还支持各种概率路径，包括diffusion路径和OT路径，这让我们在训练模型时有了更多的选择。使用 Flow Matching 技术的意义则在于提升采样效率。Flow Matching论文：Flow Matching for Generative Modeling，
[https://arxiv.org/pdf/2210.02747.pdf](https://arxiv.org/pdf/2210.02747.pdf "https://arxiv.org/pdf/2210.02747.pdf")
。

![](https://i-blog.csdnimg.cn/blog_migrate/b726394726e4fc8611a1f10b68a229e0.png)
![](https://i-blog.csdnimg.cn/blog_migrate/bfce4634e787d1043d10b44dd2d4ce87.png)

图2和图6是Flow Matching对比diffusion和OT示意图