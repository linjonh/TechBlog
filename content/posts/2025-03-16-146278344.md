---
layout: post
title: "搜广推校招面经五十"
date: 2025-03-16 00:15:00 +08:00
description: "处理连续数据，以加速决策树的训练。这种方法避免了对所有特征值进行遍历，大幅提升计算效率，同时对模型精度影响很小。，避免传统 GBDT 在遍历所有样本时的高昂计算开销。见【搜广推校招面经九、十】LightGBM 使用。这种方式的核心目标是。"
keywords: "搜广推校招面经五十"
categories: ['搜广推面经']
tags: ['算法', '深度学习', '机器学习', '搜索算法', '推荐算法', '人工智能', 'Pytorch']
artid: "146278344"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146278344
    alt: "搜广推校招面经五十"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146278344
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146278344
cover: https://bing.ee123.net/img/rand?artid=146278344
image: https://bing.ee123.net/img/rand?artid=146278344
img: https://bing.ee123.net/img/rand?artid=146278344
---

# 搜广推校招面经五十
## shopee搜广推算法
## 一、如何进行连续数据的分箱，除了传统统计方法，有什么先进方法
### 1.1. 为什么要进行连续数据的分箱？
1. **降低数据复杂度** ：将连续数据转换为离散区间，简化数据分析和模型构建。
2. **减少噪声影响** ：分箱可以减少数据中的噪声，提高模型的鲁棒性。
3. **处理非线性关系** ：分箱可以帮助捕捉连续变量与目标变量之间的非线性关系。
4. **提高计算效率** ：离散化后的数据可以减少计算复杂度，加快模型训练速度。
5. **增强解释性** ：离散化的数据更容易理解和解释，特别是在业务场景中。
### 1.2. 如何进行连续数据的分箱？
#### (1)传统统计方法
1. **等宽分箱（Equal Width Binning）** ：
* 将数据范围划分为等宽的区间。
* 例如，将年龄分为0-10, 10-20, 20-30等区间。
2. **等频分箱（Equal Frequency Binning）** ：
* 将数据划分为包含相同数量样本的区间。
* 例如，将数据分为4个区间，每个区间包含25%的数据。
#### (2)先进方法
1. **基于核密度估计的分箱** ：
* 使用核密度估计（KDE）来识别数据的密度分布，并根据密度变化进行分箱。
2. **基于贝叶斯优化的分箱** ：
* 使用贝叶斯优化方法自动搜索最优的分箱策略，最大化某种评价指标（如AUC）。
3. **基于自适应分箱的方法** ：
* 使用自适应算法（如AdaBoost）动态调整分箱边界，以适应数据分布的变化。
* 使用LightGBM的直方图算法，将连续特征分成不同的桶，在每个桶内，统计样本数量、目标值的累积和（用于计算分裂增益）。（实际并没有改变特征，只是计算分裂增益）
4. **IV和WOE在分箱中的应用** ：
### 1.3. IV和WOE(多用于金融风控、信用评分等领域)
#### WOE（Weight of Evidence，证据权重）
* **定义** ：WOE用于衡量某个分箱中目标事件（如违约）与非目标事件（如未违约）的比例差异。
* **公式** ：
W O E = ln ⁡ ( Good% Bad% ) WOE =
\ln\left(\frac{\text{Good\%}}{\text{Bad\%}}\right) WOE=ln(Bad%Good%​)
其中，Good%表示该分箱中好样本的比例，Bad%表示坏样本的比例。
#### IV（Information Value，信息价值）
* **定义** ：IV用于衡量某个变量对目标变量的预测能力，通常用于特征选择。
* **公式** ：
I V = ∑ ( Good% − Bad% ) × W O E IV = \sum \left(\text{Good\%} -
\text{Bad\%}\right) \times WOE IV=∑(Good%−Bad%)×WOE
IV值越大，表示该变量的预测能力越强。
#### IV和WOE在分箱中的应用
##### 1 **基于WOE的分箱**
* **步骤** ：
1. 对连续变量进行初步分箱（如等宽分箱或等频分箱）。
2. 计算每个分箱的WOE值。
3. 根据WOE值调整分箱边界，使得每个分箱的WOE值尽可能区分好坏样本。
4. 合并或拆分分箱，使得每个分箱的WOE值具有显著差异。
* **优点** ：
* WOE分箱能够更好地捕捉变量与目标变量之间的非线性关系。
* 分箱后的变量具有更好的解释性。
##### 2 **基于IV的分箱**
* **步骤** ：
1. 对连续变量进行初步分箱。
2. 计算每个分箱的WOE值和IV值。
3. 根据IV值评估每个分箱的预测能力，调整分箱边界以最大化IV值。
4. 合并或拆分分箱，使得整体IV值最大化。
* **优点** ：
* IV分箱能够自动选择对目标变量预测能力最强的分箱策略。
* IV值可以用于特征选择，帮助筛选出最有价值的变量。
## 二、lgb如何对连续数据处理的（直方图算法）
LightGBM 使用 **直方图分箱（Histogram Binning）**
处理连续数据，以加速决策树的训练。这种方法避免了对所有特征值进行遍历，大幅提升计算效率，同时对模型精度影响很小。
### **2.1. LightGBM 直方图分箱的基本原理**
LightGBM 并不会直接使用原始的连续特征值，而是：
1. **对连续特征进行分箱（Binning）** ：将连续数值映射到固定数量的 bin（默认 255 个）。
2. **统计每个 bin 内的样本信息** ：记录样本数量、目标值的累积和等。
3. **在 bin 边界处查找最优分裂点** ，而不是遍历所有可能的特征值，从而提高训练速度。
这种方式的核心目标是 **降低计算复杂度** ，避免传统 GBDT 在遍历所有样本时的高昂计算开销。
### **2.2. 具体的连续数据处理步骤**
#### **(1) 预处理阶段：特征值离散化**
* LightGBM 采用 **无监督分箱** ，通常使用 **等频分箱（Quantile Binning）** 进行数据离散化，将连续特征值映射到 **有限数量的 bin** 。默认情况下，每个特征会被分到 **最多 255 个 bin** ，即 `max_bins=255`。
#### **(2) 计算直方图**
* 对每个 bin 统计：
* 该 bin 内的样本数量。
* 目标变量的累积值（用于计算信息增益）。
* 梯度和二阶梯度（用于计算损失）。
#### **(3) 计算最优分裂点**
* 在 **bin 的边界处** 计算信息增益，找到最优分裂点，而不是遍历所有可能的连续值。
#### **(4) 直方图共享**
* 在决策树的每个节点上，LightGBM 复用 **父节点的直方图** ，仅计算新增样本的贡献，而不重新计算整个直方图。**减少冗余计算，提高训练速度** 。
**注：LGBM 的直方图分箱本质上是一个动态特征工程方法，但它的作用仅限于加速训练，而不会改变特征本身。**
## 三、lgb和xgb的损失函数
见【搜广推校招面经九、十】
## 四、240. 搜索二维矩阵 II
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5d2229ccdecd4c4995570ed40063efcf.png)
* 思路1：
站在矩阵的右上角来说，当前元素是当前行最大，当前列的最小值。判断右上角元素和target的关系，从而简化剩余矩阵
* 代码：
class Solution:
def searchMatrix(self, matrix: List[List[int]], target: int) -> bool:
m, n = len(matrix), len(matrix[0])
i, j = 0, n - 1 # 从右上角开始
while i < m and j >= 0: # 还有剩余元素
if matrix[i][j] == target:
return True # 1.找到 target
if matrix[i][j] < target:
i += 1 # 这一行剩余元素全部小于 target，排除
else:
j -= 1 # 这一列剩余元素全部大于 target，排除
return False