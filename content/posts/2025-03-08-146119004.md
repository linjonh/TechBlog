---
layout: post
title: "开源项目介绍Native-LLM-for-Android"
date: 2025-03-08 23:28:29 +08:00
description: "Native-LLM-for-Android项目主要提供2个参考点，1、将LLM模型导出为onnx模型，2、在安卓端实现LLL模型的运行，本博文主要关注将llm导出为onnx推理（对现有的llm模型进行局部修改并导出），并以miniCPM模型为例进行测试。同时，Native-LLM-for-Android项目还有一些列模型量化代码可以学习。"
keywords: "开源项目介绍：Native-LLM-for-Android"
categories: ['开源项目分析与使用']
tags: ['深度学习', '开源', '大模型', 'Android']
artid: "146119004"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146119004
    alt: "开源项目介绍Native-LLM-for-Android"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146119004
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146119004
cover: https://bing.ee123.net/img/rand?artid=146119004
image: https://bing.ee123.net/img/rand?artid=146119004
img: https://bing.ee123.net/img/rand?artid=146119004
---

# 开源项目介绍：Native-LLM-for-Android
项目地址：Native-LLM-for-Android
创作活动时间：2025年
支持在 Android 设备上运行大型语言模型 （LLM） ，具体支持的模型包括：
DeepSeek-R1-Distill-Qwen: 1.5B
Qwen2.5-Instruct: 0.5B, 1.5B
Qwen2/2.5VL: 2B, 3B
MiniCPM-DPO/SFT: 1B, 2.7B
Gemma2-it: 2B
Phi3.5-mini-instruct: 3.8B
Llama-3.2-Instruct: 1B
Native-LLM-for-
Android项目主要提供2个参考点，1、将LLM模型导出为onnx模型，2、在安卓端实现LLL模型的运行，本博文主要关注将llm导出为onnx推理（对现有的llm模型进行局部修改并导出），并以miniCPM模型为例进行测试。同时，Native-
LLM-for-Android项目还有一些列模型量化代码可以学习。
### 1、模型运行性能
运行最快的模型是Llama3.2-1B-Instruct q8f32，达到25 token每秒，相应的硬件与os为 Nubia Z50（Android
13、8\_Gen2-CPU）；其次是Distill-Qwen-1.5B q8f32，达到22 token每秒，相应的硬件与os为Xiaomi-14T-Pro
（HyperOS 2、MediaTek\_9300±CPU）；
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/533ec39f201649acb710422c6e1c53ba.png#pic\_center)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8bc20977e1f8441681cdba4a8fdf0780.png#pic\_center)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/44a7e370f22e4e128c17a07243180666.png#pic\_center)
### 2、核心功能
这里主要关注将llm导出为onnx脱离torch环境运行，因此对Android运行代码不予理会
#### 2.1、分词器
分词器也就是Tokenizer ，一共两个功能：
1、将输入模型的文本，分为多个短词，并转换为token，
2、将模型输出的token转换为文本
`需要注意的是，不同的llm模型分词规则是不一样的，同时对于的token编码规则也不同`
一般运行onnx模型，都是基于Transformer库中的Tokenizer，这无法脱离torch环境。应该自行实现。
Native-LLM-for-Android项目中Tokenizer 依赖的是mnn-llm仓库中的实现.
具体代码链接为：
，是纯c++代码，与torch环境毫无关联
同时，在每一种模型的Android-onnx代码路径下，都有对于的Tokenizer的c++实现代码
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/50855c98e9ed4e2585ebbb06401fdc74.png#pic\_center)
#### 2.2、导出onnx模型
在Native-LLM-for-Android项目下Export\_ONNX目录中，每一个模型都有单独的导出代码
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/30b64e77581c42b9a3ae61f7a22ee74f.png#pic\_center)
如Gemma模型的导出，分别执行A-B-C步骤，导出3个模型，在最后的导出代码中含onnx推理代码
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2f0539964998438ea785505d99fc34b3.png#pic\_center)
其中关于QwenVL模型的导出较为复杂，需要对transformers库中modeling\_qwen2\_vl.py文件进行改写覆盖，将单个模型拆分为5个模型运行。其中A模型是VIT的主体部分，E模型是llm的主体部分，BCD模型是一些切片索引操作，被单独导出为模型。关于E模型导出有报错，可以参考
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/95f2abf569fc4a54a5f8d46992f5cb48.png#pic\_center)
如果导出模型报错
RuntimeError: The serialized model is larger than the 2GiB limit imposed by the protobuf library. Therefore the output file must be a file path, so that the ONNX external data can be written to the same directory. Please specify the output file name.
则尝试将torch版本降低到2.4.1
pip install torch2.4.1 torchvision0.19.1 torchaudio==2.4.1 --index-url
https://download.pytorch.org/whl/cu121
#### 2.3、onnx模型量化
关于onnx模型量化，可以参考：，根据介绍，onnx量化可以分为动态量化与静态量化，动态量化在推理时根据输入数据动态计算缩放因子与零点；静态量化，使用校准数据集离线计算缩放因子（Scale）和零点（Zero
Point）。`通常，建议对 RNN 和基于 Transformer 的模型使用动态量化，对 CNN 模型使用静态量化`
在Native-LLM-for-Android-main\Do\_Quantize\Dynamic\_Quant 目录下有多个模型量化代码，具体如下图所示
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ba6a5e46f34d49efbe9c8ecf92d17a2a.png#pic\_center)
q8\_f16的量化代码如下所示，可以看到对于大尺寸的模型的量化有一个关键参数项 is\_large\_model
import os
import gc
import glob
import sys
import onnx
import torch
import subprocess
import onnx.version\_converter
from onnxsim import simplify
from onnxslim import slim
from onnxruntime.quantization import QuantType, quantize\_dynamic
from onnxruntime.transformers.optimizer import optimize\_model
from transformers import AutoModelForCausalLM
# Path Setting
original\_folder\_path = r"C:\Users\Downloads\Model\_ONNX" # The original folder.
quanted\_folder\_path = r"C:\Users\Downloads\Model\_ONNX\_Optimized" # The optimized folder.
model\_path = os.path.join(original\_folder\_path, "Model.onnx") # The original fp32 model path.
quanted\_model\_path = os.path.join(quanted\_folder\_path, "Model\_Optimized.onnx") # The optimized model stored path.
download\_path = r'C:\Users\Downloads\Qwen2-1.5B-Instruct' # Set the folder path where the LLM whole project downloaded, otherwise set "NONE".
use\_gpu = True # If true, the transformers.optimizer will remain the FP16 processes.
provider = 'CPUExecutionProvider' # ['CPUExecutionProvider', 'CUDAExecutionProvider']
use\_low\_memory\_mode\_in\_Android = False # If you need to use low memory mode on Android, please set it to True.
# Preprocess, it also cost alot of memory during preprocess, you can close this command and keep quanting. Call subprocess may get permission failed on Windows system.
# (optional process)
# subprocess.run([f'python -m onnxruntime.quantization.preprocess --auto\_merge --all\_tensors\_to\_one\_file --input {model\_path} --output {quanted\_folder\_path}'], shell=True)
# Start Quantize
quantize\_dynamic(
model\_input=model\_path,
model\_output=quanted\_model\_path,
per\_channel=True, # True for model accuracy but cost a lot of time during quanting process.
reduce\_range=False, # True for some x86\_64 platform.
weight\_type=QuantType.QInt8, # It is recommended using uint8 + Symmetric False
extra\_options={'ActivationSymmetric': False, # True for inference speed. False may keep more accuracy.
'WeightSymmetric': False, # True for inference speed. False may keep more accuracy.
'EnableSubgraph': True, # True for more quant.
'ForceQuantizeNoInputCheck': False, # True for more quant.
'MatMulConstBOnly': True # False for more quant. Sometime, the inference speed may get worse.
},
nodes\_to\_exclude=None, # Specify the node names to exclude quant process. Example: nodes\_to\_exclude={'/Gather'}
use\_external\_data\_format=True # Save the model into two parts.
)
model\_size\_bytes = sys.getsizeof(onnx.load(model\_path).SerializeToString())
model\_size\_gb = model\_size\_bytes \* 9.31322575e-10 # 1 / (1024 \* 1024 \* 1024)
if model\_size\_gb > 2.0:
is\_large\_model = True
else:
is\_large\_model = True if use\_low\_memory\_mode\_in\_Android else False
# ONNX Model Optimizer
slim(
model=quanted\_model\_path,
output\_model=quanted\_model\_path,
no\_shape\_infer=False, # True for more optimize but may get errors.
skip\_fusion\_patterns=False,
no\_constant\_folding=False,
save\_as\_external\_data=is\_large\_model,
verbose=False
)
if download\_path == "NONE":
num\_heads = 0 # default
hidden\_size = 0 # default
else:
if ('vl' in download\_path.lower()) & ('qwen' in download\_path.lower()):
if "2.5" in download\_path or "3b" in download\_path.lower():
from transformers import Qwen2\_5\_VLForConditionalGeneration
model = Qwen2\_5\_VLForConditionalGeneration.from\_pretrained(download\_path, torch\_dtype=torch.float16, device\_map='cpu', trust\_remote\_code=True, low\_cpu\_mem\_usage=True).eval()
else:
from transformers import Qwen2VLForConditionalGeneration
model = Qwen2VLForConditionalGeneration.from\_pretrained(download\_path, torch\_dtype=torch.float16, device\_map='cpu', trust\_remote\_code=True, low\_cpu\_mem\_usage=True).eval()
else:
model = AutoModelForCausalLM.from\_pretrained(download\_path, torch\_dtype=torch.float16, device\_map='cpu', trust\_remote\_code=True, low\_cpu\_mem\_usage=True).eval()
num\_heads = model.config.num\_attention\_heads
hidden\_size = model.config.hidden\_size
del model
gc.collect()
# transformers.optimizer
model = optimize\_model(quanted\_model\_path,
use\_gpu=use\_gpu,
opt\_level=2,
num\_heads=num\_heads,
hidden\_size=hidden\_size,
provider=provider,
verbose=False,
model\_type='bert')
model.convert\_float\_to\_float16(
keep\_io\_types=True,
force\_fp16\_initializers=True,
use\_symbolic\_shape\_infer=True, # True for more optimize but may get errors.
op\_block\_list=['DynamicQuantizeLinear', 'DequantizeLinear', 'DynamicQuantizeMatMul', 'Range', 'MatMulIntegerToFloat']
)
model.save\_model\_to\_file(quanted\_model\_path, use\_external\_data\_format=is\_large\_model)
del model
gc.collect()
# onnxslim 2nd
slim(
model=quanted\_model\_path,
output\_model=quanted\_model\_path,
no\_shape\_infer=False, # True for more optimize but may get errors.
skip\_fusion\_patterns=False,
no\_constant\_folding=False,
save\_as\_external\_data=is\_large\_model,
verbose=False
)
# Upgrade the Opset version. (optional process)
model = onnx.load(quanted\_model\_path)
model = onnx.version\_converter.convert\_version(model, 21)
onnx.save(model, quanted\_model\_path, save\_as\_external\_data=is\_large\_model)
if is\_large\_model:
pattern = os.path.join(quanted\_folder\_path, '\*.data')
files\_to\_delete = glob.glob(pattern)
for file\_path in files\_to\_delete:
try:
os.remove(file\_path)
except Exception as e:
print(f"Error deleting {file\_path}: {e}")
# It is not recommended to convert an FP16 ONNX model to the ORT format because this process adds a Cast operation to convert the FP16 process back to FP32.
### 3、导出minicpm模型onnx推理
#### 3.1 下载模型
pip install modelscope
基于modelscope 库可以下载MiniCPM-2B-dpo-fp16模型
from modelscope import snapshot\_download
model\_dir = snapshot\_download('OpenBMB/MiniCPM-2B-dpo-fp16',cache\_dir=".cache\_dir")
#### 3.2 导出onnx模型
这里以MiniCPM-2B-split导出方式为例
先在命令行进入 `F:\Native-LLM-for-Android-main\Export\_ONNX\MiniCPM\MiniCPM-2B-split`
目录
然后创建，model\_a，model\_b两个目录，用于存储2个onnx模型，并将代码修改为以下形式
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c8f291d740724e62b31c1a7c11dac318.png)
最后在命令行中执行 `python .\MiniCPM\_Export.py` 即可实现模型导出为onnx，并进行推理测试
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ca901639a1b94a93bd8318ec91e99e6c.png)
这里可以发现代码的推理速度居然为0.375token/s，简直巨慢。
按照单个模型导出，并进行推理测试，发现效果如下所示，可以发现性能有6倍的提升，这表明数据通信也占据了大量的耗时。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3de90063f9e44fcd902093fb137b7987.png)
#### 3.3 单独运行onnx
基于以下代码可以运行onnx模型，但无法脱离transformers库，除非手写tokenizer实现分词，并实现token与文本的对应关系。
import numpy as np
import onnxruntime
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
path = 'F:\DMT\.cache\_dir\OpenBMB\MiniCPM-2B-dpo-fp16' # Set the folder path where the MiniCPM whole project downloaded.
onnx\_model\_A = r'F:\Native-LLM-for-Android-main\Export\_ONNX\MiniCPM\MiniCPM-2B-single\model\_q8\_f16\MiniCPM\_part\_A\_Optimized.onnx' # Assign a path where the exported MiniCPM\_part\_A stored.
# Run the exported model by ONNX Runtime
query = "山东省最高的山是哪座山, 它比黄山高还是矮？差距多少？"
max\_seq\_len = 1024 # Please modify the same variable, which declared in the modified modeling\_minicpm.py on line 1008, at the same time.
num\_heads = 36
head\_dim = 64
num\_key\_value\_heads = 36
num\_layers = 40
hidden\_size = 2304
max\_single\_chat\_length = 341 # It a adjustable value, but must less than max\_seq\_len.
tokenizer = AutoTokenizer.from\_pretrained(path, trust\_remote\_code=True)
# ONNX Runtime settings
session\_opts = onnxruntime.SessionOptions()
session\_opts.log\_severity\_level = 3 # error level, it a adjustable value.
session\_opts.inter\_op\_num\_threads = 0 # Run different nodes with num\_threads. Set 0 for auto.
session\_opts.intra\_op\_num\_threads = 0 # Under the node, execute the operators with num\_threads. Set 0 for auto.
session\_opts.enable\_cpu\_mem\_arena = True # True for execute speed; False for less memory usage.
session\_opts.execution\_mode = onnxruntime.ExecutionMode.ORT\_SEQUENTIAL
session\_opts.graph\_optimization\_level = onnxruntime.GraphOptimizationLevel.ORT\_ENABLE\_ALL
session\_opts.add\_session\_config\_entry("session.intra\_op.allow\_spinning", "1")
session\_opts.add\_session\_config\_entry("session.inter\_op.allow\_spinning", "1")
ort\_session\_A = onnxruntime.InferenceSession(onnx\_model\_A, sess\_options=session\_opts, providers=['CPUExecutionProvider'])
in\_name\_A = ort\_session\_A.get\_inputs()
out\_name\_A = ort\_session\_A.get\_outputs()
in\_name\_A0 = in\_name\_A[0].name
in\_name\_A1 = in\_name\_A[1].name
in\_name\_A2 = in\_name\_A[2].name
in\_name\_A3 = in\_name\_A[3].name
in\_name\_A4 = in\_name\_A[4].name
in\_name\_A5 = in\_name\_A[5].name
out\_name\_A0 = out\_name\_A[0].name
out\_name\_A1 = out\_name\_A[1].name
out\_name\_A2 = out\_name\_A[2].name
# Pre-process inputs
prompt = tokenizer.apply\_chat\_template([{"role": 'user', "content": query}], tokenize=False, add\_generation\_prompt=False)
token = tokenizer(prompt, return\_tensors='pt')['input\_ids']
ids\_len = token.shape[1] + np.zeros(1, dtype=np.int64)
input\_ids = np.zeros(max\_seq\_len, dtype=np.int32)
input\_ids[:ids\_len[0]] = token[0, :]
attention\_mask = np.array([-65504.0], dtype=np.float32)
history\_len = np.zeros(1, dtype=np.int64)
past\_key\_states\_A = np.zeros((num\_layers, num\_key\_value\_heads, max\_seq\_len, head\_dim), dtype=np.float16)
past\_values\_states\_A = past\_key\_states\_A
num\_decode = 0
print('\nTest Question: ' + query + "\n\nMiniCPM Answering:\n")
# Start to run LLM
start\_time = time.time()
while history\_len < max\_single\_chat\_length:
token\_id, past\_key\_states\_A, past\_values\_states\_A = ort\_session\_A.run(
[out\_name\_A0, out\_name\_A1, out\_name\_A2],
{in\_name\_A0: input\_ids,
in\_name\_A1: attention\_mask,
in\_name\_A2: past\_key\_states\_A,
in\_name\_A3: past\_values\_states\_A,
in\_name\_A4: history\_len,
in\_name\_A5: ids\_len})
if token\_id == 2: # the stop\_id in MiniCPM is "2"
break
else:
history\_len[0] += ids\_len[0]
ids\_len[0] = 1
num\_decode += 1
attention\_mask[0] = 0.0
input\_ids[0] = token\_id
print(tokenizer.decode(token\_id), end="", flush=True)
end\_time = time.time()
print(f"\n\nDecode: {(num\_decode / (end\_time - start\_time)):.3f} token/s")