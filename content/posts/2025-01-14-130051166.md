---
layout: post
title: "万字长文深度剖析AIGC技术网络架构自监督"
date: 2025-01-14 21:16:21 +0800
description: "作者|派派星 编辑| CVHub点击下方卡片，关注“自动驾驶之心”公众号ADAS巨卷干货，即可获取点"
keywords: "aigc技术栈"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Aigc']
artid: "130051166"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=130051166
  alt: "万字长文深度剖析AIGC技术网络架构自监督"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=130051166
featuredImagePreview: https://bing.ee123.net/img/rand?artid=130051166
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     万字长文深度剖析AIGC技术！（网络架构&amp;自监督）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <div id="js_content">
     <blockquote>
      <p style="text-align:left;">
       作者 | 派派星  编辑 | CVHub
      </p>
     </blockquote>
     <p style="text-align:center;">
      点击下方
      <strong>
       卡片
      </strong>
      ，关注“
      <strong>
       自动驾驶之心
      </strong>
      ”公众号
     </p>
     <p style="text-align:center;">
      ADAS巨卷干货，即可获取
     </p>
     <p style="text-align:justify;">
      <strong>
       点击进入→
      </strong>
      <a href="" rel="nofollow">
       <strong>
        自动驾驶之心【全栈算法】技术交流群
       </strong>
      </a>
     </p>
     <h3>
      前景回顾
     </h3>
     <p>
      Welcome to back! 在
      <a href="" rel="nofollow">
       《万字长文带你解读AIGC入门篇》
      </a>
      一文中，我们详细为大家介绍了
      <code>
       AIGC
      </code>
      的相关概念、背景及其如此火爆的原因，接下来我们将进一步深入探讨AIGC背后的技术栈。
     </p>
     <p>
      作为本系列的技术篇，将从多个角度来介绍
      <code>
       AIGC
      </code>
      的技术栈，其中包括了
      <code>
       Transformer
      </code>
      、
      <code>
       SSL
      </code>
      、
      <code>
       VAE
      </code>
      、
      <code>
       GAN
      </code>
      、
      <code>
       Diffusion
      </code>
      等大家均耳熟能详的词汇，这些技术都是
      <code>
       AIGC
      </code>
      领域中必不可少且非常重要的一部分，对于理解
      <code>
       AIGC
      </code>
      的原理和实现方式都非常关键。下面开始吧？
     </p>
     <h3>
      3. AIGC 背后的基础技术
     </h3>
     <p>
      本文将 AIGC 视为一组使用人工智能方法生成内容的任务或应用程序。其中，生成技术是指使用机器学习模型生成新的内容，例如 GAN 和扩散模型。创作技术是指利用生成技术生成的内容进行进一步的创作和编辑，例如对生成的文本进行编辑和改进。
     </p>
     <h4>
      3.1 生成技术
     </h4>
     <p>
      在
      <code>
       AlexNet
      </code>
      的惊人成功之后，深度学习引起了极大的关注，它有点成为了人工智能的代名词。与传统的基于规则的算法不同，深度学习是一种数据驱动的方法，通过随机梯度下降优化模型参数。深度学习在获取卓越的特征表示方面的成功，取决于更好的网络架构和更多的数据，这极大地加速了
      <code>
       AIGC
      </code>
      的发展。
     </p>
     <h5>
      3.1.1 网络架构
     </h5>
     <p>
      众所周知，深度学习的两个主流领域是自然语言处理(NLP)和计算机视觉(CV)，它们的研究显著改进了骨干架构，并在其他领域启发了改进后骨干架构的各种应用，例如语音领域。在 NLP 领域，Transformer 架构已经取代了循环神经网络(RNN)成为事实上的标准骨干。而在 CV 领域，视觉 Transformer(ViT) 除了传统的卷积神经网络(CNN)外，也展示了其强大的性能。在这里，我们将简要介绍这些主流骨干架构的工作原理及其代表性的变种。
     </p>
     <ul>
      <li>
       <p>
        <strong>
         RNN
        </strong>
        &amp;
        <strong>
         LSTM
        </strong>
        &amp;
        <strong>
         GRU
        </strong>
       </p>
      </li>
     </ul>
     <p>
      <code>
       RNN
      </code>
      主要用于处理
      <strong>
       时间序列
      </strong>
      数据，例如
      <strong>
       语言
      </strong>
      或
      <strong>
       音频
      </strong>
      。标准的RNN有三层：
      <strong>
       输入层
      </strong>
      、
      <strong>
       隐藏层
      </strong>
      和
      <strong>
       输出层
      </strong>
      。
      <code>
       RNN
      </code>
      的信息流有两个方向，第一个方向是从输入到隐藏层再到输出的方向。而
      <code>
       RNN
      </code>
      中循环的本质在于其沿着时间方向的第二个信息流。除了当前的输入，当前时刻 𝑡 的隐藏状态还依赖于上一个时刻 𝑡−1 的隐藏状态。这种双向的设计很好地处理了序列顺序，但当序列变得很长时，会出现
      <strong>
       梯度消失
      </strong>
      或
      <strong>
       梯度爆炸
      </strong>
      的问题。
     </p>
     <p>
      为了缓解这个问题，引入了长短时记忆网络即
      <code>
       LSTM
      </code>
      ，其“细胞”状态充当了一个“高速公路”，有助于信息在序列方向上的流动。
      <code>
       LSTM
      </code>
      是减轻梯度爆炸/消失问题最流行的方法之一，但是由于它有三种门，因此会导致
      <strong>
       较高的复杂度和更高的内存需求
      </strong>
      。
     </p>
     <p>
      接下来出场的便是
      <strong>
       门控循环单元
      </strong>
      (
      <code>
       GRU
      </code>
      )，该技术通过将细胞状态和隐藏状态合并，并用所谓的更新状态替换遗忘门和输入门，简化了
      <code>
       LSTM
      </code>
      。
     </p>
     <p>
      最后，便是双向循环神经网络(
      <code>
       Bidirectional RNN
      </code>
      )，通过在细胞中捕获过去和未来信息来改进基本的
      <code>
       RNN
      </code>
      ，即时间 t 的状态是基于时间 t-1 和 t+1 计算的。根据任务不同，RNN 可以具有不同数量的输入和输出，例如一对一，多对一，一对多和多对多。其中多对多可以用于机器翻译，也称为
      <strong>
       序列到序列
      </strong>
      (
      <code>
       seq2seq
      </code>
      )模型。另一方面，注意力机制也被频繁引入，使得模型的解码器能够看到每个编码器标记，并根据其重要性动态更新权重。
     </p>
     <img alt="407fe0c8cd98802762542ec5b13354c3.png" src="https://i-blog.csdnimg.cn/blog_migrate/219efd3c463f42407fe3e3cb2593df97.png">
      <figcaption>
       图3.1 Transformer
      </figcaption>
      <ul>
       <li>
        <p>
         <strong>
          Transformer
         </strong>
        </p>
       </li>
      </ul>
      <p>
       与传统的
       <code>
        Seq2seq
       </code>
       模型相比，
       <code>
        Transformer
       </code>
       提出了自注意力机制，并将其成功应用于
       <code>
        Encoder-Decoder
       </code>
       模型中。Transformer 模型由
       <strong>
        编码器
       </strong>
       和
       <strong>
        解码器
       </strong>
       两部分组成，采用了
       <strong>
        残差连接
       </strong>
       和
       <strong>
        层归一化
       </strong>
       等技术，其中核心组件为
       <strong>
        多头注意力机制
       </strong>
       和
       <strong>
        前馈神经网络
       </strong>
       。多头注意力机制通过自注意力实现，并采用了多头的设计，而前馈神经网络则是由两个全连接层组成。这种自注意力机制的定义采用了缩放点积的形式，能够更好地处理序列中的依赖关系。
      </p>
      <p>
       与逐个输入句子信息以建立位置信息的 RNN 不同，Transformer 通过构建全局依赖关系获得强大的建模能力，但也因此失去了带有归纳偏差的信息。因此，需要使用位置编码使模型能够感知输入信号的位置信息。有两种类型的位置编码。固定位置编码用不同频率的正弦和余弦表示。可学习的位置编码由一组可学习参数组成。不可否认的是，Transformer 已俨然成为 CV 和 NLP 任务的标杆之作，由其衍生的门派数不胜数。
      </p>
      <ul>
       <li>
        <p>
         <strong>
          CNN
         </strong>
        </p>
       </li>
      </ul>
      <p>
       在 CV 领域，CNN 有着不可撼动的地位。CNN 的核心在于卷积层。卷积层中的卷积核（也称为滤波器）是一组共享的权重参数，用于对图像进行操作，其灵感来源于生物视觉皮层细胞。卷积核在图像上滑动并与像素值进行相关操作，最终得到特征映射并实现图像的特征提取。例如：
      </p>
      <ol>
       <li>
        <p>
         <code>
          GoogleNet
         </code>
         的 Inception 模块允许在每个块中选择多个卷积核大小，增加了卷积核的多样性，因此提高了CNN的性能；
        </p>
       </li>
       <li>
        <p>
         <code>
          ResNet
         </code>
         是 CNN 的一个里程碑，引入残差连接，稳定了训练，使模型能够通过更深的建模获得更好的性能。此后，它成为CNN中不可或缺的一部分；
        </p>
       </li>
       <li>
        <p>
         为了扩展 ResNet 的工作，
         <code>
          DenseNet
         </code>
         在所有先前层和后续层之间建立密集连接，从而使模型具有更好的建模能力；
        </p>
       </li>
       <li>
        <p>
         <code>
          EfficientNet
         </code>
         使用一种称为缩放方法的技术，使用一组固定的缩放系数来统一缩放卷积神经网络架构的宽度，深度和分辨率，从而使模型更加高效。
        </p>
       </li>
       <li>
        <p>
         而与 NLP 领域中的 Transformer 相似，
         <code>
          ViT
         </code>
         则是最近几年才在 CV 领域中引入的新的变体。ViT 使用 transformer 模块来处理图像，并在 Vision Transformer Encoder 中使用自注意力机制，而不是传统的卷积神经网络。ViT 将输入的图像分成一些小块，然后将这些小块变换成一系列的向量，这些向量将被送入 transformer 编码器。通过这种方式，ViT 可以利用 transformer 强大的建模能力来处理图像，并在许多计算机视觉任务中达到了与 CNN 相当的性能。
        </p>
       </li>
      </ol>
      <img alt="8ec52f3d2d0f087f89c786ca1029ec76.png" src="https://i-blog.csdnimg.cn/blog_migrate/abf344440b0a5a1ff7c58efd225646e9.png">
       <figcaption>
        图3.2 ViT
       </figcaption>
       <ul>
        <li>
         <p>
          <strong>
           ViT
          </strong>
         </p>
        </li>
       </ul>
       <p>
        Transformer 在 NLP 领域的成功启发了许多学者将其应用到 CV 领域，其中 ViT 是第一种采用 Transformer 的 CV 模型。ViT 将图像平铺为一系列二维块，并在序列的开头插入一个类别标记以提取分类信息。在嵌入位置编码之后，标记嵌入被输入到一个标准 Transformer 模型中。
       </p>
       <p>
        ViT 的这种简单有效的实现使其高度可扩展。例如：
       </p>
       <ol>
        <li>
         <p>
          <code>
           Swin
          </code>
          是通过在更深层次上合并图像块来构建分层特征映射，以高效地处理图像分类和密集识别任务，由于它仅在每个局部窗口内计算自注意力，因此减少了计算复杂度；
         </p>
        </li>
        <li>
         <p>
          <code>
           DeiT
          </code>
          采用教师-学生训练策略，通过引入蒸馏标记，减少了 Transformer 模型对大量数据的依赖性；
         </p>
        </li>
        <li>
         <p>
          <code>
           CaiT
          </code>
          引入了类别注意力机制以有效增加模型深度。
         </p>
        </li>
        <li>
         <p>
          <code>
           T2T
          </code>
          通过 Token Fusion 有效地定位模型，并通过递归地聚合相邻 Token 来引入 CNN 先验的层次化深而窄的结构。
         </p>
        </li>
       </ol>
       <p>
        通过
        <strong>
         置换等变性
        </strong>
        ，Transformer 从其翻译不变性中解放了 CNN，允许更长距离的依赖关系和更少的归纳偏差，使它们成为更强大的建模工具，并比 CNN 更适合于下游任务。在当前大模型和大数据集的范式下，Transformer 逐渐取代 CNN 成为计算机视觉领域的主流模型。
       </p>
       <h5>
        3.1.2 自监督学习
       </h5>
       <p>
        不可否认的是，深度学习能够从更好的骨干结构中获益，但自监督学习同样重要，该技术可以利用更大的无标签训练数据集。在这里，我们总结了最相关的自监督预训练技术，并根据训练数据类型（例如语言、视觉和联合预训练）对它们进行分类。
       </p>
       <img alt="af0bf3c705996402b81418a39521abab.png" src="https://i-blog.csdnimg.cn/blog_migrate/38b90f75d1f07c120f48fbfe8d43a61b.png">
        <figcaption>
         图3.3 BERT
        </figcaption>
        <ul>
         <li>
          <p>
           <strong>
            Language pretraining
           </strong>
          </p>
         </li>
        </ul>
        <p>
         语言预训练方法主要有三种主流的方法。第一种方法是使用掩码对编码器进行预训练，代表作是
         <code>
          BERT
         </code>
         。具体来说，BERT 从未掩码的语言标记预测掩码的语言标记。然而，掩码-预测任务和下游任务之间存在显着差异，因此像
         <code>
          BERT
         </code>
         这样的掩码语言建模在没有微调的情况下很少用于文本生成。
        </p>
        <p>
         相比之下，自回归语言预训练方法适用于少样本或零样本文本生成。其中最流行的是
         <code>
          GPT
         </code>
         家族，采用的是解码器而不是编码器。具体来说，
         <code>
          GPT-1
         </code>
         是第一种采用解码器的模型，
         <code>
          GPT-2
         </code>
         和
         <code>
          GPT-3
         </code>
         进一步研究了大规模数据和大型模型在转移能力中的作用。
        </p>
        <p>
         基于
         <code>
          GPT-3
         </code>
         ，
         <code>
          ChatGPT
         </code>
         的前所未有的成功近来引起了广泛关注。此外，一些语言模型采用了原始
         <code>
          Transformer
         </code>
         的编码器和解码器。
         <code>
          BART
         </code>
         使用各种类型的噪声扰动输入，预测原始干净的输入，类似于去噪自编码器。
         <code>
          MASS
         </code>
         和
         <code>
          PropheNet
         </code>
         采用了类似于
         <code>
          BERT
         </code>
         的方法，将掩码序列作为编码器的输入，解码器以自回归的方式预测掩码标记。
        </p>
        <img alt="2eca550d2ddc59c7bdf9a2e13946ae21.png" src="https://i-blog.csdnimg.cn/blog_migrate/944f42f92d7e8da045617b6a9b497c40.png">
         <figcaption>
          图3.4 MAE
         </figcaption>
         <ul>
          <li>
           <p>
            <strong>
             Visual pretraining
            </strong>
           </p>
          </li>
         </ul>
         <p>
          视觉预训练主要包含两种类型，第一种类型是基于掩码学习的无监督自编码器，它们旨在学习良好的图像表征
          <code>
           MAE
          </code>
          。第二种类型是基于自监督的预测模型，最流行的是
          <code>
           ImageNet
          </code>
          中学到的视觉特征(
          <code>
           ImageNet-pretraining
          </code>
          )和自监督学习方法，如
          <code>
           RotNet
          </code>
          和
          <code>
           MoCo
          </code>
          等等。这些方法先前采用的自监督任务包括但不仅限于
          <strong>
           图像旋转预测
          </strong>
          和
          <strong>
           图像补丁重建
          </strong>
          等。
         </p>
         <img alt="14a52b7cd175ad4f5b5fe4d8758f7a9c.png" src="https://i-blog.csdnimg.cn/blog_migrate/eb7a3c492d67f615e7e5366dcb2dd22c.png">
          <figcaption>
           图3.5 CLIP
          </figcaption>
          <ul>
           <li>
            <p>
             <strong>
              Joint pretraining
             </strong>
            </p>
           </li>
          </ul>
          <p>
           最后一种预训练方式是联合学习方法，它使用
           <strong>
            多模态
           </strong>
           输入进行联合预训练。通过从互联网上收集大量的图像和文本配对数据集，多模态学习取得了前所未有的进展，其中交叉模态匹配是关键技术。对比预训练被广泛应用于在同一表示空间中匹配图像嵌入和文本编码。其中，
           <code>
            CLIP
           </code>
           是最流行的一个，由
           <code>
            OpenAI
           </code>
           提出，它使用
           <strong>
            文本
           </strong>
           和
           <strong>
            图像
           </strong>
           作为联合输入，通过学习一个共同的嵌入空间来进行分类任务。
          </p>
          <p>
           此外，
           <code>
            SimCLR
           </code>
           和
           <code>
            DALL·E
           </code>
           都是联合学习的成功应用，前者使用自监督任务对图像进行增强，后者是一个生成模型，可以根据文字描述生成图像。
           <code>
            ALIGN
           </code>
           则扩展了 CLIP，使用嘈杂的文本监督，使得文本-图像数据集不需要清洗，可以扩展到更大的规模。Florence 进一步扩展了跨模态共享表示，从粗略场景到细粒度物体，从静态图像到动态视频等，因此，学习到的共享表示更加通用，表现出卓越的性能。
          </p>
          <h4>
           3.2 创作技术
          </h4>
          <p>
           <strong>
            深度生成模型
           </strong>
           (
           <code>
            DGMs
           </code>
           )是一组使用神经网络生成样本的概率模型，大体可分为两大类：基于
           <strong>
            似然
           </strong>
           的和基于
           <strong>
            能量
           </strong>
           的。基于似然的概率模型，如自回归模型和流模型，具有可追踪的似然，这为优化模型权重提供了一种直接的方法，即针对观察到（训练）数据的对数似然进行优化。变分自编码器(VAEs)中的似然则不完全可追踪，但可以优化可追踪的下限，因此，
           <code>
            VAE
           </code>
           也被认为属于基于似然的组，其指定了一个归一化的概率。相反，能量模型以未归一化概率即能量函数为特点。在没有对标准化常数可追踪性的限制下，能量模型在参数化方面更加灵活，但难以训练。此外，
           <code>
            GAN
           </code>
           和
           <strong>
            扩散模型
           </strong>
           虽然是从不同的时期发展而来，但与
           <strong>
            能量模型
           </strong>
           均密切相关。接下来，我们将介绍每一类基于似然的模型以及如何训练基于能量的模型以及 GAN 和扩散模型的机制。
          </p>
          <h5>
           3.2.1 Likelihood-based models
          </h5>
          <ul>
           <li>
            <p>
             <strong>
              Autoregressive models
             </strong>
            </p>
           </li>
          </ul>
          <p>
           <strong>
            自回归模型
           </strong>
           是一种可以用来预测序列数据的模型，它能够学习序列数据的联合分布，并且使用先前时间步的变量作为输入来预测每个变量在序列中的取值。这种模型假设序列数据的联合分布可以被分解成一系列条件分布的乘积，这也就是所说的“条件概率分解”。
          </p>
          <img alt="ee1832164a1dc934bd8909ce7c629f35.png" src="https://i-blog.csdnimg.cn/blog_migrate/a1d7f32f6dccf77aeedc0df4ee24d707.png">
           <figcaption>
            公式3.1
           </figcaption>
           <p>
            上面我们简单跟大家聊到过
            <code>
             RNN
            </code>
            ，本质上自回归模型和
            <code>
             RNN
            </code>
            都需要使用前面的时间步来预测当前时间步的值，但是它们的实现方式略有不同。在自回归模型中，前面的时间步直接作为输入提供给模型，而在 RNN 中，前面的时间步通过隐藏状态传递给模型。因此，可以将自回归模型看作是一个前馈神经网络，它接收前面所有时间步的变量作为输入。
           </p>
           <p>
            在早期的工作中，自回归模型主要用于建模离散数据。其中，
            <code>
             Fully Visible Sigmoid Belief Network, FVSBN
            </code>
            使用逻辑回归函数来估计条件分布，而
            <code>
             Neural Autoregressive Distribution Estimation, NADE
            </code>
            则使用单隐藏层的神经网络。随着研究的发展，自回归模型的应用逐渐扩展到连续变量的建模。自回归模型已经在多个领域得到了广泛应用，包括计算机视觉如
            <code>
             PixelCNN
            </code>
            和
            <code>
             PixelCNN++
            </code>
            、音频生成
            <code>
             WaveNet
            </code>
            和自然语言处理
            <code>
             Transformer
            </code>
            等等。这些应用中，自回归模型被用来生成图像、音频、文本等序列数据。
           </p>
           <ul>
            <li>
             <p>
              <strong>
               VAE
              </strong>
             </p>
            </li>
           </ul>
           <p>
            自编码器是一类相似的模型，它们通过编码器
            <code>
             Encoder
            </code>
            将输入数据映射到低维的潜在表示空间，然后再通过解码器
            <code>
             Decoder
            </code>
            将这个低维表示还原回原始数据。整个编码-解码的过程旨在学习输入数据的潜在结构，以便于重建数据和生成新的样本。
           </p>
           <p>
            变分自编码器
            <code>
             VAE
            </code>
            则是自编码器的一种变体，它使用了贝叶斯定理，通过学习潜在变量
            <code>
             Latent variable
            </code>
            的分布，从而学习原始数据的分布。为了训练 VAE，需要最大化一个较复杂的目标函数，它由一个最大化数据似然的项和一个正则化项组成。正则化项通常使用KL散度来度量潜在变量的分布和标准正态分布之间的差异。
           </p>
           <p>
            关于
            <code>
             AE
            </code>
            和
            <code>
             VAE
            </code>
            的介绍，请移步至微信公众号 CVHub 上点击
            <a href="" rel="nofollow">
             《万字长文带你入门变分自编码器》
            </a>
            自行查阅。
           </p>
           <h5>
            3.2.2  Energy-based models
           </h5>
           <p>
            由于自回归模型和流模型都具有可计算的似然函数即
            <code>
             tractable likelihood
            </code>
            ，因此可以直接通过最大化数据对数似然来优化模型的参数。然而，这种优化方法也限制了模型的形式。例如，自回归模型必须分解为一系列条件概率的乘积形式，而流模型必须采用可逆的转换。这些限制可能会使模型的表达能力受到一定的限制，但也有助于使模型更加可解释和可控。例如，自回归模型可以方便地计算条件概率分布，因此更适用于生成序列数据，而流模型则可以实现精确的概率密度估计，因此更适用于密度估计和采样等任务。
           </p>
           <p>
            能量模型则是一类非标准化概率模型，其概率可以表示为一个未知归一化常数的指数函数。假设能量模型只涉及单个变量 ，则它的能量函数可以表示为 ，对应的概率密度可以通过下面的公式计算得到：
           </p>
           <img alt="ed7af4c9430ab976f35e2ab151f74973.png" src="https://i-blog.csdnimg.cn/blog_migrate/a027d5f1568171b3707e45812183e7c5.png">
            <figcaption>
             公式3.2
            </figcaption>
            <p>
             其中  是未知的归一化常数，保证概率密度函数的积分等于 1。因为能量模型的概率密度函数没有直接给出归一化常数，所以它也被称为非标准化概率模型。
            </p>
            <ul>
             <li>
              <p>
               <strong>
                MCMC &amp; NCE
               </strong>
              </p>
             </li>
            </ul>
            <p>
             早期优化能量模型的方法采用了基于
             <code>
              MCMC
             </code>
             即马尔可夫链蒙特卡罗的方法来估计对数似然的梯度，但这需要进行繁琐的随机样本抽取。因此，一些工作旨在改善 MCMC 的效率，代表性的工作是
             <code>
              Langevin MCMC
             </code>
             。尽管如此，通过 MCMC 获取所需梯度需要大量的计算，而对比散度
             <code>
              contrastive divergence, CD
             </code>
             成为一种流行的方法，通过各种变体的近似来减少计算量，包括持久 CD ，平均场 CD 和多网格 CD 。
            </p>
            <p>
             另一条研究路线是通过噪声对比估计
             <code>
              Notice Contrastive Estimation, NCE
             </code>
             来优化能量模型，该方法将概率模型与另一个噪声分布进行对比。具体来说，它优化以下损失函数：
            </p>
            <img alt="4b31f7407ee37bde0153c2db006e320e.png" src="https://i-blog.csdnimg.cn/blog_migrate/57179fbc1a897ee5c2eb5d9b0ead3c8f.png"/>
            <figcaption>
             公式3.3
            </figcaption>
            <ul>
             <li>
              <p>
               <strong>
                Score matchingScore matching
               </strong>
              </p>
             </li>
            </ul>
            <p>
             得分匹配是一种用于优化基于能量的模型的无 MCMC 方法，旨在最小化模型和观察到的数据之间的对数概率密度的导数。但是，通常无法获得数据得分函数，而去噪得分匹配是一种代表性方法，它使用带噪声的样本来近似数据得分，通过迭代去除噪声，从而生成干净的样本。
            </p>
            <h4>
             3.2.3 from GAN to diffusion model
            </h4>
            <p>
             当涉及到深度生成模型时，您首先想到什么？答案取决于您的背景，但是 GAN 无疑是最常提到的模型之一。GAN 代表生成对抗网络，是由 Goodfellow 及其团队于 2014 年首次提出的，并于 2016 年被图灵奖 Yann Lecun 评为“机器学习领域过去10年中最有趣的想法”。
            </p>
            <p>
             最近，一种称为扩散模型(
             <code>
              diffusion model
             </code>
             )的新型深度生成模型家族挑战了 GAN 长期以来的统治地位。扩散模型在图像合成方面取得了压倒性的成功，并扩展到其他形式，如视频、音频、文本、图形等。考虑到它们对生成AI的发展的支配性影响，因此本文将集中围绕 GAN 和扩散模型进行讲解。
            </p>
            <img alt="db0e576c751d9aaa5a1e276c0e236885.png" src="https://i-blog.csdnimg.cn/blog_migrate/81b307d7c0d0bc82f0b4cad2ab28cc15.png"/>
            <figcaption>
             图3.6 GAN
            </figcaption>
            <ul>
             <li>
              <p>
               <strong>
                GAN
               </strong>
              </p>
             </li>
            </ul>
            <p>
             GAN 的架构如上图所示，它分别由两个网络组件组成，即鉴别器（D）和生成器（G）。其中，D 将真实图像与 G 生成的图像区分开来，而 G 的目标是欺骗 D。给定一个潜变量 ，G 的输出是 ，构成一个概率分布 。GAN 的目标是使  逼近观察数据分布 。通过对抗学习来实现这个目标，可以将其解释为一种最小-最大博弈：
            </p>
            <img alt="8331c30cfd66b7cdfe89db8d4920f5e7.png" src="https://i-blog.csdnimg.cn/blog_migrate/5842e2802c2ad3d2ab32016457116b48.png"/>
            <figcaption>
             公式3.4
            </figcaption>
            <p>
             GAN 的训练过程是通过鉴别器和生成器之间的博弈来实现的，最终的结果是一个鉴别器可以正确地将真实数据和生成数据区分开来，而生成器可以生成与真实数据相似的数据。另一方面，GAN 的不稳定性和生成样本缺乏多样性是其存在的缺陷，这是因为 GAN 的训练过程是通过对抗性的学习实现的。GAN 和自回归模型的基本区别在于 GAN 学习隐式数据分布，而后者学习的是受模型结构强制的显式分布。
            </p>
            <img alt="3df576dc262dca1ab899216162a6d08d.png" src="https://i-blog.csdnimg.cn/blog_migrate/edccdc583c8d9950f6873b035c8dc679.png"/>
            <figcaption>
             图3.7 diffusion model
            </figcaption>
            <ul>
             <li>
              <p>
               <strong>
                Diffusion model
               </strong>
              </p>
             </li>
            </ul>
            <p>
             过去几年中，使用扩散模型（一种特殊的分层VAE）的应用已经爆炸性增长。扩散模型，也被称为去噪扩散概率模型
             <code>
              DDPMs
             </code>
             或基于
             <code>
              score
             </code>
             的生成模型，其可以生成与训练数据相似的新数据。受非平衡热力学的启发，DDPM 可以被定义为参数化马尔可夫链，通过扩散步骤慢慢添加随机噪声到训练数据，并学习反向扩散过程以从纯噪声中构建所需的数据样本。下面我们以最简短的语言详细的看一下大致的原理。
            </p>
            <p>
             在正向扩散过程中，DDPM 通过连续添加高斯噪声来破坏训练数据。给定数据分布 ，DDPM 通过逐渐扰动输入数据，将训练数据映射到噪声。这通常通过一个简单的随机过程来实现，该过程从数据样本开始，迭代生成更嘈杂的样本 ，使用简单的高斯扩散核 ：
            </p>
            <img alt="83787a43ca7c4fb3650e5f8467799eb0.png" src="https://i-blog.csdnimg.cn/blog_migrate/8af06ea2f71a788f28b56d764100354c.png"/>
            <figcaption>
             公式3.5
            </figcaption>
            <p>
             通过上述第一行公式的迭代过程，我们可以获得任意步数  下的加噪图像，其中  和  是超参数，为扰动过程的步数和每一步扰动的幅度。为了简化讨论，我们这里只考虑使用高斯噪声作为转移核的情况，用  表示。经过一定的转换，我们得到任意步数  下的加噪图像，如下式所示：
            </p>
            <img alt="8760e50b445a273d5a9a7a0f1a344dde.png" src="https://i-blog.csdnimg.cn/blog_migrate/c6f9bf6dc91943ef58453d9f7ad1d6af.png"/>
            <figcaption>
             公式3.6
            </figcaption>
            <p>
             在反向去噪过程中，DDPM 通过执行迭代去噪来学习恢复数据，即通过撤销正向扩散来生成数据。这个过程代表了数据合成，DDPM 通过将随机噪声转化为真实数据来进行训练。它也被形式化定义为一个随机过程，从  开始迭代去噪输入数据，并生成可以遵循真实数据分布  的 。因此，该模型的优化目标如下：
            </p>
            <img alt="15568d3f4891318ac46f3fb8332a8feb.png" src="https://i-blog.csdnimg.cn/blog_migrate/e78532cfbcaddf3fe6ce8c95871f44ed.png"/>
            <figcaption>
             公式3.7
            </figcaption>
            <p>
             在 DDPM 中，正向扩散过程和反向去噪过程通常都需要使用数千个步骤来逐步注入噪声，以及在生成过程中进行去噪。因此，这会导致整个生成过程非常耗时，也是它一直被大家诟病的问题。不过，虽然技术的不断迭代，现如今越来越多快速 DDPM 的方法呈井喷式涌现出来，未来可期！
            </p>
            <h3>
             写在最后
            </h3>
            <p>
             关于《万字长文带你解读AIGC》系列之技术篇就先讲到这里，后续我们将围绕下列内容继续介绍：
            </p>
            <ul>
             <li>
              <p>
               《万字长文带你解读AIGC》系列之入门篇，主要介绍
               <code>
                AIGC
               </code>
               的相关概念并深入洞察分析其为什么会如此火爆的原因等；
              </p>
             </li>
             <li>
              <p>
               《万字长文带你解读AIGC》系列之技术篇，主要介绍
               <code>
                AIGC
               </code>
               背后的底层技术栈，如
               <code>
                Transforemr
               </code>
               、
               <code>
                SSL
               </code>
               、
               <code>
                VAE
               </code>
               、
               <code>
                GAN
               </code>
               、
               <code>
                Diffusion
               </code>
               等；
              </p>
             </li>
             <li>
              <p>
               《万字长文带你解读AIGC》系列之任务篇，主要介绍与
               <code>
                AIGC
               </code>
               相关的任务，如
               <code>
                ChatGPT
               </code>
               、图生文、文生图、多模态等；
              </p>
             </li>
             <li>
              <p>
               《万字长文带你解读AIGC》系列之应用篇，主要介绍
               <code>
                AIGC
               </code>
               产业的实际应用，如电影、音乐、代码、广告、游戏等；
              </p>
             </li>
             <li>
              <p>
               《万字长文带你解读AIGC》系列之总结篇，该篇章主要对上述内容进行一个全面的总结，集中讨论目前面临的挑战，并对生成式AI在不久的将来可能的发展进行相关的展望。
              </p>
             </li>
            </ul>
            <p style="text-align:center;">
             <strong>
              视频课程来了！
             </strong>
            </p>
            <p style="text-align:left;">
             <strong>
              <strong>
               自动驾驶之心为大家汇集了毫米波雷达视觉融合、高精地图、BEV感知、传感器标定、传感器部署、自动驾驶协同感知、语义分割、自动驾驶仿真、L4感知、决策规划、轨迹预测等多个方向学习视频，欢迎大家自取（扫码进入学习）
              </strong>
             </strong>
            </p>
            <p style="text-align:center;">
             <img alt="b63d3379f16b2d27827db46fe220d550.png" src="https://i-blog.csdnimg.cn/blog_migrate/bcd2e921711ecb9bc73b5e1cea381d77.png"/>
            </p>
            <p style="text-align:center;">
             （扫码学习最新视频）
            </p>
            <p style="text-align:center;">
             <strong>
              国内首个自动驾驶学习社区
             </strong>
            </p>
            <p>
             近1000人的交流社区，和20+自动驾驶技术栈学习路线，想要了解更多自动驾驶感知（分类、检测、分割、关键点、车道线、3D目标检测、Occpuancy、多传感器融合、目标跟踪、光流估计、轨迹预测）、自动驾驶定位建图（SLAM、高精地图）、自动驾驶规划控制、领域技术方案、AI模型部署落地实战、行业动态、岗位发布，欢迎扫描下方二维码，加入自动驾驶之心知识星球，这是一个真正有干货的地方，
             <strong>
              与领域大佬交流入门、学习、工作、跳槽上的各类难题，日常分享论文+代码+视频
             </strong>
             ，期待交流！
            </p>
            <p style="text-align:left;">
            </p>
            <p style="text-align:center;">
             <img alt="c10d5e8982298ba5aa2193543529ae00.jpeg" src="https://i-blog.csdnimg.cn/blog_migrate/b5f34c37d71e5e1954a490ae6399260d.png"/>
            </p>
            <p style="text-align:center;">
             【
             <strong>
              自动驾驶之心
             </strong>
             】全栈技术交流群
             <br/>
            </p>
            <p>
             <strong>
              自动驾驶之心是首个自动驾驶开发者社区，聚焦目标检测、语义分割、全景分割、实例分割、关键点检测、车道线、目标跟踪、3D目标检测、BEV感知、多传感器融合、SLAM、光流估计、深度估计、轨迹预测、高精地图、NeRF、规划控制、模型部署落地、自动驾驶仿真测试、产品经理、硬件配置、AI求职交流等方向；
             </strong>
            </p>
            <p style="text-align:center;">
             <img alt="cede7db8d4dc343734be836a6c3248c6.jpeg" src="https://i-blog.csdnimg.cn/blog_migrate/86e6e4f1c591d8601f536bcff6b3f617.png"/>
            </p>
            <p style="text-align:center;">
             <strong>
              添加汽车人助理微信邀请入群
             </strong>
            </p>
            <p style="text-align:center;">
             <strong>
              备注：学校/公司+方向+昵称
             </strong>
            </p>
           </img>
          </img>
         </img>
        </img>
       </img>
      </img>
     </img>
    </div>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f43565f4175746f626f742f:61727469636c652f64657461696c732f313330303531313636" class_="artid" style="display:none">
 </p>
</div>
