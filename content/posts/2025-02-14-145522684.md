---
arturl_encode: "68747470733a2f2f626c6f672e6373:646e2e6e65742f73756e79756875615f6b6579626f6172642f:61727469636c652f64657461696c732f313435353232363834"
layout: post
title: "Ollama部署-DeepSeek-R170B-模型的详细步骤"
date: 2025-02-14 14:57:58 +08:00
description: "通过上述步骤，你可以在 Ollama 上成功部署 DeepSee"
keywords: "ollama run deepseek-r1:70b"
categories: ['未分类']
tags: ['前端', 'Chrome']
artid: "145522684"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=145522684
  alt: "Ollama部署-DeepSeek-R170B-模型的详细步骤"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145522684
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145522684
---

# Ollama部署 DeepSeek-R1:70B 模型的详细步骤

#### **1. 确认环境准备**

##### **(1) 硬件要求**

* **显存需求**
  ：70B 参数的模型需要大量显存。若使用 NVIDIA T4（16GB 显存），需多卡并行（如 8 卡）或开启量化（如
  `q4_0`
  、
  `q8_0`
  ）。
* **内存需求**
  ：建议至少 64GB 系统内存。
* **存储空间**
  ：模型文件约 40-140GB（取决于量化方式）。

##### **(2) 软件依赖**

* 安装
  **Ollama**
  （确保版本支持自定义模型）：

  ```bash
  curl -fsSL https://ollama.com/install.sh | sh

  ```

---

#### **2. 下载 DeepSeek-R1:70B 模型**

##### **(1) 若模型已存在于本地**

* 直接通过 Ollama 加载：

  ```bash
  ollama run deepseek-r1:70b

  ```

##### **(2) 若需自定义模型**

* 创建
  `Modelfile`
  定义模型参数（示例）：

  ```dockerfile
  FROM deepseek-r1:70b
  PARAMETER num_gpu 8  # 使用 8 卡 GPU
  PARAMETER num_ctx 4096  # 上下文长度
  PARAMETER quantize q4_0  # 量化方式（可选）

  ```
* 构建自定义模型：

  ```bash
  ollama create deepseek-r1-custom -f Modelfile

  ```

---

#### **3. 启动 Ollama 服务**

##### **(1) 启动模型**

* 前台运行（调试模式）：

  ```bash
  ollama serve

  ```
* 后台运行（生产环境）：

  ```bash
  systemctl start ollama

  ```

##### **(2) 检查模型状态**

* 查看已加载模型：

  ```bash
  ollama list

  ```

  输出应包含：

  ```
  NAME            ID              SIZE      MODIFIED
  deepseek-r1:70b 0c1615a8ca32    42 GB     2 hours ago

  ```

---

#### **4. 配置外部访问**

默认情况下，Ollama 仅监听
`127.0.0.1:11434`
，需修改为允许外部访问：

##### **(1) 修改监听地址**

* 编辑 Ollama 环境变量：

  ```bash
  sudo vim /etc/systemd/system/ollama.service

  ```

  添加：

  ```ini
  [Service]
  Environment="OLLAMA_HOST=0.0.0.0:11434"

  ```
* 重启服务：

  ```bash
  sudo systemctl daemon-reload
  sudo systemctl restart ollama

  ```

##### **(2) 开放防火墙端口**

* 开放
  `11434`
  端口：

  ```bash
  sudo ufw allow 11434/tcp
  sudo ufw reload

  ```

##### **(3) 验证监听状态**

```bash
netstat -tuln | grep 11434

```

输出应为：

```
tcp  0  0 0.0.0.0:11434  0.0.0.0:*  LISTEN

```

---

#### **5. 调用模型 API**

##### **(1) 通过 Curl 测试**

```bash
curl http://<服务器IP>:11434/api/generate -d '{
  "model": "deepseek-r1:70b",
  "prompt": "你好，DeepSeek！",
  "stream": false
}'

```

##### **(2) 使用 Python 客户端**

```python
import requests

response = requests.post(
"http://<服务器 IP>:11434/api/generate",
json={
"model": "deepseek-r1:70b",
"prompt": "如何部署大模型？",
"stream": False
}
)
print(response.json()["response"])

```

---

#### **6. 性能优化**

##### **(1) 多 GPU 并行**

* 启动时指定 GPU 数量：

  ```bash
  OLLAMA_NUM_GPU=8 ollama serve

  ```

##### **(2) 量化模型**

* 使用
  `q4_0`
  或
  `q8_0`
  量化减少显存占用：

  ```bash
  ollama run deepseek-r1:70b --quantize q4_0

  ```

##### **(3) 调整批处理大小**

* 在
  `Modelfile`
  中设置：

  ```dockerfile
  PARAMETER num_batch 512 # 根据显存调整

  ```

---

#### **7. 常见问题解决**

##### **(1) 显存不足**

* **现象**
  ：
  `CUDA out of memory`
* **解决**
  ：
  + 减少
    `num_batch`
    。
  + 启用量化（
    `quantize q4_0`
    ）。
  + 增加 GPU 数量。

##### **(2) 服务无法启动**

* **现象**
  ：
  `Failed to bind port 11434`
* **解决**
  ：
  + 检查端口占用：
    `lsof -i :11434`
    。
  + 关闭冲突进程或更换端口。

##### **(3) 模型加载失败**

* **现象**
  ：
  `Model deepseek-r1:70b not found`
* **解决**
  ：
  + 确认模型文件路径正确。
  + 重新下载模型：
    `ollama pull deepseek-r1:70b`
    。

---

#### **总结**

通过上述步骤，你可以在 Ollama 上成功部署 DeepSeek-R1:70B 模型，并支持外部网络访问。如果遇到性能问题，优先通过
**量化**
和
**多 GPU 并行**
优化资源占用。若需进一步扩展，可结合 Kubernetes 或 Docker Swarm 实现集群化部署。