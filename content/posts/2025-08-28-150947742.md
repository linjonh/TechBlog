---
layout: post
title: "多模态大模型研究每日简报2025-08-28"
date: 2025-08-28T13:31:38+0800
description: "训练数据方面，SCAR框架提出多模态数据集评价指标，多个团队发布了孟加拉语、韩语等低资源数据集及儿科问答基准。智能体研究关注强化学习优化和人机协作，如SWIRL框架和InquireMobile系统。训练策略上，提出轻量级知识整合框架NLKI和自奖励视觉推理方法Vision-SR1。行业应用涵盖机器人控制、电商分类、医疗预测等领域，如Long-VLA机器人模型和动脉瘤预测网络MCMeshGAN。"
keywords: "多模态大模型研究每日简报【2025-08-28】"
categories: ['未分类']
tags: ['论文阅读', '计算机视觉', '机器学习', '人工智能']
artid: "150947742"
arturl: "https://blog.csdn.net/shayuchaor/article/details/150947742"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150947742
    alt: "多模态大模型研究每日简报2025-08-28"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150947742
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150947742
cover: https://bing.ee123.net/img/rand?artid=150947742
image: https://bing.ee123.net/img/rand?artid=150947742
img: https://bing.ee123.net/img/rand?artid=150947742
---



# 多模态大模型研究每日简报【2025-08-28】



**训练数据相关**

* **SCAR: A Characterization Scheme for Multi-Modal Dataset** (<https://arxiv.org/abs/2508.19659>): 提出了一种用于表征数据集内在结构属性的原则性方案 SCAR，包括尺度（Scale）、覆盖率（Coverage）、真实性（Authenticity）和丰富度（Richness）四个关键指标，用于多模态数据集的分析和构建。
* **Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement** (<https://arxiv.org/abs/2508.19887>): 发布了一个孟加拉语的视觉问答数据集Bangla-Bayanno，包含超过52K的问答对，旨在促进低资源多模态学习的研究。该数据集使用了LLM辅助的翻译改进流程，以确保数据质量。
* **KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts** (<https://arxiv.org/abs/2508.19944>): 提出了一个韩语文本丰富的视觉问答基准 KRETA，用于评估视觉语言模型在理解和推理视觉环境中的文本的能力。
* **PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark** (<https://arxiv.org/abs/2508.16439>): 提出了一个新的综合性多模态儿科问答基准 PediatricsMQA，旨在解决大型语言模型中存在的年龄偏差问题，该基准包含基于文本和视觉的多项选择题，覆盖了从出生前到青春期的 131 个儿科主题。
* **RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation** (<https://arxiv.org/abs/2506.18088>): 提出了一个用于自动生成大规模多样且逼真数据的框架RoboTwin 2.0，以及用于双臂操作的统一评估协议。核心是一个包含731个物体实例的物体库RoboTwin-OD，具有语义和操作相关注释。

**Agent相关**

* **SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control** (<https://arxiv.org/abs/2508.20018>): 提出了一种用于多智能体系统的交错强化学习的分阶段工作流程SWIRL。SWIRL 将 MARL 重新构建为一系列单智能体强化学习任务，一次更新一个智能体，同时保持其他智能体固定。这种公式实现了稳定的训练并促进了智能体之间的有效协调。
* **InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning** (<https://arxiv.org/abs/2508.19679>): 提出一种交互式系统，该系统在关键决策点主动寻求人类确认。提出了一个受强化学习启发的模型InquireMobile，具有两阶段训练策略和交互式预动作推理机制，在InquireBench上实现了46.8%的查询成功率提升。
* **Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations** (<https://arxiv.org/abs/2508.19942>): 探讨了如何使用社交互动Agent (SIA) 作为 AI 驱动的知识转移促进者，通过多模态行为（口头、副语言、非语言）与用户进行自主和社交智能的互动，模拟各种组织环境中的专家角色。

**训练策略**

* **NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks** (<https://arxiv.org/abs/2508.19724>): 提出了一个端到端框架 (NLKI)，用于将常识知识整合到小型视觉语言模型 (sVLM) 中，通过检索自然语言事实并提示 LLM 制作自然语言解释，从而提升 sVLM 在常识视觉问答任务中的表现。
* **Self-Rewarding Vision-Language Model via Reasoning Decomposition** (<https://arxiv.org/abs/2508.19652>): 提出了一种自奖励方法 Vision-SR1，该方法通过强化学习改进视觉推理，而无需外部视觉监督。Vision-SR1 将 VLM 推理分解为视觉感知和语言推理两个阶段，并通过自包含验证和重提示来优化模型。
* **A Survey on Training-free Alignment of Large Language Models** (<https://arxiv.org/abs/2508.09016>): 综述了大型语言模型的免训练对齐方法，包括预解码、解码中和后解码三个阶段的技术，讨论了它们的机制和局限性，并指出了关键挑战和未来方向。

**大模型的行业应用**

* **Visio-Verbal Teleimpedance Interface: Enabling Semi-Autonomous Control of Physical Interaction via Eye Tracking and Speech** (<https://arxiv.org/abs/2508.20037>): 提出了一种视觉-口头遥阻抗界面，通过操作员的视线和口头互动相结合，将 3D 刚度椭球体命令发送到远程机器人。
* **Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach** (<https://arxiv.org/abs/2508.20013>): 通过开发和部署多模态分层分类框架，解决电子商务产品分类中关键的行业挑战，即平台异构性和现有分类法的结构限制。
* **MathBuddy: A Multimodal System for Affective Math Tutoring** (<https://arxiv.org/abs/2508.19993>): 提出了一种情感感知的 LLM 驱动的数学辅导系统 MathBuddy，该系统动态地对学生的情绪进行建模，并将其映射到相关的教学策略，使辅导员与学生的对话更具同理心。
* **Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation** (<https://arxiv.org/abs/2508.19958>): 提出了一种专为长时程机器人任务设计的端到端 VLA 模型 Long-VLA。该方法采用了一种新的相位感知输入掩蔽策略，该策略自适应地将每个子任务分割成移动和交互阶段，使模型能够专注于与相位相关的感觉线索并增强子任务兼容性。
* **Scalable Object Detection in the Car Interior With Vision Foundation Models** (<https://arxiv.org/abs/2508.19651>): 提出了一种用于车内场景理解的新型对象检测和定位（ODAL）框架，该框架通过分布式架构利用视觉基础模型，将计算任务分解到车载系统和云端之间。
* **Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction** (<https://arxiv.org/abs/2508.19862>): 提出了第一个用于 3D 动脉瘤生长预测的多模态条件网格到网格生成对抗网络 MCMeshGAN。
* **HoneyBee: A Scalable Modular Framework for Creating Multimodal Oncology Datasets with Foundational Embedding Models** (<https://arxiv.org/abs/2405.07460>): HONeYBEE 是一个开源框架，它集成了多模态生物医学数据以用于肿瘤学应用。

**文生图/文生视频**

* **AudioStory: Generating Long-Form Narrative Audio with Large Language Models** (<https://arxiv.org/abs/2508.20088>): 提出了一种统一的框架AudioStory，该框架将大型语言模型 (LLM) 与 TTA 系统集成，以生成结构化的长篇音频叙事。AudioStory 具有强大的指令跟随推理生成能力，采用 LLM 将复杂的叙事查询分解为具有上下文线索的时间排序子任务，从而实现连贯的场景转换和情感基调一致性。
* **Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models** (<https://arxiv.org/abs/2508.19791>): 提出了一项关于颜色的案例研究——颜色是文本提示中通常与对象关联的基本属性，它为严格的评估提供了丰富的测试平台。分析表明，预训练模型很难生成忠实反映多个颜色属性的图像，提出了一种专门的图像编辑技术。

**底层模型架构**

* **Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies** (<https://arxiv.org/abs/2508.20072>): 提出 Discrete Diffusion VLA，一个单transformer策略，使用离散扩散对离散动作块进行建模，并使用与 VLM 主干相同的交叉熵目标进行训练。
* **Apple Intelligence Foundation Language Models: Tech Report 2025** (<https://arxiv.org/abs/2507.13575>): 介绍了两种多语言、多模态基础语言模型，为 Apple 设备和服务上的 Apple Intelligence 功能提供支持：i 一个 30 亿参数的设备上模型，通过 KV 缓存共享和 2 位量化感知训练等架构创新针对 Apple 芯片进行了优化；ii 一个可扩展的服务器模型，建立在一种新型的并行跟踪混合专家 PT-MoE 转换器上，该转换器结合了跟踪并行性、混合专家稀疏计算和交错的全局-局部注意力，以在 Apple 的私有云计算平台上以具有竞争力的成本提供高质量。

**其他**

* **11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis** (<https://arxiv.org/abs/2508.20068>): 提出了一个系统评估框架，以评估最先进的 MLLM 相对于人类表现的空间推理能力。核心是 11Plus-Bench，这是一个源自现实标准化空间能力测试的高质量基准。
* **GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity** (<https://arxiv.org/abs/2508.19972>): 提出了一种新颖的免训练对象幻觉检测框架 GLSim，该框架利用图像和文本模态之间互补的全局和局部嵌入相似性信号，从而能够在各种场景中进行更准确和可靠的幻觉检测。
* **Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models** (<https://arxiv.org/abs/2508.19967>): 对 25 个最先进的 VLM 在四个基准图像数据集上的地理定位能力进行了全面评估，这些数据集是在不同的环境中捕获的。
* **TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations** (<https://arxiv.org/abs/2508.19866>): 提出了一种新颖的基于transformer的模型 TrajFusionNet，该模型将未来的行人轨迹和车辆速度预测相结合，作为预测交叉意图的先验。
* **FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter** (<https://arxiv.org/abs/2508.19639>): 提出了一种新的基于 VLM 的框架 FakeSV-VLM，用于检测短视频平台上的虚假新闻。
* **Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model** (<https://arxiv.org/abs/2508.19626>): 提出了一种模型 LF-VAR，该模型利用量化的病灶测量分数和病灶类型标签来指导皮肤图像的临床相关和可控合成，基于语言提示实现具有特定病灶特征的可控皮肤合成。
* **Explain Before You Answer: A Survey on Compositional Visual Reasoning** (<https://arxiv.org/abs/2508.17298>): 对 2023 年至 2025 年期间的 260 多篇论文进行了全面调查，这些论文来自顶级会议（CVPR、ICCV、NeurIPS、ICML、ACL 等），系统地回顾了构图视觉推理文献。
* **HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs** (<https://arxiv.org/abs/2508.10576>): 引入了 HumanSense，这是一个综合基准，旨在评估 MLLM 的以人为中心的感知和交互能力，特别关注对扩展多模态上下文的深刻理解和合理反馈的制定。
* **Step-Audio 2 Technical Report** (<https://arxiv.org/abs/2507.16632>): 提出 Step-Audio 2，这是一个端到端的多模态大型语言模型，专为行业强度的音频理解和语音对话而设计。
* **Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents** (<https://arxiv.org/abs/2507.14819>): 提出了一种从文档中生成基于意图的图表的任务：给定用户指定的意图和文档，目标是生成符合该意图并基于文档的图表。
* **EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents** (<https://arxiv.org/abs/2505.11717>): 提出了一种新的攻击 EnvInjection，该攻击通过将扰动添加到渲染网页的原始像素值来操纵环境，从而诱导 Web 代理执行目标操作。
* **Do Vision Encoders Truly Explain Object Hallucination?: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore** (<https://arxiv.org/abs/2502.20034>): 提出了一种更精细的评估指标 Fine-grained CLIPScore (F-CLIPScore)，通过在名词级别结合文本嵌入来增强对象级别的粒度。
* **Know “No” Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP** (<https://arxiv.org/abs/2501.10913>): 提出了数据生成管道，该管道利用大型语言模型 (LLM) 和多模态 LLM 来生成包含否定意义的字幕。
* **On Domain-Adaptive Post-Training for Multimodal Large Language Models** (<https://arxiv.org/abs/2411.19930>): 研究了通过后训练对 MLLM 进行领域自适应，重点关注数据合成、训练流程和任务评估。
* **ReCLIP++: Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation** (<https://arxiv.org/abs/2408.06747>): 提出显式地建模和纠正 CLIP 中存在的偏差，以促进无监督语义分割任务。

#### 编辑精选

1. **RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation** (<https://arxiv.org/abs/2506.18088>): RoboTwin 2.0 提供了一个可扩展的框架，用于自动生成多样化和逼真的数据，以及用于双臂操作的统一评估协议，对于推动实际机器人操作具有重要意义。
2. **SCAR: A Characterization Scheme for Multi-Modal Dataset** (<https://arxiv.org/abs/2508.19659>): SCAR 为多模态数据集的结构属性提供了一种原则性的表征方案，有助于更深入地理解数据特性对模型泛化能力的影响，并为数据驱动的 AI 研究提供指导。
3. **Self-Rewarding Vision-Language Model via Reasoning Decomposition** (<https://arxiv.org/abs/2508.19652>): Vision-SR1 通过自奖励和推理分解的方式，有效提升了视觉语言模型的视觉推理能力，缓解了幻觉问题，为 VLM 的训练和改进提供了一种有价值的思路。



