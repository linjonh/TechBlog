---
layout: post
title: "dify报错Failed-to-establish-a-rconnection-Errno-111Connection-refused"
date: 2025-03-08 17:19:54 +0800
description: "dify报错：Failed to establish a rconnection: 【Errno 111】Connection refused。开放局域网即可。"
keywords: "dify报错：Failed to establish a rconnection: 【Errno 111】Connection refused"
categories: ['未分类']
tags: ['人工智能']
artid: "146119203"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146119203
    alt: "dify报错Failed-to-establish-a-rconnection-Errno-111Connection-refused"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146119203
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146119203
cover: https://bing.ee123.net/img/rand?artid=146119203
image: https://bing.ee123.net/img/rand?artid=146119203
img: https://bing.ee123.net/img/rand?artid=146119203
---

# dify报错：Failed to establish a rconnection: 【Errno 111】Connection refused

在深度学习和大模型应用日益普及的今天，本地化部署因其安全性、性能和定制性受到了广泛关注。笔者在进行deepseek本地化部署的过程中，尝试使用ollama+dify或lm-
studio+dify的组合来添加本地大模型，却遇到了一些棘手的问题。本文将详细记录这一问题的出现、排查过程及最终的解决方案，希望能为同样遇到此类问题的朋友们提供一些参考和帮助。

### 问题现象

在配置过程中，当尝试通过dify添加本地大模型时，系统提示出错信息：“Failed to establish a rconnection: 【Errno
111】Connection
refused”。这一错误表明系统在尝试建立连接时被目标主机拒绝，无法完成通信。由于本次部署是在Linux环境下进行的，与之前在Windows环境下的部署有所不同，因此需要针对Linux系统的特点进行排查。

### 排查过程

#### 1\. 基本网络测试

首先，为了确认网络连接是否正常，我通过浏览器访问了相应的地址。输入`127.0.0.1`（本地回环地址）进行测试，发现网页可以正常加载，这表明本地网络连接是没有问题的。

#### 2\. Docker相关地址测试

接着，考虑到部署过程中使用了Docker容器技术，我对Docker相关的地址进行了访问测试。遗憾的是，在尝试访问这些地址时，出现了预期的出错情况。由于Docker容器的网络隔离机制，直接通过主机地址访问容器内的服务往往是不通的，这一点在Linux环境下尤为明显。因此，这一步的测试结果虽然显示出错，但也在情理之中。

#### 3\. 深入排查

在确认了基本网络连接无问题后，问题依然存在，这说明问题可能出在更深层次的配置上。我开始深入研究ollama和lm-
studio的配置文件，检查端口映射、网络设置等是否正确。同时，也查阅了大量相关文档和社区论坛，希望能从中找到解决问题的线索。

### 解决方案

经过一番艰苦的排查，终于找到了问题的症结所在。原来，在Linux环境下，Docker容器默认只监听容器内部的网络接口，而不会自动映射到主机的网络接口上。这就导致了虽然可以通过Docker内部访问服务，但无法从主机（或外部网络）直接访问。

为了解决这个问题，我分别对ollama和lm-studio进行了相应的配置调整：

#### 1\. 设置OLLAMA_HOST

对于ollama服务，我设置环境变量`OLLAMA_HOST=0.0.0.0`。这一设置使得ollama服务监听所有网络接口，而不仅仅是容器内部接口，从而允许从主机或其他设备上访问。

#### 2\. 打开lm-studio的网络服务

在lm-studio的设置中，我找到了“在网络中提供服务”这一选项，并将其打开。这一步骤确保了lm-studio服务能够接收来自外部网络的连接请求。

完成以上设置后，我重启了相关服务，并再次尝试通过dify添加本地大模型。这一次，连接成功建立，模型顺利添加到了dify平台中。

### 结语

本次在deepseek本地化部署中遇到的“Failed to establish a rconnection: 【Errno 111】Connection
refused”问题，最终通过调整OLLAMA_HOST环境变量和lm-
studio的网络配置得到了解决。这一过程不仅加深了我对Linux网络配置和Docker容器技术的理解，也提醒我在遇到问题时，需要耐心排查、细致分析，才能找到问题的根源并有效解决。

希望本文的分享能对正在经历类似问题的朋友们有所帮助，让我们一起在探索深度学习和大模型应用的道路上越走越顺！



