---
layout: post
title: "Qwen-VL阿里通义千问视觉语言模型模型架构和损失函数介绍"
date: 2025-09-10T00:59:03+0800
description: "组件功能实现特点视觉编码器提取图像特征基于CLIP的ViT-BigG强大的视觉特征提取能力视觉-语言适配器连接视觉与文本特征空间带位置注入的MLP/Cross-Attention核心创新点之一，注入空间位置信息大语言模型多模态信息融合与推理强大的语言理解和生成能力训练策略高效学习多模态能力三阶段（预训练-&gt;SFT-&gt;RLHF）循序渐进，高效且性能强大Qwen-VL架构的核心思想。"
keywords: "Qwen-VL（阿里通义千问视觉语言模型）模型架构和损失函数介绍"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "151376513"
arturl: "https://blog.csdn.net/qq_54708219/article/details/151376513"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151376513
    alt: "Qwen-VL阿里通义千问视觉语言模型模型架构和损失函数介绍"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151376513
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151376513
cover: https://bing.ee123.net/img/rand?artid=151376513
image: https://bing.ee123.net/img/rand?artid=151376513
img: https://bing.ee123.net/img/rand?artid=151376513
---



# Qwen-VL（阿里通义千问视觉语言模型）模型架构和损失函数介绍



---

Qwen-VL是一个强大的**视觉-语言（Vision-Language, VL）大模型**，它能够同时理解图像和文本，并完成诸如视觉问答（VQA）、图像标注（Captioning）、指代表达理解（REC）等复杂任务。其核心架构可以概括为 **“三大组件”** 和 **“三阶段训练”**。

#### 核心架构三大组件

Qwen-VL的架构主要包含三个核心部分，其整体工作流程如下图所示（概念上）：

1. **视觉编码器 (Vision Encoder) - 处理图像**
2. **位置感知的视觉-语言适配器 (Position-aware Vision-Language Adapter) - 连接桥梁**
3. **大语言模型 (Large Language Model, LLM) - 处理文本和推理**

##### 1. 视觉编码器 (Vision Encoder)

* **功能**： 负责将输入的图像（例如 448x448 分辨率）转换为一系列抽象的视觉特征（Visual Tokens），相当于把图像“翻译”成LLM能看懂的“语言”。
* **实现**： Qwen-VL采用了 **ViT（Vision Transformer）** 架构，具体是基于 **OpenAI的CLIP模型中的ViT-BigG** 版本进行初始化。
* **工作流程**：

  1. **图像分块 (Patchify)**： 将输入图像分割成固定大小（如14x14像素）的 patches。
  2. **线性投射 (Linear Projection)**： 每个图像块被展平并通过一个线性层（Linear Layer）映射到一个高维向量（也称为视觉token或视觉特征）。
  3. **添加位置编码**： 为每个视觉token添加位置信息，因为Transformer本身不具备空间感知能力。
  4. **通过Transformer层**： 这些视觉token序列会输入到ViT的Transformer层中进行深层编码和交互，最终输出一系列富含语义信息的视觉特征向量。
* **输出**： 一个视觉特征序列，假设输入图像被分成 `N` 个patches，则输出一个形状为 `[N, D]` 的矩阵，其中 `D` 是特征维度。

##### 2. 位置感知的视觉-语言适配器 (Position-aware Vision-Language Adapter)

* **功能**： 这是连接视觉编码器和LLM的**关键桥梁**。它的作用有两个：

  1. **维度对齐**： 将视觉编码器输出的高维视觉特征（维度 `D_vision`）投影到LLM的文本特征空间（维度 `D_llm`）。
  2. **注入位置信息**： 这是Qwen-VL的一个**创新点**。普通的Adapter可能只做简单的线性投影，但Qwen-VL的Adapter会**显式地将每个视觉token对应的位置坐标（如x, y, width, height）也进行编码并注入**到特征中。这使得LLM不仅能理解图像内容，还能知道内容的具体位置，这对于指代表达理解（REC）等需要定位的任务至关重要。
* **实现**： 通常是一个轻量级的模块，例如一个两层的前馈神经网络（MLP），或者使用交叉注意力（Cross-Attention）机制。

##### 3. 大语言模型 (Large Language Model) - 核心大脑

* **功能**： 作为模型的“大脑”，负责理解和融合视觉与文本信息，并最终生成响应（答案、描述等）。
* **实现**： Qwen-VL基于阿里自研的**Qwen（通义千问）语言模型**作为其LLM主干网络。Qwen本身是一个decoder-only的Transformer架构模型。
* **工作流程**：

  1. **输入拼接**： 经过Adapter处理后的视觉tokens（现在可以看作是“图像语言”的tokens）与用户输入的文本tokens拼接在一起，形成一个多模态序列。
  2. **因果建模**： 这个组合序列被送入LLM。LLM以自回归（Autoregressive）的方式处理这个序列，就像处理纯文本一样。它会根据之前的所有token（包括视觉和文本）来预测下一个token。
  3. **输出生成**： 最终，LLM输出文本形式的答案或描述。例如，对于问题“图片里有什么？”，LLM会生成一个描述性句子；对于“框出左边的猫”，LLM可能会生成一个代表坐标的文本序列 `<box>(x1, y1), (x2, y2)</box>`。

#### 三阶段训练策略

为了让模型高效地掌握多模态能力，Qwen-VL采用了精心设计的三阶段训练流程：

1. **预训练阶段 (Pre-training)**

   * **目标**： 让模型学习视觉和语言概念之间的基本对齐。例如，学会“猫”这个词对应图像中的猫的样子。
   * **数据**： 使用大规模的**图像-文本对**数据（如LAION、COYO等）。
   * **任务**： 类似于“看图说话”，给定一张图像，让模型生成对应的文本描述。在这个阶段，**视觉编码器和LLM的参数是冻结的**，只训练**Visual-Language Adapter**，成本低且高效。
2. **多任务监督微调阶段 (Multi-task Supervised Fine-tuning)**

   * **目标**： 教会模型执行各种具体的下游任务，如VQA、REC、Captioning等。
   * **数据**： 使用多种任务的标注数据混合在一起（如VQA v2、RefCOCO、Visual Genome等）。
   * **任务**： 将不同任务的数据统一转换为“指令-响应”的格式。例如，对于REC任务，输入可能是“<image>请框出：穿红色衣服的人”，期望输出是“<box>(100, 150, 200, 300)</box>”。这个阶段会**解锁并训练所有模型参数**（视觉编码器、Adapter、LLM），让模型全面适应多模态任务。
3. **强化学习人类偏好对齐阶段 (RLHF - Reinforcement Learning from Human Feedback) - 仅Qwen-VL-Chat**

   * **目标**： 让模型的输出更符合人类的偏好，比如更有帮助、更安全、更无害。
   * **方法**： 使用人类反馈数据训练一个奖励模型（Reward Model, RM），然后通过强化学习算法（如PPO）来微调模型，使其生成的回答能获得奖励模型的高分。

#### 损失函数设计

Qwen-VL的损失函数设计与它的**三阶段训练策略**紧密耦合，每个阶段的目标不同，其核心的损失函数也随之变化。其核心思想是：**在所有阶段，都统一使用下一个token预测（Next Token Prediction）的自回归（Autoregressive）损失函数，但不同阶段的数据构造和训练权重决定了模型的学习方向。**

#### 核心损失函数：自回归语言建模损失

无论处理的是文本还是图像，Qwen-VL最终都将所有输入（图像、文本、边界框）转换成一个由token组成的序列。模型的任务始终是：**给定之前的所有token，预测序列中的下一个token。**

其损失函数是标准的**交叉熵损失（Cross-Entropy Loss）**，也称为**语言建模损失**。公式如下：

![\mathcal{L}(\theta) = -\frac{1}{N} \sum_{i=1}^{N} \log P(t_i | t_{<i}, V; \theta)](https://latex.csdn.net/eq?%5Cmathcal%7BL%7D%28%5Ctheta%29%20%3D%20-%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5E%7BN%7D%20%5Clog%20P%28t_i%20%7C%20t_%7B%3Ci%7D%2C%20V%3B%20%5Ctheta%29)

其中：

* ![\theta](https://latex.csdn.net/eq?%5Ctheta)
* V 代表输入的图像。
* ![t_{<i}](https://latex.csdn.net/eq?t_%7B%3Ci%7D)
* N 是目标序列的总长度。

**关键在于“目标序列”是如何构建的。** 下面我们分阶段来看这个损失函数是如何具体应用的。

##### 阶段一：预训练 (Pre-training) 的损失函数

* **目标**： 学习视觉基础表征和视觉-语言的初步对齐。
* **数据**： 大规模图像-文本对 `(Image, Text)`。
* **输入/输出序列构建**：

  + **输入**： `[<Image>] + Text_input`
  + **目标**： `Text_output` （即与图像对应的描述文本）
  + 在实际实现中，`Text_input` 通常是一个简单的指令，如“请描述这张图片”，而 `Text_output` 是详细的描述。
* **损失计算**：

  + 模型需要生成的目标序列是 `Text_output` 部分。
  + **损失仅计算在 `Text_output` 对应的token上**。图像token和指令token部分在计算损失时会被忽略（通常通过一个叫做`loss mask`的机制实现）。
* **可训练参数**： 此阶段通常**只训练视觉-语言适配器（Adapter）**，冻结视觉编码器和LLM的参数。这样做是为了高效地利用大量弱标注数据，让适配器学会如何将视觉特征“翻译”给LLM。

##### 阶段二：多任务监督微调 (Multi-task SFT) 的损失函数

* **目标**： 教会模型执行各种下游任务（VQA, REC, Captioning等）。
* **数据**： 多种任务的标注数据混合，格式为 `(Image, Instruction, Response)`。
* **输入/输出序列构建（关键）**：

  + 所有任务都被统一转换为**基于指令-响应的文本序列**。这是损失函数设计的关键。
  + **视觉问答 (VQA)**:

    - **输入**： `[<Image>] + "问题：图片中是什么动物？"`
    - **目标**： `"答案：一只猫"`
  + **指代表达理解 (REC)**:

    - **输入**： `[<Image>] + "请框出：穿红色衣服的人"`
    - **目标**： `"<box>(x1, y1), (x2, y2)</box>"` （边界框坐标被**离散化**并编码为特殊token序列，就像文本一样被处理）
  + **图像标注 (Captioning)**:

    - **输入**： `[<Image>] + "请描述这张图片"`
    - **目标**： `"一个男人正在公园里骑自行车。"`
* **损失计算**：

  + 同样，**损失只计算在响应（Response）部分**，即模型需要生成的目标文本上。指令和图像部分被mask掉。
  + 通过将不同任务的答案统一为文本序列，模型可以使用**同一个简单的损失函数**来学习所有任务。模型自己会根据指令来判断该执行什么任务并生成相应格式的答案。
* **可训练参数**： 此阶段会**解锁并训练所有参数**（视觉编码器、适配器、LLM），进行端到端的全面微调，使模型能力得到极大提升。

##### 阶段三：人类偏好对齐 (RLHF) 的损失函数

这个阶段仅针对对话模型（如Qwen-VL-Chat），其损失函数不再是简单的交叉熵。

* **目标**： 让模型的输出更符合人类的偏好（有帮助、安全、无害）。
* **方法**： 通常分为两步：

  1. **训练奖励模型 (Reward Model, RM)**:

     + 收集人类对模型多个回答的排序数据（如回答A比回答B更好）。
     + 训练一个单独的奖励模型，其损失函数是**排序损失（如Pairwise Ranking Loss）**，目标是让RM对更好回答的打分高于更差的回答。
  2. **强化学习微调 (Reinforcement Learning Fine-tuning)**:

     + **损失函数**： 使用**近端策略优化（PPO）** 等RL算法。
     + **核心思想**： 损失函数由三部分组成：

       - **奖励项 (Reward Term)**: 最大化当前模型策略生成的回答从RM获得的奖励。
       - **KL散度项 (KL Penalty Term)**: 最小化当前策略与原始SFT模型策略之间的KL散度，防止模型为了获得高奖励而“放飞自我”，偏离太多导致 nonsense。
       - **预训练损失项 (Pre-training Loss Term)**: 通常还会加入原始的语言建模损失，以保持模型的通用语言能力不退化。

总结：

| 训练阶段 | 核心损失函数 | 数据格式 | 关键点 |
| --- | --- | --- | --- |
| **预训练** | 交叉熵损失（仅文本输出部分） | `(Image, Text)` | 冻结主干，只训Adapter，学基础对齐 |
| **多任务SFT** | 交叉熵损失（仅响应部分） | `(Image, Instruction, Response)` | **统一序列格式**是核心，端到端训练，所有任务一损函数 |
| **RLHF** | PPO损失（奖励+KL散度） | 人类偏好排序数据 | 优化输出质量，使其符合人类价值观 |

**Qwen-VL损失函数设计的精髓在于：**

1. **统一性**： 巧妙地**将所有多模态任务转化为文本生成任务**，使得一个简单的、经过充分验证的**自回归交叉熵损失函数**就能指导所有阶段的学习。这种设计极大地简化了训练流程。
2. **灵活性**： 通过改变输入-目标序列的构建方式，可以轻松地融入新的任务和数据。
3. **位置信息整合**： 在SFT阶段，将边界框坐标作为文本token处理，使得定位任务（REC）的损失计算也完美地融入了语言建模损失中，无需设计额外的回归损失或分割损失。
4. **分阶段优化**： 不同阶段使用相同的损失函数但不同的数据组织和可训练参数，逐步、高效地构建起强大的多模态理解与生成能力。

这种“以不变应万变”的损失函数设计，是大模型时代处理多模态任务的一种非常有效和流行的范式。

#### 总结与特点

| 组件 | 功能 | 实现 | 特点 |
| --- | --- | --- | --- |
| **视觉编码器** | 提取图像特征 | 基于CLIP的ViT-BigG | 强大的视觉特征提取能力 |
| **视觉-语言适配器** | 连接视觉与文本特征空间 | 带位置注入的MLP/Cross-Attention | **核心创新点之一**，注入空间位置信息 |
| **大语言模型** | 多模态信息融合与推理 | Qwen (Transformer Decoder) | 强大的语言理解和生成能力 |
| **训练策略** | 高效学习多模态能力 | 三阶段（预训练->SFT->RLHF） | 循序渐进，高效且性能强大 |

**Qwen-VL架构的核心思想**是：**将一个强大的视觉编码器提取的特征，通过一个精心设计的、能保留位置信息的适配器，投射到另一个强大的语言模型的特征空间中，最后由语言模型作为统一的理解和生成引擎，处理一切任务。**

这种设计使得Qwen-VL不仅性能强大，而且非常灵活，能够处理高分辨率图像、多图输入、以及需要精细空间理解的复杂任务。



