---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6c7a6d31323237383832382f:61727469636c652f64657461696c732f313436313137373536"
layout: post
title: "通义千问语言模型Qwen2.5架构详解"
date: 2025-03-08 16:09:44 +08:00
description: "此外，将基于API的模型 Qwen-Plus 与领先的专有和开源模型进行了对比，包括 GPT4-o、Claude-3.5-Sonnet、Llama-3.1-405B和DeepSeek-V2.5。Qwen2.5-Math-72B-Instruct的整体性能超越了Qwen2-Math-72B-Instruct和GPT4-o，甚至是非常小的专业模型如 Qwen2.5-Math-1.5B-Instruct也能在与大型语言模型的竞争中取得高度竞争力的表现。近来也出现了明显的转向小型语言模型（SLMs）的趋势。"
keywords: "qwen"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '架构', '机器学习', '人工智能', 'Python']
artid: "146117756"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146117756
    alt: "通义千问语言模型Qwen2.5架构详解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146117756
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146117756
cover: https://bing.ee123.net/img/rand?artid=146117756
image: https://bing.ee123.net/img/rand?artid=146117756
img: https://bing.ee123.net/img/rand?artid=146117756
---

# 通义千问语言模型Qwen2.5架构详解

前文我们了解了Deepseek不同版本之间的区别，了解到如DeepSeek-R1-Distill-Qwen-1.5B作为学生模型，Qwen作为学生模型的基础架构，通过蒸馏学习以Qwen系列大模型作为教师模型（如Qwen-7B、Qwen-32B）的中文理解、数学推理等任务能力。具体可以看我的csdn文章：
[Deepseek不同版本之间的区别（一文看懂）-CSDN博客](https://lzm07.blog.csdn.net/article/details/146094097 "Deepseek不同版本之间的区别（一文看懂）-CSDN博客")

|  |  |  |  |
| --- | --- | --- | --- |
| 角色 | 具体模型 | 参数量 | 功能定位 |
| 教师模型 | Qwen系列大模型（如Qwen-7B、Qwen-32B） | 7B/32B | 提供知识来源（输出分布、特征表示） |
| 学生模型 | DeepSeek-R1-Distill-Qwen-1.5B | 1.5B | 通过蒸馏继承教师能力并轻量化 |

DeepSeek-R1的蒸馏模型是Qwen系列，自然我们要学习一下什么是Qwen架构。

## ****引言****

Qwen2.5是阿里通义千问团队最新开源的顶尖AI大模型，具备多种参数规模的选项，包括0.5B、1.5B、3B、7B、14B、32B和72B。该模型在预训练时采用了最新的大规模数据集，涵盖了多达
18万亿个tokens
，使得Qwen2.5在
自然语言理解、文本生成、编程能力和数学能力等

（

多模态

）
方面都有了显著的提升。Qwen2.5不仅支持长文本处理，能够生成超过8K tokens 的内容，还增强了对系统提示的灵活适应性，提升了角色扮演和机器人的背景设定能力。此外，Qwen2.5支持多达29种语言，包括中文、英文、法文、西班牙文、葡萄牙文和德文等。Qwen2.5-Coder和Qwen2.5-Math是专门针对编程和数学问题优化的模型，在相关领域展现了卓越的性能。

Qwen2.5在MMLU-rudex（通用知识）、MBPP（代码能力）和 MATH（数学能力）基准测试中，分别取得了86.8、88.2和83.1的优异成绩，展示出其在多领域的强大性能。

## ****一、**** ****Qwen2.5**** ****语言模型**** ****简介****

Qwen2.5是阿里通义千问团队与2024年9月推出的最新开源AI大模型，包括了
****语言模型 Qwen2.5****
，以及专门针对
****编程的 Qwen2.5-Coder****
和
****数学的 Qwen2.5-Math****
模型。所有开放权重的模型都是稠密的、decoder-only的语言模型，提供多种不同规模的版本，包括：

1. Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, 以及72B;
2. Qwen2.5-Coder: 1.5B, 7B, 以及即将推出的32B;
3. Qwen2.5-Math: 1.5B, 7B, 以及72B。

![](https://i-blog.csdnimg.cn/direct/9bb6e5f4d9f641358c9d20053eb21ecf.png)

除了3B和72B的版本外，所有的开源模型都采用了Apache 2.0许可证。并且通过Model Studio提供了旗舰语言模型 Qwen-Plus和Qwen-Turbo的API。

如需了解更多关于Qwen2.5、Qwen2.5-Coder和Qwen2.5-Math的详细信息，请随时访问以下链接：

1. ****Qwen2.5 LLM****
   ：https://qwenlm.github.io/blog/qwen2.5-llm
2. ****Qwen2.5-Coder****
   ：https://qwenlm.github.io/blog/qwen2.5-coder
3. ****Qwen2.5-Math****
   ：
   [Qwen2.5-Math: The world's leading open-sourced mathematical LLMs | Qwen](https://qwenlm.github.io/blog/qwen2.5-math "Qwen2.5-Math: The world's leading open-sourced mathematical LLMs | Qwen")

文件下载：

1. ****GitHub仓库****
   ：https://github.com/QwenLM/Qwen2.5
2. ****HuggingFace模型库****
   ：
   [https://huggingface.co/Qwen](https://huggingface.co/Qwen "https://huggingface.co/Qwen")
3. ****modelscope模型库****
   ：https://modelscope.cn/organization/qwen

## ****二、模型**** ****要点****

就 Qwen2.5 语言模型而言，所有模型都在最新的大规模数据集上进行了预训练，该数据集包含多达
****18T tokens****
（18万亿tokens）。相较于Qwen2，Qwen2.5获得了显著更多的
知识（MMLU：85+）
，并在
编程能力（HumanEval 85+）
和
数学能力（MATH 80+）
方面有了大幅提升。此外，新模型在指令执行、
生成长文本（超过8K标记）
、理解结构化数据（例如表格）以及生成结构化输出特别是JSON方面取得了显著改进。Qwen2.5模型总体上对各种system prompt更具适应性，增强了角色扮演实现和聊天机器人的条件设置功能。与Qwen2类似，Qwen2.5语言模型支持高达128K tokens，并能生成最多8K tokens的内容。它们同样保持了对包括中文、英文、法文、西班牙文、葡萄牙文、德文、意大利文、俄文、日文、韩文、越南文、泰文、阿拉伯文等 29 种以上语言的支持。 在下表中提供了有关模型的基本信息。

专业领域的专家语言模型，即用于编程的 Qwen2.5-Coder 和用于数学的 Qwen2.5-Math，相比其前身CodeQwen1.5和Qwen2-Math有了实质性的改进。具体来说，Qwen2.5-Coder在包含5.5 T tokens编程相关数据上进行了训练，使即使较小的编程专用模型也能在编程评估基准测试中表现出媲美大型语言模型的竞争力。 同时，Qwen2.5-Math支持 中文 和 英文，并整合了多种推理方法，包括
****CoT（Chain of Thought）****
、
****PoT（Program of Thought）****
和
****TIR（Tool-Integrated Reasoning）****
。

![](https://i-blog.csdnimg.cn/direct/238564060a744c349753a2896e424ff8.png)

****模型要点总结如下：****

1. ****多样化模型规模****
   ：Qwen2.5提供从0.5B到72B不同参数规模的模型，以满足各种应用需求。
2. ****扩展的预训练数据集****
   ：Qwen2.5的预训练数据集从7T tokens扩展到18T tokens，显著提升了模型的知识储备。
3. ****增强的多语言能力****
   ：Qwen2.5支持超过29种语言，包括中文和英文，确保了广泛的多语言支持。
4. ****提升的编程和数学能力****
   ：Qwen2.5-Coder和Qwen2.5-Math针对编程和数学问题进行了专门优化，提供了更专业的解决方案。
5. ****长文本处理能力****
   ：Qwen2.5支持高达128K tokens的上下文长度，能够生成最长8K tokens的内容，增强了对长文本的处理能力。
6. ****结构化数据处理****
   ：该模型在理解和生成结构化数据（如表格）方面有显著改善，尤其在 JSON 输出方面表现突出。
7. ****系统提示适应性****
   ：Qwen2.5对各种系统提示具有更强的适应性，提升了角色扮演和机器人的条件设置能力。

## ****三、Qwen-2.5架构****

### ****1. 核心改进****

Qwen-2.5是Qwen系列的最新升级版本，
针对长文本处理、多语言支持和计算效率进行了优化
，主要改进如下：

（1）
****动态NTK-aware RoPE****
：扩展上下文窗口至32K tokens，支持超长文本推理。有关动态NTK-aware RoPE的更多内容可以看的我CSDN文章：
[transformer架构嵌入层位置编码之动态NTK-aware位置编码_动态的位置编码嵌入-CSDN博客](https://lzm07.blog.csdn.net/article/details/145865840 "transformer架构嵌入层位置编码之动态NTK-aware位置编码_动态的位置编码嵌入-CSDN博客")

（2）
****混合注意力机制****
：结合MQA（多查询注意力）和GQA（分组查询注意力），显存占用降低30%。

（3）
****激活函数优化****
：采用
SwiGLU v2
，增强非线性表达能力。

（4）
****量化感知训练****
：
原生支持GPTQ-Int4量化
，精度损失小于1%。

### ****2. 架构组成****

|  |  |
| --- | --- |
| 模块 | 说明 |
| ****输入嵌入层**** | 支持多语言Tokenizer，词表大小152K，包含中/英/代码/符号等混合语料 |
| ****位置编码**** | 动态NTK-aware旋转位置编码（RoPE），自适应调整基频 |
| ****Transformer层**** | 32层，每层包含：  - 混合注意力（MQA/GQA 动态切换）  - SwiGLU v2前馈网络  - RMSNorm归一化 |
| ****输出层**** | 动态权重投影，支持多任务输出（文本生成、分类、回归） |

### ****3. 关键技术****

（1）动态注意力切换：根据输入长度自动选择MQA（短文本）或GQA（长文本），平衡速度与显存。

（2）量化感知训练：在训练中模拟4-bit量化误差，提升后续GPTQ量化后的稳定性。

（3）多阶段预训练：第一阶段：通用语料（1T tokens）；第二阶段：数学/代码强化（200B tokens）。

### ****4.架构流程图****

![](https://i-blog.csdnimg.cn/direct/ef0a6ed9021e4740aad6bae3a244abe7.png)

### ****5.Python 示例代码****

（1）CMD中环境安装

安装python的基础框架库。

```
pip install transformers accelerate torch
```

建议使用国内的镜像，如清华镜像，速度会快很多：

```
pip install transformers accelerate torch -i https://pypi.tuna.tsinghua.edu.cn/simple
```

（2）加载模型与推理

python代码如下：

```
from transformers import AutoTokenizer, AutoModelForCausalLM



# 加载 Qwen-2.5-4B 模型

model_name = "Qwen/Qwen2.5-4B"

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(

    model_name,

    device_map="auto",

    torch_dtype="auto"

)



# 长文本生成示例

input_text = """问题：已知函数 f(x) = x²，求其在 x=2 处的导数。

解答步骤："""

inputs = tokenizer(input_text, return_tensors="pt").to(model.device)



# 使用混合注意力自动切换策略

outputs = model.generate(

    **inputs,

    max_new_tokens=200,

    temperature=0.7,

    top_p=0.9

)



print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

（3）输出示例

```
问题：已知函数 f(x) = x²，求其在 x=2 处的导数。

解答步骤：

1. 导数的定义为 f'(x) = lim_{h→0} [f(x+h) - f(x)] / h。

2. 代入 f(x) = x²，得 f'(x) = lim_{h→0} [(x+h)² - x²] / h。

3. 展开分子：(x² + 2xh + h²) - x² = 2xh + h²。

4. 化简表达式：(2xh + h²)/h = 2x + h。

5. 取极限 h→0，得 f'(x) = 2x。

6. 因此，f'(2) = 2*2 = 4。

答案：4
```

这里是简化操作流程，具体内容可以看我接下来的文章。或者参照我前面文章：
[个人windows电脑上安装DeepSeek大模型（完整详细可用教程）_deepseek-r1-distill-qwen-1.5b-gguf-CSDN博客](https://lzm07.blog.csdn.net/article/details/145491693 "个人windows电脑上安装DeepSeek大模型（完整详细可用教程）_deepseek-r1-distill-qwen-1.5b-gguf-CSDN博客")

### ****6.**** ****性能对比****

|  |  |  |  |
| --- | --- | --- | --- |
| 指标 | Qwen-2.5-4B | Qwen-1.5-7B | Llama-3-8B |
| 上下文长度 | 32K | 16K | 8K |
| 数学推理（GSM8K） | 82.5% | 78.1% | 75.3% |
| 显存占用（FP16） | 8GB | 14GB | 16GB |
| 生成速度（tokens/s） | 45 | 32 | 28 |

## ****四、**** ****API****

除了开源模型之外，阿里还通过API服务提供了更多的模型。可以访问
[阿里云百炼平台](https://help.aliyun.com/zh/model-studio/developer-reference/what-is-qwen-llm "阿里云百炼平台")
获取更多详情，包括定价信息。关于API模型的性能如下：

|  |  |  |  |
| --- | --- | --- | --- |
| 模型 | 模型名称 | 描述 | 定价 |
| Qwen-Plus | qwen-plus-latest, qwen-plus-0919 | Qwen2.5的旗舰模型，在广泛的任务中实现了顶级性能，适用于需要高级推理和深刻理解的复杂任务。 | 0.0008 /0.002 |
| Qwen-Turbo | qwen-turbo-latest, qwen-turbo-0919 | Qwen2.5的均衡模型，提供快速且准确度高的响应，非常适合实时应用。 | 0.0003 /0.0006 |
| Qwen-VL-Max | qwen-vl-max-latest, qwen-vl-max-0919 | Qwen VL的旗舰模型，具有优秀的图像理解和视频推理能力，可以更好地识别图片中的多语言文字和手写体的文字。 | 0.02 / 0.02 |

对于每个模型，都提供两个不同的模型名称。第一个名称通常是带有最新模型的相对稳定的服务。然而，为了确保服务的稳定性，通常首先在以 -latest 结尾的模型名称上部署最新模型，并且还有一个以日期（如 -0919）结尾的快照版本。

## ****五、**** ****模型性能****

### ****1.**** ****Qwen2.5****

为了展示Qwen2.5的能力，
用最大的开源模型 Qwen2.5-72B

（

一个拥有720亿参数的稠密decoder-only语言模型

）

与领先的开源模型如Llama-3.1-70B和Mistral-Large-V2进行了基准测试
。在多个基准测试中展示了经过指令调优的版本的综合结果，评估了模型的能力和人类偏好。

![](https://i-blog.csdnimg.cn/direct/872f561268eb4a6b9a924dccf01873e0.png)

除了指令微调的模型之外，旗舰开源模型 Qwen2.5-72B 的基础语言模型性能达到了顶级水准，即便是在与Llama-3-405B这样更大的模型对比时也是如此。

![](https://i-blog.csdnimg.cn/direct/ffa9d37bd27d4410a9252a6090b76f86.png)

此外，将基于API的模型 Qwen-Plus 与领先的专有和开源模型进行了对比，包括 GPT4-o、Claude-3.5-Sonnet、Llama-3.1-405B和DeepSeek-V2.5。这一对比展示了 Qwen-Plus 在当前大型语言模型领域中的竞争地位。结果显示，Qwen-Plus 显著优于DeepSeek-V2.5，并且在与Llama-3.1-405B的竞争中表现出了竞争力，尽管在某些方面仍不及GPT4-o和Claude-3.5-Sonnet。这次基准测试不仅突显了Qwen-Plus的优势，也指出了未来需要改进的地方，进一步强化了我们在大型语言模型领域持续改进和创新的承诺。

![](https://i-blog.csdnimg.cn/direct/84d275a3767f43768a951f6761c16e1b.png)

Qwen2.5的一个重要更新是重新引入了140亿参数和320亿参数模型，即 Qwen2.5-14B 和 Qwen2.5-32B。这些模型在多样化的任务中超越了同等规模或更大规模的基线模型，例如Phi-3.5-MoE-Instruct和Gemma2-27B-IT。它们在模型大小和能力之间达到了最佳平衡，提供了匹配甚至超过一些较大模型的性能。此外，基于API的模型 Qwen2.5-Turbo 相比这两个开源模型提供了极具竞争力的性能，同时提供了成本效益高且快速的服务。

![](https://i-blog.csdnimg.cn/direct/d59bd4affba44fe2a98b86bc24010920.png)

近来也出现了明显的转向小型语言模型（SLMs）的趋势。尽管历史上小型语言模型（SLMs）的表现一直落后于大型语言模型（LLMs），但二者之间的性能差距正在迅速缩小。值得注意的是，
即使是只有大约30亿参数的模型现在也能取得高度竞争力的结果
。附带的图表显示了一个重要的趋势：在MMLU中得分超过65的新型模型正变得越来越小，这凸显了语言模型的知识密度增长速度加快。特别值得一提的是，Qwen2.5-3B 成为这一趋势的一个典型例子，它仅凭约30亿参数就实现了令人印象深刻的性能，展示了其相对于前辈模型的高效性和能力。

![](https://i-blog.csdnimg.cn/direct/c81194997a4c4e72abde997d91d8ce31.png)

除了在基准评估中取得的显著增强外，Qwen2.5还改进了后训练方法。四个主要更新包括
支持最长可达8K标记的长文本生成
，
大幅提升了对结构化数据的理解能力
，
生成结构化输出（尤其是JSON格式）更加可靠
，并且
在多样化的系统提示下的表现得到了加强
，这有助于有效进行角色扮演。

### ****2.**** ****Qwen2.5-Coder****

****Qwen2.5-Coder特别为编程应用而设计****
。接下来展示Qwen2.5-Coder-7B-Instruct的性能结果，并将其与领先的开源模型进行了基准测试，其中包括那些参数量大得多的模型。

![](https://i-blog.csdnimg.cn/direct/074472448ed14992912d24f1d8a01ee7.png)

Qwen2.5-Coder是个人编程助手的优秀选择。尽管它的体积较小，但在多种编程语言和任务中，它的表现超过了众多大型语言模型，展现了其卓越的编程能力。

### ****3.**** ****Qwen2.5-Math****

在数学专用语言模型方面，相比于Qwen2-Math，
Qwen2.5-Math在更大规模的数学相关数据上进行了预训练
，包括由Qwen2-Math生成的合成数据。此外，
增加了对中文的支持
，并通过赋予其进行CoT（Chain of Thought）、PoT（Program of Thought）和TIR（Tool-Integrated Reasoning）的能力来加强其推理能力。 Qwen2.5-Math-72B-Instruct的整体性能超越了Qwen2-Math-72B-Instruct和GPT4-o，甚至是非常小的专业模型如 Qwen2.5-Math-1.5B-Instruct也能在与大型语言模型的竞争中取得高度竞争力的表现。

![](https://i-blog.csdnimg.cn/direct/ab8d2a9ff7394e3dbf82b62cd3a37d56.png)

## ****六、**** ****使用Qwen2.5****

最简单的方法使用过
阿里云百炼平台
提供的通义千问API来使用
[阿里云百炼平台](https://www.aliyun.com/product/bailian "阿里云百炼平台")
，百炼平台已经兼容OpenAI接口规范。

```
from openai import OpenAIimport osclient = OpenAI(    api_key=os.getenv("YOUR_API_KEY"),    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",)completion = client.chat.completions.create(    model="qwen-plus-latest",    messages=[      {'role': 'user', 'content': 'Tell me something about large language models.'}    ])  print(completion.choices[0].message.content)
```

通过 Hugging Face Transformers库来使用，正如在
[model card](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct "model card")
中演示的那样:

```
from transformers import AutoModelForCausalLM, AutoTokenizermodel_name = "Qwen/Qwen2.5-7B-Instruct"model = AutoModelForCausalLM.from_pretrained(    model_name,    torch_dtype="auto",    device_map="auto")tokenizer = AutoTokenizer.from_pretrained(model_name)prompt = "Give me a short introduction to large language model."messages = [    {"role": "user", "content": prompt}]text = tokenizer.apply_chat_template(    messages,    tokenize=False,    add_generation_prompt=True)model_inputs = tokenizer([text], return_tensors="pt").to(model.device)generated_ids = model.generate(    **model_inputs,    max_new_tokens=512)generated_ids = [    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
```

要使用vLLM运行Qwen2.5并部署一个与OpenAI API兼容的服务，可以运行如下命令：

```
python -m vllm.entrypoints.openai.api_server \    --model Qwen/Qwen2.5-7B-Instruct
```

如果你使用的是vllm>=0.5.3，可以使用vllm serve命令。然后你就可以通过curl来与Qwen2.5 进行对话了：

```
curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{  "model": "Qwen/Qwen2.5-7B-Instruct",  "messages": [    {"role": "user", "content": "Tell me something about large language models."}  ],  "temperature": 0.7,  "top_p": 0.8,  "repetition_penalty": 1.05,  "max_tokens": 512}'
```

此外，Qwen2.5支持vLLM内置的工具调用功能。此功能要求 vllm>=0.6。如果你想启用此功能，请使用以下命令启动vLLM的OpenAI API兼容服务：

```
vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes
```

之后你可以像使用
[GPT’s tool calling](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models "GPT’s tool calling")
那样来使用它。

Qwen2.5同样支持
[Ollama’s tool calling](https://ollama.com/blog/tool-support "Ollama’s tool calling")
.你可以通过启动Ollama的OpenAI兼容服务，并以与使用GPT的工具调用相同的方式来使用它。

Qwen2.5的聊天模板中也包含了一个工具调用模板，这意味着你可以使用Hugging Face
[transformers’ tool calling support](#advanced-tool-use--function-calling)
.

vLLM / Ollama / Transformers的工具调用支持使用受
[Nous’ Hermes](https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B "Nous’ Hermes")
的格式启发的工具调用模板。 此前
[Qwen-Agent](https://github.com/QwenLM/Qwen-Agent "Qwen-Agent")
提供了使用Qwen2自己的工具调用模板的工具调用支持（这较难与vllm和Ollama集成），而Qwen2.5既保持了与Qwen2模板和Qwen-Agent的兼容性。

## ****七、应用场景****

（1）机器人和虚拟助手：Qwen2.5 可以作为对话系统的核心，提供自然语言理解和生成，实现用户的交互需求。

（2）内容创作和编辑：能够自动生成文章、故事、诗歌等文本内容，辅助用户进行编辑和创作。

（3）教育和学习辅助：可帮助学生和教师进行语言学习、作业辅导及知识测试。

（4）编程辅助：Qwen2.5-Coder 模型专注于编程任务，提供代码建议和调试支持。

（5）数学问题解决：Qwen2.5-Math 模型支持中英文数学问题的解决，适用于教育和研究领域。

（6）多语言翻译：可用于生成翻译文本，具备编码器-解码器架构的能力。