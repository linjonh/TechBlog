---
layout: post
title: "深度学习-卷积神经网络"
date: 2025-03-04 10:22:57 +0800
description: "AdaptiveAvgPool（7）就是无论刚开始输入特征图有多大，最后只能变为7*7的特征图。最常用的就是最大池化，可以认为最大池化不需要引入计算，而平均池化需要引出计算（计算平均数）Pooling(2)就是每2*2个格子pooling成一个格子，相当于减半。每种池化还分为Pooling和AdaptiveAvgPool。最后，进行拉直，还是进行Linear操作。池化分为最大池化和平均池化。"
keywords: "深度学习---卷积神经网络"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Cnn']
artid: "146006782"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146006782
    alt: "深度学习-卷积神经网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146006782
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146006782
cover: https://bing.ee123.net/img/rand?artid=146006782
image: https://bing.ee123.net/img/rand?artid=146006782
img: https://bing.ee123.net/img/rand?artid=146006782
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     深度学习---卷积神经网络
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h3>
     一、卷积尺寸计算公式
    </h3>
    <p>
     <img alt="" height="573" src="https://i-blog.csdnimg.cn/direct/1466a2413d1646308666f5f9ad79fd4f.png" width="1256"/>
    </p>
    <p>
    </p>
    <h3>
     二、池化
    </h3>
    <p>
     <img alt="" height="713" src="https://i-blog.csdnimg.cn/direct/bef77090500b42499cb89bec5a964990.png" width="1302"/>
    </p>
    <p>
     池化分为最大池化和平均池化
    </p>
    <p>
     最常用的就是最大池化，可以认为最大池化不需要引入计算，而平均池化需要引出计算（计算平均数）
    </p>
    <p>
     每种池化还分为Pooling和AdaptiveAvgPool
    </p>
    <p>
     Pooling(2)就是每2*2个格子pooling成一个格子，相当于减半
    </p>
    <p>
     <img alt="" height="89" src="https://i-blog.csdnimg.cn/direct/75f42eee757a4d1da295167c5053feb8.png" width="401"/>
    </p>
    <p>
     AdaptiveAvgPool（7）就是无论刚开始输入特征图有多大，最后只能变为7*7的特征图
    </p>
    <p>
     <img alt="" height="100" src="https://i-blog.csdnimg.cn/direct/f301d932136b41229cd58e50a242cdeb.png" width="334"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     最后，进行拉直，还是进行Linear操作
    </p>
    <p>
     <img alt="" height="634" src="https://i-blog.csdnimg.cn/direct/3aea335ca65f498a9b0dd8bd12ebced7.png" width="1306"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     <img alt="" height="727" src="https://i-blog.csdnimg.cn/direct/7428bf996668438ab863e83af9220bda.png" width="1298"/>
    </p>
    <h2>
     三、计算Loss值
    </h2>
    <p>
    </p>
    <p>
     <img alt="" height="730" src="https://i-blog.csdnimg.cn/direct/37137ee3a535487a9cee21dec409964e.png" width="1320"/>
    </p>
    <p>
     我们计算Loss值，需要计算出来的概率分布，而经过卷积池化，Linear后得到的y'(上图）不是概率分布，因此我们进行y'=Softmax(y)操作，得到真正的y'的概率分布。
    </p>
    <p>
     <img alt="" height="731" src="https://i-blog.csdnimg.cn/direct/02d90f62a04b41d4bd4aaea39e44d44c.png" width="1355"/>
    </p>
    <p>
     <img alt="" height="627" src="https://i-blog.csdnimg.cn/direct/170a90d2e255481bbf514a05bb383f7f.png" width="1051"/>
    </p>
    <p>
     得到y'我们就可以 计算Loss，这里就引入了
     <span style="color:#000000">
      CrossEntropy Loss： 交叉熵损失，在使用中，我们可以不用关注计算过程，我们只需调用CrossEntropyLoss即可得到Loss
     </span>
    </p>
    <p>
     <span style="color:#000000">
      得到Loss之后，我们就可以
     </span>
     使用PyTorch中的
     <code>
      loss.backward()
     </code>
     方法来自动计算梯度
     <span style="color:#000000">
      ，计算每个卷积核的梯度，更新模型。
     </span>
    </p>
    <p>
     <img alt="" height="670" src="https://i-blog.csdnimg.cn/direct/fffa7eb2e8544df3bce2304e9878026d.png" width="1312"/>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f44323436303539363432312f:61727469636c652f64657461696c732f313436303036373832" class_="artid" style="display:none">
 </p>
</div>


