---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f44323436303539363432312f:61727469636c652f64657461696c732f313436303036373832"
layout: post
title: "深度学习-卷积神经网络"
date: 2025-03-04 10:22:57 +0800
description: "AdaptiveAvgPool（7）就是无论刚开始输入特征图有多大，最后只能变为7*7的特征图。最常用的就是最大池化，可以认为最大池化不需要引入计算，而平均池化需要引出计算（计算平均数）Pooling(2)就是每2*2个格子pooling成一个格子，相当于减半。每种池化还分为Pooling和AdaptiveAvgPool。最后，进行拉直，还是进行Linear操作。池化分为最大池化和平均池化。"
keywords: "深度学习---卷积神经网络"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Cnn']
artid: "146006782"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146006782
    alt: "深度学习-卷积神经网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146006782
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146006782
cover: https://bing.ee123.net/img/rand?artid=146006782
image: https://bing.ee123.net/img/rand?artid=146006782
img: https://bing.ee123.net/img/rand?artid=146006782
---

# 深度学习---卷积神经网络

### 一、卷积尺寸计算公式

![](https://i-blog.csdnimg.cn/direct/1466a2413d1646308666f5f9ad79fd4f.png)

### 二、池化

![](https://i-blog.csdnimg.cn/direct/bef77090500b42499cb89bec5a964990.png)

池化分为最大池化和平均池化

最常用的就是最大池化，可以认为最大池化不需要引入计算，而平均池化需要引出计算（计算平均数）

每种池化还分为Pooling和AdaptiveAvgPool

Pooling(2)就是每2\*2个格子pooling成一个格子，相当于减半

![](https://i-blog.csdnimg.cn/direct/75f42eee757a4d1da295167c5053feb8.png)

AdaptiveAvgPool（7）就是无论刚开始输入特征图有多大，最后只能变为7\*7的特征图

![](https://i-blog.csdnimg.cn/direct/f301d932136b41229cd58e50a242cdeb.png)

最后，进行拉直，还是进行Linear操作

![](https://i-blog.csdnimg.cn/direct/3aea335ca65f498a9b0dd8bd12ebced7.png)

![](https://i-blog.csdnimg.cn/direct/7428bf996668438ab863e83af9220bda.png)

## 三、计算Loss值

![](https://i-blog.csdnimg.cn/direct/37137ee3a535487a9cee21dec409964e.png)

我们计算Loss值，需要计算出来的概率分布，而经过卷积池化，Linear后得到的y'(上图）不是概率分布，因此我们进行y'=Softmax(y)操作，得到真正的y'的概率分布。

![](https://i-blog.csdnimg.cn/direct/02d90f62a04b41d4bd4aaea39e44d44c.png)

![](https://i-blog.csdnimg.cn/direct/170a90d2e255481bbf514a05bb383f7f.png)

得到y'我们就可以 计算Loss，这里就引入了
CrossEntropy Loss： 交叉熵损失，在使用中，我们可以不用关注计算过程，我们只需调用CrossEntropyLoss即可得到Loss

得到Loss之后，我们就可以
使用PyTorch中的
`loss.backward()`
方法来自动计算梯度
，计算每个卷积核的梯度，更新模型。

![](https://i-blog.csdnimg.cn/direct/fffa7eb2e8544df3bce2304e9878026d.png)