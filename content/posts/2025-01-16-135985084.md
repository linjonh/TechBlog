---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f6366795f62616e712f:61727469636c652f64657461696c732f313335393835303834"
layout: post
title: "十大开源LLM大模型"
date: 2025-01-16 02:31:48 +08:00
description: "本文介绍了2024年即将推出的十大开源大型语言模型，包括LLaM"
keywords: "开源llm模型"
categories: ['未分类']
tags: ['Reactjs', 'Javascript']
artid: "135985084"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=135985084
    alt: "十大开源LLM大模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=135985084
featuredImagePreview: https://bing.ee123.net/img/rand?artid=135985084
---

# 十大开源LLM大模型

大型
[**语言模型**](https://www.jdon.com/tag-49140/)
（
**LLM）
**对于当前生成
[**人工智能**](https://www.jdon.com/tag-30661/)
的革命至关重要。语言模型和解释器是基于 Transformer（一种强大的神经
[**架构**](https://www.jdon.com/forum/91/)
）的人工智能 (AI) 系统。它们被称为**
“大型”**
，因为它们包含数亿（如果不是数十亿）来自大量文本
[**数据**](https://www.jdon.com/tag-49580/)
的预训练参数。

在本文中，我们将了解
**2024 年**
将推出的_
**十大开源
[**大模型**](https://www.jdon.com/tag-49140/)**
_。尽管
**ChatGPT**
和（专有）
**LLM**
只出现了一年，但开源社区已经取得了重大进展，现在有许多开源 LLM 可用于各种应用程序。继续阅读以发现最受欢迎的内容！

**1.LLaMA 2**
  
大多数顶级大模型公司都谨慎地开发他们的项目。元脱颖而出。 Meta 提供了有关
**LLaMA 2**
及其强大的开​​源替代方案的重要信息。LLaMA 2 是一个
**7-700 亿参数的**
生成文本模型，于
**2023 年 7 月**
完成。该模型适用于商业和学习。 RLHF 对此进行了改进。构建并训练此文本生成模型以教授聊天机器人自然语言。 Meta 提供开放、可定制的 LLaMA 2、Chat 和 Code Llama。

2. BLOOM
  
**2022 年**
，Flourish 开发了
**BLOOM，
**这是一种自回归**
大型语言模型 (LLM)**
，可通过使用大量文本数据扩展提示来生成文本。超过 70 个国家的专家和志愿者在一年内开发了该项目。开源
**LLM BLOOM**
模型包含 1760 亿个参数。它可以流畅、连贯地使用 46 种语言和 13 种编程语言编写。
**BLOOM 的**
执行、评估和改进以及训练数据和源代码都是公开的。 Hugging Face 用户
**免费使用BLOOM 。**

**3.BERT**
  
LLM 技术依赖于BERT（
**来自 Transformers 的双向编码器表示**
）神经架构。谷歌研究人员发布了“注意力就是你所需要的”。 2017 年。BERT 是早期的变压器测试。 2018 年
**Google 语言模型 BERT**
作为开源软件提供。它迅速掌握了自然语言处理任务。
  
Bert 先进的早期 LLM 开发能力和开源特性使其成为流行的
**语言模型 (LLM)**
。 2020 年，随着 Bert 的推出，Google 搜索将支持
**70 多种语言**
。许多预训练的Bert 模型都是开源的。这些模型有助于检测有害评论、临床记录和情绪。

**4.Falcon 180B**
  
新的
**Falcon 180B**
表明专有和开源大语言模型之间的差异正在快速缩小，如果 Falcon 40B 在 Hugging Face 的大语言模型记分牌上排名第一，还没有给开源 LLM 社区留下深刻的印象的话。 Falcon 180B由
**阿联酋**
技术创新研究所于9月20日至23日推出，正在使用
**3.5万亿**
代币和
**1800亿个参数**
进行训练。 Hugging Face表示，鉴于其惊人的处理能力，Falcon 180B可以与
**Google的PaLM 2**
（运行Google Bard的LLM）竞争。 Falcon 180B在一些 NLP 任务上已经超越了
**LLaMA 2**
和GPT-3.5 。

重要的是要记住，Falcon 180B 需要强大的处理能力才能运行，同时可以免费用于商业和研究环境。

**5.OPT-175B**
  
2022 年，Meta 发布了开放式预训练
**Transformers 语言模型 (TLM**
)，实现了一个重要的里程碑，这是他们利用开源来解放 LLM 竞赛的目标的一部分。 OPT 由一组预训练的 Transformer 组成，仅解码器，参数范围从 1
**25M 到 175B**
。最强大的兄弟是
**OPT-175B**
，这是一种开源 LLM，是市场上最复杂的，其
[**性能**](https://www.jdon.com/tag-300/)
与 GPT-3 类似。公众可以访问源代码和预训练模型。但是，如果您计划与大模型建立人工智能驱动的业务，您最好考虑另一种选择，因为 OPT-175B 仅在允许该模型用于研究用例的非商业许可下可用。

**6.XGen-7B**
  
越来越多的企业参加 LLM 竞赛。 Salesforce 是最新进入该市场的公司之一，于 2023 年 7 月发布了 XGen-7B LLM。作者声称，大多数开源
**LLM**
专注于提供冗长的答复，但细节很少（即，简短的提示，
[**上下文**](https://www.jdon.com/69062.html)
很少） ）。
**XGen-7B**
试图创建一个可以处理更大上下文窗口的工具。具体来说，XGen 最复杂的变体 (XGen-7B-8K-base) 支持 8K 上下文窗口，即输入和输出中的全部文本量。

**虽然 XGen 仅利用 7B 个参数进行训练（比LLaMA 2**
或 Falcon 等最强大的开源 LLM 少得多），但效率是另一个首要目标。尽管 XGen 尺寸很小，但它仍然可以产生出色的结果。除了
**XGen-7 B-{4K,8K}**
-inst 版本（使用教学数据和 RLHF 进行训练并在非商业许可下提供）之外，该模型可用于商业和研究用途。

**7.GPT-NeoX 和 GPT-NeoX**
  
**GPT-NeoX 和 GPT-J**
由非营利性人工智能研究中心 EleutherAI 的科学家开发，是 GPT 的两个优秀的开源替代品。
**GPT-NeoX**
中有 200 亿个参数，GPT-J 中有 60 亿个参数。尽管大多数高级大模型可以使用超过 1000 亿个参数进行训练，但这两个大模型能够得出高精度的研究结果。它们可以用于许多不同的领域和应用场景，因为它们是在来自各种来源的 22 个高质量数据集上进行训练的。与 GPT-3 相比，
**GPT-NeoX 和 GPT-J**
尚未使用 RLHF 进行训练。

**GPT-NeoX 和 GPT-J**
可用于任何自然语言处理活动，包括研究、营销活动规划、情感分析和文本生成。通过 NLP Cloud
[**API**](https://www.jdon.com/tag-33426/)
，您可以免费获得两个大模型。

**8. Vicuna 13-B**
  
**使用从ShareGPT**
收集的用户共享对话，LLaMa 13B 模型经过改进，创建了开源对话模型 Vicuna-13B。 Vicuna-13B 是一款具有多种用途的智能聊天机器人；下面列出了各个行业的一些例子，包括客户服务、医疗保健、教育、金融和旅行/酒店。根据使用 GPT-4 作为判断的初步评估，Vicuna-13B在 90% 以上的情况下超越了 LLaMa 和 Alpaca 等其他模型，达到了ChatGPT和
**Google Bar**
rd 90% 以上的质量。

**9. YI 34B**
  
**YI 34B**
中国01 AI开发了一种新的语言模型，名为Yi 34B。目前，该模型在 Hugging Face Open LLM 排行榜上名列前茅。该公司的目标是开发能够说中文和英文的双语模型。与原始的 4K 令牌上下文窗口相比，该模型现在可以使用多达 32K 令牌进行训练。

令人印象深刻的是，该公司最近发布了 34B 模型的 200,000 代币版本。这些模型可以被许可用于商业用途，并且可用于研究目的。 34B 模型拥有 3 万亿代币，在算术和编码方面表现出色。该公司已提供监督微调对话模型和基本模型的基准。该模型有多个 4 位和 8 位版本可用。

**10.Mistral ​​​​​​​8x7B**
  
**Mistral AI**
于 2023 年 12 月推出了 Mixtral 8X 7B 基础模型。采用开放权重，它是一种优秀的专家模型稀疏混合 (SMoE)，在许多参数较小的基准测试中，其性能优于 Llama 2 和 GPT 3.5。

它是在 Apache 2.0 下获得许可的；因此，您如何改变它并从中获利并没有太多限制。Mistral是一个仅解码器模型，是一个稀疏的专家混合网络。前馈块从八个参数组中选择。路由器网络在每一层为每个令牌选择其中两个组（“专家”），以便处理它并贡献其输出。此方法减少了延迟和成本，同时增加了模型参数的数量。 Mistral 每个代币仅使用 467 亿个参数中的 129 亿个参数。这表明它使用与 12.9B 模型相同数量的资源并以相同的速率处理输入和输出。

Mixtral 8x7B 目前被Hugging Face Open LLM 排行榜评为市场上排名前 10 的大模型，平均得分为 68.42、ARC 66.04、
**HellaSwag 86.49、**
**MMLU**
71.82和 TruthfulQA 46.78。在大多数基准测试中，它的性能优于 GPT 3.5 和 Llama 2；但是，不如 GPT 4。在大多数基准测试中，它的推理速度比Llama 2 70B 快 6 倍。具体来说，它在大多数常见基准测试中均达到或超过 GPT3.5。

Mixtral 在 BBQ 基准上的偏差比 Llama 2 少。Mixtral 比 Llama 2 具有更大胆的积极情绪，每个维度的变异性相似。Mixtral 在几乎所有方面都优于
**GPT 3.5**
，除了 Mt Bench 分数。 Mixtral 在科学和代码生产方面表现良好。它可能会被改进成为一个MT-Bench分数为8.3的指令跟踪模型。 Mixtral 还支持多种语言，可以使用**英语、法语、意大利语、德语和西班牙语，**这使其成为适合各种应用程序和用户的灵活模型。 Mistral AI 始终致力于扩展模型的语言能力。

**如何选择合适的开源LLM？**
  
选择正确的开源**LLM（大型语言模型）**涉及根据您的特定
[**需求**](https://www.jdon.com/colorUML.html)
和偏好考虑各种因素。这取决于您使用大模型的情况和目的。不同的模型可能在不同的应用中表现出色。此外，模型架构和模型大小可能会提供更好的性能。您可以选择适合您的项目要求的大模型。