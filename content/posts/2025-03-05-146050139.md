---
arturl_encode: "68747470733a2f2f626c:6f672e6373646e2e6e65742f7869616f79753037303332312f:61727469636c652f64657461696c732f313436303530313339"
layout: post
title: "python之爬虫入门实例"
date: 2025-03-05 18:37:55 +0800
description: "【代码】python之爬虫入门实例。"
keywords: "python之爬虫入门实例"
categories: ['Python']
tags: ['爬虫', '开发语言', 'Python']
artid: "146050139"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146050139
    alt: "python之爬虫入门实例"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146050139
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146050139
cover: https://bing.ee123.net/img/rand?artid=146050139
image: https://bing.ee123.net/img/rand?artid=146050139
img: https://bing.ee123.net/img/rand?artid=146050139
---

# python之爬虫入门实例

## 链家二手房数据抓取与Excel存储

### 目录

1. 开发环境准备
2. 爬虫流程分析
3. 核心代码实现
4. 关键命令详解
5. 进阶优化方案
6. 注意事项与扩展

---

### 一、开发环境准备

#### 1.1 必要组件安装

```bash
# 安装核心库
pip install requests beautifulsoup4 openpyxl pandas

# 各库作用说明：
- requests：网络请求库（版本≥2.25.1）
- beautifulsoup4：HTML解析库（版本≥4.11.2）
- openpyxl：Excel文件操作库（版本≥3.1.2）
- pandas：数据分析库（版本≥2.0.3）

```

#### 1.2 开发环境验证

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

print("所有库加载成功！")

```

---

### 二、爬虫流程分析

#### 2.1 技术路线图

发送HTTP请求








获取HTML源码








解析房源列表








提取字段数据








数据清洗








存储Excel

#### 2.2 目标页面结构

```plain
https://cq.lianjia.com/ershoufang/
├── div.leftContent
│   └── ul.sellListContent
│       └── li[data-houseid]  # 单个房源
│           ├── div.title > a  # 标题
│           ├── div.flood > div  # 地址
│           ├── div.priceInfo > div.totalPrice  # 总价
│           └── div.followInfo  # 关注量

```

---

### 三、核心代码实现

#### 3.1 完整代码（带详细注释）

```python
"""
链家二手房数据采集器
版本：1.2
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
from time import sleep

# 配置请求头（模拟浏览器访问）
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept-Language': 'zh-CN,zh;q=0.9'
}

def get_house_data(max_page=5):
    """
    获取链家二手房数据
    参数：
        max_page: 最大爬取页数（默认5页）
    返回：
        pandas.DataFrame格式的清洗后数据
    """
    all_data = []
    
    for page in range(1, max_page+1):
        # 构造分页URL
        url = f"https://cq.lianjia.com/ershoufang/pg{page}/"
        
        try:
            # 发送HTTP请求（加入延迟防止封IP）
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # 检测HTTP状态码
            sleep(1.5)  # 请求间隔
            
            # 解析HTML文档
            soup = BeautifulSoup(response.text, 'lxml')
            
            # 定位房源列表
            house_list = soup.select('ul.sellListContent > li[data-houseid]')
            
            for house in house_list:
                # 数据提取（带异常处理）
                try:
                    title = house.select_one('div.title a').text.strip()
                    address = house.select_one('div.flood > div').text.strip()
                    total_price = house.select_one('div.totalPrice').text.strip()
                    unit_price = house.select_one('div.unitPrice').text.strip()
                    follow = house.select_one('div.followInfo').text.split('/')[0].strip()
                    
                    # 数据清洗
                    cleaned_data = {
                        '标题': title,
                        '地址': address.replace(' ', ''),
                        '总价(万)': float(total_price.replace('万', '')),
                        '单价(元/㎡)': int(unit_price.replace('元/㎡', '').replace(',', '')),
                        '关注量': int(follow.replace('人关注', ''))
                    }
                    all_data.append(cleaned_data)
                
                except Exception as e:
                    print(f"数据解析异常：{str(e)}")
                    continue
        
        except requests.exceptions.RequestException as e:
            print(f"网络请求失败：{str(e)}")
            continue
    
    return pd.DataFrame(all_data)

def save_to_excel(df, filename='house_data.xlsx'):
    """
    将数据保存为Excel文件
    参数：
        df: pandas.DataFrame数据框
        filename: 输出文件名
    """
    # 配置Excel写入参数
    writer = pd.ExcelWriter(
        filename,
        engine='openpyxl',
        datetime_format='YYYY-MM-DD',
        options={'strings_to_numbers': True}
    )
    
    df.to_excel(
        writer,
        index=False,
        sheet_name='链家数据',
        float_format="%.2f",
        freeze_panes=(1,0)
    )
    
    # 保存并优化列宽
    writer.book.save(filename)
    print(f"数据已保存至 {filename}")

if __name__ == '__main__':
    # 执行数据采集
    house_df = get_house_data(max_page=3)
    
    # 数据保存
    if not house_df.empty:
        save_to_excel(house_df)
        print(f"成功采集 {len(house_df)} 条数据")
    else:
        print("未获取到有效数据")

```

---

### 四、关键命令详解

#### 4.1 核心方法说明

##### 4.1.1 pandas.to\_excel参数解析

```python
df.to_excel(
    excel_writer,       # Excel写入器对象
    sheet_name='Sheet1',# 工作表名称
    na_rep='',          # 缺失值填充
    float_format=None,  # 浮点数格式化
    columns=None,       # 指定输出列
    header=True,        # 是否包含列名
    index=True,         # 是否保留索引
    index_label=None,   # 索引列标题
    startrow=0,         # 起始行
    startcol=0,         # 起始列
    engine=None,        # 写入引擎
    merge_cells=True,   # 合并单元格
    encoding=None,      # 文件编码
    inf_rep='inf'       # 无穷大表示
)

```

#### 4.2 防反爬策略

```python
# 1. 请求头伪装
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',
    'Accept-Encoding': 'gzip, deflate, br',
    'Referer': 'https://cq.lianjia.com/'
}

# 2. IP代理池示例
proxies = {
    'http': 'http://10.10.1.10:3128',
    'https': 'http://10.10.1.10:1080',
}

# 3. 请求速率控制
import random
sleep(random.uniform(1, 3))

```

---

### 五、进阶优化方案

#### 5.1 数据存储优化

```python
# 多Sheet存储
with pd.ExcelWriter('output.xlsx') as writer:
    df1.to_excel(writer, sheet_name='重庆')
    df2.to_excel(writer, sheet_name='北京')

# 追加模式写入
def append_to_excel(df, filename):
    from openpyxl import load_workbook
    book = load_workbook(filename)
    writer = pd.ExcelWriter(filename, engine='openpyxl')
    writer.book = book
    df.to_excel(writer, startrow=writer.sheets['Sheet1'].max_row, index=False)
    writer.save()

```

#### 5.2 异常监控体系

```python
# 错误日志记录
import logging
logging.basicConfig(
    filename='spider.log',
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

try:
    # 爬虫代码
except Exception as e:
    logging.error(f"严重错误：{str(e)}", exc_info=True)

```

---

### 六、注意事项

1. **法律合规**
     
   严格遵守《网络安全法》和网站Robots协议，控制采集频率
2. **数据清洗**
     
   建议增加字段校验：

```python
def validate_price(price):
    return 10 < price < 2000  # 重庆房价合理范围校验

```

3. **性能调优**
   * 启用多线程采集（需控制并发数）
   * 使用lxml解析器替代html.parser
   * 禁用BeautifulSoup的格式化功能
4. **存储扩展**

| 存储方式 | 优点 | 缺点 |
| --- | --- | --- |
| Excel | 查看方便 | 大数据性能差 |
| CSV | 通用格式 | 无多Sheet支持 |
| SQLite | 轻量级数据库 | 需要SQL知识 |
| MySQL | 适合大规模存储 | 需要部署数据库 |

---

```markdown
# 快速使用指南

1. 安装依赖库：
```bash
pip install -r requirements.txt

```

2. 运行爬虫：

```bash
python lianjia_spider.py

```

3. 输出文件：

* `house_data.xlsx`
  ：清洗后的完整数据
* `spider.log`
  ：错误日志记录

```plain

通过本方案可实现日均10万级数据的稳定采集，建议根据实际需求调整采集频率和存储方案。请务必遵守相关法律法规，合理使用爬虫技术。

```