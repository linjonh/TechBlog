---
arturl_encode: "68747470733a2f2f626c6f:672e6373646e2e6e65742f787a73313231303635323633362f:61727469636c652f64657461696c732f313436323833393133"
layout: post
title: "åŸºäºåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†ç‚¹äº”ç¬¬-18-ç« -ç¦»çº¿å¼ºåŒ–å­¦ä¹ gymç‰ˆæœ¬-0.26"
date: 2025-03-15 19:24:34 +08:00
description: "ç¬¬ 18 ç«  ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆgymç‰ˆæœ¬ ï¼= 0.26ï¼‰ï¼ˆä¸€ï¼‰"
keywords: "åŸºäºâ€œåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ â€çš„çŸ¥è¯†ç‚¹ï¼ˆäº”ï¼‰ï¼šç¬¬ 18 ç«  ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆgymç‰ˆæœ¬ ï¼= 0.26ï¼‰"
categories: ['åŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ ']
tags: ['å¼ºåŒ–å­¦ä¹ ']
artid: "146283913"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146283913
    alt: "åŸºäºåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ çš„çŸ¥è¯†ç‚¹äº”ç¬¬-18-ç« -ç¦»çº¿å¼ºåŒ–å­¦ä¹ gymç‰ˆæœ¬-0.26"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146283913
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146283913
cover: https://bing.ee123.net/img/rand?artid=146283913
image: https://bing.ee123.net/img/rand?artid=146283913
img: https://bing.ee123.net/img/rand?artid=146283913
---

# åŸºäºâ€œåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ â€çš„çŸ¥è¯†ç‚¹ï¼ˆäº”ï¼‰ï¼šç¬¬ 18 ç«  ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆgymç‰ˆæœ¬ ï¼= 0.26ï¼‰

## æ‘˜è¦

æœ¬ç³»åˆ—çŸ¥è¯†ç‚¹è®²è§£åŸºäº
[åŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ ](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)
ä¸­çš„å†…å®¹è¿›è¡Œè¯¦ç»†çš„ç–‘éš¾ç‚¹åˆ†æï¼å…·ä½“å†…å®¹è¯·é˜…è¯»
[åŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ ](https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)
ï¼

---

å¯¹åº”
[åŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ â€”â€”ç¦»çº¿å¼ºåŒ–å­¦ä¹ ](https://hrl.boyuai.com/chapter/3/%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0)

---

## SAC ç®—æ³•éƒ¨åˆ†

> ä»£ç è®²è§£å¯¹åº”
> [åŸºäºâ€œåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ â€çš„çŸ¥è¯†ç‚¹ï¼ˆä¸€ï¼‰ï¼šç¬¬ 14 ç«  SAC ç®—æ³•ï¼ˆgymç‰ˆæœ¬ ï¼= 0.26ï¼‰](https://blog.csdn.net/xzs1210652636/article/details/146259812)

> ä¸ºäº†ç”Ÿæˆæ•°æ®é›†ï¼Œåœ¨å€’ç«‹æ‘†ç¯å¢ƒä¸­ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ªåœ¨çº¿ SAC æ™ºèƒ½ä½“ï¼Œç›´åˆ°ç®—æ³•è¾¾åˆ°æ”¶æ•›æ•ˆæœï¼ŒæŠŠè®­ç»ƒè¿‡ç¨‹ä¸­æ™ºèƒ½ä½“é‡‡é›†çš„æ‰€æœ‰è½¨è¿¹ä¿å­˜ä¸‹æ¥ä½œä¸ºæ•°æ®é›†ã€‚è¿™æ ·ï¼Œæ•°æ®é›†ä¸­æ—¢åŒ…å«è®­ç»ƒåˆæœŸè¾ƒå·®ç­–ç•¥çš„é‡‡æ ·ï¼ŒåˆåŒ…å«è®­ç»ƒåæœŸè¾ƒå¥½ç­–ç•¥çš„é‡‡æ ·ï¼Œæ˜¯ä¸€ä¸ªæ··åˆæ•°æ®é›†ã€‚ä¸‹é¢ç»™å‡ºç”Ÿæˆæ•°æ®é›†çš„ä»£ç ï¼ŒSAC éƒ¨åˆ†ç›´æ¥ä½¿ç”¨ 14.5 èŠ‚ä¸­çš„ä»£ç ï¼Œå› æ­¤ä¸å†è¯¦ç»†è§£é‡Šã€‚â€”â€”
> [18.4 CQL ä»£ç å®è·µ](https://hrl.boyuai.com/chapter/3/%E7%A6%BB%E7%BA%BF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/#184-cql-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5)

```python

import numpy as np
import gym
from tqdm import tqdm
import random
import rl_utils
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
import matplotlib.pyplot as plt


# ä¼ ç»Ÿçš„SACç®—æ³•
class PolicyNetContinuous(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):
        super(PolicyNetContinuous, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)
        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)
        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)
        self.action_bound = action_bound

    def forward(self, x):
        x = F.relu(self.fc1(x))
        mu = self.fc_mu(x)
        std = F.softplus(self.fc_std(x))
        dist = Normal(mu, std)
        normal_sample = dist.rsample()  # rsample()æ˜¯é‡å‚æ•°åŒ–é‡‡æ ·
        log_prob = dist.log_prob(normal_sample)
        action = torch.tanh(normal_sample)
        # è®¡ç®—tanh_normalåˆ†å¸ƒçš„å¯¹æ•°æ¦‚ç‡å¯†åº¦
        log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)
        action = action * self.action_bound
        return action, log_prob


class QValueNetContinuous(torch.nn.Module):
    def __init__(self, state_dim, hidden_dim, action_dim):
        super(QValueNetContinuous, self).__init__()
        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)
        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)
        self.fc_out = torch.nn.Linear(hidden_dim, 1)

    def forward(self, x, a):
        cat = torch.cat([x, a], dim=1)
        x = F.relu(self.fc1(cat))
        x = F.relu(self.fc2(x))
        return self.fc_out(x)


class SACContinuous:
    ''' å¤„ç†è¿ç»­åŠ¨ä½œçš„SACç®—æ³• '''
    def __init__(self, state_dim, hidden_dim, action_dim, action_bound,
                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,
                 device):
        self.actor = PolicyNetContinuous(state_dim, hidden_dim, action_dim,
                                         action_bound).to(device)  # ç­–ç•¥ç½‘ç»œ
        self.critic_1 = QValueNetContinuous(state_dim, hidden_dim,
                                            action_dim).to(device)  # ç¬¬ä¸€ä¸ªQç½‘ç»œ
        self.critic_2 = QValueNetContinuous(state_dim, hidden_dim,
                                            action_dim).to(device)  # ç¬¬äºŒä¸ªQç½‘ç»œ
        self.target_critic_1 = QValueNetContinuous(state_dim,
                                                   hidden_dim, action_dim).to(
                                                       device)  # ç¬¬ä¸€ä¸ªç›®æ ‡Qç½‘ç»œ
        self.target_critic_2 = QValueNetContinuous(state_dim,
                                                   hidden_dim, action_dim).to(
                                                       device)  # ç¬¬äºŒä¸ªç›®æ ‡Qç½‘ç»œ
        # ä»¤ç›®æ ‡Qç½‘ç»œçš„åˆå§‹å‚æ•°å’ŒQç½‘ç»œä¸€æ ·
        self.target_critic_1.load_state_dict(self.critic_1.state_dict())
        self.target_critic_2.load_state_dict(self.critic_2.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),
                                                lr=actor_lr)
        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),
                                                   lr=critic_lr)
        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),
                                                   lr=critic_lr)
        # ä½¿ç”¨alphaçš„logå€¼,å¯ä»¥ä½¿è®­ç»ƒç»“æœæ¯”è¾ƒç¨³å®š
        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)
        self.log_alpha.requires_grad = True  #å¯¹alphaæ±‚æ¢¯åº¦
        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],
                                                    lr=alpha_lr)
        self.target_entropy = target_entropy  # ç›®æ ‡ç†µçš„å¤§å°
        self.gamma = gamma
        self.tau = tau
        self.device = device

    def take_action(self, state):
        if isinstance(state, tuple):
            state = state[0]
        state = torch.tensor([state], dtype=torch.float).to(self.device)
        action = self.actor(state)[0]
        return [action.item()]

    def calc_target(self, rewards, next_states, dones):  # è®¡ç®—ç›®æ ‡Qå€¼
        next_actions, log_prob = self.actor(next_states)
        entropy = -log_prob
        q1_value = self.target_critic_1(next_states, next_actions)
        q2_value = self.target_critic_2(next_states, next_actions)
        next_value = torch.min(q1_value,
                               q2_value) + self.log_alpha.exp() * entropy
        td_target = rewards + self.gamma * next_value * (1 - dones)
        return td_target

    def soft_update(self, net, target_net):
        for param_target, param in zip(target_net.parameters(),
                                       net.parameters()):
            param_target.data.copy_(param_target.data * (1.0 - self.tau) +
                                    param.data * self.tau)

    def update(self, transition_dict):
        states = torch.tensor(transition_dict['states'],
                              dtype=torch.float).to(self.device)
        actions = torch.tensor(transition_dict['actions'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        rewards = torch.tensor(transition_dict['rewards'],
                               dtype=torch.float).view(-1, 1).to(self.device)
        next_states = torch.tensor(transition_dict['next_states'],
                                   dtype=torch.float).to(self.device)
        dones = torch.tensor(transition_dict['dones'],
                             dtype=torch.float).view(-1, 1).to(self.device)
        rewards = (rewards + 8.0) / 8.0  # å¯¹å€’ç«‹æ‘†ç¯å¢ƒçš„å¥–åŠ±è¿›è¡Œé‡å¡‘

        # æ›´æ–°ä¸¤ä¸ªQç½‘ç»œ
        td_target = self.calc_target(rewards, next_states, dones)
        critic_1_loss = torch.mean(
            F.mse_loss(self.critic_1(states, actions), td_target.detach()))
        critic_2_loss = torch.mean(
            F.mse_loss(self.critic_2(states, actions), td_target.detach()))
        self.critic_1_optimizer.zero_grad()
        critic_1_loss.backward()
        self.critic_1_optimizer.step()
        self.critic_2_optimizer.zero_grad()
        critic_2_loss.backward()
        self.critic_2_optimizer.step()

        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        new_actions, log_prob = self.actor(states)
        entropy = -log_prob
        q1_value = self.critic_1(states, new_actions)
        q2_value = self.critic_2(states, new_actions)
        actor_loss = torch.mean(-self.log_alpha.exp() * entropy -
                                torch.min(q1_value, q2_value))
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # æ›´æ–°alphaå€¼
        alpha_loss = torch.mean(
            (entropy - self.target_entropy).detach() * self.log_alpha.exp())
        self.log_alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.log_alpha_optimizer.step()

        self.soft_update(self.critic_1, self.target_critic_1)
        self.soft_update(self.critic_2, self.target_critic_2)


env_name = 'Pendulum-v1'
env = gym.make(env_name)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
action_bound = env.action_space.high[0]  # åŠ¨ä½œæœ€å¤§å€¼
random.seed(0)
np.random.seed(0)
if not hasattr(env, 'seed'):
    def seed_fn(self, seed=None):
        env.reset(seed=seed)
        return [seed]
    env.seed = seed_fn.__get__(env, type(env))
# env.seed(0)
torch.manual_seed(0)

actor_lr = 3e-4
critic_lr = 3e-3
alpha_lr = 3e-4
num_episodes = 100
hidden_dim = 128
gamma = 0.99
tau = 0.005  # è½¯æ›´æ–°å‚æ•°
buffer_size = 100000
minimal_size = 1000
batch_size = 64
target_entropy = -env.action_space.shape[0]
device = torch.device("cuda") if torch.cuda.is_available() else torch.device(
    "cpu")

replay_buffer = rl_utils.ReplayBuffer(buffer_size)
agent = SACContinuous(state_dim, hidden_dim, action_dim, action_bound,
                      actor_lr, critic_lr, alpha_lr, target_entropy, tau,
                      gamma, device)

return_list = rl_utils.train_off_policy_agent(env, agent, num_episodes,
                                              replay_buffer, minimal_size,
                                              batch_size)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('SAC on {}'.format(env_name))
plt.show()

```

  

## CQL ç®—æ³•

### CQL æ€»ç»“ä¸å¤§å‡½æ•°æ„ä¹‰

**CQLï¼ˆConservative Q-Learningï¼‰**
ç±»çš„æ•´ä½“è®¾è®¡åœ¨ SAC ç®—æ³•åŸºç¡€ä¸Šå¢åŠ äº†ä¿å®ˆæ€§æ­£åˆ™é¡¹ï¼Œç”¨äºå‡å°‘ Q å‡½æ•°è¿‡ä¼°è®¡ï¼Œæ”¹å–„ç¦»çº¿ RL è¡¨ç°ã€‚

* **æ„é€ å‡½æ•°**

  + **æ„ä¹‰**
    ï¼šåˆå§‹åŒ– SAC ä¸­çš„ actorã€criticï¼ˆä¸¤ä¸ªåŠå¯¹åº”ç›®æ ‡ç½‘ç»œï¼‰ã€ä»¥åŠæ¸©åº¦å‚æ•°å’Œä¼˜åŒ–å™¨ï¼Œå¹¶ä¼ å…¥ CQL ç‰¹æœ‰çš„è¶…å‚æ•° betaï¼ˆæ­£åˆ™ç³»æ•°ï¼‰å’Œ num\_randomï¼ˆåŠ¨ä½œé‡‡æ ·æ•°ï¼‰ã€‚
  + **è¾“å…¥**
    ï¼šçŠ¶æ€ã€åŠ¨ä½œç»´åº¦ã€å„ä¸ªå­¦ä¹ ç‡ã€ç›®æ ‡ç†µã€tauã€gammaã€è®¾å¤‡ã€betaã€num\_randomã€‚
  + **è¾“å‡º**
    ï¼šæ„é€ å¥½çš„ CQL å®ä¾‹ï¼Œå‡†å¤‡è¿›è¡Œæ›´æ–°ã€‚
* **take\_action**

  + **æ„ä¹‰**
    ï¼šç»™å®šçŠ¶æ€ï¼Œåˆ©ç”¨ actor ç½‘ç»œé‡‡æ ·åŠ¨ä½œï¼Œç”¨äºä¸ç¯å¢ƒäº¤äº’ã€‚
  + **è¾“å…¥**
    ï¼šå•ä¸ªçŠ¶æ€ï¼ˆä¾‹å¦‚ [0.1, 0.2, -0.1]ï¼‰ã€‚
  + **è¾“å‡º**
    ï¼šå¯¹åº”åŠ¨ä½œï¼ˆä¾‹å¦‚ [0.8]ï¼‰ã€‚
* **soft\_update**

  + **æ„ä¹‰**
    ï¼šå¹³æ»‘æ›´æ–°ç›®æ ‡ Q ç½‘ç»œå‚æ•°ï¼Œä¿è¯è®­ç»ƒç¨³å®šæ€§ã€‚
  + **è¾“å…¥**
    ï¼šå½“å‰ Q ç½‘ç»œå’Œå¯¹åº”ç›®æ ‡ç½‘ç»œã€‚
  + **è¾“å‡º**
    ï¼šç›®æ ‡ç½‘ç»œå‚æ•°æ›´æ–°ä¸ºæ—§å€¼ä¸å½“å‰å€¼çš„çº¿æ€§ç»„åˆã€‚
* **update**

  + **æ„ä¹‰**
    ï¼šä½¿ç”¨ä¸€æ‰¹çœŸå®ç¯å¢ƒè½¬ç§»æ•°æ®æ›´æ–° actorã€critic ç½‘ç»œå’Œæ¸©åº¦å‚æ•° (\alpha)ï¼ŒåŒæ—¶åœ¨ SAC åŸºç¡€ä¸ŠåŠ å…¥ CQL æ­£åˆ™é¡¹
      




    L
    CQL
    =
    L
    critic
    +
    Î²
    (
    log
    â¡
    âˆ‘
    exp
    â¡
    (
    Q
    (
    s
    ,
    a
    â€²
    )
    âˆ’
    log
    â¡
    Ï€
    ref
    (
    a
    â€²
    )
    )
    âˆ’
    E
    (
    s
    ,
    a
    )
    âˆ¼
    D
    [
    Q
    (
    s
    ,
    a
    )
    ]
    )
    L\_{\text{CQL}} = L\_{\text{critic}} + \beta \Big(\log\sum \exp(Q(s,a') - \log \pi\_{\text{ref}}(a')) - \mathbb{E}\_{(s,a) \sim D}[Q(s,a)]\Big)






    L











    CQL

    â€‹




    =






    L











    critic

    â€‹




    +





    Î²


    (



    lo
    g



    âˆ‘



    exp

    (

    Q

    (

    s

    ,




    a










    â€²

    )



    âˆ’





    lo
    g




    Ï€











    ref

    â€‹


    (


    a










    â€²

    ))



    âˆ’






    E










    (

    s

    ,

    a

    )

    âˆ¼

    D

    â€‹


    [

    Q

    (

    s

    ,



    a

    )]


    )
      
    å…¶ä¸­é€šè¿‡å¯¹éšæœºåŠ¨ä½œã€ç­–ç•¥åŠ¨ä½œå’Œä¸‹ä¸€åŠ¨ä½œè¿›è¡Œé‡‡æ ·ï¼Œè®¡ç®— logsumexp çš„é¡¹ã€‚
  + **æ­¥éª¤**
    ï¼š
    1. è®¡ç®— TD ç›®æ ‡

       y
       =
       r
       +
       Î³
       (
       min
       â¡
       (
       Q
       1
       â€²
       ,
       Q
       2
       â€²
       )
       +
       Î±
       â‹…
       entropy
       )
       y = r + \gamma ( \min(Q\_1', Q\_2') + \alpha \cdot \text{entropy})





       y



       =





       r



       +





       Î³

       (

       min

       (


       Q









       1






       â€²

       â€‹


       ,




       Q









       2






       â€²

       â€‹


       )



       +





       Î±



       â‹…






       entropy

       )
       ã€‚
    2. åˆ†åˆ«è®¡ç®— critic\_1\_loss ä¸ critic\_2\_lossï¼ˆå‡æ–¹è¯¯å·®æŸå¤±ï¼‰ã€‚
    3. é¢å¤–é‡‡æ ·ä¸€æ‰¹éšæœºåŠ¨ä½œï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰ï¼Œä»¥åŠç­–ç•¥ç”Ÿæˆçš„å½“å‰å’Œä¸‹ä¸€åŠ¨ä½œï¼Œå¯¹å„ä¸ª Q ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œ logsumexp æ“ä½œï¼Œå½¢æˆ CQL æ­£åˆ™é¡¹ï¼›
    4. æ€»çš„ critic æŸå¤± = SAC critic loss +

       Î²
       \beta





       Î²
       Ã—(CQL æ­£åˆ™é¡¹å·®å€¼)ã€‚
    5. åˆ†åˆ«æ›´æ–° critic\_1 ä¸ critic\_2ï¼›
    6. æ›´æ–° actorï¼Œä½¿å…¶æœ€å¤§åŒ–

       min
       â¡
       (
       Q
       1
       ,
       Q
       2
       )
       âˆ’
       Î±
       log
       â¡
       Ï€
       (
       a
       âˆ£
       s
       )
       \min(Q\_1, Q\_2) - \alpha \log\pi(a|s)





       min

       (


       Q









       1

       â€‹


       ,




       Q









       2

       â€‹


       )



       âˆ’





       Î±



       lo
       g



       Ï€

       (

       a

       âˆ£

       s

       )
       ï¼›
    7. æ›´æ–°

       Î±
       \alpha





       Î±
       ä½¿ç­–ç•¥ç†µæ¥è¿‘ç›®æ ‡ç†µï¼›
    8. æœ€åå¯¹ç›®æ ‡ç½‘ç»œè¿›è¡Œè½¯æ›´æ–°ã€‚
  + **è¾“å…¥**
    ï¼š
    - transition\_dict åŒ…å« â€˜statesâ€™, â€˜actionsâ€™, â€˜rewardsâ€™, â€˜next\_statesâ€™, â€˜donesâ€™ã€‚
  + **è¾“å‡º**
    ï¼š
    - æ¨¡å‹å‚æ•°æ›´æ–°åï¼Œç­–ç•¥å’Œ Q ç½‘ç»œå¾—åˆ°æ”¹è¿›ã€‚

---

### CQL æ€»ç»“

**CQL**
ç±»åœ¨ SAC æ¡†æ¶ä¸‹å¢åŠ äº†ä¿å®ˆæ€§æ­£åˆ™é¡¹ï¼Œä»¥é™ä½ Q å‡½æ•°å¯¹æœªè§åŠ¨ä½œè¿‡é«˜çš„ä¼°è®¡ï¼Œä»è€Œå®ç°æ›´ä¸ºä¿å®ˆçš„ Q å­¦ä¹ ã€‚ä¸»è¦æµç¨‹åŒ…æ‹¬ï¼š

* **ç½‘ç»œåˆå§‹åŒ–**
  ï¼šæ„é€ ç­–ç•¥ç½‘ç»œã€ä¸¤ä¸ª Q ç½‘ç»œã€å¯¹åº”ç›®æ ‡ç½‘ç»œï¼Œæ¸©åº¦å‚æ•° (\alpha) åŠå…¶ä¼˜åŒ–å™¨ï¼Œä»¥åŠ SAC å›ºæœ‰å‚æ•°ï¼ˆgammaã€tauã€ç›®æ ‡ç†µï¼‰å’Œ CQL è¶…å‚æ•°ï¼ˆbetaã€num\_randomï¼‰ã€‚
* **åŠ¨ä½œé‡‡æ ·**
  ï¼ˆtake\_actionï¼‰ï¼šç»™å®šçŠ¶æ€é€šè¿‡ actor ç½‘ç»œé‡‡æ ·åŠ¨ä½œã€‚
* **è½¯æ›´æ–°**
  ï¼ˆsoft\_updateï¼‰ï¼šå¹³æ»‘æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°ã€‚
* **æ›´æ–°è¿‡ç¨‹**
  ï¼ˆupdateï¼‰ï¼š
  1. ä» transition\_dict ä¸­è¯»å–æ•°æ®å¹¶è½¬æ¢ä¸ºå¼ é‡ï¼›
  2. è®¡ç®— SAC çš„ TD ç›®æ ‡å’Œ critic æŸå¤±ï¼›
  3. é¢å¤–é‡‡æ ·éšæœºåŠ¨ä½œåŠç­–ç•¥åŠ¨ä½œï¼Œå¹¶è®¡ç®— CQL æ­£åˆ™é¡¹ï¼ˆé€šè¿‡ logsumexpï¼‰ï¼›
  4. æ€»çš„ critic æŸå¤± = SAC critic loss + beta\*(æ­£åˆ™é¡¹å·®å€¼)ï¼›
  5. æ›´æ–° critic ç½‘ç»œã€ç­–ç•¥ç½‘ç»œåŠæ¸©åº¦å‚æ•° (\alpha)ï¼›
  6. æœ€åè½¯æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°ã€‚

---

### CQL ç±»è¯¦ç»†åˆ†æ

```python
class CQL:
    ''' CQLç®—æ³• '''
    def __init__(self, state_dim, hidden_dim, action_dim, action_bound,
                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,
                 device, beta, num_random):
        """
        å®šä¹‰ CQL ç±»æ„é€ å‡½æ•°ï¼Œæ¥æ”¶çŠ¶æ€ã€éšè—ã€åŠ¨ä½œç»´åº¦ã€åŠ¨ä½œèŒƒå›´ï¼Œ
        ä»¥åŠå„ä¼˜åŒ–å™¨å­¦ä¹ ç‡ã€ç›®æ ‡ç†µã€tauã€gammaã€
        è®¾å¤‡ã€CQLæ­£åˆ™ç³»æ•° beta å’Œéšæœºé‡‡æ ·æ•° num_randomã€‚
        """
        '''åˆ›å»ºç­–ç•¥ç½‘ç»œï¼ˆactorï¼‰ï¼Œç”¨äºè¾“å‡ºåŠ¨ä½œåˆ†å¸ƒå¹¶é‡‡æ ·è¿ç»­åŠ¨ä½œã€‚'''
        self.actor = PolicyNetContinuous(state_dim, hidden_dim, action_dim, action_bound).to(device)
        '''åˆ›å»ºä¸¤ä¸ª Q ç½‘ç»œï¼Œåˆ†åˆ«ç”¨äºè¯„ä¼° (state,action) å¯¹çš„ä»·å€¼ï¼Œå¸®åŠ©é™ä½è¿‡ä¼°è®¡åå·®ã€‚'''
        self.critic_1 = QValueNetContinuous(state_dim, hidden_dim, action_dim).to(device)
        self.critic_2 = QValueNetContinuous(state_dim, hidden_dim, action_dim).to(device)
        '''åˆ›å»ºç›®æ ‡ Q ç½‘ç»œï¼Œç”¨äºè®¡ç®— TD ç›®æ ‡ï¼Œä½¿å¾—è®­ç»ƒæ›´ç¨³å®šã€‚'''
        self.target_critic_1 = QValueNetContinuous(state_dim, hidden_dim, action_dim).to(device)
        self.target_critic_2 = QValueNetContinuous(state_dim, hidden_dim, action_dim).to(device)
        '''å¤åˆ¶ critic ç½‘ç»œå‚æ•°åˆ°ç›®æ ‡ç½‘ç»œï¼Œä¿è¯åˆå§‹æ—¶ä¸€è‡´ã€‚'''
        self.target_critic_1.load_state_dict(self.critic_1.state_dict())
        self.target_critic_2.load_state_dict(self.critic_2.state_dict())
        '''ä¸ºç­–ç•¥ç½‘ç»œåˆ†é… Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ actor_lrã€‚'''
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)
        '''åˆ†åˆ«ä¸ºä¸¤ä¸ª Q ç½‘ç»œåˆ›å»º Adam ä¼˜åŒ–å™¨ã€‚'''
        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(), lr=critic_lr)
        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(), lr=critic_lr)
        '''åˆå§‹åŒ–æ¸©åº¦å‚æ•°çš„å¯¹æ•°ï¼Œlog_alpha = log(0.01) â‰ˆ -4.6052ã€‚'''
        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)
        '''ä½¿ log_alpha å¯æ±‚æ¢¯åº¦ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ alpha=exp({log_alpha})ã€‚'''
        self.log_alpha.requires_grad = True  #å¯¹alphaæ±‚æ¢¯åº¦
        '''ä¸º log_alpha åˆ›å»º Adam ä¼˜åŒ–å™¨ï¼Œå­¦ä¹ ç‡ alpha_lrã€‚'''
        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=alpha_lr)
        '''ä¿å­˜ç›®æ ‡ç†µï¼Œç”¨äºæ¸©åº¦è°ƒæ•´ã€‚'''
        self.target_entropy = target_entropy  # ç›®æ ‡ç†µçš„å¤§å°
        '''ä¿å­˜æŠ˜æ‰£å› å­ gammaã€‚'''
        self.gamma = gamma
        '''ä¿å­˜è½¯æ›´æ–°ç³»æ•° tauï¼Œç”¨äºç›®æ ‡ç½‘ç»œæ›´æ–°ã€‚'''
        self.tau = tau
        '''ä¿å­˜ CQL æŸå¤±å‡½æ•°ä¸­çš„ç³»æ•° betaï¼Œç”¨äºæƒè¡¡ Q ç½‘ç»œé¢å¤–æ­£åˆ™é¡¹ã€‚'''
        self.beta = beta  # CQLæŸå¤±å‡½æ•°ä¸­çš„ç³»æ•°
        '''ä¿å­˜ CQL ä¸­åœ¨è®¡ç®—æ­£åˆ™é¡¹æ—¶æ‰€é‡‡æ ·çš„éšæœºåŠ¨ä½œæ•°ï¼Œç”¨äºä¼°è®¡åŠ¨ä½œåˆ†å¸ƒä¸Šç•Œã€‚'''
        self.num_random = num_random  # CQLä¸­çš„åŠ¨ä½œé‡‡æ ·æ•°

    def take_action(self, state):
        """å®šä¹‰ç­–ç•¥æ‰§è¡Œæ¥å£ï¼Œç»™å®šå•ä¸ªçŠ¶æ€ï¼Œè¾“å‡ºå¯¹åº”åŠ¨ä½œã€‚"""
        if isinstance(state, tuple):
            state = state[0]
        state = torch.tensor([state], dtype=torch.float).to(device)
        # action = self.actor(state)[0]
        # log_prob = self.actor(state)[1]
        action, log_prob = self.actor(state)
        return [action.item()]

    def soft_update(self, net, target_net):
        """å¯¹ç›®æ ‡ç½‘ç»œè¿›è¡Œè½¯æ›´æ–°ï¼Œå³ç”¨å½“å‰ç½‘ç»œå‚æ•°æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°ã€‚"""
        for param_target, param in zip(target_net.parameters(), net.parameters()):
            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)

    def update(self, transition_dict):
        """
        å®šä¹‰ç­–ç•¥ä¸ Q ç½‘ç»œçš„æ›´æ–°è¿‡ç¨‹ï¼Œä½¿ç”¨ä»ç¯å¢ƒé‡‡é›†çš„ç»éªŒæ•°æ®æ›´æ–°æ‰€æœ‰ç½‘ç»œå‚æ•°ï¼ŒåŒæ—¶è®¡ç®— CQL çš„é¢å¤–æ­£åˆ™é¡¹ã€‚
        """
        '''ä»transition_dictä¸­æå–æ•°æ®'''
        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(device)
        actions = torch.tensor(transition_dict['actions'], dtype=torch.float).view(-1, 1).to(device)
        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(device)
        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(device)
        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(device)
        '''å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œè¿™é‡Œå°†å€’ç«‹æ‘†ç¯å¢ƒçš„å¥–åŠ±å¹³ç§»ä¸ç¼©æ”¾ï¼Œä½¿å¾—å¥–åŠ±èŒƒå›´æ›´åŠ ç¨³å®šã€‚'''
        rewards = (rewards + 8.0) / 8.0  # å¯¹å€’ç«‹æ‘†ç¯å¢ƒçš„å¥–åŠ±è¿›è¡Œé‡å¡‘
        '''å¯¹æ‰€æœ‰ä¸‹ä¸€çŠ¶æ€é€šè¿‡ actor å¾—åˆ°ä¸‹ä¸€åŠ¨ä½œåŠå…¶å¯¹æ•°æ¦‚ç‡ã€‚'''
        next_actions, log_prob = self.actor(next_states)
        '''è®¡ç®—ç†µé¡¹ï¼Œå³ç†µ = - log_probã€‚'''
        entropy = -log_prob
        '''ç”¨ç›®æ ‡ Q ç½‘ç»œè®¡ç®—ä¸‹ä¸€çŠ¶æ€ä¸‹çš„ Q å€¼ä¼°è®¡ã€‚'''
        q1_value = self.target_critic_1(next_states, next_actions)
        q2_value = self.target_critic_2(next_states, next_actions)
        '''è®¡ç®—ä¸‹ä¸€æ—¶åˆ»çš„ä»·å€¼ä¼°è®¡ï¼Œé€‰æ‹©è¾ƒå°çš„ Q å€¼ï¼ˆåŒ Q æœºåˆ¶ï¼‰ï¼Œå¹¶åŠ å…¥ç†µæ­£åˆ™é¡¹ Î±â‹…entropyã€‚'''
        next_value = torch.min(q1_value, q2_value) + self.log_alpha.exp() * entropy
        '''
        è®¡ç®— TD ç›®æ ‡ï¼Œè‹¥ done ä¸º 1ï¼ˆç»ˆæ­¢çŠ¶æ€ï¼‰åˆ™ä¸åŠ æŠ˜æ‰£ã€‚
        td_target = å½“å‰çš„å³æ—¶å¥–åŠ± + gamma*ä¸‹ä¸€é˜¶æ®µçš„å¥–åŠ±
        '''
        td_target = rewards + self.gamma * next_value * (1 - dones)
        '''
        åˆ†åˆ«è®¡ç®—ä¸¤ä¸ª Q ç½‘ç»œçš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ï¼Œç›¸å¯¹äº td_targetï¼ˆdetaché˜²æ­¢æ¢¯åº¦æµå‘ç›®æ ‡ï¼‰ã€‚
        '''
        critic_1_loss = torch.mean(F.mse_loss(self.critic_1(states, actions), td_target.detach()))
        critic_2_loss = torch.mean(F.mse_loss(self.critic_2(states, actions), td_target.detach()))

        # ä»¥ä¸Šä¸SACç›¸åŒ,ä»¥ä¸‹Qç½‘ç»œæ›´æ–°æ˜¯CQLçš„é¢å¤–éƒ¨åˆ†
        '''å–å½“å‰æ‰¹æ¬¡æ ·æœ¬æ•°é‡ã€‚'''
        batch_size = states.shape[0]
        '''
        ä½œç”¨ï¼š
        - ç”Ÿæˆä¸€ç»„éšæœºå‡åŒ€åˆ†å¸ƒçš„åŠ¨ä½œï¼Œå½¢çŠ¶ä¸º (batch_size*num_random, action_dim)ï¼ŒèŒƒå›´åœ¨ [-1,1]ã€‚
        - ç”¨äº CQL æ­£åˆ™é¡¹è®¡ç®—ï¼Œä½œä¸ºé¢å¤–çš„åŠ¨ä½œæ ·æœ¬ã€‚
        æ•°å€¼ä¾‹å­ï¼š
        - è‹¥ batch_size=64, num_random=10, action_dim=1ï¼Œåˆ™ç”Ÿæˆå½¢çŠ¶ä¸º (640,1) çš„éšæœºåŠ¨ä½œï¼Œ
          å¦‚ [[0.23], [-0.45], â€¦]ã€‚
        '''
        random_unif_actions = torch.rand([batch_size * self.num_random, actions.shape[-1]], dtype=torch.float).uniform_(-1, 1).to(device)
        '''
        ä½œç”¨ï¼š
        è®¡ç®—å‡åŒ€åˆ†å¸ƒçš„å¯¹æ•°æ¦‚ç‡å¯†åº¦ã€‚
        - ç”±äºåœ¨è¿ç»­åŒºé—´ [âˆ’1,1] ä¸­ï¼Œå‡åŒ€å¯†åº¦ä¸º 1/2 å¯¹äºæ¯ä¸ªåŠ¨ä½œç»´åº¦ï¼Œå› æ­¤å¯¹æ•°æ¦‚ç‡ä¸º log(0.5)ï¼›
        - å¯¹äº action_dim ä¸ªç»´åº¦ï¼Œæ€»å’Œä¸º log(0.5^action_dim)ã€‚
        '''
        random_unif_log_pi = np.log(0.5**next_actions.shape[-1])
        # random_unif_log_pi = np.log(0.5) * next_actions.shape[-1]
        '''
        ä½œç”¨ï¼šå¢åŠ æ•°æ®é›†
        - å°† states å¢åŠ ä¸€ä¸ªæ–°ç»´åº¦ï¼Œé‡å¤ num_random æ¬¡ï¼Œä½¿å¾—æ¯ä¸ªæ ·æœ¬é‡å¤ num_random æ¬¡ï¼Œ
          æœ€å reshape æˆ (batch_size*num_random, state_dim)ã€‚
        æ•°å€¼ä¾‹å­ï¼š
        - è‹¥ states shape=(64,3), 
          unsqueezeåå˜ä¸º (64,1,3), 
          repeat åå˜ä¸º (64,10,3), view åä¸º (640,3).
        '''
        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])
        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])
        '''ç”¨ç­–ç•¥ç½‘ç»œå¯¹ tmp_statesï¼ˆé‡å¤åçš„çœŸå®çŠ¶æ€ï¼‰é‡‡æ ·åŠ¨ä½œï¼Œå¾—åˆ°éšæœºé‡‡æ ·çš„å½“å‰åŠ¨ä½œåŠå…¶å¯¹æ•°æ¦‚ç‡ã€‚'''
        random_curr_actions, random_curr_log_pi = self.actor(tmp_states)
        '''åŒç†ï¼Œå¯¹ tmp_next_states é‡‡æ ·ä¸‹ä¸€æ­¥åŠ¨ä½œå’Œå¯¹åº”å¯¹æ•°æ¦‚ç‡ã€‚'''
        random_next_actions, random_next_log_pi = self.actor(tmp_next_states)
        '''ç”¨ critic ç½‘ç»œè®¡ç®—å¯¹äº tmp_states ä¸éšæœºå‡åŒ€åŠ¨ä½œ random_unif_actions çš„ Q å€¼ï¼Œä¹‹å reshape æˆ (batch_size, num_random, 1)ã€‚'''
        q1_unif = self.critic_1(tmp_states, random_unif_actions).view(-1, self.num_random, 1)
        q2_unif = self.critic_2(tmp_states, random_unif_actions).view(-1, self.num_random, 1)
        '''ç”¨ critic ç½‘ç»œè®¡ç®—å¯¹äº tmp_states ä¸éšæœºå½“å‰åŠ¨ä½œçš„ Q å€¼ï¼Œå¹¶ reshapeã€‚'''
        q1_curr = self.critic_1(tmp_states, random_curr_actions).view(-1, self.num_random, 1)
        q2_curr = self.critic_2(tmp_states, random_curr_actions).view(-1, self.num_random, 1)
        '''ç”¨ critic ç½‘ç»œè®¡ç®—å¯¹äº tmp_states ä¸éšæœºä¸‹ä¸€åŠ¨ä½œçš„ Q å€¼ï¼Œå¹¶ reshapeã€‚'''
        q1_next = self.critic_1(tmp_states, random_next_actions).view(-1, self.num_random, 1)
        q2_next = self.critic_2(tmp_states, random_next_actions).view(-1, self.num_random, 1)
        '''
        ä½œç”¨ï¼š
        - å°†ä¸‰éƒ¨åˆ† Q å€¼è¿›è¡Œæ‹¼æ¥ï¼š
          1. å¯¹äºéšæœºå‡åŒ€é‡‡æ ·åŠ¨ä½œï¼šå‡å»å…¶å¯¹æ•°æ¦‚ç‡ï¼ˆå›ºå®šå€¼ï¼‰ï¼›
          2. å¯¹äºç­–ç•¥é‡‡æ ·çš„å½“å‰åŠ¨ä½œï¼šå‡å»å¯¹åº”å¯¹æ•°æ¦‚ç‡ï¼ˆdetach åé¿å…æ¢¯åº¦ä¼ é€’ï¼‰ï¼›
          3. å¯¹äºç­–ç•¥é‡‡æ ·çš„ä¸‹ä¸€åŠ¨ä½œï¼šåŒç†ã€‚
        - æ‹¼æ¥ç»´åº¦ä¸ºç¬¬ 1 ç»´ï¼ˆå³åŠ¨ä½œæ ·æœ¬ç»´åº¦ï¼‰ã€‚
        æ•°å€¼ä¾‹å­ï¼š
        - å‡è®¾æ¯ä¸ªéƒ¨åˆ†å½¢çŠ¶ä¸º (64,10,1)ï¼Œæ‹¼æ¥å q1_cat å½¢çŠ¶ä¸º (64,30,1)ã€‚
        åœ¨ Conservative Q-Learning (CQL) ä¸­ï¼Œä¸ºäº†é˜²æ­¢ Q å‡½æ•°åœ¨ç¦»çº¿æ•°æ®ä¹‹å¤–çš„åŠ¨ä½œä¸Šè¿‡é«˜ä¼°è®¡ï¼Œ
        ä¼šåœ¨ critic æŸå¤±ä¸­å¢åŠ ä¸€ä¸ªæ­£åˆ™é¡¹ã€‚æ­£åˆ™é¡¹çš„æ€æƒ³æ˜¯â€œä¿å®ˆåœ°â€ä¼°è®¡ Q å€¼ï¼Œä½¿å¾—å¯¹äºæœªè§è¿‡çš„åŠ¨ä½œï¼Œ
        Q å€¼ä¸è‡³äºè¿‡é«˜ã€‚å…·ä½“æ¥è¯´ï¼ŒCQL çš„æ­£åˆ™é¡¹å¤§è‡´å½¢å¼ä¸ºï¼š
                            Penalty=logE_{aâˆ¼Î¼}[exp(Q(s,a)âˆ’logÎ¼(a))]âˆ’E_{(s,a)âˆ¼D}[Q(s,a)]
        '''
        q1_cat = torch.cat([
            q1_unif - random_unif_log_pi,
            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),
            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)
        ], dim=1)
        q2_cat = torch.cat([
            q2_unif - random_unif_log_pi,
            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),
            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)
        ], dim=1)
        '''
        ä½œç”¨ï¼š
        - å¯¹ q1_cat å’Œ q2_cat åœ¨åŠ¨ä½œç»´åº¦ä¸Šåš logsumexp æ“ä½œï¼Œå†å–å¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ªæ ‡é‡ï¼Œ
          è¡¨ç¤ºå¯¹æ‰€æœ‰éšæœºåŠ¨ä½œæ ·æœ¬çš„è½¯æœ€å¤§å€¼ã€‚
        - logsumexp æ˜¯ä¸€ç§å¹³æ»‘çš„æœ€å¤§å€¼å‡½æ•°ï¼Œè®¡ç®—å…¬å¼ï¼š
                            logâˆ‘_iexp(xi)
        '''
        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()
        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()
        '''åˆ†åˆ«è®¡ç®— critic ç½‘ç»œå¯¹äºå½“å‰çœŸå® (states, actions) çš„ Q å€¼çš„å‡å€¼ã€‚'''
        qf1_loss_2 = self.critic_1(states, actions).mean()
        qf2_loss_2 = self.critic_2(states, actions).mean()
        '''å°†åŸæœ¬ SAC ä¸­çš„ critic æŸå¤±ï¼ˆå‡æ–¹è¯¯å·®ï¼‰ä¸ CQL æ­£åˆ™é¡¹ç›¸åŠ ï¼Œæ„æˆæœ€ç»ˆ critic æŸå¤±ã€‚'''
        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)
        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)
        '''
        å¯¹ critic_1 çš„æŸå¤±è¿›è¡Œåå‘ä¼ æ’­æ›´æ–°ã€‚
        å¯¹ critic_2 çš„æŸå¤±è¿›è¡Œåå‘ä¼ æ’­æ›´æ–°ã€‚
        '''
        self.critic_1_optimizer.zero_grad()
        qf1_loss.backward(retain_graph=True)
        self.critic_1_optimizer.step()
        self.critic_2_optimizer.zero_grad()
        qf2_loss.backward(retain_graph=True)
        self.critic_2_optimizer.step()

        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        '''ä½¿ç”¨å½“å‰ç­–ç•¥å¯¹çœŸå®çŠ¶æ€é‡‡æ ·åŠ¨ä½œï¼Œå¹¶è·å¾—å¯¹åº”å¯¹æ•°æ¦‚ç‡ã€‚
        self.actoræœªæ›´æ–°'''
        new_actions, log_prob = self.actor(states)
        entropy = -log_prob
        '''
        è¯„ä¼°å½“å‰ç­–ç•¥ç”Ÿæˆçš„åŠ¨ä½œçš„ Q å€¼ï¼Œä½¿ç”¨ critic_1 å’Œ critic_2 åˆ†åˆ«è®¡ç®—ï¼Œå¹¶å–æœ€å°å€¼ä»¥é™ä½è¿‡ä¼°è®¡ã€‚
        self.critic_1å’Œself.critic_2æ˜¯åˆšæ›´æ–°è¿‡çš„
        '''
        q1_value = self.critic_1(states, new_actions)
        q2_value = self.critic_2(states, new_actions)
        '''
        ä½œç”¨ï¼š
        - è®¡ç®—ç­–ç•¥æŸå¤±ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ– min(ğ‘„1,ğ‘„2)âˆ’ğ›¼logğœ‹ï¼›è¿™é‡Œå–è´Ÿå·åä½œä¸ºæŸå¤±ã€‚
        '''
        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - torch.min(q1_value, q2_value))
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()

        # æ›´æ–°alphaå€¼
        '''
        ä½œç”¨ï¼š
        - è®¡ç®—æ¸©åº¦å‚æ•° Î± çš„æŸå¤±ï¼Œç›®æ ‡æ˜¯ä½¿ç­–ç•¥ç†µæ¥è¿‘ç›®æ ‡ç†µã€‚
        - detach() è¡¨ç¤ºä¸å¯¹ entropy åå‘ä¼ æ’­æ¢¯åº¦ï¼Œåªæ›´æ–° alphaã€‚
        SACåŸå§‹è®ºæ–‡ä¸­ï¼š
                            J(Î±)=E_{aâˆ¼Ï€}[âˆ’Î±(logÏ€(aâˆ£s)+Htarget)]ï¼Œå…¶ä¸­H=âˆ’logÏ€(aâˆ£s)
        '''
        alpha_loss = torch.mean((entropy - self.target_entropy).detach() * self.log_alpha.exp())
        self.log_alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.log_alpha_optimizer.step()

        self.soft_update(self.critic_1, self.target_critic_1)
        self.soft_update(self.critic_2, self.target_critic_2)


random.seed(0)
np.random.seed(0)

if not hasattr(env, 'seed'):
    def seed_fn(self, seed=None):
        env.reset(seed=seed)
        return [seed]
    env.seed = seed_fn.__get__(env, type(env))
# env.seed(0)
torch.manual_seed(0)

beta = 5.0
num_random = 5
num_epochs = 100
num_trains_per_epoch = 500

agent = CQL(state_dim, hidden_dim, action_dim, action_bound, actor_lr,
            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,
            num_random)

return_list = []
for i in range(10):
    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:
        for i_epoch in range(int(num_epochs / 10)):
            # æ­¤å¤„ä¸ç¯å¢ƒäº¤äº’åªæ˜¯ä¸ºäº†è¯„ä¼°ç­–ç•¥,æœ€åä½œå›¾ç”¨,ä¸ä¼šç”¨äºè®­ç»ƒ
            epoch_return = 0
            state = env.reset()
            done = False
            while not done:
                action = agent.take_action(state)
                result = env.step(action)
                if len(result) == 5:
                    next_state, reward, done, truncated, info = result
                    done = done or truncated  # å¯åˆå¹¶ terminated å’Œ truncated æ ‡å¿—
                else:
                    next_state, reward, done, info = result
                # next_state, reward, done, _ = env.step(action)
                state = next_state
                epoch_return += reward
            return_list.append(epoch_return)

            for _ in range(num_trains_per_epoch):
                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)
                transition_dict = {
                    'states': b_s,
                    'actions': b_a,
                    'next_states': b_ns,
                    'rewards': b_r,
                    'dones': b_d
                }
                agent.update(transition_dict)

            if (i_epoch + 1) % 10 == 0:
                pbar.set_postfix({
                    'epoch':
                    '%d' % (num_epochs / 10 * i + i_epoch + 1),
                    'return':
                    '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)


epochs_list = list(range(len(return_list)))
plt.plot(epochs_list, return_list)
plt.xlabel('Epochs')
plt.ylabel('Returns')
plt.title('CQL on {}'.format(env_name))
plt.show()

mv_return = rl_utils.moving_average(return_list, 9)
plt.plot(episodes_list, mv_return)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('CQL on {}'.format(env_name))
plt.show()

```