---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f796a685f53453030372f:61727469636c652f64657461696c732f313436313334303631"
layout: post
title: "Phi-4-multimodal图文音频统一的多模态大模型架构训练方法数据细节"
date: 2025-03-09 16:19:36 +0800
description: "Phi-4-Multimodal 是一种参数高效的多模态模型，通过 LoRA 适配器和模式特定路由器实现文本、视觉和语音/音频的无缝集成。训练过程包括多阶段优化，确保在不同模式和任务上的性能，数据来源多样，覆盖高质量网络和合成数据。它的设计体现了小型语言模型在多模态任务上的潜力。"
keywords: "语音图像输入大模型"
categories: ['大语言模型', '多模态']
tags: ['多模态', 'Llm']
artid: "146134061"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146134061
    alt: "Phi-4-multimodal图文音频统一的多模态大模型架构训练方法数据细节"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146134061
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146134061
cover: https://bing.ee123.net/img/rand?artid=146134061
image: https://bing.ee123.net/img/rand?artid=146134061
img: https://bing.ee123.net/img/rand?artid=146134061
---

# Phi-4-multimodal：图、文、音频统一的多模态大模型架构、训练方法、数据细节

Phi-4-Multimodal 是一种参数高效的多模态模型，通过 LoRA 适配器和模式特定路由器实现文本、视觉和语音/音频的无缝集成。训练过程包括多阶段优化，确保在不同模式和任务上的性能，数据来源多样，覆盖高质量网络和合成数据。它的设计体现了小型语言模型在多模态任务上的潜力

### 模型架构

![模型架构](https://i-blog.csdnimg.cn/img_convert/ce86840a8f13806fed801728c6465fa2.png)

Phi-4-Multimodal 的基础是
**Phi-4-Mini**
语言模型，这是一个 3.8 亿参数的模型，设计为高效处理文本任务。架构包括：

* **Transformer层和维度：**
  32 层Transformer，隐藏状态大小为 3072。
* **效率技术：**
  使用分组查询注意力（GQA），通过 24 个查询头和 8 个键/值头减少 KV 缓存大小至标准模型的三分之一，提升计算效率。
* **分词器：**
  使用 o200k 基础 tiktoken，分词表大小为 200,064，支持多语言和多模态输入。

为了扩展到多模态功能，模型通过 LoRA 适配器和模式特定路由器集成视觉和音频模式：

* **视觉模式：**

  + **图像编码器：**
    使用 SigLIP-400M，结合 LLM2CLIP 在图像-文本对上微调，分辨率为 448x448。SigLIP-400M 是一个视觉-语言模型，专门为图像理解优化。
  + **项目器：**
    一个 2 层 MLP，将视觉特征映射到文本嵌入维度 3072，确保视觉输入与语言模型的嵌入空间兼容。
  + **LoRA 适配器（LoRA\_V）：**
    添加到语言解码器的所有线性层，参数约 3.7 亿，用于监督微调阶段。LoRA 是一种参数高效的微调技术，通过低秩更新适配模型。
  + **动态多裁剪策略：**
    在训练中处理不同图像大小，裁剪数量计算为 ⌈H/C⌉ × ⌈W/C⌉，预训练最多 16 个裁剪，监督微调最多 36 个，必要时调整大小。
* **语音/音频模式：**

  + **输入特征：**
    80 维 log-Mel 滤波器组特征，帧率为 10ms，标记率为 80ms（每分钟 750 个标记），适合高效音频处理。
  + **音频编码器：**
    包括 3 个卷积层和 24 个符合块，注意维度为 1024，前馈维度为 1536，16 个注意头，子采样率为 8。符合块结合了自注意力机制和卷积，适合捕获音频的时序和频率特征。
  + **项目器：**
    一个 2 层 MLP，将 1024 维语音特征映射到 3072 维文本嵌入，确保音频输入与语言模型的嵌入空间兼容。
  + **LoRA 适配器（LoRA\_A）：**
    应用于所有注意和 MLP 层，秩为 320，参数约 4.6 亿，通过低秩更新适配音频处理。
* **多模态集成：**
  模型采用 LoRA 适配器的混合设计，通过模式特定路由器选择适当的适配器，处理文本、视觉和语音/音频输入，无干扰地支持多模态推理。这是一种参数高效的方法，保持基础语言模型的完整性，同时添加新功能。

**总参数量为 5.6 亿**
，相比 Phi-4-Mini 的 3.8 亿，增加了约 1.8 亿参数，主要用于视觉和音频编码器及 LoRA 适配器。上下文长度为 128K 标记，受益于 GQA 和其他效率技术，适合处理长序列输入。

### 训练方法

Phi-4-Multimodal 的训练过程分多个阶段，针对不同模式和任务优化，确保模型在多模态任务上的性能。训练步骤如下：

* **基础语言模型预训练：**

  + 在 5 万亿个高质量标记上预训练，包括网络数据和合成数据。数据来源经过精心挑选，确保覆盖多种语言和任务，如功能调用、总结和指令跟随。
* **视觉训练：**

  + **阶段 1：项目器对齐**
    - 使用标题数据训练项目器，确保视觉特征与语言模型嵌入空间的对齐。
  + **阶段 2：联合视觉训练**
    - 在完整数据集上训练项目器和编码器，针对 OCR 和密集理解任务，数据集包括图像-文本对、OCR PDF 和现实图像。
  + **阶段 3：生成视觉-语言训练**
    - 在解码器上训练 LoRA，使用单帧 SFT 数据，开发生成能力，数据集包括公共和内部多模态数据集，如通用图像、图表/表格/图表、PowerPoint、OCR、多图像和视频。
  + **阶段 4：多帧训练**
    - 视觉编码器冻结，在多帧 SFT 数据上训练，上下文长度为 64k，适合处理多帧场景。
* **语音/音频训练：**

  + **预训练：**
    使用 200 万小时匿名语音-文本对，覆盖 8 种语言（中文、英语、法语、德语、意大利语、日语、葡萄牙语、西班牙语），训练音频编码器和项目器，解码器冻结，初始化为自动编码解码（AED）ASR 模型。
  + **后训练：**
    使用 1 亿个精选 SFT 样本更新项目器和 LoRA\_A，50,000 步。最大音频长度为总结的 30 分钟（22,500 个标记），其他任务的 30 秒（375 个标记），包括 ASR（40,000 小时，2,800 万 SFT 示例）、AST（30,000 小时，2,800 万 SFT 示例，7 种语言到/从英语，CoT）、SQA/SQQA（2,600 万 SFT 示例，合成 QA 对，TTS 生成查询）、总结（100 万 SFT 示例，英语，多说话者，GPT-4 查询）和音频理解（1,700 万 SFT 示例，公共音频/音乐，GPT-4 Q&A）。
* **视觉-语音联合训练：**

  + 在视觉和语音单独训练后，冻结语言基础、音频编码器和项目器，微调视觉适配器 LoRA\_V、编码器和项目器，使用视觉-语音 SFT 数据加上语言/视觉后训练数据，确保多模态协同工作。
* **推理训练：**

  + **阶段 1：预训练**
    - 在 600 亿推理链式思维 CoT 标记上预训练，从前沿 LLM 中提取，通过拒绝采样过滤错误输出，确保数据质量。
  + **阶段 2：微调**
    - 在 20 万个高质量 CoT 样本上微调，覆盖不同领域，如数学、编码和逻辑推理。
  + **阶段 3：直接偏好优化（DPO）训练**
    - 在 30 万个偏好样本上应用，将错误输出标记为“非首选”，纠正输出为“首选”，通过人类反馈进一步对齐模型。

### 训练数据细节

Phi-4-Multimodal 是一种由 Microsoft 开发的先进多模态大模型，能够处理文本、图像和音频输入并生成文本输出。其训练数据细节涵盖语言、视觉-语言、视觉-语音和语音/音频四个主要类别，数据来源包括网络、合成和真实数据，数据量庞大且经过精心优化。

#### 语言训练数据

语言训练是 Phi-4-Multimodal 的基础，基于 Phi-4-Mini 语言模型的预训练和后训练数据：

* **预训练数据：**

  + **数据来源：**
    高质量网络数据和合成数据，特别强调数学和编码数据集以提升复杂推理能力。
  + **数据量：**
    5 万亿个标记（tokens）。
  + **描述：**
    合成数据通过精心策划，确保覆盖高价值的任务，如数学竞赛问题和编码任务，显著提升模型在这些领域的表现。
* **后训练数据：**

  + **功能调用、总结和代码完成：**
    使用额外数据进行后训练，具体数量未公开，但涉及多种任务。
  + **推理训练：**
    使用 600 亿个推理链式思维（CoT）标记，从前沿大型语言模型（LLM）中提取，通过拒绝采样过滤错误输出，确保数据质量。
  + **微调：**
    在 20 万个高质量 CoT 样本上微调，覆盖数学、编码和逻辑推理等不同领域。
  + **直接偏好优化（DPO）：**
    在 30 万个偏好样本上应用，将错误输出标记为“非首选”，纠正输出为“首选”，通过人类反馈进一步对齐模型。

#### 视觉-语言训练数据

视觉-语言训练扩展了模型处理图像和相关文本的能力，分为预训练和监督微调（SFT）两个阶段：

* **预训练数据：**

  + **数据类型：**
    包括图像-文本对、图像接地数据、OCR PDF、现实图像和图表理解数据。
  + **数据量：**
    文本部分约 0.5 万亿标记，具体图像数量未公开。
  + **描述：**
    数据覆盖广泛，包括公共和内部多模态数据集，最高图像分辨率达 1344x1344，适合 OCR 和密集理解任务。
* **监督微调（SFT）数据：**

  + **数据类型：**
    通用图像、图表/表格/图表、PowerPoint、OCR、多图像、视频和安全数据集。
  + **数据量：**
    文本部分约 0.3 万亿标记。
  + **描述：**
    数据来源包括公共和内部数据集，确保生成能力和多模态任务性能。

#### 视觉-语音训练数据

视觉-语音训练数据是合成生成的，基于视觉-语言 SFT 数据：

* **数据创建方法：**
  复用视觉-语言 SFT 数据，通过文本转语音（TTS）引擎生成语音查询，基于词错误率（WER）过滤质量。
* **数据量：**
  具体数量未公开，但依赖于视觉-语言 SFT 数据规模（约 0.3 万亿标记文本部分）。

#### 语音/音频训练数据

语音/音频训练数据分为预训练和后训练两个阶段，数据量巨大，覆盖多种任务：

* **预训练数据：**

  + **数据来源：**
    200 万小时匿名语音-文本对，覆盖 8 种语言：中文、英语、法语、德语、意大利语、日语、葡萄牙语、西班牙语。
  + **描述：**
    用于训练音频编码器和项目器，确保语音特征与语言模型嵌入空间对齐，初始化为自动编码解码（AED）ASR 模型。
* **后训练数据：**

  + **自动语音识别（ASR）：**
    - **数据量：**
      40,000 小时。
    - **SFT 示例：**
      2.8 百万。
  + **自动语音翻译（AST）：**
    - **数据量：**
      30,000 小时。
    - **SFT 示例：**
      2.8 百万（7 种语言到/从英语，包含 CoT）。
  + **语音问答（SQA/SQQA）：**
    - **SFT 示例：**
      2.6 百万（合成 QA 对，TTS 生成查询）。
  + **总结（SSUM）：**
    - **SFT 示例：**
      100,000（英语，多说话者，GPT-4 查询）。
  + **音频理解（AU）：**
    - **SFT 示例：**
      1.7 百万（公共音频/音乐，GPT-4 Q&A）。

训练数据汇总表：

![](https://i-blog.csdnimg.cn/img_convert/57201611aa6e672df8d3cc991d6810cf.png)

**一个意想不到的细节是，语音预训练数据高达 200 万小时。**

### 性能

![](https://i-blog.csdnimg.cn/img_convert/f95f158c7f3dd03aa4e988c995f9b823.png)

参考文献：

* Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs，https://arxiv.org/pdf/2503.01743
* HunyuanVideo: A Systematic Framework For Large Video Generative Models，https://arxiv.org/pdf/2412.03603