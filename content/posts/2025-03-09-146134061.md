---
layout: post
title: "Phi-4-multimodal图文音频统一的多模态大模型架构训练方法数据细节"
date: 2025-03-09 16:19:36 +0800
description: "Phi-4-Multimodal 是一种参数高效的多模态模型，通过 LoRA 适配器和模式特定路由器实现文本、视觉和语音/音频的无缝集成。训练过程包括多阶段优化，确保在不同模式和任务上的性能，数据来源多样，覆盖高质量网络和合成数据。它的设计体现了小型语言模型在多模态任务上的潜力。"
keywords: "Phi-4-multimodal：图、文、音频统一的多模态大模型架构、训练方法、数据细节"
categories: ['大语言模型', '多模态']
tags: ['多模态', 'Llm']
artid: "146134061"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146134061
    alt: "Phi-4-multimodal图文音频统一的多模态大模型架构训练方法数据细节"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146134061
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146134061
cover: https://bing.ee123.net/img/rand?artid=146134061
image: https://bing.ee123.net/img/rand?artid=146134061
img: https://bing.ee123.net/img/rand?artid=146134061
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Phi-4-multimodal：图、文、音频统一的多模态大模型架构、训练方法、数据细节
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     Phi-4-Multimodal 是一种参数高效的多模态模型，通过 LoRA 适配器和模式特定路由器实现文本、视觉和语音/音频的无缝集成。训练过程包括多阶段优化，确保在不同模式和任务上的性能，数据来源多样，覆盖高质量网络和合成数据。它的设计体现了小型语言模型在多模态任务上的潜力
    </p>
    <h3>
     <a id="_6">
     </a>
     模型架构
    </h3>
    <p>
     <img alt="模型架构" src="https://i-blog.csdnimg.cn/img_convert/ce86840a8f13806fed801728c6465fa2.png"/>
    </p>
    <p>
     Phi-4-Multimodal 的基础是
     <strong>
      Phi-4-Mini
     </strong>
     语言模型，这是一个 3.8 亿参数的模型，设计为高效处理文本任务。架构包括：
    </p>
    <ul>
     <li>
      <strong>
       Transformer层和维度：
      </strong>
      32 层Transformer，隐藏状态大小为 3072。
     </li>
     <li>
      <strong>
       效率技术：
      </strong>
      使用分组查询注意力（GQA），通过 24 个查询头和 8 个键/值头减少 KV 缓存大小至标准模型的三分之一，提升计算效率。
     </li>
     <li>
      <strong>
       分词器：
      </strong>
      使用 o200k 基础 tiktoken，分词表大小为 200,064，支持多语言和多模态输入。
     </li>
    </ul>
    <p>
     为了扩展到多模态功能，模型通过 LoRA 适配器和模式特定路由器集成视觉和音频模式：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        视觉模式：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         图像编码器：
        </strong>
        使用 SigLIP-400M，结合 LLM2CLIP 在图像-文本对上微调，分辨率为 448x448。SigLIP-400M 是一个视觉-语言模型，专门为图像理解优化。
       </li>
       <li>
        <strong>
         项目器：
        </strong>
        一个 2 层 MLP，将视觉特征映射到文本嵌入维度 3072，确保视觉输入与语言模型的嵌入空间兼容。
       </li>
       <li>
        <strong>
         LoRA 适配器（LoRA_V）：
        </strong>
        添加到语言解码器的所有线性层，参数约 3.7 亿，用于监督微调阶段。LoRA 是一种参数高效的微调技术，通过低秩更新适配模型。
       </li>
       <li>
        <strong>
         动态多裁剪策略：
        </strong>
        在训练中处理不同图像大小，裁剪数量计算为 ⌈H/C⌉ × ⌈W/C⌉，预训练最多 16 个裁剪，监督微调最多 36 个，必要时调整大小。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        语音/音频模式：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         输入特征：
        </strong>
        80 维 log-Mel 滤波器组特征，帧率为 10ms，标记率为 80ms（每分钟 750 个标记），适合高效音频处理。
       </li>
       <li>
        <strong>
         音频编码器：
        </strong>
        包括 3 个卷积层和 24 个符合块，注意维度为 1024，前馈维度为 1536，16 个注意头，子采样率为 8。符合块结合了自注意力机制和卷积，适合捕获音频的时序和频率特征。
       </li>
       <li>
        <strong>
         项目器：
        </strong>
        一个 2 层 MLP，将 1024 维语音特征映射到 3072 维文本嵌入，确保音频输入与语言模型的嵌入空间兼容。
       </li>
       <li>
        <strong>
         LoRA 适配器（LoRA_A）：
        </strong>
        应用于所有注意和 MLP 层，秩为 320，参数约 4.6 亿，通过低秩更新适配音频处理。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        多模态集成：
       </strong>
       模型采用 LoRA 适配器的混合设计，通过模式特定路由器选择适当的适配器，处理文本、视觉和语音/音频输入，无干扰地支持多模态推理。这是一种参数高效的方法，保持基础语言模型的完整性，同时添加新功能。
      </p>
     </li>
    </ul>
    <p>
     <strong>
      总参数量为 5.6 亿
     </strong>
     ，相比 Phi-4-Mini 的 3.8 亿，增加了约 1.8 亿参数，主要用于视觉和音频编码器及 LoRA 适配器。上下文长度为 128K 标记，受益于 GQA 和其他效率技术，适合处理长序列输入。
    </p>
    <h3>
     <a id="_35">
     </a>
     训练方法
    </h3>
    <p>
     Phi-4-Multimodal 的训练过程分多个阶段，针对不同模式和任务优化，确保模型在多模态任务上的性能。训练步骤如下：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        基础语言模型预训练：
       </strong>
      </p>
      <ul>
       <li>
        在 5 万亿个高质量标记上预训练，包括网络数据和合成数据。数据来源经过精心挑选，确保覆盖多种语言和任务，如功能调用、总结和指令跟随。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        视觉训练：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         阶段 1：项目器对齐
        </strong>
        - 使用标题数据训练项目器，确保视觉特征与语言模型嵌入空间的对齐。
       </li>
       <li>
        <strong>
         阶段 2：联合视觉训练
        </strong>
        - 在完整数据集上训练项目器和编码器，针对 OCR 和密集理解任务，数据集包括图像-文本对、OCR PDF 和现实图像。
       </li>
       <li>
        <strong>
         阶段 3：生成视觉-语言训练
        </strong>
        - 在解码器上训练 LoRA，使用单帧 SFT 数据，开发生成能力，数据集包括公共和内部多模态数据集，如通用图像、图表/表格/图表、PowerPoint、OCR、多图像和视频。
       </li>
       <li>
        <strong>
         阶段 4：多帧训练
        </strong>
        - 视觉编码器冻结，在多帧 SFT 数据上训练，上下文长度为 64k，适合处理多帧场景。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        语音/音频训练：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         预训练：
        </strong>
        使用 200 万小时匿名语音-文本对，覆盖 8 种语言（中文、英语、法语、德语、意大利语、日语、葡萄牙语、西班牙语），训练音频编码器和项目器，解码器冻结，初始化为自动编码解码（AED）ASR 模型。
       </li>
       <li>
        <strong>
         后训练：
        </strong>
        使用 1 亿个精选 SFT 样本更新项目器和 LoRA_A，50,000 步。最大音频长度为总结的 30 分钟（22,500 个标记），其他任务的 30 秒（375 个标记），包括 ASR（40,000 小时，2,800 万 SFT 示例）、AST（30,000 小时，2,800 万 SFT 示例，7 种语言到/从英语，CoT）、SQA/SQQA（2,600 万 SFT 示例，合成 QA 对，TTS 生成查询）、总结（100 万 SFT 示例，英语，多说话者，GPT-4 查询）和音频理解（1,700 万 SFT 示例，公共音频/音乐，GPT-4 Q&amp;A）。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        视觉-语音联合训练：
       </strong>
      </p>
      <ul>
       <li>
        在视觉和语音单独训练后，冻结语言基础、音频编码器和项目器，微调视觉适配器 LoRA_V、编码器和项目器，使用视觉-语音 SFT 数据加上语言/视觉后训练数据，确保多模态协同工作。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        推理训练：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         阶段 1：预训练
        </strong>
        - 在 600 亿推理链式思维 CoT 标记上预训练，从前沿 LLM 中提取，通过拒绝采样过滤错误输出，确保数据质量。
       </li>
       <li>
        <strong>
         阶段 2：微调
        </strong>
        - 在 20 万个高质量 CoT 样本上微调，覆盖不同领域，如数学、编码和逻辑推理。
       </li>
       <li>
        <strong>
         阶段 3：直接偏好优化（DPO）训练
        </strong>
        - 在 30 万个偏好样本上应用，将错误输出标记为“非首选”，纠正输出为“首选”，通过人类反馈进一步对齐模型。
       </li>
      </ul>
     </li>
    </ul>
    <h3>
     <a id="_63">
     </a>
     训练数据细节
    </h3>
    <p>
     Phi-4-Multimodal 是一种由 Microsoft 开发的先进多模态大模型，能够处理文本、图像和音频输入并生成文本输出。其训练数据细节涵盖语言、视觉-语言、视觉-语音和语音/音频四个主要类别，数据来源包括网络、合成和真实数据，数据量庞大且经过精心优化。
    </p>
    <h4>
     <a id="_67">
     </a>
     语言训练数据
    </h4>
    <p>
     语言训练是 Phi-4-Multimodal 的基础，基于 Phi-4-Mini 语言模型的预训练和后训练数据：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        预训练数据：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         数据来源：
        </strong>
        高质量网络数据和合成数据，特别强调数学和编码数据集以提升复杂推理能力。
       </li>
       <li>
        <strong>
         数据量：
        </strong>
        5 万亿个标记（tokens）。
       </li>
       <li>
        <strong>
         描述：
        </strong>
        合成数据通过精心策划，确保覆盖高价值的任务，如数学竞赛问题和编码任务，显著提升模型在这些领域的表现。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        后训练数据：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         功能调用、总结和代码完成：
        </strong>
        使用额外数据进行后训练，具体数量未公开，但涉及多种任务。
       </li>
       <li>
        <strong>
         推理训练：
        </strong>
        使用 600 亿个推理链式思维（CoT）标记，从前沿大型语言模型（LLM）中提取，通过拒绝采样过滤错误输出，确保数据质量。
       </li>
       <li>
        <strong>
         微调：
        </strong>
        在 20 万个高质量 CoT 样本上微调，覆盖数学、编码和逻辑推理等不同领域。
       </li>
       <li>
        <strong>
         直接偏好优化（DPO）：
        </strong>
        在 30 万个偏好样本上应用，将错误输出标记为“非首选”，纠正输出为“首选”，通过人类反馈进一步对齐模型。
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="_82">
     </a>
     视觉-语言训练数据
    </h4>
    <p>
     视觉-语言训练扩展了模型处理图像和相关文本的能力，分为预训练和监督微调（SFT）两个阶段：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        预训练数据：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         数据类型：
        </strong>
        包括图像-文本对、图像接地数据、OCR PDF、现实图像和图表理解数据。
       </li>
       <li>
        <strong>
         数据量：
        </strong>
        文本部分约 0.5 万亿标记，具体图像数量未公开。
       </li>
       <li>
        <strong>
         描述：
        </strong>
        数据覆盖广泛，包括公共和内部多模态数据集，最高图像分辨率达 1344x1344，适合 OCR 和密集理解任务。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        监督微调（SFT）数据：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         数据类型：
        </strong>
        通用图像、图表/表格/图表、PowerPoint、OCR、多图像、视频和安全数据集。
       </li>
       <li>
        <strong>
         数据量：
        </strong>
        文本部分约 0.3 万亿标记。
       </li>
       <li>
        <strong>
         描述：
        </strong>
        数据来源包括公共和内部数据集，确保生成能力和多模态任务性能。
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="_96">
     </a>
     视觉-语音训练数据
    </h4>
    <p>
     视觉-语音训练数据是合成生成的，基于视觉-语言 SFT 数据：
    </p>
    <ul>
     <li>
      <strong>
       数据创建方法：
      </strong>
      复用视觉-语言 SFT 数据，通过文本转语音（TTS）引擎生成语音查询，基于词错误率（WER）过滤质量。
     </li>
     <li>
      <strong>
       数据量：
      </strong>
      具体数量未公开，但依赖于视觉-语言 SFT 数据规模（约 0.3 万亿标记文本部分）。
     </li>
    </ul>
    <h4>
     <a id="_103">
     </a>
     语音/音频训练数据
    </h4>
    <p>
     语音/音频训练数据分为预训练和后训练两个阶段，数据量巨大，覆盖多种任务：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        预训练数据：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         数据来源：
        </strong>
        200 万小时匿名语音-文本对，覆盖 8 种语言：中文、英语、法语、德语、意大利语、日语、葡萄牙语、西班牙语。
       </li>
       <li>
        <strong>
         描述：
        </strong>
        用于训练音频编码器和项目器，确保语音特征与语言模型嵌入空间对齐，初始化为自动编码解码（AED）ASR 模型。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        后训练数据：
       </strong>
      </p>
      <ul>
       <li>
        <strong>
         自动语音识别（ASR）：
        </strong>
        <ul>
         <li>
          <strong>
           数据量：
          </strong>
          40,000 小时。
         </li>
         <li>
          <strong>
           SFT 示例：
          </strong>
          2.8 百万。
         </li>
        </ul>
       </li>
       <li>
        <strong>
         自动语音翻译（AST）：
        </strong>
        <ul>
         <li>
          <strong>
           数据量：
          </strong>
          30,000 小时。
         </li>
         <li>
          <strong>
           SFT 示例：
          </strong>
          2.8 百万（7 种语言到/从英语，包含 CoT）。
         </li>
        </ul>
       </li>
       <li>
        <strong>
         语音问答（SQA/SQQA）：
        </strong>
        <ul>
         <li>
          <strong>
           SFT 示例：
          </strong>
          2.6 百万（合成 QA 对，TTS 生成查询）。
         </li>
        </ul>
       </li>
       <li>
        <strong>
         总结（SSUM）：
        </strong>
        <ul>
         <li>
          <strong>
           SFT 示例：
          </strong>
          100,000（英语，多说话者，GPT-4 查询）。
         </li>
        </ul>
       </li>
       <li>
        <strong>
         音频理解（AU）：
        </strong>
        <ul>
         <li>
          <strong>
           SFT 示例：
          </strong>
          1.7 百万（公共音频/音乐，GPT-4 Q&amp;A）。
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
    <p>
     训练数据汇总表：
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/57201611aa6e672df8d3cc991d6810cf.png"/>
    </p>
    <p>
     <strong>
      一个意想不到的细节是，语音预训练数据高达 200 万小时。
     </strong>
    </p>
    <h3>
     <a id="_139">
     </a>
     性能
    </h3>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/img_convert/f95f158c7f3dd03aa4e988c995f9b823.png"/>
    </p>
    <p>
     参考文献：
    </p>
    <ul>
     <li>
      <p>
       Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs，https://arxiv.org/pdf/2503.01743
      </p>
     </li>
     <li>
      <p>
       HunyuanVideo: A Systematic Framework For Large Video Generative Models，https://arxiv.org/pdf/2412.03603
      </p>
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f796a685f53453030372f:61727469636c652f64657461696c732f313436313334303631" class_="artid" style="display:none">
 </p>
</div>


