---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f736f6c6f6d6f6e7a772f:61727469636c652f64657461696c732f313436313135383339"
layout: post
title: "liunx学习四文本处理stdout-stderr,Cut,paste,sort,tr,head,Tail,join,Split,grep,..."
date: 2025-03-09 11:29:11 +08:00
description: "程序正常排水管 → 用。"
keywords: "liunx学习(四)(文本处理(stdout stderr,Cut,paste,sort,tr,head,Tail,join,Split,grep,...))"
categories: ['未分类']
tags: ['开发语言', '学习', 'Linux']
artid: "146115839"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146115839
    alt: "liunx学习四文本处理stdout-stderr,Cut,paste,sort,tr,head,Tail,join,Split,grep,..."
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146115839
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146115839
cover: https://bing.ee123.net/img/rand?artid=146115839
image: https://bing.ee123.net/img/rand?artid=146115839
img: https://bing.ee123.net/img/rand?artid=146115839
---

# liunx学习(四)(文本处理(stdout stderr,Cut,paste,sort,tr,head,Tail,join,Split,grep,...))

我们可以用
**“文字处理流水线”**
的比喻来理解 Linux 的文本处理工具，让你像工厂加工零件一样高效处理文本数据：

---

#### **1. 核心工具分工**

| 工具 | 比喻 | 核心功能 |
| --- | --- | --- |
| **`grep`** | **关键词扫描仪** | 快速过滤包含特定关键词的行 |
| **`sed`** | **批量替换机器人** | 自动查找并替换文本内容 |
| **`awk`** | **数据分拣大师** | 按列处理结构化数据（如CSV） |
| **`cut`** | **剪刀手** | 按列或字符位置裁剪文本 |

---

#### **2. 实战场景演示**

##### **场景1：用 `grep` 找日志中的错误**

```
# 在日志中搜索 "ERROR" 关键词（像用放大镜找特定文字）
grep "ERROR" app.log
# 高级用法：忽略大小写 (-i)，显示行号 (-n)
grep -i -n "critical" system.log
```

##### **场景2：用 `sed` 批量修改配置**

```
# 将文件中的 "旧密码" 全部替换为 "新密码"
sed 's/旧密码/新密码/g' config.txt > new_config.txt
# 直接修改原文件（-i 选项）
sed -i 's/192.168.1.1/10.0.0.1/g' network.conf
```

##### **场景3：用 `awk` 分析销售数据**

```
# 提取 CSV 文件的第2列（金额），并计算总和
awk -F ',' '{sum += $2} END {print "总销售额:", sum}' sales.csv
# 打印第1列（产品名）和第3列（销量），按销量排序
awk -F ',' '{print $1, $3}' sales.csv | sort -k2 -n
```

##### **场景4：用 `cut` 裁剪用户列表**

```
# 提取 /etc/passwd 中的用户名（第1列，冒号分隔）
cut -d ':' -f1 /etc/passwd
# 截取日志时间戳（前20个字符）
cut -c1-20 access.log
```

---

#### **3. 图形编辑器对比**

| 编辑器 | 比喻 | 特点 |
| --- | --- | --- |
| **gedit** | 智能写字板 | 简单易用，适合新手 |
| **nano** | 迷你便签 | 命令行下的基础编辑器 |
| **vim** | 瑞士军刀编辑器 | 功能强大，学习曲线陡峭 |

---

#### **4. 组合使用示例**

##### **分析网站访问日志**

```
# 找出访问量最高的前5个IP地址
cat access.log | awk '{print $1}' | sort | uniq -c | sort -nr | head -5
```

* **步骤分解**
  ：

  1. `cat`
     读取日志文件
  2. `awk`
     提取第1列（IP地址）
  3. `sort`
     排序相同IP相邻
  4. `uniq -c`
     统计每个IP出现次数
  5. `sort -nr`
     按访问量倒序
  6. `head -5`
     显示前5名

---

#### **5. 为什么需要这些工具？**

| 需求场景 | 适用工具 | 示例 |
| --- | --- | --- |
| **快速定位错误** | `grep` | `grep "OutOfMemory" java.log` |
| **批量修改配置** | `sed` | `sed -i 's/enabled=false/enabled=true/g' *.conf` |
| **报表数据统计** | `awk` | `awk '{sum += $3} END {print sum}' data.txt` |
| **提取特定字段** | `cut` | `cut -d ',' -f2-4 input.csv` |

---

#### **避坑指南**

1. **备份原文件**
   ：使用
   `sed -i`
   或批量操作前，先备份：

   ```
   cp important.txt important.txt.bak
   ```
2. **正则表达式测试**
   ：复杂替换先用
   `echo`
   测试：

   ```
   echo "hello 123" | sed 's/[0-9]\+/NUMBERS/'
   ```
3. **处理空格/特殊字符**
   ：用引号包裹变量和路径：

   ```
   grep "special file" "my document.txt"
   ```

---

#### **一句话总结**

* **`grep`**
  ：文字扫描仪，专找关键词
* **`sed`**
  ：替换机器人，批量改内容
* **`awk`**
  ：数据分拣机，表格处理专家
* **`cut`**
  ：精准剪刀手，按列裁剪文本

掌握这些工具，你就能像工厂流水线一样高效处理海量文本！ 📊🔧

我们可以用
**“厨房的水槽和漏水报警器”**
的比喻来理解 Linux 的
`stdout`
和
`stderr`
，让你像管理厨房流水一样控制程序的输出：

---

#### **1. 核心概念**

| 名称 | 比喻 | 作用 | 默认流向 |
| --- | --- | --- | --- |
| **stdout** | **正常排水管** | 输出程序正常结果 | 终端屏幕 |
| **stderr** | **漏水报警器** | 输出错误和警告信息 | 终端屏幕 |

---

#### **2. 工作场景**

假设你正在用
**“智能电饭煲”**
（程序）做饭：

* **正常流程（stdout）**
  ：

  ```
  电饭煲 >> 煮饭进度：50%... 100%... 饭已煮好！
  ```

  → 这些信息通过
  **排水管**
  （stdout）流到屏幕。
* **异常情况（stderr）**
  ：

  ```
  电饭煲 !! 警报：水量不足！  
  电饭煲 !! 错误：未检测到内胆！
  ```

  → 这些信息通过
  **报警器**
  （stderr）单独报告到屏幕。

---

#### **3. 重定向操作**

##### **(1) 分开收集正常排水和漏水**

```
# 把正常进度存到日志，错误警报存到错误文件
电饭煲 > 煮饭日志.txt 2> 错误记录.txt
```

* **结果**
  ：

  + `煮饭日志.txt`
    ：包含“煮饭进度：50%...”
  + `错误记录.txt`
    ：包含“水量不足！”

##### **(2) 合并排水管和报警器**

```
# 所有信息（正常+错误）都导入同一个文件
电饭煲 &> 完整记录.txt
```

* **适用场景**
  ：需要完整记录所有输出时。

##### **(3) 静默处理**

```
# 丢弃所有正常输出和错误（类似关闭排水和报警）
电饭煲 > /dev/null 2>&1
```

* **比喻**
  ：维修时不关心正常流程，也不处理警报。

---

#### **4. 实际命令示例**

##### **查找文件时分离结果和错误**

```
# 正常结果存 found.txt，错误信息存 errors.log
find / -name "*.conf" > found.txt 2> errors.log
```

#### **1. `find / -name "*.conf"`**

* `find`
  ：Linux 命令，用于在指定路径下查找文件和目录。
* `/`
  ：根目录，表示从整个系统的根目录开始搜索（即搜索所有目录）。
* `-name "*.conf"`
  ：
  + `-name`
    选项表示按文件名搜索。
  + `"*.conf"`
    表示匹配
    **所有以
    `.conf`
    结尾的文件**
    （
    `*`
    代表通配符，匹配任意字符）。

👉
**这一部分的作用**
：在整个系统中查找所有
`.conf`
配置文件。

---

#### **2. `> found.txt`**

* `>`
  ：
  **标准输出（stdout）重定向符**
  ，表示将
  **正常的输出**
  写入指定的文件。
* `found.txt`
  ：要保存搜索结果的文件。

👉
**这一部分的作用**
：将
`find`
命令成功找到的
`.conf`
文件路径
**保存到
`found.txt`**
，不会显示在终端上。

**示例**
： 假设系统中有
`/etc/nginx/nginx.conf`
和
`/home/user/app.conf`
，那么
`found.txt`
的内容可能是：

```
/etc/nginx/nginx.conf /home/user/app.conf
```

---

#### **3. `2> errors.log`**

* `2>`
  ：
  **标准错误（stderr）重定向符**
  ，表示将
  **错误信息**
  写入指定的文件。
  + `2`
    代表
    **标准错误（stderr）**
    ，
    `>`
    代表
    **重定向**
    。
* `errors.log`
  ：用于存储
  `find`
  命令的错误信息。

👉
**这一部分的作用**
：如果
`find`
命令在搜索过程中遇到权限不足等错误，这些错误信息
**不会显示在终端，而是写入
`errors.log`**
。

**示例**
： 如果某些目录无法访问（比如
`/root/`
需要超级用户权限），那么
`errors.log`
的内容可能是：

```
find: ‘/root’: Permission denied find: ‘/var/log/private’: Permission denied
```

---

### **最终效果**

* **成功找到的
  `.conf`
  文件**
  的路径保存在
  `found.txt`
  。
* **权限错误或无法访问的目录**
  的错误信息保存在
  `errors.log`
  。
* 终端不会显示任何信息。

---

### **扩展知识**

| **符号** | **作用** |
| --- | --- |
| `>` | **重定向 stdout** （标准输出）到文件（覆盖原文件） |
| `>>` | **重定向 stdout** （标准输出）到文件（ **追加** 到文件末尾） |
| `2>` | **重定向 stderr** （标准错误）到文件（覆盖原文件） |
| `2>>` | **重定向 stderr** （标准错误）到文件（ **追加** 到文件末尾） |
| `&>` | **同时重定向 stdout 和 stderr** 到文件（覆盖原文件） |
| `&>>` | **同时重定向 stdout 和 stderr** 到文件（ **追加** 到文件末尾） |

---

### **更高级用法**

如果想要
**同时**
把输出和错误存入
**同一个**
文件：

```
find / -name "*.conf" &> output.log
```

或者：

```
find / -name "*.conf" > output.log 2>&1
```

这样
`output.log`
里
**既有正常的结果，也有错误信息**
。

---

#### **总结**

* `find / -name "*.conf"`
  ：查找所有
  `.conf`
  文件。
* `> found.txt`
  ：将搜索到的结果保存到
  `found.txt`
  。
* `2> errors.log`
  ：将错误信息保存到
  `errors.log`
  ，而不是显示在终端。

你可以试着运行这个命令，并查看
`found.txt`
和
`errors.log`
的内容，看看它们是否符合你的预期！🚀

##### **调试脚本时合并输出**

```
# 所有信息输出到终端并保存到日志
./我的脚本.sh 2>&1 | tee debug.log
```

---

#### **为什么需要区分两者？**

1. **精准排错**
   ：快速定位错误信息，不受正常输出干扰。
2. **灵活处理**
   ：正常日志可长期保存，错误信息单独报警。
3. **用户体验**
   ：在终端中，错误信息通常用红色显示，更醒目。

---

#### **常见问题解答**

##### **Q1：为什么错误信息不能用 `>` 捕获？**

* **答**
  ：
  `>`
  默认只处理 stdout，而错误信息走 stderr 通道，需用
  `2>`
  。

##### **Q2： `2>&1` 是什么意思？**

* **答**
  ：将 stderr（错误）合并到 stdout（正常输出）的流向，实现统一处理。

##### **Q3：如何同时显示并保存输出？**

* **答**
  ：用
  `tee`
  命令（类似分流水管）：

  ```
  ./程序.sh 2>&1 | tee 完整日志.txt  # 屏幕显示+文件保存
  总结
  ```

---

#### **总结**

* **stdout**
  ：程序正常排水管 → 用
  `>`
  或
  `>>`
  管理
* **stderr**
  ：程序漏水报警器 → 用
  `2>`
  或
  `2>>`
  管理
* **合并控制**
  ：用
  `&>`
  或
  `2>&1`
  统一处理

像管理厨房水流一样掌控程序输出，让调试和日志管理更高效！ 🚰🚨

##### 

我们可以用
**“精准剪刀手”**
的比喻来理解
**`cut`**
命令，让你像裁剪布料一样切割文本数据，只保留需要的部分：

---

#### **1. 核心功能 —— 按需裁剪**

| 场景 | 比喻 | `cut` 的作用 |
| --- | --- | --- |
| **提取列** | 从布料上剪下第二列花纹 | 从 CSV 文件中提取第二列数据 |
| **截取字符** | 剪下布料的前5厘米 | 截取每行前5个字符 |
| **分隔字段** | 按布料上的标记线裁剪 | 按逗号/冒号等符号分割字段 |

---

#### **2. 常用操作演示**

##### **场景1：按分隔符提取列（如CSV）**

```
# 原始数据：姓名,年龄,城市
echo "张三,30,北京
李四,25,上海" > 用户信息.csv

# 剪下第二列（年龄）
cut -d ',' -f2 用户信息.csv
# 输出：
# 30
# 25
```

* **参数解释**
  ：
    
  `-d ','`
  → 按逗号分列（Delimiter）
    
  `-f2`
  → 取第2列（Field）

---

##### **场景2：截取固定位置字符**

```
# 原始数据：固定宽度日志
echo "2023-08-25 10:00 ERROR: Disk full
2023-08-25 10:05 INFO: Backup completed" > system.log

# 剪下每行的前10个字符（日期）
cut -c1-10 system.log
# 输出：
# 2023-08-25
# 2023-08-25
```

* **参数解释**
  ：
    
  `-c1-10`
  → 截取第1到第10个字符（Character）

---

##### **场景3：提取多个字段**

```
# 提取第1列和第3列（姓名和城市）
cut -d ',' -f1,3 用户信息.csv
# 输出：
# 张三,北京
# 李四,上海
```

---

#### **3. 组合使用技巧**

##### **分析系统用户**

```
# 提取 /etc/passwd 中的用户名（第1列，冒号分隔）
cut -d ':' -f1 /etc/passwd
# 输出示例：
# root
# bin
# daemon
```

##### **处理命令输出**

```
# 查看当前目录文件的权限（ls -l 的第1列）
ls -l | cut -d ' ' -f1
# 输出示例：
# drwxr-xr-x
# -rw-r--r--
```

---

#### **避坑指南**

1. **分隔符匹配**
   ：
     
   确保
   `-d`
   指定的分隔符与数据一致（如 CSV 用逗号，日志用空格）。
2. **字段编号从1开始**
   ：
     
   `-f1`
   是第一列，不是第0列。
3. **处理空格问题**
   ：
     
   多个连续空格可能被识别为一个分隔符，建议用
   `tr`
   命令压缩空格：

   ```
   ls -l | tr -s ' ' | cut -d ' ' -f3  # 提取文件所属用户
   ```

---

#### **一句话总结**

`cut`
就像
**“文本剪刀手”**
—— 按列裁剪、按符号分割、按位置截取，轻松提取结构化数据！ ✂️📊

我们可以用
**“并排粘合纸张”**
的比喻来理解
**`paste`**
命令，让你像整理表格一样轻松合并文件内容：

---

#### **1. `paste` 的核心功能**

| 场景 | 比喻 | `paste` 的作用 |
| --- | --- | --- |
| **合并两列数据** | 将两页纸并排粘贴成表格 | 将两个文件的内容按列合并 |
| **自定义分隔符** | 用不同胶水分隔列 | 用指定符号（如逗号、竖线）分隔 |
| **处理不等长文件** | 空位补白纸 | 行数不同时，空白位置留空 |

---

#### **2. 基础用法演示**

##### **场景1：简单合并两文件**

```
# 文件1：姓名
cat names.txt
张三
李四
王五

# 文件2：电话号码
cat phones.txt
13800138000
13900139000

# 并排粘贴（默认用制表符分隔）
paste names.txt phones.txt
张三    13800138000
李四    13900139000
王五                    # 电话号码文件少一行，第三行留空
```

##### **场景2：指定分隔符**

```
# 用逗号分隔列（适合生成CSV）
paste -d ',' names.txt phones.txt
张三,13800138000
李四,13900139000
王五,                  # 空数据保留逗号
```

---

#### **3. 高级技巧**

##### **合并多个文件**

```
# 合并三个文件（姓名、电话、邮箱）
paste -d '|' names.txt phones.txt emails.txt
张三|13800138000|zhangsan@example.com
李四|13900139000|lisi@example.com
王五||wangwu@example.com    # 电话为空
```

##### **与 `cut` 命令配合使用**

```
# 提取用户名并合并登录时间
cut -d ':' -f1 /etc/passwd > users.txt
date "+%Y-%m-%d" > date.txt
paste -d ' ' users.txt date.txt
root 2024-05-30
bin 2024-05-30
daemon 2024-05-30
...
```

### **第一行：提取用户名**

```
cut -d ':' -f1 /etc/passwd > users.txt
```

**解析：**

* `cut`
  ：Linux 里的
  **文本切割命令**
  ，用于按列提取数据。
* `-d ':'`
  ：
  **指定分隔符**
  （delimiter）为
  `:`
  ，即数据是用
  `:`
  分隔的。
* `-f1`
  ：
  **提取第 1 列**
  （即用户名）。
* `/etc/passwd`
  ：
  **Linux 系统的用户信息文件**
  ，存储着用户账户信息，每一行表示一个用户，格式如下：

  ```
  root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/usr/sbin/nologin daemon:x:2:2:daemon:/sbin:/usr/sbin/nologin
  ```

  其中，
  `:`
  分隔的第一列是用户名：
  + ```
    root
    bin
    daemon
    ...
    ```

**输出
`users.txt`
文件内容（示例）：**

```
root bin daemon ...
```

👉
**这一行的作用**
：从
`/etc/passwd`
提取
**所有用户名**
，并存入
`users.txt`
。

---

### **第二行：获取当前日期**

```
date "+%Y-%m-%d" > date.txt
```

**解析：**

* `date`
  ：Linux 命令，
  **显示当前系统时间**
  。
* `"+%Y-%m-%d"`
  ：
  + `%Y`
    ：4 位数的年份（如
    `2024`
    ）。
  + `%m`
    ：2 位数的月份（如
    `03`
    ）。
  + `%d`
    ：2 位数的日期（如
    `08`
    ）。
* `> date.txt`
  ：
  + `>`
    表示
    **重定向**
    ，即把
    `date`
    命令的
    **输出保存到
    `date.txt`
    文件**
    。

**示例：**
如果今天是
**2024 年 5 月 30 日**
，
`date.txt`
的内容就是：

```
2024-05-30
```

👉
**这一行的作用**
：获取
**今天的日期**
，并存入
`date.txt`
。

---

### **第三行：合并用户名和日期**

```
paste -d ' ' users.txt date.txt
```

**解析：**

* `paste`
  ：Linux 命令，用于
  **按列合并文件**
  。
* `-d ' '`
  ：
  + `-d`
    选项用于
    **指定列之间的分隔符**
    。
  + `' '`
    代表
    **用空格作为分隔符**
    。
* `users.txt date.txt`
  ：
  + 把
    `users.txt`
    和
    `date.txt`
    按行合并。
  + `users.txt`
    的每一行（用户名）和
    `date.txt`
    的每一行（日期）合并。

**示例：**
假设：

* `users.txt`
  :

  ```
  root bin daemon
  ```
* `date.txt`
  :

  ```
  2024-05-30 2024-05-30 2024-05-30
  ```

执行
`paste -d ' ' users.txt date.txt`
后的输出：

```
root 2024-05-30 bin 2024-05-30 daemon 2024-05-30 ...
```

👉
**这一行的作用**
：
**把用户名和当前日期拼接在一起**
，形成
**用户登录记录格式**
。

---

#### **4. 注意事项**

1. **行数不一致**
   ：
     
   若文件行数不同，
   `paste`
   会保留所有行，缺失数据的位置留空。
2. **默认分隔符**
   ：
     
   默认用
   **制表符（Tab）**
   分隔列，可用
   `-d`
   指定其他符号（如
   `,`
   `:`
   `|`
   ）。
3. **输入来源**
   ：
     
   可合并文件，也可直接处理命令输出（通过管道）：

   ```
   ls | paste -d ',' - - -  # 将文件列表按3列排版
   file1.txt,file2.txt,file3.txt
   image.jpg,document.pdf,music.mp3
   ```

---

#### **一句话总结**

`paste`
就像
**“数据双面胶”**
—— 轻松将多个文件或文本流并排粘合，快速生成表格化数据！ 📄📄➡️📊

## 排序

我们可以用
**“图书管理员整理书架”**
的比喻来理解
`sort`
命令，让你像整理书籍一样轻松排序文本内容：

---

#### **1. 核心功能 —— 灵活排序**

| 场景 | 比喻 | `sort` 的作用 |
| --- | --- | --- |
| **字母排序** | 按书名首字母排列书籍 | 将文本按字典顺序排列 |
| **数字排序** | 按书籍出版年份排序 | 识别数字并按大小排序（需加 `-n` ） |
| **倒序排列** | 从Z到A排列书名 | 用 `-r` 反向排序 |
| **混合排序** | 先按分类再按年份排序 | 多条件排序（如 `-k2,2n` 按第二列数字排序） |

---

#### **2. 基础用法演示**

##### **场景1：简单整理单词表**

```
# 原始文件 words.txt
cat words.txt
apple
Banana
cherry
date

# 默认按字母排序（注意大写优先）
sort words.txt
Banana
apple
cherry
date
```

##### **场景2：处理数字（如成绩单）**

```
# 原始文件 scores.txt
cat scores.txt
张三 85
李四 92
王五 78

# 按分数从高到低排序（-n数字排序，-r倒序）
sort -k2 -nr scores.txt
李四 92
张三 85
王五 78
```

---

#### **3. 高级技巧**

##### **去重整理（类似剔除重复书籍）**

```
# 文件中有重复行
cat duplicates.txt
苹果
香蕉
苹果
橘子

# 排序并去重（-u 选项）
sort -u duplicates.txt
香蕉
橘子
苹果
```

##### **按月份排序（整理月度报告）**

```
# 原始文件 months.txt
cat months.txt
March
January
December
July

# 按月份顺序排序（-M 识别月份缩写）
sort -M months.txt
January
March
July
December
```

---

#### **4. 保存整理结果**

##### **重定向到新文件（不修改原文件）**

```
# 整理书籍列表并另存为新文件
sort 杂乱书单.txt > 整齐书单.txt
```

##### **直接覆盖原文件**

```
# 整理后替换原文件（-o 选项更安全）
sort -o 原文件.txt 原文件.txt
```

---

#### **避坑指南**

1. **数字排序陷阱**
   ：
     
   默认按字符排序（如100会排在2前面），
   **必须用
   `-n`
   正确排序数字**
   ：

   ```
   sort -n numbers.txt  # 正确：1,2,10,100
   ```
2. **跨列排序**
   ：
     
   用
   `-k`
   指定列（如
   `-k3`
   按第三列排序），结合
   `-t`
   指定分隔符：

   ```
   # 按CSV文件第三列数字排序
   sort -t ',' -k3n data.csv
   #按数值排序，而不是按字典序。

   ```
3. **处理中文排序**
   ：
     
   默认按编码排序可能不直观，可设置本地化：

   ```
   LC_ALL=C sort file.txt  # 按字节排序（快速）
   LC_ALL=zh_CN.UTF-8 sort file.txt  # 按中文拼音排序（需系统支持）
   ```

---

#### **一句话总结**

`sort`
就像
**“智能图书管理员”**
—— 按字母、数字、月份灵活排序，还能去重分列，让杂乱数据瞬间井然有序！ 📚🔢

## **tr-命令**

Linux 中的命令是用于翻译或替换字符的命令行实用程序。它从标准输入读取并写入标准输出。虽然通常用于翻译应用程序，但在 Linux 的文本处理方面具有多种功能。从替换字符列表到删除或压缩字符重复，为基于流的文本作提供了强大的工具。
`tr`
`tr`
`tr`

下面是一个基本的用法示例：

```astro-code
echo 'hello' | tr 'a-z' 'A-Z'
```

在此示例中，用于将小写 'hello' 转换为大写 'HELLO'。它是 Linux 环境中文本处理任务的必备工具。
`tr`

`tr`
是 Linux 中一个简单但强大的文本处理工具，它的核心功能是
**对字符进行替换、删除或压缩**
。你可以把它想象成一个“字符魔术师”——你告诉它要改什么、删什么，它就能快速处理文本中的字符。

---

#### **通俗解释**

假设你有一段文字，比如
`hello`
，想对它做一些修改：

1. **替换字符**
   ：把小写字母全部变成大写（
   `hello`
   →
   `HELLO`
   ）。
2. **删除字符**
   ：删掉所有数字（
   `h3ll0`
   →
   `hll`
   ）。
3. **压缩重复字符**
   ：把连续的空格变成单个空格（
   `Hello   World`
   →
   `Hello World`
   ）。

这些操作都可以用
`tr`
一行命令搞定！

---

#### **具体功能 & 用法**

##### 1️⃣ **替换字符**

* **语法**
  ：
  `tr '原字符' '新字符'`
* **例子**
  ：

```
echo "hello" | tr 'a-z' 'A-Z'  # 小写转大写 → HELLO
echo "abc" | tr 'abc' '123'    # 替换字符 → 123
```

##### 

* **语法**
  ：
  `tr -d '要删除的字符'`
* **例子**
  ：

  ```
  echo "h3ll0" | tr -d '0-9'  # 删除所有数字 → hll
  echo "Hello!" | tr -d '!'    # 删除感叹号 → Hello
  ```

##### 3️⃣ **压缩重复字符**

* **语法**
  ：
  `tr -s '要压缩的字符'`
* **例子**
  ：

  ```
  echo "Hello   World" | tr -s ' '  # 压缩连续空格 → Hello World
  echo "woooow" | tr -s 'o'        # 压缩重复的 o → wow
  ```

---

#### **日常场景类比**

* **替换**
  ：就像把一篇英文文章里的所有逗号换成句号。
* **删除**
  ：像用橡皮擦掉文本中所有数字。
* **压缩**
  ：把“我我我我要要要吃饭”压缩成“我要吃饭”。

---

#### **小贴士**

* `tr`
  **只能处理字符**
  ，不能直接操作文件，所以通常结合管道符
  `|`
  或输入重定向使用。
* **支持范围**
  ：比如
  `a-z`
  表示所有小写字母，
  `0-9`
  表示所有数字。
* **经典用途**
  ：处理日志、格式化文本、数据清洗等。

试试这个命令，感受它的便捷吧！

```
echo "Test 123 Text" | tr -d '0-9' | tr 'a-z' 'A-Z'
# 输出：TEST  TEXT（删掉数字并转大写）
```

## **head 命令**

Linux 中的命令是一个文本处理实用程序，允许用户输出文件的第一部分（或“头”）。它通常用于预览文件的开头，而无需将整个文档加载到内存中，这可以作为快速检查非常大文件中的数据的有效方法。默认情况下，该命令将每个文件的前 10 行打印到标准输出，这是大多数系统中的终端。
`head`
`head`

```astro-code
head file.txt
```

可以使用选项自定义输出行数。例如，要显示前 5 行，我们使用 option 后跟行数：
`-n`

```astro-code
head -n 5 file.txt
```

我们可以用
**“快速翻阅书的前几页”**
的比喻来理解
`head`
命令，让你像浏览书籍目录一样快速预览文件内容：

---

#### **1. 核心功能 —— 预览文件开头**

| 场景 | 比喻 | `head` 的作用 |
| --- | --- | --- |
| **日志预览** | 只看日记的前几篇 | 查看日志文件的前10行（默认） |
| **数据检查** | 快速扫一眼表格的标题行 | 确认CSV文件的列名和格式 |
| **大文件处理** | 避免打开巨型百科全书 | 仅加载文件开头，节省内存和时间 |

---

#### **2. 基础用法演示**

##### **默认查看前10行**

```
# 预览系统日志的前10行（像翻开书的前10页）
head /var/log/syslog
```

##### **自定义行数（比如看前3行）**

```
# 只看“用户名单.csv”的前3行（标题+2条数据）
head -n 3 用户名单.csv
姓名,年龄,城市
张三,30,北京
李四,25,上海
```

---

#### **3. 高级技巧**

##### **结合其他命令（管道操作）**

```
# 列出当前目录文件，取前5个结果
ls -l | head -n5
总用量 16
-rw-r--r-- 1 user user  120 5月 30 10:00 笔记.txt
-rw-r--r-- 1 user user 1024 5月 30 09:30 报告.pdf
drwxr-xr-x 2 user user 4096 5月 29 15:00 图片
快速检查脚本开头
```

##### **快速检查脚本开头**

```
# 查看脚本的前2行（确认解释器路径）
head -n2 自动备份.sh
#!/bin/bash
# 这是一个自动备份脚本
```

---

#### **4. 注意事项**

1. **只读不写**
   ：
   `head`
   仅用于查看，
   **不会修改原文件**
   。
2. **处理二进制文件慎用**
   ：预览二进制文件（如图片、压缩包）会显示乱码。
3. **负数行数（扩展功能）**
   ：
     
   某些系统支持
   `-n -5`
   表示“显示除最后5行外的所有内容”（但非所有Linux版本通用）。

---

#### **对比 `tail` 命令**

* **`head`**
  ：关注开头（书的封面和目录）
* **`tail`**
  ：关注结尾（书的结局和附录）
* **组合使用**
  ：

  ```
  # 查看文件的第11-20行（先取前20行，再取后10行）
  head -n20 大文件.txt | tail -n10
  ```

---

#### **一句话总结**

`head`
就像
**“快速翻书助手”**
—— 一键查看文件开头，避免淹没在数据海洋中，帮你高效锁定关键信息！ 📖👀

## **Tail 命令**

Linux 中的命令是用于文本处理的实用程序。从根本上说，它用于输出文件的最后一部分。该命令从标准输入或文件中读取数据，并将最后的字节、行、块、字符或字输出到标准输出（或其他文件）。默认情况下，将每个文件的最后 10 行返回到标准输出。当用户对文本文件中的最新条目（如日志文件）感兴趣时，此命令很常见。
`tail`
`N`
`tail`

以下是 tail 命令的使用示例：

```astro-code
tail /var/log/syslog
```

在上面的示例中，该命令将打印文件的最后 10 行。这在检查最新的系统日志条目时特别有用。
`tail`
`/var/log/syslog`

`tail`
是 Linux 中一个非常实用的命令，它的核心功能是
**快速查看文件末尾的内容**
。你可以把它想象成“直接翻到书的最后几页”——不用从头开始读，直接看最新添加的部分。特别适合用来
**实时监控日志、检查文件更新**
等场景。

---

#### **通俗解释**

假设你有一个不断更新的日记（比如系统日志），里面记录了最近发生的事情。但日记太长了，你只关心
**今天最后写了什么**
。这时候：

* `tail`
  会直接显示日记的最后几行（默认显示最后10行）。
* 如果日记还在实时更新（比如每秒新增一行），
  `tail`
  甚至可以像“监控摄像头”一样，持续显示新内容。

---

#### **基本功能 & 经典用法**

##### 1️⃣ **查看文件的最后几行**

* **默认行为**
  ：显示文件最后10行。

  ```
  tail /var/log/syslog  # 查看系统日志的最后10行
  ```

##### 2️⃣ **指定显示的行数**

* 用
  `-n`
  参数自定义行数（比如最后20行）：

  ```
  tail -n 20 /var/log/syslog  # 显示最后20行
  ```

##### 3️⃣ **实时监控文件更新（日志追踪）**

* 用
  `-f`
  参数实时“跟随”文件变化（按
  `Ctrl+C`
  退出）：

  ```
  tail -f /var/log/syslog  # 实时显示日志的新内容
  ```

##### 4️⃣ **同时监控多个文件**

* 同时跟踪多个文件的更新（显示时会标注文件名）：

  ```
  tail -f /var/log/syslog /var/log/auth.log  # 监控系统日志和登录日志
  ```

---

#### **日常场景类比**

* **查日志**
  ：像查看微信聊天记录的“最新几条消息”。
* **实时监控**
  ：像在直播间里，不断滚动显示观众的新评论。
* **指定行数**
  ：比如老师让你“背诵课文最后3句”，而不是整篇课文。

---

#### **常用场景示例**

1. **检查最近的服务错误**
   ：

   ```
   tail -n 50 /var/log/nginx/error.log  # 看最近50条Nginx错误日志
   ```
2. **实时调试程序输出**
   ：

   ```
   tail -f /path/to/your/app.log  # 实时观察程序打印的日志
   ```
3. **快速验证文件是否更新**
   ：

   ```
   tail -n 1 data.csv  # 查看CSV文件的最后一行，确认新数据是否写入
   ```

---

#### **小贴士**

* 如果文件需要权限（如系统日志），可能需要用
  `sudo`
  ：

  ```
  sudo tail /var/log/syslog
  ```
* `-F`
  和
  `-f`
  的区别：
  `-F`
  更强大，即使文件被删除后重新创建（如日志轮转），它也能继续追踪。
* 组合其他命令使用（比如
  `grep`
  ）：

  ```
  tail -f /var/log/syslog | grep "error"  # 实时过滤出包含“error”的日志
  ```

## **Linux 上文本处理中的 join 命令**

`join`
是 Linux 中一个强大的文本处理命令。它允许你将两个文件的行合并到一个公共字段上，其工作方式类似于 SQL 中的 'Join'作。当您处理大量数据时，它特别有用。具体来说，使用两个文件中的行来形成包含以有意义的方式相关的行对的行。
`join`

例如，如果您有两个文件，其中包含一个项目列表，一个包含成本，另一个包含数量，则可以使用合并这两个文件，以便每个项目在同一行上具有成本和数量。
`join`

```astro-code
# Syntax
join file1.txt file2.txt
```

请注意，只有当文件排序时，命令才能正常工作。 了解提供的所有选项和标志以在文本处理任务中有效使用至关重要。
`join`
`join`

`join`
是 Linux 中一个用于
**合并两个文件内容**
的命令，它的功能类似于 Excel 的
`VLOOKUP`
或 SQL 中的
`JOIN`
操作。简单来说，就是根据两个文件中
**共有的某一列**
（比如ID、名称等），把两个文件的内容“拼接到一起”。

---

#### **通俗解释**

假设你有两个表格：

1. **文件1**
   ：记录商品名称和价格

   ```
   苹果 5元
   香蕉 3元
   橘子 4元
   ```
2. **文件2**
   ：记录商品名称和库存

   ```
   苹果 100个
   香蕉 200个
   橘子 150个
   ```

你想把这两个文件合并成一个完整的表格：

```
苹果 5元 100个
香蕉 3元 200个
橘子 4元 150个
```

这就是
`join`
的作用——根据共同的字段（这里是“商品名称”），将两个文件的数据合并到一行。

---

#### **核心功能 & 用法**

##### 1️⃣ **基本合并**

* **语法**
  ：
  `join 文件1 文件2`
* **要求**
  ：两个文件必须
  **按共同列排序**
  （默认以第一列为共同列）。
* **示例**
  ：

  ```
  join prices.txt stock.txt
  # 输出：
  # 苹果 5元 100个
  # 香蕉 3元 200个
  # 橘子 4元 150个
  ```

##### 2️⃣ **指定共同列**

* 如果共同列不是第一列，可以用
  `-1`
  和
  `-2`
  指定：

  ```
  # 文件1的共同列是第2列，文件2的共同列是第3列
  join -1 2 -2 3 file1.txt file2.txt
  ```

##### 3️⃣ **处理未匹配的行**

* 默认只显示匹配成功的行，如果想保留未匹配的行：

  ```
  join -a 1 file1.txt file2.txt  # 保留文件1中未匹配的行
  join -a 2 file1.txt file2.txt  # 保留文件2中未匹配的行
  join -a 1 -a 2 file1.txt file2.txt  # 保留所有未匹配的行
  ```

##### 4️⃣ **自定义分隔符**

* 如果文件不是用空格分隔（比如用逗号），用
  `-t`
  指定：

  ```
  join -t ',' file1.csv file2.csv
  ```

---

#### **日常场景类比**

* **合并成绩单**
  ：把“学生姓名+数学成绩”和“学生姓名+语文成绩”合并成一张总表。
* **数据关联**
  ：像网购时，把“订单号+商品ID”和“商品ID+商品名称”关联起来，显示完整信息。

---

#### **常用场景示例**

1. **合并日志文件**
   ：

   ```
   join server1.log server2.log  # 按时间戳合并两台服务器的日志
   ```
2. **关联用户信息**
   ：

   ```
   # 文件1：user_id 姓名
   # 文件2：user_id 邮箱
   join users.txt emails.txt > user_details.txt
   ```
3. **处理CSV文件**
   ：

   ```
   join -t ',' -1 1 -2 1 data1.csv data2.csv  # 合并两个CSV文件的第一列

   ```

---

#### **注意事项**

1. **必须提前排序**
   ：
     
   `join`
   要求两个文件按共同列排序，否则会失败！可以用
   `sort`
   命令先排序：

   ```
   sort file1.txt > file1_sorted.txt
   sort file2.txt > file2_sorted.txt
   join file1_sorted.txt file2_sorted.txt
   ```
2. **默认行为**
   ：

   * 共同列默认是第一列。
   * 默认输出格式：共同列 + 文件1的剩余列 + 文件2的剩余列。
3. **局限性**
   ：

   * 如果两个文件有重复的共同列，结果可能不符合预期。
   * 复杂合并需求（如多列关联）可能需要结合
     `awk`
     或
     `paste`
     。

---

#### **小贴士**

* 如果不确定文件是否排序，先用
  `sort`
  处理。
* 用
  `-v 1`
  或
  `-v 2`
  可以
  **只显示未匹配的行**
  （反向检查）。
* 结合
  `cut`
  、
  `awk`
  等命令，可以进一步处理合并后的输出。

试试这个完整流程：

```
# 准备文件
echo -e "苹果 5元\n香蕉 3元\n橘子 4元" > prices.txt
echo -e "苹果 100个\n香蕉 200个\n橘子 150个" > stock.txt

# 排序文件（如果未排序）
sort prices.txt > prices_sorted.txt
sort stock.txt > stock_sorted.txt

# 合并文件
join prices_sorted.txt stock_sorted.txt
# 输出：
# 苹果 5元 100个
# 香蕉 3元 200个
# 橘子 4元 150个
```

## **Linux 文本处理：Split 命令**

Linux 提供了一组广泛的工具来作文本数据。顾名思义，其中一种实用程序是用于将大文件拆分为较小文件的命令。Linux 中的命令根据用户指定的行或字节将文件分成多个相等的部分。
`split`
`split`

这是一个有用的命令，因为它具有实际适用性。例如，如果您有一个大型数据文件，由于其大小而无法有效使用，则可以使用 split 命令将文件分解为更易于管理的部分。

该命令的基本语法是：
`split`

```astro-code
split [options] [input [prefix]]
```

默认情况下，该命令将文件划分为多个较小的文件，每个文件 1000 行。如果未提供 input 文件，或者如果它作为 - 给出，则它会从标准输入中读取。
`split`

例如，要将名为 'bigfile.txt' 的文件拆分为每个 500 行的文件，命令为：

```astro-code
split -l 500 bigfile.txt 
```

`split`
是 Linux 中一个非常实用的文件分割工具，它的作用就像
**把一本厚书拆分成多个小册子**
。当你有一个超大文件（比如日志、数据集），直接打开或传输很不方便时，
`split`
能快速将它切成多个小文件，每个小文件按顺序编号，方便后续处理。

---

#### **通俗解释**

假设你有一个 3000 页的小说（文件），想分成 3 本 1000 页的小册子：

* `split`
  会自动将文件按指定大小（比如每 1000 行或 100MB）切割。
* 切割后的文件会按顺序命名为
  `xaa`
  、
  `xab`
  、
  `xac`
  （类似“分册1、分册2、分册3”）。

---

#### **核心功能 & 用法**

##### 1️⃣ **按行数分割**

* **语法**
  ：
  `split -l 行数 大文件 输出文件名前缀`
* **示例**
  ：

  ```
  split -l 500 bigfile.txt  # 将 bigfile.txt 按每500行切割，生成 xaa、xab...
  split -l 1000 access.log log_part_  # 自定义前缀为 log_part_，生成 log_part_aa、log_part_ab...
  ```

##### 2️⃣ **按文件大小分割**

* **语法**
  ：
  `split -b 大小 大文件 输出文件名前缀`
* **示例**
  ：

  ```
  split -b 100M video.mp4  # 将视频文件按每100MB切割
  split -b 1G data.tar.gz  # 按每1GB切割大压缩包
  ```

##### 3️⃣ **合并分割后的文件**

切割后的文件可以随时用
`cat`
合并还原：

```
cat xaa xab xac > original_bigfile.txt  # 按顺序拼接所有分块

```

---

#### **日常场景类比**

* **分割日志**
  ：像把一年的日记按月份拆分成12个文件。
* **传输大文件**
  ：像把一部电影分成多个小文件，方便用U盘分次拷贝。
* **并行处理**
  ：像把任务清单拆成多份，分给不同人同时处理。

---

#### **常用场景示例**

1. **分割日志文件**
   ：

   ```
   split -l 5000 server.log  # 将日志按每5000行切割，用于分批分析
   ```
2. **切割大压缩包**
   （方便上传下载）：

   ```
   split -b 500M huge_backup.zip  # 按500MB切割，生成 xaa、xab...
   ```
3. **处理CSV数据集**
   ：

   ```
   split -l 10000 data.csv data_part_  # 按每1万行切割，生成 data_part_aa、data_part_ab...
   ```

---

#### **注意事项**

1. **默认行为**
   ：

   * 不指定前缀时，输出文件名为
     `xaa`
     、
     `xab`
     、
     `xac`
     ...（按字母顺序）。
   * 默认按
     **行数**
     切割（每1000行），或按
     **字节数**
     切割（如果用了
     `-b`
     ）。
2. **按大小分割时的单位**
   ：

   * `-b 100K`
     表示 100KB，
     `-b 1M`
     表示 1MB，
     `-b 2G`
     表示 2GB。
3. **二进制文件 vs 文本文件**
   ：

   * 切割文本文件建议用
     `-l`
     （按行），避免切断一行内容。
   * 切割二进制文件（如图片、压缩包）必须用
     `-b`
     （按字节）。

---

#### **小贴士**

* 用
  `--verbose`
  可以看到分割过程的实时反馈。
* 合并文件时，
  **必须按字母顺序拼接**
  （如
  `xaa xab xac`
  ），否则内容会错乱。
* 结合
  `split`
  和
  `cat`
  ，可以轻松实现“分卷压缩”功能。

试试这个完整示例：

```
# 创建一个示例文件（共6行）
echo -e "Line1\nLine2\nLine3\nLine4\nLine5\nLine6" > demo.txt

# 按每2行切割，自定义前缀为 part_
split -l 2 demo.txt part_

# 查看分割结果
ls part_*  # 输出 part_aa part_ab part_ac
cat part_aa  # 输出 Line1 Line2
cat part_ab  # 输出 Line3 Line4
cat part_ac  # 输出 Line5 Line6

# 合并还原
cat part_aa part_ab part_ac > restored.txt
```

## **管道命令**

管道 （） 是 Linux 中的一项强大功能，用于将两个或多个命令连接在一起。此机制允许将一个命令的输出作为输入“管道”到另一个命令。关于文本处理，使用 pipe 特别有用，因为它允许您作、分析和转换文本数据，而无需创建中间文件或程序。
`|`

下面是一个简单的示例，通过管道输入两个命令和 ， 以列出当前目录中的所有文本文件：
`ls`
`grep`

```astro-code
ls | grep 'txt$'
```

在此示例中，列出当前目录中的文件，并筛选掉任何不以 .管道命令 获取 的输出并将其用作 的输入 。整个命令的输出是当前目录中的文本文件列表。
`ls`
`grep 'txt$'`
`.txt`
`|`
`ls`
`grep 'txt$'`

`管道（|）`
是 Linux 中一个神奇的“连接器”，它的作用就像
**流水线上的传送带**
——把前一个命令的输出，直接传给后一个命令作为输入。这样一来，多个命令可以串联起来，像工厂流水线一样协作处理数据，
**无需手动保存中间结果**
，效率极高！

---

#### **通俗解释**

假设你想把一堆苹果加工成苹果汁：

1. **第一步**
   ：清洗苹果（对应命令
   `清洗`
   ）。
2. **第二步**
   ：榨汁（对应命令
   `榨汁`
   ）。
3. **第三步**
   ：装瓶（对应命令
   `装瓶`
   ）。

用管道符
`|`
连接这些步骤，就像把三个机器用传送带连起来：

```
清洗苹果 | 榨汁 | 装瓶
```

最终直接得到瓶装苹果汁，中间不需要手动搬运半成品！

---

#### **核心功能 & 用法**

##### 1️⃣ **基础使用**

* **语法**
  ：
  `命令A | 命令B`
* **作用**
  ：将命令A的输出，直接传给命令B处理。
* **示例**
  ：

  ```
  ls | grep '.txt$'     # 列出当前目录下的所有txt文件
  cat log.txt | wc -l   # 统计log.txt文件的行数
  ```

##### 2️⃣ **多级管道**

可以串联多个命令，形成复杂处理流程：

```
cat log.txt | grep "error" | sort | uniq -c
# 分解步骤：
# 1. 读取文件 → 2. 过滤含"error"的行 → 3. 排序 → 4. 统计重复次数
```

##### 3️⃣ **实时监控**

结合
`tail -f`
实时处理动态更新的文件（如日志）：

```
tail -f /var/log/syslog | grep "fail"
# 实时监控系统日志，只显示包含“fail”的行
```

---

#### **日常场景类比**

* **筛选文件**
  ：像用筛子过滤沙子，只保留符合条件的小颗粒。
* **数据流水线**
  ：像快递分拣系统，包裹（数据）经过扫描、分类、打包等多个环节。
* **实时报警**
  ：像监控摄像头检测到异常时，自动触发警报。

---

#### **常用场景示例**

💡
**示例**

sh

复制编辑

`find . -name "*.py"`

🌟
**可能的输出**

bash

复制编辑

`./main.py ./src/utils.py ./tests/test.py`

---

### **2️⃣ `|` （管道）**

**管道
`|`
把前一个命令的输出，作为下一个命令的输入。**

---

### **3️⃣ `xargs wc -l`**

📌
**`xargs`
作用**
：把前面
`find`
的输出（文件列表）当作参数传递给
`wc -l`
。

📌
**`wc -l`
作用**
：统计文件的
**行数**
（
`-l`
代表
`line count`
）。

💡
**示例**

sh

复制编辑

`wc -l main.py src/utils.py tests/test.py`

🌟
**可能的输出**

bash

复制编辑

`120 main.py 80 src/utils.py 60 tests/test.py 260 total`

## **🔹 可能的问题 & 解决方案**

❌
**问题：文件名包含空格或特殊字符**

---

#### **🎯 你学到了什么？**

---

## **🔹 总结**

sh

复制编辑

`find . -name "*.py" | xargs wc -l`

| 部分 | 含义 |
| --- | --- |
| `find . -name "*.py"` | 查找当前目录及子目录下所有 Python 文件 |
| ` | ` |
| `xargs wc -l` | 统计所有 Python 文件的行数 |

---

1. **查找特定进程**
   ：

   ```
   ps aux | grep nginx  # 查找所有与nginx相关的进程
   ```

   **统计代码行数**
   ：

   ```
   find . -name "*.py" | xargs wc -l  # 统计当前目录下所有Python文件的总行数
   ```

   ## **🔹 逐部分解析**

   ##### 

   ```
   find . -name "*.py"
   📌 find 命令用于查找文件，这里的参数是：
   . 👉 在 当前目录（以及所有子目录）中查找-name "*.py" 👉 文件名匹配 .py 结尾（即所有 Python 文件）

   这里 find 命令的输出是一系列 .py 文件路径，
   xargs 负责把这些路径传递给 wc -l 进行行数统计。
   120 main.py 👉 main.py 里有 120 行
   80 src/utils.py 👉 src/utils.py 里有 80 行
   60 tests/test.py 👉 tests/test.py 里有 60 行
   260 total 👉 所有 Python 文件的总行数
   问题：xargs 默认按空格分割，遇到文件名有空格时可能会出错。
   ```
2. **解决方案**
   ：用
   `find`
   的
   `-print0`
   和
   `xargs -0`
   组合，避免分割错误：
3. ```
   find . -name "*.py" -print0 | xargs -0 wc -l

   -print0 让 find 以 NULL 分隔符 输出文件名，xargs -0 让 xargs 以 NULL 读取文件名。
   ```

   ```
   find 可以递归查找文件。
   |（管道） 可以连接命令，让前一个命令的输出成为下一个命令的输入。
   xargs 用于构造命令参数。
   wc -l 统计行数，-l 代表 line count。
   find . -name "*.py" | xargs wc -l 用于 统计所有 Python 文件的行数。
   避免空格问题 用 find . -name "*.py" -print0 | xargs -0 wc -l。
   ```
4. **批量重命名文件**
   ：

   ```
   ls *.jpg | sed 's/^/mv & &_backup/' | sh  # 给所有jpg文件添加_backup后缀
   ```

#### **`sed 's/^/mv & &_backup/'`**

📌
**`sed`
（流编辑器）用于修改文本**
，这里的
`s/^/mv & &_backup/`
规则可以拆解如下：

##### **🔸 `s/^/.../`**

* `s/.../.../`
  👉
  **替换（substitute）**
* `^`
  👉
  **匹配行首**
* `&`
  👉
  **表示整个匹配到的文本**
* `mv & &_backup`
  👉
  **在每一行的前面加上
  `mv`
  ，并在后面加上
  `_backup`**

#### **`| sh`**

📌
**`sh`
让
`sed`
生成的命令执行**
。

---

#### **注意事项**

1. **管道是单向的**
   ：数据只能从左到右传递，不能逆向。
2. **只传递标准输出**
   ：如果命令A有错误输出（如
   `stderr`
   ），需要用
   `2>&1`
   重定向：

   ```
   commandA 2>&1 | commandB  # 将错误输出也传给commandB
   ```

`2>&1`
**把
`stderr`
重定向到
`stdout`**
，这样
`commandA`
的所有输出都合并成
**标准输出**
。

1. **性能优化**
   ：避免在管道中处理超大文件时内存溢出，可结合
   `xargs`
   分块处理。

---

#### **小贴士**

* 管道符
  `|`
  在键盘上的位置通常是
  `Shift+\`
  （反斜杠键）。
* 结合
  `>`
  或
  `>>`
  可以将最终结果保存到文件：

  ```
  ls | grep '.txt$' > txt_files.txt  # 将结果保存到txt_files.txt
  ```
* 管道是 Linux 哲学“
  **一个命令只做一件事，并做好**
  ”的完美体现，灵活组合小命令，能解决复杂问题！

试试这个命令，感受管道的魅力吧！

```
curl https://example.com | grep "title" | sed 's/<[^>]*>//g'
# 分解步骤：
# 1. 下载网页 → 2. 过滤出标题行 → 3. 去掉HTML标签
```

```
 sed 's/<[^>]*>//g'
📌 作用：使用 sed 删除 HTML 标签，保留纯文本。

🔍 解析 s/<[^>]*>//g

s/.../.../g：替换模式
s：表示替换
/：分隔符
<[^>]*>：匹配HTML 标签
<：匹配开头的 <
[^>]*：匹配任意字符（除 > 之外的）
>：匹配结尾的 >
//：替换为空（删除匹配内容）
g：全局替换
```



## **文本处理中的 T 形**

`tee`
是 Linux 系统中广泛使用的命令，属于文本处理工具的范畴。它执行双重功能：命令从标准输入读取并写入标准输出和文件。此作的名称来源于管道中的 T 型分流器，它将水流分成两个方向，与命令的功能平行。
`tee`

Linux 中文本处理的基本语法是：
`tee`

```astro-code
command | tee file
```

在此构造中，'command' 表示从中读取输出的命令，而 'file' 表示写入输出的文件。对于想要记录其终端业务的用户来说，这是一个非常有用的命令，因为它既可以在终端中查看结果，也可以同时将输出存储在文件中。
`tee`
`tee`

`tee`
是 Linux 中一个非常实用的“
**分流器**
”命令，它的核心功能是
**同时将数据送到两个地方**
：既在终端显示结果，又将结果保存到文件。就像现实中的“三通水管”——水流（数据）进来后，分叉流向两个方向。

---

#### **通俗解释**

假设你正在给朋友讲电话，同时想录音备份对话：

* 你的耳朵听到声音（类似终端显示输出）。
* 录音设备同时保存声音到文件（类似写入文件）。
* `tee`
  就是那个“电话录音器”，让你
  **边听边录**
  。

---

#### **核心功能 & 用法**

##### 1️⃣ **基础使用：显示并保存输出**

* **语法**
  ：
  `命令 | tee 文件名`
* **作用**
  ：将命令的输出
  **同时显示在屏幕**
  并
  **保存到文件**
  。
* **示例**
  ：

  ```
  ls -l | tee file_list.txt  # 列出文件详情，同时保存到 file_list.txt
  ping google.com | tee ping.log  # 测试网络连通性，实时查看并记录日志
  ```

##### 2️⃣ **追加内容到文件（不覆盖）**

* 默认会覆盖文件，用
  `-a`
  参数追加内容：

  ```
  echo "新内容" | tee -a log.txt  # 在 log.txt 末尾追加，而不是覆盖
  ```

##### 3️⃣ **同时处理多个管道**

* `tee`
  可以连接多个后续命令：

  ```
  ls | tee files.txt | grep '.txt'  # 保存所有文件列表到 files.txt，同时过滤出 txt 文件
  ```

##### 4️⃣ **分割输出到多个文件**

* 一次性保存到多个文件：

  ```
  echo "重要数据" | tee file1.txt file2.txt  # 同时写入 file1 和 file2
  ```

---

#### **日常场景类比**

* **备份对话**
  ：像开会时一边听讲，一边记笔记。
* **监控并记录**
  ：像工厂流水线上，摄像头同时监控画面和存储录像。
* **数据分发**
  ：像快递中心将包裹同时发往两个不同的目的地。

---

#### **常用场景示例**

1. **调试脚本并保存日志**
   ：

   ```
   ./run_script.sh | tee script.log  # 实时查看脚本输出，同时记录到文件
   ```
2. **安装软件时查看进度并保存信息**
   ：

   ```
   sudo apt install nginx | tee install.log  # 安装过程实时显示，日志保存到 install.log
   ```
3. **同时处理和分析数据**
   ：

   ```
   cat data.csv | tee raw_data.csv | awk '{print $1}' > column1.txt  # 保存原始数据，并提取第一列
   ```

---

#### **注意事项**

1. **覆盖风险**
   ：默认会覆盖目标文件，谨慎操作（必要时用
   `-a`
   追加）。
2. **权限问题**
   ：若写入受保护的文件（如系统日志），需结合
   `sudo`
   ：

   ```
   echo "配置更新" | sudo tee /etc/config  # 用 sudo 提升写入权限
   ```
3. **二进制文件慎用**
   ：
   `tee`
   适合处理文本，处理二进制文件（如图片）可能损坏数据。

---

#### **小贴士**

* 结合
  `tee`
  和
  `grep`
  可以实时过滤并保存日志：

  ```
  tail -f /var/log/syslog | tee full.log | grep "error"  # 保存完整日志，同时只显示错误信息
  ```
* 用
  `tee`
  备份管道数据：

  ```
  tar czvf - my_folder | tee backup.tar.gz | md5sum > checksum.txt  # 压缩文件夹并生成校验码
  ```
* 若想完全静默保存（不显示在终端），可以重定向到
  `/dev/null`
  ：

  ```
  ls | tee files.txt > /dev/null  # 保存到文件，但不显示在屏幕
  ```

试试这个例子，感受
`tee`
的便捷：

```
# 生成随机数，统计数量并保存原始数据
seq 1 100 | shuf | tee random_numbers.txt | wc -l
# 输出：100（同时 random_numbers.txt 保存了乱序的1-100）
```

## **NL （Number Lines） 简介**

`nl`
command 是用于对文本文件中的行进行编号的实用程序。也称为“数字线”，当您需要概述文件中某些行的位置时，它会很方便。默认情况下， nl 仅对非空行进行编号，但可以根据用户的需要修改此行为。

它遵循如下语法：

```astro-code
nl [options] [file_name]
```

如果未指定文件，将等待来自用户终端 （stdin） 的输入。其清晰易读的输出使其成为任何 Linux 用户文本处理工具包的重要组成部分。
`nl`

`nl`
是 Linux 中一个简单但实用的命令，专门用于
**给文本文件的行添加行号**
。你可以把它想象成“自动给书的每一页加页码”——无论是代码、日志还是普通文本，
`nl`
能快速标注每一行的位置，方便定位和引用。

---

#### **通俗解释**

假设你有一份待办清单：

```
买菜
洗衣服
（空行）
写报告
```

用
`nl`
处理后，会自动给非空行加上编号：

```
     1  买菜
     2  洗衣服
     3  写报告
```

（空行被跳过，但可以通过选项强制编号）

---

#### **核心功能 & 用法**

##### 1️⃣ **基本行号标注**

* **语法**
  ：
  `nl 文件名`
* **默认行为**
  ：仅对
  **非空行**
  编号，空行跳过。
* **示例**
  ：

  ```
  nl todo.txt
  # 输出：
  #     1  买菜
  #     2  洗衣服
  # 
  #     3  写报告
  ```

##### 2️⃣ **强制对所有行编号（包括空行）**

* 用
  `-b a`
  参数：

  ```
  nl -b a todo.txt
  # 输出：
  #     1  买菜
  #     2  洗衣服
  #     3  
  #     4  写报告
  ```

##### 3️⃣ **自定义行号格式**

* 调整行号的对齐、位数和分隔符：

  ```
  nl -n rz -w 3 -s ") " todo.txt
  # 输出：
  # 001) 买菜
  # 002) 洗衣服
  # 003) 写报告
  ```

  + `-n rz`
    ：右对齐，补零（如
    `001`
    ）。
  + `-w 3`
    ：行号占3位宽度。
  + `-s ") "`
    ：行号后跟右括号和空格。

##### 4️⃣ **指定起始行号**

* 用
  `-v`
  设置起始值：

  ```
  nl -v 10 todo.txt  # 行号从10开始
  # 输出：
  #    10  买菜
  #    11  洗衣服
  # 
  #    12  写报告
  ```

---

#### **日常场景类比**

* **代码调试**
  ：像在书本侧边栏标注行号，方便快速找到错误位置。
* **日志分析**
  ：像给会议记录每段加序号，讨论时直接说“看第5行”。
* **数据整理**
  ：像给清单条目编号，避免口头沟通时混淆顺序。

---

#### **常用场景示例**

1. **标注脚本代码行号**
   ：

   ```
   nl myscript.sh  # 显示脚本内容并标注非空行号
   ```
2. **统计文件实际内容行数**
   ：

   ```
   nl -b a file.txt | tail -n 1  # 显示总行数（包括空行）
   ```
3. **生成带序号的任务列表**
   ：

   ```
   echo -e "Task1\n\nTask2" | nl -b a -s ". "  # 输出：
   # 1. Task1
   # 2. 
   # 3. Task2
   ```

---

#### **注意事项**

1. **空行处理**
   ：默认跳过空行，需用
   `-b a`
   包含空行。
2. **输入来源**
   ：若不指定文件，
   `nl`
   会等待键盘输入（按
   `Ctrl+D`
   结束输入）。
3. **结合管道**
   ：可与其他命令联用，如
   `cat file.txt | nl`
   。

---

#### **小贴士**

* 对比
  `cat -n`
  ：
  `cat -n`
  会给所有行编号（包括空行），但格式较简单；
  `nl`
  更灵活可控。
* 复杂排版：用
  `-n`
  （对齐方式）、
  `-w`
  （行号宽度）、
  `-s`
  （分隔符）组合自定义格式。
* 快速定位：用
  `nl`
  +
  `grep`
  快速跳转到特定行：

  ```
  nl todo.txt | grep "写报告"  # 输出：     3  写报告
  ```

试试这个完整示例：

```
# 创建一个测试文件
echo -e "苹果\n\n香蕉\n橘子" > fruits.txt

# 给所有行（包括空行）编号，格式为 "行号: 内容"
nl -b a -n ln -s ": " fruits.txt
# 输出：
# 1: 苹果
# 2: 
# 3: 香蕉
# 4: 橘子
```

##### 

```
 -n ln
📌 作用：行号的对齐方式设为左对齐。

💡 -n 指定行号格式：

ln（left）：左对齐
rn（right）：右对齐（默认）
rz（右对齐，补零）
```

## **WC - 文本处理**

该命令是 Unix 或 Linux 中常用的工具，它允许用户计算文件或从标准输入管道传输的数据中的字节、字符、单词和行数。这个名字代表 'word count'，但它的作用远不止计算单词。的常见用法包括跟踪程序输出、计算代码行数等。它是分析精细和更大比例文本的宝贵工具。
`wc`
`wc`
`wc`

以下是 Linux 中的基本使用示例：
`wc`

```astro-code
wc myfile.txt
```

此命令将输出 中的行数、单词数和字符数。输出按以下顺序显示：行数、字数、字符数，后跟文件名。
`myfile.txt`

`wc`
是 Linux 中一个简单但强大的“
**文本统计器**
”，它的核心功能是
**快速统计文件的“行数、单词数、字节数”**
。你可以把它想象成“自动计数器”——无论是代码、日志还是普通文本，
`wc`
能一键告诉你这份文件有多“厚”、多“复杂”。

---

#### **通俗解释**

假设你有一篇英文作文：

```
I have a cat.
It is very cute.
```

用
`wc`
统计后，它会告诉你：

* **3 行**
  （两行文字 + 末尾空行）
* **7 个单词**
  （I, have, a, cat, It, is, very, cute）
* **30 个字节**
  （每个字母、空格、换行符都算字节）

---

#### **核心功能 & 用法**

##### 1️⃣ **基本统计（行数、单词数、字节数）**

* **语法**
  ：
  `wc 文件名`
* **默认行为**
  ：输出
  **行数 单词数 字节数 文件名**
* **示例**
  ：

  ```
  wc story.txt
  # 输出：3  7  30 story.txt
  ```

##### 2️⃣ **指定统计类型**

* **只统计行数**
  （适合数代码行数）：

  ```
  wc -l story.txt  # 输出：3 story.txt
  ```
* **只统计单词数**
  （比如文章字数）：

  ```
  wc -w story.txt  # 输出：7 story.txt
  ```
* **只统计字节数**
  （查看文件大小）：

  ```
  wc -c story.txt  # 输出：30 story.txt
  ```
* **统计字符数**
  （处理多语言文本时更准确）：

  ```
  wc -m story.txt  # 输出：30 story.txt（英文和中文混合时，字节≠字符）
  ```

##### 3️⃣ **统计多个文件**

* 批量统计并显示总计：

  ```
  wc *.txt  # 统计所有txt文件，最后一行显示总和
  ```

##### 4️⃣ **结合管道符**

* 统计其他命令的输出：

  ```
  ls | wc -l  # 统计当前目录下的文件数量
  cat log.txt | grep "error" | wc -l  # 统计日志中“error”出现的次数
  ```

---

#### **日常场景类比**

* **数行数**
  ：像翻书页数总页数。
* **数字数**
  ：像老师检查作文是否达到 800 字要求。
* **算大小**
  ：像快递员称包裹重量，判断是否超重。

---

#### **常用场景示例**

1. **统计代码行数**
   ：

   ```
   wc -l *.py  # 查看所有Python文件的行数
   ```
2. **检查日志错误次数**
   ：

   ```
   grep "ERROR" app.log | wc -l  # 统计错误日志的行数
   ```
3. **计算文本字数**
   ：

   ```
   wc -w essay.txt  # 输出文章总单词数
   ```
4. **查看文件大小（字节）**
   ：

   ```
   wc -c image.jpg  # 输出图片的字节大小
   ```

---

#### **注意事项**

1. **字节 vs 字符**
   ：

   * 英文字符：1 字符 = 1 字节。
   * 中文/特殊字符：1 字符可能占 2-4 字节（如 UTF-8 编码）。
   * 用
     `-m`
     统计字符数更准确（如
     `wc -m 中文.txt`
     ）。
2. **隐藏文件**
   ：

   * 统计文件数量时，默认不包含隐藏文件（以
     `.`
     开头的文件），需要明确指定：

     ```
     ls -a | wc -l  # 统计包括隐藏文件的数量
     ```
3. **空行统计**
   ：

   * 空行也会被计入行数（
     `-l`
     ），但不会增加单词数（
     `-w`
     ）。

---

#### **小贴士**

* 组合使用选项：

  ```
  wc -lw file.txt  # 同时统计行数和单词数
  ```
* 快速查看当前目录文件数量：

  ```
  ls | wc -l  # 统计文件总数（不包括隐藏文件）
  ```
* 统计文件夹内所有文件的总行数：

  ```
  find . -name "*.js" | xargs wc -l  # 统计所有JS代码的总行数
  ```

试试这个例子，感受
`wc`
的便捷：

```
# 创建一个测试文件
echo -e "Hello World\n你好，世界\n2024" > test.txt

# 统计行数、单词数、字符数
wc test.txt
# 输出：3  4  25 test.txt

# 只统计字符数（注意中文占多个字节）
wc -m test.txt  # 输出：15 test.txt（"你好，世界" 占 5 个字符）
```

## **在文本处理中扩展**

Expand 是 Unix 和类 Unix作系统中的命令行实用程序，可将制表符转换为空格。在处理文件输出时，它可能是必不可少的工具，因为格式可能会因选项卡而受到干扰。这在使用 Linux shell 脚本时特别有用，因为 Tab 键间距在不同的系统或文本编辑器上可能不同，从而导致格式不一致。使用空格的一致缩进可以大大提高代码的可读性。

默认情况下，该命令将制表符转换为 8 个空格。下面是一个示例用法：
`expand`

```astro-code
expand filename

```

在此示例中，是要将制表符转换为空格的文件的名称。运行命令后，Tab 键转换的内容将打印为标准输出。
`filename`

要指定每个选项卡的空格数，可以按如下方式使用该选项：
`-t`

```astro-code
expand -t 4 filename
```

在此示例中，中的每个制表符将替换为 4 个空格。然后，输出将显示在控制台上。
`filename`

`expand`
是 Linux 中一个专门用来
**把制表符（Tab）转换成空格**
的命令。你可以把它想象成“文本格式化工具”——它能解决因不同环境下 Tab 键显示不一致导致的排版混乱问题，尤其适合强迫症患者和需要严格代码格式的场景！

---

#### **通俗解释**

假设你有一份用 Tab 缩进的代码：

```
def hello():
→print("Hello")  # → 表示按了一次 Tab 键
→print("World")
```

不同编辑器可能将 Tab 显示为 4 个或 8 个空格，导致代码换行错乱。用
`expand`
统一换成空格后：

```
def hello():
    print("Hello")  # 4 个空格
    print("World")
```

无论在哪里打开，缩进都整整齐齐！

---

#### **核心功能 & 用法**

##### 1️⃣ **基本转换（默认替换为 8 空格）**

* **语法**
  ：
  `expand 文件名`
* **默认行为**
  ：将 Tab 转为 8 个空格，结果输出到屏幕（不修改原文件）。
* **示例**
  ：

  ```
  expand code.py  # 输出转换后的内容
  ```

##### 2️⃣ **自定义空格数量**

* 用
  `-t`
  指定每个 Tab 替换为几个空格：

  ```
  expand -t 4 code.py  # 每个 Tab → 4 空格（Python 常用）
  ```

##### 3️⃣ **处理多个文件**

* 批量转换并保存结果（需配合重定向）：

  ```
  expand -t 2 file1.txt file2.txt > formatted_files  # 合并转换后的内容
  ```

##### 4️⃣ **仅转换行首的 Tab**

* 用
  `-i`
  参数只处理行首的 Tab（保留行中的 Tab）：

  ```
  expand -i -t 4 data.txt  # 仅缩进对齐，保留行内 Tab（如表格数据）
  ```

---

#### **日常场景类比**

* **统一格式**
  ：像把不同品牌的衣架换成统一样式，让衣柜整齐划一。
* **跨平台协作**
  ：像把不同单位的尺寸统一为厘米，避免设计图尺寸混乱。
* **代码规范**
  ：像严格规定“每级缩进必须用4空格”，确保团队代码风格一致。

---

#### **常用场景示例**

1. **格式化代码缩进**
   ：

   ```
   expand -t 4 old_code.py > new_code.py  # 生成用4空格缩进的新文件
   ```
2. **检查文件中的隐藏 Tab**
   ：

   ```
   cat -A file.txt  # 显示特殊字符（^I 代表 Tab）
   expand -t 4 file.txt | cat -A  # 确认所有 ^I 是否替换成了空格
   ```
3. **与其他命令结合使用**
   ：

   ```
   grep "ERROR" log.txt | expand -t 2  # 过滤错误日志并转换缩进
   ```

---

#### **注意事项**

1. **不修改原文件**
   ：默认只输出到屏幕，需用
   `>`
   重定向保存：

   ```
   expand -t 4 input.txt > output.txt  # 保存转换后的内容
   ```
2. **混合缩进处理**
   ：如果文件中混合了 Tab 和空格，建议先用
   `expand`
   统一处理。
3. **逆向操作**
   ：如果想将空格转回 Tab，可用
   `unexpand`
   命令：

   ```
   unexpand -t 4 formatted.txt  # 将4空格转回 Tab
   ```

---

#### **小贴士**

* 用
  `cat -T`
  可显示文件中的 Tab（显示为
  `^I`
  ）：

  ```
  cat -T file.txt  # 找出隐藏的 Tab
  ```
* 在脚本中预格式化：

  ```
  #!/bin/bash
  expand -t 4 "$1" | grep "pattern"  # 处理输入文件并搜索
  ```
* 快速测试效果：

  ```
  echo -e "→Apple\t→Banana" | expand -t 4  # 输出：    Apple    Banana
  ```

试试这个完整示例：

```
# 创建含 Tab 的测试文件
echo -e "→水果列表：\n→苹果\n→香蕉" | sed 's/→/\t/g' > fruits.txt

# 查看原始 Tab（显示为 ^I）
cat -T fruits.txt  # 输出：^I水果列表：^I^I苹果...

# 转换为 4 空格
expand -t 4 fruits.txt
# 输出：
#    水果列表：
#    苹果
#    香蕉
```

## **在文本处理中展开**

Linux 中的命令是处理文本处理时的重要工具。它主要用于将空格转换为文件中的制表符或从终端输出。此命令的工作原理是将空格替换为制表符，使文档或输出更加连贯和整洁。它主要用于设置结构的格式，尤其是在编程脚本中，其中使用 tabs 缩进是一种常见的做法。
`unexpand`

使用命令的示例：
`unexpand`

```astro-code
unexpand -t 4 file.txt
```

“-t 4” 开关告诉 unexpand 将 中的每四个空格替换为一个制表符。
`file.txt`

`unexpand`
是 Linux 中与
`expand`
对应的“
**逆向格式化工具**
”——它专门
**把空格转换回制表符（Tab）**
。你可以把它想象成“文本压缩器”：将连续的空格缩成一个 Tab，让文件更紧凑，尤其适合需要节省空间或遵循 Tab 缩进规范的场景（比如某些编程风格要求必须用 Tab 缩进）。

---

#### **通俗解释**

假设你有一份用 4 个空格缩进的代码：

```
def hello():
    print("Hello")  # 缩进是 4 个空格
    print("World")
```

用
`unexpand`
处理后，4 个空格会被替换成一个 Tab（显示为
`→`
）：

```
def hello():
→print("Hello")  # → 表示一个 Tab
→print("World")
```

这样文件体积更小，且符合 Tab 缩进规范！

---

#### **核心功能 & 用法**

##### 1️⃣ **基本转换（默认仅处理行首空格）**

* **语法**
  ：
  `unexpand 文件名`
* **默认行为**
  ：仅将
  **行首**
  的连续空格转为 Tab（每个 Tab 默认替换 8 空格）。
* **示例**
  ：

  ```
  unexpand code.py  # 转换行首空格为 Tab，输出到屏幕
  ```

##### 2️⃣ **自定义空格数量**

* 用
  `-t`
  指定“多少个空格换一个 Tab”：

  ```
  unexpand -t 4 code.py  # 每 4 个空格 → 1 个 Tab
  ```

##### 3️⃣ **处理所有空格（不仅是行首）**

* 用
  `-a`
  参数转换
  **行中所有符合条件的空格**
  ：

  ```
  unexpand -a -t 4 data.txt  # 全文件中的 4 空格都转 Tab
  ```

##### 4️⃣ **批量处理并保存**

* 转换多个文件并输出到新文件：

  ```
  unexpand -t 4 file1.py file2.py > tab_indented_files  # 合并结果
  ```

---

#### **日常场景类比**

* **压缩空间**
  ：像把散装饼干重新装进密封袋，节省存储空间。
* **统一规范**
  ：像把不同地区的电压统一转换，适配本地电器。
* **代码风格**
  ：像团队规定必须用 Tab 缩进，将历史代码中的空格批量替换。

---

#### **常用场景示例**

1. **将 Python 空格缩进转为 Tab**
   ：

   ```
   unexpand -t 4 space_code.py > tab_code.py  # 生成 Tab 缩进的新文件
   ```
2. **处理混合缩进的配置文件**
   ：

   ```
   unexpand -a -t 2 config.yml  # 将全文中每 2 个空格转 Tab
   ```
3. **快速压缩日志文件**
   ：

   ```
   unexpand -t 4 log.txt | gzip > log.tar.gz  # 转换后压缩，节省空间
   ```

---

#### **注意事项**

1. **默认只处理行首**
   ：不加
   `-a`
   时，仅转换行首空格（保留行内的空格）。
2. **不修改原文件**
   ：默认输出到屏幕，需用
   `>`
   保存结果：

   ```
   unexpand -t 4 input.txt > output.txt  # 保存转换后的文件
   ```
3. **混合缩进问题**
   ：如果文件中混用 Tab 和空格，建议先用
   `expand`
   统一为空格，再用
   `unexpand`
   转换。

---

#### **小贴士**

* 用
  `cat -T`
  查看文件中的 Tab（显示为
  `^I`
  ）：

  ```
  cat -T file.py  # 显示所有 Tab（→ 会变成 ^I）
  ```
* **逆向操作**
  ：若想将 Tab 转回空格，用
  `expand`
  命令：

  ```
  expand -t 4 tab_file.py  # 将 Tab 转为 4 空格
  ```
* 快速测试效果：

  ```
  echo "    Hello World" | unexpand -t 4  # 输出：→Hello World
  ```

试试这个完整示例：

```
# 创建用 4 空格缩进的测试文件
echo -e "def test():\n    print('OK')" > space_code.py

# 转换为 Tab 缩进（4 空格 → 1 Tab）
unexpand -t 4 space_code.py > tab_code.py

# 查看结果中的 Tab（显示为 ^I）
cat -T tab_code.py
# 输出：def test():^Iprint('OK')
```

## **统一**

在 Linux 中，是一个非常有用的文本处理命令行程序。它通过比较或过滤掉相邻的重复行来帮助检查和作文本文件。无论您是处理数据列表还是大型文本文档，该命令都允许您查找和过滤掉重复的行，甚至提供文件中每个唯一行的计数。请务必记住，只会删除彼此相邻的重复项，因此为了充分利用此命令，通常首先使用命令对数据进行排序。
`uniq`
`uniq`
`uniq`
`sort`

使用的示例是：
`uniq`

```astro-code
sort names.txt | uniq
```

在此示例中， 是一个包含名称列表的文件。该命令对文件中的所有行进行排序，然后该命令删除所有重复的行。生成的输出将是 中的唯一名称列表。
`names.txt`
`sort`
`uniq`
`names.txt`

`uniq`
是 Linux 中一个实用的“
**去重小助手**
”，专门用于
**处理相邻的重复行**
。它的核心功能是
**删除连续的重复行**
或
**统计重复次数**
，就像整理书架上重复的书本，让内容更简洁清晰。

---

#### **通俗解释**

假设你有一个购物清单，不小心重复写了几次：

```
苹果
苹果
香蕉
橘子
橘子
橘子
```

用
`uniq`
处理后，相邻的重复行会被合并：

```
苹果
香蕉
橘子
```

（注意：如果重复行不相邻，
`uniq`
不会处理！需要先用
`sort`
排序）

---

#### **核心功能 & 用法**

##### 1️⃣ **基础去重（仅处理相邻重复行）**

* **语法**
  ：
  `uniq 文件名`
* **默认行为**
  ：删除
  **连续重复的行**
  ，保留唯一一行。
* **示例**
  ：

  ```
  uniq list.txt  # 直接去重（需确保重复行相邻）
  ```

##### 2️⃣ **先排序再去重（经典组合）**

* 结合
  `sort`
  命令处理任意位置的重复行：

  ```
  sort shopping_list.txt | uniq  # 先排序，再去重（输出全局唯一行）
  ```

##### 3️⃣ **统计重复次数**

* 用
  `-c`
  参数显示每行重复的次数：

  ```
  uniq -c log.txt  # 输出：
  #   3 苹果
  #   1 香蕉
  #   2 橘子
  ```

##### 4️⃣ **仅显示重复行**

* 用
  `-d`
  只保留重复过的行：

  ```
  uniq -d list.txt  # 输出：苹果、橘子（香蕉不重复，被过滤）
  ```

##### 5️⃣ **忽略大小写**

* 用
  `-i`
  参数让大小写视为相同：

  ```
  echo -e "Apple\napple\nBanana" | uniq -i  # 输出：Apple、Banana
  ```

---

#### **日常场景类比**

* **整理书单**
  ：像把重复购买的书籍清单合并成一本。
* **统计投票**
  ：像计票时合并相同选项的票数。
* **清理日志**
  ：像删除服务器日志中连续出现的相同错误。

---

#### **常用场景示例**

1. **统计访问日志的独立 IP**
   ：

   ```
   cat access.log | awk '{print $1}' | sort | uniq -c  # 统计每个IP的访问次数
   ```
2. **清理重复的待办事项**
   ：

   ```
   sort todo.txt | uniq > cleaned_todo.txt  # 生成去重后的清单
   ```
3. **查找重复文件内容**
   ：

   ```
   sort files.txt | uniq -d  # 找出所有重复的文件名
   ```

---

#### **注意事项**

1. **必须排序**
   ：
     
   `uniq`
   只能处理
   **相邻的重复行**
   ，因此通常需要先用
   `sort`
   排序：

   ```
   sort file.txt | uniq  # 正确用法
   ```

   * 直接
     `uniq file.txt`
     可能漏掉非相邻的重复行！
2. **与
   `sort -u`
   的区别**
   ：
     
   `sort -u`
   也能去重，但功能不同：

   ```
   sort -u file.txt      # 排序后直接去重（结果等效于 sort | uniq）
   sort file.txt | uniq  # 可结合 uniq 的其他功能（如统计次数）
   ```
3. **保留原顺序**
   ：
     
   如果不想打乱原文件顺序，但要去重，需用其他方法（如
   `awk`
   ）。

---

#### **小贴士**

* 快速去重并保存：

  ```
  sort input.txt | uniq > output.txt  # 经典组合
  ```
* 仅显示唯一行（不重复的行）：

  ```
  sort file.txt | uniq -u  # 找出从未重复过的行
  ```
* 处理 CSV 文件中的重复数据：

  ```
  cut -d ',' -f 1 data.csv | sort | uniq -c  # 统计第一列的重复次数
  ```

试试这个例子，感受
`uniq`
的便捷：

```
# 创建一个测试文件
echo -e "苹果\n苹果\n香蕉\n橘子\n橘子\n橘子" > fruits.txt

# 直接去重（因重复行相邻，有效）
uniq fruits.txt
# 输出：
# 苹果
# 香蕉
# 橘子

# 统计重复次数
uniq -c fruits.txt
# 输出：
# 2 苹果
# 1 香蕉
# 3 橘子
```

## **文本处理中的 GREP**

GREP（全局正则表达式打印）被认为是类 Unix作系统（包括 Linux）文本处理领域的重要工具。它是一个强大的实用程序，用于搜索和过滤与给定模式匹配的文本。当它识别到与模式匹配的行时，它会将该行打印到屏幕上，从而提供了一种在文件中查找文本的有效编码方法。

GREP 是许多 shell 脚本、bash 命令和命令行作的重要组成部分，是每个 Linux 发行版中预装的多功能工具。它包含三个主要部分 - 格式、作和正则表达式。多年来，它已有效地用于多种编程语言和数据科学应用程序。

以下是简单 GREP 命令的示例：

```astro-code
grep "pattern" fileName
```

此命令将在文件中搜索指定的模式，并将该行打印到终端。

还有一个替代 - .
`grep`
`ripgrep`

`ripgrep`
是一个速度极快的文本处理器，支持并扩展了它的所有功能。
`grep`

`grep`
是 Linux 中一个
**文本搜索神器**
，它的核心功能是
**在文件中快速查找包含特定关键词或模式的行**
。你可以把它想象成“文本探测器”——无论是日志文件、代码还是文档，只要告诉它要找什么，它就能瞬间定位目标！

---

#### **通俗解释**

假设你有一本电话簿（文件），想找所有姓“张”的人：

```
张三 13800138000
李四 13912345678
张伟 13500000000
```

用
`grep`
搜索“张”，它会立刻返回：

```
张三 13800138000
张伟 13500000000
```

---

#### **核心功能 & 用法**

##### 1️⃣ **基础搜索**

* **语法**
  ：
  `grep "关键词" 文件名`
* **默认行为**
  ：输出包含关键词的
  **整行内容**
  。
* **示例**
  ：

  ```
  grep "error" log.txt    # 在 log.txt 中查找包含 "error" 的行
  grep "TODO" code.py    # 在代码中查找所有待办标记
  ```

##### 2️⃣ **高级搜索（正则表达式）**

* 支持复杂模式匹配（如模糊搜索、范围匹配）：

  ```
  grep "^2024" dates.txt    # 查找以 2024 开头的行（如日期）
  grep "[Aa]pple" fruits.txt  # 匹配 Apple 或 apple
  grep "user[0-9]" data.txt  # 查找 user0、user1...user9
  ```

##### 3️⃣ **常用参数**

* **忽略大小写**
  ：
  `-i`

  ```
  grep -i "apple" file.txt  # 匹配 Apple、APPLE、apple...
  ```
* **显示行号**
  ：
  `-n`

  ```
  grep -n "bug" code.py  # 输出：行号:代码内容
  ```
* **统计匹配次数**
  ：
  `-c`

  ```
  grep -c "success" log.txt  # 输出匹配到的行数
  ```
* **递归搜索目录**
  ：
  `-r`

  ```
  grep -r "function" ./src  # 在 src 目录下所有文件中搜索
  ```

##### 4️⃣ **反向搜索（排除匹配行）**

* 用
  `-v`
  过滤
  **不包含**
  关键词的行：

  ```
  grep -v "debug" log.txt  # 输出所有不含 "debug" 的行
  ```

---

#### **日常场景类比**

* **查日志**
  ：像用 Ctrl+F 在网页中搜索关键词。
* **找代码**
  ：像在书里用荧光笔标记重点段落。
* **数据清洗**
  ：像从一堆卡片中挑出符合要求的。

---

#### **常用场景示例**

1. **快速定位程序崩溃原因**
   ：

   ```
   grep -n "Exception" server.log  # 显示错误发生的行号及内容
   ```
2. **统计 API 调用次数**
   ：

   ```
   grep -c "/api/v1/login" access.log  # 统计登录接口的调用次数
   ```
3. **搜索多个文件**
   ：

   ```
   grep "deprecated" *.java  # 在所有 Java 文件中查找过时代码
   ```
4. **组合其他命令**
   ：

   ```
   ps aux | grep "nginx"     # 查找所有 nginx 进程
   history | grep "ssh"      # 查找历史命令中的 ssh 记录
   ```

---

#### **注意事项**

1. **默认区分大小写**
   ：搜索 "Error" 不会匹配 "ERROR"，需用
   `-i`
   。
2. **特殊字符转义**
   ：搜索
   `.`
   、
   `*`
   等正则符号时，需加反斜杠转义：

   ```
   grep "19216811" log.txt  # 搜索 IP 地址
   ```
3. **性能优化**
   ：大文件搜索时，可用
   `fgrep`
   （固定字符串搜索，速度更快）。

---

#### **小贴士**

* 使用
  `--color=auto`
  高亮显示匹配内容（默认已启用）：

  ```
  grep "warning" log.txt --color=auto
  ```
* 结合管道符实时过滤：

  ```
  tail -f log.txt | grep "INFO"  # 实时监控并过滤日志
  ```
* 快速生成摘要：

  ```
  grep -o "user_[0-9]*" log.txt | sort | uniq -c  # 统计不同用户的出现次数
  ```

---

#### **升级版工具：ripgrep ( `rg` )**

`ripgrep`
是
`grep`
的现代替代品，速度更快、功能更强：

* **默认递归搜索目录**
  （无需
  `-r`
  参数）。
* **自动忽略.gitignore中的文件**
  （如排除编译产物）。
* **语法更简洁**
  ：

  ```
  rg "pattern"          # 当前目录递归搜索
  rg -i "error" log/    # 忽略大小写，搜索 log 目录
  rg -t py "TODO"       # 仅在 Python 文件中搜索
  ```

试试这个例子，感受
`grep`
的高效：

```
# 创建测试文件
echo -e "Apple\nbanana\nCherry\n12345" > test.txt

# 搜索包含字母a的行（不区分大小写）
grep -i "a" test.txt
# 输出：
# Apple
# banana
# Cherry
```

## awk - 文本处理

awk 是一种功能强大的文本处理语言，广泛用于类 Unix作系统，包括 Linux。awk 以其三位原始开发人员 - Alfred Aho、Peter Weinberger 和 Brian Kernighan 命名，擅长对文本文件执行作，例如排序、过滤和报告生成。

该语言包含脚本中的一组命令，用于定义模式-作对。从本质上讲，awk 逐行读取输入文件，识别与脚本中指定的匹配模式，并因此对这些匹配项执行作。

虽然 awk 是一种具有变量、表达式和控制结构的完整语言，但它最常用作 bash shell 脚本中的单行命令，利用其多功能的文本作功能。

下面是一个如何使用 awk 打印文件每行的前两个字段的示例：

```
awk '{print $1,$2}' filename

```

这将显示 'filename' 中每行的第一个和第二个字段 （通常用空格分隔）。

`awk`
是 Linux 中一个
**超级强大的文本处理工具**
，它的核心功能是
**按列处理结构化文本**
（比如日志、表格数据）。你可以把它想象成“
**数据手术刀**
”——无论是提取特定列、过滤数据，还是做统计计算，
`awk`
都能轻松搞定！

---

#### **通俗解释**

假设你有一个学生成绩表（每列用空格分隔）：

```
张三 90 85
李四 78 92
王五 88 76
```

用
`awk`
可以：

* **提取第一列**
  （姓名）：
  `awk '{print $1}'`

  ```
  张三
  李四
  王五
  ```
* **计算每个人的总分**
  ：
  `awk '{print $1, $2+$3}'`

  ```
  张三 175
  李四 170
  王五 164
  ```

---

#### **核心功能 & 用法**

##### 1️⃣ **基础列提取**

* **语法**
  ：
  `awk '{print $列号}' 文件名`
* **默认行为**
  ：按
  **空格/Tab**
  分割列，
  `$1`
  代表第一列，
  `$0`
  代表整行。
* **示例**
  ：

  ```
  awk '{print $1, $3}' data.txt  # 输出第1列和第3列
  awk '{print $NF}' data.txt     # 输出最后一列（NF表示总列数）
  ```

##### 2️⃣ **指定分隔符**

* 用
  `-F`
  指定列分隔符（如逗号、冒号）：

```
awk -F ',' '{print $2}' users.csv  # 按逗号分列，提取第2列
awk -F ':' '{print $1}' /etc/passwd  # 提取系统用户名称
```

##### 

* 只处理符合条件的行：

  ```
  awk '$2 > 80 {print $1}' scores.txt  # 输出第二列大于80的行的姓名
  awk '/error/ {print}' log.txt        # 输出包含 "error" 的行
  ```

  ##### 

  ```
  2️⃣ '$2 > 80 {print $1}'
  📌 这个部分是 awk 的脚本逻辑**，解释如下：

  🔹 $2 > 80：

  $2 表示 第二列（假设第一列是名字，第二列是分数）。
  > 80 表示筛选分数大于 80 的行。
  🔹 {print $1}：

  {} 表示要执行的操作。
  print $1 表示打印第一列（即姓名）。
  ```

##### 4️⃣ **统计与计算**

* 内置变量和计算功能：

  ```
  # 统计文件总行数
  awk 'END {print NR}' data.txt  # NR 是已处理的总行数

  # 计算第二列平均值
  awk '{sum+=$2} END {print sum/NR}' data.txt
  ```

---

#### **日常场景类比**

* **提取数据**
  ：像从 Excel 表格中复制特定列。
* **过滤信息**
  ：像用筛子过滤出符合标准的颗粒。
* **生成报告**
  ：像自动汇总表格中的总和、平均值。

---

#### **常用场景示例**

1. **提取日志中的时间戳和错误码**
   ：

   ```
   awk '{print $1, $5}' access.log  # 假设第1列是时间，第5列是状态码
   ```
2. **统计 CSV 文件的总销售额**
   ：

   ```
   awk -F ',' '{sum += $3} END {print sum}' sales.csv  # 第3列是金额
   ```
3. **过滤出 CPU 使用率超过 80% 的进程**
   ：

   ```
   ps aux | awk '$3 > 80 {print $11}'  # $3是CPU%，$11是进程名
   ```
4. **格式化输出**
   ：

   ```
   awk '{printf "姓名:%s 总分:%d\n", $1, $2+$3}' scores.txt
   # 输出：
   # 姓名:张三 总分:175
   # 姓名:李四 总分:170
   ```

---

#### **注意事项**

1. **默认分隔符是空格/Tab**
   ：如果数据用其他符号分隔（如逗号），必须用
   `-F`
   指定。
2. **列号从1开始**
   ：
   `$1`
   是第一列，
   `$0`
   是整行。
3. **大小写敏感**
   ：
   `awk`
   区分大小写，
   `Error`
   和
   `error`
   不同。

---

#### **小贴士**

* **快速查看文件结构**
  ：

  ```
  awk '{print NF}' file.txt  # 显示每行的列数（检查是否一致）
  ```
* **批量替换文本**
  ：

  ```
  awk '{gsub("old","new",$2); print}' file.txt  # 将第2列的old替换为new
  ```
* **处理复杂日志**
  ：

  ```
  # 提取时间在 "2024-01-01" 之后的错误日志
  awk '/ERROR/ && $1 > "2024-01-01" {print}' app.log
  ```

---

#### **进阶技巧**

1. **BEGIN 和 END 块**
   ：

   ```
   # 在开头和结尾执行操作（如添加表头）
   awk 'BEGIN {print "姓名 总分"} {print $1, $2+$3} END {print "---结束---"}' scores.txt
   ```
2. **使用外部变量**
   ：

   ```
   threshold=80
   awk -v t=$threshold '$2 > t {print}' data.txt  # 用变量传递阈值
   ```
3. **多条件组合**
   ：

   ```
   awk '$2 > 80 && $3 < 90 {print $1}' scores.txt  # 第二列>80且第三列<90的行
   ```

试试这个例子，感受
`awk`
的威力：

```
# 创建测试文件
echo -e "苹果 5 20\n香蕉 3 15\n橘子 4 18" > fruits.txt

# 计算每种水果的总价（数量*单价）
awk '{total = $2 * $3; print $1 " 总价：" total "元"}' fruits.txt
# 输出：
# 苹果 总价：100元
# 香蕉 总价：45元
# 橘子 总价：72元
```