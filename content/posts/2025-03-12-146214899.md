---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f4171753431352f:61727469636c652f64657461696c732f313436323134383939"
layout: post
title: "大模型系列llama.cpp本地运行大模型"
date: 2025-03-12 21:18:55 +08:00
description: "上一篇链接:我们讲了ollama本地运行大模型，这里我们介绍另一种本地运行大模型的方法：llamacpp。"
keywords: "【大模型系列】llama.cpp本地运行大模型"
categories: ['Llm']
tags: ['Ai']
artid: "146214899"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146214899
    alt: "大模型系列llama.cpp本地运行大模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146214899
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146214899
cover: https://bing.ee123.net/img/rand?artid=146214899
image: https://bing.ee123.net/img/rand?artid=146214899
img: https://bing.ee123.net/img/rand?artid=146214899
---

# 【大模型系列】llama.cpp本地运行大模型

上一篇链接:
[【大模型系列】使用ollama本地运行千问2.5模型](https://blog.csdn.net/Aqu415/article/details/145862222)
我们讲了ollama本地运行大模型，这里我们介绍另一种本地运行大模型的方法：llamacpp

##### 软件下载

下载地址：https://github.com/ggml-org/llama.cpp/releases
  
下载cpu版本的llamacpp；建议下载3982版本的，新版本我试了在win上运行不了
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2c60bc8a07f14729999c9154fe6da089.png)
  
下载后是一个压缩包，解压后配置Path

##### 配置path

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d16f425db8dd468c8e5ea9ee1b3277f9.png)
  
把解压后的路径加入到path中

##### 校验

任意新开命令行，输入指令：

```cpp
llama-cli -v

```

出现以下内容表示安装成功
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b22b252dbbc748c0ac00c28668818f92.png)

##### 启动

到模型所在目录，执行以下命令即可；下载模型可见我另一篇：链接:
[【大模型系列】入门常识备忘](https://blog.csdn.net/Aqu415/article/details/145668409)
，这里我们下载的gguf格式的模型

```cpp
llama-server -m qwen2.5-7b-instruct-q5_0.gguf --port 8088 -c 2048

```

port 是服务的端口
  
c 是context的意思，即最大能处理多少个token

启动后就可以通过浏览器 http://localhost:8088 访问服务了

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/71dbaef3d2094ddebbfd93a63d20a3b2.png)
  
调试开始

over~~