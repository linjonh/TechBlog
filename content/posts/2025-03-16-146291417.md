---
layout: post
title: "python_巨潮年报pdf下载"
date: 2025-03-16 10:49:54 +08:00
description: "另外，要耐心，整个下载过程要十几二十小时，视电脑配置和网络而定。1 了解一些股票的基本面需要看历年年报，在巨潮一个个下载比较费时间，所以考虑用python把年报pdf下载下来。2 写代码，遍历每个股票，一个股票一个Excel，记录该股票所有年报url。step one获取的公告链接是网页查看的url，要下载pdf需要获取pdf对应的url。1 分5个线程，把txt文件分到五个文件夹里，文件夹以0、1、2、3、4命名。要下载的pdf很多，但股票个数也就五千多个，本人搞了个简单的多线程。"
keywords: "python_巨潮年报pdf下载"
categories: ['随想']
tags: ['Pdf']
artid: "146291417"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146291417
    alt: "python_巨潮年报pdf下载"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146291417
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146291417
cover: https://bing.ee123.net/img/rand?artid=146291417
image: https://bing.ee123.net/img/rand?artid=146291417
img: https://bing.ee123.net/img/rand?artid=146291417
---

# python\_巨潮年报pdf下载
\* \* \*
## 前置：
1 了解一些股票的基本面需要看历年年报，在巨潮一个个下载比较费时间，所以考虑用python把年报pdf下载下来。
2 如果要下载的股票个数很多，提前预备好大硬盘。本人下载深沪两市年报，大概150G。另外，要耐心，整个下载过程要十几二十小时，视电脑配置和网络而定。
## 步骤：
### step one: pip安装必要包，获取年报url列表
pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install openpyxl -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install akshare -i https://pypi.tuna.tsinghua.edu.cn/simple
pandas 本地处理数据、openpyxl
excel表格处理需要这个包、akshare获取url接口（要了解akshare具体用法可以看其官网，直接百度搜素就能找到其官网）
1 获取股票代码列表，可以在通达信中获取。（通达信行情-》A股-》按“34”键-》导出表格）
2 写代码，遍历每个股票，一个股票一个Excel，记录该股票所有年报url。（巨潮最早能查到2000年）
def temp\_000():
import akshare as ak
pre\_dir = r'E:/temp002/'
with open('./stock\_ticker.txt',mode='r',encoding='utf-8') as fr:
contents = fr.read()
stock\_ticker\_list = contents.split('\n')
for symbol\_str in stock\_ticker\_list:
try:
df = ak.stock\_zh\_a\_disclosure\_report\_cninfo(symbol=symbol\_str, market="沪深京",
category="年报",
start\_date="20000101",
end\_date="20250315")
df.to\_excel(pre\_dir+symbol\_str+'.xlsx',engine='openpyxl')
except:
print(symbol\_str)
pass
3 这个过程大概半个小时到一个小时
![](https://i-blog.csdnimg.cn/direct/7fad9703e857459c9e2f40a868e57aa2.png)
![](https://i-blog.csdnimg.cn/direct/f9cf04e21d694bfd908a720186096400.png)
公告链接，这一列就是我们要的
### step two: 将查看url列表转换为pdf url
step one获取的公告链接是网页查看的url，要下载pdf需要获取pdf对应的url
1 拿一个公告链接用浏览器打开，寻找pdf对应的url
http://www.cninfo.com.cn/new/disclosure/detail?stockCode=000001&announcementId=1222806509&orgId=gssz0000001&announcementTime=2025-03-15
00:00:00
![](https://i-blog.csdnimg.cn/direct/e3858146a0ad44bb832744cd3ec85842.png)
2 对比pdf
url与公告链接的关系，将所有公告链接转换成pdf对应的url![](https://i-blog.csdnimg.cn/direct/4cf84d48db5e44a480c9a58d568a1fee.png)
def temp\_001():
pre\_dir = r'E:/temp002/'
tar\_dir = r'E:/temp006/'
file\_list = os.listdir(pre\_dir)
for file\_one in file\_list:
ticker = file\_one[0:6]
pre\_file\_path = pre\_dir + file\_one
df = pd.read\_excel(pre\_file\_path,engine='openpyxl')
url\_list = df['公告链接'].to\_list()
pdf\_url\_list = []
for u\_one in url\_list:
u\_one\_00 = u\_one.split('&')
node\_00 = u\_one\_00[1].replace('announcementId=','')
node\_01 = u\_one\_00[-1].replace('announcementTime=','')
node\_01 = node\_01[0:10]
tar\_node = f'http://static.cninfo.com.cn/finalpage/{node\_01}/{node\_00}.PDF'
pdf\_url\_list.append(tar\_node)
pass
pdf\_url\_list\_str = '\n'.join(pdf\_url\_list)
with open(f'{tar\_dir}/{ticker}.txt', mode='w', encoding='utf-8') as fw:
fw.write(pdf\_url\_list\_str)
pass
pass
3 这个过程几分钟，一个股票对应一个txt文件
![](https://i-blog.csdnimg.cn/direct/8b21fc819341445390044067e8308886.png)
### step three: 多进程下载pdf
要下载的pdf很多，但股票个数也就五千多个，本人搞了个简单的多线程
1 分5个线程，把txt文件分到五个文件夹里，文件夹以0、1、2、3、4命名
2 创建下载后放置pdf文件的文件夹，文件夹同样以0、1、3、4命名
3 写代码，执行。等待执行完毕，整个过程十几小时以上，看电脑配置和网络情况。
import os,threading
'''
多线程下载财报
'''
# 创建多个线程
def temp\_thread():
threads = []
for i in range(5):
thread = threading.Thread(target=temp\_005,args=(i,))
threads.append(thread)
thread.start()
pass
for thread in threads:
thread.join()
print('all thread finished')
pass
# 执行项
def temp\_005(i):
import requests
pre\_dir = r'E:/temp006/'+str(i)+'/'
tar\_dir = r'E:/temp007/'+str(i)+'/'
file\_list = os.listdir(pre\_dir)
for file\_one in file\_list:
ticker = file\_one[0:6]
tar\_dir00 = tar\_dir + ticker + os.path.sep
if not os.path.exists(tar\_dir00):
os.mkdir(tar\_dir00)
url\_file\_path = pre\_dir + file\_one
with open(url\_file\_path,'r') as fr:
url\_str = fr.read()
url\_list = url\_str.split('\n')
try:
for url\_one in url\_list:
tar\_file\_name00 = url\_one.split('/')
tar\_file\_name = f"{tar\_file\_name00[-2]}\_{tar\_file\_name00[-1]}.pdf"
try:
res = requests.get(url\_one)
if res.status\_code == 200:
with open(tar\_dir00+tar\_file\_name,'wb') as fw:
fw.write(res.content)
pass
else:
error\_str = f'下载失败，状态码：{res.status\_code}。{url\_one}\n'
with open(f'./{i}.txt','a',encoding='utf-8') as fw:
fw.write(error\_str)
except:
print(url\_one)
pass
pass
except:
print(ticker)
pass
print('----------------end---',i)
pass
if \_\_name\_\_ == '\_\_main\_\_':
temp\_thread()
pass
执行完毕后，研究历年财报再也不用一个个下载啦，哈哈哈