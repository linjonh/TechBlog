---
layout: post
title: "python_巨潮年报pdf下载"
date: 2025-03-16 10:49:54 +0800
description: "另外，要耐心，整个下载过程要十几二十小时，视电脑配置和网络而定。1 了解一些股票的基本面需要看历年年报，在巨潮一个个下载比较费时间，所以考虑用python把年报pdf下载下来。2 写代码，遍历每个股票，一个股票一个Excel，记录该股票所有年报url。step one获取的公告链接是网页查看的url，要下载pdf需要获取pdf对应的url。1 分5个线程，把txt文件分到五个文件夹里，文件夹以0、1、2、3、4命名。要下载的pdf很多，但股票个数也就五千多个，本人搞了个简单的多线程。"
keywords: "python_巨潮年报pdf下载"
categories: ['随想']
tags: ['Pdf']
artid: "146291417"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146291417
    alt: "python_巨潮年报pdf下载"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146291417
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146291417
cover: https://bing.ee123.net/img/rand?artid=146291417
image: https://bing.ee123.net/img/rand?artid=146291417
img: https://bing.ee123.net/img/rand?artid=146291417
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     python_巨潮年报pdf下载
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <hr id="hr-toc" name="tableOfContents"/>
    <p>
    </p>
    <h2 id="%E5%89%8D%E7%BD%AE%EF%BC%9A" name="%E5%89%8D%E7%BD%AE%EF%BC%9A">
     前置：
    </h2>
    <p>
     1 了解一些股票的基本面需要看历年年报，在巨潮一个个下载比较费时间，所以考虑用python把年报pdf下载下来。
    </p>
    <p>
     2 如果要下载的股票个数很多，提前预备好大硬盘。本人下载深沪两市年报，大概150G。另外，要耐心，整个下载过程要十几二十小时，视电脑配置和网络而定。
    </p>
    <h2 id="%E6%AD%A5%E9%AA%A4%EF%BC%9A" name="%E6%AD%A5%E9%AA%A4%EF%BC%9A">
     步骤：
    </h2>
    <h3 id="step%20one%3A%C2%A0pip%E5%AE%89%E8%A3%85%E5%BF%85%E8%A6%81%E5%8C%85%EF%BC%8C%E8%8E%B7%E5%8F%96%E5%B9%B4%E6%8A%A5url%E5%88%97%E8%A1%A8" name="step%20one%3A%C2%A0pip%E5%AE%89%E8%A3%85%E5%BF%85%E8%A6%81%E5%8C%85%EF%BC%8C%E8%8E%B7%E5%8F%96%E5%B9%B4%E6%8A%A5url%E5%88%97%E8%A1%A8">
     step one: pip安装必要包，获取年报url列表
    </h3>
    <pre>pip install pandas -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install openpyxl -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install akshare -i https://pypi.tuna.tsinghua.edu.cn/simple</pre>
    <p>
     pandas 本地处理数据、openpyxl excel表格处理需要这个包、akshare获取url接口（要了解akshare具体用法可以看其官网，直接百度搜素就能找到其官网）
    </p>
    <p>
     1 获取股票代码列表，可以在通达信中获取。（通达信行情-》A股-》按“34”键-》导出表格）
    </p>
    <p>
     2 写代码，遍历每个股票，一个股票一个Excel，记录该股票所有年报url。（巨潮最早能查到2000年）
    </p>
    <pre><code class="hljs">def temp_000():
    import akshare as ak
    pre_dir = r'E:/temp002/'
    with open('./stock_ticker.txt',mode='r',encoding='utf-8') as fr:
        contents = fr.read()
    stock_ticker_list = contents.split('\n')

    for symbol_str in stock_ticker_list:
        try:
            df = ak.stock_zh_a_disclosure_report_cninfo(symbol=symbol_str, market="沪深京",
                                                                                            category="年报",
                                                                                            start_date="20000101",
                                                                                            end_date="20250315")
            df.to_excel(pre_dir+symbol_str+'.xlsx',engine='openpyxl')
        except:
            print(symbol_str)
    pass</code></pre>
    <p>
     3 这个过程大概半个小时到一个小时
    </p>
    <p>
     <img alt="" height="943" src="https://i-blog.csdnimg.cn/direct/7fad9703e857459c9e2f40a868e57aa2.png" width="1288"/>
    </p>
    <p>
     <img alt="" height="1020" src="https://i-blog.csdnimg.cn/direct/f9cf04e21d694bfd908a720186096400.png" width="1920"/>
    </p>
    <p>
     公告链接，这一列就是我们要的
    </p>
    <h3 id="step%20two%3A%C2%A0%E5%B0%86%E6%9F%A5%E7%9C%8Burl%E5%88%97%E8%A1%A8%E8%BD%AC%E6%8D%A2%E4%B8%BApdf%20url" name="step%20two%3A%C2%A0%E5%B0%86%E6%9F%A5%E7%9C%8Burl%E5%88%97%E8%A1%A8%E8%BD%AC%E6%8D%A2%E4%B8%BApdf%20url">
     step two: 将查看url列表转换为pdf url
    </h3>
    <p>
     step one获取的公告链接是网页查看的url，要下载pdf需要获取pdf对应的url
    </p>
    <p>
     1 拿一个公告链接用浏览器打开，寻找pdf对应的url
    </p>
    <p>
     http://www.cninfo.com.cn/new/disclosure/detail?stockCode=000001&amp;announcementId=1222806509&amp;orgId=gssz0000001&amp;announcementTime=2025-03-15 00:00:00
    </p>
    <p>
     <img alt="" height="937" src="https://i-blog.csdnimg.cn/direct/e3858146a0ad44bb832744cd3ec85842.png" width="1920"/>
    </p>
    <p>
     2 对比pdf url与公告链接的关系，将所有公告链接转换成pdf对应的url
     <img alt="" height="235" src="https://i-blog.csdnimg.cn/direct/4cf84d48db5e44a480c9a58d568a1fee.png" width="1384"/>
    </p>
    <pre><code class="hljs">def temp_001():
    pre_dir = r'E:/temp002/'
    tar_dir = r'E:/temp006/'
    file_list = os.listdir(pre_dir)
    for file_one in file_list:
        ticker = file_one[0:6]
        pre_file_path = pre_dir + file_one
        df = pd.read_excel(pre_file_path,engine='openpyxl')
        url_list = df['公告链接'].to_list()
        pdf_url_list = []
        for u_one in url_list:
            u_one_00 = u_one.split('&amp;')
            node_00 = u_one_00[1].replace('announcementId=','')
            node_01 = u_one_00[-1].replace('announcementTime=','')
            node_01 = node_01[0:10]
            tar_node = f'http://static.cninfo.com.cn/finalpage/{node_01}/{node_00}.PDF'
            pdf_url_list.append(tar_node)
            pass
        pdf_url_list_str = '\n'.join(pdf_url_list)
        with open(f'{tar_dir}/{ticker}.txt', mode='w', encoding='utf-8') as fw:
            fw.write(pdf_url_list_str)
        pass
    pass</code></pre>
    <p>
     3 这个过程几分钟，一个股票对应一个txt文件
    </p>
    <p>
     <img alt="" height="346" src="https://i-blog.csdnimg.cn/direct/8b21fc819341445390044067e8308886.png" width="1152"/>
    </p>
    <h3 id="step%20three%3A%C2%A0%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%B8%8B%E8%BD%BDpdf%C2%A0" name="step%20three%3A%C2%A0%E5%A4%9A%E8%BF%9B%E7%A8%8B%E4%B8%8B%E8%BD%BDpdf%C2%A0">
     step three: 多进程下载pdf
    </h3>
    <p>
     要下载的pdf很多，但股票个数也就五千多个，本人搞了个简单的多线程
    </p>
    <p>
     1 分5个线程，把txt文件分到五个文件夹里，文件夹以0、1、2、3、4命名
    </p>
    <p>
     2 创建下载后放置pdf文件的文件夹，文件夹同样以0、1、3、4命名
    </p>
    <p>
     3 写代码，执行。等待执行完毕，整个过程十几小时以上，看电脑配置和网络情况。
    </p>
    <pre><code class="hljs">import os,threading

'''
多线程下载财报
'''

# 创建多个线程
def temp_thread():
    threads = []
    for i in range(5):
        thread = threading.Thread(target=temp_005,args=(i,))
        threads.append(thread)
        thread.start()
        pass
    for thread in threads:
        thread.join()
    print('all thread finished')
    pass


# 执行项
def temp_005(i):
    import requests
    pre_dir = r'E:/temp006/'+str(i)+'/'
    tar_dir = r'E:/temp007/'+str(i)+'/'
    file_list = os.listdir(pre_dir)
    for file_one in file_list:
        ticker = file_one[0:6]
        tar_dir00 = tar_dir + ticker + os.path.sep
        if not os.path.exists(tar_dir00):
            os.mkdir(tar_dir00)
        url_file_path = pre_dir + file_one
        with open(url_file_path,'r') as fr:
            url_str = fr.read()
        url_list = url_str.split('\n')
        try:
            for url_one in url_list:
                tar_file_name00 = url_one.split('/')
                tar_file_name = f"{tar_file_name00[-2]}_{tar_file_name00[-1]}.pdf"
                try:
                    res = requests.get(url_one)
                    if res.status_code == 200:
                        with open(tar_dir00+tar_file_name,'wb') as fw:
                            fw.write(res.content)
                        pass
                    else:
                        error_str = f'下载失败，状态码：{res.status_code}。{url_one}\n'
                        with open(f'./{i}.txt','a',encoding='utf-8') as fw:
                            fw.write(error_str)
                except:
                    print(url_one)
                pass
            pass
        except:
            print(ticker)
        pass
    print('----------------end---',i)
    pass


if __name__ == '__main__':
    temp_thread()
    pass</code></pre>
    <p>
     执行完毕后，研究历年财报再也不用一个个下载啦，哈哈哈
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f33373936373635322f:61727469636c652f64657461696c732f313436323931343137" class_="artid" style="display:none">
 </p>
</div>


