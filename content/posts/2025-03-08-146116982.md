---
layout: post
title: "TokenSentencePiece论文阅读-大模型中主流的分词算法"
date: 2025-03-08 15:27:37 +0800
description: "Token：SentencePiece论文阅读--大模型中主流的分词算法"
keywords: "Token：SentencePiece论文阅读--大模型中主流的分词算法"
categories: ['Llm']
tags: ['语言模型', '论文阅读', '算法', '大语言模型基础', '人工智能']
artid: "146116982"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146116982
    alt: "TokenSentencePiece论文阅读-大模型中主流的分词算法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146116982
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146116982
cover: https://bing.ee123.net/img/rand?artid=146116982
image: https://bing.ee123.net/img/rand?artid=146116982
img: https://bing.ee123.net/img/rand?artid=146116982
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Token：SentencePiece论文阅读--大模型中主流的分词算法
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <hr/>
    <blockquote>
     <p>
      更多内容：
      <a href="https://flowus.cn/xiaoj_share/share/2763da97-9b40-4939-8451-17c16cd80276?code=VK2AMB" rel="nofollow">
       XiaoJ的知识星球
      </a>
     </p>
    </blockquote>
    <hr/>
    <p>
    </p>
    <p>
    </p>
    <hr/>
    <h2>
     <a id="1_9">
     </a>
     1.系统概述
    </h2>
    <p>
     SentencePiece：一种简单且独立于语言的文本分词器和去分词器，主要用于基于神经网络的文本生成系统。
    </p>
    <p>
     SentencePiece 实现了两种子词分割算法，字节对编码 BPE 和 unigram 语言模型，并扩展了原始句子的直接训练。
    </p>
    <p>
     深度神经网络对自然语言处理产生了巨大影响。神经机器翻译 （NMT）可以利用神经网络通过简单的端到端架构直接执行翻译。
    </p>
    <h3>
     <a id="1SentencePiece_21">
     </a>
     1）SentencePiece组件
    </h3>
    <p>
     SentencePiece 包括 4 个主要组件： Normalizer、Trainer、Encoder 和 Decoder。
    </p>
    <ul>
     <li>
      <p>
       Normalizer模块，用于将语义等效的 Unicode 字符规范化为规范形式。
      </p>
     </li>
     <li>
      <p>
       Trainer 从规范化语料库训练子词分割模型。我们指定一种类型的子词模型作为 Trainer 的参数。
      </p>
     </li>
     <li>
      <p>
       Encoder 在内部执行 Normalizer 来规范化输入文本，并使用 Trainer 训练的子词模型将其分词为子词序列。
      </p>
     </li>
     <li>
      <p>
       Decoder 将子词序列转换为规范化文本。
      </p>
     </li>
    </ul>
    <h3>
     <a id="2SentencePiece_35">
     </a>
     2）SentencePiece使用示例
    </h3>
    <p>
     SentencePiece端到端示例： training (spm_train), encoding (spm_encode), and decoding (spm_decode)
    </p>
    <p>
     输入文本是通过 spm_encode 和 spm_decode 进行可逆转换的。
    </p>
    <p>
     图 1：SentencePiece 的命令行用法
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/66567a5ba6114bb4a0d73817b831c1ef.png"/>
    </p>
    <hr/>
    <h2>
     <a id="2_46">
     </a>
     2.设计和实现
    </h2>
    <p>
     本节介绍包含命令行和代码片段的 SentencePiece 的设计和实现细节。
    </p>
    <h3>
     <a id="1_lossless_tokenization_52">
     </a>
     1） 无损分词化（lossless tokenization）
    </h3>
    <h4>
     <a id="1_54">
     </a>
     （1）依赖于语言
    </h4>
    <p>
     原始句子和分词化句子示例，依赖于语言的预处理。
    </p>
    <ul>
     <li>
      <p>
       Raw text: Hello world.
      </p>
     </li>
     <li>
      <p>
       Tokenized: [Hello] [world] [.]
      </p>
     </li>
    </ul>
    <p>
     “world” 和 “.” 之间不存在空格的信息不会保留在分词序列中。Detokenization 是从 tokenized 序列中恢复原始原始输入的过程，由于这些不可逆的作，它必须依赖于语言。
    </p>
    <p>
     例如，虽然 在大多数欧洲语言中，detokenizer 通常在原始标记之间放置空格，但在日语和中文中不需要空格。
    </p>
    <ul>
     <li>
      <p>
       Raw text: [こんにちは世界。] (Hello world.)
      </p>
     </li>
     <li>
      <p>
       Tokenized: [こんにちは] [世界] [。]
      </p>
     </li>
    </ul>
    <p>
     这种特定于语言的处理通常是在手动构建的规则中实现的，这些规则的编写和维护成本很高。
    </p>
    <h4>
     <a id="2lossless_tokenization_74">
     </a>
     （2）无损分词化（lossless tokenization）
    </h4>
    <p>
     SentencePiece 将 Decoder 实现为 Encoder 的 逆运算，即：
    </p>
    <p>
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        D 
        
       
         e 
        
       
         c 
        
       
         o 
        
       
         d 
        
       
         e 
        
       
         ( 
        
       
         E 
        
       
         n 
        
       
         c 
        
       
         o 
        
       
         d 
        
       
         e 
        
       
         ( 
        
       
         N 
        
       
         o 
        
       
         r 
        
       
         m 
        
       
         a 
        
       
         l 
        
       
         i 
        
       
         z 
        
       
         e 
        
       
         ( 
        
       
         t 
        
       
         e 
        
       
         x 
        
       
         t 
        
       
         ) 
        
       
         ) 
        
       
         ) 
        
       
         = 
        
       
         N 
        
       
         o 
        
       
         r 
        
       
         m 
        
       
         a 
        
       
         l 
        
       
         i 
        
       
         z 
        
       
         e 
        
       
         ( 
        
       
         t 
        
       
         e 
        
       
         x 
        
       
         t 
        
       
         ) 
        
       
         . 
        
       
      
        Decode(Encode(Normalize(text))) = Normalize(text).
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1em; vertical-align: -0.25em;">
         </span>
         <span class="mord mathnormal">
          Deco
         </span>
         <span class="mord mathnormal">
          d
         </span>
         <span class="mord mathnormal">
          e
         </span>
         <span class="mopen">
          (
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0576em;">
          E
         </span>
         <span class="mord mathnormal">
          n
         </span>
         <span class="mord mathnormal">
          co
         </span>
         <span class="mord mathnormal">
          d
         </span>
         <span class="mord mathnormal">
          e
         </span>
         <span class="mopen">
          (
         </span>
         <span class="mord mathnormal" style="margin-right: 0.109em;">
          N
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0278em;">
          or
         </span>
         <span class="mord mathnormal">
          ma
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0197em;">
          l
         </span>
         <span class="mord mathnormal">
          i
         </span>
         <span class="mord mathnormal">
          ze
         </span>
         <span class="mopen">
          (
         </span>
         <span class="mord mathnormal">
          t
         </span>
         <span class="mord mathnormal">
          e
         </span>
         <span class="mord mathnormal">
          x
         </span>
         <span class="mord mathnormal">
          t
         </span>
         <span class="mclose">
          )))
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
         <span class="mrel">
          =
         </span>
         <span class="mspace" style="margin-right: 0.2778em;">
         </span>
        </span>
        <span class="base">
         <span class="strut" style="height: 1em; vertical-align: -0.25em;">
         </span>
         <span class="mord mathnormal" style="margin-right: 0.109em;">
          N
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0278em;">
          or
         </span>
         <span class="mord mathnormal">
          ma
         </span>
         <span class="mord mathnormal" style="margin-right: 0.0197em;">
          l
         </span>
         <span class="mord mathnormal">
          i
         </span>
         <span class="mord mathnormal">
          ze
         </span>
         <span class="mopen">
          (
         </span>
         <span class="mord mathnormal">
          t
         </span>
         <span class="mord mathnormal">
          e
         </span>
         <span class="mord mathnormal">
          x
         </span>
         <span class="mord mathnormal">
          t
         </span>
         <span class="mclose">
          )
         </span>
         <span class="mord">
          .
         </span>
        </span>
       </span>
      </span>
     </span>
    </p>
    <p>
     这种设计称为
     <strong>
      无损分词化（lossless tokenization）
     </strong>
     ，其中再现规范化文本的所有信息都保留在编码器的输出中。
    </p>
    <p>
     无损分词的基本思想是将输入文本视为 Unicode 字符序列。甚至空格也被作为普通符号处理。
    </p>
    <p>
     为了清楚起见，SentencePiece 首先用元符号 _ （U+2581） 转义空格，并将输入分词化为任意子词序列，例如：
    </p>
    <ul>
     <li>
      <p>
       Raw text: Hello_world.
      </p>
     </li>
     <li>
      <p>
       Tokenized: [Hello] [_wor] [ld] [.]
      </p>
     </li>
    </ul>
    <p>
     由于空格保留在Tokenized文本中，因此可对Tokenized文本进行去分词化（detokenize），而不产生歧义。
    </p>
    <p>
     应该注意的是，subword-nmt 对子词单元采用不同的表示形式。它侧重于如何将单词分割成子词，并使用 @@ 作为单词内边界标记。
    </p>
    <ul>
     <li>
      Tokenized: [Hello] [wor] [@@ld] [@@.]
     </li>
    </ul>
    <p>
     这种表示形式并不总是执行无损分词化，因为在处理空格时仍然存在歧义。更具体地说，不能用这种表示形式对连续的空格进行编码。
    </p>
    <h3>
     <a id="2__104">
     </a>
     2） 高效的子词训练和分割
    </h3>
    <p>
     现有的子词分割工具从预先分词化的句子中训练子词模型。引入这种预分词化是为了有效的子词训练。但不能总是假设预分词化可用，尤其是对于非分段语言。此外，预分词化使得执行无损分词变得困难。
    </p>
    <p>
     SentencePiece 采用多种加速技术进行训练和分割，以使用大量原始数据进行无损标记化。
    </p>
    <p>
     例如，给定一个长度为 N 的输入句子（或单词），在每次迭代中天真地扫描这对符号时，BPE 分割需要 O（N ） 计算成本。SentencePiece 采用 O（N log（N ）） 算法，其中合并的符号由二进制堆（优先级队列）管理。此外，单元语法语言模型的训练和分割复杂性与输入数据的大小呈线性关系。
    </p>
    <h3>
     <a id="3__ID__116">
     </a>
     3） 词汇 ID 管理
    </h3>
    <p>
     SentencePiece 管理词汇表到 id 的映射，以直接将输入文本转换为 id 序列，反之亦然。
    </p>
    <p>
     SentencePiece 为特殊元符号保留词汇 ID，例如：未知符号(), BOS(
     <s>
      ), EOS(
     </s>
     ), padding ()。它们的实际 ID 配置了命令行标志。
    </p>
    <p>
     我们还可以定义自定义元符号，将上下文信息编码为虚拟令牌。示例包括语言指标 &lt;2ja&gt; 和 &lt;2de&gt;表示多语言。
    </p>
    <h3>
     <a id="4_126">
     </a>
     4）可自定义的字符规范化
    </h3>
    <p>
     字符规范化是处理现实世界文本的一个重要预处理步骤，其中涉及语义等效的 Unicode 字符。
    </p>
    <p>
     Unicode 标准规范化形式，如 NFC（规范组合形式）和 NFKC（兼容性分解及规范组合形式），因其更好的可重复性以及作为 Unicode 标准得到广泛使用。
    </p>
    <p>
     默认情况下，SentencePiece 使用 Unicode NFKC 规范化对输入文本进行规范化。规范化规则通过
     <code>
      spm_train
     </code>
     的
     <code>
      --normalization_rule_name=nfkc
     </code>
     标志来指定。SentencePiece 中的规范化是通过字符串到字符串的映射以及最左最长匹配来实现的。规范化规则被编译成一个有限状态转换器（Aho - Corasick 自动机）以高效地执行规范化操作。
    </p>
    <p>
     SentencePiece 支持自定义规范化规则，以 TSV 文件定义，通过特定标志指定，特定任务规则可扩展默认 NFKC 规则。
    </p>
    <h3>
     <a id="5_138">
     </a>
     5）自包含模型
    </h3>
    <p>
     严格来说，NFKC 规范化可能会产生不同的结果，具体取决于 Unicode 版本。
    </p>
    <p>
     理想情况下，所有预处理的
     <strong>
      规则
     </strong>
     和
     <strong>
      参数
     </strong>
     必须以自包含的方式嵌入到模型文件中，这样只要我们使用相同的模型文件，我们就可以重现相同的实验设置。
    </p>
    <p>
     SentencePiece 模型设计为完全自包含的。模型文件不仅包括词汇表和分割参数，还包括用于字符规范化的预编译有限状态传感器。SentencePiece 的行为仅由模型文件决定，没有外部依赖项。这种设计保证了完美的可重复性。
    </p>
    <h3>
     <a id="6_API_148">
     </a>
     6）用于动态处理的库 API
    </h3>
    <p>
     文本预处理一般是离线处理，在 NMT 训练前用独立预处理器将原始输入转为 ID 序列，但存在两大问题：一是独立工具难集成到需实时处理用户输入的 NMT 应用；二是离线处理不利于子句级数据增强和噪声注入。这些技术对于提高NMT模型的准确性和鲁棒性至关重要。
    </p>
    <p>
     为了解决这些问题，SentencePiece不仅提供了离线预处理的命令行工具，还支持C++、Python和TensorFlow库API，实现实时处理，能够轻松集成到现有的NMT框架中。
    </p>
    <p>
     此外，SentencePiece还实现了子词正则化，通过动态采样和噪声注入来增强训练数据，进一步提升NMT模型的性能。
    </p>
    <p>
     图3: C++ API的使用（与图1相同）
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f9cd8fe2e7b74102a700e058c6319b2a.png"/>
    </p>
    <p>
     图4: Python API的使用（与图1相同）
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/1a9aceb167d547a6b235ffe5cb4ee3b5.png"/>
    </p>
    <p>
     图5：TensorFlow API的使用
     <br/>
     SentencePiece模型（模型协议）是TensorFlow操作的一个属性，并嵌入到TensorFlow图中，从而使模型和图完全自包含。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/622536df3f5d475d90092f2c8934e3c5.png"/>
    </p>
    <p>
     图 6 显示了用于子词正则化的 Python 示例代码，其中根据 unigram 语言模型对一个子词序列进行采样。我们可以发现文本“New York”的标记化方式不同。
    </p>
    <p>
     图 6：使用 Python API 进行子词采样
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/dbcc4c9c27b648b89946b17a7dc06848.png"/>
    </p>
    <hr/>
    <h2>
     <a id="3_179">
     </a>
     3.总结
    </h2>
    <p>
     本文介绍了SentencePiece，一个用于神经网络文本处理的开源子词分词器和解分词器。它能够直接将文本转换为ID序列，实现端到端的处理，无需依赖特定语言的资源。SentencePiece的模型文件是自包含的，确保了处理的可重复性。
    </p>
    <hr/>
    <p>
     SentencePiece参考：
    </p>
    <ul>
     <li>
      <p>
       <a href="https://arxiv.org/abs/1808.06226" rel="nofollow">
        https://arxiv.org/abs/1808.06226
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="https://github.com/google/sentencepiece">
        https://github.com/google/sentencepiece
       </a>
      </p>
     </li>
    </ul>
    <p>
     BPE参考：
    </p>
    <ul>
     <li>
      <p>
       Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proc. of ACL.
      </p>
     </li>
     <li>
      <p>
       <a href="https://aclanthology.org/P16-1162/" rel="nofollow">
        https://aclanthology.org/P16-1162/
       </a>
      </p>
     </li>
     <li>
      <p>
       <a href="https://aclanthology.org/P16-1162.pdf" rel="nofollow">
        https://aclanthology.org/P16-1162.pdf
       </a>
      </p>
     </li>
    </ul>
    <hr/>
    <blockquote>
     <p>
      声明：资源可能存在第三方来源，若有侵权请联系删除！
     </p>
    </blockquote>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34383236373130342f:61727469636c652f64657461696c732f313436313136393832" class_="artid" style="display:none">
 </p>
</div>


