---
layout: post
title: "ä»é›¶å®ç°-LLMä¸ŠåŸç†è®²é€-æœ€å°å¯è¿è¡Œ-GPT"
date: 2025-09-03T11:36:48+0800
description: "è¿™ç¯‡æ–‡ç« ä»é›¶å®ç°ä¸€ä¸ªå¯è¿è¡Œçš„ mini-GPTï¼Œç”¨é€šä¿—è§£é‡Šå’Œä»£ç ç¤ºä¾‹å¸¦ä½ ç†è§£åˆ†è¯ã€è®­ç»ƒå’Œç”Ÿæˆï¼Œè®©é›¶åŸºç¡€ä¹Ÿèƒ½ä¸Šæ‰‹è®­ç»ƒå±äºè‡ªå·±çš„å°å‹è¯­è¨€æ¨¡å‹ã€‚"
keywords: "ä»é›¶å®ç° LLMï¼ˆä¸Šï¼‰ï¼šåŸç†è®²é€ + æœ€å°å¯è¿è¡Œ GPT"
categories: ['æœªåˆ†ç±»']
tags: ['äººå·¥æ™ºèƒ½', 'Python', 'Gpt']
artid: "151143409"
arturl: "https://blog.csdn.net/qq_41865545/article/details/151143409"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151143409
    alt: "ä»é›¶å®ç°-LLMä¸ŠåŸç†è®²é€-æœ€å°å¯è¿è¡Œ-GPT"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151143409
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151143409
cover: https://bing.ee123.net/img/rand?artid=151143409
image: https://bing.ee123.net/img/rand?artid=151143409
img: https://bing.ee123.net/img/rand?artid=151143409
---



# ä»é›¶å®ç° LLMï¼ˆä¸Šï¼‰ï¼šåŸç†è®²é€ + æœ€å°å¯è¿è¡Œ GPT



#### å¼•è¨€

##### ä¸ºä»€ä¹ˆè¦å­¦ä¹  `LLM`ï¼Ÿ

å½“ä½ å’Œ `ChatGPT` å¯¹è¯æ—¶ï¼Œå®ƒä¸ä»…èƒ½å›ç­”ä½ çš„é—®é¢˜ï¼Œè¿˜èƒ½ç»­å†™æ•…äº‹ã€è®°ä½ä¸Šä¸‹æ–‡ï¼Œç”šè‡³è°ƒæ•´é£æ ¼ã€‚ä½ å¯èƒ½ä¼šæƒ³ï¼š**å®ƒæ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Ÿ**

ç­”æ¡ˆå°±æ˜¯ï¼š**å¤§è¯­è¨€æ¨¡å‹ï¼ˆ`Large Language Model`, `LLM`ï¼‰**ã€‚è¿‘å‡ å¹´ï¼Œä» `ChatGPT` åˆ° `Claude`ï¼Œä»æ–‡å¿ƒä¸€è¨€åˆ°é€šä¹‰åƒé—®ï¼Œä» `DeepSeek` åˆ° `QWen`ï¼Œå‡ ä¹æ‰€æœ‰æ–°ä¸€ä»£ `AI` äº§å“éƒ½ç¦»ä¸å¼€å®ƒã€‚

ä½†å¾ˆå¤šå­¦ä¹ è€…ä¼šæœ‰ç–‘é—®ï¼š

* **`LLM` å¤ªå¤§äº†ï¼Œæˆ‘æ˜¯ä¸æ˜¯ç©ä¸èµ·ï¼Ÿ**

  å…¶å®ä¸ç”¨ã€‚æˆ‘ä»¬ä¸ä¼šä¸Šæ¥å°±ç ”ç©¶ 1000 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œè€Œæ˜¯ç”¨ä¸€ä¸ªå‡ åä¸‡å‚æ•°çš„ **`mini-GPT`** â€”â€” å®ƒä¿ç•™äº†æ ¸å¿ƒæœºåˆ¶ï¼ˆ`Tokenizer`ã€`Attention`ã€`Context Window`ã€é‡‡æ ·ç­–ç•¥ï¼‰ï¼Œåªéœ€å‡ ç™¾è¡Œä»£ç ï¼Œå°±èƒ½åœ¨ `Google Colab` è·‘èµ·æ¥ã€‚
* **ä»£ç ä¼šä¸ä¼šå¤æ‚å¾—çœ‹ä¸æ‡‚ï¼Ÿ**

  ä¸ä¼šã€‚æˆ‘ä¼šé€æ­¥æ‹†å¼€ï¼Œä¸€è¾¹è®²æ¦‚å¿µï¼Œä¸€è¾¹å†™ä»£ç ã€‚
* **è·‘é€šè¿™ä¸ª `demo` æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ**

  å› ä¸ºå®ƒèƒ½è®©ä½ çœŸæ­£ç†è§£ï¼š

  + ä¸ºä»€ä¹ˆ `LLM` èƒ½ç»­å†™æ–‡æœ¬ï¼Ÿ
  + ä¸ºä»€ä¹ˆå®ƒèƒ½â€œè®°ä½â€ä¸Šä¸‹æ–‡ï¼Ÿ
  + ä¸ºä»€ä¹ˆä¼šå‡ºç°â€œå¤è¯»æœºâ€ï¼Ÿ
  + è°ƒæ•´ `Temperature` / `Top-k` / `Top-p` æ—¶ï¼Œä¸ºä»€ä¹ˆé£æ ¼å®Œå…¨ä¸åŒï¼Ÿ

```mermaid
flowchart TD
    A[æ–‡æœ¬è¾“å…¥] --> B[åˆ†è¯å™¨]
    B --> C[Token åºåˆ—ï¼ˆæ•°å­—IDï¼‰]
    C --> D[Embedding å±‚]
    D --> E[Transformer å— Ã—N]
    E --> F[å±‚å½’ä¸€åŒ– LayerNorm]
    F --> G[çº¿æ€§è¾“å‡ºå±‚ Head]
    G --> H[Softmax æ¦‚ç‡åˆ†å¸ƒ]
    H --> I[é¢„æµ‹ä¸‹ä¸€ä¸ª Token]
    I --> J[æ‹¼æ¥åˆ°å·²æœ‰åºåˆ—]
    J --> K[ç”Ÿæˆæ–‡æœ¬è¾“å‡º]
    K -->|ç›´åˆ°åœæ­¢æ¡ä»¶| I

```

##### æœ¬æ–‡ç›®æ ‡

è¿™ç¯‡æ–‡ç« ä¼šå¸¦ä½ å®Œæˆä»¥ä¸‹ç›®æ ‡ï¼š

1. **ä»é›¶åŸºç¡€å…¥é—¨**ï¼šç”¨æœ€ç›´è§‚çš„æ–¹å¼è§£é‡Š `LLM` å…³é”®æ¦‚å¿µã€‚
2. **ä»£ç é€æ­¥å®ç°**ï¼šä¸€æ­¥æ­¥æ„å»º `tokenizer`ã€`Transformer`ã€è®­ç»ƒå¾ªç¯ã€ç”Ÿæˆå‡½æ•°ã€‚
3. **è·‘é€šä¸€ä¸ª `demo LLM`**ï¼šåœ¨ `Google Colab` ä¸Šå®é™…è®­ç»ƒå¹¶ç”Ÿæˆæ–‡æœ¬ã€‚
4. **ç†è§£å¸¸è§é—®é¢˜**ï¼šä¸ºä»€ä¹ˆ `val_loss` å¾ˆé«˜ï¼Ÿä¸ºä»€ä¹ˆç»“æœä¼šå¤è¯»ï¼Ÿ
5. **å­¦ä¼šæ‰©å±•**ï¼šå¦‚ä½•ä» `demo` èµ°å‘æ›´å¤§ã€æ›´å®ç”¨çš„æ¨¡å‹ã€‚

##### é€‚åˆäººç¾¤

* **é›¶åŸºç¡€è¯»è€…**ï¼šä½ åªéœ€è¦ä¸€ç‚¹ç‚¹ `Python` åŸºç¡€ï¼Œå°±èƒ½çœ‹æ‡‚å¹¶è·‘èµ·æ¥ã€‚
* **æœ‰ç»éªŒçš„å¼€å‘è€…**ï¼šä½ èƒ½æ·±å…¥ç†è§£ä»£ç å®ç°çš„ç»†èŠ‚ï¼Œæ˜ç™½ `LLM` çš„å†…éƒ¨æœºåˆ¶ã€‚
* **ç ”ç©¶è€…/çˆ±å¥½è€…**ï¼šä½ èƒ½å¾—åˆ°ä¸€ä¸ªå¯æ‰©å±•çš„ `mini-GPT` æ¡†æ¶ï¼Œä½œä¸ºæ›´å¤§å®éªŒçš„èµ·ç‚¹ã€‚

---

#### ä¸€ã€`LLM` åŸºç¡€æ¦‚å¿µ

åœ¨å†™ä»£ç ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆæŠŠä¸€äº›â€œå…³é”®è¯â€è§£é‡Šæ¸…æ¥šã€‚ä½ ä¼šå‘ç°ï¼Œ`LLM` çš„æ ¸å¿ƒæ€æƒ³å…¶å®å¹¶ä¸å¤æ‚ã€‚

##### 1.1 `Token` ä¸åˆ†è¯ï¼ˆ`Tokenization`ï¼‰

`LLM` å¹¶ä¸ç›´æ¥ç†è§£â€œæ±‰å­—â€æˆ–â€œè‹±è¯­å•è¯â€ï¼Œå®ƒçœ‹åˆ°çš„åªæ˜¯ **ä¸€ä¸²æ•°å­—**ã€‚ è¿™äº›æ•°å­—çš„æœ€å°å•ä½å°±å« **`Token`**ï¼Œå®ƒå¯èƒ½æ˜¯ä¸€ä¸ªå­—æ¯ã€ä¸€ä¸ªæ±‰å­—ã€ä¸€ä¸ªå­è¯ï¼Œç”šè‡³ä¸€ä¸ªå®Œæ•´çš„å•è¯ã€‚

åˆ†è¯ï¼ˆ`Tokenization`ï¼‰å°±æ˜¯æŠŠæ–‡å­—åˆ‡åˆ†æˆ `token`ï¼Œå†è½¬æˆæ•°å­—ã€‚æ‰“ä¸ªæ¯”æ–¹ï¼šå¦‚æœä¸€å¥è¯æ˜¯æ–‡ç« ï¼Œé‚£ä¹ˆ `token` å°±æ˜¯â€œä¹é«˜ç§¯æœ¨â€ï¼Œæ¨¡å‹å°±æ˜¯å­¦ä¼šå¦‚ä½•æŠŠè¿™äº›ç§¯æœ¨æ‹¼èµ·æ¥ã€‚

* **å­—ç¬¦çº§åˆ†è¯**ï¼š`H e l l o` â†’ [6, 15, 22, 22, 25]
* **`BPE` åˆ†è¯**ï¼š`Hello` â†’ [23]ï¼ˆä¸€ä¸ªå­è¯æå®šï¼‰

**ä¸¤ç§å¸¸è§åˆ†è¯æ–¹å¼ï¼š**

1. **å­—ç¬¦çº§ï¼ˆ`char-level`ï¼‰**ï¼šç›´æ¥æŠŠæ¯ä¸ªå­—ç¬¦å½“æˆä¸€ä¸ª `token`ã€‚

   **ä¼˜ç‚¹**ï¼šå®ç°ç®€å•ï¼Œè¯­è¨€æ— å…³ã€‚

   **ç¼ºç‚¹**ï¼šåºåˆ—å¾ˆé•¿ï¼Œè®­ç»ƒéš¾åº¦å¤§ã€‚
2. **`BPE`ï¼ˆå­è¯çº§ï¼‰**ï¼šä»æœ€å°çš„å­—ç¬¦å¼€å§‹ï¼Œé€æ­¥åˆå¹¶å¸¸è§çš„å­ä¸²ï¼Œå½¢æˆå­è¯ã€‚

   **ä¼˜ç‚¹**ï¼šåºåˆ—æ›´çŸ­ï¼Œè®­ç»ƒæ›´å¿«ï¼Œæ•ˆæœæ›´å¥½ã€‚

   **ç¼ºç‚¹**ï¼šéœ€è¦è®­ç»ƒä¸€ä¸ªåˆ†è¯æ¨¡å‹ï¼ˆæˆ‘ä»¬ç”¨ `sentencepiece` å®ç°ï¼‰ã€‚

åœ¨æˆ‘ä»¬çš„ä»£ç é‡Œï¼Œå¯ä»¥é€‰æ‹© `tokenizer_mode = 'char'` æˆ– `'bpe'`ã€‚`BPE` æ›´æ¨èï¼Œèƒ½æ›´å¿«æ”¶æ•›ï¼Œä¹Ÿèƒ½ç”Ÿæˆæ›´è‡ªç„¶çš„å¥å­ã€‚

##### 1.2 ä¸Šä¸‹æ–‡çª—å£ï¼ˆ`Context Window`ï¼‰

`LLM` ä¸€æ¬¡ä¸èƒ½çœ‹å…¨ç¯‡æ–‡ç« ï¼Œå®ƒæœ‰ä¸€ä¸ªâ€œçŸ­æœŸè®°å¿†â€ï¼Œå« **ä¸Šä¸‹æ–‡çª—å£** (`block_size`)ã€‚æ¯”å¦‚ `block_size=64`ï¼Œå°±è¡¨ç¤ºæ¨¡å‹æœ€å¤šèƒ½è®°ä½æœ€è¿‘ 64 ä¸ª `token`ï¼Œå†å¾€å‰çš„å°±å¿˜äº†ã€‚

* çª—å£è¶Šå¤§ â†’ èƒ½å¤„ç†æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä½†è®­ç»ƒæ›´æ…¢ã€æ›´å æ˜¾å­˜ã€‚
* å°å®éªŒæ—¶å»ºè®® 32~64ã€‚
* åœ¨ä»£ç é‡Œï¼Œ`block_size` å°±æ˜¯è¿™ä¸ªâ€œè®°å¿†åŠ›â€çš„å¤§å°ã€‚

##### 1.3 é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆ`Next Token Prediction`ï¼‰

`LLM` çš„è®­ç»ƒç›®æ ‡éå¸¸ç®€å•ï¼šç»™å®šå‰é¢çš„ `token`ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª `token` çš„æ¦‚ç‡åˆ†å¸ƒã€‚

ä¾‹å­ï¼šè¾“å…¥ `"The quick brown"`ï¼Œæ¨¡å‹å¤§æ¦‚ç‡ä¼šé¢„æµ‹ `"fox"`ã€‚  
 è¿™å°±æ˜¯ **`Next Token Prediction`** â€”â€” ä¸æ–­é‡å¤è¿™ä¸ªæ¸¸æˆï¼Œæ¨¡å‹å°±å­¦ä¼šäº†è¯­è¨€è§„å¾‹ã€‚

![topk_topp_demo](https://i-blog.csdnimg.cn/img_convert/28ddc4ce9a3d788f460bcde18f6f3b7a.png)

##### 1.4 `Transformer` æ¶æ„

`mini-GPT` å°±æ˜¯ä¸€ä¸ªç®€åŒ–çš„ `Transformer` è§£ç å™¨ï¼Œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š

1. **`Embedding` å±‚**ï¼šæŠŠ `token id` å˜æˆå‘é‡ï¼Œå¹¶åŠ ä¸Šä½ç½®ä¿¡æ¯ã€‚
2. **è‡ªæ³¨æ„åŠ›ï¼ˆ`Self-Attention`ï¼‰**ï¼š
   * æ¯ä¸ª `token` å¯ä»¥â€œå…³æ³¨â€å‰é¢çš„ `token`ï¼›
   * æœ‰ **å› æœé®ç½©ï¼ˆ`Causal Mask`ï¼‰**ï¼Œç¡®ä¿åªçœ‹è¿‡å»ï¼Œä¸çœ‹æœªæ¥ã€‚
3. **å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥ + `LayerNorm`**ï¼šå¢åŠ è¡¨è¾¾èƒ½åŠ›ï¼Œä¿è¯è®­ç»ƒç¨³å®šã€‚

æœ€åè¾“å‡ºå±‚ `head` ç»™å‡ºä¸‹ä¸€ä¸ª `token` çš„æ¦‚ç‡åˆ†å¸ƒã€‚

##### 1.5 ç”Ÿæˆç­–ç•¥ï¼ˆ`Sampling`ï¼‰

è®­ç»ƒå®Œæ¨¡å‹åï¼Œè¦è®©å®ƒâ€œè¯´è¯â€ã€‚æ­¤æ—¶å®ƒä¼šè¾“å‡ºä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸åŒæ–¹å¼æ¥â€œæŠ½ç­¾â€ï¼š

* **`Temperature`ï¼ˆæ¸©åº¦ï¼‰**ï¼šæ§åˆ¶éšæœºæ€§ã€‚
  + å°äº 1 â†’ æ›´ä¿å®ˆã€ç¡®å®šæ€§å¼ºã€‚
  + å¤§äº 1 â†’ æ›´æœ‰åˆ›é€ æ€§ã€‚
* **`Top-k`**ï¼šåªåœ¨æ¦‚ç‡æœ€é«˜çš„ `k` ä¸ªè¯ä¸­æŒ‘ã€‚
* **`Top-p`ï¼ˆæ ¸é‡‡æ ·ï¼‰**ï¼šåŠ¨æ€é€‰æ‹©ç´¯è®¡æ¦‚ç‡ â‰¤ `p` çš„å€™é€‰ï¼Œæ›´è‡ªç„¶ã€‚
* **é¢‘ç‡æƒ©ç½šï¼ˆ`frequency penalty`ï¼‰**ï¼šè¯å‡ºç°è¶Šå¤šï¼Œä¸‹æ¬¡è¶Šéš¾é€‰ï¼Œé˜²æ­¢å¤è¯»ã€‚
* **å‡ºç°æƒ©ç½šï¼ˆ`presence penalty`ï¼‰**ï¼šåªè¦å‡ºç°è¿‡ï¼Œå°±æ‰£åˆ†ï¼Œé¼“åŠ±æ¢è¯é¢˜ã€‚
* **åœæ­¢æ¡ä»¶ / æœ€å¤§é•¿åº¦**ï¼šé¿å…æ¨¡å‹â€œä¸€ç›´è¯´ä¸‹å»â€ã€‚

åœ¨æˆ‘ä»¬çš„ `generate()` å‡½æ•°é‡Œï¼Œå¯ä»¥é€šè¿‡è®¾ç½®è¿™äº›å‚æ•°ï¼Œç›´æ¥çœ‹åˆ°ç”Ÿæˆé£æ ¼çš„å˜åŒ–ã€‚

---

#### äºŒã€ä»£ç æ¡†æ¶æ­å»º

ç†è§£äº† `LLM` çš„åŸºæœ¬æ¦‚å¿µä¹‹åï¼Œæˆ‘ä»¬è¦ä»ä»£ç å¼€å§‹åŠ¨æ‰‹ã€‚è¿™é‡Œæˆ‘ä»¬ç”¨ [`Google Colab`](https://colab.research.google.com)ï¼Œå› ä¸ºå®ƒè‡ªå¸¦ `Python` å’Œ `GPU`ï¼Œä¸éœ€è¦å®‰è£…å¤æ‚ç¯å¢ƒï¼Œè¿˜èƒ½å…è´¹ç”¨ `GPU`ï¼Œéå¸¸é€‚åˆå­¦ä¹ å’Œå®éªŒã€‚

![GIF 2-9-2025 ä¸Šåˆ 10-23-48](https://i-blog.csdnimg.cn/img_convert/3e9d4023ffde3876f0f59b03b34cbb1c.gif)

##### 2.1 å‡†å¤‡ç¯å¢ƒï¼ˆå®‰è£…ä¾èµ–ï¼‰

åœ¨ `Colab` æ–°å»ºä¸€ä¸ª `Notebook`ï¼Œè¾“å…¥ï¼š

```bash
!pip install torch sentencepiece

```

è§£é‡Šï¼š

* **`torch`**ï¼š`PyTorch`ï¼Œæ˜¯æˆ‘ä»¬è¦ç”¨çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚
* **`sentencepiece`**ï¼š`Google` å¼€æºçš„åˆ†è¯å·¥å…·ï¼Œç”¨æ¥åš `BPE` åˆ†è¯ã€‚

ğŸ‘‰ å¦‚æœä½ åœ¨æœ¬åœ°è·‘ï¼Œéœ€è¦ **`Python 3.9+`**ï¼Œå¹¶å»ºè®®æœ‰ä¸€å¼ æ”¯æŒ `CUDA` çš„æ˜¾å¡ï¼Œå¦åˆ™é€Ÿåº¦ä¼šæ¯”è¾ƒæ…¢ã€‚

##### 2.2 å‡†å¤‡è¯­æ–™æ–‡ä»¶ï¼ˆ`input.txt`ï¼‰

`LLM` çš„â€œæ•™æâ€å°±æ˜¯è¯­æ–™ã€‚æ•™æè¶Šå¤šã€è¶Šå¤šæ ·ï¼Œæ¨¡å‹å­¦å¾—è¶Šå¥½ã€‚æˆ‘ä»¬å…ˆç”¨ä¸€ä»½å°è¯­æ–™ `input.txt` è¯•è¯•ï¼š

```markdown
Once upon a time, there was a small language model.
It tried to read books, tell stories, and learn from text.
Sometimes it was good, sometimes it was silly.
But every day, it became a little bit better.

The quick brown fox jumps over the lazy dog.
Hello world! This is a simple test of tokenization, context windows, and generation.

è¯—è¨€å¿—ï¼Œæ­Œå’è¨€ã€‚è¯­è¨€æ˜¯äººç±»çš„å·¥å…·ï¼Œä¹Ÿæ˜¯æ€æƒ³çš„è½½ä½“ã€‚
æ¨¡å‹å­¦ä¹ æ–‡å­—ï¼Œå°±åƒå°å­©å­¦è¯´è¯ã€‚

```

ğŸ‘‰ åœ¨ `Colab` é‡Œï¼Œå¯ä»¥ç›´æ¥å†™å…¥æ–‡ä»¶ï¼š

```python
text_data = """Once upon a time, there was a small language model.
It tried to read books, tell stories, and learn from text.
Sometimes it was good, sometimes it was silly.
But every day, it became a little bit better.

The quick brown fox jumps over the lazy dog.
Hello world! This is a simple test of tokenization, context windows, and generation.

è¯—è¨€å¿—ï¼Œæ­Œå’è¨€ã€‚è¯­è¨€æ˜¯äººç±»çš„å·¥å…·ï¼Œä¹Ÿæ˜¯æ€æƒ³çš„è½½ä½“ã€‚
æ¨¡å‹å­¦ä¹ æ–‡å­—ï¼Œå°±åƒå°å­©å­¦è¯´è¯ã€‚
"""

with open("input.txt", "w", encoding="utf-8") as f:
    f.write(text_data)

```

âš ï¸ æé†’ï¼š

* å¦‚æœè¯­æ–™å¤ªçŸ­ï¼Œæ¨¡å‹åªä¼šâ€œèƒŒä¹¦â€ï¼Œè¾“å‡ºå‡ ä¹å’ŒåŸæ–‡ä¸€æ ·ã€‚
* å¦‚æœä½ æ¢æˆå‡ ä¸‡å­—å°è¯´ç‰‡æ®µï¼Œè¾“å‡ºä¼šæ›´çµæ´»ã€æ›´æœ‰åˆ›é€ æ€§ã€‚

##### 2.3 è¯»å–è¯­æ–™å¹¶æ£€æŸ¥

```python
from pathlib import Path

text = Path('input.txt').read_text(encoding='utf-8')
print("è¯­æ–™é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰=", len(text))
print("å¼€å¤´ 200 ä¸ªå­—ç¬¦ï¼š\n", text[:200])

```

è¿è¡Œåä¼šæ‰“å°è¯­æ–™é•¿åº¦å’Œå‰ 200 ä¸ªå­—ç¬¦ï¼Œæ–¹ä¾¿ç¡®è®¤æ–‡ä»¶è¯»å–æˆåŠŸã€‚

![read_text](https://i-blog.csdnimg.cn/img_convert/37cbe8a8749291146b6084cff1ca5dd6.png)

> æˆ‘åœ¨ `Colab` ä¸­ä½¿ç”¨çš„ `input.txt` çš„æ•°æ®æ˜¯ `Aliceâ€™s Adventures in Wonderland`ï¼Œæ‰€ä»¥æ‰“å°å†…å®¹å’Œæˆªå›¾æ˜¯ä¸ä¸€æ ·çš„ï¼Œä¸è¦çº ç»“è¿™ä¸ªç‚¹ã€‚

##### 2.4 å®ç°åˆ†è¯å™¨ï¼ˆ`Tokenizer`ï¼‰

###### 2.4.1 ä¸ºä»€ä¹ˆè¦åˆ†è¯ï¼Ÿ

è®¡ç®—æœºä¸èƒ½ç›´æ¥ç†è§£æ–‡å­—ï¼Œæ‰€ä»¥è¦æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—ã€‚åˆ†è¯å™¨ï¼ˆ`Tokenizer`ï¼‰å°±æ˜¯æŠŠæ–‡æœ¬æ‹†åˆ†æˆ **`token` â†’ æ•°å­— `id`**ï¼ŒåŒæ—¶è¿˜èƒ½æŠŠæ•°å­— `id` è½¬å›æ–‡æœ¬ã€‚

###### 2.4.2 å­—ç¬¦çº§åˆ†è¯ï¼ˆæœ€ç®€å•ï¼‰

æ¯ä¸ªå­—ç¬¦å°±æ˜¯ä¸€ä¸ª `token`ï¼š

```python
# å»ºç«‹å­—è¡¨
chars = sorted(list(set(text)))
stoi = {ch: i for i, ch in enumerate(chars)}  # å­—ç¬¦ -> æ•°å­—
itos = {i: ch for i, ch in enumerate(chars)}  # æ•°å­— -> å­—ç¬¦

# ç¼–ç  / è§£ç å‡½æ•°
def encode(s: str):
    return [stoi[c] for c in s]

def decode(ids: list):
    return "".join([itos[i] for i in ids])

print("å­—ç¬¦è¡¨å¤§å° =", len(chars))
print("encode('Hello') =", encode("Hello"))
print("decode =", decode(encode("Hello")))

```

ç¤ºä¾‹è¾“å‡ºï¼š

```makefile
å­—ç¬¦è¡¨å¤§å° = 68
encode('Hello') = [12, 45, 50, 50, 60]
decode = Hello

```

ğŸ‘‰ å¥½å¤„ï¼šå®ç°ç®€å•ã€‚ç¼ºç‚¹ï¼šåºåˆ—å¾ˆé•¿ï¼Œè®­ç»ƒä¼šæ›´éš¾ã€‚

###### 2.4.3 `BPE` åˆ†è¯ï¼ˆæ›´é«˜æ•ˆï¼‰

å¦‚æœè¯­æ–™è¾ƒå¤§ï¼Œç”¨ `BPE`ï¼ˆ`Byte Pair Encoding`ï¼‰èƒ½æ›´å¥½åœ°å‹ç¼©åºåˆ—ï¼š

```python
import sentencepiece as spm

# è®­ç»ƒä¸€ä¸ª BPE æ¨¡å‹ï¼ˆè¯è¡¨å¤§å°è®¾ä¸º 200ï¼Œé€‚åˆå°è¯­æ–™ï¼‰
spm.SentencePieceTrainer.train(
    input="input.txt",
    model_prefix="spm_bpe",
    vocab_size=200,
    model_type="bpe",
    character_coverage=1.0,
    bos_id=-1, eos_id=-1, unk_id=0, pad_id=-1,
    hard_vocab_limit=False
)

# åŠ è½½æ¨¡å‹
sp = spm.SentencePieceProcessor(model_file="spm_bpe.model")

# å®šä¹‰ç¼–ç /è§£ç å‡½æ•°
def encode_bpe(s: str):
    return sp.encode(s, out_type=int)

def decode_bpe(ids: list):
    return sp.decode(ids)

print("BPE è¯è¡¨å¤§å° =", sp.get_piece_size())
print("encode_bpe('Hello world') =", encode_bpe("Hello world"))
print("decode_bpe =", decode_bpe(encode_bpe("Hello world")))

```

ç¤ºä¾‹è¾“å‡ºï¼š

```makefile
BPE è¯è¡¨å¤§å° = 200
encode_bpe('Hello world') = [35, 78, 42]
decode_bpe = Hello world

```

ğŸ‘‰ å¥½å¤„ï¼šåºåˆ—æ›´çŸ­ï¼Œè®­ç»ƒæ›´å¿«ï¼Œæ•ˆæœæ›´å¥½ã€‚

**å¯¹æ¯”**ï¼š

* å­—ç¬¦çº§ `"Hello world"` â†’ 11 ä¸ª `token`
* `BPE` `"Hello world"` â†’ 2 ä¸ª `token`

> **æ³¨æ„**  
>  `BPE` çš„åˆ†è¯ç»“æœå¹¶ä¸æ˜¯å›ºå®šçš„ï¼Œæ¯”å¦‚ `"Hello world"` å¯èƒ½ä¼šè¢«åˆ‡æˆ 2 ä¸ªï¼Œä¹Ÿå¯èƒ½æ˜¯ 5 ä¸ª `token`ã€‚è¿™ä¸æ˜¯ `bug`ï¼Œè€Œæ˜¯å› ä¸º **`BPE` åªä¼šåˆå¹¶è®­ç»ƒè¯­æ–™é‡Œå‡ºç°è¿‡çš„é«˜é¢‘å­ä¸²**ã€‚å¦‚æœæŸä¸ªè¯åœ¨è¯­æ–™ä¸­å‡ºç°å¾—ä¸å¤Ÿé¢‘ç¹ï¼Œå°±ä¼šè¢«æ‹†æˆæ›´å°çš„å­è¯ç‰‡æ®µã€‚

æ˜¾ç„¶ï¼Œ`BPE` åºåˆ—æ›´çŸ­ï¼Œæ›´é€‚åˆé•¿æ–‡æœ¬è®­ç»ƒã€‚

![tokenizer_char_vs_bpe](https://i-blog.csdnimg.cn/img_convert/649e24f3675321c40607081a9f30d607.png)

> âš ï¸ æ³¨æ„ï¼š
>
> å¦‚æœè¯­æ–™å¤ªçŸ­è€Œ `vocab_size` å¤ªå¤§ï¼Œä¼šæŠ¥é”™ã€‚è§£å†³æ–¹æ³•ï¼šå‡å°è¯è¡¨å¤§å°ï¼ˆæ¯”å¦‚ 200ï¼‰ã€‚

##### 2.5 åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†

æœºå™¨å­¦ä¹ å¿…é¡»è¦åŒºåˆ† **è®­ç»ƒæ•°æ®** å’Œ **éªŒè¯æ•°æ®**ï¼š

* è®­ç»ƒæ•°æ®ï¼šæ¨¡å‹å­¦ä¹ ç”¨ã€‚
* éªŒè¯æ•°æ®ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆã€‚

```python
import torch

data = encode(text)  # å¦‚æœç”¨ BPEï¼Œå°±æ”¹æˆ encode_bpe
data = torch.tensor(data, dtype=torch.long)

n = int(0.9 * len(data))  # 90% è®­ç»ƒï¼Œ10% éªŒè¯
train_data = data[:n]
val_data = data[n:]

print("è®­ç»ƒé›†å¤§å° =", len(train_data))
print("éªŒè¯é›†å¤§å° =", len(val_data))

```

ç¤ºä¾‹è¾“å‡ºï¼š

```makefile
è®­ç»ƒé›†å¤§å° = 270
éªŒè¯é›†å¤§å° = 30

```

---

#### ä¸‰ã€å®ç° `Transformer` è§£ç å™¨

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®å’Œåˆ†è¯å™¨ã€‚æ¥ä¸‹æ¥è¦æ­å»ºçš„ï¼Œå°±æ˜¯ `LLM` çš„â€œå¤§è„‘â€â€”â€” **`Transformer` è§£ç å™¨**ã€‚å®ƒçš„ä»»åŠ¡å¾ˆæ˜ç¡®ï¼š**æ ¹æ®å‰é¢çš„ `token`ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª `token` çš„æ¦‚ç‡åˆ†å¸ƒ**ã€‚

æˆ‘ä»¬ä¼šé€å±‚æ‹†å¼€çœ‹ï¼š**`Embedding` â†’ è‡ªæ³¨æ„åŠ› â†’ å‰é¦ˆç½‘ç»œï¼ˆ`MLP`ï¼‰ â†’ å †å å¤šå±‚ â†’ è¾“å‡ºå±‚**ã€‚

##### 3.1 `Embedding`ï¼šæŠŠæ•°å­—å˜æˆå‘é‡

åˆ†è¯å™¨è¾“å‡ºçš„åªæ˜¯ **`token id`ï¼ˆçº¯æ•°å­—ï¼‰**ï¼Œä½†ç¥ç»ç½‘ç»œæ›´æ“…é•¿å¤„ç†å‘é‡ã€‚ æ‰€ä»¥ç¬¬ä¸€æ­¥ï¼šæŠŠæ¯ä¸ª `token id` æ˜ å°„åˆ°ä¸€ä¸ªå‘é‡ã€‚

```python
import torch
import torch.nn as nn

vocab_size = 200     # è¯è¡¨å¤§å°ï¼ˆæ ¹æ®åˆ†è¯å™¨è€Œå®šï¼‰
n_embd = 128         # å‘é‡ç»´åº¦ï¼ˆembedding ç»´åº¦ï¼‰
block_size = 64      # ä¸Šä¸‹æ–‡çª—å£å¤§å°

class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size, n_embd, block_size):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, n_embd)   # token embedding
        self.pos_emb = nn.Embedding(block_size, n_embd)   # ä½ç½® embedding

    def forward(self, idx):
        B, T = idx.shape
        tok = self.tok_emb(idx)                           # (B, T, n_embd)
        pos = self.pos_emb(torch.arange(T, device=idx.device))  # (T, n_embd)
        return tok + pos  # token å‘é‡ + ä½ç½®ä¿¡æ¯

```

* **`token embedding`**ï¼šæ¯ä¸ªè¯çš„â€œè¯­ä¹‰è¡¨ç¤ºâ€ã€‚
* **`position embedding`**ï¼šå‘Šè¯‰æ¨¡å‹è¯çš„é¡ºåºï¼Œå¦åˆ™æ¨¡å‹åªçŸ¥é“â€œæœ‰å“ªäº›è¯â€ï¼Œå´ä¸çŸ¥é“â€œé¡ºåºå¦‚ä½•â€ã€‚

##### 3.2 è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆ`Self-Attention`ï¼‰

è¿™æ˜¯ `Transformer` çš„æ ¸å¿ƒã€‚å®ƒçš„ä½œç”¨æ˜¯ï¼š**æ¯ä¸ªè¯å¯ä»¥å†³å®šè¦å¤šå…³æ³¨å‰é¢å“ªäº›è¯ï¼Œä»ä¸­è·å–ä¿¡æ¯**ã€‚

###### 3.2.1 åŸºæœ¬æ€è·¯

* æ¯ä¸ªè¾“å…¥å‘é‡ä¼šç”Ÿæˆ **æŸ¥è¯¢å‘é‡ (`Q`)ã€é”®å‘é‡ (`K`)ã€å€¼å‘é‡ (`V`)**ã€‚
* é€šè¿‡ `Q` å’Œ `K` çš„ç‚¹ç§¯ï¼Œå¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼ˆç›¸å…³æ€§ï¼‰ã€‚
* ç”¨ `Softmax` æŠŠåˆ†æ•°è½¬æˆæƒé‡ï¼Œå†åŠ æƒæ±‚å’Œå€¼å‘é‡ `V`ã€‚

###### 3.2.2 ä»£ç å®ç°

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, n_embd, n_head, block_size):
        super().__init__()
        self.n_head = n_head
        self.head_dim = n_embd // n_head
        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)
        self.proj = nn.Linear(n_embd, n_embd)
        # å› æœé®ç½©ï¼šä¿è¯ä¸èƒ½çœ‹æœªæ¥
        self.register_buffer("mask", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))

    def forward(self, x):
        B, T, C = x.shape
        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.head_dim)
        q, k, v = qkv.unbind(dim=2)  # æ‹†æˆ Q, K, V
        q, k, v = [t.transpose(1, 2) for t in (q, k, v)]  # (B, nh, T, hd)

        # æ³¨æ„åŠ›åˆ†æ•° (B, nh, T, T)
        att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float("-inf"))
        att = torch.softmax(att, dim=-1)

        # åŠ æƒæ±‚å’Œ
        y = att @ v  # (B, nh, T, hd)
        y = y.transpose(1, 2).contiguous().view(B, T, C)  # æ‹¼å› (B, T, C)
        return self.proj(y)

```

è¿™é‡Œçš„ **å› æœé®ç½© (`Causal Mask`)** éå¸¸å…³é”®ï¼šå®ƒç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°â€œè‡ªå·±å’Œå‰é¢çš„è¯â€ï¼Œä¸èƒ½å·çœ‹æœªæ¥ã€‚è¿™å°±æ˜¯â€œè‡ªå›å½’â€çš„æœ¬è´¨ã€‚

##### 3.3 å‰é¦ˆç½‘ç»œï¼ˆ`Feed Forward`, `MLP`ï¼‰

æ³¨æ„åŠ›å±‚æ•æ‰äº†ä¾èµ–å…³ç³»ï¼Œä½†è¿˜éœ€è¦å¢åŠ â€œéçº¿æ€§å˜æ¢èƒ½åŠ›â€ã€‚è¿™å°±æ˜¯ **`MLP`ï¼ˆå‰é¦ˆç½‘ç»œï¼‰** çš„ä½œç”¨ã€‚

```python
class FeedForward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # æ”¾å¤§
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),  # å†ç¼©å›å»
        )
    def forward(self, x):
        return self.net(x)

```

---

##### 3.4 `Transformer Block`

æŠŠ **æ³¨æ„åŠ›å±‚** å’Œ **å‰é¦ˆå±‚** ç»„åˆèµ·æ¥ï¼Œå¹¶åŠ ä¸Š **æ®‹å·®è¿æ¥** å’Œ **å±‚å½’ä¸€åŒ–**ã€‚ä¸€å±‚èƒ½å­¦åˆ°â€œçŸ­è·ç¦»ä¾èµ–â€ï¼Œæ¯”å¦‚â€œNew â†’ Yorkâ€ï¼›å¤šå±‚å †å ï¼Œå°±èƒ½å­¦åˆ°æ›´é•¿è·ç¦»ã€æ›´å¤æ‚çš„å…³ç³»ã€‚

```python
class TransformerBlock(nn.Module):
    def __init__(self, n_embd, n_head, block_size):
        super().__init__()
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        self.attn = CausalSelfAttention(n_embd, n_head, block_size)
        self.ffwd = FeedForward(n_embd)

    def forward(self, x):
        x = x + self.attn(self.ln1(x))  # æ®‹å·®è¿æ¥
        x = x + self.ffwd(self.ln2(x))  # æ®‹å·®è¿æ¥
        return x

```

**æ®‹å·®è¿æ¥**ï¼šä¿ç•™åŸå§‹ä¿¡æ¯ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±ã€‚

**å±‚å½’ä¸€åŒ–**ï¼šè®©è®­ç»ƒæ›´ç¨³å®šã€‚

##### 3.5 `GPT` æ¨¡å‹ä¸»ä½“

ç°åœ¨æŠŠæ‰€æœ‰éƒ¨åˆ†æ‹¼èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„ `GPT` æ¨¡å‹ã€‚

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, n_embd=128, n_head=4, n_layer=4, block_size=64):
        super().__init__()
        self.block_size = block_size
        self.embed = TokenEmbedding(vocab_size, n_embd, block_size)
        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, block_size) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd)
        self.head = nn.Linear(n_embd, vocab_size, bias=False)

    def forward(self, idx, targets=None):
        x = self.embed(idx)
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.head(x)  # (B, T, vocab_size)

        loss = None
        if targets is not None:
            # äº¤å‰ç†µï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token
            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        return logits, loss

```

* **`logits`**ï¼šæ¯ä¸ªä½ç½®å¯¹æ•´ä¸ªè¯è¡¨çš„é¢„æµ‹åˆ†æ•°ï¼ˆè¿˜æ²¡è½¬æˆæ¦‚ç‡ï¼‰ã€‚
* **`loss`**ï¼šç”¨äº¤å‰ç†µè¡¡é‡â€œé¢„æµ‹å’ŒçœŸå®ç­”æ¡ˆçš„å·®è·â€ã€‚

##### 3.6 å°å®éªŒï¼šå‰å‘ä¼ æ’­

æˆ‘ä»¬æ¥åšä¸€ä¸ªç®€å•çš„æµ‹è¯•ï¼Œçœ‹çœ‹æ¨¡å‹èƒ½å¦æ­£å¸¸è¿è¡Œï¼š

```python
model = GPT(vocab_size=vocab_size, n_embd=128, n_head=4, n_layer=2, block_size=64)
x = torch.randint(0, vocab_size, (1, 10))  # éšæœº 10 ä¸ª token
logits, loss = model(x, x)
print("logits shape =", logits.shape)
print("loss =", loss.item())

```

ç¤ºä¾‹è¾“å‡ºï¼š

```makefile
logits shape = torch.Size([1, 10, 200])
loss = 5.3

```

è§£é‡Šï¼š

* `[1, 10, 200]` â†’ æ‰¹å¤§å°=1ï¼Œåºåˆ—é•¿åº¦=10ï¼Œæ¯ä¸ªä½ç½®é¢„æµ‹ 200 ä¸ªè¯çš„åˆ†å¸ƒã€‚
* `loss=5.3` â†’ è¯´æ˜é¢„æµ‹å’Œç­”æ¡ˆå·®è·è¿˜æ¯”è¾ƒå¤§ï¼Œè¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºæ¨¡å‹è¿˜æ²¡è®­ç»ƒã€‚

![attention_heatmap](https://i-blog.csdnimg.cn/img_convert/c8f36e5f202621ce77c3da77300e8250.png)

---

#### å››ã€è®­ç»ƒå¾ªç¯

å‰é¢æˆ‘ä»¬å·²ç»å®ç°äº†æ¨¡å‹ç»“æ„ï¼Œä½†å®ƒç°åœ¨å°±åƒä¸€ä¸ªåˆšå‡ºç”Ÿçš„å­©å­ï¼š**å¤§è„‘æœ‰äº†ï¼Œä½†é‡Œé¢æ˜¯ç©ºçš„**ï¼Œè¿˜ä¸ä¼šè¯´è¯ã€‚è®­ç»ƒå¾ªç¯çš„ä½œç”¨å°±æ˜¯ä¸æ–­åœ° **å–‚é¥­ â†’ è€ƒè¯• â†’ åˆ¤åˆ† â†’ è°ƒæ•´**ï¼Œç›´åˆ°å®ƒé€æ¸å­¦ä¼šè¯­è¨€è§„å¾‹ã€‚

##### 4.1 ä¸ºä»€ä¹ˆéœ€è¦è®­ç»ƒå¾ªç¯ï¼Ÿ

è®­ç»ƒçš„æœ¬è´¨æ˜¯ï¼š

1. **å–‚é¥­**ï¼šç»™æ¨¡å‹è¾“å…¥æ–‡æœ¬ï¼ˆå‰æ–‡ï¼‰ã€‚
2. **è€ƒè¯•**ï¼šè®©å®ƒé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚
3. **åˆ¤åˆ†**ï¼šè®¡ç®—é¢„æµ‹å’ŒçœŸå®ç­”æ¡ˆçš„å·®è·ï¼ˆ`loss`ï¼‰ã€‚
4. **è°ƒæ•´**ï¼šæ ¹æ®å·®è·æ›´æ–°æ¨¡å‹çš„å‚æ•°ã€‚

å°±åƒå°å­©å­¦è¯´è¯ï¼šå¬åˆ«äººè¯´ â†’ è‡ªå·±æ¨¡ä»¿ â†’ è¢«çº æ­£ â†’ æ…¢æ…¢æ”¹è¿›ã€‚

##### 4.2 æ‰¹æ¬¡é‡‡æ ·ï¼ˆ`get_batch`ï¼‰

è®­ç»ƒæ—¶ä¸èƒ½ä¸€æ¬¡æŠŠæ‰€æœ‰æ•°æ®ä¸¢è¿›å»ï¼ˆå¤ªæ…¢ã€æ˜¾å­˜ä¼šçˆ†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬æŠŠæ•°æ®åˆ‡æˆä¸€ä¸ªä¸ª **å°æ‰¹æ¬¡ï¼ˆ`batch`ï¼‰**ã€‚

```python
import torch

def get_batch(split, block_size, batch_size, device):
    data = train_data if split == "train" else val_data
    # ç¡®ä¿ä¸ä¼šè¶Šç•Œï¼šç‰‡æ®µé•¿åº¦ = block_size
    max_start = len(data) - block_size - 1
    ix = torch.randint(0, max_start, (batch_size,))
    x = torch.stack([data[i : i + block_size] for i in ix])
    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])
    return x.to(device), y.to(device)

```

* **`x`**ï¼šè¾“å…¥åºåˆ—ï¼ˆå‰æ–‡ï¼‰ã€‚
* **`y`**ï¼šç›®æ ‡åºåˆ—ï¼ˆå°±æ˜¯ `x` å³ç§»ä¸€ä½ â†’ ä¸‹ä¸€ä¸ª `token`ï¼‰ã€‚

ä¾‹å­ï¼š

* è¾“å…¥ï¼š`The quick brown`
* ç›®æ ‡ï¼š`he quick brown fox`

è¿™æ ·æ¨¡å‹å­¦çš„å°±æ˜¯â€œå‰æ–‡ â†’ ä¸‹ä¸€ä¸ªè¯â€ã€‚

##### 4.3 æŸå¤±å‡½æ•°ï¼ˆ`Loss`ï¼‰

æˆ‘ä»¬ç”¨ **äº¤å‰ç†µï¼ˆ`CrossEntropy`ï¼‰** æ¥è¡¡é‡é¢„æµ‹å’ŒçœŸå®ç­”æ¡ˆçš„å·®è·ã€‚

* å¦‚æœæ¨¡å‹é¢„æµ‹â€œfoxâ€çš„æ¦‚ç‡é«˜ï¼Œå°±å¥–åŠ±å®ƒï¼ˆ`loss` å°ï¼‰ã€‚
* å¦‚æœå®ƒé¢„æµ‹â€œdogâ€çš„æ¦‚ç‡é«˜ï¼Œå°±æƒ©ç½šå®ƒï¼ˆ`loss` å¤§ï¼‰ã€‚

å¯ä»¥ç†è§£ä¸ºâ€œé¢„æµ‹åˆ†å¸ƒâ€ vs â€œæ­£ç¡®ç­”æ¡ˆï¼ˆ`one-hot` å‘é‡ï¼‰â€çš„å·®è·ã€‚

åœ¨ `GPT` ç±»é‡Œæˆ‘ä»¬å·²ç»å†™è¿‡ï¼š

```python
loss = nn.functional.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))

```

##### 4.4 ä¼˜åŒ–å™¨ï¼ˆ`Optimizer`ï¼‰

ä¼˜åŒ–å™¨çš„ä½œç”¨å°±æ˜¯ **æ›´æ–°å‚æ•°**ï¼Œè®©æ¨¡å‹ä¸€æ­¥æ­¥å˜èªæ˜ã€‚æˆ‘ä»¬ç”¨ **`AdamW`**ï¼Œè¿™æ˜¯ `Transformer` çš„å¸¸è§é€‰æ‹©ã€‚

```python
import math

optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), weight_decay=0.01)

```

å‚æ•°è§£é‡Šï¼š

* `lr=3e-4`ï¼šå­¦ä¹ ç‡ï¼Œå†³å®šâ€œæ¯æ¬¡è°ƒæ•´çš„å¹…åº¦â€ã€‚
* `betas`ï¼šåŠ¨é‡å‚æ•°ï¼Œå¸®åŠ©æ”¶æ•›æ›´ç¨³å®šã€‚
* `weight_decay`ï¼šæƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

##### 4.5 å­¦ä¹ ç‡è°ƒåº¦ï¼ˆ`Warmup` + `Cosine Decay`ï¼‰

å¦‚æœä¸€ä¸Šæ¥å°±ç”¨å¤§å­¦ä¹ ç‡ï¼Œæ¨¡å‹å¯èƒ½â€œå“åäº†â€ï¼Œè®­ç»ƒä¼šä¸ç¨³å®šã€‚æ‰€ä»¥æˆ‘ä»¬é‡‡ç”¨ï¼š

* **`Warmup`**ï¼šå‰æœŸå°æ­¥æ…¢è·‘ï¼ˆé€æ¸åŠ å¤§å­¦ä¹ ç‡ï¼‰ã€‚
* **`Cosine Decay`**ï¼šåæœŸæ…¢æ…¢æ”¶å°¾ï¼ˆé€æ¸å‡å°å­¦ä¹ ç‡ï¼‰ã€‚

```python
def cosine_lr(step, max_steps, base_lr, warmup):
    if step < warmup:
        return base_lr * (step + 1) / max(1, warmup)
    t = (step - warmup) / max(1, max_steps - warmup)
    return 0.5 * (1 + math.cos(math.pi * t)) * base_lr

```

##### 4.6 æ¢¯åº¦è£å‰ªï¼ˆ`Gradient Clipping`ï¼‰

æœ‰æ—¶å€™æ¢¯åº¦ä¼šçªç„¶çˆ†ç‚¸ï¼Œå¯¼è‡´è®­ç»ƒå´©æ‰ã€‚è§£å†³åŠæ³•æ˜¯ï¼šæŠŠæ¢¯åº¦è£å‰ªåœ¨ä¸€å®šèŒƒå›´å†…ã€‚

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

```

##### 4.7 è®­ç»ƒä¸»å¾ªç¯

ç°åœ¨æŠŠæ‰€æœ‰ä¸œè¥¿æ‹¼èµ·æ¥ï¼š

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
model = GPT(vocab_size=vocab_size, n_embd=128, n_head=4, n_layer=2, block_size=64).to(device)

max_steps = 1000
batch_size = 32
warmup = 100
base_lr = 3e-4

optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, betas=(0.9, 0.95), weight_decay=0.01)

for step in range(1, max_steps + 1):
    # è·å–ä¸€æ‰¹è®­ç»ƒæ•°æ®
    x, y = get_batch("train", block_size=64, batch_size=batch_size, device=device)

    # å‰å‘ä¼ æ’­
    logits, loss = model(x, y)

    # åå‘ä¼ æ’­
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    # åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
    lr = cosine_lr(step, max_steps, base_lr, warmup)
    for pg in optimizer.param_groups:
        pg["lr"] = lr

    optimizer.step()

    # æ¯ 100 æ­¥åšä¸€æ¬¡éªŒè¯
    if step % 100 == 0:
        with torch.no_grad():
            vx, vy = get_batch("val", block_size=64, batch_size=batch_size, device=device)
            _, vloss = model(vx, vy)
        print(f"step {step}: train_loss={loss.item():.3f} | val_loss={vloss.item():.3f}")

```

##### 4.8 è¾“å‡ºç¤ºä¾‹

è¿è¡Œåï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„æ—¥å¿—ï¼š

```markdown
step 100: train_loss=2.019 | val_loss=6.231
step 200: train_loss=1.444 | val_loss=5.789
step 300: train_loss=0.824 | val_loss=5.159
step 400: train_loss=0.486 | val_loss=4.909
...

```

è§£é‡Šï¼š

* **`train_loss`**ï¼šæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°ï¼Œåº”è¯¥éšè®­ç»ƒä¸‹é™ã€‚
* **`val_loss`**ï¼šæ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚å¦‚æœ `val_loss` å…ˆé™åå‡ï¼Œè¯´æ˜è¿‡æ‹Ÿåˆã€‚

![loss_curve](https://i-blog.csdnimg.cn/img_convert/ff0868d23ff9a79a147a00c9458df53f.png)

##### 4.9 å¦‚ä½•åˆ¤æ–­è®­ç»ƒæ•ˆæœï¼Ÿ

1. **æ­£å¸¸æƒ…å†µ**ï¼š`train_loss` å’Œ `val_loss` éƒ½ä¸‹é™ â†’ æ¨¡å‹åœ¨å­¦è§„å¾‹ã€‚
2. **è¿‡æ‹Ÿåˆ**ï¼š`train_loss` ä¸€ç›´ä¸‹é™ï¼Œä½† `val_loss` ä¸Šå‡ â†’ æ¨¡å‹åªä¼šèƒŒä¹¦ã€‚
3. **æ¬ æ‹Ÿåˆ**ï¼š`train_loss` é•¿æœŸå¾ˆé«˜ â†’ æ¨¡å‹å¤ªå° / å­¦ä¹ ç‡å¤ªä½ / æ•°æ®å¤ªå°‘ã€‚

> åœ¨å°è¯­æ–™å®éªŒé‡Œï¼Œä¸è¦çº ç»“ `loss` æ•°å€¼ï¼Œé‡è¦çš„æ˜¯å­¦ä¼šè®­ç»ƒæµç¨‹ã€‚çœŸæ­£è¦è®­ç»ƒèƒ½ç”¨çš„æ¨¡å‹ï¼Œéœ€è¦æ›´å¤§çš„è¯­æ–™å’Œæ›´é•¿çš„è®­ç»ƒæ—¶é—´ã€‚

---

åˆ°è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»å®Œæ•´åœ°å®ç°äº†ä¸€ä¸ª **`mini-GPT`**ï¼Œä»æœ€åŸºç¡€çš„ **`Token` / åˆ†è¯å™¨**ï¼Œåˆ° **`Transformer` æ¶æ„**ï¼›ä» **è®­ç»ƒå¾ªç¯**ï¼Œåˆ°èƒ½è·‘é€šçš„ **ç¬¬ä¸€ä¸ªè¯­è¨€æ¨¡å‹**ã€‚è¯·ç»§ç»­é˜…è¯»ï¼š[ä¸‹ç¯‡ï¼šã€Šä»é›¶å®ç° LLMï¼ˆä¸‹ï¼‰ï¼šæ¨ç†ç”Ÿæˆã€å¸¸è§é—®é¢˜ä¸è¿›é˜¶ä¼˜åŒ–ã€‹](https://blog.csdn.net/qq_41865545/article/details/151143636)

---

ğŸ **å½©è›‹ï¼šä¸€é”®è¿è¡Œ `Notebook`**

å¦‚æœä½ ä¸æƒ³ä»é›¶å¤åˆ¶ç²˜è´´ä»£ç ï¼Œæˆ–è€…æƒ³ç›´æ¥ä½“éªŒå®Œæ•´çš„ `mini-GPT` å®ç°ï¼Œæˆ‘å·²ç»å‡†å¤‡äº†ä¸€ä»½ **`Google Colab Notebook`**ï¼š

ğŸ‘‰ [ç‚¹å‡»è¿™é‡Œç›´æ¥è¿è¡Œ `mini-GPTï¼ˆColabï¼‰`](https://colab.research.google.com/drive/1cjIV2xQUbXi-NUkXArMQ9yvzrEUQrVeZ?usp=sharing)



