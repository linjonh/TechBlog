---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f75303132343530333239:2f61727469636c652f64657461696c732f3532363931373138"
layout: post
title: "es中的scan-and-scroll搜索"
date: 2024-12-03 15:20:17 +0800
description: "在es上搜索数据时，默认es只会返回10条文档，当我们想获取更多结果，或者只要结果中的一个区间的数据"
keywords: "es scroll scan"
categories: ['Elasticsearch']
tags: ['无标签']
artid: "52691718"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=52691718
    alt: "es中的scan-and-scroll搜索"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=52691718
featuredImagePreview: https://bing.ee123.net/img/rand?artid=52691718
---

# es中的scan and scroll搜索

在es上搜索数据时，默认es只会返回10条文档，当我们想获取更多结果，或者只要结果中的一个区间的数据时，可以通过szie和from来指定。

**[plain]**
[view plain](http://blog.csdn.net/mgxcool/article/details/49906895# "view plain")
[copy](http://blog.csdn.net/mgxcool/article/details/49906895# "copy")

1. GET /\_search?size=3&from=20

如上的查询语句，会返回排序后的结果中第20到第22条数据。es在收到这样的一个请求之后，每一个分片都会返回一个top22的搜索结果，然后将这些结果汇总排序，再选出top22，最后取第20到第22条数据作为结果返回。

这样会带来一个问题，当我们搜索的时候，如果想取出第10001条数据，那么就相当于每个一分片都要对数据进行排序，取出前10001条文档，然后es再将这些结果汇总再次排序，之后取出第10001条数据。这样对于es来说就会产生相当大的资源和性能开销。如果我们不要求es对结果进行排序，那么就会消耗很少的资源，所以针对此种情况，es提供了scan and scroll的搜索方式。

**[plain]**
[view plain](http://blog.csdn.net/mgxcool/article/details/49906895# "view plain")
[copy](http://blog.csdn.net/mgxcool/article/details/49906895# "copy")

1. GET /old\_index/\_search?search\_type=scan&scroll=1m
2. {
3. "query": { "match\_all": {}},
4. "size":  1000
5. }

我们可以首先通过如上的请求发起一个搜索，但是这个请求不会返回任何文档，它会返回一个\_scroll\_id，接下来我们再通过这个id来从es中读取数据：

**[plain]**
[view plain](http://blog.csdn.net/mgxcool/article/details/49906895# "view plain")
[copy](http://blog.csdn.net/mgxcool/article/details/49906895# "copy")

1. GET /\_search/scroll?scroll=1m
2. c2Nhbjs1OzExODpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzExOTpRNV9aY1VyUVM4U0
3. NMd2pjWlJ3YWlBOzExNjpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzExNzpRNV9aY1Vy
4. UVM4U0NMd2pjWlJ3YWlBOzEyMDpRNV9aY1VyUVM4U0NMd2pjWlJ3YWlBOzE7dG90YW
5. xfaGl0czoxOw==

此时除了会返回搜索结果以外，还会再次返回一个\_scroll\_id，当我们下次继续取数据时，需要用最新的id。