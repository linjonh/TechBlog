---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f4d756c7469706c655f782f:61727469636c652f64657461696c732f313436303830303330"
layout: post
title: "论文阅读笔记0-A-Vision-Language-Action-Flow-Model-for-General-Robot-Control"
date: 2025-03-06 21:01:13 +0800
description: "π0: A Vision-Language-Action Flow Model for General Robot Control 论文阅读笔记"
keywords: "vision-language-action flow model"
categories: ['论文阅读笔记']
tags: ['语言模型', '论文阅读', '笔记', '机器人', '人工智能']
artid: "146080030"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146080030
    alt: "论文阅读笔记0-A-Vision-Language-Action-Flow-Model-for-General-Robot-Control"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146080030
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146080030
cover: https://bing.ee123.net/img/rand?artid=146080030
image: https://bing.ee123.net/img/rand?artid=146080030
img: https://bing.ee123.net/img/rand?artid=146080030
---

# 论文阅读笔记——π0: A Vision-Language-Action Flow Model for General Robot Control

[π0 论文](https://arxiv.org/pdf/2410.24164)

> π
> 0
> π\_0
>
>
>
>
>
>
> π
>
>
>
>
>
>
>
>
>
> 0
>
> ​
>
> 是基于预训练的 VLM 模型增加了 action expert ，并结合了 flow matching 方法训练的自回归模型，能够直接输出模型的 action chunk（50）。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c41d57004a7b44cda9168b4883d17a92.png#pic_center)

* π0 采用 Flow Matching 技术来建模连续动作的分布，这一创新使模型能够精确控制高频率的灵巧操作任务，同时具备处理多模态数据的能力。
* 架构受到 Transfusion 的启发 ：通过单一 Transformer 处理多目标任务，其中
  **连续输出**
  由流匹配损失监督，
  **离散输出**
  则由交叉熵损失监督。π0 在此基础上进行了针对性优化，特别为机器人领域的动作和状态 tokens 设计了一组独立的动作专家（Transformer）模块。
* 在推理阶段，π0 采用了高效的执行策略：新增的动作专家模块需要进行 10 次 flow matching 去噪迭代，而基础视觉语言模型（PaliGemma）仅需执行一次前向传播即可生成输出。
    
  ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1d8b14bd257f4325a576bcb23dd8aa37.png#pic_center)
  **输入**
  ：
* PaliGemma 接受的图像

  [
  I
  t
  1
  ，
  …
  …
  ，
  I
  t
  n
  ]
  [I\_t^1 ，……， I\_t^n]





  [


  I









  t





  1

  ​


  ，



  ……



  ，


  I









  t





  n

  ​


  ]
  和语言指令

  l
  t
  l\_t






  l









  t

  ​

  。
* 输入噪声

  A
  t
  =
  [
  a
  t
  ,
  a
  t
  +
  1
  ,
  …
  …
  ,
  a
  t
  +
  H
  −
  1
  ]
  A\_t = [a\_t,a\_{t+1},……,a\_{t+H-1}]






  A









  t

  ​




  =





  [


  a









  t

  ​


  ,




  a










  t

  +

  1

  ​


  ,



  ……



  ,




  a










  t

  +

  H

  −

  1

  ​


  ]
  未来动作的 action chunk（50）。
* 自身状态

  q
  t
  q\_t






  q









  t

  ​

  ，通过线性层映射到 embedding 维度。

**训练**
：

* **加噪**
  ：对于每一个 action chunk 中的每一个动作

  a
  t
  a\_t






  a









  t

  ​

  ，引入噪声

  ϵ
  \epsilon





  ϵ
  ，生成噪声 action，

  A
  t
  τ
  =
  τ
  A
  t
  +
  (
  1
  −
  τ
  )
  ϵ
  A\_t^\tau =\tau A\_t +(1-\tau)\epsilon






  A









  t





  τ

  ​




  =





  τ


  A









  t

  ​




  +





  (

  1



  −





  τ

  )

  ϵ
  （flow matching 的条件概率分布） 。
* **向量场预测**
  ：将噪声动作

  A
  t
  τ
  A\_t^{\tau}






  A









  t






  τ

  ​

  和观测值

  o
  t
  o\_t






  o









  t

  ​

  输入动作专家，预测一个向量场

  ν
  θ
  (
  A
  t
  τ
  ,
  o
  t
  )
  \nu\_{\theta}(A\_t^{\tau},o\_t)






  ν










  θ

  ​


  (


  A









  t






  τ

  ​


  ,




  o









  t

  ​


  )
* **优化**
  ：flow matching 损失函数训练模型 。

  L
  τ
  (
  θ
  )
  =
  E
  p
  (
  A
  t
  ∣
  o
  t
  )
  ,
  q
  (
  A
  t
  τ
  ∣
  A
  t
  )
  ∣
  ∣
  v
  θ
  (
  A
  t
  τ
  ,
  o
  t
  )
  −
  u
  (
  A
  t
  τ
  ∣
  A
  t
  )
  ∣
  ∣
  2
  L^\tau(\theta)=\mathbb{E}\_{p(\mathbf{A}\_t|\mathbf{o}\_t),q(\mathbf{A}\_t^\tau|\mathbf{A}\_t)}||\mathbf{v}\_\theta(\mathbf{A}\_t^\tau,\mathbf{o}\_t)-\mathbf{u}(\mathbf{A}\_t^\tau|\mathbf{A}\_t)||^2






  L









  τ

  (

  θ

  )



  =






  E










  p

  (


  A









  t

  ​


  ∣


  o









  t

  ​


  )

  ,

  q

  (


  A









  t





  τ

  ​


  ∣


  A









  t

  ​


  )

  ​


  ∣∣


  v









  θ

  ​


  (


  A









  t





  τ

  ​


  ,




  o









  t

  ​


  )



  −





  u

  (


  A









  t





  τ

  ​


  ∣


  A









  t

  ​


  )

  ∣


  ∣









  2
  。

**输出**
：使用与 H 个噪声动作相对应的Transformer输出，使用线性投影将其解码为

ν
θ
(
A
t
τ
,
o
t
)
\nu\_{\theta}(A\_t^{\tau},o\_t)






ν










θ

​


(


A









t






τ

​


,




o









t

​


)
。

推理时，真实动作序列通过 Euler 步进法进行恢复：

A
t
τ
+
δ
=
A
t
τ
+
δ
ν
θ
(
A
t
τ
,
o
t
)
A\_t^{\tau+\delta} = A\_t^{\tau} + \delta \nu\_{\theta}(A\_t^{\tau},o\_t)






A









t






τ

+

δ

​




=






A









t






τ

​




+





δ


ν










θ

​


(


A









t






τ

​


,




o









t

​


)
，其中

δ
=
0.1
\delta =0.1





δ



=





0.1
，推理频率为 50 Hz。

* **注意力掩码**
  ： 采用三块注意力掩码，图像和语言提示作为第一块，

  q
  t
  q\_t






  q









  t

  ​

  作为第二块，噪声化动作

  A
  t
  τ
  A\_t^{\tau}






  A









  t






  τ

  ​

  作为第三块。图像和语言块内部互相可见，但不能看到机器人相关变量（

  q
  t
  q\_t






  q









  t

  ​

  和

  A
  t
  τ
  A\_t^{\tau}






  A









  t






  τ

  ​

  ），可以看到自身以及图像和语言块，但不能看到

  A
  t
  τ
  A\_t^{\tau}






  A









  t






  τ

  ​

  ，

  A
  t
  τ
  A\_t^{\tau}






  A









  t






  τ

  ​

  可以看到所有块，但靠前的 token 看不到靠后的 token。
* **时间步采样**
  ： 流匹配的时间步

  τ
  \tau





  τ
  从 beta 分布中采样，倾向于采较低的时间步（更高的噪声级别），这帮助模型更好地学习动作分布。

#### 数据集

预训练混合数据由 OXE 数据集和 π 数据集的部分子集构成。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5610ea9185694f958f902fe97aae4a0e.png#pic_center)

### 性能表现

灵巧操作任务对机器人的适应能力提出了极高要求，特别是当这些任务与预训练数据有差异时，模型是否能够通过微调快速学习新行为成为关键。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/937cd4b0a4634ac481b95653b799172f.png#pic_center)
  
实验结果表明，微调后的

π
0
\pi\_0






π









0

​

在所有任务中均表现优于其他方法，包括 OpenVLA、Octo、ACT和 Diffusion Policy。特别是在纸巾更换和抽屉整理等困难任务中，

π
0
\pi\_0






π









0

​

通过其预训练的通用能力和灵活的微调策略展现了显著优势。而在“简单”任务如叠毛巾中，

π
0
\pi\_0






π









0

​

的性能更是大幅超越未经过预训练的模型，显示了预训练数据的重要作用。

### 复杂任务表现 在这里插入图片描述