---
layout: post
title: "从零开始实现大语言模型十四高阶训练技巧"
date: 2025-03-07 23:46:31 +0800
description: "本文介绍深度学习领域优化训练学习率的两种方法Learning Rate Warmup和Cosine Decay，优化深度神经网络模型参数梯度的方法Gradient Clipping，以及优化训练超参数的方法Hyperparameters Search，并实现预训练大语言模型的函数`hyper_pretrain_model`。"
keywords: "从零开始实现大语言模型（十四）：高阶训练技巧"
categories: ['从零开始实现大语言模型']
tags: ['大语言模型', '从零开始实现大语言模型', '人工智能', 'Llm', 'Deepseek', 'Chatgpt']
artid: "146108187"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146108187
    alt: "从零开始实现大语言模型十四高阶训练技巧"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146108187
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146108187
cover: https://bing.ee123.net/img/rand?artid=146108187
image: https://bing.ee123.net/img/rand?artid=146108187
img: https://bing.ee123.net/img/rand?artid=146108187
---

# 从零开始实现大语言模型（十四）：高阶训练技巧
## 1\. 前言
预训练大语言模型的流程与训练普通神经深度网络模型本质上并没有任何不同。可以使用深度学习实践中已经被证明非常有效的高阶训练技巧，优化大语言模型预训练流程，使大语言模型预训练效率更高，训练过程更稳定。
本文介绍深度学习领域优化训练学习率的两种方法Learning Rate Warmup和Cosine
Decay，优化深度神经网络模型参数梯度的方法Gradient Clipping，以及优化训练超参数的方法Hyperparameters
Search，并实现预训练大语言模型的函数`hyper\_pretrain\_model`。
## 2\. 优化训练学习率
### 2.1 Learning Rate Warmup
学习率是训练深度学习模型过程中最关键的超参数，没有之一。学习率可以控制深度神经网络模型参数的迭代更新速度，学习率越大，则参数的迭代更新速度越快，学习率越小，则参数的更新速度越慢。但是过大的学习率会导致损失函数在Error
Surface上发生跳跃，使训练过程不稳定，模型难以收敛。如果如学习率太小，则会导致参数深度神经网络参数每次更新的幅度很小，使神经网络模型的训练效率很低，而且容易使损失函数陷入Error
Surface中的局部最优解。
Learning Rate
Warmup（学习率预热）是一种经过深度学习实践证明非常有效的优化深度神经网络前几次迭代训练学习率，以降低深度神经网络参数随机初始化带来的不确定性风险，从而提升训练过程稳定性的方法。Learning
Rate
Warmup会指定一个非常小的初始学习率`initial\_lr`，以及预热步骤`warmup\_steps`，并在训练深度神经网络的前`warmup\_steps`次迭代流程中，将学习率逐步从`initial\_lr`提升至不使用Learning
Rate Warmup时设定的值`peak\_lr`。在深度学习实践中，预热步骤`warmup\_steps`一般会占总训练次数的 0.1 % 0.1\%
0.1%至 10 % 10\% 10%。
如下面的代码所示，假设学习率的设定值`peak\_lr`为0.01，Learning Rate
Warmup指定的初始学习率`initial\_lr`为0.0001，`warmup\_steps`为15。使用Learning Rate
Warmup优化训练学习率，需要在训练深度神经网络的前`warmup\_steps`次迭代流程中，计算当次迭代训练使用的学习率大小，并修改优化器中使用学习率的值：
import os
import torch
import random
import tiktoken
from torch.utils.data import Dataset, DataLoader
# from [从零开始实现大语言模型（二）：文本数据处理] import LLMDataset
# from [从零开始实现大语言模型（七）：多头注意力机制] import MultiHeadAttention
# from [从零开始实现大语言模型（八）：Layer Normalization] import LayerNorm
# from [从零开始实现大语言模型（九）：前馈神经网络与GELU激活函数] import GELU, FeedForward
# from [从零开始实现大语言模型（十一）：构建大语言模型GPTModel] import TransformerBlock, GPTModel
torch.manual\_seed(123)
train\_data\_path = "train\_data"
vocabulary = "gpt2"
special\_token\_id = 50256
context\_len = 1024
stride = 1024
batch\_size = 2
embedding\_dim = 768
num\_layers = 12
num\_heads = 12
context\_len = 1024
vocabulary\_size = 50257
dropout = 0.1
qkv\_bias = False
num\_epochs = 15
initial\_lr = 0.0001
peak\_lr = 0.01
warmup\_steps = 15
train\_dataset = LLMDataset(train\_data\_path, vocabulary, special\_token\_id, context\_len, stride)
train\_loader = DataLoader(dataset=train\_dataset, batch\_size=batch\_size, shuffle=True, drop\_last=True)
gpt2\_small = GPTModel(
embedding\_dim=embedding\_dim,
num\_layers=num\_layers,
num\_heads=num\_heads,
context\_len=context\_len,
vocabulary\_size=vocabulary\_size,
dropout=dropout,
qkv\_bias=qkv\_bias
)
optimizer = torch.optim.AdamW(gpt2\_small.parameters(), weight\_decay=0.1)
lr\_increment = (peak\_lr - initial\_lr) / warmup\_steps
global\_step = -1
track\_lrs = []
for epoch in range(num\_epochs):
for input\_batch, target\_batch in train\_loader:
optimizer.zero\_grad()
global\_step += 1
if global\_step < warmup\_steps:
lr = initial\_lr + global\_step \* lr\_increment
else:
lr = peak\_lr
for param\_group in optimizer.param\_groups:
param\_group["lr"] = lr
track\_lrs.append(optimizer.param\_groups[0]["lr"])
可以使用如下代码绘制大语言模型`gpt2\_small`的每轮迭代训练过程所使用的学习率：
import matplotlib.pyplot as plt
plt.ylabel("Learning rate")
plt.xlabel("Step")
total\_training\_steps = len(train\_loader) \* num\_epochs
plt.plot(range(total\_training\_steps), track\_lrs)
plt.show()
执行上面代码，生成大语言模型`gpt2\_small`的整个迭代训练流程中的学习率变化情况图像如下：
![图一](https://i-blog.csdnimg.cn/direct/6228307eb8f14356af30122022e638db.png#pic\_center)
### 2.2 Cosine Decay
在深度学习实践中，一般会将Learning Rate Warmup与Cosine Decay结合起来，共同优化训练学习率。Learning Rate
Warmup只作用于深度神经网络的前`warmup\_steps`轮迭代训练过程（预热阶段），使训练学习率从一个很小的`initial\_lr`逐步提升至`peak\_lr`。Cosine
Decay会在预热阶段之后的全部迭代训练过程中，以余弦曲线的方式逐步减小训练学习率，以降低模型参数的更新速度，减少损失函数越过Error
Surface上极小值的概率，提升训练过程稳定性。
如下面的代码所示，在预热阶段之后使用Cosine Decay策略调整训练学习率，需要使用`global\_step -
warmup\_steps`得到预热阶段后的迭代训练步数，以及使用`total\_training\_steps -
warmup\_steps`计算出预热阶段后的总迭代训练次数，并通过`(global\_step - warmup\_steps) /
(total\_training\_steps -
warmup\_steps)`计算出去掉预热阶段后的训练进度百分比`progress`。使用`math.cos(math.pi \*
progress)`可以计算得到一个介于1到-1之间的余弦值，当`progress`为0时，余弦值为1，当`progress`为1时，余弦值为-1，余弦值的变化速率曲线为余弦曲线。使用`0.5
\* (1 + math.cos(math.pi \* progress))`对余弦值做变换，使余弦值的取值范围由`[1, -1]`变换到`[1,
0]`，最后使用 `min\_lr + (peak\_lr - min\_lr) \* 0.5 \* (1 + math.cos(math.pi \*
progress))`计算得到当前迭代训练步数对应的训练学习率：
import math
min\_lr = 0.1 \* initial\_lr
track\_lrs = []
global\_step = -1
for epoch in range(num\_epochs):
for input\_batch, target\_batch in train\_loader:
optimizer.zero\_grad()
global\_step += 1
if global\_step < warmup\_steps:
lr = initial\_lr + global\_step \* lr\_increment
else:
progress = (global\_step - warmup\_steps) / (total\_training\_steps - warmup\_steps)
lr = min\_lr + (peak\_lr - min\_lr) \* 0.5 \* (1 + math.cos(math.pi \* progress))
for param\_group in optimizer.param\_groups:
param\_group["lr"] = lr
track\_lrs.append(optimizer.param\_groups[0]["lr"])
可以使用如下代码绘制使用Learning Rate Warmup与Cosine Decay策略后的学习率变化情况图像：
plt.ylabel("Learning rate")
plt.xlabel("Step")
plt.plot(range(total\_training\_steps), track\_lrs)
plt.show()
执行上面代码，生成的学习率变化情况图像如下：
![图二](https://i-blog.csdnimg.cn/direct/588ee61378504d65b31db0eed0198a78.png#pic\_center)
## 3\. 优化模型参数梯度
### 3.1 Gradient Clipping
Gradient
Clipping（梯度裁剪）是一种通过限制参数梯度大小，以解决深度神经网络训练过程中的梯度爆炸问题，从而提升训练过程稳定性的模型参数梯度优化方法。深度学习实践中常用的Gradient
Clipping方法有两种：基于梯度值的裁剪和基于梯度范数的裁剪。
基于梯度值的裁剪方法的原理非常简单，其会直接将深度神经网络参数的梯度中大于`clip\_value`的梯度设置成`clip\_value`，并将小于`-clip\_value`的梯度设置成`-clip\_value`，使深度神经网络参数梯度的绝对值值不超过`clip\_value`。基于梯度范数的裁剪方法首先会计算神经网络参数梯度的[p-范数](https://baike.baidu.com/item/%E8%8C%83%E6%95%B0?fromModule=lemma\_search-
box)，如果p-范数大于`max\_norm`，则会将每个梯度值均乘以 max\_norm p\_norm
\frac{\text{max\\\_norm}}{\text{p\\\_norm}} p\_normmax\_norm​，使神经网络参数梯度的p-
范数等于`max\_norm`。
假设深度神经网络共包含4个参数，后向传播流程计算出的参数梯度 G = [ 1 2 2 4 ]
G=\begin{bmatrix}1&2\\\2&4\end{bmatrix}
G=[12​24​]，使用基于梯度范数的裁剪方法优化模型参数梯度，设置参数梯度2-范数的最大值`max\_norm`为2.0，首先需要计算神经网络参数梯度的2-范数
∥ G ∥ 2 = 1 2 \+ 2 2 \+ 2 2 \+ 4 2 = 25 = 5
\|G\|\_2=\sqrt{1^2+2^2+2^2+4^2}=\sqrt{25}=5 ∥G∥2​=12+22+22+42 ​=25 ​=5。因为 ∥ G ∥
2 > 2 \|G\|\_2>2 ∥G∥2​>2，因此会将每个梯度值均乘以 max\_norm p\_norm = 2 5
\frac{\text{max\\\_norm}}{\text{p\\\_norm}}=\frac{2}{5}
p\_normmax\_norm​=52​，即将神经网络参数梯度裁剪成 G ′ = 2 5 × G = [ 2 5 4 5 4 5 8 5 ]
G'=\frac{2}{5}\times G=\begin{bmatrix}\frac{2}{5}&\frac{4}{5}\\\
\frac{4}{5}&\frac{8}{5}\end{bmatrix} G′=52​×G=[52​54​​54​58​​]。
如下面的代码所示，定义计算深度神经网络参数梯度最大值的函数`find\_largest\_gradient`，并使用`torch.tensor`函数创建训练样本`input\_batch`及训练样本标签`target\_batch`。将训练样本`input\_batch`输入大语言模型`gpt2\_small`，使用`calc\_loss\_batch`函数计算大语言模型的预测输出与训练样本标签之间的交叉熵损失loss，并通过`loss.backward()`计算大语言模型参数梯度。最后使用`find\_largest\_gradient`函数打印输入大语言模型参数梯度的最大值：
# from [从零开始实现大语言模型（十三）：预训练大语言模型GPTModel] import calc\_loss\_batch
def find\_largest\_gradient(model):
max\_grad = None
for param in model.parameters():
if param.grad is not None:
grad\_values = param.grad.data.flatten()
max\_grad\_param = grad\_values.max()
if max\_grad is None or max\_grad\_param > max\_grad:
max\_grad = max\_grad\_param
return max\_grad
device = torch.device("cpu")
input\_batch = torch.tensor(
[[16833, 3626, 6100], # [["every effort moves"],
[40, 1107, 588]] # ["I really like"]]
)
target\_batch = torch.tensor(
[[3626, 6100, 345], # [[" effort moves you"],
[588, 428, 11311]] # [" really like chocolate"]]
)
loss = calc\_loss\_batch(input\_batch, target\_batch, gpt2\_small, device)
loss.backward()
print(find\_largest\_gradient(gpt2\_small))
执行上面代码，打印结果如下：
tensor(0.6413)
使用上述基于梯度范数的裁剪方法优化模型参数梯度，设置大语言模型参数梯度的2-范数最大值`max\_norm`为1.0，并打印经过Gradient
Clipping优化之后的大语言模型参数梯度的最大值：
torch.nn.utils.clip\_grad\_norm\_(gpt2\_small.parameters(), max\_norm=1.0)
print(find\_largest\_gradient(gpt2\_small))
执行上面代码，打印结果如下：
tensor(0.0348)
## 4\. 实现高阶预训练函数
可以结合上述3种高阶训练技巧实现预训练大语言模型的函数`hyper\_pretrain\_model`。修改前文[从零开始实现大语言模型（十三）：预训练大语言模型GPTModel]()中实现的预训练大语言模型的函数`pretrain\_model`，在每轮for循环使用`calc\_loss\_batch`函数计算大语言模型的预测输出与训练样本标签之间的交叉熵损失之前，先使用2中所述优化训练学习率的两种方法Learning
Rate Warmup和Cosine
Decay，计算当次迭代训练使用的学习率大小，并修改训练优化器中使用学习率的值。在使用`optimizer.step()`方法更新大语言模型参数之前，先使用3中所述优化模型参数梯度的方法Gradient
Clipping，优化模型参数梯度。具体代码如下所示：
# from [从零开始实现大语言模型（十三）：预训练大语言模型GPTModel] import calc\_loss\_loader, val\_and\_save
def hyper\_pretrain\_model(
model, optimizer, train\_loader, num\_epochs, device, eval\_freq, eval\_iter, tokenizer, start\_context,
save\_freq, checkpoint\_dir, warmup\_steps=10, initial\_lr=3e-05, min\_lr=1e-6, max\_norm=1.0,
checkpoint=None, val\_loader=None
):
if not os.path.exists(checkpoint\_dir):
os.makedirs(checkpoint\_dir, exist\_ok=True)
if checkpoint is not None:
model\_checkpoint\_path = os.path.join(checkpoint\_dir, f"model\_{checkpoint:06d}.pth")
optimizer\_checkpoint\_path = os.path.join(checkpoint\_dir, f"optimizer\_{checkpoint:06d}.pth")
model.load\_state\_dict(torch.load(model\_checkpoint\_path))
optimizer.load\_state\_dict(torch.load(optimizer\_checkpoint\_path))
else:
checkpoint = -1
train\_losses, val\_losses, track\_tokens\_seen, track\_lrs = [], [], [], []
tokens\_seen, global\_step = 0, -1
peak\_lr = optimizer.param\_groups[0]["lr"]
total\_training\_steps = len(train\_loader) \* num\_epochs
lr\_increment = (peak\_lr - initial\_lr) / warmup\_steps
for epoch in range(num\_epochs):
model.train()
for i, (input\_batch, target\_batch) in enumerate(train\_loader):
if global\_step % eval\_freq == 0:
model.train()
optimizer.zero\_grad()
global\_step += 1
if global\_step < warmup\_steps:
lr = initial\_lr + global\_step \* lr\_increment
else:
progress = (global\_step - warmup\_steps) / (total\_training\_steps - warmup\_steps)
lr = min\_lr + (peak\_lr - min\_lr) \* 0.5 \* (1 + math.cos(math.pi \* progress))
for param\_group in optimizer.param\_groups:
param\_group["lr"] = lr
track\_lrs.append(lr)
loss = calc\_loss\_batch(input\_batch, target\_batch, model, device)
loss.backward()
if global\_step > warmup\_steps:
torch.nn.utils.clip\_grad\_norm\_(model.parameters(), max\_norm=max\_norm)
optimizer.step()
tokens\_seen += input\_batch.numel()
print(f"Epoch {epoch + 1} (Batch {i:06d}): Train loss {loss.item():.3f}")
checkpoint, train\_loss, val\_loss = val\_and\_save(
model, optimizer, train\_loader, val\_loader, epoch, global\_step, eval\_freq,
eval\_iter, start\_context, tokenizer, save\_freq, checkpoint\_dir, checkpoint, device
)
if train\_loss is not None:
train\_losses.append(train\_loss)
val\_losses.append(val\_loss)
track\_tokens\_seen.append(tokens\_seen)
checkpoint, \_, \_ = val\_and\_save(
model, optimizer, train\_loader, val\_loader, epoch, global\_step, 1,
eval\_iter, start\_context, tokenizer, 1, checkpoint\_dir, checkpoint, device
)
print(f"Epoch {epoch + 1} finished, checkpoint: {checkpoint:06d}")
return train\_losses, val\_losses, track\_tokens\_seen, track\_lrs
## 5\. 优化训练超参数
### 5.1 Hyper-parameters Search
超参数(hyper-
parameters)是指需要在搭建和训练深度神经网络之前手动设置的一些参数。在深度学习中，有两类超参数，一类超参数是深度神经网络结构超参数，比如深度神经网络的层数，Embedding向量的维度等等。另一类超参数是训练超参数，例如训练深度神经网络使用的学习率，每个batch中训练样本数量等等。
优化深度神经网络结构超参数的方法被统称为神经网络结构搜索(NAS, neural architecture
search)。神经网络结构搜索方法大致可分类3类，其中一类被称为“大海捞针”，即根据实践经验定义一个有限的超参数搜索空间，逐一使用搜索空间中的超参数组合构建并训练深度神经网络直至收敛，取验证集上测试指标最高的超参数组合作为搜索结果。另一类是不可微方法，其一般会将验证集上的测试指标作为环境给的奖励，使用强化学习算法搜索出较优的超参数组合。还有一类是可微方法，其核心思想是定义一个神经网络结构超参数的可微函数作为目标函数，基于Super-
net对目标函数关于超参数求梯度，直接使用梯度更新超参数。
>
> 本文不会详细介绍深度神经网络结构超参数优化方法，不同大语言模型的结构基本相同，Embedding向量维度等结构超参数一般会取决于可用的计算资源，工业界实践中一般不会使用神经网络架构搜索方法确定大语言模型的结构超参数。《从零开始实现大语言模型》系列专栏全部完成之后，我应该会写几篇博客详细神经网络结构搜索，感兴趣的读者可以关注[我的个人博客]()。
预训练大语言模型的时间成本及计算成本都非常高，例如训练大语言模型Llama 2的数据共包含2T（万亿）个tokens，花费184320 A100
GPU时，换算成云计算资源价值，大约需要690000美元。在预训练大语言模型的工业界实践中，一般会在正式开始训预训练大语言模型之前，在相对小的数据集上，使用Hyper-
parameters Search得到一个比较好的训练超参数组合。Hyper-parameters
Search的核心思想就是“大海捞针”，即定义一个有限的超参数搜索空间`HPARAM\_GRID`，逐一使用搜索空间中的超参数组合训练大语言模型，取验证集上交叉熵损失最小的超参数组合作为正式预训练大语言模型时所用的训练超参数。具体代码如下所示：
import itertools
def hparams\_search\_train(
model, optimizer, train\_loader, val\_loader, num\_epochs, device,
eval\_iter, warmup\_steps, initial\_lr, min\_lr, max\_norm
):
global\_step = -1
peak\_lr = optimizer.param\_groups[0]["lr"]
total\_training\_steps = len(train\_loader) \* num\_epochs
lr\_increment = (peak\_lr - initial\_lr) / warmup\_steps
for epoch in range(num\_epochs):
model.train()
for input\_batch, target\_batch in train\_loader:
optimizer.zero\_grad()
global\_step += 1
if global\_step < warmup\_steps:
lr = initial\_lr + global\_step \* lr\_increment
else:
progress = (global\_step - warmup\_steps) / (total\_training\_steps - warmup\_steps)
lr = min\_lr + (peak\_lr - min\_lr) \* 0.5 \* (1 + math.cos(math.pi \* progress))
for param\_group in optimizer.param\_groups:
param\_group["lr"] = lr
loss = calc\_loss\_batch(input\_batch, target\_batch, model, device)
loss.backward()
if global\_step > warmup\_steps:
torch.nn.utils.clip\_grad\_norm\_(model.parameters(), max\_norm=max\_norm)
optimizer.step()
train\_loss = calc\_loss\_loader(train\_loader, model, device, eval\_iter)
val\_loss = calc\_loss\_loader(val\_loader, model, device, eval\_iter)
return train\_loss, val\_loss
HPARAM\_GRID = {
"batch\_size": [2, 4, 8, 16],
"dropout": [0.0, 0.1, 0.2],
"warmup\_steps": [10, 20, 30],
"weight\_decay": [0.1, 0.01, 0.0],
"max\_norm": [1.0, 0.5, 2.0],
"peak\_lr": [0.0001, 0.0005, 0.001, 0.005],
"initial\_lr": [0.00005, 0.0001],
"min\_lr": [0.00005, 0.00001, 0.0001],
"num\_epochs": [5, 10, 15, 20, 25],
}
hyperparameter\_combinations = list(itertools.product(\*HPARAM\_GRID.values()))
print(f"Total hyperparameter configurations: {len(hyperparameter\_combinations)}")
device = torch.device("cpu")
val\_data\_path = "val\_data"
train\_dataset = LLMDataset(train\_data\_path, vocabulary, special\_token\_id, context\_len, stride)
val\_dataset = LLMDataset(val\_data\_path, vocabulary, special\_token\_id, context\_len, stride)
best\_val\_loss, best\_train\_loss = float("inf"), float("inf")
best\_hparams = {}
for i, combination in enumerate(hyperparameter\_combinations):
print(f"Evaluating configuration {i + 1} of {len(hyperparameter\_combinations)}")
HPARAM\_CONFIG = dict(zip(HPARAM\_GRID.keys(), combination))
torch.manual\_seed(123)
train\_loader = DataLoader(dataset=train\_dataset, batch\_size=HPARAM\_CONFIG["batch\_size"], shuffle=True, drop\_last=True)
val\_loader = DataLoader(dataset=val\_dataset, batch\_size=HPARAM\_CONFIG["batch\_size"], shuffle=False, drop\_last=False)
model = GPTModel(
embedding\_dim=embedding\_dim, num\_layers=num\_layers, num\_heads=num\_heads, context\_len=context\_len,
vocabulary\_size=vocabulary\_size, dropout=HPARAM\_CONFIG["dropout"], qkv\_bias=qkv\_bias
)
model.to(device)
optimizer = torch.optim.AdamW(
model.parameters(), lr=HPARAM\_CONFIG["peak\_lr"],
weight\_decay=HPARAM\_CONFIG["weight\_decay"]
)
train\_loss, val\_loss = hparams\_search\_train(
model, optimizer, train\_loader, val\_loader, HPARAM\_CONFIG["num\_epochs"], device, eval\_iter=1,
warmup\_steps=HPARAM\_CONFIG["warmup\_steps"], initial\_lr=HPARAM\_CONFIG["initial\_lr"],
min\_lr=HPARAM\_CONFIG["min\_lr"], max\_norm=HPARAM\_CONFIG["max\_norm"]
)
if val\_loss < best\_val\_loss:
best\_val\_loss = val\_loss
best\_train\_loss = train\_loss
best\_hparams = HPARAM\_CONFIG
print(f"Evaluating configuration {i + 1} completed.")
print(f"Current best hyper-parameters: {best\_hparams}")
print(f"Current best Val loss: {best\_val\_loss} | Training loss {best\_train\_loss}")
print("============================================================================")
print("Hyper-parameter search completed.")
print(f"Best hyper-parameters: {best\_hparams}")
print(f"Best Val loss: {best\_val\_loss} | Training loss {best\_train\_loss}")
>
> 神经网络结构搜索领域的不可微方法并不适用于大语言模型训练超参数搜索，训练大语言模型所需的计算量太大，且使用强化学习算法搜索超参数，需要从头开始完整训练一次大语言模型才能获得1个奖励，强化学习算法一般至少需要上万至数十万次奖励反馈才能收敛。
>
>
> 神经网络结构搜索领域的可微方法同样不适用于大语言模型训练超参数搜索，所有可微方法的核心思想都是定义一个神经网络结构超参数的可微函数作为目标函数，然而基本没有办法找到一个神经网络训练超参数的可微函数。
## 6\. 结束语
预训练大语言模型的流程与训练普通神经深度网络模型本质上并没有任何不同，其难度不在于算法，而在于数据，更在于算力。绝大部分企业都没有预训练大语言模型的算力资源，因此如何利用开源大语言模型成了大语言模型工业实践中的重中之重，接下来一起看看如何加载开源大语言模型参数吧！