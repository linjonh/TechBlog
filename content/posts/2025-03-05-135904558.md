---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f4442415f4875616e677a6a2f:61727469636c652f64657461696c732f313335393034353538"
layout: post
title: "Azure-架构师学习笔记-Azure-Databricks-16-Delta-Lake-和-ADLS整合"
date: 2025-03-05 17:24:14 +0800
description: "上文提到了Delta Lake， 但是这是一个概念，如果落实到具体的资源服务上，又会有一定的修改和限制。本文介绍一下Delta Lake如何跟Azure Data Lake Store 整合。Delta Lake是一个开源框架，可以构建在ADLS之上。ADLS 并不内置事务保障或者Delta Lake提供的性能优化。所以单纯ADLS 很难满足现今的数据需求。"
keywords: "【Azure 架构师学习笔记】- Azure Databricks (16) -- Delta Lake 和 ADLS整合"
categories: ['架构师学习笔记', 'Databrikcs', 'Azure', 'Azure']
tags: ['Azuredatabricks', 'Azure']
artid: "135904558"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=135904558
    alt: "Azure-架构师学习笔记-Azure-Databricks-16-Delta-Lake-和-ADLS整合"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=135904558
featuredImagePreview: https://bing.ee123.net/img/rand?artid=135904558
cover: https://bing.ee123.net/img/rand?artid=135904558
image: https://bing.ee123.net/img/rand?artid=135904558
img: https://bing.ee123.net/img/rand?artid=135904558
---

# 【Azure 架构师学习笔记】- Azure Databricks (16) -- Delta Lake 和 ADLS整合

> 本文属于
> [【Azure 架构师学习笔记】系列](https://blog.csdn.net/dba_huangzj/category_12145228.html)
> 。
>   
> 本文属于【Azure Databricks】系列。
>   
> 接上文
> [【【Azure 架构师学习笔记】- Azure Databricks (15) --Delta Lake 和Data Lake](https://blog.csdn.net/DBA_Huangzj/article/details/145638901)

## 前言

上文提到了Delta Lake， 但是这是一个概念，如果落实到具体的资源服务上，又会有一定的修改和限制。本文介绍一下Delta Lake如何跟Azure Data Lake Store 整合。

Delta Lake是一个开源框架，可以构建在ADLS之上。ADLS 并不内置事务保障或者Delta Lake提供的性能优化。所以单纯ADLS 很难满足现今的数据需求。

## Delta Lake 与ADLS 的优势互补

ADLS 对于“存”操作是很快速的，但是对于“搜索、list”文件则很慢。Delta Lake通过减少昂贵的文件列举操作而加快数据检索。
  
所有的云存储系统使用扁平的命名空间。不像文件系统的层级结构，所以在list文件时很慢，就类似没有索引的数据库中的数据表，需要遍历所有数据。 ADLS 没有真正的目录，而是在对象名字前面添加前缀来实现。所以list 文件意味着系统要扫描全部文件，并且用前缀筛选。

另外， ADLS 上如果受到API 过度调用， 则会出现瓶颈，特别是对大数据集。如果一个大表的数据被拆分到ADLS 上的很多文件，那么ADLS 就要遍历所有文件，然后筛选，从而导致额外的延时。

对此，Delta Lake把文件路径存储在一个事务日志文件中，就如数据库中的索引，list文件时先从事务日志文件中查找，然后再去具体的路径中检索数据。

## 使用演示

下面使用Spark来演示一下如何操作Delta Lake，这里借用前面的ADB 环境，ADB 实际上非必须，只是借用上面的Spark环境。

### 环境配置

使用下面python命令配置Spark和Delta， 替换代码中的中文字部分即可。

```python
%python
import pyspark
from delta import *

extra_packages = [
    "org.apache.hadoop:hadoop-azure:3.3.4",
    # version must match hadoop-client-runtime
]

builder = (
    pyspark.sql.SparkSession.builder.appName("DeltaOnADLS")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.hadoop.fs.azurebfs.impl", "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem")
    .config("fs.azure.account.key.ADLS 名字.dfs.core.windows.net", "你的ADLS 的access key")
)

spark = configure_spark_with_delta_pip(
    builder, extra_packages=extra_packages
).getOrCreate()

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ae7eea91ad1640779ab7b78240984f9a.png)
  
如果过程中报这样的错： “org.apache.hadoop.fs.FileAlreadyExistsException: Operation failed: “This endpoint does not support BlobStorageEvents or SoftDelete. Please disable these account features if you would like to use this endpoint.”
  
”
  
需要禁用ADLS 中这个设置，因为它影响了Delta的ACID 特性。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/542270c404904888b18d8e1e996ed37e.png)

### 写入数据

```python

# Create sample data
data = spark.range(0, 5)

# Write data to a Delta table in ADLS
data.write.format("delta").save("abfss://bronze@medallionadls01.dfs.core.windows.net/delta-table")


```

写入后从ADLS 上可以看到新建了一个文件夹：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0178298944e34c6383923ef9661e55b8.png)
  
文件夹内部是具有transaction log (这里是\_delta\_log)的文件集合。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6dfefdc4412849fda2d7ac79a1d6014e.png)

### 读取数据

执行下面命令，注意修改路径为上面写入数据的路径和文件夹

```python
# Read Delta table from ADLS
df = spark.read.format("delta").load("abfss://bronze@medallionadls01.dfs.core.windows.net/delta-table")
df.show()

```

结果如下：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6be8b907e66143bda266dddbd5c153e5.png)

### 更新和删除数据

```python
from delta.tables import DeltaTable

# Load Delta table
deltaTable = DeltaTable.forPath(spark, "abfss://bronze@medallionadls01.dfs.core.windows.net/delta-table")

# Update rows where id is less than 4
deltaTable.update("id < 4", {"id": "id + 20"})

# Delete rows where id is less than 5
deltaTable.delete("id < 5")

# Read Delta table from ADLS
df = spark.read.format("delta").load("abfss://bronze@medallionadls01.dfs.core.windows.net/delta-table")
df.show()

```

结果如下：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/843678e93b9b478ca9176aad470e70ab.png)

### Merge数据

```python
# Merge new data into Delta table
new_data = spark.range(0, 30)

deltaTable.alias("old").merge(
    new_data.alias("new"),
    "old.id = new.id"
).whenMatchedUpdate(set={"id": "new.id"}).whenNotMatchedInsert(values={"id": "new.id"}).execute()

# Read Delta table from ADLS
df = spark.read.format("delta").load("abfss://bronze@medallionadls01.dfs.core.windows.net/delta-table")
df.show()


```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c711303c2cea4af2a5d22f60f1432b16.png)

## 小结

本文演示了如何在ADLS 上搭建delta并进行了简单的数据操作。