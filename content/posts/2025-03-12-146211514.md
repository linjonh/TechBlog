---
layout: post
title: "第4节分类任务"
date: 2025-03-12 18:19:57 +0800
description: "【梯度消失和梯度爆炸：求梯度即从后往前链式求导，如果层数太多，假如有100层，如果所有导数都是0.02，那么0.02的100次方就趋向于0，也就是梯度趋向于零，这时没有办法更新参数，也就是梯度消失；当我们需要判断某张图是不是鸟，我们并不需要看完整张图，而是可以把鸟嘴，或者爪子等等各个很小的部分作为卷积核去和整张图卷积，得到的值越大就说明越像，进而可以说明这张图是不是鸟。其实也是可以的，如下图所示，把五个卷积核得到的结果再叠放起来，还可以继续卷（但应注意此时的卷积核维度要发生变化，不能还是。"
keywords: "第4节：分类任务"
categories: ['深度学习自学记录']
tags: ['深度学习']
artid: "146211514"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146211514
    alt: "第4节分类任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146211514
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146211514
cover: https://bing.ee123.net/img/rand?artid=146211514
image: https://bing.ee123.net/img/rand?artid=146211514
img: https://bing.ee123.net/img/rand?artid=146211514
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     第4节：分类任务
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <strong>
      引入：
     </strong>
    </p>
    <p>
     独热编码（one-hot）：对于分类任务的输出，也就是是或不是某类的问题，采取独热编码的形式将y由一离散值转化为连续的概率分布，最大值所在下标为预测类
    </p>
    <p>
     输入的处理：对于任意一张彩色图片，通常转化为用3
     <em>
      224
     </em>
     224的矩阵表示（通道数，高度，宽度），如果直接展平这个图片，就会得到
     <code>
      224*224+224*224+224*224
     </code>
     个参数，直接用这么多参数去全连接肯定是不合适的，正确处理方法——卷积神经网络
    </p>
    <p>
     卷积神经网络：卷积操作是通过卷积核在输入图片上滑动，每次计算卷积核覆盖区域与卷积核的
     <strong>
      逐元素乘积
     </strong>
     ，然后将所有乘积结果
     <strong>
      求和
     </strong>
     ，得到输出特征图的一个值【输出的结果越大，就说明大图中的这一个部分和卷积核越像】
    </p>
    <pre><code class="prism language-cpp">输入图片【特征图】
<span class="token punctuation">[</span>  
  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
  <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>  
<span class="token punctuation">]</span>

卷积核
<span class="token punctuation">[</span>  
  <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
  <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>  
<span class="token punctuation">]</span>
  
输出结果【特征图】
<span class="token punctuation">[</span>  
  <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  
  <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>  
<span class="token punctuation">]</span>
</code></pre>
    <p>
     但是按照这样的规则卷积，会使得特征图尺寸变小，
     <strong>
      若考虑维持特征图的尺寸，则可以考虑通过zeropadding操作，外圈补零
     </strong>
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/3e6b08d2ddc24e45b469f5b997d18eb1.png#pic_center">
      <br/>
      <br/>
      <br/>
      <br/>
     </img>
    </p>
    <p>
     <strong>
      卷积神经网路和图片分类的关系：
     </strong>
    </p>
    <p>
     当我们需要判断某张图是不是鸟，我们并不需要看完整张图，而是可以把鸟嘴，或者爪子等等各个很小的部分作为卷积核去和整张图卷积，得到的值越大就说明越像，进而可以说明这张图是不是鸟
    </p>
    <p>
     但是我们怎么知道卷积核就是鸟嘴呢？？？
    </p>
    <p>
     所以我们让深度学习来训练的目标就是，让卷积核变成我们想要的
    </p>
    <p>
     【
     <strong>
      卷积核就是深度学习中的参数
     </strong>
     ，卷积核大小称为神经元的感受野，使用更大的卷积核就可以有更大的感受野】
     <br/>
     <br/>
     <br/>
     <br/>
     <br/>
     <br/>
    </p>
    <p>
     <strong>
      卷积实例：
     </strong>
    </p>
    <p>
     <strong>
      【核心思想：把3
      <em>
       224
      </em>
      224的图片通过多次卷积多次池化变为1024
      <em>
       7
      </em>
      7（减少参数），然后拉直展平得到一维向量1024
      <em>
       7
      </em>
      7=50176，然后通过全连接实现linear(50176,3)，将结果通过softmax得到分类的概率分布结果，最后求预测和实际的loss，梯度回传更新参数（即卷积核），循环往复……】
     </strong>
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/66f13f42915042d5836042592edcad37.png#pic_center"/>
    </p>
    <blockquote>
     <p>
      第一步：卷积+池化
     </p>
    </blockquote>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7a298143dae1494a9ec448a5cb3e8c28.png#pic_center"/>
    </p>
    <p>
     输入和卷积核都是图片的形式，也就是都是三通道的，所以卷积的过程也是三个通道同时卷积，也就是两个
     <code>
      3*3*3
     </code>
     的矩阵进行卷积，对应位置相乘有27个数，再相加得到一个数
    </p>
    <p>
     【这样其实实现了参数的减少，和引入卷积的目的对应上了！但是只依靠卷积每次维度减少2未免也太少了，或者使用了padding特征图大小直接是不变的，这里先不讨论，看后面】
    </p>
    <p>
     如果想继续卷呢？其实也是可以的，如下图所示，把五个卷积核得到的结果再叠放起来，还可以继续卷（但应注意此时的卷积核维度要发生变化，不能还是
     <code>
      3*3*3
     </code>
     了，要变成
     <code>
      5*3*3
     </code>
     ）
    </p>
    <p>
     【而且后续再卷就不太能用“卷积结果越大越相似”这种直观的理解来解释了，又变得黑盒了】
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/638c107f092146e8b24db0d117aff6c2.png#pic_center">
      <br/>
      <br/>
     </img>
    </p>
    <p>
     那么应该如何减小特征图大小呢？
    </p>
    <p>
     方法一：扩大步长 【不常用，会丢失信息，且引入计算】
     <br/>
     【公式：(输入-卷积核+2padding) / 步长+1 】
    </p>
    <p>
     方法二：pooling池化【常用！】
     <br/>
     【平均池化又引入了计算。。。实际使用时更常用最大池化，取最显著的即可】
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/6f846cfed95c44bdbcd44a802754c959.png#pic_center">
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f83f3d3732ee486690a3bec5ca584d92.png#pic_center"/>
     </img>
    </p>
    <blockquote>
     <p>
      第二步：卷积到全连接
     </p>
    </blockquote>
    <p>
     （略）
    </p>
    <blockquote>
     <p>
      第三步：结果到输出
     </p>
    </blockquote>
    <p>
     其实就是把左边的结果过一遍softmax（直接调用），变成右边的概率分布（三种概率和为1）
    </p>
    <p>
     也就是说该图片是猫的概率为0.953
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7a6e3464e73940279adc6e5926888a22.png#pic_center"/>
    </p>
    <blockquote>
     <p>
      第四步：求预测和结果的loss
     </p>
    </blockquote>
    <p>
     衡量分类问题的损失：交叉熵损失CrossEntropy Loss（直接调用）
    </p>
    <p>
     具体公式先算了
     <br/>
     <br/>
     <br/>
     <br/>
     <br/>
     <strong>
      扩展内容：
     </strong>
    </p>
    <p>
     <strong>
      AlexNet：relu+dropout+池化+归一化
     </strong>
    </p>
    <p>
     【relu：比sigmoid更能预防梯度消失，因为relu大于0的部分求导始终为1，而sigmoid趋向无穷大的时候导数趋向于0了（结合后面梯度消失理解）】
    </p>
    <p>
     【dropout：在每一轮训练中随机取一些神经元不用，缓解过拟合】
    </p>
    <p>
     【过拟合：参数越多越容易过拟合】
    </p>
    <p>
     【池化：就是前面讲的池化】
    </p>
    <p>
     【归一化：防止受到数据量纲影响，相对大小才有意义，保持学习有效性，缓解梯度消失和梯度爆炸】
     <br/>
     <br/>
     <br/>
     <strong>
      vgg：更深更大，用小卷积核代替大卷积核
     </strong>
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9fab2f6507394470b3e97d9f235ec07c.png#pic_center"/>
    </p>
    <p>
     比如上图，一个5
     <em>
      5的卷积核即实现1块覆盖25块（25合1），而两个3
     </em>
     3的卷积核（卷了两次）可以代替一个5*5的卷积核，相比之下参数量更小（18和25），防止过拟合
     <br/>
     <br/>
    </p>
    <p>
     <strong>
      ResNet：一乘一卷积+残差连接
     </strong>
    </p>
    <p>
     解决问题：模型越深反而效果越差，而且不是因为过拟合，这是因为梯度消失合梯度爆炸
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fac4073c9a324f65b8ba71adabd1e8c3.png#pic_center"/>
    </p>
    <p>
     【梯度消失和梯度爆炸：求梯度即从后往前链式求导，如果层数太多，假如有100层，如果所有导数都是0.02，那么0.02的100次方就趋向于0，也就是梯度趋向于零，这时没有办法更新参数，也就是梯度消失；反之如果所有导数都是10，那么10的100次方就是梯度爆炸】
    </p>
    <p>
     【梯度爆炸比较好处理，可以人为设置导数超过1时就设置为1，但是梯度消失很难处理，考虑残差连接】
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/03faaba56ebf42d9b03ecec036df32c5.png#pic_center"/>
    </p>
    <p>
     上图所示即为残差连接，模型的输出=模型原本的输出+模型原本的输入，这样out对x求导永远是大于1的，可以防止梯度消失！
    </p>
    <p>
     但是模型的作用一般就是调整维度，所以模型的输入和输出维度不一致要怎么相加？用一乘一卷积做调整
    </p>
    <p>
     <img alt="IMG_5453.jpeg" src="https://i-blog.csdnimg.cn/img_convert/2fa201403f8affbf33af13a46276e1b3.jpeg"/>
    </p>
    <p>
     一乘一卷积的另一个作用是：比直接卷积减少了参数量
    </p>
    <p>
     <img alt="IMG_5451.jpeg" src="https://i-blog.csdnimg.cn/img_convert/4e791cf5d91eff711a6d539fe6d9184d.jpeg"/>
     <br/>
     <br/>
     <br/>
     <br/>
    </p>
    <p>
     <strong>
      深度透析：
     </strong>
    </p>
    <p>
     <strong>
      卷积 = 一种参数共享的“不全连接”
     </strong>
    </p>
    <p>
     【参数共享：指卷积核的w1234是共享的】
    </p>
    <p>
     【不全连接：如下图所示，对第一个卷积结果2来说，它只和1245这四个位置有关，因为就是这么卷的。好处是比全连接有效减少了参数！不容易过拟合！】
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9a3d06e071eb4f3d8166b15a97e13dc1.png#pic_center"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35313037363236372f:61727469636c652f64657461696c732f313436323131353134" class_="artid" style="display:none">
 </p>
</div>


