---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35313037363236372f:61727469636c652f64657461696c732f313436323131353134"
layout: post
title: "第4节分类任务"
date: 2025-03-12 18:19:57 +08:00
description: "【梯度消失和梯度爆炸：求梯度即从后往前链式求导，如果层数太多，假如有100层，如果所有导数都是0.02，那么0.02的100次方就趋向于0，也就是梯度趋向于零，这时没有办法更新参数，也就是梯度消失；当我们需要判断某张图是不是鸟，我们并不需要看完整张图，而是可以把鸟嘴，或者爪子等等各个很小的部分作为卷积核去和整张图卷积，得到的值越大就说明越像，进而可以说明这张图是不是鸟。其实也是可以的，如下图所示，把五个卷积核得到的结果再叠放起来，还可以继续卷（但应注意此时的卷积核维度要发生变化，不能还是。"
keywords: "第4节：分类任务"
categories: ['深度学习自学记录']
tags: ['深度学习']
artid: "146211514"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146211514
    alt: "第4节分类任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146211514
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146211514
cover: https://bing.ee123.net/img/rand?artid=146211514
image: https://bing.ee123.net/img/rand?artid=146211514
img: https://bing.ee123.net/img/rand?artid=146211514
---

# 第4节：分类任务

**引入：**

独热编码（one-hot）：对于分类任务的输出，也就是是或不是某类的问题，采取独热编码的形式将y由一离散值转化为连续的概率分布，最大值所在下标为预测类

输入的处理：对于任意一张彩色图片，通常转化为用3
*224*
224的矩阵表示（通道数，高度，宽度），如果直接展平这个图片，就会得到
`224*224+224*224+224*224`
个参数，直接用这么多参数去全连接肯定是不合适的，正确处理方法——卷积神经网络

卷积神经网络：卷积操作是通过卷积核在输入图片上滑动，每次计算卷积核覆盖区域与卷积核的
**逐元素乘积**
，然后将所有乘积结果
**求和**
，得到输出特征图的一个值【输出的结果越大，就说明大图中的这一个部分和卷积核越像】

```cpp
输入图片【特征图】
[  
  [1, 0, 1],  
  [0, 1, 0],  
  [1, 0, 1]  
]

卷积核
[  
  [1, 0],  
  [0, -1]  
]
  
输出结果【特征图】
[  
  [0, 0],  
  [0, 0]  
]

```

但是按照这样的规则卷积，会使得特征图尺寸变小，
**若考虑维持特征图的尺寸，则可以考虑通过zeropadding操作，外圈补零**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3e6b08d2ddc24e45b469f5b997d18eb1.png#pic_center)

**卷积神经网路和图片分类的关系：**

当我们需要判断某张图是不是鸟，我们并不需要看完整张图，而是可以把鸟嘴，或者爪子等等各个很小的部分作为卷积核去和整张图卷积，得到的值越大就说明越像，进而可以说明这张图是不是鸟

但是我们怎么知道卷积核就是鸟嘴呢？？？

所以我们让深度学习来训练的目标就是，让卷积核变成我们想要的

【
**卷积核就是深度学习中的参数**
，卷积核大小称为神经元的感受野，使用更大的卷积核就可以有更大的感受野】

**卷积实例：**

**【核心思想：把3
*224*
224的图片通过多次卷积多次池化变为1024
*7*
7（减少参数），然后拉直展平得到一维向量1024
*7*
7=50176，然后通过全连接实现linear(50176,3)，将结果通过softmax得到分类的概率分布结果，最后求预测和实际的loss，梯度回传更新参数（即卷积核），循环往复……】**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/66f13f42915042d5836042592edcad37.png#pic_center)

> 第一步：卷积+池化

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7a298143dae1494a9ec448a5cb3e8c28.png#pic_center)

输入和卷积核都是图片的形式，也就是都是三通道的，所以卷积的过程也是三个通道同时卷积，也就是两个
`3*3*3`
的矩阵进行卷积，对应位置相乘有27个数，再相加得到一个数

【这样其实实现了参数的减少，和引入卷积的目的对应上了！但是只依靠卷积每次维度减少2未免也太少了，或者使用了padding特征图大小直接是不变的，这里先不讨论，看后面】

如果想继续卷呢？其实也是可以的，如下图所示，把五个卷积核得到的结果再叠放起来，还可以继续卷（但应注意此时的卷积核维度要发生变化，不能还是
`3*3*3`
了，要变成
`5*3*3`
）

【而且后续再卷就不太能用“卷积结果越大越相似”这种直观的理解来解释了，又变得黑盒了】

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/638c107f092146e8b24db0d117aff6c2.png#pic_center)

那么应该如何减小特征图大小呢？

方法一：扩大步长 【不常用，会丢失信息，且引入计算】
  
【公式：(输入-卷积核+2padding) / 步长+1 】

方法二：pooling池化【常用！】
  
【平均池化又引入了计算。。。实际使用时更常用最大池化，取最显著的即可】

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6f846cfed95c44bdbcd44a802754c959.png#pic_center)
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/f83f3d3732ee486690a3bec5ca584d92.png#pic_center)

> 第二步：卷积到全连接

（略）

> 第三步：结果到输出

其实就是把左边的结果过一遍softmax（直接调用），变成右边的概率分布（三种概率和为1）

也就是说该图片是猫的概率为0.953

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7a6e3464e73940279adc6e5926888a22.png#pic_center)

> 第四步：求预测和结果的loss

衡量分类问题的损失：交叉熵损失CrossEntropy Loss（直接调用）

具体公式先算了
  
  
  
  
  
**扩展内容：**

**AlexNet：relu+dropout+池化+归一化**

【relu：比sigmoid更能预防梯度消失，因为relu大于0的部分求导始终为1，而sigmoid趋向无穷大的时候导数趋向于0了（结合后面梯度消失理解）】

【dropout：在每一轮训练中随机取一些神经元不用，缓解过拟合】

【过拟合：参数越多越容易过拟合】

【池化：就是前面讲的池化】

【归一化：防止受到数据量纲影响，相对大小才有意义，保持学习有效性，缓解梯度消失和梯度爆炸】
  
  
  
**vgg：更深更大，用小卷积核代替大卷积核**

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9fab2f6507394470b3e97d9f235ec07c.png#pic_center)

比如上图，一个5
*5的卷积核即实现1块覆盖25块（25合1），而两个3*
3的卷积核（卷了两次）可以代替一个5*5的卷积核，相比之下参数量更小（18和25），防止过拟合

**ResNet：一乘一卷积+残差连接**

解决问题：模型越深反而效果越差，而且不是因为过拟合，这是因为梯度消失合梯度爆炸

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/fac4073c9a324f65b8ba71adabd1e8c3.png#pic_center)

【梯度消失和梯度爆炸：求梯度即从后往前链式求导，如果层数太多，假如有100层，如果所有导数都是0.02，那么0.02的100次方就趋向于0，也就是梯度趋向于零，这时没有办法更新参数，也就是梯度消失；反之如果所有导数都是10，那么10的100次方就是梯度爆炸】

【梯度爆炸比较好处理，可以人为设置导数超过1时就设置为1，但是梯度消失很难处理，考虑残差连接】

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/03faaba56ebf42d9b03ecec036df32c5.png#pic_center)

上图所示即为残差连接，模型的输出=模型原本的输出+模型原本的输入，这样out对x求导永远是大于1的，可以防止梯度消失！

但是模型的作用一般就是调整维度，所以模型的输入和输出维度不一致要怎么相加？用一乘一卷积做调整

![IMG_5453.jpeg](https://i-blog.csdnimg.cn/img_convert/2fa201403f8affbf33af13a46276e1b3.jpeg)

一乘一卷积的另一个作用是：比直接卷积减少了参数量

![IMG_5451.jpeg](https://i-blog.csdnimg.cn/img_convert/4e791cf5d91eff711a6d539fe6d9184d.jpeg)

**深度透析：**

**卷积 = 一种参数共享的“不全连接”**

【参数共享：指卷积核的w1234是共享的】

【不全连接：如下图所示，对第一个卷积结果2来说，它只和1245这四个位置有关，因为就是这么卷的。好处是比全连接有效减少了参数！不容易过拟合！】

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9a3d06e071eb4f3d8166b15a97e13dc1.png#pic_center)