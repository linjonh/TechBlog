---
layout: post
title: "Spark-环境下-Scala-和-Python-两种语言的对比"
date: 2024-11-08 16:40:27 +0800
description: "2015年前后，互联网行业中的“ 大数据” 概念掀起一股热潮。而Apache Spark作为类Had"
keywords: "python scala 对比"
categories: ['Python']
tags: ['Python']
artid: "93739973"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=93739973
    alt: "Spark-环境下-Scala-和-Python-两种语言的对比"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=93739973
featuredImagePreview: https://bing.ee123.net/img/rand?artid=93739973
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Spark 环境下 Scala 和 Python 两种语言的对比！
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     2015年前后，互联网行业中的“
     <strong>
      大数据”
     </strong>
     概念掀起一股热潮。
    </p>
    <p>
     而Apache Spark作为类Hadoop MapReduce的通用并行框架，一款专为大规模数据处理而设计的分布式计算引擎，以其优越的性能，较为完善的生态，受到了大数据从业人员的青睐。
    </p>
    <p>
     Spark的框架使用Scala编写 （注：Scala是一种运行在Java虚拟机上，实现和Java类库互联互通的面向对象及函数式编程语言） ， 而Spark的开发目前主要使用三种语言：Scala、Python、Java。
    </p>
    <p>
     相比于Java，Spark中用Scala开发语法简洁许多，且支持类型推断，可大大提升开发效率。 更为重要的是，Java不支持REPL（Read-Evaluate-Print-Loop交互式编程环境），而REPL又对数据处理十分关键（很多时候需要即时查看结果）。
    </p>
    <p>
     可以说Spark中的开发工作，Scala相对Java胜出了。
    </p>
    <p>
     Python学习交流群：1004391443
    </p>
    <p>
     那么，一向以简洁易上手，“可读性爆表”著称，且拥有交互式编程环境的Python，在Spark环境下与Scala相比又如何呢？
    </p>
    <p>
     本文将从以下几个方面来谈。
    </p>
    <p>
     性能
    </p>
    <p>
     Python作为一门解释型语言，性能可以说是从业人员诟病最多的一环。
    </p>
    <p>
     Scala基于Java Virtual Machine，在数据分析处理过程中比Python快上近10倍，另外Scala可以无缝调用Java API，所以它同Hadoop框架（由Java开发）的交互、兼容非常好；Python在这方面就相形见绌了，在Spark环境下想实现同HDFS的交互，开发人员甚至需要使用第三方插件，如hadopy。
    </p>
    <p>
     同时，用Python代码去调Spark库性能平庸，且在多进程并行之下比等效的Scala代码慢许多。
    </p>
    <p>
     不过，速度不够，硬件（核数）来凑，Python这种由于语言特性导致的性能弱势，会随着核数的增加而被填补。而当核数充足的情况下，性能也通常不会是我们在Spark开发中决定选择Scala/Python的最主要因素。
    </p>
    <p>
     上手难度/语法
    </p>
    <p>
     在Python Console中输入importthis显示出的“Python之禅”，是对Python使用的一篇指导规则，“人生苦短，我用Python”更是业内无数Python程序员奉为圭臬的信条。
    </p>
    <p>
    </p>
    <p>
     <img alt="" class="has" src="http://p2.pstatp.com/large/pgc-image/3ede4cc357dc419eb0339736f73aaf43"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     （Python之禅）
    </p>
    <p>
     阅读一个命名规范良好的Python程序就像阅读英文一样流畅。这种接近伪代码的代码，能够使你高效的解决问题，无论是在Spark中还是其他环境。
    </p>
    <p>
     此外，Python的中括号（切片操作）真是谁用谁知道，用过都说好。相比之下，Spark编程时用到Scala的take/takeRight方法，以及substring配以indexOf，打包编译几分钟，刚跑起来报个下标越界：-1的异常，着实令人抓狂。
    </p>
    <p>
     编写Scala代码，通常需要添加并不是很有意义但又不得不加的“val”或“var”关键字，而Python“生死看淡，上来就干”，想一个漂亮的变量名，后面即取即用。
    </p>
    <p>
    </p>
    <p>
     <img alt="" class="has" src="http://p2.pstatp.com/large/pgc-image/12ad6126cca64c66b174c3df02b8ccc4"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     不过，笔者真的为Scala的API链式调用感到畅快，能一段搞定的话还是比绞尽脑汁去想花里胡哨的变量名来得实在。
    </p>
    <p>
     此外Scala有些独特的语法规则，像通常不用“return”关键字，将函数体的最后一行作为返回值，可能会让其他语言的程序员上手初期感到些许不适，但Spark中有些Scala语法糖特特特别甜 （像RDD入HBase库经过包装之后的API十分简洁明了）。
    </p>
    <p>
     总得来说，Python语法简单，有着更加标准的程序库，适合简单的逻辑处理，而Scala更适合复杂的工作流。
    </p>
    <p>
     并发性
    </p>
    <p>
     CPython解释器中，由于GIL（全局解释器锁）的存在，使用Python写Spark程序时，不管进程有多少线程，每次只有一个CPU在进程中处于活动状态。
    </p>
    <p>
    </p>
    <p>
     <img alt="" class="has" src="http://p2.pstatp.com/large/pgc-image/162f8355f41545b1a862104036795fa7"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     GIL虽然保证了内存管理的线程安全，但每当需要部署新的代码/程序时，就得新启动更多的进程，需要额外的内存开销。在此场景下，Scala就显得更为高效和好用了。
    </p>
    <p>
     类型安全
    </p>
    <p>
     产品需求是不断变化的，代码重构是时刻准备的。
    </p>
    <p>
    </p>
    <p>
     <img alt="" class="has" src="http://p2.pstatp.com/large/pgc-image/d0d38733809443f98544d0dc66d13a97"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     Scala具有优雅的类型推断机制，使得它看上去像是一门动态类型语言，不过，作为一门严格意义的静态类型语言，它令你在省去明确指定类型的同时保证类型安全。
    </p>
    <p>
     Scala写Spark程序，可以在编译时就捕获到类型不匹配的错误。用静态类型语言重构Spark程序比动态语言容易许多，有时你可能会发觉，更改Python代码后新产生的bug比修复的原有程序的bug还多 （如遇此情形，送上一首《易燃易爆炸》，望笑纳） 。Python中有种哲学叫“duck typing”，类型检查的微妙程度可见一斑。
    </p>
    <p>
    </p>
    <p>
     <img alt="" class="has" src="http://p2.pstatp.com/large/pgc-image/593a09884e0349fcb79f67888879c833"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     Spark集成
    </p>
    <p>
     Spark框架的原生语言是Scala，当企业级应用需实现某些特定功能，要修改底层源码时，或功能呈现不及预期，需要排查原因时，使用Scala会更加得心应手；在执行调优、优化时，Scala也会更加方便。
    </p>
    <p>
     Python代码在JVM中会被包装，因此无法控制函数中包含的内容。此外，最新的Spark版本中的一些新功能可能仅在Scala中可用，然后才能在Python中移植。Scala在工程方面相对更有优势。
    </p>
    <p>
     高级特性/应用
    </p>
    <p>
     想来，Python当前被看作人工智能领域的首选语言，多少有scikit-learn的功劳，其中集成了各种特征工程API和常用算法。
    </p>
    <p>
     相比之下，Scala没有足够的数据科学工具和库，Spark-mllib中只集成了部分常用的机器学习方法，但优势是实现了分布式，可用于大规模数据处理。
    </p>
    <p>
     关于交互式分析工具，Scala有Zeppelin，Python有Jupyter，也都可以很好地实现数据分析和可视化，算是平分秋色。流式计算方面，Scala是最佳选择，因为Python通过PySpark调SparkStreaming不及Scala高效和成熟。
    </p>
    <p>
     结语
    </p>
    <p>
     本文针对Apache Spark环境下的两种主流语言Scala和Python，从几个维度切入做了分析和对比。
    </p>
    <p>
     总得来说，Python更加面向分析，而Scala更加面向工程，但它们都是构建数据科学应用程序的优秀语言。
    </p>
    <p>
     最后提一句，即便一上来就限定了Spark开发编程的大环境，笔者也只是粗略的对两种语言做了比对，丝毫没有也不敢妄言定论孰优孰劣。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f507974686f6e637879:2f61727469636c652f64657461696c732f3933373339393733" class_="artid" style="display:none">
 </p>
</div>


