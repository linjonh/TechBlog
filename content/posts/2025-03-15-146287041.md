---
layout: post
title: "注意力机制让AI拥有黄金七秒记忆的魔法-自注意力"
date: 2025-03-15 22:44:12 +0800
description: "⾃注意⼒就是⾃⼰对⾃⼰的注意，它允许模型在同⼀序列中的不同位置之间建⽴依赖关系。⽤我们刚才讲过的最简单的注意⼒来理解，如果我们把x2替换为x1⾃身，那么我们其实就实现了x1每⼀个位置对⾃身其他序列的所有位置的加权和。"
keywords: "注意力机制：让AI拥有黄金七秒记忆的魔法--（自注意力）"
categories: ['机器学习', '大模型']
tags: ['深度学习', '人工智能', 'Python']
artid: "146287041"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146287041
    alt: "注意力机制让AI拥有黄金七秒记忆的魔法-自注意力"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146287041
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146287041
cover: https://bing.ee123.net/img/rand?artid=146287041
image: https://bing.ee123.net/img/rand?artid=146287041
img: https://bing.ee123.net/img/rand?artid=146287041
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     注意力机制：让AI拥有黄金七秒记忆的魔法--（自注意力）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2>
     <a id="AI_0">
     </a>
     注意力机制：让AI拥有"黄金七秒记忆"的魔法–（自注意力）
    </h2>
    <p>
     ⾃注意⼒就是⾃⼰对⾃⼰的注意，它允许模型在同⼀序列中的不同位置之间建⽴依赖关系。⽤我们刚才讲过的最简单的注意⼒来理解，如果我们把x2替换为x1⾃身，那么我们其实就实现了x1每⼀个位置对⾃身其他序列的所有位置的加权和。
    </p>
    <p>
     如下：
    </p>
    <pre><code class="prism language-py"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token comment"># 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token comment"># 计算原始权重，形状为 (batch_size, seq_len, seq_len)</span>
raw_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 对原始权重进行 softmax 归一化，形状为 (batch_size, seq_len, seq_len)</span>
attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>raw_weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
<span class="token comment"># 计算加权和，形状为 (batch_size, seq_len, feature_dim) </span>
attn_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> x<span class="token punctuation">)</span>
</code></pre>
    <p>
     知道了这个，那么下面继续展示一下如何对输入序列进行不同的线性变换，得到Q，K和V向量，然后应用放缩点积注意力即可：
    </p>
    <pre><code class="prism language-py"><span class="token comment"># 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, feature_dim)</span>
<span class="token comment"># 定义线性层用于将 x 转换为 Q, K, V 向量</span>
linear_q <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
linear_k <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
linear_v <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token comment"># 通过线性层计算 Q, K, V</span>
Q <span class="token operator">=</span> linear_q<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, feature_dim)</span>
K <span class="token operator">=</span> linear_k<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, feature_dim)</span>
V <span class="token operator">=</span> linear_v<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, feature_dim)</span>
<span class="token comment"># 计算 Q 和 K 的点积，作为相似度分数 , 也就是自注意力原始权重</span>
raw_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, seq_len)</span>
<span class="token comment"># 将自注意力原始权重进行缩放</span>
scale_factor <span class="token operator">=</span> K<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>  <span class="token comment"># 这里是 4 ** 0.5</span>
scaled_weights <span class="token operator">=</span> raw_weights <span class="token operator">/</span> scale_factor <span class="token comment"># 形状 (batch_size, seq_len, seq_len)</span>
<span class="token comment"># 对缩放后的权重进行 softmax 归一化，得到注意力权重</span>
attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scaled_weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, seq_len)</span>
<span class="token comment"># 将注意力权重应用于 V 向量，计算加权和，得到加权信息</span>
attn_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> V<span class="token punctuation">)</span> <span class="token comment"># 形状 (batch_size, seq_len, feature_dim)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" 加权信息 :"</span><span class="token punctuation">,</span> attn_outputs<span class="token punctuation">)</span>
</code></pre>
    <pre><code class="prism language-markdown"> 加权信息 : tensor([[[ 0.5676, -0.0132, -0.8214, -0.0548],

         [ 0.5352, -0.1170, -0.5392, -0.0256],

         [ 0.6141, -0.1343, -0.5587, -0.0331]],



        [[ 0.5973, -0.2426, -0.3217, -0.0335],

         [ 0.5996, -0.1914, -0.2840,  0.0152],

         [ 0.6117, -0.2507, -0.3363, -0.0404]]], grad_fn=&lt;BmmBackward0&gt;)
</code></pre>
    <h3>
     <a id="1_60">
     </a>
     1.多头自注意力
    </h3>
    <p>
     多头⾃注意⼒（Multi-head Attention）机制是注意⼒机制的⼀种扩展，它可以帮助模型从不同的表示⼦空间捕获输⼊数据的多种特征。具体⽽⾔，多头⾃注意⼒在计算注意⼒权重和输出时，会对
     <strong>
      Q
     </strong>
     、
     <strong>
      K
     </strong>
     、
     <strong>
      V
     </strong>
     向量分别进⾏多次线性变换，从⽽获得不同的头（Head），并进⾏并⾏计算
    </p>
    <p>
     如下图所示：
    </p>
    <img alt="image-20250315222533884" src="https://i-blog.csdnimg.cn/img_convert/12a929408de1da6244e43ce361e7cfd3.png">
     <p>
      以下是多头⾃注意⼒的计算过程。
     </p>
     <p>
      （1）初始化：设定多头⾃注意⼒的头数。每个头将处理输⼊数据的⼀个⼦空间。
     </p>
     <p>
      （2）线性变换：对
      <strong>
       Q
      </strong>
      、
      <strong>
       K
      </strong>
      、
      <strong>
       V
      </strong>
      向量进⾏数次线性变换，每次变换使⽤不同的权重矩阵。这样，我们可以获得多组不同的
      <strong>
       Q
      </strong>
      、
      <strong>
       K
      </strong>
      、
      <strong>
       V
      </strong>
      向量。
     </p>
     <p>
      （3）缩放点积注意⼒：将每组
      <strong>
       Q
      </strong>
      、
      <strong>
       K
      </strong>
      、
      <strong>
       V
      </strong>
      向量输⼊缩放点积注意⼒中进⾏计算，每个头将⽣成⼀个加权输出。
     </p>
     <p>
      （4）合并：将所有头的加权输出拼接起来，并进⾏⼀次线性变换，得到多头⾃注意⼒的最终输出。
     </p>
     <ul>
      <li>
       假设我们有一个输入序列的表示矩阵 X（例如编码器的输出或者词嵌入），
      </li>
      <li>
       我们通过三个不同的线性层（也就是不同的权重矩阵）分别计算 Query、Key 和 Value：
       <ul>
        <li>
         <span class="katex--inline">
          <span class="katex">
           <span class="katex-mathml">
            q 
           
          
            = 
           
          
            X 
           
           
           
             W 
            
           
             q 
            
           
          
         
           q=XW_q
           </span>
           <span class="katex-html">
            <span class="base">
             <span class="strut" style="height: 0.625em; vertical-align: -0.1944em;">
             </span>
             <span class="mord mathnormal" style="margin-right: 0.0359em;">
              q
             </span>
             <span class="mspace" style="margin-right: 0.2778em;">
             </span>
             <span class="mrel">
              =
             </span>
             <span class="mspace" style="margin-right: 0.2778em;">
             </span>
            </span>
            <span class="base">
             <span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;">
             </span>
             <span class="mord mathnormal" style="margin-right: 0.0785em;">
              X
             </span>
             <span class="mord">
              <span class="mord mathnormal" style="margin-right: 0.1389em;">
               W
              </span>
              <span class="msupsub">
               <span class="vlist-t vlist-t2">
                <span class="vlist-r">
                 <span class="vlist" style="height: 0.1514em;">
                  <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                   <span class="pstrut" style="height: 2.7em;">
                   </span>
                   <span class="sizing reset-size6 size3 mtight">
                    <span class="mord mathnormal mtight" style="margin-right: 0.0359em;">
                     q
                    </span>
                   </span>
                  </span>
                 </span>
                 <span class="vlist-s">
                  ​
                 </span>
                </span>
                <span class="vlist-r">
                 <span class="vlist" style="height: 0.2861em;">
                  <span class="">
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </li>
        <li>
         <span class="katex--inline">
          <span class="katex">
           <span class="katex-mathml">
            k 
           
          
            = 
           
          
            X 
           
           
           
             W 
            
           
             k 
            
           
          
         
           k=XW_k
           </span>
           <span class="katex-html">
            <span class="base">
             <span class="strut" style="height: 0.6944em;">
             </span>
             <span class="mord mathnormal" style="margin-right: 0.0315em;">
              k
             </span>
             <span class="mspace" style="margin-right: 0.2778em;">
             </span>
             <span class="mrel">
              =
             </span>
             <span class="mspace" style="margin-right: 0.2778em;">
             </span>
            </span>
            <span class="base">
             <span class="strut" style="height: 0.8333em; vertical-align: -0.15em;">
             </span>
             <span class="mord mathnormal" style="margin-right: 0.0785em;">
              X
             </span>
             <span class="mord">
              <span class="mord mathnormal" style="margin-right: 0.1389em;">
               W
              </span>
              <span class="msupsub">
               <span class="vlist-t vlist-t2">
                <span class="vlist-r">
                 <span class="vlist" style="height: 0.3361em;">
                  <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                   <span class="pstrut" style="height: 2.7em;">
                   </span>
                   <span class="sizing reset-size6 size3 mtight">
                    <span class="mord mathnormal mtight" style="margin-right: 0.0315em;">
                     k
                    </span>
                   </span>
                  </span>
                 </span>
                 <span class="vlist-s">
                  ​
                 </span>
                </span>
                <span class="vlist-r">
                 <span class="vlist" style="height: 0.15em;">
                  <span class="">
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </li>
        <li>
         <span class="katex--inline">
          <span class="katex">
           <span class="katex-mathml">
            v 
           
          
            = 
           
          
            X 
           
           
           
             W 
            
           
             v 
            
           
          
         
           v= XW_v
           </span>
           <span class="katex-html">
            <span class="base">
             <span class="strut" style="height: 0.4306em;">
             </span>
             <span class="mord mathnormal" style="margin-right: 0.0359em;">
              v
             </span>
             <span class="mspace" style="margin-right: 0.2778em;">
             </span>
             <span class="mrel">
              =
             </span>
             <span class="mspace" style="margin-right: 0.2778em;">
             </span>
            </span>
            <span class="base">
             <span class="strut" style="height: 0.8333em; vertical-align: -0.15em;">
             </span>
             <span class="mord mathnormal" style="margin-right: 0.0785em;">
              X
             </span>
             <span class="mord">
              <span class="mord mathnormal" style="margin-right: 0.1389em;">
               W
              </span>
              <span class="msupsub">
               <span class="vlist-t vlist-t2">
                <span class="vlist-r">
                 <span class="vlist" style="height: 0.1514em;">
                  <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                   <span class="pstrut" style="height: 2.7em;">
                   </span>
                   <span class="sizing reset-size6 size3 mtight">
                    <span class="mord mathnormal mtight" style="margin-right: 0.0359em;">
                     v
                    </span>
                   </span>
                  </span>
                 </span>
                 <span class="vlist-s">
                  ​
                 </span>
                </span>
                <span class="vlist-r">
                 <span class="vlist" style="height: 0.15em;">
                  <span class="">
                  </span>
                 </span>
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </li>
       </ul>
      </li>
      <li>
       这里，
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          W 
          
         
           q 
          
         
        
       
         W_q
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;">
           </span>
           <span class="mord">
            <span class="mord mathnormal" style="margin-right: 0.1389em;">
             W
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.1514em;">
                <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mathnormal mtight" style="margin-right: 0.0359em;">
                   q
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.2861em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
       、
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          W 
          
         
           k 
          
         
        
       
         W_k
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 0.8333em; vertical-align: -0.15em;">
           </span>
           <span class="mord">
            <span class="mord mathnormal" style="margin-right: 0.1389em;">
             W
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3361em;">
                <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mathnormal mtight" style="margin-right: 0.0315em;">
                   k
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
       和
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          W 
          
         
           v 
          
         
        
       
         W_v
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 0.8333em; vertical-align: -0.15em;">
           </span>
           <span class="mord">
            <span class="mord mathnormal" style="margin-right: 0.1389em;">
             W
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.1514em;">
                <span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mathnormal mtight" style="margin-right: 0.0359em;">
                   v
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
          </span>
         </span>
        </span>
       </span>
       是模型在训练过程中学习到的参数矩阵。
      </li>
     </ul>
     <p>
      多头⾃注意⼒机制的优势在于，
      <mark>
       通过同时学习多个⼦空间的特征
      </mark>
      ，可以
      <mark>
       提⾼模型捕捉⻓距离依赖和不同语义层次的能⼒
      </mark>
      。
     </p>
     <p>
      如下列代码：
     </p>
     <pre><code class="prism language-py"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token comment"># 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, seq_len, feature_dim) </span>
<span class="token comment"># 定义头数和每个头的维度</span>
num_heads <span class="token operator">=</span> <span class="token number">2</span>
head_dim <span class="token operator">=</span> <span class="token number">2</span>
<span class="token comment"># feature_dim 必须是 num_heads * head_dim 的整数倍</span>
<span class="token keyword">assert</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> num_heads <span class="token operator">*</span> head_dim
<span class="token comment"># 定义线性层用于将 x 转换为 Q, K, V 向量</span>
linear_q <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
linear_k <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
linear_v <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token comment"># 通过线性层计算 Q, K, V</span>
Q <span class="token operator">=</span> linear_q<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, seq_len, feature_dim) </span>
K <span class="token operator">=</span> linear_k<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, seq_len, feature_dim) </span>
V <span class="token operator">=</span> linear_v<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, seq_len, feature_dim) </span>
<span class="token comment"># 将 Q, K, V 分割成 num_heads 个头</span>
<span class="token keyword">def</span> <span class="token function">split_heads</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> feature_dim <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    head_dim <span class="token operator">=</span> feature_dim <span class="token operator">//</span> num_heads
    output <span class="token operator">=</span> tensor<span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> head_dim<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span>  output <span class="token comment"># 形状 (batch_size, num_heads, seq_len, feature_dim)</span>
Q <span class="token operator">=</span> split_heads<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span>
K <span class="token operator">=</span> split_heads<span class="token punctuation">(</span>K<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span>
V <span class="token operator">=</span> split_heads<span class="token punctuation">(</span>V<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span>
<span class="token comment"># 计算 Q 和 K 的点积，作为相似度分数 , 也就是自注意力原始权重</span>
raw_weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, seq_len)</span>
<span class="token comment"># 对自注意力原始权重进行缩放</span>
scale_factor <span class="token operator">=</span> K<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">**</span> <span class="token number">0.5</span>
scaled_weights <span class="token operator">=</span> raw_weights <span class="token operator">/</span> scale_factor  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, seq_len)</span>
<span class="token comment"># 对缩放后的权重进行 softmax 归一化，得到注意力权重</span>
attn_weights <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scaled_weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, seq_len)</span>
<span class="token comment"># 将注意力权重应用于 V 向量，计算加权和，得到加权信息</span>
attn_outputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attn_weights<span class="token punctuation">,</span> V<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span>
<span class="token comment"># 将所有头的结果拼接起来</span>
<span class="token keyword">def</span> <span class="token function">combine_heads</span><span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> head_dim <span class="token operator">=</span> tensor<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
    feature_dim <span class="token operator">=</span> num_heads <span class="token operator">*</span> head_dim
    output <span class="token operator">=</span> tensor<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> seq_len<span class="token punctuation">,</span> feature_dim<span class="token punctuation">)</span>
    <span class="token keyword">return</span> output<span class="token comment"># 形状 : (batch_size, seq_len, feature_dim)</span>
attn_outputs <span class="token operator">=</span> combine_heads<span class="token punctuation">(</span>attn_outputs<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, seq_len, feature_dim) </span>
<span class="token comment"># 对拼接后的结果进行线性变换</span>
linear_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
attn_outputs <span class="token operator">=</span> linear_out<span class="token punctuation">(</span>attn_outputs<span class="token punctuation">)</span>  <span class="token comment"># 形状 (batch_size, seq_len, feature_dim) </span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">" 加权信息 :"</span><span class="token punctuation">,</span> attn_outputs<span class="token punctuation">)</span>
</code></pre>
     <p>
      多头自注意力机制就是将输⼊向量投影到多个向量空间，在每个向量空间中执⾏点积注意⼒计算，然后连接各头的结果。
     </p>
     <p>
      在实际应⽤中，多头⾃注意⼒通常作为更复杂模型（如Transformer）的⼀个组成部分。这些复杂的模型通常包含其他组件，例如前馈神经⽹络（Feed-Forward Neural Network）和层归⼀化（Layer Normalization），以提⾼模型的表达能⼒和稳定性。
     </p>
     <p>
     </p>
     <h3>
      <a id="2_144">
      </a>
      2.注意力掩码
     </h3>
     <p>
      注意⼒中的掩码机制，不同于BERT训练过程中的那种对训练⽂本的“掩码”。注意⼒掩码的作⽤是避免模型在计算注意⼒分数时，将不相关的单词考虑进来。掩码操作可以防⽌模型学习到不必要的信息。
     </p>
     <p>
      要直观地解释掩码，我们先回忆⼀下填充（Padding）的概念。在NLP任务中，我们经常需要将不同⻓度的⽂本输⼊模型。为了能够批量处理这些⽂本，我们需要将它们填充⾄相同的⻓度。
     </p>
     <p>
      以这段有关损失函数的代码为例。
     </p>
     <pre><code class="prism language-py">criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span>word2idx_en<span class="token punctuation">[</span><span class="token string">''</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 损失函数</span>
</code></pre>
     <p>
      这段代码中的ignore_index=word2idx_en[‘’]，就是为了告诉模型，是附加的冗余信息，模型在反向传播更新参数的时候没有必要关注它，因此也没有什么单词会被翻译成。
     </p>
     <p>
     </p>
     <p>
      填充掩码（Padding Mask）的作⽤和上⾯损失函数中的ignore_index参数有点类似，都是避免在计算注意⼒分数时，将填充位置的单词考虑进来（⻅右图）。因为填充位置的单词对于实际任务来说是⽆意义的，⽽且可能会引⼊噪声，影响模型的性能。
     </p>
     <p>
      <img alt="image-20250315223900477" src="https://i-blog.csdnimg.cn/img_convert/5d87ec4b56cbb0b7dc365e538a2ea4a5.png"/>
     </p>
     <p>
      加⼊了掩码机制之后的注意⼒如下图所示，我们会把
      <mark>
       将注意⼒权重矩阵与⼀个注意⼒掩码矩阵相加
      </mark>
      ，使得不需要的信息所对应的权重变得⾮常⼩（接近负⽆穷）。然后，通过应⽤softmax函数，将不需要的信息对应的权重变得接近于0，从⽽实现忽略它们的⽬的。
     </p>
     <p>
      <img alt="image-20250315223935095" src="https://i-blog.csdnimg.cn/img_convert/0d45f7e384c5c2be2239e883ac276288.png"/>
     </p>
     <p>
      在Transformer中，使⽤了⾃注意⼒机制、多头⾃注意⼒机制和掩码，不仅有前⾯介绍的填充掩码，还有⼀种解码器专⽤的
      <mark>
       后续注意⼒掩码
      </mark>
      （Subsequent Attention Mask），简称后续掩码，也叫前瞻掩码（Look-ahead Masking），这是为了在训练时为解码器遮蔽未来的信息。
     </p>
    </img>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f753031313134363230332f:61727469636c652f64657461696c732f313436323837303431" class_="artid" style="display:none">
 </p>
</div>


