---
layout: post
title: "å¤§æ¨¡å‹å¾®è°ƒ-Prompt-Tuningä¸P-Tuning-çš„åŒºåˆ«"
date: 2025-08-25T10:40:30+0800
description: "Prompt Tuningä¸P-Tuningæ–¹æ³•å¯¹æ¯” Prompt Tuningå’ŒP-Tuningæ˜¯ä¸¤ç§å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡å°è§„æ¨¡å¯è®­ç»ƒæç¤ºå‘é‡é€‚é…ä¸‹æ¸¸ä»»åŠ¡ã€‚Prompt Tuningï¼ˆGoogleæå‡ºï¼‰ä»…åœ¨è¾“å…¥å±‚æ·»åŠ å¯å­¦ä¹ çš„è™šæ‹ŸtokenåµŒå…¥ï¼Œè®­ç»ƒå‚æ•°é‡å°‘ä½†æ€§èƒ½ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹èƒ½åŠ›ã€‚P-Tuningï¼ˆæ¸…åå›¢é˜Ÿï¼‰æœ€åˆé‡‡ç”¨è¿ç»­åµŒå…¥æ›¿ä»£ç¦»æ•£æç¤ºï¼ˆv1ï¼‰ï¼Œæ”¹è¿›ç‰ˆP-Tuning v2åœ¨Transformerå„å±‚æ’å…¥æç¤ºå‘é‡ï¼Œæ˜¾è‘—æå‡è¡¨è¾¾èƒ½åŠ›ï¼Œæ¥è¿‘å…¨å‚æ•°å¾®è°ƒæ•ˆæœã€‚ æ ¸å¿ƒåŒºåˆ«ï¼š å‚æ•°ä½ç½®ï¼šPrompt Tuningä»…"
keywords: "å¤§æ¨¡å‹å¾®è°ƒ Prompt Tuningä¸P-Tuning çš„åŒºåˆ«?"
categories: ['æœªåˆ†ç±»']
tags: ['Prompt']
artid: "150761880"
arturl: "https://blog.csdn.net/xuejianxinokok/article/details/150761880"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150761880
    alt: "å¤§æ¨¡å‹å¾®è°ƒ-Prompt-Tuningä¸P-Tuning-çš„åŒºåˆ«"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150761880
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150761880
cover: https://bing.ee123.net/img/rand?artid=150761880
image: https://bing.ee123.net/img/rand?artid=150761880
img: https://bing.ee123.net/img/rand?artid=150761880
---



# å¤§æ¨¡å‹å¾®è°ƒ Prompt Tuningä¸P-Tuning çš„åŒºåˆ«?



**Prompt Tuning** å’Œ **P-Tuning** éƒ½å±äº **å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ˆPEFT, Parameter-Efficient Fine-Tuningï¼‰**ï¼Œä¸»è¦æ˜¯ä¸ºäº†é¿å…å¯¹å¤§æ¨¡å‹å…¨éƒ¨å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œè€Œæ˜¯é€šè¿‡å°è§„æ¨¡å‚æ•°ï¼ˆprompt embeddingï¼‰æ¥é€‚é…ä¸‹æ¸¸ä»»åŠ¡ã€‚ä½†ä¸¤è€…çš„å®ç°æ–¹å¼å’Œåº”ç”¨åœºæ™¯æœ‰ä¸€äº›åŒºåˆ«ï¼š

æç¤ºå¾®è°ƒ ä»…åœ¨è¾“å…¥åµŒå…¥å±‚ä¸­åŠ å…¥å¯è®­ç»ƒçš„æç¤ºå‘é‡ã€‚åœ¨ç¦»æ•£æç¤ºæ–¹æ³•çš„åŸºç¡€ä¸Šï¼Œæç¤ºå¾®è°ƒé¦–å…ˆåœ¨**è¾“å…¥ç«¯**æ’å…¥ä¸€ç»„è¿ç»­åµŒå…¥æ•°å€¼çš„æç¤ºè¯å…ƒï¼Œè¿™äº›æç¤ºè¯å…ƒå¯ä»¥ä»¥è‡ªç”±å½¢å¼ æˆ–å‰  
ç¼€å½¢å¼ æ¥å¢å¼ºè¾“å…¥æ–‡æœ¬ï¼Œç”¨äºè§£å†³ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡ã€‚åœ¨å…·ä½“å®ç°ä¸­ï¼Œåªéœ€è¦  
å°†å¯å­¦ä¹ çš„ç‰¹å®šä»»åŠ¡æç¤ºå‘é‡ä¸è¾“å…¥æ–‡æœ¬å‘é‡ç»“åˆèµ·æ¥ä¸€èµ·è¾“å…¥åˆ°è¯­è¨€æ¨¡å‹ä¸­ã€‚  
P-tuning æå‡ºäº†ä½¿ç”¨è‡ªç”±å½¢å¼æ¥ç»„åˆè¾“å…¥æ–‡æœ¬å’Œæç¤ºå‘é‡ï¼Œé€šè¿‡åŒå‘ LSTM  
æ¥å­¦ä¹ è½¯æç¤ºè¯å…ƒçš„è¡¨ç¤ºï¼Œå®ƒå¯ä»¥åŒæ—¶é€‚ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆä»»åŠ¡ã€‚å¦ä¸€ç§  
ä»£è¡¨æ€§æ–¹æ³•ç§°ä¸º Prompt Tuning ï¼Œå®ƒä»¥å‰ç¼€å½¢å¼æ·»åŠ æç¤ºï¼Œ**ç›´æ¥åœ¨è¾“å…¥å‰æ‹¼  
æ¥è¿ç»­å‹å‘é‡**ã€‚åœ¨æç¤ºå¾®è°ƒçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰æç¤ºçš„åµŒå…¥å‘é‡ä¼šæ ¹æ®ç‰¹å®šä»»åŠ¡  
è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œç„¶è€Œç”±äºåªåœ¨è¾“å…¥å±‚ä¸­åŒ…å«äº†æå°‘é‡çš„å¯è®­ç»ƒå‚æ•°ï¼Œæœ‰ç ”ç©¶å·¥ä½œ  
è¡¨æ˜è¯¥æ–¹æ³•çš„æ€§èƒ½é«˜åº¦ä¾èµ–åº•å±‚è¯­è¨€æ¨¡å‹çš„èƒ½åŠ› ã€‚ä¸‹å›¾ å±•ç¤ºäº†æç¤ºå¾®è°ƒç®—  
æ³•çš„ç¤ºæ„å›¾ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/975184bec8bb479ca83b7f8aca169bfc.png)

---

### 1. **Prompt Tuning**

* **æå‡ºè€…**ï¼šLester et al., 2021ï¼ˆGoogleï¼‰
* **æ ¸å¿ƒæ€æƒ³**ï¼š

  + åœ¨è¾“å…¥åºåˆ—å‰é¢åŠ ä¸Š **å¯å­¦ä¹ çš„â€œè™šæ‹Ÿtoken embeddingâ€**ï¼ˆprompt embeddingï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥è°ƒæ¨¡å‹çš„åŸå§‹å‚æ•°ã€‚
  + è¿™äº› prompt embeddings åœ¨è®­ç»ƒæ—¶ä¼šè¢«æ›´æ–°ï¼Œè€Œæ¨¡å‹çš„å…¶ä»–å‚æ•°ä¿æŒå†»ç»“ã€‚
* **åº”ç”¨æ–¹å¼**ï¼š

  + å¸¸ç”¨äº **encoder-decoderæ¨¡å‹ï¼ˆå¦‚T5ï¼‰** æˆ– decoder-onlyï¼ˆå¦‚GPTï¼‰ä»»åŠ¡ã€‚
  + è¾“å…¥ `[Prompt Embeddings] + [ä¸‹æ¸¸ä»»åŠ¡è¾“å…¥]` â†’ æ¨¡å‹è¾“å‡ºã€‚
* **ç‰¹ç‚¹**ï¼š

  + è®­ç»ƒå‚æ•°é‡æå°ï¼ˆåªè®­ç»ƒ prompt embeddingï¼‰ã€‚
  + Prompt æ˜¯ç›´æ¥åŠ åœ¨ embedding å±‚ï¼Œå’Œ token embedding ç»´åº¦ç›¸åŒã€‚
  + æ›´åå‘ **NLPç”Ÿæˆ/åˆ†ç±»ä»»åŠ¡**ã€‚

---

### 2. **P-Tuning**

* **æå‡ºè€…**ï¼šLiu et al., 2021ï¼ˆæ¸…åï¼‰
* **æ ¸å¿ƒæ€æƒ³**ï¼š

  + æœ€åˆçš„ P-Tuning v1ï¼šç”¨ **è¿ç»­å¯å­¦ä¹  embedding** ä»£æ›¿ç¦»æ•£ promptã€‚
  + **P-Tuning v2**ï¼ˆæ”¹è¿›ç‰ˆï¼ŒACL 2022ï¼‰ï¼šé€šè¿‡ **æ·±å±‚æ’å…¥è™šæ‹Ÿ prompt embedding åˆ° Transformer çš„å¤šå±‚ä¸­ï¼ˆä¸æ˜¯åªåœ¨è¾“å…¥å±‚ï¼‰**ã€‚
* **åº”ç”¨æ–¹å¼**ï¼š

  + é€‚ç”¨äº **åˆ†ç±»ã€ç”Ÿæˆã€ä¿¡æ¯æŠ½å–** ç­‰å¤šç§ä»»åŠ¡ã€‚
  + v2 æ›´é€‚åˆ **å°æ•°æ®é›†åœºæ™¯**ï¼Œå› ä¸ºå®ƒçš„è¡¨ç¤ºèƒ½åŠ›æ¯”å•å±‚ prompt tuning æ›´å¼ºã€‚
* **ç‰¹ç‚¹**ï¼š

  + P-Tuning v1 å’Œ Prompt Tuning ç±»ä¼¼ï¼Œéƒ½æ˜¯åŠ è¿ç»­ embeddingã€‚
  + P-Tuning v2 æ¯” Prompt Tuning æ›´å¼ºï¼Œå› ä¸ºä¸ä»…åœ¨è¾“å…¥å±‚ï¼Œè€Œä¸”åœ¨ Transformer å„å±‚éƒ½æ’å…¥å¯å­¦ä¹ å‚æ•°ã€‚
  + è¡¨ç°æ›´æ¥è¿‘å…¨å‚æ•°å¾®è°ƒï¼Œä½†ä»ä¿æŒå‚æ•°é«˜æ•ˆã€‚

---

### 3. **åŒºåˆ«æ€»ç»“**

| å¯¹æ¯”ç‚¹ | Prompt Tuning | P-Tuning (v1) | P-Tuning v2 |
| --- | --- | --- | --- |
| å‚æ•°ä½ç½® | è¾“å…¥å±‚å‰åŠ è™šæ‹Ÿ embedding | è¾“å…¥å±‚å‰åŠ è¿ç»­ embedding | å„å±‚ Transformer æ’å…¥è™šæ‹Ÿ prompt |
| è®­ç»ƒå‚æ•°é‡ | æå°‘ | æå°‘ | è¾ƒå°‘ï¼ˆä½†æ¯” Prompt Tuning å¤šï¼‰ |
| è¡¨è¾¾èƒ½åŠ› | ç›¸å¯¹è¾ƒå¼± | ç±»ä¼¼ Prompt Tuning | æ›´å¼ºï¼Œæ¥è¿‘å…¨é‡å¾®è°ƒ |
| é€‚ç”¨ä»»åŠ¡ | NLPä¸‹æ¸¸ä»»åŠ¡ï¼ˆåˆ†ç±»ã€ç”Ÿæˆï¼‰ | NLPä»»åŠ¡ | å°æ•°æ®/å¤æ‚ä»»åŠ¡ï¼Œæ³›åŒ–æ›´å¥½ |
| æå‡ºæ–¹ | Google (Lester et al., 2021) | æ¸…å (Liu et al., 2021) | æ¸…å (v2, ACL 2022) |

---

**ä¸€å¥è¯æ€»ç»“**ï¼š

* **Prompt Tuning**ï¼šåªåœ¨è¾“å…¥ embedding å±‚åŠ å¯å­¦ä¹  prompt â†’ è½»é‡ä½†è¡¨è¾¾èƒ½åŠ›æœ‰é™ã€‚
* **P-Tuning**ï¼šä¸ä»…èƒ½åœ¨è¾“å…¥å±‚åŠ  embeddingï¼Œè¿˜èƒ½åœ¨ Transformer æ·±å±‚æ’å…¥è™šæ‹Ÿ promptï¼ˆå°¤å…¶ v2ï¼‰ â†’ è¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼Œæ•ˆæœæ¥è¿‘å…¨å‚æ•°å¾®è°ƒã€‚

---

ä¸¾ä¾‹è¯´æ˜ã€‚

---

### 1. **æ™®é€š Embeddingï¼ˆç¦»æ•£ token embeddingï¼‰**

åœ¨ NLP é‡Œï¼Œè¾“å…¥é€šå¸¸æ˜¯ç¦»æ•£çš„ tokenï¼ˆå¦‚ â€œappleâ€ã€â€œæˆ‘â€ã€â€œä¸­å›½â€ï¼‰ã€‚  
è¿™äº› token ä¼šå…ˆé€šè¿‡ **è¯è¡¨æŸ¥æ‰¾** å˜æˆå‘é‡ï¼š

ä¾‹å¦‚è¯è¡¨å¤§å° = 10000ï¼Œembedding ç»´åº¦ = 768ï¼š

```
"apple"   â†’ [0.12, -0.34, 0.98, ... , 0.45]  (768ç»´å‘é‡)
"ä¸­å›½"     â†’ [0.87, 0.22, -0.54, ... , -0.11]

```

è¿™äº› embedding æ˜¯æ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶å­¦å¥½çš„ã€‚

---

### 2. **è™šæ‹Ÿ embeddingï¼ˆvirtual tokens / prompt embeddingï¼‰**

* **æ„æ€**ï¼šäººä¸ºåŠ ä¸€äº› **ä¸å­˜åœ¨äºè¯è¡¨ä¸­çš„â€œå‡tokenâ€**ï¼Œä½†æ˜¯å®ƒä»¬æœ‰ embedding å‘é‡ã€‚
* è¿™äº›å‘é‡æ˜¯ **éšæœºåˆå§‹åŒ–** çš„ï¼Œç„¶åé€šè¿‡è®­ç»ƒå­¦ä¹ ï¼Œè€Œä¸æ˜¯å›ºå®šçš„è¯è¡¨ lookupã€‚
* å®ƒä»¬æœ¬èº«æ²¡æœ‰å¯¹åº”çš„æ–‡å­—ï¼Œåªæ˜¯æ¨¡å‹å‰é¢é™„åŠ çš„â€œæç¤ºä¿¡å·â€ã€‚

#### ä¸¾ä¾‹ï¼š

å‡è®¾æˆ‘ä»¬è¦åš **æƒ…æ„Ÿåˆ†ç±»**ï¼ˆå¥å­ â†’ ç§¯æ/æ¶ˆæï¼‰ï¼Œè¾“å…¥æ˜¯ï¼š

```
"è¿™éƒ¨ç”µå½±å¾ˆç²¾å½©"

```

ç”¨ Prompt Tuning æ—¶ï¼Œå¯ä»¥åŠ  5 ä¸ªè™šæ‹Ÿ tokenï¼š

```
[v1][v2][v3][v4][v5] è¿™éƒ¨ç”µå½±å¾ˆç²¾å½©

```

å…¶ä¸­ `[v1]...[v5]` å°±æ˜¯ **è™šæ‹Ÿ embedding**ï¼š

```
[v1] â†’ [0.01, 0.77, -0.32, ...]
[v2] â†’ [0.55, -0.88, 0.14, ...]
...

```

è¿™äº› embedding ä¸å±äºè¯è¡¨ï¼Œä½†ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¼š **å¦‚ä½•å¼•å¯¼æ¨¡å‹è¾“å‡ºâ€œç§¯æ/æ¶ˆæâ€**ã€‚

---

### 3. **è¿ç»­ embeddingï¼ˆcontinuous promptï¼‰**

* **æ„æ€**ï¼šPrompt ä¸å†ç”¨è‡ªç„¶è¯­è¨€ï¼ˆâ€œPlease classify the sentimentâ€¦â€ï¼‰ï¼Œè€Œæ˜¯ç›´æ¥ç”¨ **è¿ç»­å‘é‡**ã€‚
* è¿™ä¸ªæ¦‚å¿µæœ€æ—©æ˜¯ P-Tuning v1 æçš„ï¼Œæœ¬è´¨å’Œè™šæ‹Ÿ embedding å¾ˆåƒï¼Œä½†å¼ºè°ƒå®ƒæ˜¯ **è¿ç»­ç©ºé—´é‡Œçš„å¯å­¦ä¹ å‘é‡**ï¼Œè€Œä¸æ˜¯ç¦»æ•£ tokenï¼ˆä¸å¯å†æ˜ å°„å›â€œæ–‡å­—â€ï¼‰ã€‚

#### ä¸¾ä¾‹ï¼š

ç¦»æ•£ promptï¼ˆäººå†™çš„æ–‡å­—ï¼‰å¯èƒ½æ˜¯ï¼š

```
"è¿™éƒ¨ç”µå½±å¾ˆç²¾å½© [MASK]"

```

è¿ç»­ promptï¼ˆP-Tuningï¼‰åˆ™æ˜¯ï¼š

```
[0.12, -0.33, 0.98, ...]  (embedding1)
[0.54,  0.11, -0.66, ...] (embedding2)
[0.22, -0.77, 0.44, ...]  (embedding3)
è¿™éƒ¨ç”µå½±å¾ˆç²¾å½©

```

åŒºåˆ«åœ¨äºï¼š

* **ç¦»æ•£ prompt** = ç”¨çœŸå® tokenï¼ˆå¦‚ â€œPleaseâ€, â€œanswerâ€ï¼‰æ‹¼å‡ºæ¥ã€‚
* **è¿ç»­/è™šæ‹Ÿ prompt** = ç›´æ¥ç”¨å¯è®­ç»ƒçš„å‘é‡ï¼Œä¸éœ€è¦æ˜ å°„å›æ–‡å­—ã€‚

---

### 4. **æ€»ç»“**

* **è™šæ‹Ÿ embedding** = ç»™æ¨¡å‹è¾“å…¥å‰é¢åŠ â€œå‡ tokenâ€ï¼Œå®ƒä»¬çš„ embedding éšè®­ç»ƒè°ƒæ•´ã€‚
* **è¿ç»­ embedding** = ç›´æ¥è®­ç»ƒè¿ç»­çš„ embedding å‘é‡ï¼Œä¸ä¸€å®šå¯¹åº”è¯è¡¨é‡Œçš„ä»»ä½• tokenã€‚
* æœ¬è´¨ä¸Šä¸¤è€…å·®åˆ«ä¸å¤§ï¼Œå¾ˆå¤šæ—¶å€™æ˜¯ä¸åŒè®ºæ–‡é‡Œå¯¹ç±»ä¼¼æ¦‚å¿µçš„å«æ³•ï¼ŒåŒºåˆ«ä¸»è¦åœ¨ï¼š

  + Prompt Tuning å¼ºè°ƒ **è™šæ‹Ÿ token embedding**
  + P-Tuning å¼ºè°ƒ **è¿ç»­å¯å­¦ä¹  embedding**ï¼ˆä¸ä¾èµ–ç¦»æ•£ tokenï¼‰

---

ä¸‹è¾¹æ˜¯ä¸€ä¸ª **PyTorch ç¤ºä¾‹**ï¼Œæ¼”ç¤ºå¦‚ä½•åœ¨è¾“å…¥åºåˆ—å‰é¢åŠ ä¸Š **è™šæ‹Ÿ/è¿ç»­ embedding**ã€‚

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç®€åŒ–çš„æ¨¡å‹ï¼ˆç±»ä¼¼ BERTï¼‰ï¼Œè¾“å…¥æ˜¯ token embeddingï¼Œæˆ‘ä»¬æƒ³åœ¨è¾“å…¥å‰é¢åŠ å‡ ä¸ªå¯å­¦ä¹ çš„ **prompt embedding**ã€‚

---

### ğŸ”¹ PyTorch ç¤ºä¾‹ä»£ç 

```python
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self, vocab_size=10000, embed_dim=16, prompt_len=5):
        super().__init__()
        # æ™®é€š embedding (è¯è¡¨)
        self.embedding = nn.Embedding(vocab_size, embed_dim)

        # prompt embedding (è™šæ‹Ÿ/è¿ç»­å‘é‡ï¼Œä¸å±äºè¯è¡¨)
        self.prompt_embedding = nn.Parameter(torch.randn(prompt_len, embed_dim))

        # ä¸€ä¸ªç®€å•çš„åˆ†ç±»å¤´
        self.fc = nn.Linear(embed_dim, 2)  # å‡è®¾2åˆ†ç±»ä»»åŠ¡

    def forward(self, input_ids):
        """
        input_ids: [batch_size, seq_len]  (æ™®é€šè¾“å…¥tokençš„id)
        """
        batch_size = input_ids.size(0)

        # 1. æŠŠ token id è½¬æ¢æˆ embedding
        token_embeds = self.embedding(input_ids)  # [batch, seq_len, embed_dim]

        # 2. prompt embedding (å¤åˆ¶åˆ° batch ç»´åº¦)
        prompt_embeds = self.prompt_embedding.unsqueeze(0).expand(batch_size, -1, -1)  # [batch, prompt_len, embed_dim]

        # 3. æ‹¼æ¥ prompt å’ŒåŸå§‹è¾“å…¥
        full_embeds = torch.cat([prompt_embeds, token_embeds], dim=1)  # [batch, prompt_len+seq_len, embed_dim]

        # å‡è®¾æˆ‘ä»¬åªå–æœ€åä¸€ä¸ª token ä½ç½®åšåˆ†ç±»
        last_hidden = full_embeds[:, -1, :]  # [batch, embed_dim]

        # åˆ†ç±»
        logits = self.fc(last_hidden)
        return logits


# ======================
# ğŸ”¹ æµ‹è¯•
# ======================
batch_size = 2
seq_len = 4
vocab_size = 10000
embed_dim = 16
prompt_len = 5

model = SimpleModel(vocab_size, embed_dim, prompt_len)

# æ¨¡æ‹Ÿä¸¤ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªé•¿åº¦ä¸º4
input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

print("è¾“å…¥ token ids:\n", input_ids)

logits = model(input_ids)
print("è¾“å‡º logits:\n", logits)

```

---

### ğŸ”¹ è¿è¡Œé€»è¾‘è¯´æ˜

1. **æ™®é€š embedding**ï¼šæŠŠè¾“å…¥ token id è½¬æˆå‘é‡ã€‚

   * ä¾‹å¦‚ `[12, 87, 325, 99] â†’ 4ä¸ª embedding`
2. **prompt embedding**ï¼šè®­ç»ƒæ—¶é¢å¤–å¼•å…¥çš„å‘é‡ï¼Œä¾‹å¦‚ `5 ä¸ªè™šæ‹Ÿ token`ã€‚

   * ä¾‹å¦‚ `[p1, p2, p3, p4, p5] â†’ 5ä¸ª embedding`
3. **æ‹¼æ¥è¾“å…¥**ï¼š

   ```
   [p1, p2, p3, p4, p5, token1, token2, token3, token4]

   ```
4. **æ¨¡å‹åªæ›´æ–° prompt embedding**ï¼Œè€ŒåŸå§‹æ¨¡å‹å‚æ•°å¯ä»¥å†»ç»“ï¼ˆåªè®© `self.prompt_embedding` å­¦ä¹ ï¼‰ã€‚

---



