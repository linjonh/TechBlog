---
layout: post
title: "机器学习-时序预测1"
date: 2025-09-01T16:57:16+0800
description: "摘要：文章探讨了运筹优化与机器学习结合的发展趋势，重点分析了时序预测在调度优化中的重要性。作者比较了多种时序预测方法，包括MLP、DNN架构（GNN、LSTM等）和传统模型，详细解析了RNN的工作原理及其梯度问题，并重点介绍了LSTM通过细胞状态和门控机制解决长期依赖问题的原理，最后讨论了不同任务下激活函数的选择策略。"
keywords: "机器学习-时序预测1"
categories: ['未分类']
tags: ['机器学习', '人工智能']
artid: "151074806"
arturl: "https://blog.csdn.net/weixin_47158797/article/details/151074806"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151074806
    alt: "机器学习-时序预测1"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151074806
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151074806
cover: https://bing.ee123.net/img/rand?artid=151074806
image: https://bing.ee123.net/img/rand?artid=151074806
img: https://bing.ee123.net/img/rand?artid=151074806
---



# 机器学习-时序预测1

最近面试过程中，Predict-then-Optimize是运筹优化算法工程师未来的发展方向。就像我之前写过的[运筹优化（OR）-在机器学习（ML）浪潮中何去何从？-CSDN博客](https://blog.csdn.net/weixin_47158797/article/details/150858903?spm=1001.2014.3001.5501 "运筹优化（OR）-在机器学习（ML）浪潮中何去何从？-CSDN博客")，机器学习适合预测、运筹优化适合决策。我研究的基本就是调度优化方面，因此对时序需求的预测显得十分重要。

而我之前只是使用多层感知机（MLP）做过一些回归预测，它本身不理解时序序列的顺序性。DNN其实是时序预测的好工具，其中GNN、LSTM、GRU、Transformer都是非常好的DNN特定架构。MLP也属于DNN，只是他不能理解时间顺序。

![](https://i-blog.csdnimg.cn/direct/7b4fcb23835e437d9ce926fe88ded6a4.png)

此外，时序预测还可以使用梯度提升树、Prophet以及传统的SARIMA。今天就先梳理一下循环神经网络（GNN）的使用，以及他的变体LSTM。

## 循环神经网络GNN

与标准神经网络不同，RNN引入了一个隐藏状态![h_t](https://latex.csdn.net/eq?h_t)

GNN的“循环”也体现在：

**结构上：**同一个神经网络单元在时间序列上反复调用自己，并通过隐藏状态形成一个从过去指向未来的反馈连接；

**数学上：**隐藏状态的计算是一个递归公式（![h_t = f(h_{t-1}, x_t)](https://latex.csdn.net/eq?h_t%20%3D%20f%28h_%7Bt-1%7D%2C%20x_t%29)），当前状态不断“循环”地依赖于过去的状态。

然而，RNN有一个致命的问题，就是会产生梯度爆炸/消失的问题，这就需要LSTM解决。

## 长短期记忆网络LSTM

引入了两个核心组件：**细胞状态**和**门控机制（Gates）**。

细胞状态![C_{t}](https://latex.csdn.net/eq?C_%7Bt%7D)： LSTM 的“长期记忆高速公路”；

遗忘门![f_t](https://latex.csdn.net/eq?f_t)：![](https://i-blog.csdnimg.cn/direct/305b4bd0a13740479e30bcc071d8fbac.png)，用sigmoid激活函数输出0-1的向量，![f_t](https://latex.csdn.net/eq?f_t)中每个元素表示![C_{t-1}](https://latex.csdn.net/eq?C_%7Bt-1%7D)的每个分量保留多少；

输入门![i_{t}](https://latex.csdn.net/eq?i_%7Bt%7D)：![](https://i-blog.csdnimg.cn/direct/a5955c463dd44753baa6441c0832aa99.png)，用sigmoid激活函数输出0-1的向量，![i_{t}](https://latex.csdn.net/eq?i_%7Bt%7D)中每个元素表示哪些新信息重要；

输出门![o_{t}](https://latex.csdn.net/eq?o_%7Bt%7D)：![](https://i-blog.csdnimg.cn/direct/c214951f40664ef4aa2cf1964f5bb936.png)，用sigmoid激活函数输出0-1的向量，![o_{t}](https://latex.csdn.net/eq?o_%7Bt%7D)中每个元素表示哪些信息作为![h_t](https://latex.csdn.net/eq?h_t)；

候选细胞状态![\tilde{C}_{t}](https://latex.csdn.net/eq?%5Ctilde%7BC%7D_%7Bt%7D)：![](https://i-blog.csdnimg.cn/direct/612bf6e96d224f8d8ff572fe15c376bc.png)

更新细胞状态：![](https://i-blog.csdnimg.cn/direct/25d50637c3814f59ab88a691fba8349e.png)，前一项去除不需要的旧信息，后一项添加有用的新信息，用到了遗忘门和输入门；

![](https://i-blog.csdnimg.cn/direct/a52f425b3b86428fa85ccfaf2b131166.png)，生成最终的隐藏状态，用到了输出门。

上述是LSTM单元的更新，最后进行输出层的更新。

![](https://i-blog.csdnimg.cn/direct/27a1938666cb46679d53dbb73f5cfcdd.png)，其中，对于激活函数![g(\cdot)](https://latex.csdn.net/eq?g%28%5Ccdot%29)的选择，序列生成任务可以用softmax、分类任务可以用sigmoid/softmax、时间序列预测等回归任务则通常是线性函数。

上述就是LSTM对GNN的改进，GRU则是LSTM的简化而高效的变体[机器学习-时序预测2-CSDN博客](https://blog.csdn.net/weixin_47158797/article/details/151076054?sharetype=blogdetail&sharerId=151076054&sharerefer=PC&sharesource=weixin_47158797&spm=1011.2480.3001.8118 "机器学习-时序预测2-CSDN博客")。



