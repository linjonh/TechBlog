---
layout: post
title: "LLM自动化评测"
date: 2025-03-14 17:46:07 +0800
description: "【代码】LLM自动化评测。"
keywords: "LLM自动化评测"
categories: ['未分类']
tags: ['Python']
artid: "146263023"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146263023
    alt: "LLM自动化评测"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146263023
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146263023
cover: https://bing.ee123.net/img/rand?artid=146263023
image: https://bing.ee123.net/img/rand?artid=146263023
img: https://bing.ee123.net/img/rand?artid=146263023
---

# LLM自动化评测
使用的数据集：[ceval-exam](https://huggingface.co/datasets/ceval/ceval-exam)
import requests
from datasets import load\_dataset, concatenate\_datasets
import re
from tqdm import tqdm
import re, time, tiktoken, ollama
from ollama import ChatResponse
from ollama import Options
def llm(model, query, temperature=0.6, stream=False, encoding=tiktoken.encoding\_for\_model("gpt-4"), max\_tokens=None):
# return "A"
options = Options(
temperature=temperature,
num\_gpu=0, # num\_gpu=0即使用CPU计算
# num\_thread=32,
# num\_ctx=4096, # 上下文窗口大小
)
# 流式输出
response = ollama.chat(
model=model,
messages=[
{
"role": "system",
"content": "你是一个做题专家。请完成下列单项选择题。\n\n## output format\n只能输出一个选项编号字母，不要有解析等其他任何内容。",
},
{
"role": "user",
"content": query,
},
],
options=options,
stream=stream,
keep\_alive=0
)
if stream:
chunks = ""
# 逐块打印响应内容
for chunk in response:
chunks += chunk["message"]["content"]
# print(chunk["message"]["content"], end="", flush=True)
if max\_tokens != None and len(encoding.encode(chunks)) > max\_tokens:
break
response = chunks
else:
# print(response["message"]["content"])
response = response["message"]["content"]
# stream=True时无效
# with open("tmp.txt", "a", encoding="utf-8") as f:
# f.write(response + "\n"+ 100\*'\*' + '\n')
if '' in response and '' in response:
response = re.sub(r'.\*?', '', response, flags=re.DOTALL)
return response.strip()
task\_list = [
"computer\_network",
"operating\_system",
"computer\_architecture",
"college\_programming",
"college\_physics",
"college\_chemistry",
"advanced\_mathematics",
"probability\_and\_statistics",
"discrete\_mathematics",
"electrical\_engineer",
"metrology\_engineer",
"high\_school\_mathematics",
"high\_school\_physics",
"high\_school\_chemistry",
"high\_school\_biology",
"middle\_school\_mathematics",
"middle\_school\_biology",
"middle\_school\_physics",
"middle\_school\_chemistry",
"veterinary\_medicine",
"college\_economics",
"business\_administration",
"marxism",
"mao\_zedong\_thought",
"education\_science",
"teacher\_qualification",
"high\_school\_politics",
"high\_school\_geography",
"middle\_school\_politics",
"middle\_school\_geography",
"modern\_chinese\_history",
"ideological\_and\_moral\_cultivation",
"logic",
"law",
"chinese\_language\_and\_literature",
"art\_studies",
"professional\_tour\_guide",
"legal\_professional",
"high\_school\_chinese",
"high\_school\_history",
"middle\_school\_history",
"civil\_servant",
"sports\_science",
"plant\_protection",
"basic\_medicine",
"clinical\_medicine",
"urban\_and\_rural\_planner",
"accountant",
"fire\_engineer",
"environmental\_impact\_assessment\_engineer",
"tax\_accountant",
"physician",
]
task\_chinese\_name\_list = [
"计算机网络",
"操作系统",
"计算机架构",
"大学编程",
"大学物理",
"大学化学",
"高等数学",
"概率与统计",
"离散数学",
"电气工程师",
"计量工程师",
"高中数学",
"高中物理",
"高中化学",
"高中生物学",
"中学数学",
"中学生物学",
"中学物理",
"中学化学",
"兽医学",
"大学经济学",
"工商管理",
"马克思主义",
"毛泽东思想",
"教育科学",
"教师资格",
"高中政治",
"高中地理",
"中学政治",
"中学地理",
"现代中国史",
"思想道德修养",
"逻辑",
"法律",
"汉语与文学",
"艺术研究",
"专业旅游指南",
"法律专业",
"高中汉语",
"高中历史",
"中学历史",
"公务员",
"体育科学",
"植物保护",
"基础医学",
"临床医学",
"城市与农村规划",
"会计",
"消防工程师",
"环境影响评估工程师",
"税务会计",
"医生",
]
def test\_split(model\_name):
encoding = tiktoken.encoding\_for\_model("gpt-4")
model\_name\_write = model\_name.replace(":", "\_").replace("/", "\_")
# with open(f"{model\_name\_write}.txt", "w", encoding="utf-8") as f:
# f.write(f"")
# 加载数据集
sum\_total = 0
sum\_correct = 0
for i in range(26, len(task\_list)):
try:
dataset\_tmp = load\_dataset(r"ceval/data", name=task\_list[i])
dataset = concatenate\_datasets(
[dataset\_tmp["dev"], dataset\_tmp["val"]]
)
print(f"\nNo.{i}: {task\_list[i]}({task\_chinese\_name\_list[i]})数据集加载完成, len(dataset)={len(dataset)}")
except:
print(f"\nNo.{i}: {task\_list[i]}({task\_chinese\_name\_list[i]})数据集加载失败")
continue
# 初始化统计变量
correct = 0
total = len(dataset)
for item in tqdm(dataset, desc=f"No.{i}: Processing"):
# for item in dataset:
try:
# 构造完整问题
user\_prompt = f"{item['question']}\nA. {item['A']}\nB. {item['B']}\nC. {item['C']}\nD. {item['D']}\n答案："
# 调用Ollama API
model\_answer = llm(model\_name, user\_prompt, stream=True, encoding=encoding, max\_tokens=4096)
# 提取并验证答案
"""从模型输出中提取答案选项（A/B/C/D）"""
match = re.search(r"[A-D]", model\_answer.upper())
extracted = match.group(0) if match else None
if extracted and extracted == item["answer"]:
correct += 1
except:
print("\nerror.")
# 输出结果
sum\_total += total
sum\_correct += correct
print(f"No.{i}: {task\_list[i]}({task\_chinese\_name\_list[i]})数据集准确率: {correct}/{total} = {correct/total:.2%}")
with open(f"{model\_name\_write}.txt", "a", encoding="utf-8") as f:
f.write(f"No.{i}: {task\_list[i]}({task\_chinese\_name\_list[i]})数据集准确率: {correct}/{total} = {correct/total:.2%}\n\n")
with open(f"{model\_name\_write}.txt", "a", encoding="utf-8") as f:
f.write(f"总准确率: {sum\_correct}/{sum\_total} = {sum\_correct/sum\_total:.2%}\n\n")
print(f"总准确率: {sum\_correct}/{sum\_total} = {sum\_correct/sum\_total:.2%}")
# huihui\_ai/qwen2.5-abliterate:7b-instruct-q4\_K\_M
# qwen2.5:3b-instruct-q8\_0
# qwen2.5:7b-instruct-q5\_K\_M
# deepseek-r1-7b:latest
# test\_split(model\_name="qwen2.5:3b-instruct-q8\_0")
# test\_split(model\_name="qwen2.5:7b-instruct-q5\_K\_M")
# test\_split(model\_name="huihui\_ai/qwen2.5-abliterate:7b-instruct-q4\_K\_M")
# test\_split(model\_name="qwen2.5:1.5b")
# test\_split(model\_name="qwen2.5:1.5b-instruct-fp16")
# test\_split(model\_name="qwen2.5:3b")
# test\_split(model\_name="gemma3:4b")
# test\_split(model\_name="qwen2.5:7b")
# test\_split(model\_name="gemma3:4b-it-q8\_0")
# test\_split(model\_name="qwen2.5:0.5b-instruct-fp16")
# test\_split(model\_name="qwen2.5:0.5b")
test\_split(model\_name="deepseek-r1:1.5b")
# test\_split(model\_name="deepseek-r1:1.5b-qwen-distill-fp16")
# test\_split(model\_name="deepseek-r1:7b")