---
layout: post
title: "每天一篇目标检测文献一"
date: 2025-03-09 14:20:43 +0800
description: "针对密集行人检测存在小目标检测精度低、模型复杂的问题,提出一种改进 YOLOv8 的轻量化密集 行人检测方法。引入 DualConv 模块替换原始 Conv 模块,帮助更深的卷积层更有效地提取信息,减少计算  冗余并提高检测精度;通过融合 RepViTBlock 结构和分离与增强注意力机制 SEMA(Separated and Enhancement Attention)改进 C2f,构建 RS-C2f 结构,提升模型的泛化和特征融合能力,并降低参数量;"
keywords: "yolov8消融实验"
categories: ['目标检测']
tags: ['计算机视觉', '目标检测', '人工智能', 'Yolo', 'Cnn']
artid: "146130223"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146130223
    alt: "每天一篇目标检测文献一"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146130223
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146130223
cover: https://bing.ee123.net/img/rand?artid=146130223
image: https://bing.ee123.net/img/rand?artid=146130223
img: https://bing.ee123.net/img/rand?artid=146130223
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     每天一篇《目标检测》文献（一）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     今天看的是《改进 YOLOv8 的轻量化密集行人检测方法》。
    </p>
    <hr/>
    <hr id="hr-toc" name="tableOfContents"/>
    <h2>
     一、摘要
    </h2>
    <p>
     针对密集行人检测存在小目标检测精度低、模型复杂的问题,提出一种改进 YOLOv8 的轻量化密集 行人检测方法。引入 DualConv 模块替换原始 Conv 模块,帮助更深的卷积层更有效地提取信息,减少计算  冗余并提高检测精度;通过融合 RepViTBlock 结构和分离与增强注意力机制 SEMA(Separated and Enhancement Attention)改进 C2f,构建 RS-C2f 结构,提升模型的泛化和特征融合能力,并降低参数量;设计全新 的空间金字塔模块 SPPELAN_BiFPN,使模型对小目标行人检测精度显著提高,同时优化计算效率;采用 Focal_Shape-IoU 作为边界框回归损失函数,加快网络的收敛速度,提高对小目标的检测准确率。实验结果  表明,改进模型的 mAP@0.5、Precision 和 Recall 在 CrowdHuman 数据集上提升 2.4%、1.1%和 2.1%,在 WiderPerson 数据集上提升 1.3%、1.0%和 1.7%,同时参数量下降 39.6%。在嵌入式设备上单帧图像平均运 行时间为 55.1ms,平均精度为 90.7%,召回率为 82.9%,表明改进模型在保证轻量化的同时提升了检测精度和速度。
    </p>
    <h2 id="%E4%BA%8C%E3%80%81%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D" name="%E4%BA%8C%E3%80%81%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D">
     二、背景介绍
    </h2>
    <p>
     行人检测是计算机视觉的一项关键技术，随着技术进步，检测技术也不断发展。传统检测主要依靠特征提取和分类器设计，虽然取得不错的成果，但实际应用仍存在问题。近些年，随着深度学习的发展，卷积神经网络（CNN）、区域卷积神经网络（R-CNN）及其衍生模型（Fast R-CNN和Faster R-CNN）都促进了行人检测技术的增长。SSD和YOLO这类实时检测的算法也满足了动态和复杂环境下的检测需求，但在复杂背景且聚集的情况下，检测变得更加困难。
    </p>
    <p>
     为了解决这些问题，也有许多研究者做出了贡献。尽管效果良好，但仍存在以下的问题：
    </p>
    <ol>
     <li>
      模型参数较大，复杂度高，不适合资源受限的设备
     </li>
     <li>
      行人等小尺寸目标分布密集且像素面积小，容易漏检
     </li>
     <li>
      行人信息容易丢失或特征冗余
     </li>
     <li>
      数据集目标尺度差异大，边框可能标注不准确，增加低质量样本对锚框回归的干扰
     </li>
    </ol>
    <p>
     针对以上问题，本文进行了如下的改进：
    </p>
    <ol>
     <li>
      采用DualConv模块替换Conv模块，减少计算冗余
     </li>
     <li>
      融合RepViTBlock结构和分离与增强注意力机制SEMA改进C2f，构建RS-C2f结构，降低模型参数量和计算量，实现轻量化
     </li>
     <li>
      设计SPPELAN_BiFPN空间金字塔模块，避免信息丢失或特征冗余，提升重要特征的表达能力
     </li>
     <li>
      采用Focal_Shape-IoU作为边界框回归损失函数，减少低质量样本对锚框回归的影响
     </li>
    </ol>
    <h2 id="%E4%B8%89%E3%80%81YOLOv8%E4%BB%8B%E7%BB%8D" name="%E4%B8%89%E3%80%81YOLOv8%E4%BB%8B%E7%BB%8D">
     三、YOLOv8介绍
    </h2>
    <p>
     YOLOv8是对YOLOv5的改进，网络结构如下：
    </p>
    <p>
     <img alt="" height="1361" src="https://i-blog.csdnimg.cn/direct/454f4bc007754f29b709839935de14f1.png" width="1052"/>
    </p>
    <p>
     相对于yolov5的改进主要有以下几点：
    </p>
    <ul>
     <li>
      用C2f替代C3模块，减少冗杂参数，具有更少参数量和更强的特征提取能力
     </li>
     <li>
      引用自适应NMS算法，调整阈值，减少误检和漏检
     </li>
     <li>
      采用基于Anchor-Free的检测方式，直接预测目标中心和宽高，减少Anchor框的超参数
     </li>
    </ul>
    <h2 id="%E5%9B%9B%20%E6%94%B9%E8%BF%9B%E7%BB%93%E6%9E%84%E4%BB%8B%E7%BB%8D" name="%E5%9B%9B%20%E6%94%B9%E8%BF%9B%E7%BB%93%E6%9E%84%E4%BB%8B%E7%BB%8D">
     四 改进结构介绍
    </h2>
    <p>
     针对上述的不足，本文对YOLOv8进行了改进，改进的结构如下：
    </p>
    <p>
     <img alt="" height="936" src="https://i-blog.csdnimg.cn/direct/2949281346e4406a855880a6ee2aeaba.png" width="1138"/>
    </p>
    <p>
     可以看到，相比于原结构，改进的YOLOv8结构中，将Backbone中的第二三四个C2f模块替换成了RS-C2f模块，第四和第五个CBS模块替换成DualConv模块，输出SPPF替换成了SPPELAN_BiFPN。Neck中第一和第三四个C2f模块也被替换成RS-C2f模块，第二个CBS模块替换成了DualConv模块。在Head中没有修改，都是对三个模块产生的结果进行检测任务。
    </p>
    <h3 id="4.1%20%E5%8F%8C%E5%8D%B7%E7%A7%AF%E5%86%85%E6%A0%B8%EF%BC%88DualConv%EF%BC%89" name="4.1%20%E5%8F%8C%E5%8D%B7%E7%A7%AF%E5%86%85%E6%A0%B8%EF%BC%88DualConv%EF%BC%89">
     4.1 双卷积内核（DualConv）
    </h3>
    <p>
     双卷积内核配置激活函数和归一化层，增强了网络的非线性表达能力。我们这里假设输出特征图大小为D×D×N，输入特征映射在卷积层通过了N个大小为K×K×M的卷积滤波器进行滤波，则计算量
     <img alt="FL_{Conv}" class="mathcode" src="https://latex.csdn.net/eq?FL_%7BConv%7D">
      定义如下：
     </img>
    </p>
    <p style="text-align:center">
     <img alt="FL_{Conv}=K^{2}\times D^{2}\times M\times N" class="mathcode" src="https://latex.csdn.net/eq?FL_%7BConv%7D%3DK%5E%7B2%7D%5Ctimes%20D%5E%7B2%7D%5Ctimes%20M%5Ctimes%20N"/>
    </p>
    <p>
     K×K是卷积核的大小，D是输出特征图的宽、高的维度，M是输入通道数（输入特征图的深度），N是卷积滤波器数量和输出通道数（输出特征图的深度）。
    </p>
    <p>
     有了这个公式，我们再来看双卷积内核，其中采用群卷积和对偶卷积中的组数G调节卷积核K×K在卷积滤波器中的占比。对于一个指定的G，带线啊哦为（K×K+1×1）的组合卷积核在通道中的比例为
     <img alt="\frac{1}{G}" class="mathcode" src="https://latex.csdn.net/eq?%5Cfrac%7B1%7D%7BG%7D">
      ，而剩余1×1的卷积核比例则为（1-
      <img alt="\frac{1}{G}" class="mathcode" src="https://latex.csdn.net/eq?%5Cfrac%7B1%7D%7BG%7D">
       ）。所以G个卷积滤波器构成的双卷积层中，组合卷积核计算量则为：
      </img>
     </img>
    </p>
    <p style="text-align:center">
     <img alt="FL_{K+1}=\frac{\left ( K^{2}+1 \right )\times D^{2}\times M\times N}{G}" class="mathcode" src="https://latex.csdn.net/eq?FL_%7BK&amp;plus;1%7D%3D%5Cfrac%7B%5Cleft%20%28%20K%5E%7B2%7D&amp;plus;1%20%5Cright%20%29%5Ctimes%20D%5E%7B2%7D%5Ctimes%20M%5Ctimes%20N%7D%7BG%7D"/>
    </p>
    <p>
     剩余1×1的卷积核计算量为：
    </p>
    <p style="text-align:center">
     <img alt="FL_{1}=\frac{D^{2}\times M\times N\times \left ( G -1 \right )}{G }" class="mathcode" src="https://latex.csdn.net/eq?FL_%7B1%7D%3D%5Cfrac%7BD%5E%7B2%7D%5Ctimes%20M%5Ctimes%20N%5Ctimes%20%5Cleft%20%28%20G%20-1%20%5Cright%20%29%7D%7BG%20%7D"/>
    </p>
    <p>
     总计算量为：
    </p>
    <p style="text-align:center">
     <img alt="FL_{ALL}=FL_{K+1}+FL_{1}=\frac{\left ( K^{2}+G \right )\times D^{2}\times M\times N}{G }" class="mathcode" src="https://latex.csdn.net/eq?FL_%7BALL%7D%3DFL_%7BK&amp;plus;1%7D&amp;plus;FL_%7B1%7D%3D%5Cfrac%7B%5Cleft%20%28%20K%5E%7B2%7D&amp;plus;G%20%5Cright%20%29%5Ctimes%20D%5E%7B2%7D%5Ctimes%20M%5Ctimes%20N%7D%7BG%20%7D"/>
    </p>
    <p>
     比较一下卷积层和标准卷积层的计算成本（FLOPs）：
    </p>
    <p style="text-align:center">
     <img alt="R=\frac{FL_{ALL}}{FL_{Conv}}=\frac{1}{G}+\frac{1}{K^{2}}" class="mathcode" src="https://latex.csdn.net/eq?R%3D%5Cfrac%7BFL_%7BALL%7D%7D%7BFL_%7BConv%7D%7D%3D%5Cfrac%7B1%7D%7BG%7D&amp;plus;%5Cfrac%7B1%7D%7BK%5E%7B2%7D%7D"/>
    </p>
    <p>
     双卷积内核结构如下：
    </p>
    <p>
     <img alt="" height="685" src="https://i-blog.csdnimg.cn/direct/e68e7f6b67c64e118481d08dcd602584.png" width="1200"/>
    </p>
    <p>
     它结合了1×1 和3×3卷积核同时处理相同的输入特征图通道，利用组卷积技术排列卷积滤波器，减少计算成本和参数量。
    </p>
    <p>
     采用DualConv代替标准卷积，能够使更深的卷积层更有效地提取信息，降低模型参数量和计算量，并在最大程度上减少特征信息丢失。
    </p>
    <h3 id="4.2%20RS-C2f%E6%A8%A1%E5%9D%97" name="4.2%20RS-C2f%E6%A8%A1%E5%9D%97">
     4.2 RS-C2f模块
    </h3>
    <p>
     分离与增强注意力模块（SEAM）和RepViTBlock的结构如下图所示：
    </p>
    <p>
     <img alt="" height="629" src="https://i-blog.csdnimg.cn/direct/5d183e5b44614ec1b944def0144e3391.png" width="1084"/>
    </p>
    <p>
     左图中左边是CSMM（通道和空间混合模块），右边是SEAM的体系结构。右图中的模块通过结构重新参数化技术将分词混频器和通道混频器分开，SE层在RepViT中是可选的。
    </p>
    <p>
     整个RS-C2f模块的网络结构图如下所示：
    </p>
    <p>
     <img alt="" height="810" src="https://i-blog.csdnimg.cn/direct/789db6039b5c4745aab550eb7e87818c.png" width="1105"/>
    </p>
    <p>
     行人检测中目标通常较小，为有效提高对小目标的检测能力，作者提出在RepViTBlcok模块的输出操作前面加入了分离与增强注意力模块，融合成RVB-SEAM模块，同时将C2f模块中所有的Bottleneck改为RVB-SEAM，得到了全新的RS-C2f。这样，模型运算效率提升，参数量也减小，行人相应损失也更好地补偿，检测能力得到提升。
    </p>
    <h3 id="4.3%20%E7%A9%BA%E9%97%B4%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0%E5%8C%96%E6%94%B9%E8%BF%9B%EF%BC%88SPPELAN_BiFPN%EF%BC%89" name="4.3%20%E7%A9%BA%E9%97%B4%E9%87%91%E5%AD%97%E5%A1%94%E6%B1%A0%E5%8C%96%E6%94%B9%E8%BF%9B%EF%BC%88SPPELAN_BiFPN%EF%BC%89">
     4.3 空间金字塔池化改进（SPPELAN_BiFPN）
    </h3>
    <p>
     空间金字塔池化是经典的特征提取方法，通过对输入特征图进行不同尺度的池化操作，可以获取更加丰富和具有代表性的特征信息。然而，在YOLOv8中，空间金字塔池化模块SPPF侧重于加速池化操作，未考虑定位的准确性，对特征的选择关注较少。
    </p>
    <p>
     本文结合了BiFPN与Concat组成的Concat_BiFPN替换了SPPLELAN中的Concat，形成了新的空间特征金字塔池化模块SPPELAN_BiFPN。结构如下所示：
    </p>
    <p>
     <img alt="" height="785" src="https://i-blog.csdnimg.cn/direct/29aaa1fb5ee14f368cd9334c7aecea3c.png" width="1174"/>
    </p>
    <p>
     SPPELAN充分利用SPP的空间金字塔池化能力和ELAN的高校特征聚合能力。同时BiFPN通过改进特征融合路径来提升效率。BiFPN特征融合和SPPELAN_BiFPN结构图如下：
    </p>
    <p>
     <img alt="" height="730" src="https://i-blog.csdnimg.cn/direct/4b20cd1d1ec74d0296a681d4ff614152.png" width="1135"/>
    </p>
    <p>
     融合过后的模块可以确保各尺度特征之间的平衡，避免信息丢失或特征的冗余，提升重要特征表达能力。
    </p>
    <h3 id="4.4%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BC%98%E5%8C%96%EF%BC%88Focal_Shap-IoU%EF%BC%89" name="4.4%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%BC%98%E5%8C%96%EF%BC%88Focal_Shap-IoU%EF%BC%89">
     4.4 损失函数优化（Focal_Shap-IoU）
    </h3>
    <p>
     小目标检测在训练期间前景类和背景类之间存在极端不平衡的问题，有人提出一种新的动态缩放的交叉熵损失函数（Focal Loss）,通过一个动态缩放因子，以达到动态降低训练过程中易区分样本的权重，公式如下：
    </p>
    <p style="text-align:center">
     <img alt="FL\left ( P_{t} \right )=-\left ( 1-P_{t} \right )^{\gamma }\times log\left ( P_{t} \right )" class="mathcode" src="https://latex.csdn.net/eq?FL%5Cleft%20%28%20P_%7Bt%7D%20%5Cright%20%29%3D-%5Cleft%20%28%201-P_%7Bt%7D%20%5Cright%20%29%5E%7B%5Cgamma%20%7D%5Ctimes%20log%5Cleft%20%28%20P_%7Bt%7D%20%5Cright%20%29"/>
    </p>
    <p>
     <img alt="P_{t}" class="mathcode" src="https://latex.csdn.net/eq?P_%7Bt%7D"/>
     代表模型对真实样本的预测概率，γ是控制异常值抑制程度的参数，γ=0时，Focal Loss退化为交叉熵损失，γ&gt;0时，Focal Loss降低易分类样本的权重，使难分类样本得到关注。
    </p>
    <p>
     同时考虑边框回归样本自身形状与尺度对回归结果产生的影响，有人提出了回归准确性的边界框回归损失函数Shape-IoU，对应损失为：
    </p>
    <p style="text-align:center">
     <img alt="L_{Shape-IoU}=1-IoU+d^{Shape}+0.5\times \Omega ^{Shape}" class="mathcode" src="https://latex.csdn.net/eq?L_%7BShape-IoU%7D%3D1-IoU&amp;plus;d%5E%7BShape%7D&amp;plus;0.5%5Ctimes%20%5COmega%20%5E%7BShape%7D"/>
    </p>
    <p>
     IoU为交并比，用于衡量预测框与真实框之间的重叠程度；
     <img alt="d^{Shape}" class="mathcode" src="https://latex.csdn.net/eq?d%5E%7BShape%7D"/>
     是形状距离，用于衡量预测框与真实框之间的距离；
     <img alt="\Omega ^{Shape}" class="mathcode" src="https://latex.csdn.net/eq?%5COmega%20%5E%7BShape%7D"/>
     是惩罚项，用于惩罚预测框与真实框之间的形状差异。其定义如下：
    </p>
    <p style="text-align:center">
     <img alt="d^{Shape}=\frac{HH\times \left ( x_{c}-x^{gt}_{c} \right )^{2}}{c^{2}}+\frac{WW\times \left ( y_{c}-y^{gt}_{c} \right )^{2}}{c^{2}}" class="mathcode" src="https://latex.csdn.net/eq?d%5E%7BShape%7D%3D%5Cfrac%7BHH%5Ctimes%20%5Cleft%20%28%20x_%7Bc%7D-x%5E%7Bgt%7D_%7Bc%7D%20%5Cright%20%29%5E%7B2%7D%7D%7Bc%5E%7B2%7D%7D&amp;plus;%5Cfrac%7BWW%5Ctimes%20%5Cleft%20%28%20y_%7Bc%7D-y%5E%7Bgt%7D_%7Bc%7D%20%5Cright%20%29%5E%7B2%7D%7D%7Bc%5E%7B2%7D%7D"/>
    </p>
    <p style="text-align:center">
     <img alt="\Omega ^{Shape}=\sum_{t=H,W}\left ( 1-e^{-\omega _{t}} \right )^{4}" class="mathcode" src="https://latex.csdn.net/eq?%5COmega%20%5E%7BShape%7D%3D%5Csum_%7Bt%3DH%2CW%7D%5Cleft%20%28%201-e%5E%7B-%5Comega%20_%7Bt%7D%7D%20%5Cright%20%29%5E%7B4%7D"/>
    </p>
    <p style="text-align:center">
     <img alt="\left\{\begin{matrix} \omega _{W}=HH\times \frac{\left | W-W^{gt} \right |}{max\left ( W,W^{gt} \right )} \\ \omega_{H}=WW\times \frac{\left | H-H^{gt} \right |}{max\left ( H,H^{gt} \right )} \end{matrix}\right." class="mathcode" src="https://latex.csdn.net/eq?%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%20%5Comega%20_%7BW%7D%3DHH%5Ctimes%20%5Cfrac%7B%5Cleft%20%7C%20W-W%5E%7Bgt%7D%20%5Cright%20%7C%7D%7Bmax%5Cleft%20%28%20W%2CW%5E%7Bgt%7D%20%5Cright%20%29%7D%20%5C%5C%20%5Comega_%7BH%7D%3DWW%5Ctimes%20%5Cfrac%7B%5Cleft%20%7C%20H-H%5E%7Bgt%7D%20%5Cright%20%7C%7D%7Bmax%5Cleft%20%28%20H%2CH%5E%7Bgt%7D%20%5Cright%20%29%7D%20%5Cend%7Bmatrix%7D%5Cright."/>
    </p>
    <p>
     WW和HH分别代表水平和垂直方向的权重系数，W和H代表锚框的宽和高，
     <img alt="W^{gt}H^{gt}" class="mathcode" src="https://latex.csdn.net/eq?W%5E%7Bgt%7DH%5E%7Bgt%7D"/>
     代表GT框的宽和高，
     <img alt="\left ( x_{c},y_{c} \right )\left ( x^{gt}_{c},y^{gt}_{c} \right )" class="mathcode" src="https://latex.csdn.net/eq?%5Cleft%20%28%20x_%7Bc%7D%2Cy_%7Bc%7D%20%5Cright%20%29%5Cleft%20%28%20x%5E%7Bgt%7D_%7Bc%7D%2Cy%5E%7Bgt%7D_%7Bc%7D%20%5Cright%20%29"/>
     分别表示真实框和预测框的中心位置。
    </p>
    <p>
     结合以上两种损失函数，构建了Focal_Shape-IoU损失函数：
    </p>
    <p style="text-align:center">
     <img alt="L_{Focal_Shape-IoU}=IoU^{\gamma }L_{Shape-IoU}" class="mathcode" src="https://latex.csdn.net/eq?L_%7BFocal_Shape-IoU%7D%3DIoU%5E%7B%5Cgamma%20%7DL_%7BShape-IoU%7D"/>
    </p>
    <p>
     该损失函数解决了样本不平衡和边框回归不稳定的问题，减少低质量样本对锚框回归的影响，提高检测稳定性。
    </p>
    <h2 id="%E4%BA%94%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C" name="%E4%BA%94%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C">
     五、实验结果
    </h2>
    <h3 id="5.1%20%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83" name="5.1%20%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83">
     5.1 实验环境
    </h3>
    <p>
     实验环境参数如下：
    </p>
    <p>
     <img alt="" height="278" src="https://i-blog.csdnimg.cn/direct/3471517a3ef04f10b91a7bada465b85c.png" width="685"/>
    </p>
    <p>
     实验采用的数据集是Wider Person和CrowdHuman。
    </p>
    <h3 id="5.2%20%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C" name="5.2%20%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C">
     5.2 对比实验
    </h3>
    <p>
     本文对不同损失函数、不同C2f模块和不同特征金字塔改进模块和不同算法上都进行了实验。实验结果较好：
    </p>
    <p>
     <strong>
      不同损失函数：
     </strong>
    </p>
    <p>
     <img alt="" height="783" src="https://i-blog.csdnimg.cn/direct/b13cf788a2f04adaba85818000bfefe2.png" width="1319"/>
    </p>
    <p>
     <strong>
      不同C2f模块：
     </strong>
    </p>
    <p>
     <img alt="" height="1717" src="https://i-blog.csdnimg.cn/direct/e1e28bee11274c1cb9ff8e47a7e1fd7d.png" width="1371"/>
    </p>
    <p>
     <strong>
      不同空间特征金字塔改进模块：
     </strong>
    </p>
    <p>
     <img alt="" height="1001" src="https://i-blog.csdnimg.cn/direct/9e86c4f752ed4f3a8028468e261fedc2.png" width="1417"/>
    </p>
    <p>
     <strong>
      不同算法：
     </strong>
    </p>
    <p>
     <img alt="" height="445" src="https://i-blog.csdnimg.cn/direct/d6cb00e7a8e849ea9fc5a1b6105f9cbd.png" width="1415"/>
    </p>
    <h3 id="5.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C" name="5.3%20%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C">
     5.3 消融实验
    </h3>
    <p>
     Crow Human数据集：
    </p>
    <p>
     <img alt="" height="532" src="https://i-blog.csdnimg.cn/direct/c6a133f393f543c39c2e8cffac00dcd2.png" width="1411"/>
    </p>
    <p>
     WiderPerson数据集：
    </p>
    <p>
     <img alt="" height="898" src="https://i-blog.csdnimg.cn/direct/75c5dc59242f4b73a61039f241e5659a.png" width="1410"/>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35323838393331372f:61727469636c652f64657461696c732f313436313330323233" class_="artid" style="display:none">
 </p>
</div>


