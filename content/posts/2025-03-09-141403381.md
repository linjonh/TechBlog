---
layout: post
title: "Andrej-Karpathy-神经网络从Zero到Hero-2.语言模型的两种实现方式-Bigram-和-神经网络"
date: 2025-03-09 22:55:25 +0800
description: "【系列笔记】本文主要参考，演示。"
keywords: "【Andrej Karpathy 神经网络从Zero到Hero】--2.语言模型的两种实现方式 （Bigram 和 神经网络）"
categories: ['自然语言处理', '深度学习']
tags: ['语言模型', '深度学习']
artid: "141403381"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=141403381
    alt: "Andrej-Karpathy-神经网络从Zero到Hero-2.语言模型的两种实现方式-Bigram-和-神经网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=141403381
featuredImagePreview: https://bing.ee123.net/img/rand?artid=141403381
cover: https://bing.ee123.net/img/rand?artid=141403381
image: https://bing.ee123.net/img/rand?artid=141403381
img: https://bing.ee123.net/img/rand?artid=141403381
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【Andrej Karpathy 神经网络从Zero到Hero】--2.语言模型的两种实现方式 （Bigram 和 神经网络）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <p>
    </p>
    <p>
     【系列笔记】
     <br/>
     <a href="https://blog.csdn.net/muyuu/article/details/141354063?sharetype=blogdetail&amp;sharerId=141354063&amp;sharerefer=PC&amp;sharesource=muyuu&amp;spm=1011.2480.3001.8118">
      【Andrej Karpathy 神经网络从Zero到Hero】–1. 自动微分autograd实践要点
     </a>
    </p>
    <p>
     本文主要参考
     <a href="https://www.bilibili.com/video/BV1Rx4y1k7Fu/?spm_id_from=333.999.0.0&amp;vd_source=f1d5905443a1bb5ec85e99cddb8f2650" rel="nofollow">
      大神Andrej Karpathy 大模型讲座 | 构建makemore 系列之一：讲解语言建模的明确入门
     </a>
     ，演示
    </p>
    <ol>
     <li>
      如何利用统计数值构建一个简单的 Bigram 语言模型
     </li>
     <li>
      如何用一个神经网络来复现前面 Bigram 语言模型的结果，以此来展示神经网络相对于传统 n-gram 模型的拓展性。
     </li>
    </ol>
    <hr/>
    <h2>
     <a id="_Bigram__13">
     </a>
     统计 Bigram 语言模型
    </h2>
    <p>
     首先给定一批数据，每个数据是一个英文名字，例如：
    </p>
    <pre><code class="prism language-bash"><span class="token punctuation">[</span><span class="token string">'emma'</span>,
 <span class="token string">'olivia'</span>,
 <span class="token string">'ava'</span>,
 <span class="token string">'isabella'</span>,
 <span class="token string">'sophia'</span>,
 <span class="token string">'charlotte'</span>,
 <span class="token string">'mia'</span>,
 <span class="token string">'amelia'</span>,
 <span class="token string">'harper'</span>,
 <span class="token string">'evelyn'</span><span class="token punctuation">]</span>
</code></pre>
    <p>
     Bigram语言模型的做法很简单，首先将数据中的英文名字都做成一个个bigram的数据
    </p>
    <img src="https://i-blog.csdnimg.cn/direct/f960aaf584914b0cb3a840257f59e547.png#pic_center" width="80%">
     <p>
      其中每个格子中是对应的二元组，eg: “rh” ，在所有数据中出现的次数。那么一个自然的想法是对于给定的字母，取其对应的行，将次数归一化转成概率值，然后根据概率分布抽取下一个可能的字母：
     </p>
     <pre><code class="prism language-python">g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span>
P <span class="token operator">=</span> N<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># N 即为上述 counts 矩阵</span>
P <span class="token operator">=</span> P <span class="token operator">/</span> P<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># P是每行归一化后的概率值</span>

<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  
  out <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  ix <span class="token operator">=</span> <span class="token number">0</span>  <span class="token comment">## start符和end符都用 id=0 表示，这里是start</span>
  <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
    p <span class="token operator">=</span> P<span class="token punctuation">[</span>ix<span class="token punctuation">]</span> <span class="token comment"># 当前字符为 ix 时，预测下一个字符的概率分布，实质是一个多项分布（即可能抽到的值有多个，eg: 掷色子是六项分布）</span>
    ix <span class="token operator">=</span> torch<span class="token punctuation">.</span>multinomial<span class="token punctuation">(</span>p<span class="token punctuation">,</span> num_samples<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> replacement<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    out<span class="token punctuation">.</span>append<span class="token punctuation">(</span>itos<span class="token punctuation">[</span>ix<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> ix <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span> <span class="token comment">## 当运行到end符，停止生成</span>
      <span class="token keyword">break</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
     <p>
      输出类似于：
     </p>
     <pre><code class="prism language-bash">mor.
axx.
minaymoryles.
kondlaisah.
anchshizarie.
</code></pre>
     <h3>
      <a id="_64">
      </a>
      质量评价方法
     </h3>
     <p>
      我们还需要方法来评估语言模型的质量，一个直观的想法是:
      <br/>
      <span class="katex--display">
       <span class="katex-display">
        <span class="katex">
         <span class="katex-mathml">
          P 
         
        
          ( 
         
         
         
           s 
          
         
           1 
          
         
         
         
           s 
          
         
           2 
          
         
        
          . 
         
        
          . 
         
        
          . 
         
         
         
           s 
          
         
           n 
          
         
        
          ) 
         
        
          = 
         
        
          P 
         
        
          ( 
         
         
         
           s 
          
         
           1 
          
         
        
          ) 
         
        
          P 
         
        
          ( 
         
         
         
           s 
          
         
           2 
          
         
        
          ∣ 
         
         
         
           s 
          
         
           1 
          
         
        
          ) 
         
        
          ⋯ 
         
        
          P 
         
        
          ( 
         
         
         
           s 
          
         
           n 
          
         
        
          ∣ 
         
         
         
           s 
          
          
          
            n 
           
          
            − 
           
          
            1 
           
          
         
        
          ) 
         
        
       
         P(s_1s_2...s_n) = P(s_1)P(s_2|s_1)\cdots P(s_n|s_{n-1})
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 1em; vertical-align: -0.25em;">
           </span>
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            P
           </span>
           <span class="mopen">
            (
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   1
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   2
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mord">
            ...
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.1514em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mathnormal mtight">
                   n
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mclose">
            )
           </span>
           <span class="mspace" style="margin-right: 0.2778em;">
           </span>
           <span class="mrel">
            =
           </span>
           <span class="mspace" style="margin-right: 0.2778em;">
           </span>
          </span>
          <span class="base">
           <span class="strut" style="height: 1em; vertical-align: -0.25em;">
           </span>
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            P
           </span>
           <span class="mopen">
            (
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   1
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mclose">
            )
           </span>
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            P
           </span>
           <span class="mopen">
            (
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   2
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mord">
            ∣
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   1
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mclose">
            )
           </span>
           <span class="mspace" style="margin-right: 0.1667em;">
           </span>
           <span class="minner">
            ⋯
           </span>
           <span class="mspace" style="margin-right: 0.1667em;">
           </span>
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            P
           </span>
           <span class="mopen">
            (
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.1514em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mathnormal mtight">
                   n
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.15em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mord">
            ∣
           </span>
           <span class="mord">
            <span class="mord mathnormal">
             s
            </span>
            <span class="msupsub">
             <span class="vlist-t vlist-t2">
              <span class="vlist-r">
               <span class="vlist" style="height: 0.3011em;">
                <span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;">
                 <span class="pstrut" style="height: 2.7em;">
                 </span>
                 <span class="sizing reset-size6 size3 mtight">
                  <span class="mord mtight">
                   <span class="mord mathnormal mtight">
                    n
                   </span>
                   <span class="mbin mtight">
                    −
                   </span>
                   <span class="mord mtight">
                    1
                   </span>
                  </span>
                 </span>
                </span>
               </span>
               <span class="vlist-s">
                ​
               </span>
              </span>
              <span class="vlist-r">
               <span class="vlist" style="height: 0.2083em;">
                <span class="">
                </span>
               </span>
              </span>
             </span>
            </span>
           </span>
           <span class="mclose">
            )
           </span>
          </span>
         </span>
        </span>
       </span>
      </span>
      <br/>
      但上述计算方式有一个问题，概率值都是小于1的，当序列的长度比较长时，上述数值会趋于0，计算时容易下溢。因此
      <mark>
       实践中往往使用
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          l 
         
        
          o 
         
        
          g 
         
        
          ( 
         
        
          P 
         
        
          ) 
         
        
       
         log(P)
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 1em; vertical-align: -0.25em;">
           </span>
           <span class="mord mathnormal" style="margin-right: 0.0197em;">
            l
           </span>
           <span class="mord mathnormal">
            o
           </span>
           <span class="mord mathnormal" style="margin-right: 0.0359em;">
            g
           </span>
           <span class="mopen">
            (
           </span>
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            P
           </span>
           <span class="mclose">
            )
           </span>
          </span>
         </span>
        </span>
       </span>
       来代替，为了可以对比不同长度的序列的预测效果，再进一步使用
       <span class="katex--inline">
        <span class="katex">
         <span class="katex-mathml">
          l 
         
        
          o 
         
        
          g 
         
        
          ( 
         
        
          P 
         
        
          ) 
         
        
          / 
         
        
          n 
         
        
       
         log(P)/n
         </span>
         <span class="katex-html">
          <span class="base">
           <span class="strut" style="height: 1em; vertical-align: -0.25em;">
           </span>
           <span class="mord mathnormal" style="margin-right: 0.0197em;">
            l
           </span>
           <span class="mord mathnormal">
            o
           </span>
           <span class="mord mathnormal" style="margin-right: 0.0359em;">
            g
           </span>
           <span class="mopen">
            (
           </span>
           <span class="mord mathnormal" style="margin-right: 0.1389em;">
            P
           </span>
           <span class="mclose">
            )
           </span>
           <span class="mord">
            /
           </span>
           <span class="mord mathnormal">
            n
           </span>
          </span>
         </span>
        </span>
       </span>
       表示一个序列平均的质量
      </mark>
      。
     </p>
     <p>
      上述统计 Bigram 模型在训练数据上的平均质量为：
     </p>
     <pre><code class="prism language-python">log_likelihood <span class="token operator">=</span> <span class="token number">0.0</span>
n <span class="token operator">=</span> <span class="token number">0</span>

<span class="token keyword">for</span> w <span class="token keyword">in</span> words<span class="token punctuation">:</span> <span class="token comment"># 所有word里的二元组概率叠加</span>
  chs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> ch1<span class="token punctuation">,</span> ch2 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>chs<span class="token punctuation">,</span> chs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    ix1 <span class="token operator">=</span> stoi<span class="token punctuation">[</span>ch1<span class="token punctuation">]</span>
    ix2 <span class="token operator">=</span> stoi<span class="token punctuation">[</span>ch2<span class="token punctuation">]</span>
    prob <span class="token operator">=</span> P<span class="token punctuation">[</span>ix1<span class="token punctuation">,</span> ix2<span class="token punctuation">]</span>
    logprob <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>
    log_likelihood <span class="token operator">+=</span> logprob
    n <span class="token operator">+=</span> <span class="token number">1</span> <span class="token comment"># 所有word里的二元组数量之和</span>

nll <span class="token operator">=</span> <span class="token operator">-</span>log_likelihood
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>nll<span class="token operator">/</span>n<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span> <span class="token comment">## 值为 2.4764，表示前面做的bigram模型，对现有训练数据的置信度</span>
                  <span class="token comment">## 这个值越低表示当前模型越认可训练数据的质量，而由于训练数据是我们认为“好”的数据，因此反过来就说明这个模型好</span>
</code></pre>
     <p>
      但这里有一个问题是，例如：
     </p>
     <pre><code class="prism language-python">log_likelihood <span class="token operator">=</span> <span class="token number">0.0</span>
n <span class="token operator">=</span> <span class="token number">0</span>

<span class="token comment">#for w in words:</span>
<span class="token keyword">for</span> w <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"andrejz"</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
  chs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> ch1<span class="token punctuation">,</span> ch2 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>chs<span class="token punctuation">,</span> chs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    ix1 <span class="token operator">=</span> stoi<span class="token punctuation">[</span>ch1<span class="token punctuation">]</span>
    ix2 <span class="token operator">=</span> stoi<span class="token punctuation">[</span>ch2<span class="token punctuation">]</span>
    prob <span class="token operator">=</span> P<span class="token punctuation">[</span>ix1<span class="token punctuation">,</span> ix2<span class="token punctuation">]</span>
    logprob <span class="token operator">=</span> torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>prob<span class="token punctuation">)</span>
    log_likelihood <span class="token operator">+=</span> logprob
    n <span class="token operator">+=</span> <span class="token number">1</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ch1<span class="token punctuation">}</span></span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ch2<span class="token punctuation">}</span></span><span class="token string">: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>prob<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>logprob<span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>log_likelihood<span class="token operator">=</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
nll <span class="token operator">=</span> <span class="token operator">-</span>log_likelihood
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>nll<span class="token operator">=</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>nll<span class="token operator">/</span>n<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre>
     <p>
      输出是
     </p>
     <pre><code class="prism language-python"><span class="token punctuation">.</span>a<span class="token punctuation">:</span> <span class="token number">0.1377</span> <span class="token operator">-</span><span class="token number">1.9829</span>
an<span class="token punctuation">:</span> <span class="token number">0.1605</span> <span class="token operator">-</span><span class="token number">1.8296</span>
nd<span class="token punctuation">:</span> <span class="token number">0.0384</span> <span class="token operator">-</span><span class="token number">3.2594</span>
dr<span class="token punctuation">:</span> <span class="token number">0.0771</span> <span class="token operator">-</span><span class="token number">2.5620</span>
re<span class="token punctuation">:</span> <span class="token number">0.1336</span> <span class="token operator">-</span><span class="token number">2.0127</span>
ej<span class="token punctuation">:</span> <span class="token number">0.0027</span> <span class="token operator">-</span><span class="token number">5.9171</span>
jz<span class="token punctuation">:</span> <span class="token number">0.0000</span> <span class="token operator">-</span>inf
z<span class="token punctuation">.</span><span class="token punctuation">:</span> <span class="token number">0.0667</span> <span class="token operator">-</span><span class="token number">2.7072</span>
log_likelihood<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span>inf<span class="token punctuation">)</span>
nll<span class="token operator">=</span>tensor<span class="token punctuation">(</span>inf<span class="token punctuation">)</span>
inf
</code></pre>
     <p>
      可以发现由于，jz 在计数矩阵 N 中为0，即数据中没有出现过，导致 log(loss) 变成了负无穷，这里为了避免这样的情况，需要做
      <mark>
       平滑处理
      </mark>
      ，即
      <code>
       P = N.float()
      </code>
      改成
      <code>
       P = (N+1).float()
      </code>
      ，这样上述代码输出变成：
     </p>
     <pre><code class="prism language-python"><span class="token punctuation">.</span>a<span class="token punctuation">:</span> <span class="token number">0.1376</span> <span class="token operator">-</span><span class="token number">1.9835</span>
an<span class="token punctuation">:</span> <span class="token number">0.1604</span> <span class="token operator">-</span><span class="token number">1.8302</span>
nd<span class="token punctuation">:</span> <span class="token number">0.0384</span> <span class="token operator">-</span><span class="token number">3.2594</span>
dr<span class="token punctuation">:</span> <span class="token number">0.0770</span> <span class="token operator">-</span><span class="token number">2.5646</span>
re<span class="token punctuation">:</span> <span class="token number">0.1334</span> <span class="token operator">-</span><span class="token number">2.0143</span>
ej<span class="token punctuation">:</span> <span class="token number">0.0027</span> <span class="token operator">-</span><span class="token number">5.9004</span>
jz<span class="token punctuation">:</span> <span class="token number">0.0003</span> <span class="token operator">-</span><span class="token number">7.9817</span>
z<span class="token punctuation">.</span><span class="token punctuation">:</span> <span class="token number">0.0664</span> <span class="token operator">-</span><span class="token number">2.7122</span>
log_likelihood<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">28.2463</span><span class="token punctuation">)</span>
nll<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token number">28.2463</span><span class="token punctuation">)</span>
<span class="token number">3.5307815074920654</span>
</code></pre>
     <p>
      避免了出现
      <code>
       inf
      </code>
      这种数据溢出问题。
     </p>
     <hr/>
     <h2>
      <a id="_146">
      </a>
      神经网络语言模型
     </h2>
     <p>
      接下来尝试用神经网络的方式构建上述bigram语言模型：
     </p>
     <pre><code class="prism language-python"><span class="token comment"># 构建训练数据</span>
xs<span class="token punctuation">,</span> ys <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment"># 分别是前一个字符和要预测的下一个字符的id</span>
<span class="token keyword">for</span> w <span class="token keyword">in</span> words<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
  chs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token builtin">list</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">[</span><span class="token string">'.'</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> ch1<span class="token punctuation">,</span> ch2 <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>chs<span class="token punctuation">,</span> chs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    ix1 <span class="token operator">=</span> stoi<span class="token punctuation">[</span>ch1<span class="token punctuation">]</span>
    ix2 <span class="token operator">=</span> stoi<span class="token punctuation">[</span>ch2<span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>ch1<span class="token punctuation">,</span> ch2<span class="token punctuation">)</span>
    xs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ix1<span class="token punctuation">)</span>
    ys<span class="token punctuation">.</span>append<span class="token punctuation">(</span>ix2<span class="token punctuation">)</span>    
    
xs <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>xs<span class="token punctuation">)</span>
ys <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>ys<span class="token punctuation">)</span>
<span class="token comment"># 输出示例：. e</span>
<span class="token comment">#          e m</span>
<span class="token comment">#          m m</span>
<span class="token comment">#          m a</span>
<span class="token comment">#          a .</span>
<span class="token comment">#       xs: tensor([ 0,  5, 13, 13,  1])</span>
<span class="token comment">#       ys: tensor([ 5, 13, 13,  1,  0])</span>

<span class="token comment"># 随机初始化一个 27*27 的参数矩阵</span>
g <span class="token operator">=</span> torch<span class="token punctuation">.</span>Generator<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">2147483647</span><span class="token punctuation">)</span>
W <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">)</span><span class="token punctuation">,</span> generator<span class="token operator">=</span>g<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 基于正态分布随机初始化</span>
<span class="token comment"># 前向传播</span>
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
xenc <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">27</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 将输入数据xs做成one-hot embedding</span>
logits <span class="token operator">=</span> xenc @ W <span class="token comment"># 用于模拟统计模型中的统计数值矩阵，由于 W 是基于正态分布采样，logits 并非直接是计数值，可以认为是 log(counts)</span>
<span class="token comment">## tensor([[-0.5288, -0.5967, -0.7431,  ...,  0.5990, -1.5881,  1.1731],</span>
<span class="token comment">##        [-0.3065, -0.1569, -0.8672,  ...,  0.0821,  0.0672, -0.3943],</span>
<span class="token comment">##        [ 0.4942,  1.5439, -0.2300,  ..., -2.0636, -0.8923, -1.6962],</span>
<span class="token comment">##        ...,</span>
<span class="token comment">##        [-0.1936, -0.2342,  0.5450,  ..., -0.0578,  0.7762,  1.9665],</span>
<span class="token comment">##        [-0.4965, -1.5579,  2.6435,  ...,  0.9274,  0.3591, -0.3198],</span>
<span class="token comment">##        [ 1.5803, -1.1465, -1.2724,  ...,  0.8207,  0.0131,  0.4530]])</span>
counts <span class="token operator">=</span> logits<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 将 log(counts) 还原成可以看作是 counts 的矩阵</span>
<span class="token comment">## tensor([[ 0.5893,  0.5507,  0.4756,  ...,  1.8203,  0.2043,  3.2321],</span>
<span class="token comment">##        [ 0.7360,  0.8548,  0.4201,  ...,  1.0856,  1.0695,  0.6741],</span>
<span class="token comment">##        [ 1.6391,  4.6828,  0.7945,  ...,  0.1270,  0.4097,  0.1834],</span>
<span class="token comment">##        ...,</span>
<span class="token comment">##        [ 0.8240,  0.7912,  1.7245,  ...,  0.9438,  2.1732,  7.1459],</span>
<span class="token comment">##        [ 0.6086,  0.2106, 14.0621,  ...,  2.5279,  1.4320,  0.7263],</span>
<span class="token comment">##        [ 4.8566,  0.3177,  0.2802,  ...,  2.2722,  1.0132,  1.5730]])</span>
probs <span class="token operator">=</span> counts <span class="token operator">/</span> counts<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 用于模拟统计模型中的概率矩阵，这其实即是 softmax 的实现</span>
loss <span class="token operator">=</span> <span class="token operator">-</span>probs<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> ys<span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># loss = log(P)/n， 这其实即是 cross-entropy 的实现</span>
</code></pre>
     <p>
      接下来可以通过
      <code>
       loss.backward()
      </code>
      来更新参数 W:
     </p>
     <pre><code class="prism language-python"><span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  
  <span class="token comment"># forward pass</span>
  xenc <span class="token operator">=</span> F<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>xs<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">27</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span> 
  logits <span class="token operator">=</span> xenc @ W <span class="token comment"># predict log-counts</span>
  counts <span class="token operator">=</span> logits<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span>
  probs <span class="token operator">=</span> counts <span class="token operator">/</span> counts<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> keepdims<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> 
  loss <span class="token operator">=</span> <span class="token operator">-</span>probs<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>num<span class="token punctuation">)</span><span class="token punctuation">,</span> ys<span class="token punctuation">]</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">0.01</span><span class="token operator">*</span><span class="token punctuation">(</span>W<span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">## 这里加上了L2正则，防止过拟合</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  
  <span class="token comment"># backward pass</span>
  W<span class="token punctuation">.</span>grad <span class="token operator">=</span> <span class="token boolean">None</span> <span class="token comment"># 每次反向传播前置为None</span>
  loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
  
  <span class="token comment"># update</span>
  W<span class="token punctuation">.</span>data <span class="token operator">+=</span> <span class="token operator">-</span><span class="token number">50</span> <span class="token operator">*</span> W<span class="token punctuation">.</span>grad  
</code></pre>
     <p>
      注意这里
      <code>
       logits = xenc @ W
      </code>
      由于
      <code>
       xenc
      </code>
      是 one-hot 向量，因此这里
      <code>
       logits
      </code>
      相当于是抽出了 W 中的某一行，而结合 bigram 模型中，loss 实际上是在计算实际的 log(P[x_i, y_i])，
      <mark>
       那么可以认为这里 W 其实是在拟合 bigram 中的计数矩阵 N（不过实际是 logW 在拟合 N）
      </mark>
      。
     </p>
     <p>
      另外上述神经网络的 loss 最终也是达到差不多 2.47 的最低 loss。这是合理的，因为从上面的分析可知，这个神经网络是完全在拟合 bigram 计数矩阵的，没有使用更复杂的特征提取方法，因此效果最终也会差不多。
     </p>
     <p>
      这里 loss 中还加了一个 L2 正则，主要目的是压缩 W，使得它向全 0 靠近，
      <mark>
       这里的效果非常类似于 bigram 中的平滑手段，想象给一个极大的平滑：P = (N+10000).float()`，那么 P 会趋于一个均匀分布，而 W 全为 0 会导致 counts = logits.exp() 全为 1，即也在拟合一个均匀分布。这里前面的参数 0.01 即是用来调整平滑强度的，如果这个给的太大，那么平滑太大了，就会学成一个均匀分布（当然实际不会希望这样，所以不会给很大）
      </mark>
     </p>
    </img>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="687474:70733a2f2f626c6f672e6373646e2e6e65742f6d757975752f:61727469636c652f64657461696c732f313431343033333831" class_="artid" style="display:none">
 </p>
</div>


