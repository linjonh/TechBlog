---
layout: post
title: "论文阅读笔记OpenVLA-An-Open-Source-Vision-Language-Action-Model"
date: 2025-03-09 13:25:19 +0800
description: "OpenVLA: An Open-Source Vision-Language-Action Model 论文阅读笔记"
keywords: "openvla论文翻译"
categories: ['论文阅读笔记']
tags: ['语言模型', '论文阅读', '笔记', '机器人', '人工智能']
artid: "146131087"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146131087
    alt: "论文阅读笔记OpenVLA-An-Open-Source-Vision-Language-Action-Model"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146131087
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146131087
cover: https://bing.ee123.net/img/rand?artid=146131087
image: https://bing.ee123.net/img/rand?artid=146131087
img: https://bing.ee123.net/img/rand?artid=146131087
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     论文阅读笔记——OpenVLA: An Open-Source Vision-Language-Action Model
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <a href="https://arxiv.org/pdf/2406.09246" rel="nofollow">
      OpenVLA 论文
     </a>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/0dab49bca5844c499025e2ed4d0dd460.png#pic_center"/>
    </p>
    <blockquote>
     <p>
      OpenVLA 是一种具有 70 亿参数的开源视觉-语言-动作模型（
      <code>
       Vision-Language-Action, VLA
      </code>
      ），旨在将视觉感知、语言理解和机器人动作控制无缝结合。其核心是一个预训练的视觉条件语言模型（
      <code>
       Vision-Conditioned Language Model
      </code>
      ），通过在
      <code>
       Open-X Embodiment
      </code>
      数据集上进行微调，该数据集包含了 970k 条多样化的机器人操作轨迹，涵盖了广泛的场景和任务。OpenVLA 的架构和训练方法使其在机器人操作和多模态任务中展现出强大的潜力。
     </p>
    </blockquote>
    <p>
     与 Octo 等先前的工作不同，OpenVLA 采用了一种更为端到端的方法。Octo 等模型通常由预训练的组件（如语言嵌入或视觉编码器）组成，并与从头初始化的附加模型组件结合，在策略训练过程中学习如何将这些组件“拼接”在一起。相比之下，OpenVLA 直接对视觉-语言模型（VLM）进行微调，通过将机器人动作视为语言模型词汇中的 token 来生成机器人动作。这种方法不仅简化了模型架构，还增强了多模态任务中的一致性和泛化能力。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4672ac9caf374e9784530b038cf1e33f.png#pic_center">
      <br/>
      最近的 VLM 架构一般涵盖：
     </img>
    </p>
    <ol>
     <li>
      一个视觉编码器，将图像输入映射为多个“图像块嵌入”
     </li>
     <li>
      一个投影器，将视觉编码器的输出嵌入映射到语言模型的输入空间
     </li>
     <li>
      一个大型语言模型LLM骨干
     </li>
    </ol>
    <p>
     作者将机器人动作预测问题重新表述为一个“视觉-语言”任务。具体来说，模型接收观察图像和自然语言任务指令作为输入，并将其映射为一串预测的机器人动作。为了实现这一目标，作者将连续的机器人动作离散化，映射到语言模型的分词器所使用的离散 token 上，从而将动作预测问题转化为语言模型的输出空间问题。
     <br/>
     <strong>
      动作离散化方法
     </strong>
     <br/>
     为了使视觉-语言模型（VLM）的语言模型骨干能够预测机器人动作，作者采用了一种
     <strong>
      基于分位数
     </strong>
     的离散化方法。对于每个动作维度，作者将训练数据中动作的第 1 分位数到第 99 分位数之间的区间均匀划分为若干“箱子”。与 Brohan 等人使用的最小-最大边界方法不同，这种基于分位数的方法能够有效
     <strong>
      忽略数据中的异常动作
     </strong>
     ，避免异常值过度扩展离散化区间，从而保持动作离散化的
     <strong>
      有效粒度
     </strong>
     。
     <br/>
     通过这种方法，作者将一个 N 维的机器人动作表示为 N 个离散整数，每个整数的取值范围为
     <span class="katex--inline">
      <span class="katex">
       <span class="katex-mathml">
        [ 
        
       
         0 
        
       
         , 
        
       
         255 
        
       
         ] 
        
       
      
        [0, 255]
       </span>
       <span class="katex-html">
        <span class="base">
         <span class="strut" style="height: 1em; vertical-align: -0.25em;">
         </span>
         <span class="mopen">
          [
         </span>
         <span class="mord">
          0
         </span>
         <span class="mpunct">
          ,
         </span>
         <span class="mspace" style="margin-right: 0.1667em;">
         </span>
         <span class="mord">
          255
         </span>
         <span class="mclose">
          ]
         </span>
        </span>
       </span>
      </span>
     </span>
     。然而，OpenVLA 使用的语言模型骨干（Llama 分词器）在微调期间仅保留了 **100 个“特殊词”**用于新引入的词，这
     <mark>
      远不足以覆盖 256 个动作 token
     </mark>
     。
    </p>
    <p>
     <strong>
      分词器词汇的调整
     </strong>
     <br/>
     为了解决这一问题，作者借鉴了 Brohan 等人的方法，选择了一种简单而有效的策略：用动作 token 覆盖 Llama 分词器词汇中使用频率最低的 256 个词（即词汇表中最后的 256 个词）。这种方法既避免了分词器词汇的扩展，又确保了动作 token 的有效表示。
    </p>
    <p>
     最终使用
     <code>
      Open X-Embodiment dataset
     </code>
     对模型进行微调，使用
     <code>
      lora finetune
     </code>
     且发现微调
     <code>
      vision encoder
     </code>
     可以帮忙捕捉机器人数据的细粒度空间细节。
    </p>
    <h4>
     <a id="_20">
     </a>
     性能表现
    </h4>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a0dbf2cf78124675bd4f1e04e66739e8.png#pic_center">
      <br/>
      结果显示，OpenVLA在大多数任务中表现最强，并且在通用策略中具有最高的总体成功率。
      <strong>
       RT-2-X也表现良好，优于RT-1-X和Octo（RT-1-X和Octo在这些泛化任务中通常遇到困难），但不如OpenVLA
      </strong>
      。
     </img>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f4d756c7469706c655f782f:61727469636c652f64657461696c732f313436313331303837" class_="artid" style="display:none">
 </p>
</div>


