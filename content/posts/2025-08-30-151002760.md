---
layout: post
title: "ABP-ClickHouse-å®æ—¶-OLAPç‰©åŒ–è§†å›¾ä¸å†™å…¥èšåˆ"
date: 2025-08-30T02:55:20+0800
description: "é¢å‘ .NET/ABP å·¥ç¨‹å¸ˆï¼Œæ–‡ç« ç»™å‡ºâ€œç›´è¿å¾®æ‰¹â€å’Œâ€œKafkaâ†’ClickHouseâ€ä¸¤æ¡å®æ—¶ OLAP é“¾è·¯ï¼šä»¥ ReplacingMergeTree å»é‡ã€AggregatingMergeTreeï¼‹å¢é‡ç‰©åŒ–è§†å›¾å®ç°å†™æ—¶èšåˆï¼›æä¾›å¯å¤ç° SQL ä¸ ABP Worker ä»£ç ã€TTL çƒ­/æ¸©/å†·åˆ†å±‚ä¸è§‚æµ‹æ–¹æ³•ï¼Œå¹¶å«å†™å…¥/æŸ¥è¯¢å‹æµ‹åŠ ClickHouse vs ES/Pinot é€‰å‹å»ºè®®ã€‚"
keywords: "ABP + ClickHouse å®æ—¶ OLAPï¼šç‰©åŒ–è§†å›¾ä¸å†™å…¥èšåˆ"
categories: ['Vnext', 'Abp', '.Net']
tags: ['å®æ—¶Olap', 'Vnext', 'Linq', 'Clickhouse', 'C', 'Abp']
artid: "151002760"
arturl: "https://blog.csdn.net/Kookoos/article/details/151002760"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151002760
    alt: "ABP-ClickHouse-å®æ—¶-OLAPç‰©åŒ–è§†å›¾ä¸å†™å…¥èšåˆ"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151002760
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151002760
cover: https://bing.ee123.net/img/rand?artid=151002760
image: https://bing.ee123.net/img/rand?artid=151002760
img: https://bing.ee123.net/img/rand?artid=151002760
---



# ABP + ClickHouse å®æ—¶ OLAPï¼šç‰©åŒ–è§†å›¾ä¸å†™å…¥èšåˆ



## ABP + ClickHouse å®æ—¶ OLAPï¼šç‰©åŒ–è§†å›¾ä¸å†™å…¥èšåˆ âœ¨

> **ç‰ˆæœ¬å»ºè®®**ï¼šClickHouse **25.3+**ï¼ˆåŸç”Ÿ `JSON` åˆ—ç”Ÿäº§å¯ç”¨ï¼‰ï¼Œ.NET 8ï¼ŒABP 8.x/9.xï¼Œ`ClickHouse.Client` â‰¥ 7.xã€‚ä½äº 25.3 æ—¶ï¼Œè¯·ç”¨ `String + JSONExtract*` å¤„ç† JSONã€‚

---

---

### 1) åœºæ™¯ä¸ç›®æ ‡ ğŸ

* ç›®æ ‡ï¼š10â€“100k EPS å†™å…¥ã€ç§’çº§å‡ºæ•°ã€ä½æˆæœ¬é•¿æœŸç•™å­˜
* å…³é”®èƒ½åŠ›ï¼š

  + **å¢é‡ç‰©åŒ–è§†å›¾ï¼ˆMVï¼‰**ï¼šå†™å…¥è·¯å¾„å®æ—¶æ›´æ–°ï¼Œä¸‹æ¸¸èšåˆè¡¨ç›´æŸ¥
  + **å†™å…¥å»é‡**ï¼š`ReplacingMergeTree` åŸºäºæ’åºé”® + ç‰ˆæœ¬åˆ—ï¼Œ**åˆå¹¶æœŸ**å»é‡ï¼›ä¸€è‡´æ€§æ•æ„ŸæŸ¥è¯¢ç”¨ `FINAL` å…œåº•
  + **å†·çƒ­åˆ†å±‚**ï¼š`TTL â€¦ TO VOLUME` + å­˜å‚¨ç­–ç•¥ï¼ˆçƒ­â†’æ¸©â†’å†·â†’åˆ é™¤ï¼‰
  + **æ‰¹é‡åŒ–**ï¼šä¼˜å…ˆ**æ‰¹é‡ INSERT**ï¼›æˆ– Kafka Engineâ†’MV è½ MergeTree

---

### 2) æ€»ä½“æ¶æ„ï¼ˆåŒæ¥å…¥è·¯å¾„ï¼‰ğŸŒ

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/f902cecc58b0423f9ecdf64933fe4894.png#pic_center)

---

### 3) å†™å…¥é“¾è·¯æ—¶åº â±ï¸

Client






ABP API/Worker






Kafka Topic






Kafka Engine è¡¨






events_raw






mv_hour (Raw->>Agg)






agg_hour






















POST /api/events


Channel å¾®æ‰¹èšåˆ (M æ¡/T ms)


BulkCopy INSERT (æ‰¹é‡)





INSERT è§¦å‘ MVï¼ˆå†™æ—¶ï¼‰

Trigger


å†™å…¥èšåˆ State


produce JSONEachRow


poll åˆ†åŒºæ¶ˆæ¯


mv_kafka_to_raw è½æ˜ç»†





INSERT è§¦å‘ MVï¼ˆå†™æ—¶ï¼‰

Trigger


å†™å…¥èšåˆ State









alt


[ç›´è¿æ¨¡å¼]

[Kafka æ¨¡å¼]




Client




ABP API/Worker




Kafka Topic




Kafka Engine è¡¨




events_raw




mv_hour (Raw->>Agg)




agg_hour

---

### 4) è¡¨è®¾è®¡ä¸å»é‡ç­–ç•¥ ğŸ§±

#### 4.1 Raw æ˜ç»†è¡¨ï¼ˆæœ€ç»ˆä¸€è‡´çš„å¹‚ç­‰/å»é‡ï¼‰

> ç»Ÿä¸€å°†é«˜é‡å¤ç»´åº¦åˆ—ï¼ˆå¦‚ç§Ÿæˆ·ï¼‰è®¾ä¸º `LowCardinality(String)`ï¼Œé™ä½å­˜å‚¨/åŠ é€Ÿ GROUP BYã€‚

```sql
CREATE TABLE IF NOT EXISTS rt.events_raw
(
  tenant       LowCardinality(String),
  event_key    String,
  ts           DateTime64(3, 'UTC'),
  value        Float64,
  ver          UInt64,                 -- ç‰ˆæœ¬åˆ—ï¼ˆæ›´å¤§è¦†ç›–æ—§ï¼‰
  -- 25.3+ å¯ç”¨ JSONï¼›<25.3 ç”¨ String + JSONExtract*
  props        String
)
ENGINE = ReplacingMergeTree(ver)
PARTITION BY toYYYYMM(ts)
ORDER BY (tenant, event_key, ts)
SETTINGS index_granularity = 8192;

```

* **å»é‡å‘ç”Ÿåœ¨åˆå¹¶**ï¼ˆæœ€ç»ˆä¸€è‡´ï¼‰ï¼›`FINAL` ä¸ºå…³é”®æŸ¥è¯¢å…œåº•ï¼ˆæœ‰ä»£ä»·ï¼Œæ…å¸¸æ€åŒ–ï¼‰

#### 4.2 èšåˆè¡¨ï¼šå¢é‡èšåˆï¼ˆState/Mergeï¼‰

```sql
CREATE TABLE IF NOT EXISTS rt.events_hourly
(
  tenant  LowCardinality(String),
  bucket  DateTime('UTC'),
  c   AggregateFunction(count),
  s   AggregateFunction(sum, Float64),
  p95 AggregateFunction(quantileTDigest, Float64),
  p99 AggregateFunction(quantileTDigest, Float64)
)
ENGINE = AggregatingMergeTree()
PARTITION BY toYYYYMM(bucket)
ORDER BY (tenant, bucket);

```

#### 4.3 è¡¨ä¸ MV å…³ç³» ğŸ—ºï¸

events_kafka (Kafka Engine)






mv_kafka_to_raw






events_raw (ReplacingMergeTree)






mv_hour (Raw->Agg)






agg_hour (AggregatingMergeTree)

---

### 5) å†·çƒ­åˆ†å±‚ä¸ä¿ç•™ç­–ç•¥ â„ï¸ğŸ”¥

```sql
CREATE TABLE rt.events_raw ( ... )
ENGINE = ReplacingMergeTree(ver)
PARTITION BY toYYYYMM(ts)
ORDER BY (tenant, event_key, ts)
SETTINGS storage_policy = 'hot_warm_cold'
TTL ts + INTERVAL 7 DAY   TO VOLUME 'warm',
    ts + INTERVAL 180 DAY TO VOLUME 'cold',
    ts + INTERVAL 365 DAY DELETE;

-- ç­–ç•¥å˜æ›´åå¯å¼ºåˆ¶ä¸€æ¬¡
ALTER TABLE rt.events_raw MATERIALIZE TTL;

-- è§‚æµ‹ç­–ç•¥ä¸è¿ç§»
SELECT policy_name, volumes FROM system.storage_policies;
SELECT name, disk_name, path FROM system.parts
 WHERE database='rt' AND table='events_raw' AND active;

```

**TTL ç”Ÿå‘½å‘¨æœŸ**

HOT (SSD)  
0-7d






WARM (HDD)  
7-180d






COLD (HDD/S3)  
>180d






DELETE

---

### 6) ç®¡çº¿ SQLï¼šKafkaâ†’Rawâ†’èšåˆï¼ˆåŒå±‚ MVï¼‰ğŸ§©

#### 6.1 Kafka å…¥å£ & é”™è¯¯ä¾§è·¯

```sql
CREATE TABLE ingest.events_kafka
(
  tenant String,
  event_key String,
  ts      DateTime64(3, 'UTC'),
  value   Float64,
  ver     UInt64
)
ENGINE = Kafka
SETTINGS
  kafka_broker_list='kafka:9092',
  kafka_topic_list='events',
  kafka_group_name='abp-events',
  kafka_format='JSONEachRow',
  kafka_num_consumers=4,
  kafka_handle_error_mode='stream';

-- ä»…è§£æå¤±è´¥æ—¶ï¼Œè™šæ‹Ÿåˆ— _raw_message/_error æœ‰å€¼
CREATE TABLE ingest.events_dlq
(
  ts DateTime DEFAULT now(),
  _error String,
  _raw   String
) ENGINE=MergeTree ORDER BY ts;

CREATE MATERIALIZED VIEW ingest.mv_events_dlq TO ingest.events_dlq AS
SELECT now(), _error, _raw_message
FROM ingest.events_kafka
WHERE length(_error) > 0;

```

> âš ï¸ **Kafka Engine å¤§å¤šè®¾ç½®/æ¨¡å¼éœ€ DROP/CREATE å…¥å£è¡¨åç”Ÿæ•ˆ**ï¼ˆä¸‹æ¸¸ MV/ç›®æ ‡è¡¨å¯å¤ç”¨ï¼‰ï¼›æ³¨æ„æ¶ˆè´¹ç»„ offset ç®¡ç†ã€‚

#### 6.2 Raw è½åœ°

```sql
CREATE TABLE rt.events_raw
(
  tenant LowCardinality(String),
  event_key String,
  ts DateTime64(3,'UTC'),
  value Float64,
  ver   UInt64,
  props String
) ENGINE=ReplacingMergeTree(ver)
ORDER BY (tenant,event_key,ts)
PARTITION BY toYYYYMM(ts);

CREATE MATERIALIZED VIEW ingest.mv_kafka_to_raw
TO rt.events_raw AS
SELECT tenant, event_key, ts, value, ver, '' AS props
FROM ingest.events_kafka
WHERE length(_error)=0;

```

#### 6.3 Rawâ†’Aggï¼ˆå¢é‡ MVï¼‰

```sql
CREATE TABLE rt.events_hourly
(
  tenant LowCardinality(String),
  bucket DateTime('UTC'),
  c   AggregateFunction(count),
  s   AggregateFunction(sum, Float64),
  p95 AggregateFunction(quantileTDigest, Float64),
  p99 AggregateFunction(quantileTDigest, Float64)
) ENGINE=AggregatingMergeTree()
ORDER BY (tenant,bucket)
PARTITION BY toYYYYMM(bucket);

CREATE MATERIALIZED VIEW rt.mv_hour TO rt.events_hourly AS
SELECT
  tenant,
  toStartOfHour(ts) AS bucket,
  countState() AS c,
  sumState(value) AS s,
  quantileTDigestState(0.95)(value) AS p95,
  quantileTDigestState(0.99)(value) AS p99
FROM rt.events_raw
GROUP BY tenant, bucket;

```

**æŸ¥è¯¢ç«¯ï¼ˆåˆå¹¶çŠ¶æ€ï¼‰**ï¼š

```sql
SELECT tenant, bucket,
       countMerge(c) AS cnt,
       sumMerge(s)   AS total,
       quantileTDigestMerge(0.95)(p95) AS p95,
       quantileTDigestMerge(0.99)(p99) AS p99
FROM rt.events_hourly
WHERE tenant='t1'
  AND bucket >= now() - INTERVAL 24 HOUR
GROUP BY tenant, bucket
ORDER BY bucket;

```

---

### 7) ABP é›†æˆï¼ˆ.NETï¼Œç›´è¿å¾®æ‰¹ï¼‰âš™ï¸

#### 7.1 Background Worker + æœ‰ç•Œ Channel + **è¿æ¥å¤ç”¨** + BulkCopy

```csharp
// modules/Abp.Analytics.Pipeline/ClickHouseWriteWorker.cs
using System.Data;
using System.Threading.Channels;
using Volo.Abp.BackgroundWorkers;
using Volo.Abp.DependencyInjection;
using ClickHouse.Client.ADO;
using ClickHouse.Client.Copy;

public sealed class ClickHouseWriteWorker 
    : AsyncPeriodicBackgroundWorkerBase, ISingletonDependency
{
    private readonly Channel<EventRow> _channel;
    private readonly string _conn;
    private ClickHouseConnection? _con;

    public ClickHouseWriteWorker(AbpAsyncTimer timer, IConfiguration cfg)
        : base(timer)
    {
        _channel = Channel.CreateBounded<EventRow>(
            new BoundedChannelOptions(100_000){ FullMode = BoundedChannelFullMode.Wait });
        _conn = cfg.GetConnectionString("ClickHouse")!;
        Timer.Period = 50; // æ¯ 50ms æ£€æŸ¥ä¸€æ¬¡
    }

    public bool TryEnqueue(EventRow row) => _channel.Writer.TryWrite(row);

    protected override async Task DoWorkAsync(PeriodicBackgroundWorkerContext ctx)
    {
        // è¿æ¥å¤ç”¨ï¼Œé™ä½å»ºè¿/æ¡æ‰‹å¼€é”€
        _con ??= new ClickHouseConnection(_conn);
        if (_con.State != ConnectionState.Open)
            await _con.OpenAsync(ctx.CancellationToken);

        var batch = new List<object[]>(capacity: 5000);
        while (batch.Count < 5000 && _channel.Reader.TryRead(out var r))
            batch.Add(new object[]{ r.Tenant, r.Key, r.Ts, r.Value, r.Ver, r.Payload });

        if (batch.Count == 0) return;

        using var bc = new ClickHouseBulkCopy(_con)
        {
            DestinationTableName = "rt.events_raw",
            BatchSize = 5000,
            MaxDegreeOfParallelism = 2
        };
        await bc.WriteToServerAsync(batch, ctx.CancellationToken);
    }
}

public record EventRow(string Tenant, string Key, DateTime Ts, double Value, ulong Ver, string Payload);

```

```csharp
// æ¨¡å—æ³¨å†Œ
[DependsOn(typeof(AbpBackgroundWorkersModule))]
public class AnalyticsPipelineModule : AbpModule
{
    public override void OnApplicationInitialization(ApplicationInitializationContext context)
    {
        var worker = context.ServiceProvider.GetRequiredService<ClickHouseWriteWorker>();
        context.AddBackgroundWorkerAsync(worker);
    }
}

```

> å»ºè®®ï¼šå¯¹ `TryEnqueue` çš„å¤±è´¥è®¡æ•°ã€æ‰¹é‡å†™æˆåŠŸç‡ã€Flush å‘¨æœŸã€é˜Ÿåˆ—æ·±åº¦åšæŒ‡æ ‡ä¸ŠæŠ¥ï¼ˆPrometheus/Grafanaï¼‰ã€‚

---

### 8) JSON å­—æ®µç­–ç•¥ ğŸ“¦

* **25.3+**ï¼š`props JSON`ï¼ˆåŠç»“æ„åŒ–/åŠ¨æ€ schema æ›´å‹å¥½ï¼‰
* **<25.3**ï¼š`props String` + `JSONExtract*`ï¼›é«˜é¢‘è·¯å¾„å¯åš**ç‰©åŒ–åˆ—**æˆ–å®½è¡¨åŒ–ï¼Œå‡å°‘è¿è¡ŒæœŸå¼€é”€

---

### 9) å‹æµ‹ä¸è§‚æµ‹ ğŸ§ª

* **å†™å…¥å‹æµ‹**ï¼š1k/5k/10k EPSï¼›æ‰¹å¤§å° 500/2k/5kï¼›å¯¹æ¯”ç›´è¿ BulkCopy ä¸ Kafka Engine
* **æŸ¥è¯¢å‹æµ‹**ï¼šè¿‘ 24h å°æ—¶æ¡¶ p50/p95/p99ï¼ˆ`quantileTDigest*`ï¼‰
* **ç³»ç»Ÿè¡¨ä¸æ—¥å¿—**ï¼š

  + Kafka æ¶ˆè´¹ï¼š`system.kafka_consumers`ï¼ˆæ»åã€åˆ†åŒºã€groupï¼‰
  + åˆå¹¶/åˆ†ç‰‡ï¼š`system.merges`ã€`system.parts`ã€`system.part_log`
  + è§†å›¾é“¾è·¯ï¼šå¼€å¯ `log_query_views=1` åè¯» `system.query_views_log`
  + å¼‚æ­¥æ’å…¥ï¼ˆè‹¥ç”¨ï¼‰ï¼š`system.asynchronous_insert_log`
  + åˆ†å±‚ç­–ç•¥ï¼š`system.storage_policies`ã€`system.moves`
  + å¦‚éœ€ï¼Œ`SYSTEM FLUSH LOGS` å¼ºåˆ·æ—¥å¿—è¡¨è½ç›˜

---

### 10) ç»´æŠ¤è¦ç‚¹ä¸å¸¸è§å‘ ğŸ› ï¸

1. **MV å»é‡ Ã— Async Insert**ï¼šéƒ¨åˆ†ç‰ˆæœ¬é»˜è®¤**æ‹’ç»** `async_insert=1` ä¸ `deduplicate_blocks_in_dependent_materialized_views=1` åŒå¼€ï¼›ä¼˜å…ˆå®¢æˆ·ç«¯å¾®æ‰¹ã€‚
2. **Kafka è¡¨ DDL/Setting å˜æ›´**ï¼šé€šå¸¸éœ€ **DROP/CREATE** å…¥å£è¡¨ï¼ˆMV/ç›®æ ‡è¡¨ä¿ç•™å³å¯ç»­è·‘ï¼‰ï¼›è®°å¾—å¤„ç† offsetã€‚
3. **ReplacingMergeTree ä¸€è‡´æ€§**ï¼šåˆå¹¶æœŸå»é‡ï¼›å…³é”®æŠ¥è¡¨æŒ‰ä¸»é”®è¿‡æ»¤ + `FINAL` å…œåº•ï¼Œé¿å…å¸¸æ€ `FINAL`ã€‚
4. **TTL/åˆ†å±‚**ï¼šç­–ç•¥å/å·åä¸æœåŠ¡å™¨ `storage_configuration` ä¿æŒä¸€è‡´ï¼›ç­–ç•¥å˜æ›´å `ALTER TABLE ... MATERIALIZE TTL`ï¼Œå¹¶è§‚æµ‹ `system.parts` çš„ `disk_name`ã€‚

---

### 11) ä¸ Elasticsearch / Pinot çš„é€‰å‹å¯¹æ¯” ğŸ§­

| ç»´åº¦ | ClickHouse | Elasticsearch | Apache Pinot |
| --- | --- | --- | --- |
| å®æ—¶èšåˆ | åˆ—å­˜ + **å¢é‡ MV**ï¼Œè¶…å¼ºæ‰«æ/èšåˆ | èšåˆå¯ç”¨ä½†åæœç´¢åœºæ™¯ | Kafka å®æ—¶ Upsert/Partial Upsertã€åœ¨çº¿èšåˆä½å»¶è¿Ÿ |
| å»é‡/æ›´æ–° | Replacing/Collapsingã€æ’å…¥å¹‚ç­‰ | æ–‡æ¡£è¦†ç›–æ›´æ–°ã€ILM | Upsert ä¸€ç­‰å…¬æ°‘ |
| åˆ†å±‚/ç•™å­˜ | TTL + Storage Policy | ILMï¼šHot/Warm/Cold | å†å²å½’æ¡£éœ€é¢å¤–è®¾è®¡ |
| æŸ¥è¯¢ç”Ÿæ€ | æ ‡å‡† SQL | DSL/SQLã€å…¨æ–‡å¼º | SQL æ–¹è¨€ï¼Œé¢å‘çœ‹æ¿/æŒ‡æ ‡å¼º |

> ç²—ç»“è®ºï¼š**æ•°å€¼èšåˆä¸ºä¸»ã€SQL å¤æ‚** â†’ ClickHouseï¼›**å¼º UGC æœç´¢/ç›¸å…³æ€§** â†’ Elasticsearchï¼›**æä½å»¶è¿Ÿ + Kafka ä¸€ä½“åŒ–** â†’ Pinotã€‚

---

### 12) å»é‡è¯­ä¹‰ä¸€å›¾æ‡‚ ğŸ§ 

åå°åˆå¹¶(merge)






æŸ¥è¯¢(æ—  FINAL)  
å¯èƒ½å«æ—§ç‰ˆæœ¬






æŸ¥è¯¢(FINAL)  
è¯»æ—¶å»é‡










Inserted






Merged






QueryResult






QueryFinal

---

### 13) å¯å¤ç°ç›®å½•ç»“æ„ ğŸ“

```
abp-clickhouse-rt-olap/
  modules/Abp.Analytics.Pipeline/        # ABP æ¨¡å— & ClickHouseWriteWorkerï¼ˆç›´è¿å¾®æ‰¹ï¼‰
  deploy/clickhouse/
    init.sql                              # å»ºè¡¨/MV/TTL/ç­–ç•¥ SQL
    config.d/storage.xml                  # å­˜å‚¨ç­–ç•¥ (ç¤ºä¾‹)
  scripts/bench/
    gen-events.cs                         # æ•°æ®ç”Ÿæˆ/å‹æµ‹
    queries.sql                           # p50/p95/p99 æŸ¥è¯¢
  dashboards/
    grafana.json                          # çœ‹æ¿ (å¯é€‰)
  docs/
    decision-matrix.md                    # é€‰å‹ä¸ç»“è®º

```

---

### 14) å¿«é€Ÿæ ¡éªŒ âœ…

**å»ºè¡¨ + MVï¼ˆç®€ç‰ˆï¼‰**

```sql
CREATE TABLE IF NOT EXISTS rt.events_raw
(
  tenant LowCardinality(String),
  event_key String,
  ts DateTime64(3,'UTC'),
  value Float64,
  ver   UInt64,
  props String
) ENGINE=ReplacingMergeTree(ver)
ORDER BY (tenant,event_key,ts)
PARTITION BY toYYYYMM(ts);

CREATE TABLE IF NOT EXISTS rt.events_hourly
(
  tenant LowCardinality(String),
  bucket DateTime('UTC'),
  c   AggregateFunction(count),
  s   AggregateFunction(sum, Float64),
  p95 AggregateFunction(quantileTDigest, Float64),
  p99 AggregateFunction(quantileTDigest, Float64)
) ENGINE=AggregatingMergeTree()
ORDER BY (tenant,bucket)
PARTITION BY toYYYYMM(bucket);

CREATE MATERIALIZED VIEW IF NOT EXISTS rt.mv_hour
TO rt.events_hourly AS
SELECT tenant, toStartOfHour(ts) AS bucket,
       countState() AS c, sumState(value) AS s,
       quantileTDigestState(0.95)(value) AS p95,
       quantileTDigestState(0.99)(value) AS p99
FROM rt.events_raw
GROUP BY tenant, bucket;

-- ç¤ºä¾‹æŸ¥è¯¢
SELECT tenant, bucket,
       countMerge(c) AS cnt,
       sumMerge(s)   AS total,
       quantileTDigestMerge(0.95)(p95) AS p95,
       quantileTDigestMerge(0.99)(p99) AS p99
FROM rt.events_hourly
WHERE tenant='t1'
  AND bucket >= now() - INTERVAL 24 HOUR
GROUP BY tenant, bucket
ORDER BY bucket;

```

---



