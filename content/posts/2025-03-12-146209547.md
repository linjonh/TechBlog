---
layout: post
title: "AGI大模型2GPTGenerative-Pre-trained-Transformer"
date: 2025-03-12 17:42:47 +0800
description: "Transformer 是一种深度学习架构，用于处理**自然语言处理（NLP）**任务，如机器翻译、文本生成和语义理解。Pre-trained（预训练）指的是在深度学习中，模型在一个大规模数据集上进行的初步训练，以便获得对语言或任务的基础理解。你可以简单理解为它是⼀个⿊盒⼦，当我们在做⽂本翻译任务是，我输⼊进去⼀个中⽂，经过这个⿊盒⼦之后，输出来翻译过后的英⽂。GPT中的“生成式”指的是该模型能够根据输入自动生成文本内容，而不仅仅是从已有的文本库中检索答案。Transformer 简单⼀些的解释。"
keywords: "AGI大模型（2）：GPT：Generative Pre-trained Transformer"
categories: ['Agi']
tags: ['Transformer', 'Gpt', 'Agi']
artid: "146209547"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146209547
    alt: "AGI大模型2GPTGenerative-Pre-trained-Transformer"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146209547
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146209547
cover: https://bing.ee123.net/img/rand?artid=146209547
image: https://bing.ee123.net/img/rand?artid=146209547
img: https://bing.ee123.net/img/rand?artid=146209547
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     AGI大模型（2）：GPT：Generative Pre-trained Transformer
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h4>
     1 Generative Pre-trained Transformer
    </h4>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      1.1 Generative生成式
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     GPT中的“生成式”指的是该模型能够根据输入自动生成文本内容，而不仅仅是从已有的文本库中检索答案。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     具体来说：
    </p>
    <ul>
     <li style="text-align:justify">
      生成（Generative）：GPT是一个生成式AI模型，能够根据给定的提示（Prompt）动态生成连贯、符合语境的文本，而不是简单地匹配已有内容。
     </li>
     <li style="text-align:justify">
      与检索式（Retrieval-based）不同：检索式AI通常依赖于数据库或预设答案，而生成式AI可以创造新文本，适应不同的语境和需求。
     </li>
     <li style="text-align:justify">
      基于概率预测：GPT基于深度学习和概率模型，每次生成文本时，都会根据训练数据预测下一个最可能出现的单词，从而形成流畅的表达。
     </li>
    </ul>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     简单来说，“生成式”就是让AI像人一样创造内容，而不是单纯复制已有的内容。
    </p>
    <p>
    </p>
    <p>
     <strong>
      1.2 Pre-trained（预训练）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Pre-trained（预训练）指的是在深度学习中，模型在一个大规模数据集上进行的初步训练，以便获得对语言或任务的基础理解。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     在GPT（Generative Pre-trained Transformer）中，预训练的过程如下：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     （1）大规模数据训练：
    </p>
    <ul>
     <li style="text-align:justify">
      先在海量文本数据（如书籍、文章、网页）上进行训练，学习词汇、语法、句子结构、语义等知识。
     </li>
     <li style="text-align:justify">
      这个阶段不针对特定任务，而是让模型掌握通用语言能力。
     </li>
    </ul>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     （2）自回归语言建模：
    </p>
    <ul>
     <li style="text-align:justify">
      GPT使用自回归（Autoregressive）方法，根据上下文预测下一个单词，逐步生成文本。
     </li>
     <li style="text-align:justify">
      例如输入“今天的天气”，模型可能预测出“很好”或“阴天”等合理的词。
     </li>
    </ul>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     （3）微调（Fine-tuning，可选）：
    </p>
    <ul>
     <li style="text-align:justify">
      预训练完成后，可以在特定任务（如聊天、代码生成、医学诊断）上进行微调，让模型更适应具体应用。
     </li>
    </ul>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     为什么要预训练？
    </p>
    <ul>
     <li style="text-align:justify">
      节省计算资源：不必从零训练，可以用预训练好的模型进行微调。
     </li>
     <li style="text-align:justify">
      提高泛化能力：模型在大量数据上学到的语言知识，可以泛化到不同任务。
     </li>
     <li style="text-align:justify">
      更快适应特定任务：预训练模型可以快速适应客服、写作、编程等应用。
     </li>
    </ul>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     简单理解：预训练就像让AI先“读万卷书”，然后再针对不同任务“精雕细琢”！
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <p style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     <strong>
      1.3 Transformer变换模型
     </strong>
    </p>
    <p style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     Transformer 是一种深度学习架构，用于处理**自然语言处理（NLP）**任务，如机器翻译、文本生成和语义理解。它由 Google 在 2017 年提出，彻底改变了 NLP 领域。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      Transformer 简单⼀些的解释
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     你可以简单理解为它是⼀个⿊盒⼦，当我们在做⽂本翻译任务是，我输⼊进去⼀个中⽂，经过这个⿊盒⼦之后，输出来翻译过后的英⽂。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="564" src="https://i-blog.csdnimg.cn/direct/500d4654065447f3bc8fc65f514a1c5e.png" width="799"/>
    </p>
    <p style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     <strong>
      Transformer 的核心概念
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Transformer 主要由以下核心组件组成：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （1）自注意力机制（Self-Attention）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      作用：
     </strong>
     让模型关注句子中所有单词之间的关系，而不仅仅是相邻的词。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      优势：
     </strong>
     可以处理长距离依赖，即理解句子中相隔很远的单词之间的联系。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      示例：
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     句子："The cat, which was very fluffy, sat on the mat."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     传统方法 可能只关注相邻的词，比如 "sat" 只和 "on" 相关。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     Transformer 可以让 "cat" 和 "fluffy" 之间建立联系，即使它们相隔较远。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （2）多头注意力（Multi-Head Attention）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      作用：
     </strong>
     增强模型的表达能力，让它同时关注不同层面的信息。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      优势：
     </strong>
     可以关注不同的上下文，例如一个头关注主语，另一个关注动词。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （3）前馈神经网络（Feed-Forward Network, FFN）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      作用：
     </strong>
     对每个单词进行非线性变换，提高模型的表示能力。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      优势：
     </strong>
     增强模型的表达能力，使其更具泛化性。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （4）位置编码（Positional Encoding）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      作用：
     </strong>
     因为 Transformer 没有循环结构（不像 RNN），所以需要额外加位置编码，让模型知道单词在句子中的顺序。
    </p>
    <p>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      注意力机制的核心思想
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （1）计算每个词的重要性
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     注意力机制的关键是计算输入序列中每个单词对当前目标的相关性。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     例子：
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     句子："The cat sat on the mat because it was warm."
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     "it" 可能指代 "mat"（垫子），模型需要重点关注 "mat" 而不是 "cat"。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （2）通过权重调整关注度
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     对于每个输入词，模型计算一个注意力分数（权重）。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     权重越高，说明这个词对当前任务越重要。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     最终的输出是所有词的加权平均，但重要的词占更大比例。
    </p>
    <p>
    </p>
    <h4 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     2 ChatGPT 的技术原理
    </h4>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ChatGPT背后的GPT模型是在⼀个超⼤语料基础上预训练出的⼤语⾔模型，采⽤从左到右进⾏填字概率预测的⾃回归语⾔模型，并基于prompt来适应不同领域的任务。
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     再简单⼀些讲：GPT 模型使⽤ Google ⼏年前推出的 Transformer 架构 来预测下⼀个单词的概率分布，通过训练在⼤型⽂本语料库上学习到的 语⾔模式来⽣成⾃然语⾔⽂本
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
    </p>
    <h4 style="margin-left:0.0001pt; margin-right:0px; text-align:justify">
     3 大模型训练
    </h4>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <img alt="" height="502" src="https://i-blog.csdnimg.cn/direct/a29c822c0e10448eaed6d294953ef407.png" width="756"/>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     ⼤模型的训练整体上分为三个阶段：预训练、SFT（监督微调）以及RLHF（基于⼈类反馈的强化学习）
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （1）预训练（Pre-training）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     预训练的过程类似于从婴⼉成⻓为中学⽣的阶段，在这个阶段我们会学习各种各样的知识，我们的语⾔习惯、知识体系等重要部分都会形成；对于⼤模型来说，在这个阶段它会学习各种不同种类的语料，学习到语⾔的统计规律和⼀般知识 ⼤模型的训练
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （2）监督微调（SFT，Supervised Fine Tuning）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     SFT的过程类似于从中学⽣成⻓为⼤学⽣的阶段，在这个阶段我们会学习到专业知识，⽐如⾦融、法律等领域，我们的头脑会更专注于特定领域。对于⼤模型来说，在这个阶段它可以学习各种⼈类的对话语料，甚⾄是⾮常专业的垂直领域知识，在监督微调过程之后，它可以按照⼈类的意图去回答专业领域的问题
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     <strong>
      （3）基于⼈类反馈的强化学习（RLHF，Reinforcement Learning from Human Feedback）
     </strong>
    </p>
    <p style="margin-left:.0001pt; margin-right:0; text-align:justify">
     RLHF的过程类似于从⼤学⽣步⼊职场的阶段，在这个阶段我们会开始进⾏⼯作，但是我们的⼯作可能会受到领导和客户的表扬，也有可能会受到批评，我们会根据反馈调整⾃⼰的⼯作⽅法，争取在职场获得更多的正⾯反馈。对于⼤模型来说，在这个阶段它会针对同⼀问题进⾏多次回答，⼈类会对这些回答打分，⼤模型会在此阶段学习到如何输出分数最⾼的回答，使得回答更符合⼈类的偏好。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f753031333933383537382f:61727469636c652f64657461696c732f313436323039353437" class_="artid" style="display:none">
 </p>
</div>


