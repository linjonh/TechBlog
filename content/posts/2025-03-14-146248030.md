---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f7a675f686f7665722f:61727469636c652f64657461696c732f313436323438303330"
layout: post
title: "FastGPT原理分析-数据集创建第一步"
date: 2025-03-14 08:21:06 +08:00
description: "本文介绍了FastGpt创建数据集的总体流程，并详细分析了第一步的实现步骤和实现原理。可以看到，这一步只是把数据放到了Mongodb的训练队列的表中，那么，当数据插入到Mongodb后，该如何处理这些数据？而处理这些数据的任务又是如何触发的呢？这些实现的原理和逻辑，在下一篇文章中进行分析。"
keywords: "FastGPT原理分析-数据集创建第一步"
categories: ['大模型Llm']
tags: ['语言模型', '大模型', '人工智能', 'Llm']
artid: "146248030"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146248030
    alt: "FastGPT原理分析-数据集创建第一步"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146248030
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146248030
cover: https://bing.ee123.net/img/rand?artid=146248030
image: https://bing.ee123.net/img/rand?artid=146248030
img: https://bing.ee123.net/img/rand?artid=146248030
---

# FastGPT原理分析-数据集创建第一步

#### 概述

FastGPT的文件上传过程分为两个阶段：第一个阶段：是文件上传。第二个阶段：是对文件进行向量化处理和QA化处理。本文介绍文件上传的总体流程，并对创建数据集的第一步的详细实现逻辑进行分析。

#### 数据集创建总体流程

数据集创建分为两个步骤：

1. 第一步：文件上传和预处理，插入记录到mongodb的训练队列dataset\_trainings表中。
2. 第二步：监控mongodb的插入操作，并启动数据处理：(1)嵌入向量的计算或(2)QA文本拆分。

```
// 创建数据集并插入mongodb的数据表中
createCollectionAndInsertData
   -> pushDataListToTrainingQueue // 将数据集数据推送到训练队列

// 注册处理函数，并自动处理数据
startMongoWatch
   -> createDatasetTrainingMongoWatch // 监控mongodb的插入操作，触发文本处理任务
      -> generateQA(); # QA问答对的处理
      -> generateVector();  # 嵌入向量处理

```

#### 数据集创建第一步的实现

数据集创建的第一步就是上传文件并对数据进行预处理，然后获取相关数据处理的参数，并把参数保存到mongodb的训练队列中。后续的处理任务，会根据数据集的处理配置来调用相应的模型或服务进行处理。数据集处理第一步的主要逻辑如下：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0a798dfcc2044c069b97041b62ec1a9e.png#pic_center)

#### 数据创建第一步实现逻辑

创建数据集是在createCollectionAndInsertData函数中进行处理的，该函数又主要调用了pushDataListToTrainingQueue，该函数的实现逻辑如下：

1. **模型验证与配置**
   ：

* 验证参数中的推理大模型和嵌入向量大模型是否有效。
* 根据训练模式（chunk/qa/auto）设置最大token数和权重
* 如果模型无效或训练模式无效，会抛出错误

2. **数据预处理**
   ：

* 使用
  `simpleText`
  函数清理问题和答案中的空白字符
* 处理索引数据，同样进行文本清理
* 过滤掉空问题、重复内容和超过最大token限制的数据
* 将数据分类为成功、过长、重复和错误四类

3. **数据插入**

* 使用批量插入的方式将数据插入到MongoDB中，每批200条
* 如果批量插入失败，会记录失败的文档并尝试单独插入
* 支持在传入的session中执行，如果没有传入session则创建新的事务

这个函数主要用于将数据集数据预处理后插入到训练队列中，确保数据的有效性和完整性，同时支持事务处理以保证数据一致性。

#### 数据创建第一步主要实现源码分析

```
export async function pushDataListToTrainingQueue({

  // 验证模型配置：检查agentModel和vectorModel的有效性，设置最大token数和权重。
  const { model, maxToken, weight } = await (async () => {
    const agentModelData = getLLMModel(agentModel);
    if (!agentModelData) {
      return Promise.reject(`File model ${agentModel} is inValid`);
    }
    const vectorModelData = getVectorModel(vectorModel);
    if (!vectorModelData) {
      return Promise.reject(`Vector model ${vectorModel} is inValid`);
    }
    

```

* 设置最大token数和权重

```typescript
    if (trainingMode === TrainingModeEnum.chunk) {
      return {
        maxToken: vectorModelData.maxToken * 1.5,
        model: vectorModelData.model,
        weight: vectorModelData.weight
      };
    }

    if (trainingMode === TrainingModeEnum.qa || trainingMode === TrainingModeEnum.auto) {
      return {
        maxToken: agentModelData.maxContext * 0.8,
        model: agentModelData.model,
        weight: 0
      };
    }

```

* 过滤过长或重复的内容

```typescript
  // 过滤重复和过长的内容
  // filter repeat or equal content
  const set = new Set();
  const filterResult: Record<string, PushDatasetDataChunkProps[]> = {
    success: [],
    overToken: [],
    repeat: [],
    error: []
  };

```

* 处理QA问答对的，并去掉空的字符

```typescript
  // format q and a, remove empty char
  data.forEach((item) => {
    item.q = simpleText(item.q);
    item.a = simpleText(item.a);


```

* 批量插入mongodb的dataset\_trainings表中

```typescript
  // 使用 insertMany 批量插入
  // 数据插入：批量插入数据（每批200条）
  const batchSize = 200;
  const insertData = async (startIndex: number, session: ClientSession) => {
    const list = filterResult.success.slice(startIndex, startIndex + batchSize);

    if (list.length === 0) return;

    try {
      await MongoDatasetTraining.insertMany(
        list.map((item) => ({

```

#### 总结

本文介绍了FastGpt创建数据集的总体流程，并详细分析了第一步的实现步骤和实现原理。可以看到，这一步只是把数据放到了Mongodb的训练队列的表中，那么，当数据插入到Mongodb后，该如何处理这些数据？而处理这些数据的任务又是如何触发的呢？这些实现的原理和逻辑，在下一篇文章中进行分析。