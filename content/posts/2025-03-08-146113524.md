---
layout: post
title: "pytorch-retain_grad-vs-requires_grad"
date: 2025-03-08 11:48:39 +0800
description: "requires_grad大家都挺熟悉的，因此穿插在retain_grad的例子里进行捎带讲解就行。属性并不会在反向传播的过程中被自动保存下来（这是为了节省内存，毕竟我们只需要计算那些手动设置。的张量的梯度，并进行梯度更新，对吧？警告的大致意思是：访问了非叶子节点的。的梯度也在反向传播以后被正确保存了！因此，我们只需要添加一行代码。可以看到，现在非叶子节点。属性，但非叶子节点的。"
keywords: "pytorch retain_grad vs requires_grad"
categories: ['未分类']
tags: ['人工智能', 'Pytorch', 'Python']
artid: "146113524"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146113524
    alt: "pytorch-retain_grad-vs-requires_grad"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146113524
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146113524
cover: https://bing.ee123.net/img/rand?artid=146113524
image: https://bing.ee123.net/img/rand?artid=146113524
img: https://bing.ee123.net/img/rand?artid=146113524
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     pytorch retain_grad vs requires_grad
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     requires_grad大家都挺熟悉的，因此穿插在retain_grad的例子里进行捎带讲解就行。下面看一个代码片段：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token comment"># 创建一个标量 tensor，并开启梯度计算</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 中间计算：y 依赖于 x，是非叶子节点</span>
y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">3</span>

<span class="token comment"># 继续计算，得到 z</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token number">4</span>

<span class="token comment"># 反向传播</span>
z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 查看梯度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x.grad:"</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y.grad:"</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
</code></pre>
    <p>
     输出结果为：
    </p>
    <pre><code class="prism language-python">x<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">12.</span><span class="token punctuation">)</span>
y<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> <span class="token boolean">None</span>
<span class="token operator">/</span>tmp<span class="token operator">/</span>ipykernel_219007<span class="token operator">/</span><span class="token number">1060175670</span><span class="token punctuation">.</span>py<span class="token punctuation">:</span><span class="token number">17</span><span class="token punctuation">:</span> UserWarning<span class="token punctuation">:</span> The <span class="token punctuation">.</span>grad attribute of a Tensor that <span class="token keyword">is</span> <span class="token keyword">not</span> a leaf Tensor <span class="token keyword">is</span> being accessed<span class="token punctuation">.</span> Its <span class="token punctuation">.</span>grad attribute won't be populated during autograd<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span> If you indeed want the <span class="token punctuation">.</span>grad field to be populated <span class="token keyword">for</span> a non<span class="token operator">-</span>leaf Tensor<span class="token punctuation">,</span> use <span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> on the non<span class="token operator">-</span>leaf Tensor<span class="token punctuation">.</span> If you access the non<span class="token operator">-</span>leaf Tensor by mistake<span class="token punctuation">,</span> make sure you access the leaf Tensor instead<span class="token punctuation">.</span> See github<span class="token punctuation">.</span>com<span class="token operator">/</span>pytorch<span class="token operator">/</span>pytorch<span class="token operator">/</span>pull<span class="token operator">/</span><span class="token number">30531</span> <span class="token keyword">for</span> more informations<span class="token punctuation">.</span> <span class="token punctuation">(</span>Triggered internally at aten<span class="token operator">/</span>src<span class="token operator">/</span>ATen<span class="token operator">/</span>core<span class="token operator">/</span>TensorBody<span class="token punctuation">.</span>h<span class="token punctuation">:</span><span class="token number">489.</span><span class="token punctuation">)</span>
  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y.grad:"</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre>
    <p>
     警告的大致意思是：访问了非叶子节点的
     <code>
      .grad
     </code>
     属性，但非叶子节点的
     <code>
      .grad
     </code>
     属性并不会在反向传播的过程中被自动保存下来（这是为了节省内存，毕竟我们只需要计算那些手动设置
     <code>
      .requires_grad
     </code>
     为
     <code>
      True
     </code>
     的张量的梯度，并进行梯度更新，对吧？）
    </p>
    <p>
     因此，我们只需要添加一行代码
     <code>
      y.retain_grad()
     </code>
     ，修改后的代码如下：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch

<span class="token comment"># 创建一个标量 tensor，并开启梯度计算</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 中间计算：y 依赖于 x，是非叶子节点</span>
y <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token number">3</span>
y<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 继续计算，得到 z</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token number">4</span>

<span class="token comment"># 反向传播</span>
z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 查看梯度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x.grad:"</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y.grad:"</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>  
</code></pre>
    <p>
     输出结果为：
    </p>
    <pre><code class="prism language-python">x<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">12.</span><span class="token punctuation">)</span>
y<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">4.</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     可以看到，现在非叶子节点
     <code>
      y
     </code>
     的梯度也在反向传播以后被正确保存了！
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34353831323232302f:61727469636c652f64657461696c732f313436313133353234" class_="artid" style="display:none">
 </p>
</div>


