---
layout: post
title: 2024-年-8-个顶级开源-LLM大语言模型
date: 2024-09-21 18:12:14 +08:00
categories: ['Ai']
tags: ['语言模型', '开源', '人工智能']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=135084958
    alt: 2024-年-8-个顶级开源-LLM大语言模型
artid: 135084958
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=135084958
featuredImagePreview: https://bing.ee123.net/img/rand?artid=135084958
---

# 2024 年 8 个顶级开源 LLM（大语言模型）

如果没有所谓的大型语言模型（LLM），当前的生成式人工智能革命就不可能实现。LLM 基于
[transformers](https://www.datacamp.com/tutorial/an-introduction-to-using-transformers-and-hugging-face "transformers")
（一种强大的神经架构）是用于建模和处理人类语言的 AI 系统。它们之所以被称为“大”，是因为它们有数亿甚至数十亿个参数，这些参数是使用大量文本数据语料库预先训练的。

LLM 是流行且广泛使用的聊天机器人（如
[ChatGPT](https://www.datacamp.com/blog/a-chat-with-chatgpt-on-the-method-behind-the-bot "ChatGPT")
和
[Google Bard](https://www.datacamp.com/blog/bard-vs-chatgpt-for-data-science "Google Bard")
）
[的基础模型](https://www.datacamp.com/blog/what-are-foundation-models "的基础模型")
。特别是，ChatGPT 由 OpenAI 开发和拥有的 LLM
[GPT-4](https://www.datacamp.com/blog/what-we-know-gpt4 "GPT-4")
提供支持，而 Google Bard 则基于 Google 的 PaLM 2 模型。

ChatGPT 和 Bard 以及许多其他流行的聊天机器人都有一个共同点，即它们的基础 LLM 是专有的。这意味着它们归公司所有，只有在购买许可证后才能由客户使用。该许可证附带权利，但也对如何使用LLM进行了可能的限制，以及有关该技术背后机制的有限信息。

然而，LLM 领域的一个平行运动正在迅速加快步伐：开源 LLM。随着人们对主要由 Microsoft、Google 和 Meta 等大型科技公司控制的专有 LLM 缺乏透明度和有限可访问性的担忧日益加剧，开源 LLM 有望使快速增长的 LMM 和生成式 AI 领域更加可访问、透明和创新。

本文旨在探讨 2023 年可用的顶级开源 LLM。尽管自 ChatGPT 推出和（专有）LLM 普及以来仅一年，但开源社区已经取得了重要的里程碑，有大量开源 LLM 可用于不同目的。继续阅读以查看最受欢迎的！

### 使用开源 LLM 的好处

选择开源 LLM 而不是专有 LLM 有多种短期和长期好处。 下面，您可以找到最令人信服的理由列表：

#### 增强数据安全性和隐私性

使用专有 LLM 的最大问题之一是 LLM 提供商泄露数据或未经授权访问敏感数据的风险。事实上，关于涉嫌将个人和机密数据用于培训目的，已经存在一些争议。

通过使用开源 LLM，公司将全权负责保护个人数据，因为他们将完全控制个人数据。

#### 节省成本，减少对供应商的依赖

大多数专有的 LLM 需要许可证才能使用它们。从长远来看，这可能是一些公司，尤其是中小企业可能无法负担的重要费用。开源 LLM 并非如此，因为它们通常是免费使用的。

但是，需要注意的是，运行 LLM 需要大量资源，即使仅用于推理，这意味着您通常需要为使用云服务或强大的基础设施付费。

#### 代码透明度和语言模型自定义

选择开源 LLM 的公司将可以访问 LLM 的工作原理，包括它们的源代码、架构、训练数据以及训练和推理机制。这种透明度是审查的第一步，也是定制的第一步。

由于每个人都可以访问开源 LLM，包括它们的源代码，因此使用它们的公司可以针对其特定用例对其进行自定义。

#### 积极的社区支持和促进创新

开源运动有望使 LLM 和生成式 AI 技术的使用和访问民主化。允许开发人员检查 LLM 的内部工作是该技术未来发展的关键。通过降低全球编码人员的准入门槛，开源 LLM 可以通过减少偏见、提高准确性和整体性能来促进创新并改进模型。

#### 解决人工智能对环境的影响

随着 LLM 的普及，研究人员和环境监管机构对运行这些技术所需的碳足迹和耗水量提出了担忧。专有的 LLM 很少发布有关培训和运营 LLM 所需资源的信息，也很少发布相关的环境足迹。

通过开源 LLM，研究人员有更多机会了解这些信息，这可以为旨在减少
[AI 环境足迹](https://www.datacamp.com/blog/environmental-impact-data-digital-technology "AI 环境足迹")
的新改进打开大门。

### 2024 年 8 个顶级开源大语言模型

#### 1. LLaMA 2

![骆驼 2](https://i-blog.csdnimg.cn/blog_migrate/c21386d341ca49ba765bc5fc43b5385d.png)

LLM 领域的大多数顶级参与者都选择闭门造车地建立他们的 LLM。但 Meta 正在采取行动成为一个例外。随着其强大的开源大型语言模型 Meta AI （LLaMA） 及其改进版本 （LLaMA 2） 的发布，Meta 正在向市场发出一个重要信号。

[LLaMA 2](https://ai.meta.com/resources/models-and-libraries/llama/ "LLaMA 2")
于 2023 年 7 月实现用于研究和商业用途，是一个预训练的生成文本模型，具有 7 到 700 亿个参数。它已通过
[来自人类反馈的强化学习](https://www.datacamp.com/blog/what-is-reinforcement-learning-from-human-feedback "来自人类反馈的强化学习")
（RLHF） 进行了微调。它是一种生成文本模型，可以用作聊天机器人，可以适应各种自然语言生成任务，包括编程任务。Meta 已经推出了  LLaMA 2, Llama Chat, 和
[Code Llama](https://ai.meta.com/blog/code-llama-large-language-model-coding/ "Code Llama ")
的开放定制版本。

#### 2. BLOOM

![图片6.png](https://i-blog.csdnimg.cn/blog_migrate/43c288526aacb4fa82578263cf221c3d.png)

BLOOM 于 2022 年推出，经过与来自 70+ 个国家的志愿者和 Hugging Face 的研究人员为期一年的合作项目，
[BLOOM](https://huggingface.co/bigscience/bloom "BLOOM")
是一个自回归 LLM，经过训练，可以使用工业规模的计算资源在大量文本数据上从提示中连续文本化。

BLOOM 的发布标志着生成式 AI 民主化的一个重要里程碑。BLOOM 拥有 176 亿个参数，是最强大的开源 LLM 之一，能够以 46 种语言和 13 种编程语言提供连贯准确的文本。

透明度是 BLOOM 的支柱，在这个项目中，每个人都可以访问源代码和训练数据，以便运行、研究和改进它。

BLOOM 可以通过 Hugging Face 生态系统免费使用。

#### 3. BERT

![图片5.png](https://i-blog.csdnimg.cn/blog_migrate/cea33fd8b706f58d619badd006797bfd.png)

LLM 的底层技术是一种称为 transformer 的神经架构。它是由谷歌开发人员于 2017 年在论文
[《注意力是你所需要的一切](https://arxiv.org/abs/1706.03762 "《注意力是你所需要的一切")
》中提到的。测试 transformers 潜力的首批实验之一是 BERT。

BERT（Bidirectional Encoder Representations from Transformers）于 2018 年由 Google 作为开源 LLM 推出，在许多自然语言处理任务中迅速实现了最先进的性能。

由于其在 LLM 早期的创新功能及其开源性质，Bert 是最受欢迎和使用最广泛的 LLM 之一。例如，在 2020 年，谷歌宣布已通过 70 多种语言的 Google 搜索采用了 Bert。

目前有数以千计的开源、免费和预训练的 Bert 模型可用于特定用例，例如情感分析、临床笔记分析和有害评论检测。

#### 4. Falcon 180B

![图片2.png](https://i-blog.csdnimg.cn/blog_migrate/f397b5f85c237514f19915118ee29e13.png)

如果说
[Falcon 40B](https://www.datacamp.com/tutorial/introduction-to-falcon-40b " Falcon 40B")
已经给开源 LLM 社区留下了深刻的印象（它在 Hugging Face 的开源大型语言模型排行榜上排名 #1），那么新的
[Falcon 180B](https://falconllm.tii.ae/falcon-models.html "Falcon 180B")
表明专有和开源 LLM 之间的差距正在迅速缩小。

Falcon 180B 由阿拉伯技术创新研究所于 2023 年 9 月发布，可以接受 1800 亿个参数和 3.5 万亿个 Token。凭借这种令人印象深刻的计算能力， Falcon 180B 在各种 NLP 任务中已经超过了 LLaMA 3 和 GPT-5.2，而 Hugging Face 表明它可以与谷歌的 PaLM 2 相媲美，后者是为
[Google Bard](https://www.datacamp.com/blog/google-bard-for-data-science-projects "Google Bard")
提供支持的 LLM。

虽然免费用于商业和研究用途，但重要的是要注意 Falcon 180B 需要珍贵的计算资源才能运行。

#### 5. OPT-175B

![图片9.png](https://i-blog.csdnimg.cn/blog_migrate/02d40b24d7f2e3df531ef2239902b77b.png)

2022 年发布的
[Open Pre-trained Transformers](https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/ "Open Pre-trained Transformers 语言模型")
（OPT）语言模型标志着 Meta 通过开源解放 LLM 竞赛战略的又一个重要里程碑。

OPT 包括一套仅解码器的预训练转换器，参数范围从 125M 到 175B。OPT-175B 是市场上最先进的开源 LLM 之一，是 GPT 最强大的兄弟，性能与 GPT-3 相似。预训练模型和源代码都向公众开放。

然而，如果你正在考虑开发一家具有 LLM 的人工智能驱动型公司，你最好考虑另外的模型，因为OPT-175B 是在非商业许可下发布的，只允许将该模型用于研究。

#### 6. XGen-7B

![图片8.png](https://i-blog.csdnimg.cn/blog_migrate/b26fdcde0ba63238fc09da30f284a4f8.png)

越来越多的公司正在加入LLM竞赛。最后加入擂台的是 Salesforce，该公司于 2023年 7 月推出了
[XGen-7B](https://blog.salesforceairesearch.com/xgen/ "XGen-7B")
LLM。

根据作者的说法，大多数开源 LLM 专注于提供信息有限的大答案（即几乎没有上下文的简短提示）。XGen-7B 背后的想法是构建一个支持更长上下文窗口的工具。特别是，XGen （XGen-7B-8K-base） 的最高级方差允许 8K 上下文窗口，即输入和输出文本的累积大小。

效率是 XGen 的另一个重要优先事项，它只使用 7B 参数进行训练，远低于大多数强大的开源 LLM，如 LLaMA 2 或 Falcon。

尽管体积相对较小，但 XGen 仍然可以提供出色的效果。该模型可用于商业和研究目的，但 XGen-7B-{4K，8K}-inst 变体除外，该变体已在教学数据和 RLHF上进行了训练，并在非商业许可下发布。

#### 7. GPT-NeoX 和 GPT-NeoX

![图片1.png](https://i-blog.csdnimg.cn/blog_migrate/677fcc98ae9b90fe7acdc6f3011acbc6.png)

GPT-NeoX 和 GPT-J 由非营利性 AI 研究实验室
[EleutherAI](https://www.eleuther.ai/ "EleutherAI")
的研究人员开发，是 GPT 的两个很好的开源替代品。

GPT-NeoX 有 20 亿个参数，而 GPT-J 有 6 亿个参数。尽管大多数高级 LLM 可以使用超过 100 亿个参数进行训练，但这两个 LLM 仍然可以提供高精度的结果。

他们已经接受了来自不同来源的 22 个高质量数据集的训练，这些数据集使它们能够在多个领域和许多用例中使用。与 GPT-3 相比，GPT-NeoX 和 GPT-J 尚未使用 RLHF 进行训练。

任何自然语言处理任务都可以使用 GPT-NeoX 和 GPT-J 执行，从文本生成和情感分析到研究和营销活动开发。

这两个 LLM 都可以通过
[NLP Cloud API](https://nlpcloud.com/ " NLP Cloud API")
免费获得。

#### 8. Vicuna 13-B

![图片3.jpg](https://i-blog.csdnimg.cn/blog_migrate/0b379836c3a42b2ec0304e53798113cb.jpeg)

[Vicuna-13B](https://www.datacamp.com/tutorial/vicuna-13b-tutorial "Vicuna-13B")
是一个开源对话模型，通过使用从
[ShareGPT](https://sharegpt.com/ "ShareGPT")
收集的用户共享对话对 LLaMa 13B 模型进行微调而训练而来。

作为一款智能聊天机器人，Vicuna-13B 的应用不胜枚举，下面在客户服务、医疗、教育、金融、旅游/酒店等不同行业进行说明。

使用 GPT-4 作为评委的初步评估显示，Vicuna-13B 达到了 ChatGPT 和 Google Bard 的 90% 以上质量，然后在超过 90% 的情况下优于 LLaMa 和 Alpaca 等其他模型。

### 选择适合您需求的开源 LLM

开源 LLM 空间正在迅速扩大。如今，开源 LLM 比私有 LLM 多得多，随着全球开发人员合作升级当前的 LLM 并设计更优化的 LLM，性能差距可能很快就会弥合。

在这个充满活力和令人兴奋的背景下，可能很难为您的目的选择合适的开源 LLM。以下是在选择一个特定的开源 LLM 之前您应该考虑的一些因素的列表：

* **您要做什么？**
  这是你要问自己的第一件事。开源 LLM 始终是开放的，但其中一些仅出于研究目的而发布。因此，如果您打算创办一家公司，请注意可能的许可限制。
* **为什么需要大语言模型？**
  这一点也非常重要。LLM 目前很流行。每个人都在谈论他们和他们无穷无尽的机会。但是，如果你可以在不需要 LLM 的情况下构建你的想法，那么就不要使用它们。这不是强制性的（您可能会节省很多钱并防止进一步使用资源）。
* **您需要多大的精度？**
  这是一个重要的方面。最先进的 LLM 的大小和准确性之间存在直接关系。这意味着，总的来说，LLM 在参数和训练数据方面越大，模型就越准确。因此，如果您需要高精度，您应该选择更大的 LLM，例如 LLaMA 或 Falcon。
* **你想投资多少钱？**
  这与上一个问题密切相关。模型越大，训练和操作模型所需的资源就越多。这意味着要使用额外的基础设施或云提供商的更高账单，以防您想在云中操作 LLM。LLM 是强大的工具，但它们需要大量资源才能使用它们，即使是开源的。
* **你能用预训练的模型实现你的目标吗？**
  如果你可以简单地使用预先训练的模型，为什么还要投入金钱和精力从头开始训练你的 LLM？有许多版本的开源 LLM 针对特定用例进行了训练。如果您的想法适合这些用例之一，那就为它而生。

### 结论

开源 LLM 正处于激动人心的运作。随着它们的快速发展，生成式人工智能领域似乎不一定会被有能力构建和使用这些强大工具的大玩家所垄断。

我们列举了 8 个开源 LLM，但这个数字要高得多，而且还在快速增长。