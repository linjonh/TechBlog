---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35313932333939372f:61727469636c652f64657461696c732f313436313631353534"
layout: post
title: "机器学习周报-文献阅读"
date: 2025-03-15 12:02:19 +0800
description: "本周阅读了题目为A Water Quality Prediction Model Based on Long Short-Term Memory Networks and Optimization Algorithms的文章，文章提出了一种基于AWPSO-LSTMAT的预测模型。与之前的SVR、LSTM、CNN-LSTM和CNN-GRU等预测模型相比，通过修改和优化原始算法，AWPSO-LSTMAT的预测效果明显提高。"
keywords: "机器学习周报--文献阅读"
categories: ['未分类']
tags: ['机器学习', '人工智能']
artid: "146161554"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146161554
    alt: "机器学习周报-文献阅读"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146161554
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146161554
cover: https://bing.ee123.net/img/rand?artid=146161554
image: https://bing.ee123.net/img/rand?artid=146161554
img: https://bing.ee123.net/img/rand?artid=146161554
---

# 机器学习周报--文献阅读

### 摘要

本周阅读了题目为A Water Quality Prediction Model Based on Long Short-Term Memory Networks and Optimization Algorithms的文章，文章提出了一种基于AWPSO-LSTMAT的预测模型。与之前的SVR、LSTM、CNN-LSTM和CNN-GRU等预测模型相比，通过修改和优化原始算法，AWPSO-LSTMAT的预测效果明显提高。预测PH值、溶解氧（DO）、氨氮（AN）和总磷（TP）的平均绝对误差（MAE），文章所设计的预测模型比其他现有模型具有更高的预测精度和更强的泛化能力。

### Abstract

This week, I read the article titled “A Water Quality Prediction Model Based on Long Short-Term Memory Networks and Optimization Algorithms,” which proposes a prediction model based on AWPSO-LSTMAT. Compared with previous prediction models such as SVR, LSTM, CNN-LSTM, and CNN-GRU, the prediction performance of AWPSO-LSTMAT is significantly improved through modifications and optimizations of the original algorithms. In terms of predicting pH, dissolved oxygen (DO), ammonia nitrogen (AN), and total phosphorus (TP), the mean absolute error (MAE) of the proposed model is lower than that of other existing models. The designed prediction model demonstrates higher prediction accuracy and stronger generalization ability.

## 1 文章内容

### 1.1 模型结构

#### 1.1.1 LSTMAT的结构设置

水质因子随时间呈现出长周期循环和非线性变化等特征，这使得独立的LSTM模型难以有效地提取趋势特征。虽然单个CNN网络可以有效地提取变化的特征，但它对时间序列数据不敏感，可能无法达到预期的预测结果。因此，本文将CNN的特征提取能力与LSTM对时间序列数据的敏感性相结合。具体来说，在LSTM处理序列信息之前，使用CNN提取数据的主要特征，从而提高预测精度。此外，为了应对评估长时间序列中信息重要性的挑战，在LSTM模型中引入了AT机制，以关注对水质预测更为关键的特征。

LSTMAT模型的结构如下图所示。该模型分为两部分：CNN特征提取和LSTMAT。CNN特征提取部分由2个卷积层和2个最大池化层组成，用于提取水质因子特征，揭示数据关系，去除冗余信息和噪声，从而提高预测精度。LSTMAT部分包括1个重塑层、2个LSTM层、1个注意力层和2个密集层。本节旨在挖掘和学习CNN特征提取后数据的时间特征，通过降维来增强数据特征。此外，CNN特征提取部分包括一个Dropout层，LSTMAT部分包括3个Dropout图层，以减少过拟合。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/93bc9ca67b224ce7b6cf8d18d88a1c3b.png)

#### 1.1.2 AWPSO算法优化模型

模型中某些关键参数的值会影响预测结果的质量。为了优化这些关键参数并减少手动调整的影响，本文引入了AWPSO算法。它将第一层中的LSTM隐藏单元的数量、第二层中的LPTM隐藏单元数量、Dropout层的数量、训练周期的数量和批量大小视为优化变量，以找到全局最优解。在介绍了AWPSO算法后，模型运算的主要过程如下：

1. 数据标准化
     
   将实验数据归一化到[0,1]之间的范围内，以提高模型的收敛速度和精度。归一化公式如下：

   X
   n
   o
   r
   m
   =
   X
   −
   X
   m
   i
   n
   X
   m
   a
   x
   −
   X
   m
   i
   n
   X\_{norm}=\frac{X-X\_{min}}{X\_{max}-X\_{min}}






   X










   n

   or

   m

   ​




   =


















   X










   ma

   x

   ​


   −


   X










   min

   ​













   X

   −


   X










   min

   ​


   ​

   ，其中

   X
   n
   o
   r
   m
   X\_{norm}






   X










   n

   or

   m

   ​

   是归一化数据；X为原始实验数据；

   X
   m
   i
   n
   X\_{min}






   X










   min

   ​

   是原始数据的最小值；

   X
   m
   a
   x
   X\_{max}






   X










   ma

   x

   ​

   是原始数据的最大值。
2. 数据拆分
     
   将预处理后的水质数据按9:1的比例分为训练集和测试集。
3. 参数初始化
     
   初始化AWPSO算法，设置相关参数，定义优化参数。搜索维度为5，优化参数定义为第一层LSTM隐藏单元的数量、第二层LSTM隐单元的数量，Dropout层的数量、训练周期的数量和批量大小，并确定其各自的优化范围
4. 定义适应度函数
     
   定义适应度函数。本文使用MAPE（平均绝对百分比误差）作为粒子的适应度函数。MAPE计算公式：

   M
   A
   P
   E
   =
   100
   %
   n
   ∑
   i
   =
   1
   n
   ∣
   y
   ^
   i
   −
   y
   i
   y
   i
   ∣
   MAPE=\frac{100\%}{n}\sum^n\_{i=1}|\frac{\hat y\_{i}-y\_i}{y\_i}|





   M

   A

   PE



   =

















   n












   100%

   ​





   ∑










   i

   =

   1





   n

   ​




   ∣














   y









   i

   ​





















   y





   ^

   ​











   i

   ​


   −


   y









   i

   ​


   ​


   ∣
   ，其中MAPE是平均绝对百分比误差；

   y
   ^
   i
   \hat y\_i













   y





   ^

   ​










   i

   ​

   是模型的预测值；yi是实际值；n是样本数；i是样本索引。
5. 迭代训练与优化
     
   将分割后的水质数据分别输入预测模型进行迭代训练。在迭代过程中不断搜索和更新最佳的个人和全局位置及其适应度值。
6. 检查终止
     
   检查迭代次数是否已达到终止条件。如果满足终止条件，则将当前优化变量（即最佳参数）输入预测模型进行训练和预测。如果不满足终止条件，则返回步骤4继续更新。
7. 最终预测的逆归一化
     
   对输出结果进行逆归一化处理，得到最终的水质预测值。逆归一化公式如下：

   X
   =
   X
   n
   o
   r
   m
   (
   X
   m
   a
   x
   −
   X
   m
   i
   n
   )
   +
   X
   m
   i
   n
   X=X\_{norm}(X\_{max}-X\_{min})+X\_{min}





   X



   =






   X










   n

   or

   m

   ​


   (


   X










   ma

   x

   ​




   −






   X










   min

   ​


   )



   +






   X










   min

   ​

AIWPSO-LSTMAT算法的流程图如下图所示
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/c685f00c0bde47bcb0de59fddbf73b54.png)

### 1.2 实验与结果讨论

#### 1.2.1 处理缺失数据

由于监测站的定期维护和偶尔的设备故障，一些水质数据可能会丢失。为了确保实验的有效性，必须向预测模型提供完整的数据。该论文
**使用线性插值来处理缺失数据**
。线性插值公式如下：

y
k
=
y
ω
+
(
y
r
−
y
ω
)
k
−
ω
r
−
ω
y\_k=y\_\omega+(y\_r-y\_\omega)\frac{k-\omega}{r-\omega}






y









k

​




=






y









ω

​




+





(


y









r

​




−






y









ω

​


)













r

−

ω












k

−

ω

​

，其中，

k
,
ω
k,\omega





k

,



ω
和

r
r





r
表示时间，

y
k
y\_k






y









k

​

表示时间k处的缺失值；

y
ω
y\_\omega






y









ω

​

表示与

y
k
y\_k






y









k

​

之前的最近时间ω对应的已知数据；

y
r
y\_r






y









r

​

表示与

y
k
y\_k






y









k

​

之后的最近时间r对应的已知数据。

在完成缺失数据后，数据集以9:1的比例分为训练集和测试集。下图显示了四种类型的水质因子数据，并填写了缺失值。下图所示，四种类型的水质因子数据表现出非线性和非平稳性等特征。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d7de3358878245fdbf11d8b294d3e199.png)

#### 1.2.2 模型评估指标

为了合理有效地评估模型的预测性能，论文中使用决定系数（R²）平均绝对误差（MAE）和均方根误差（RMSE）来评估预测结果。

1. 决定系数（R²）
     
   反映模型拟合数据的准确性，范围从[0,1]。接近1的值表示模型的拟合度更好，而接近0的值表示拟合度较差。计算公式如下：
     



   R
   2
   =
   1
   −
   ∑
   i
   =
   1
   n
   (
   y
   ^
   i
   −
   y
   i
   )
   2
   ∑
   i
   =
   1
   n
   (
   y
   ˉ
   i
   −
   y
   i
   )
   2
   R^2=1-\frac{\sum^n\_{i=1}(\hat y\_i-y\_i)^2}{\sum^n\_{i=1}(\bar y\_i-y\_i)^2}






   R









   2



   =





   1



   −


















   ∑










   i

   =

   1





   n

   ​


   (









   y





   ˉ

   ​










   i

   ​


   −


   y









   i

   ​



   )









   2













   ∑










   i

   =

   1





   n

   ​


   (









   y





   ^

   ​










   i

   ​


   −


   y









   i

   ​



   )









   2

   ​

   ，其中，

   y
   ^
   i
   \hat y\_i













   y





   ^

   ​










   i

   ​

   表示预测的目标变量值，

   y
   ˉ
   i
   \bar y\_i













   y





   ˉ

   ​










   i

   ​

   是测量数据的平均值；

   y
   i
   y\_i






   y









   i

   ​

   表示目标变量的真值。
2. MAE
     
   反映预测值和真实值之间的平均绝对偏差。较小的值表示预测更接近真实值，而较大的值表示差异更大。
     



   M
   A
   E
   =
   1
   n
   ∑
   i
   =
   1
   n
   ∣
   y
   ^
   i
   −
   y
   i
   ∣
   MAE=\frac{1}{n}\sum^n\_{i=1}|\hat y\_i-y\_i|





   M

   A

   E



   =

















   n












   1

   ​





   ∑










   i

   =

   1





   n

   ​




   ∣









   y





   ^

   ​










   i

   ​




   −






   y









   i

   ​


   ∣
3. RMSE
     
   测量预测值和真实值之间的偏差。值越接近0表示模型的稳定性越高，而值越大则表示稳定性越差。计算公式如下（参数如上所述）：

   R
   M
   S
   E
   =
   1
   n
   ∑
   i
   =
   1
   n
   (
   y
   ^
   i
   −
   y
   i
   )
   2
   RMSE=\sqrt {\frac{1}{n}\sum^n\_{i=1}(\hat y\_i-y\_i)^2}





   RMSE



   =

























   n












   1

   ​





   ∑










   i

   =

   1





   n

   ​


   (









   y





   ^

   ​










   i

   ​




   −




   y









   i

   ​



   )









   2


   ​

#### 1.2.3 比较实验

下图为不同模型各因素预测效果比较，通过比较所示的各种模型的预测结果，很明显，与其他模型相比，该论文提出的模型实现了更好的曲线拟合，并且在跳跃数据方面表现良好。

虽然SVR在预测跳跃数据时有优势，其在小值水质因子（氨氮、总磷）的预测曲线波动较大，稳定性较差。LSTM很好地拟合了pH值和溶解氧的实际值，但缺乏捕捉变化特征的能力，导致精度较低，对氨氮和总磷的跳跃点拟合不佳。CNN-LSTM模型结合了CNN和LSTM的优点，比LSTM具有更好的拟合效果。然而，由于在最终的隐藏层和输出层中随机分配了权重，它没有强调更重要特征的影响，导致拟合效果不佳。CNN-GRU模型很好地拟合了小值水质因子，波动最小，但在较大值因子（溶解氧）上表现不佳，与所提出的模型相比，拟合精度较低。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6eae772ad4944f81b3e794a9377d50a3.png)

#### 1.2.4 消融实验（ABLATION EXPERIMENTS）

从下图中的可视化可以看出，与所提出的模型相比，
**没有AWPSO算法和AT机制的预测曲线与实际曲线的拟合程度较低。**

**引入AWPSO优化算法可以自适应调整粒子惯性权重，有效地平衡了局部和全局搜索，提高了模型的参数优化能力**
。因此，MAE和RMSE分别下降了23.225%和17.957%，R2上升了6.704%。
**当添加AT机制时，该模型侧重于对水质预测更重要的特征**
，导致MAE和RMSE分别降低了20.134%和17.667%，

R
2
R^2






R









2
增加了6.477%。引入AWPSO算法和AT机制的引入有效地减少了预测误差，提高了模型在水质预测中的性能。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ebe9f0eb4b82411ebd81971c7aa087f7.png)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/947e656e961e4f5ca939ca09e5b3ccca.png)

AWPSO通过自适应调整惯性权重，平衡全局搜索（避免局部最优）与局部搜索（精细调参），显著优化了LSTM层数、Dropout率等参数。
  
AT机制通过动态分配注意力权重，使模型聚焦于对预测更重要的时序特征（如污染事件峰值），提升对突变数据的拟合能力。

## 2相关知识

### 2.1 自适应权重粒子群优化（AWPSO）算法

粒子群优化（PSO）算法是
**常用的一种群智能优化算法**
，在实际应用中具有很大的局限性。它主要模拟鸟群的觅食行为。粒子群算法中的每个粒子都有两个属性：
**速度（velocity），表示粒子的运动速度;位置（position），表示粒子的运动方向。**
每个粒子在搜索空间中独立地搜索最优解，将其自身的最优值视为当前个体最优值，并与整个群共享该个体最优值。通过不断的更新，群体将最优个体值识别为当前的全局最优解。然而，PSO算法具有一定的局限性，例如在搜索的早期阶段容易陷入局部最优，并且在训练迭代期间可能收敛过快。

自适应权重粒子群优化算法（AWPSO）的具体实现步骤：

1. 初始化参数：设置粒子群大小（种群规模）、最大迭代次数、惯性权重范围、学习因子等参数。
2. 种群初始化：随机生成初始位置和速度的粒子群。
3. 适应度计算：计算每个粒子的适应度值，用于评估粒子的优劣。
4. 更新个体最优和全局最优：比较当前粒子的位置与pbest，更新pbest；在所有粒子的pbest中找出最优的作为gbest。
5. 速度更新：根据公式

   v
   i
   (
   t
   +
   1
   )
   =
   w
   ⋅
   v
   i
   (
   t
   )
   +
   c
   1
   ⋅
   r
   1
   ⋅
   (
   p
   b
   e
   s
   t
   i
   −
   x
   i
   (
   t
   )
   )
   +
   c
   2
   ⋅
   r
   2
   ⋅
   (
   g
   b
   e
   s
   t
   −
   x
   i
   (
   t
   )
   )
   v\_i(t+1)=w·v\_i(t)+c\_1·r\_1·(pbest\_i-x\_i(t))+c\_2·r\_2·(gbest-x\_i(t))






   v









   i

   ​


   (

   t



   +





   1

   )



   =





   w

   ⋅




   v









   i

   ​


   (

   t

   )



   +






   c









   1

   ​


   ⋅




   r









   1

   ​


   ⋅



   (

   p

   b

   es


   t









   i

   ​




   −






   x









   i

   ​


   (

   t

   ))



   +






   c









   2

   ​


   ⋅




   r









   2

   ​


   ⋅



   (

   g

   b

   es

   t



   −






   x









   i

   ​


   (

   t

   ))
   更新粒子速度，其中,

   w
   w





   w
   是惯性权重，

   c
   1
   c\_1






   c









   1

   ​

   和

   c
   2
   c\_2






   c









   2

   ​

   是学习因子，

   r
   1
   r\_1






   r









   1

   ​

   和

   r
   2
   r\_2






   r









   2

   ​

   是随机数。
6. 位置更新：根据新的速度更新粒子的位置

   x
   i
   (
   t
   +
   1
   )
   =
   x
   i
   (
   t
   )
   +
   v
   i
   (
   t
   +
   1
   )
   x\_i(t+1)=x\_i(t)+v\_i(t+1)






   x









   i

   ​


   (

   t



   +





   1

   )



   =






   x









   i

   ​


   (

   t

   )



   +






   v









   i

   ​


   (

   t



   +





   1

   )
7. 惯性权重和学习因子调整：根据种群的进化状态或粒子的适应度动态调整惯性权重和学习因子。例如，可以采用线性递减、非线性递减或基于种群多样性的自适应调整策略。
8. 检查终止条件：判断是否达到最大迭代次数或满足收敛精度，如果是，则输出最优解；否则，继续迭代

```python
import numpy as np


def objective_function(x):
    # 定义目标函数
    return np.sum(x ** 2)


# 参数初始化：设置粒子群大小、维度、最大迭代次数、惯性权重范围、学习因子和边界范围。
#
class AWPSO:
    def __init__(self, n_particles, dimensions, max_iter, w_min, w_max, c1, c2, bounds):
        self.n_particles = n_particles
        self.dimensions = dimensions
        self.max_iter = max_iter
        self.w_min = w_min
        self.w_max = w_max
        self.c1 = c1
        self.c2 = c2
        self.bounds = bounds

        # 初始化粒子位置和速度
        self.X = np.random.uniform(bounds[0], bounds[1], (n_particles, dimensions))
        self.V = np.random.uniform(-1, 1, (n_particles, dimensions))

        # 初始化个体最优和全局最优
        self.pbest = self.X.copy()
        self.pbest_fitness = np.array([objective_function(x) for x in self.pbest])
        self.gbest_idx = np.argmin(self.pbest_fitness)
        self.gbest = self.pbest[self.gbest_idx]

```

惯性权重更新：根据当前迭代次数动态调整惯性权重。
  
速度和位置更新：根据速度更新公式调整粒子速度，并更新粒子位置。
  
边界处理：确保粒子位置在预定义的边界范围内。
  
适应度计算：计算每个粒子的适应度值。
  
个体最优和全局最优更新：根据适应度值更新个体最优和全局最优位置。

```python
    def optimize(self):
        for t in range(self.max_iter):
            # 更新惯性权重
            w = self.w_max - (self.w_max - self.w_min) * t / self.max_iter

            # 更新速度和位置
            r1, r2 = np.random.rand(2)
            self.V = w * self.V + self.c1 * r1 * (self.pbest - self.X) + self.c2 * r2 * (self.gbest - self.X)
            self.X = self.X + self.V

            # 边界处理
            self.X = np.clip(self.X, self.bounds[0], self.bounds[1])

            # 计算适应度
            fitness = np.array([objective_function(x) for x in self.X])

            # 更新个体最优
            update_mask = fitness < self.pbest_fitness
            self.pbest[update_mask] = self.X[update_mask]
            self.pbest_fitness[update_mask] = fitness[update_mask]

            # 更新全局最优
            self.gbest_idx = np.argmin(self.pbest_fitness)
            self.gbest = self.pbest[self.gbest_idx]

        return self.gbest, objective_function(self.gbest)


# 参数设置
n_particles = 30
dimensions = 2
max_iter = 100
w_min = 0.4
w_max = 0.9
c1 = 2.0
c2 = 2.0
bounds = (-5, 5)

# 运行AWPSO算法
awpso = AWPSO(n_particles, dimensions, max_iter, w_min, w_max, c1, c2, bounds)
best_position, best_fitness = awpso.optimize()

print("最优解位置:", best_position)
print("最优解适应度:", best_fitness)

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/469116327dec4307860df5cc575e2108.png)

### 2.2 代码

模型构建（CNN + LSTM + Attention）

```python
import torch
import torch.nn as nn

# 自定义注意力机制层
class AttentionLayer(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.W = nn.Linear(input_dim, 1)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        att_weights = self.softmax(self.W(x).squeeze(-1))  # (batch_size, seq_len)
        weighted = torch.sum(x * att_weights.unsqueeze(-1), dim=1)  # (batch_size, input_dim)
        return weighted

# 定义完整模型
class AWPSOLSTMAT(nn.Module):
    def __init__(self, input_dim=4, lstm_units1=64, lstm_units2=32, dropout_rate=0.2):
        super().__init__()
        # CNN特征提取
        self.cnn = nn.Sequential(
            nn.Conv1d(input_dim, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Dropout(dropout_rate),
            nn.Conv1d(64, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
        )
        # LSTM + Attention
        self.lstm1 = nn.LSTM(input_size=64, hidden_size=lstm_units1, batch_first=True)
        self.lstm2 = nn.LSTM(input_size=lstm_units1, hidden_size=lstm_units2, batch_first=True)
        self.attention = AttentionLayer(lstm_units2)
        # 全连接层
        self.fc = nn.Sequential(
            nn.Linear(lstm_units2, 32),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(32, 1)
        )
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_dim)
        x = x.permute(0, 2, 1)  # (batch_size, input_dim, seq_len)
        x = self.cnn(x)         # (batch_size, 64, pooled_seq_len)
        x = x.permute(0, 2, 1)  # (batch_size, pooled_seq_len, 64)
        x, _ = self.lstm1(x)    # (batch_size, pooled_seq_len, lstm_units1)
        x, _ = self.lstm2(x)    # (batch_size, pooled_seq_len, lstm_units2)
        x = self.attention(x)   # (batch_size, lstm_units2)
        x = self.fc(x)          # (batch_size, 1)
        return x.squeeze(-1)

```

模型训练与评估

```python
# 初始化模型
model = AWPSOLSTMAT(input_dim=4, lstm_units1=64, lstm_units2=32, dropout_rate=0.2)
criterion = nn.L1Loss()  # MAE
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 训练循环
def train(model, dataloader, epochs=100):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}')

# 评估函数
def evaluate(model, dataloader):
    model.eval()
    total_mae = 0
    with torch.no_grad():
        for X_batch, y_batch in dataloader:
            outputs = model(X_batch)
            total_mae += criterion(outputs, y_batch).item()
    return total_mae / len(dataloader)

# 训练并测试
train(model, train_loader, epochs=50)
test_mae = evaluate(model, test_loader)
print(f'Test MAE: {test_mae:.4f}')

```

### 总结

文提出基于AWPSO - LSTMAT的水质预测模型，以周河溪屯桥监测站数据验证。与主流模型对比，该模型预测精度高、泛化能力强，在水质管理、预警和决策中作用显著，可用于周河水质趋势预测平台，为当地水环境管理提供参考。