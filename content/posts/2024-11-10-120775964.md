---
layout: post
title: "智能座舱架构与功能开发流程详解"
date: 2024-11-10 17:02:21 +0800
description: "作者 |Jessie出品|焉知智能汽车座舱发展主要经历了四个阶段：包括电子座舱阶段、智能助理阶段、人"
keywords: "智能座舱tbox"
categories: ['未分类']
tags: ['编程语言', '算法', '大数据', '人工智能', 'Python']
artid: "120775964"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=120775964
  alt: "智能座舱架构与功能开发流程详解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=120775964
featuredImagePreview: https://bing.ee123.net/img/rand?artid=120775964
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     智能座舱架构与功能开发流程详解
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <div id="js_content">
     <p style="text-align:left;">
      <img alt="62d9ed21dbea8015a02008acd1fa07fd.png" src="https://i-blog.csdnimg.cn/blog_migrate/f49b5dc8b47cbdedad1f5d5ba71885f5.jpeg"/>
     </p>
     <p style="text-align:left;">
      <strong>
       作者 |
      </strong>
      Jessie
     </p>
     <p style="text-align:left;">
      <strong>
       出品 |
      </strong>
      焉知
     </p>
     <p>
      智能汽车座舱发展主要经历了四个阶段：包括电子座舱阶段、智能助理阶段、人机共驾阶段、第三生活空间。当前随着智能汽车在AI算法、智能驾驶上的不断发展进入了L3级自动驾驶的“人机共驾阶段”。在智能座舱的表现形式为该阶段包含对语音控制和手势控制技术突破，车内软硬件一体化聚合，实现车辆感知精细化，车辆可在上车-行驶-下车的整个用车周期中，为驾乘人主动提供场景化的服务，实现机器自主/半自主决策。AI座舱核心价值将表现为基于场景的主动化交互和服务，很多也被称之为SOA的智能车服务。
     </p>
     <p>
      <img alt="43ab6fe5cfe4dbacd08165f82e64e1bd.png" src="https://i-blog.csdnimg.cn/blog_migrate/3723d26abf1b9c98d0e7d3c0f7cb9a96.jpeg"/>
     </p>
     <p>
      传统座舱引入各种辅助驾驶功能之后，要求驾驶员能够熟练掌握驾驶舱的交互方式，能够了解系统的能力与使用限制，能够理解系统的输入／输出关系，在此基础上决定如何操控辅助驾驶系统。在后续下一代架构中，智能座舱将实现语音控制和手势控制技术突破，基于多种模式感知手段的融合，使感知更精准和更主动。
     </p>
     <p style="text-align:center;">
      <img alt="c6770e1a89e46f8d3e18e122a6f8bdd8.png" src="https://i-blog.csdnimg.cn/blog_migrate/6f1d1370f912ef3e9b7ee66653c00a36.png"/>
     </p>
     <p style="text-align:center;">
      <strong>
       智能座舱基础架构解析
      </strong>
      <br/>
     </p>
     <p>
      整个智能座舱架构参照3层模型构成，其中底层是硬件层，包含摄像头，麦克风阵列，内嵌式存储器（磁盘）EMMC、内存DDR等；中间层是系统软件层，包含操作驾驶域系统驱动（Linux/QNX Drive）与座舱域系统驱动（Android Drive\SPI）；中间层之上是功能软件层，包含与智能驾驶公用部分的感知软件，智能座舱自身域的感知软件，功能安全分析层。车机端的在向上层是服务层，包含启用摄像头人脸识别、自动语音识别、数据服务、场景网关、账号鉴权等。
     </p>
     <p>
      <img alt="78ae9bf4117cf0866292be03fad3247a.png" src="https://i-blog.csdnimg.cn/blog_migrate/4cc974c43ca008e7ba2ed137f33fce67.jpeg"/>
     </p>
     <p>
      座舱AI智能交互系统是一个独立系统，独立迭代，每月OTA。整个智能座舱系统架构可以参考如下设计模型进行相应的信息交互。与智能驾驶域不同，智能座舱域更偏向于交互层级，也即更加重视智能互联。因此对于网络通信、数据流等信息更加重视。
     </p>
     <p>
      <img alt="d6e5fd7a88b85e6d9fb7807d6bf23254.png" src="https://i-blog.csdnimg.cn/blog_migrate/be7581c33c38a54405c4411da1e2227d.jpeg"/>
     </p>
     <p>
      从下到上整体智能座舱系统包括如下几个大的控制单元应用：
     </p>
     <p>
      <strong>
       1、车机硬件
      </strong>
     </p>
     <p>
      车机硬件主要是原始感光或应声部件，用于接收DMS摄像头输入的驾驶员面部或手部信息及OMS输入的乘员信息。同时，接收车内乘员输入的相关语音信息，车载音响、显示等硬件单元。
     </p>
     <p>
      <strong>
       2、图像或语音处理芯片
      </strong>
     </p>
     <p>
      这里的图像或语音处理芯片功能包含对人脸识别、情绪识别、手势识别、危险行为识别、多模语音、功能算法等应用。
     </p>
     <ul>
      <li>
       <p>
        <strong>
         感知软件：
        </strong>
        包含多模感知算法、数据闭环的数据埋点、插件管理和基础组件
       </p>
      </li>
      <li>
       <p>
        <strong>
         功能安全：
        </strong>
        实现芯片处理的硬件级别及软件级别的功能安全分析及构建
       </p>
      </li>
      <li>
       <p>
        <strong>
         系统管理：
        </strong>
        包括底层OTA、配置组件、功能安全、诊断、生命周期控制等
       </p>
      </li>
      <li>
       <p>
        <strong>
         公共管理：
        </strong>
        基本日志、链路、配置等软件管理
       </p>
      </li>
     </ul>
     <p>
      <strong>
       3、系统及中间件平台
      </strong>
     </p>
     <p>
      与智能驾驶类似，智能座舱在系统平台层面需要建立硬件适配及驱动控制，包含进行安全数字输入输出单元、电源能量分配、编解码、音频输出、显示、can通信等单元。
     </p>
     <p>
      <strong>
       4、车机服务
      </strong>
     </p>
     <p>
      作为智能座舱的核心服务，则更加依赖于车机服务进行相应的能力控制。整个车机服务包括系统控制、车身控制、数据服务、OTA、底盘状态及车身数据等内容。
     </p>
     <p>
      <strong>
       具体说来实现如下功能：
      </strong>
     </p>
     <ul>
      <li>
       <p>
        <strong>
         AI芯片管理：
        </strong>
        包含该AI芯片级以上的系统管理与配合，进行进程监控、OTA、HBSerrvice
       </p>
      </li>
      <li>
       <p>
        <strong>
         感知数据软件包SDK：
        </strong>
        包含接收传感器感知数据结果，融入AI芯片算法中，并提供数据包Pack的录制功能
       </p>
      </li>
      <li>
       <p>
        <strong>
         控制软件包SDK：
        </strong>
        提供软件生命周期管理，感知算法控制开关，录制开关等功能
       </p>
      </li>
      <li>
       <p>
        <strong>
         应用框架：
        </strong>
        完成相关业务流程，比如场景定义、多模态语义解析等
       </p>
      </li>
      <li>
       <p>
        <strong>
         业务层：
        </strong>
        在应用框架之上，完成相关业务实现过程，比如FaceID注册，工作模式定义、OTA、数据闭环等
       </p>
      </li>
      <li>
       <p>
        <strong>
         数据服务：
        </strong>
        包含数据管理、数据处理、数据挖掘、数据回灌；数据指标评测、诊断管理；模型训练、模型测试、模型管理；数据标注、标注管理等一系列服务。
       </p>
      </li>
     </ul>
     <p>
      <strong>
       5、决策中心
      </strong>
     </p>
     <p>
      决策中心包括通过感知SDK建立场景SDK，从而构建定制化场景及图像/语音感知能力。
     </p>
     <p>
      多模态座舱交互技术总体包含：语音+手势+视线智能人机交互系统。这里我们把图像和语音感知处理能力统称为多模态交互应用技术框架。其处理过程包含定义车身数据库、车内感知数据库，并进行用户交互行为数据库构建，开发用于云端场景推荐匹配SDK，后续用于解决全场景联调服务推荐功能。进一步的，采集用户典型场景行为数据，将实际用户行为数据输入个性化配置引擎可推动实现端上场景SDK。最终解决车控、音乐、支付等常规服务推荐功能。
     </p>
     <p>
      <strong>
       6、交互应用
      </strong>
     </p>
     <p>
      整个交互应用包括车身控制、系统控制、第三方APP交互控制、语音播报、用户界面等几个方面。同时，对于第三方应用中的地图、天气、音乐等也有一定要求。
     </p>
     <p>
      <strong>
       7、云端服务
      </strong>
     </p>
     <p>
      由于大量的数据涉及远程传输和监控，且智能座舱的大算力算法模块处理也更加依赖云端管理和计算能力。智能座舱云端服务包括算法模型训练、在线场景仿真、数据安全、OTA管理、数据仓储、账号服务等。
     </p>
     <ul>
      <li>
       <p>
        <strong>
         场景网关：
        </strong>
        融合多个服务，比如驾驶员监控的faceID或语音识别进行场景理解，用于行为分析，推送
       </p>
      </li>
      <li>
       <p>
        <strong>
         账号鉴权：
        </strong>
        对服务接入进行鉴权，只有授权账号才能进行服务
       </p>
      </li>
      <li>
       <p>
        <strong>
         faceID：
        </strong>
        驾驶员人脸识别
       </p>
      </li>
      <li>
       <p>
        <strong>
         数据闭环管理：
        </strong>
        数据接入平台、OTA升级等
       </p>
      </li>
     </ul>
     <p style="text-align:center;">
      <img alt="ebfe260838fd27112d63a8cf5a64e567.png" src="https://i-blog.csdnimg.cn/blog_migrate/109b16ebc39c1a77a423bd62ca347c0d.png"/>
     </p>
     <p style="text-align:center;">
      <strong>
       智能座舱算法算力解析
      </strong>
      <br/>
     </p>
     <p>
      智能座舱的高速发展催生算法数量攀升，算力需求增加。到2021年，摄像头能够覆盖轿车乘客，IMS检测最多达5人，多模语音分离最多也达到5人，2022年，大概有150个算法驱动300个以上的场景应用；到2023年，开发者生态建立后，第三方感知将大幅增加，全车的离线多模语音交互将需要更多的算力。车载智能化AI系统包括车载AI场景、算法、开发工具、计算架构、车载AI芯片。整个智能座舱AI系统视觉、语音、多模融合。23年，座舱AI算法将达到白万级。
     </p>
     <p>
      在数据方面整体提高50%的处理效率，在算法方面平衡计算和带宽上的高效神经网络结构。在算力上将从个位数量级向百位数量级增长，一般情况智能AI座舱是一个独立系统，独立迭代，每月OTA。
     </p>
     <p>
      如下表示了智能座舱在其AI算法发展上的能力分配表。总结起来智能座舱算法模块主要分为几个大类：
     </p>
     <p>
      <strong>
       驾驶员面部识别类：
      </strong>
      包含人头识别、人眼识别、眼睛识别等；
     </p>
     <p>
      <strong>
       驾驶员动作识别类：
      </strong>
      手势动作识别、身体动作识别、嘴唇识别等；
     </p>
     <p>
      <strong>
       座舱声音识别类：
      </strong>
      前排双音区检测、声纹识别、语音性别识别/年龄识别等；
     </p>
     <p>
      <strong>
       座舱光线识别类：
      </strong>
      座舱氛围灯、座舱主体背景、座舱内饰等；
     </p>
     <p>
      如下图表示了比较全面的智能座舱算法库。
     </p>
     <p>
      <img alt="29fd8ffc55504604084feaeb76485b93.png" src="https://i-blog.csdnimg.cn/blog_migrate/9450cc8e0abaae170536e76ad8973272.jpeg"/>
     </p>
     <p>
      车载智能交互算力需求趋势，表示传感器增长趋势主要体现在舱内传感器数量和像素的提升，带来对算力需求的大幅提升。此外，对于智能座舱而言，麦克风数量也从集中双麦克风/分布式4麦克风，向分布式6-8个麦克风方向发展。
     </p>
     <p>
      <img alt="c4b7e038077d86503db172fb4cef1ba3.png" src="https://i-blog.csdnimg.cn/blog_migrate/8e6a6946dd7bf127f5322c418493935e.png"/>
     </p>
     <p style="text-align:center;">
      <img alt="40c3f0413ea0c2fcd9b9d34179eb2bda.png" src="https://i-blog.csdnimg.cn/blog_migrate/69e86dac599bb81043cc7d4f9e80bb47.png"/>
     </p>
     <p style="text-align:center;">
      <strong>
       智能座舱开发流程
      </strong>
      <br/>
     </p>
     <p>
      智能座舱开发流程涉及利用新场景、场景库进行场景定义；利用HMI设计工具进行UI/UE设计（包含界面及交互逻辑设计）；利用HMI 框架构建工具搭建整个交互设计平台；由开发人员基于搭建的交互设计平台进行软硬件开发；测试人员深入贯穿于整个开发过程进行阶段性单元测试和集成测试。测试结果部署于车端进行搭载。整个过程由开发设计人员进行全方面维护。
     </p>
     <p>
      <img alt="02bd232bacf76dc6d522308928e20b22.png" src="https://i-blog.csdnimg.cn/blog_migrate/de4c298b72eb57341939a0d90a12f63c.png"/>
     </p>
     <p>
      细化下来，将其中的开发过程进行放大可以看到：从数据平台到开发平台搭建及软硬件开发过程涉及如下：
     </p>
     <p>
      <img alt="2c5735edc4867a73e26348da2f9e3ade.png" src="https://i-blog.csdnimg.cn/blog_migrate/4ebed7f69ed9b3dc4779412daa284f74.jpeg"/>
     </p>
     <p>
      <strong>
       1、数据开发框架
      </strong>
     </p>
     <p>
      整个开发数据平台是一个全闭环流程，该闭环流程涉及四大数据处理过程，最终形成可用于训练的的有效模型。我们将整个数据框架的闭环过程进行细化，不难发现从数据采集到数据模型整个闭环过程都是连续不间断的过程，其过程就是不断探索对应场景下的数据真值。其中数据采集是从数据后台通过数据回灌进行数据挖掘服务，通过数据筛选将其中达标数据注入数据标注模块，从而进行数据训练，训练过程中需要同步进行评测、同步等操作最终形成初版数据模型并进行工程集成到软件模块中。最后才是最重要的过程就是在功能评测阶段不断进行回归测试与数据分类，通过OTA等在线升级方式刷写进行车端软件更新。
     </p>
     <p>
      <img alt="ad232278765616b0784703134b799ed2.png" src="https://i-blog.csdnimg.cn/blog_migrate/f05066db9f19314c0a00277f5495f550.jpeg"/>
     </p>
     <p>
      <strong>
       1）数据缺陷
      </strong>
     </p>
     <p>
      这一过程中，首先需要从已量产的产品中提取数据缺陷DATA-Failure；数据缺陷包含数据漏检，虚假数据，数据校验不通过部分等；
     </p>
     <p>
      <strong>
       2）数据采集
      </strong>
     </p>
     <p>
      针对数据缺陷需要重新进行数据采集DATA-Collection，该采集过程包含在开发阶段通过搭建的数据采集平台进行数据采集（比如可以是实车在驾驶过程中用到的驾驶舱内外行车记录仪、全景影像、前视或周视摄像头等），也包含在已经量产的车型中设置的数据埋点或影子模式方式进行；
     </p>
     <p>
      <strong>
       3）数据标注
      </strong>
     </p>
     <p>
      采集数据进行数据标注DATA-Label，这里需要注意的是智能座舱和智能驾驶的标注方式上有所不同；如座舱主要涉及图片、语音等标注，ADAS主要涉及道路环境语义（如车道线、护栏、锥桶等标注类型）等标注；
     </p>
     <p>
      <strong>
       4）数据模型
      </strong>
     </p>
     <p>
      对于智能座舱算法而言，最重要的是进行人工智能的机器视觉算法训练，该过程涉及形成较为精准的数据模板，将标注后的数据用于进行数据模型训练DATA-Model。
     </p>
     <p>
      <strong>
       2、应用开发框架
      </strong>
     </p>
     <p>
      AI算法仓库主要是对数据平台中的数据模型进行有效训练，模型训练主要高中低三种渐进开发模式。
     </p>
     <p>
      <strong>
       高级模式：
      </strong>
      该AI算法仓库中训练模型复杂，需要耗费较多的AI算力用于权值检测、关键点检测、图像语义分割，图形骨架提取等；
     </p>
     <p>
      <strong>
       低级模式：
      </strong>
      算法仓库都是一些标准化模型，如安全带、座椅识别等标准件的识别等；这种类型的识别过程都是一些标准化的识别过程，甚至不包含浮点运算，都是整型运算，算法耗费算力小，效率高；
     </p>
     <p>
      <strong>
       中级模式：
      </strong>
      算法仓库中复杂度一般，分类较多，嵌入多模型组合进行分类，可实现诸如抽烟、打电话等驾驶员基本的操作过程识别。需要说明的是该模型对于开发团队的能力建设要求较高。
     </p>
     <p>
      <strong>
       3、应用集成框架
      </strong>
     </p>
     <p>
      应用集成框架平台包含利用AI应用开发中间件集成模型框架，搭建通信及底层组件。在开发集成过程中包含模型转换（即浮点转定点）与编译，生成标准化模型，随后通过加载模型跟配置（配置可以放到固定的地方）；定义输入输出：编写过程代码（包含处理逻辑），接收函数框架，定义消息类型（自动反序列化与序列化），释放软件等过程。后续可编译生成.so文件，并加载到感知管道Pipeline中。
     </p>
     <p>
      <img alt="191b090407c2395d20e38d579e482c65.png" src="https://i-blog.csdnimg.cn/blog_migrate/bc8d8f089fb5e497e3e13625b52bef06.jpeg"/>
     </p>
     <p>
      如上其他部分可以标准化，只需要关注process部分即可。
     </p>
     <p style="text-align:center;">
      <img alt="10ebb56b938819da9e554fcb1ef89eb6.png" src="https://i-blog.csdnimg.cn/blog_migrate/599cad8ce0f29d208ccb7b1d4d327373.png"/>
     </p>
     <p style="text-align:center;">
      <strong>
       总结
      </strong>
      <br/>
     </p>
     <p>
      本文详细描述了新型智能座舱的开发架构与开发流程，其中开发座舱架构技术的发展，也不会一直局限于座舱域，其也适用于智能驾驶域。当前，座舱域控制器通常参与车身域的控制，如控制空调、车门、车窗等。下一代智能座舱产品将更多的参与到动力&amp;底盘域的控制，例如通过语音控制给车辆换挡灯、拉起电子手刹等。当然，这类操作对安全性有更高要求。随着自动驾驶系统对动力域、底盘域的不断整合，以及面向服务架构(SOA)在车辆的普及，复杂的驾驶行为可能会抽象成个例化的驾驶服务。智能座舱域控制器通过提高自身的功能安全等级后，就可以直接调用自动驾驶域的驾驶服务，进行车辆的驾驶控制，形成人机共驾的新局面。
     </p>
     <p>
      经过对座舱域系统的系统架构和软硬件进行安全升级后，座舱的发展将补齐“驾驶控制”这最后一个短板，迈向“全方位智能座舱”。随着技术逐步的成熟，可能会导致硬件架构的进一步集中，加速促成驾驶域与座舱域的融合，并最终形成车载中央计算机。
     </p>
    </div>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35353336363236352f:61727469636c652f64657461696c732f313230373735393634" class_="artid" style="display:none">
 </p>
</div>
