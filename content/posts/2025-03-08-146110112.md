---
layout: post
title: "阿里云工作空间与Ollama一"
date: 2025-03-08 07:59:05 +0800
description: "Ollama 是一个开源框架，旨在帮助用户在其本地计算机上轻松地管理和部署大型语言模型（LLM）。它提供了一个轻量级且可扩展的解决方案，支持多种开源大模型，包括但不限于 Llama 、Gemma、Mistral 等，并允许用户自定义和创建自己的模型。本地化部署：允许企业或个人在本地环境中部署大型语言模型，提高计算效率并降低延迟。模型管理：Ollama 支持多种大型语言模型，用户可以根据需要选择和安装不同的模型。"
keywords: "ollma 租赁"
categories: ['未分类']
tags: ['阿里云', '数据库', '云计算']
artid: "146110112"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146110112
    alt: "阿里云工作空间与Ollama一"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146110112
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146110112
cover: https://bing.ee123.net/img/rand?artid=146110112
image: https://bing.ee123.net/img/rand?artid=146110112
img: https://bing.ee123.net/img/rand?artid=146110112
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     阿里云工作空间与Ollama(一)
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h3>
     <a id="_0">
     </a>
     工作空间
    </h3>
    <ol>
     <li>
      创建工作空间
     </li>
    </ol>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5786ec0ba7bd43cbb957ea9f2eee3285.png#pic_center"/>
    </p>
    <ol start="2">
     <li>
      <p>
       确认授权
       <br/>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c782cc88b5a84f9dbf06ad84d5e9ee63.png#pic_center"/>
      </p>
     </li>
     <li>
      <p>
       去授权
      </p>
      <p>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9274661b7ab647d7898f666e70d2c345.png#pic_center"/>
      </p>
     </li>
     <li>
      <p>
       同意授权
      </p>
      <p>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/e048a8900896408997aea49381b3a07c.png#pic_center"/>
      </p>
     </li>
     <li>
      <p>
       云资源授权成功
      </p>
      <p>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/0facc99d29b64606b287de93f0296149.png#pic_center"/>
      </p>
     </li>
     <li>
      <p>
       开通完成
      </p>
      <p>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/624ee222b8254e80aca9fc10f6eb6e2b.png#pic_center"/>
      </p>
     </li>
     <li>
      <p>
       前往默认工作空间
      </p>
      <p>
       <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5e2daba80ca746ab849ffda6be3ace6e.png#pic_center"/>
      </p>
     </li>
    </ol>
    <h3>
     <a id="_35">
     </a>
     一、阿里云
    </h3>
    <ol>
     <li>
      交互式建模（PAI-DSW）资源地址：
     </li>
    </ol>
    <pre><code>https://free.aliyun.com/?product=1395%2C1396%2C1397%2C1430%2C1398&amp;crowd=personal&amp;accounttraceid=eac053259c4d4a3681e76e0aa9b311b4oixa
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/68d0285b91ec4137b483f0429e0182f7.jpeg#pic_center"/>
    </p>
    <h3>
     <a id="_46">
     </a>
     二、实例创建
    </h3>
    <h4>
     <a id="21_DSW_48">
     </a>
     2.1 交互式建模（DSW）
    </h4>
    <p>
     新建实例：输入实例名称和选择GPU资源完成实例创建
    </p>
    <ul>
     <li>
      实例名称：
      <code>
       llama3
      </code>
     </li>
     <li>
      资源规格：
      <code>
       ecs.gn7i-c8g1.2xlarge (8 vCPU, 30 GiB, NVIDIA A10 * 1)
      </code>
     </li>
     <li>
      GPU资源：
      <code>
       modelscope:1.14.0-pytorch2.1.2-gpu-py310-cu121-ubuntu22.04
      </code>
     </li>
    </ul>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/0df44ac1a52846efb768e98029a76812.jpeg#pic_center"/>
    </p>
    <h4>
     <a id="22__59">
     </a>
     2.2 启动工作空间
    </h4>
    <ul>
     <li>
      点击打开按钮自动计算资源
     </li>
    </ul>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d920f69edd5f4c96a50c3b232e320192.png#pic_center"/>
    </p>
    <ul>
     <li>
      查看显卡资源(
      <code>
       nvidia-smi
      </code>
      )
     </li>
    </ul>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/65ceb7a0bf484f8a82cde1c85c473dfb.png#pic_center"/>
    </p>
    <h3>
     <a id="Ollma_71">
     </a>
     三、Ollma介绍
    </h3>
    <p>
     Ollama 是一个开源框架，旨在帮助用户在其本地计算机上轻松地管理和部署大型语言模型（LLM）。
     <br/>
     它提供了一个轻量级且可扩展的解决方案，支持多种开源大模型，包括但不限于 Llama 、Gemma、
     <br/>
     Mistral 等，并允许用户自定义和创建自己的模型。
     <br/>
     Ollama 的主要特点包括：
    </p>
    <ol>
     <li>
      本地化部署：允许企业或个人在本地环境中部署大型语言模型，提高计算效率并降低延迟。
     </li>
     <li>
      模型管理：Ollama 支持多种大型语言模型，用户可以根据需要选择和安装不同的模型。
     </li>
     <li>
      Open-WebUI集成：Ollama 可以与 Open-WebUI 快速集成，后者是一个开源的 Web 界面，用于
      <br/>
      与大型语言模型进行交互。
     </li>
     <li>
      命令行交互：Ollama 提供了一个基于命令行的交互界面，用户可以输入指令（prompt）并获取模
      <br/>
      型生成的响应。
     </li>
     <li>
      优化与微调：Ollama 允许用户对模型进行优化，包括微调和强化学习，以提高模型在特定任务上
      <br/>
      的性能。
     </li>
    </ol>
    <h4>
     <a id="31_Model_libray_87">
     </a>
     3.1 Model libray
    </h4>
    <ul>
     <li>
      地址：https://github.com/ollama/ollama
     </li>
    </ul>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/eb0da01d18484d1a90f399bf2dfcb7df.png#pic_center"/>
    </p>
    <h4>
     <a id="32_Ollmallama3_94">
     </a>
     3.2 Ollma部署llama3
    </h4>
    <ol>
     <li>
      部署 Ollama 的基本步骤通常包括：
      <ul>
       <li>
        安装 Ollama。
       </li>
       <li>
        选择并安装所需的大型语言模型。
       </li>
       <li>
        安装并配置 Open-WebUI（如果需要）。
       </li>
       <li>
        测试语言模型的效果并进行必要的优化。
       </li>
      </ul>
     </li>
     <li>
      Ollama部署llama3操作的演示
     </li>
    </ol>
    <pre><code>conda create -n ollama python=3.10
conda activate ollama
export OLLAMA_MODELS=/mnt/workspace/models
curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
    <ol start="3">
     <li>
      安装成功后执行
     </li>
    </ol>
    <pre><code>ollama serve
</code></pre>
    <ol start="4">
     <li>
      另外启动一个终端执行
     </li>
    </ol>
    <pre><code>ollama --version
ollama run llama3
</code></pre>
    <h3>
     <a id="OllamaAPI_123">
     </a>
     四、Ollama的API
    </h3>
    <p>
     Ollama has a REST API for running and managing models.
    </p>
    <h4>
     <a id="41_Generate_a_response_127">
     </a>
     4.1 Generate a response
    </h4>
    <pre><code>curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt":"Why is the sky blue?"
}'

</code></pre>
    <p>
     If stream is set to false , the response will be a single JSON object:
    </p>
    <pre><code>curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
</code></pre>
    <p>
     在 curl 命令中，向一个本地服务器发送一个 HTTP POST 请求，请求的目的是生成文本。这个请求包含了几个参数：
    </p>
    <ul>
     <li>
      <p>
       model ：指定要使用的模型，这里是 “llama3”。
      </p>
     </li>
     <li>
      <p>
       prompt ：提供了一个提示（prompt），在这个例子中是 “Why is the sky blue?”，模型将基于这
       <br/>
       个提示生成文本。
      </p>
     </li>
     <li>
      <p>
       stream ：这个参数决定了响应的输出方式。
      </p>
     </li>
    </ul>
    <p>
     当 stream 设为 true 时：
    </p>
    <ul>
     <li>
      服务器会立即开始发送响应数据，即使整个响应还没有完全生成完毕。
      <br/>
      这意味着客户端可以逐步接收到生成的文本，而不必等待整个生成过程完成。
      <br/>
      这种“流式”输出对于需要实时查看生成内容的场景很有用。
     </li>
     <li>
      当 stream 设为 false 时：
      <br/>
      客户端会等待服务器完成整个文本生成过程后才接收到响应。
      <br/>
      这意味着服务器会在内部完全处理完生成任务后，一次性将所有生成的文本发送给客户端。
      <br/>
      这种方式适用于需要整个生成文本块的场景。
     </li>
    </ul>
    <p>
     简而言之， stream 参数的设置决定了客户端是希望实时接收生成内容（ true ），还是希望在生成完成
     <br/>
     后一次性接收全部内容（ false ）。选择哪种方式取决于客户端的应用场景和需求。
    </p>
    <h4>
     <a id="42_Chat_with_a_model_168">
     </a>
     4.2 Chat with a model
    </h4>
    <pre><code>curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
   { "role": "user", "content": "why is the sky blue?" }
 ],
  "stream": false
}'
</code></pre>
    <p>
     上述curl命令都是在与Ollama的REST API进行交互, 但它们调用的端点(endpoint)和请求参数有所不同,
     <br/>
     服务于不同的使用场景。
    </p>
    <ol>
     <li>
      /api/generate 端点:
     </li>
    </ol>
    <ul>
     <li>
      该端点用于生成单次回复。
     </li>
     <li>
      请求参数:
      <ul>
       <li>
        model : 指定要使用的模型, 这里是"llama3"。
       </li>
       <li>
        prompt : 提供一个prompt, 模型将基于这个prompt生成回复。
       </li>
      </ul>
     </li>
     <li>
      适用场景: 当你只需要针对某个prompt生成一个回复, 而不需要长期的多轮对话时, 可以使用
      <br/>
      这个端点。
     </li>
    </ul>
    <ol start="2">
     <li>
      /api/chat 端点:
     </li>
    </ol>
    <ul>
     <li>
      该端点用于进行多轮对话。
      <ul>
       <li>
        请求参数:
        <ul>
         <li>
          model : 指定要使用的模型, 这里也是"llama3"。
         </li>
         <li>
          messages : 提供一个消息列表, 每个消息都有一个角色(如"user")和内容。模型将基于这
          <br/>
          些消息的上下文生成回复。
         </li>
        </ul>
       </li>
       <li>
        适用场景: 当你需要与模型进行多轮对话, 保持对话的上下文时, 应该使用这个端点。这在实现
        <br/>
        聊天机器人等应用时非常有用。
       </li>
      </ul>
     </li>
    </ul>
    <p>
     简而言之, /api/generate 用于单次回复的生成, 而 /api/chat 用于多轮对话。它们代表了与语言模型
     <br/>
     交互的两种主要方式。选择哪个端点取决于你的具体使用场景。
    </p>
    <p>
     如果你只需要一次性的回复, /api/generate 更简单直接。如果你要构建一个聊天应用,需要长期的多轮
     <br/>
     对话,则 /api/chat 更适合。
    </p>
    <p>
     上面是一个对话模型生成助手回复的过程记录。
     <br/>
     上面给出的curl命令只包含了一条用户消息, 还没有充分体现多轮对话。
     <br/>
     为了展示多轮对话, 我们可以在 messages 参数中传入更多的消息, 包括用户消息和助手消息。
     <br/>
     下面是一个修改后的示例:
    </p>
    <pre><code>curl http://localhost:11434/api/chat -d '{
  "model": "llama3",
  "messages": [
   { "role": "user", "content": "Why is the sky blue?" },
   { "role": "assistant", "content": "The sky appears blue due to a phenomenon
called Rayleigh scattering. This occurs when sunlight enters Earth's atmosphere
and interacts with molecules of gases like nitrogen and oxygen." },
   { "role": "user", "content": "Can you explain more about how Rayleigh
scattering works?" }
 ],
  "stream": false
}'

</code></pre>
    <p>
     在这个例子中:
    </p>
    <ol>
     <li>
      第一条消息是用户询问"天空为什么是蓝色的"。
     </li>
     <li>
      第二条消息是助手的初步回复, 解释了瑞利散射现象。
     </li>
     <li>
      第三条消息是用户要求助手进一步解释瑞利散射的工作原理。
     </li>
    </ol>
    <p>
     当发送这个请求时, 模型会考虑之前的所有消息, 然后生成一个新的助手回复, 进一步阐述瑞利散射。这样
     <br/>
     就形成了一个多轮对话的流程。
     <br/>
     你可以继续添加更多的用户和助手消息, 模拟一个完整的对话过程。每次请求, 模型都会基于所有历史消
     <br/>
     息生成回复, 保持对话的连贯性。
     <br/>
     这种多轮对话的交互方式更接近真实的聊天场景, 因为它保留了对话的上下文。模型可以根据之前的信息
     <br/>
     来生成更加相关和自然的回复。
     <br/>
     <strong>
      在命令行中, 如果命令太长或包含特殊字符, 直接执行可能会出现问题。
     </strong>
     上面的修改后的示例的curl命令
     <br/>
     没有返回回复。
     <br/>
     让我们尝试另一种方法: 将JSON请求体保存到一个单独的文件中, 然后在curl命令中引用这个文件。
     <br/>
     首先, 创建一个名为 request.json 的文件, 并将JSON请求体复制进去:
    </p>
    <pre><code>{
  "model": "llama3",
  "messages": [
   { "role": "user", "content": "Why is the sky blue?" },
   { "role": "assistant", "content": "The sky appears blue due to a phenomenon
called Rayleigh scattering. This occurs when sunlight enters Earth's atmosphere
and interacts with molecules of gases like nitrogen and oxygen." },
   { "role": "user", "content": "Can you explain more about how Rayleigh
scattering works?" }
 ],
  "stream": false
}

</code></pre>
    <p>
     确保JSON格式正确, 并将文件保存。
     <br/>
     然后, 修改curl命令, 使其引用这个文件:
    </p>
    <pre><code>curl -X POST -H "Content-Type: application/json" -d @request.json
http://localhost:11434/api/chat
</code></pre>
    <ul>
     <li>
      -X POST : 指定HTTP方法为POST。
     </li>
     <li>
      -H “Content-Type: application/json” : 设置请求头, 告诉服务器我们发送的是JSON格式的数
      <br/>
      据。
     </li>
     <li>
      -d @request.json : 读取 request.json 文件的内容作为请求体。
     </li>
    </ul>
    <p>
     现在, 尝试执行这个修改后的curl命令。它应该可以正常工作, 向ollama的 /api/chat 端点发送请求, 并
     <br/>
     json
     <br/>
     http://localhost:11434/api/chat
    </p>
    <pre><code>
- -X POST : 指定HTTP方法为POST。
- -H "Content-Type: application/json" : 设置请求头, 告诉服务器我们发送的是JSON格式的数
  据。
- -d @request.json : 读取 request.json 文件的内容作为请求体。

现在, 尝试执行这个修改后的curl命令。它应该可以正常工作, 向ollama的 /api/chat 端点发送请求, 并
接收到模型生成的回复。
</code></pre>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f4755414e475a48414e2f:61727469636c652f64657461696c732f313436313130313132" class_="artid" style="display:none">
 </p>
</div>


