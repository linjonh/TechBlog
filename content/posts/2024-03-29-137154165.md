---
layout: post
title: "AIGC-Stable-Diffusion发展及原理总结"
date: 2024-03-29 23:05:49 +0800
description: "AIGC全称是AI Generated Content, 直译：人工智能生成内容，也叫生成式人工智能"
keywords: "aigc生成训练数据"
categories: ['笔记', '深度学习']
tags: ['Stable', 'Diffusion', 'Aigc']
artid: "137154165"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=137154165
    alt: "AIGC-Stable-Diffusion发展及原理总结"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=137154165
featuredImagePreview: https://bing.ee123.net/img/rand?artid=137154165
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     AIGC-Stable Diffusion发展及原理总结
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p id="main-toc">
     <strong>
      目录
     </strong>
    </p>
    <p id="%E4%B8%80.%20AIGC%E4%BB%8B%E7%BB%8D-toc" style="margin-left:0px;">
     <a href="#%E4%B8%80.%20AIGC%E4%BB%8B%E7%BB%8D" rel="nofollow">
      一. AIGC介绍
     </a>
    </p>
    <p id="1.%20%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;">
     <a href="#1.%20%E4%BB%8B%E7%BB%8D" rel="nofollow">
      1. 介绍
     </a>
    </p>
    <p id="2.%20AIGC%E5%95%86%E4%B8%9A%E5%8C%96%E6%96%B9%E5%90%91-toc" style="margin-left:40px;">
     <a href="#2.%20AIGC%E5%95%86%E4%B8%9A%E5%8C%96%E6%96%B9%E5%90%91" rel="nofollow">
      2. AIGC商业化方向
     </a>
    </p>
    <p id="3.%20AIGC%E6%98%AF%E6%8A%80%E6%9C%AF%E9%9B%86%E5%90%88-toc" style="margin-left:40px;">
     <a href="#3.%20AIGC%E6%98%AF%E6%8A%80%E6%9C%AF%E9%9B%86%E5%90%88" rel="nofollow">
      3. AIGC是技术集合
     </a>
    </p>
    <p id="4.%20AIGC%E5%8F%91%E5%B1%95%E4%B8%89%E8%A6%81%E7%B4%A0-toc" style="margin-left:40px;">
     <a href="#4.%20AIGC%E5%8F%91%E5%B1%95%E4%B8%89%E8%A6%81%E7%B4%A0" rel="nofollow">
      4. AIGC发展三要素
     </a>
    </p>
    <p id="4.1%20%E6%95%B0%E6%8D%AE-toc" style="margin-left:80px;">
     <a href="#4.1%20%E6%95%B0%E6%8D%AE" rel="nofollow">
      4.1 数据
     </a>
    </p>
    <p id="4.2%20%E7%AE%97%E5%8A%9B-toc" style="margin-left:80px;">
     <a href="#4.2%20%E7%AE%97%E5%8A%9B" rel="nofollow">
      4.2 算力
     </a>
    </p>
    <p id="4.3%20%E7%AE%97%E6%B3%95-toc" style="margin-left:80px;">
     <a href="#4.3%20%E7%AE%97%E6%B3%95" rel="nofollow">
      4.3 算法
     </a>
    </p>
    <p id="4.3.1%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8BCLIP-toc" style="margin-left:120px;">
     <a href="#4.3.1%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8BCLIP" rel="nofollow">
      4.3.1 多模态模型CLIP
     </a>
    </p>
    <p id="4.3.2%20%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B-toc" style="margin-left:120px;">
     <a href="#4.3.2%20%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B" rel="nofollow">
      4.3.2 图像生成模型
     </a>
    </p>
    <p id="%E4%BA%8C.%20Stable%20Diffusion%20%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B-toc" style="margin-left:0px;">
     <a href="#%E4%BA%8C.%20Stable%20Diffusion%20%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B" rel="nofollow">
      二. Stable Diffusion 稳定扩散模型
     </a>
    </p>
    <p id="1.%20%E4%BB%8B%E7%BB%8D-toc" style="margin-left:40px;">
     <a href="#1.%20%E4%BB%8B%E7%BB%8D" rel="nofollow">
      1. 介绍
     </a>
    </p>
    <p id="1.1%20%E6%96%87%E7%94%9F%E5%9B%BE%E5%8A%9F%E8%83%BD%EF%BC%88Txt2Img)-toc" style="margin-left:80px;">
     <a href="#1.1%20%E6%96%87%E7%94%9F%E5%9B%BE%E5%8A%9F%E8%83%BD%EF%BC%88Txt2Img%29" rel="nofollow">
      1.1 文生图功能（Txt2Img)
     </a>
    </p>
    <p id="1.2%20%E5%9B%BE%E7%94%9F%E5%9B%BE%E5%8A%9F%E8%83%BD%EF%BC%88Img2Img)-toc" style="margin-left:80px;">
     <a href="#1.2%20%E5%9B%BE%E7%94%9F%E5%9B%BE%E5%8A%9F%E8%83%BD%EF%BC%88Img2Img%29" rel="nofollow">
      1.2 图生图功能（Img2Img)
     </a>
    </p>
    <p id="2.%20%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84-toc" style="margin-left:40px;">
     <a href="#2.%20%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84" rel="nofollow">
      2. 技术架构
     </a>
    </p>
    <p id="step1%20CLIP%20Text%20Encoder-toc" style="margin-left:80px;">
     <a href="#step1%20CLIP%20Text%20Encoder" rel="nofollow">
      step1 CLIP Text Encoder
     </a>
    </p>
    <p id="(1)%20CLIP%E4%BB%8B%E7%BB%8D-toc" style="margin-left:120px;">
     <a href="#%281%29%20CLIP%E4%BB%8B%E7%BB%8D" rel="nofollow">
      (1) CLIP介绍
     </a>
    </p>
    <p id="(2)%20CLIP%E6%9E%B6%E6%9E%84-toc" style="margin-left:120px;">
     <a href="#%282%29%20CLIP%E6%9E%B6%E6%9E%84" rel="nofollow">
      (2) CLIP架构
     </a>
    </p>
    <p id="step2%20LDM%E4%B9%8BVAE%20Encoder-toc" style="margin-left:80px;">
     <a href="#step2%20LDM%E4%B9%8BVAE%20Encoder" rel="nofollow">
      step2 LDM之VAE Encoder
     </a>
    </p>
    <p id="step3%20LDM%E4%B9%8BDiffusion-toc" style="margin-left:80px;">
     <a href="#step3%20LDM%E4%B9%8BDiffusion" rel="nofollow">
      step3 LDM之Diffusion
     </a>
    </p>
    <p id="(1)%20%E5%89%8D%E5%90%91%E6%89%A9%E6%95%A3%EF%BC%9A%E5%8A%A0%E5%99%AA-toc" style="margin-left:120px;">
     <a href="#%281%29%20%E5%89%8D%E5%90%91%E6%89%A9%E6%95%A3%EF%BC%9A%E5%8A%A0%E5%99%AA" rel="nofollow">
      (1) 前向扩散：加噪
     </a>
    </p>
    <p id="(2)%20%E5%8F%8D%E5%90%91%E6%89%A9%E6%95%A3%EF%BC%9A%E5%8E%BB%E5%99%AA-toc" style="margin-left:120px;">
     <a href="#%282%29%20%E5%8F%8D%E5%90%91%E6%89%A9%E6%95%A3%EF%BC%9A%E5%8E%BB%E5%99%AA" rel="nofollow">
      (2) 反向扩散：去噪
     </a>
    </p>
    <p id="a)%C2%A0SD%E6%A0%B8%E5%BF%83%EF%BC%9AU-Net-toc" style="margin-left:160px;">
     <a href="#a%29%C2%A0SD%E6%A0%B8%E5%BF%83%EF%BC%9AU-Net" rel="nofollow">
      a) SD核心：U-Net
     </a>
    </p>
    <p id="b)%C2%A0Transformer2DModel%3A%20%E5%8A%A0%E5%85%A5%E6%9D%A1%E4%BB%B6%E6%8E%A7%E5%88%B6-toc" style="margin-left:160px;">
     <a href="#b%29%C2%A0Transformer2DModel%3A%20%E5%8A%A0%E5%85%A5%E6%9D%A1%E4%BB%B6%E6%8E%A7%E5%88%B6" rel="nofollow">
      b) Transformer2DModel: 加入条件控制
     </a>
    </p>
    <p id="(3)%20%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E5%99%AA%E5%A3%B0%E9%A2%84%E6%B5%8B%E5%99%A8%EF%BC%9F-toc" style="margin-left:120px;">
     <a href="#%283%29%20%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E5%99%AA%E5%A3%B0%E9%A2%84%E6%B5%8B%E5%99%A8%EF%BC%9F" rel="nofollow">
      (3) 如何训练噪声预测器？
     </a>
    </p>
    <p id="step4%20LDM%E4%B9%8BVAE%20Decoder-toc" style="margin-left:80px;">
     <a href="#step4%20LDM%E4%B9%8BVAE%20Decoder" rel="nofollow">
      step4 LDM之VAE Decoder
     </a>
    </p>
    <p id="%E4%B8%89.%20%E5%8F%82%E8%80%83-toc" style="margin-left:0px;">
     <a href="#%E4%B8%89.%20%E5%8F%82%E8%80%83" rel="nofollow">
      三. 参考
     </a>
    </p>
    <hr id="hr-toc"/>
    <p>
    </p>
    <h2 id="%E4%B8%80.%20AIGC%E4%BB%8B%E7%BB%8D">
     一. AIGC介绍
    </h2>
    <p style="margin-left:.0001pt;text-align:justify;">
     这两年，短视频平台上“AI绘画“非常火爆，抖音一键换装、前世今生之类的模板大家没玩过也应该听说过。另一个火爆应用当属ChatGPT，自2022年ChatPGT3.5发布后，快速累积了超百万用户，可谓家喻户晓。这两个概念均来自同一个领域，即AIGC。
    </p>
    <h3 id="1.%20%E4%BB%8B%E7%BB%8D">
     1. 介绍
    </h3>
    <p>
     AIGC全称是AI Generated Content, 直译：人工智能生成内容，也叫生成式人工智能
     <strong>
      。
     </strong>
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     AIGC是继­专业生产内容（PGC, Professional-genrated Content）、用户生产内容（UGC，User-generated Content）之后的新型内容创作方式，是互联网内容创作方式的一次革新。AIGC可以在对话、故事、图像、视频和音乐制作等方面，打造全新的数字内容生成与交互形式。
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     <img alt="" height="162" src="https://i-blog.csdnimg.cn/blog_migrate/09a660ed9fadd57a61075f9493c24f37.png" width="573">
      <br/>
     </img>
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     2022年8月，一位没有绘画基础的参赛者利用AI绘画工具 Midjourney创作的《太空歌剧院》在美国科罗纳州举办的新型数字艺术家竞赛中，获得“数字艺术/数字修饰照片”类别一等奖，由此，AI绘画进入大众视野。同年，AI绘图模型Stable Diffusion开源，助力AI绘画破圈得到广泛关注。
    </p>
    <p style="margin-left:.0001pt;text-align:center;">
     <img alt="" height="286" src="https://i-blog.csdnimg.cn/blog_migrate/3d8ef6d1c0a145f244db1596f49e18ed.jpeg" width="429"/>
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     2022年11月30日，ChatGPT推出，5天后用户破百万，两个月后月活用户突破1亿，称为史上用户增长速度最快的消费级应用程序。
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     2021年开始，风投对AIGC的投资金额出现爆发式增长，2022年超20亿美元。据美国财经媒体Semafer报道，微软预计向ChatGPT的开发者OpenAI投资100亿美元。
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     2022年，因此被称为“
     <strong>
      AIGC元年”
     </strong>
     。
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <h3 id="2.%20AIGC%E5%95%86%E4%B8%9A%E5%8C%96%E6%96%B9%E5%90%91" style="margin-left:.0001pt;text-align:justify;">
     2. AIGC商业化方向
    </h3>
    <p style="margin-left:.0001pt;text-align:justify;">
     AIGC的出现，打开了一个全新的创作世界，为人们提供了无尽的可能性。AIGC生成的内容种类和范围随着技术的发展也在不断扩大。目前，常见的内容包括：
    </p>
    <ul>
     <li>
      <p style="margin-left:.0001pt;text-align:justify;">
       <span style="color:#000000;">
        AI文本生成，
       </span>
       <span style="color:#000000;">
        以
       </span>
       <span style="color:#000000;">
        OpenAI
       </span>
       <span style="color:#000000;">
        GPT
       </span>
       <span style="color:#000000;">
        系列为代表的模型，实现自动写邮件、广告营销方案等
       </span>
      </p>
     </li>
     <li>
      <p style="margin-left:.0001pt;text-align:justify;">
       <span style="color:#0070c0;">
        <strong>
         AI
        </strong>
       </span>
       <span style="color:#0070c0;">
        <strong>
         文生图
        </strong>
       </span>
       <span style="color:#0070c0;">
        <strong>
         /
        </strong>
       </span>
       <span style="color:#0070c0;">
        <strong>
         图生图
        </strong>
       </span>
       <span style="color:#000000;">
        ，
       </span>
       <span style="color:#000000;">
        如使用“跨模态模型
       </span>
       <span style="color:#000000;">
        CLIP+
       </span>
       <span style="color:#000000;">
        扩散
       </span>
       <span style="color:#000000;">
        模型
       </span>
       <span style="color:#000000;">
        Diffusion
       </span>
       <span style="color:#000000;">
        “实现的文生图模型
       </span>
       <span style="color:#0070c0;">
        <strong>
         Stable Diffusion
        </strong>
       </span>
      </p>
     </li>
     <li>
      <p style="margin-left:.0001pt;text-align:justify;">
       <span style="color:#000000;">
        AI文生视频，如OpenAI
       </span>
       <span style="color:#000000;">
        今年
       </span>
       <span style="color:#000000;">
        2.16
       </span>
       <span style="color:#000000;">
        日发布的
       </span>
       <span style="color:#000000;">
        Sora
       </span>
       <span style="color:#000000;">
        ，颠覆了全球
       </span>
       <span style="color:#000000;">
        AI
       </span>
       <span style="color:#000000;">
        生成视频市场的格局
       </span>
      </p>
     </li>
     <li>
      <p style="margin-left:.0001pt;text-align:justify;">
       <span style="color:#000000;">
        …
       </span>
      </p>
     </li>
    </ul>
    <p style="margin-left:.0001pt;text-align:justify;">
     更多AIGC应用可见：
     <a href="https://www.aigc.cn/" rel="nofollow" title="AIGC工具导航 | 生成式AI工具导航平台-全品类AI应用商店!">
      AIGC工具导航 | 生成式AI工具导航平台-全品类AI应用商店!
     </a>
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <h3 id="3.%20AIGC%E6%98%AF%E6%8A%80%E6%9C%AF%E9%9B%86%E5%90%88" style="margin-left:.0001pt;text-align:justify;">
     3. AIGC是技术集合
    </h3>
    <p style="margin-left:.0001pt;text-align:justify;">
     AIGC不是某一个单一的技术或者模型，AIGC使一个技术集合。概括来说，它是基于生成对抗网络GAN、大型预训练模型等人工智能技术，通过已有数据寻找规律，并通过适当的泛化能力生成相关内容的技术集合。简单理解就是所有的AIGC方向的模型，都不是单一模型实现的，而是通过刚才说的技术组合训练得到的。
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
    </p>
    <h3 id="4.%20AIGC%E5%8F%91%E5%B1%95%E4%B8%89%E8%A6%81%E7%B4%A0" style="margin-left:.0001pt;text-align:justify;">
     4. AIGC发展三要素
    </h3>
    <h4 id="4.1%20%E6%95%B0%E6%8D%AE">
     4.1 数据
    </h4>
    <p>
     <span style="color:#000000;">
      UGC
     </span>
     <span style="color:#000000;">
      生成的规模化内容，
     </span>
     <span style="color:#000000;">
      创造
     </span>
     <span style="color:#000000;">
      了大量
     </span>
     <span style="color:#000000;">
      学习素材，互联网数据规模
     </span>
     <span style="color:#000000;">
      快速膨胀。
     </span>
    </p>
    <h4 id="4.2%20%E7%AE%97%E5%8A%9B">
     4.2 算力
    </h4>
    <p>
     <span style="color:#000000;">
      图形处理器GPU
     </span>
     <span style="color:#000000;">
      、
     </span>
     <span style="color:#000000;">
      张量
     </span>
     <span style="color:#000000;">
      处理器
     </span>
     <span style="color:#000000;">
      TPU
     </span>
     <span style="color:#000000;">
      等算
     </span>
     <span style="color:#000000;">
      力设备
     </span>
     <span style="color:#000000;">
      性能不断
     </span>
     <span style="color:#000000;">
      提升，
     </span>
     <span style="color:#000000;">
      A100,H100
     </span>
     <span style="color:#000000;">
      等加速卡
     </span>
    </p>
    <h4 id="4.3%20%E7%AE%97%E6%B3%95">
     4.3 算法
    </h4>
    <p style="margin-left:.0001pt;text-align:justify;">
     当前AIGC技术已经从最初追求生成内容的真实性的基本要求，发展到满足生成内容多样性、可控性的进阶需求，并开始追求生成内容的组合型。
     <strong>
      数字内容的组合性
     </strong>
     一方面关注复杂场景、长文本等内容中各个元素的组合，如虚拟数字世界中人、物和环境间的交互组合,并生成整体场景；另一方面，追求概念、规则等抽象表达的组合，以此完成更加丰富和生动的数字内容生成。
     <strong>
      这些新需求对传统单一模态的人工智能算法框架提出了新的挑战
     </strong>
     。
    </p>
    <p style="margin-left:.0001pt;text-align:justify;">
     <strong>
      <span style="color:#fe2c24;">
       预训练大模型
      </span>
      和
      <span style="color:#fe2c24;">
       多模态方向
      </span>
      的发展，为AIGC技术发展和升级提供了基石
     </strong>
     。
    </p>
    <p style="margin-left:.0001pt;text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/eeb212cad3b564cc99d760f4c10bf492.png"/>
    </p>
    <p id="" style="margin-left:.0001pt;">
    </p>
    <h5 id="4.3.1%20%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8BCLIP" style="margin-left:.0001pt;">
     4.3.1 多模态模型CLIP
    </h5>
    <p>
     由于CLIP 两模块之一的Text Encoder是基于Transformer的模型，所以Transformer才被称为”跨模态重要开端之一“
    </p>
    <p id="">
    </p>
    <h5 id="4.3.2%20%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">
     4.3.2 图像生成模型
    </h5>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/435e331357c7e5759144f46cf0437e2e.png"/>
    </p>
    <div>
     <table cellspacing="0">
      <tbody>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:.9583in;">
         <p style="margin-left:0;">
          模型
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:4.9854in;">
         <p style="margin-left:0;">
          组成
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:1.6687in;">
         <p style="margin-left:0;">
          优点
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:3.4687in;">
         <p style="margin-left:0;">
          缺点
         </p>
        </td>
       </tr>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:.9583in;">
         <p style="margin-left:0;">
          GAN
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:4.9854in;">
         <p style="margin-left:0;">
          GAN = 生成器 + 判别器
         </p>
         <ul>
          <li>
           生成器将一个随机噪声向量映射到数据空间，生成一个伪造的样本
          </li>
          <li>
           判别器接收来自真实数据和生成器的样本，进行判断分类成真实还是伪造
          </li>
          <li>
           生成器的训练是通过与判别器的对抗性训练来实现的，即生成器试图生成能够愚弄判别器的样本，而判别器则试图区分真实样本和生成样本
          </li>
         </ul>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:1.5909in;">
         <p style="margin-left:0;">
          生成的图片逼真
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:3.7618in;">
         <ul>
          <li>
           由于要同时训练判别器和生成器这两个网络，训练不稳定
          </li>
          <li>
           GAN主要优化目标是使图片逼真，导致图片多样性不足
          </li>
          <li>
           GAN的生成是隐式的，由网络完成，不遵循概率分布，可解释性不强
          </li>
         </ul>
        </td>
       </tr>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:.9583in;">
         <p style="margin-left:0;">
          VAE
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:4.9854in;">
         <p style="margin-left:0;">
          VAE = 编码器 + 解码器
         </p>
         <p style="margin-left:0;">
          <span style="color:#000000;">
           AVE将输入数据编码成一个符合正态分布的数据分布，学习图片的数据分布特征
          </span>
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:1.6687in;">
         <p style="margin-left:0;">
          学习的概率分布，可解释性强，图片多样性足
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:3.684in;">
         <p style="margin-left:0;">
          ​产生图片模糊，原因可参考：
          <a href="https://blog.csdn.net/u013214262/article/details/86833305" title="破解VAE的迷思_vae生成的图像为什么模糊-CSDN博客">
           破解VAE的迷思_vae生成的图像为什么模糊-CSDN博客
          </a>
         </p>
        </td>
       </tr>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:.9583in;">
         <p style="margin-left:0;">
          Diffusion
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:4.9076in;">
         <p style="margin-left:0;">
          Diffusion = 前向扩散 + 反向扩散
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:1.7465in;">
         <ul>
          <li>
           生成的图片逼真
          </li>
          <li>
           数学可解释性强
          </li>
         </ul>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:3.6104in;">
         <p style="margin-left:0;">
          由于是在像素空间做的扩散，数据量多，训练成本高昂、速度慢，需要多步采样
         </p>
        </td>
       </tr>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:.9583in;">
         <p style="margin-left:0;">
          Latent Diffusion
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:4.9076in;">
         <p style="margin-left:0;">
          <span style="color:#00b0f0;">
           <strong>
            Latention Diffusion = VAE+Diffusion
           </strong>
          </span>
         </p>
         <p style="margin-left:0;">
          <span style="color:#00b0f0;">
           <strong>
            潜在扩散模型
           </strong>
          </span>
         </p>
         <p style="margin-left:0;">
          Latent Diffusion通过引入VAE, 解决速度慢的问题：
          <strong>
           VAE将像素空间的输入压缩编码成Latent潜在空间的概率分布，SD的Latent Space为4x64x64, 比图像像素空间3x512x512小48倍，减少空间占用，之后再进行模型扩散，这样可以加速训练。
          </strong>
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:1.7465in;">
         <ul>
          <li>
           生成的图片逼真
          </li>
          <li>
           数学可解释性强
          </li>
          <li>
           训练成本低、速度快
          </li>
         </ul>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:3.4687in;">
         <p style="margin-left:0;">
         </p>
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <hr/>
    <p>
    </p>
    <h2 id="%E4%BA%8C.%20Stable%20Diffusion%20%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B" style="margin-left:.0001pt;">
     二. Stable Diffusion 稳定扩散模型
    </h2>
    <h3>
     1. 介绍
    </h3>
    <p>
     <span style="color:#000000;">
      Stable Diffusion 是Stability AI公司于 2022 年10月发布的深度学习文字到图像生成模型。它主要用于根据文字的描述产生详细图像，能够在几秒钟内创作出令人惊叹的艺术作品。Stable Diffusion的
     </span>
     <strong>
      <span style="color:#0070c0;">
       源代码和模型权重已分别公开发布在
      </span>
     </strong>
     <strong>
      <span style="color:#0070c0;">
       GitHub
      </span>
     </strong>
     <strong>
      <span style="color:#0070c0;">
       和
      </span>
     </strong>
     <strong>
      <span style="color:#0070c0;">
       Hugging Face
      </span>
     </strong>
     <span style="color:#000000;">
      ，它的
     </span>
     <strong>
      <span style="color:#0070c0;">
       参数量只有
      </span>
     </strong>
     <strong>
      <span style="color:#0070c0;">
       1B
      </span>
     </strong>
     <strong>
      <span style="color:#0070c0;">
       左右
      </span>
     </strong>
     <span style="color:#000000;">
      ，可以在大多数配备有适度
     </span>
     <span style="color:#000000;">
      GPU
     </span>
     <span style="color:#000000;">
      的电脑硬件上运行。
     </span>
    </p>
    <div>
     <table cellspacing="0">
      <tbody>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:90px;">
         <p>
          训练数据集
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:599px;">
         <p style="margin-left:0;">
          <strong>
           LAION-5B
          </strong>
          是一个公开的数据集，源自网络上抓取的图片-标题数据，这是一个由6亿张带标题的图片组成的子集。这个最终的子集也排除了低分辨率的图像和被人工智能识别为带有水印的图像。对该模型的训练数据进行的第三方分析发现，在从所使用的原始更广泛的数据集中抽取的1200万张图片的较小子集中，大约47%的图像样本量来自100个不同的网站
         </p>
        </td>
       </tr>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:90px;">
         <p style="margin-left:0;">
          训练成本
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:599px;">
         <p style="margin-left:0;">
          亚马逊云计算服务平台，256 x NV A100 GPU, 15万个GPU小时(单卡约73h=3day)，成本为60万美元
         </p>
        </td>
       </tr>
       <tr>
        <td style="border-color:#a3a3a3;vertical-align:top;width:90px;">
         <p style="margin-left:0;">
          发行版本
         </p>
        </td>
        <td style="border-color:#a3a3a3;vertical-align:top;width:599px;">
         <p style="margin-left:0;">
          <img alt="" height="262" src="https://i-blog.csdnimg.cn/blog_migrate/b523bf8cb1a3cbb6afc078a5ece3648c.png" width="691"/>
         </p>
         <p style="margin-left:0;">
         </p>
        </td>
       </tr>
      </tbody>
     </table>
     <h4 id="1.1%20%E6%96%87%E7%94%9F%E5%9B%BE%E5%8A%9F%E8%83%BD%EF%BC%88Txt2Img)">
      1.1 文生图功能（Txt2Img)
     </h4>
     <p>
      <span style="color:#0070c0;">
       <strong>
        <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="nofollow" title="Stable Diffusion">
         Stable Diffusion
        </a>
       </strong>
      </span>
      <span style="color:#0070c0;">
       <strong>
        <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion" rel="nofollow" title="演示">
         演示
        </a>
       </strong>
      </span>
     </p>
     <p>
      <img alt="" height="218" src="https://i-blog.csdnimg.cn/blog_migrate/31a0b102b33dd387ef81726c53917eed.png" width="265"/>
     </p>
     <h4 id="1.2%20%E5%9B%BE%E7%94%9F%E5%9B%BE%E5%8A%9F%E8%83%BD%EF%BC%88Img2Img)">
      1.2 图生图功能（Img2Img)
     </h4>
     <p style="text-align:center;">
      <img alt="" class="left" height="250" src="https://i-blog.csdnimg.cn/blog_migrate/e13645712d0a6a0bddd2965cdc7f9fbf.jpeg" width="323"/>
     </p>
     <hr/>
     <p>
     </p>
    </div>
    <h3 id="2.%20%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84">
     2. 技术架构
    </h3>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#ff0000;">
      <strong>
       Stable
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       Diffusion
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       是潜在扩散模型
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       Latent Diffusion Model(
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       简称
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       LDM
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       ）的一种变体。
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <strong>
      Stable Diffusion = CLIP Text Encoder + Latent Diffusion(VAE+Diffusion)
     </strong>
    </p>
    <p style="margin-left:0in;text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/bc75bc3ef9c65a7b452d85601a650a62.png"/>
    </p>
    <p style="margin-left:0in;text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/6e066ff6aace27cc9a18230d93c3be1f.png"/>
    </p>
    <p>
    </p>
    <hr/>
    <h4 id="step1%20CLIP%20Text%20Encoder" style="margin-left:0in;">
     <span style="color:#fe2c24;">
      step1 CLIP Text Encoder
     </span>
    </h4>
    <p>
     <strong>
      SD只用到了CLIP模型的Text Encoder预训练模型，权重固定，它会将文本编码成语义向量，该语义向量对应一个图像。
     </strong>
    </p>
    <p>
    </p>
    <h5 id="(1)%20CLIP%E4%BB%8B%E7%BB%8D">
     <strong>
      (1) CLIP介绍
     </strong>
    </h5>
    <ul>
     <li>
      全称：
      <span style="color:#000000;">
       Contrastive
      </span>
      <span style="color:#000000;">
       Languange
      </span>
      <span style="color:#000000;">
       -Image
      </span>
      <span style="color:#000000;">
       Pre-Training
      </span>
      ，是
      <span style="color:#000000;">
       OpenAI
      </span>
      <span style="color:#000000;">
       2021.1发布的基于对比学习的文图多模态模型，核心
      </span>
      <span style="color:#000000;">
       是“
      </span>
      <span style="color:#0d0016;">
       <strong>
        Connecting text and images
       </strong>
      </span>
      <span style="color:#000000;">
       ”
      </span>
      。
     </li>
     <li>
      训练数据集：
      <span style="color:#000000;">
       WIT(WebImage Text，OpenAI
      </span>
      <span style="color:#000000;">
       自己网页爬虫创建的一个
      </span>
      <span style="color:#0070c0;">
       <strong>
        超过
       </strong>
      </span>
      <span style="color:#0070c0;">
       <strong>
        4
       </strong>
      </span>
      <span style="color:#0070c0;">
       <strong>
        亿图像
       </strong>
      </span>
      <span style="color:#0070c0;">
       <strong>
        -
       </strong>
      </span>
      <span style="color:#0070c0;">
       <strong>
        文本对
       </strong>
      </span>
      <span style="color:#000000;">
       的
      </span>
      <span style="color:#000000;">
       数据
      </span>
      <span style="color:#000000;">
       集)
      </span>
     </li>
     <li>
      模型组成：
      <ul>
       <li>
        <span style="color:#000000;">
         基于
        </span>
        <span style="color:#000000;">
         Transformer
        </span>
        <span style="color:#000000;">
         的
        </span>
        <span style="color:#000000;">
         Text Encoder
        </span>
       </li>
       <li>
        <span style="color:#000000;">
         基于
        </span>
        <span style="color:#000000;">
         CNN/VIT
        </span>
        <span style="color:#000000;">
         的
        </span>
        <span style="color:#000000;">
         Image Encoder
        </span>
        <span style="color:#000000;">
         两个模型
        </span>
       </li>
      </ul>
     </li>
    </ul>
    <p>
    </p>
    <h5 id="(2)%20CLIP%E6%9E%B6%E6%9E%84">
     (2) CLIP架构
    </h5>
    <p>
     <span style="color:#000000;">
      CLIP会提取文本特征和图像特征，
     </span>
     <span style="color:#000000;">
      通过对比学习，计算
     </span>
     <span style="color:#0d0016;">
      <strong>
       文本特征和图像特征的余弦相似性（cosine similarity），让模型学习到文本和图像的匹配关系。
      </strong>
     </span>
    </p>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/12de45acb804afc862207eaad264756b.png"/>
    </p>
    <p>
     SD只用到了步骤（1）的成果：
    </p>
    <ol>
     <li>
      <span style="color:#000000;">
       CLIP是自监督模型，就是说，CLIP不像传统的视觉模型，如Resnet系列模型，需要用标注好类别的图像进行训练，CLIP使用的数据集本身带有文本-图片的对应关系。
      </span>
     </li>
     <li>
      <span style="color:#000000;">
       前向推理时，N组图片
      </span>
      <span style="color:#000000;">
       -
      </span>
      <span style="color:#000000;">
       文本数据，分别编码得到语义特征和图片特征后，计算其相似度，得到
      </span>
      <span style="color:#000000;">
       NxN
      </span>
      <span style="color:#000000;">
       个预测值，表明文本特征和每一个图片特征的相似度
      </span>
     </li>
     <li>
      <span style="color:#000000;">
       若对角线是匹配的文本-
      </span>
      <span style="color:#000000;">
       图像对，则记为正样本，其他位置则记为不匹配的负样本；
      </span>
     </li>
     <li>
      定义对比损失函数Contrastive Loss，计算正负样本的相似度，得到损失函数的结果值（这个损失函数的目标是:最大化正样本对的相似度, 同时最小化负样本对的相似度——计算原理暂未研究）
     </li>
     <li>
      对损失函数进行求导，计算梯度，用于指导参数的更新方向
     </li>
     <li>
      更新模型参数，迭代优化
     </li>
    </ol>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/c5cad4ed8bc5d9c9fd86d8f04e22fefb.png"/>
    </p>
    <p>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <strong>
      <span style="color:#000000;">
       对比学习预训练学习的是
      </span>
     </strong>
     <span style="color:#00b0f0;">
      <strong>
       整个句子
      </strong>
     </span>
     <span style="color:#000000;">
      <strong>
       与其描述的图像之间的关系，而不是像猫、狗、树等
      </strong>
     </span>
     <span style="color:#000000;">
      <strong>
       单一
      </strong>
     </span>
     <span style="color:#000000;">
      <strong>
       类别；比如传统的视觉模型，如Resnet系列模型，ResNet使用的是有监督学习，训练集是经过标注的图片，标注信息就是图片的分类，这个信息相比CLIP而言就很单一了。
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/8c407f443c36f5b20ffee93c3f1087f3.png#pic_center"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <strong>
      <span style="color:#00b0f0;">
       当在整个句子上进行训练时，模型可以学习更多的东西（而非单一的类别），找到图像和文本之间的关系。
      </span>
      <span style="color:#0d0016;">
       CLIP在图像分类等比赛中的表现，也证明了CLIP对于文本-图像对比学习的优越性。
      </span>
     </strong>
    </p>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/ea7e9cb4eb200593f4e4dab5580fcc94.png"/>
    </p>
    <p style="text-align:center;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#fe2c24;">
      <strong>
       所以，我们若问SD为什么选择了CLIP Text Ecnoder作为其文本编码器？
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#0d0016;">
      <strong>
       CLIP开创性地提出训练模型学习图像和文本之间的联系，并在各项视觉比赛中证明了其优越性。CLIP预训练模型的Text Encoder编码处理的语义向量，有对应近似的图像特征向量，便于跨模态处理图像相关的下游任务，如图像生成等。
      </strong>
     </span>
    </p>
    <p>
    </p>
    <hr/>
    <h4 id="step2%20LDM%E4%B9%8BVAE%20Encoder" style="background-color:transparent;margin-left:0in;">
     <span style="color:#fe2c24;">
      step2 LDM之VAE Encoder
     </span>
    </h4>
    <p>
     <img alt="" height="185" src="https://i-blog.csdnimg.cn/blog_migrate/3b6e194ce002e9fb829a2ffbaaff744c.png" width="756"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      VAE全称：
     </span>
     <span style="color:#000000;">
      Varitional
     </span>
     <span style="color:#000000;">
      AutoEncoder
     </span>
     <span style="color:#000000;">
      (VAE)
     </span>
     <span style="color:#000000;">
      变分自编码器
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      VAE思路：
     </span>
     <span style="color:#000000;">
      VAE
     </span>
     <span style="color:#000000;">
      将输入转换成
     </span>
     <span style="color:#000000;">
      Latent
     </span>
     <span style="color:#000000;">
      空间的概率分布，如标准高斯分布；
     </span>
     <br/>
     <span style="color:#000000;">
      SD
     </span>
     <span style="color:#000000;">
      的
     </span>
     <span style="color:#000000;">
      Latent Space
     </span>
     <span style="color:#000000;">
      为
     </span>
     <span style="color:#000000;">
      4x64x64,
     </span>
     <span style="color:#000000;">
      比图像像素空间
     </span>
     <span style="color:#000000;">
      3x512x512
     </span>
     <span style="color:#000000;">
      小
     </span>
     <span style="color:#000000;">
      48
     </span>
     <span style="color:#000000;">
      倍，减少空间占用，加速训练
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     VAE组成：Encoder + decoder
     <span style="color:#000000;">
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <img alt="" height="261" src="https://i-blog.csdnimg.cn/blog_migrate/3c212b4ad7d9ef29106270174419ad5c.png" width="419"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <strong>
      <span style="color:#000000;">
       Encoder编码器？
      </span>
     </strong>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      将图像压缩为潜空间中的低维表示，这会使图像损失一部分无用信息，保留主要的特征信息。
     </span>
     <br/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      <strong>
       为什么不会丢失关键信息？
      </strong>
     </span>
     <br/>
     <span style="color:#000000;">
      自然图像不是随机的数据，它们具有很高的规律性，如：面部遵循眼睛、脸颊和嘴巴之间的特定空间关系。下面是基于CNN的VAE下采样（Encoder)，主要使用卷积（conv)和池化（pool)操作构建编码器，从这两个操作说明为什么不会丢失重要特征。（
     </span>
     <a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" rel="nofollow" title="cnn">
      cnn
     </a>
     <span style="color:#000000;">
      ）
     </span>
    </p>
    <p style="margin-left:0;">
     <img alt="" height="249" src="https://i-blog.csdnimg.cn/blog_migrate/96321f2ac4ca264251a9921c738f21b9.png" width="650"/>
    </p>
    <ol>
     <li style="margin-left:0px;">
      conv捕捉特征的框大小是filter_size,一般就是channel 2-7, 也就是说，在原图上划分N个 2x2~7x7的方格，每个小方格中各像素点分别乘以权重，然后累加得到一个特征值。经过训练的权重weight可以分辨小方格中哪个像素点比较有价值，所以可以把最重要的特征信息提取出来，比如说轮廓信息，主要的颜色分布等。
     </li>
     <li style="margin-left:0px;">
      pool用于进一步缩小特征图，pool操作也是在特征图上划分N个小方格，然后通过max/mean等方式取小方格中最大或者均值来代表这个小方格的特征，我们知道，一张图如果放大，某个像素点周围的点大部分不会突变，比如一个2x2的小方格，一共有四个像素点，很多时候，这四个点颜色是渐变/一样的，对应该像素的数据值，就是一个点255，一个点254的区别，对于单channel的灰度图，254和255也就是一个不太白，一个纯白的却别，但是主要的颜色特征还是保留下来的。
     </li>
    </ol>
    <p style="margin-left:0px;">
    </p>
    <p style="margin-left:0px;">
    </p>
    <p style="margin-left:0px;">
    </p>
    <hr/>
    <h4 id="step3%20LDM%E4%B9%8BDiffusion">
     <span style="color:#fe2c24;">
      <strong>
       step3
      </strong>
      LDM之
      <strong>
       Diffusion
      </strong>
     </span>
    </h4>
    <p>
     <span style="color:#4da8ee;">
      <strong>
       扩散模型的目的：学习从噪声生成图片的方法
      </strong>
     </span>
    </p>
    <p>
    </p>
    <p>
     先宏观了解LDM做了什么（下为图生图样例，有图片输入+关键字）
    </p>
    <ol>
     <li>
      <span style="color:#000000;">
       隐向量通过
      </span>
      <span style="color:#00b0f0;">
       <strong>
        前向扩散
       </strong>
      </span>
      <span style="color:#000000;">
       增加噪声
      </span>
      <span style="color:#000000;">
       ，
      </span>
      <span style="color:#000000;">
       将
      </span>
      <span style="color:#000000;">
       图像数据点的复杂分布逐渐转为简单分布
      </span>
     </li>
     <li>
      <span style="color:#000000;">
       通过CLIP Text Encoder
      </span>
      <span style="color:#000000;">
       编码出来的
      </span>
      <span style="color:#00b0f0;">
       <strong>
        语义向量作为监督信号
       </strong>
      </span>
      <span style="color:#000000;">
       作用
      </span>
      <span style="color:#000000;">
       到
      </span>
      <span style="color:#000000;">
       <strong>
        去
       </strong>
      </span>
      <span style="color:#000000;">
       <strong>
        噪过程
       </strong>
      </span>
      <span style="color:#000000;">
       中
      </span>
     </li>
     <li>
      <span style="color:#000000;">
       噪声向量
      </span>
      <span style="color:#000000;">
       通过
      </span>
      <span style="color:#00b0f0;">
       <strong>
        反向扩散
       </strong>
      </span>
      <span style="color:#000000;">
       去除噪声，得到图像最终的隐
      </span>
      <span style="color:#000000;">
       向量
      </span>
     </li>
    </ol>
    <p style="text-align:center;">
     <span style="color:#000000;">
      <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/55ba8b7eaf840610752e07333162d58f.png"/>
     </span>
    </p>
    <p style="text-align:center;">
     <span style="color:#000000;">
      <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/10591a8ac23a4f98cc72ad8bd038fe71.png"/>
     </span>
    </p>
    <p>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      <a class="has-card" href="https://stable-diffusion-art.com/wp-content/uploads/2022/12/cat_euler_15.gif" rel="nofollow" title="动图示例">
       <span class="link-card-box">
        <span class="link-title">
         动图示例
        </span>
        <span class="link-link">
         <img alt="icon-default.png?t=N7T8" class="link-link-icon" src="https://i-blog.csdnimg.cn/blog_migrate/003a2ce7eb50c2e24a8c624c260c5930.png"/>
         https://stable-diffusion-art.com/wp-content/uploads/2022/12/cat_euler_15.gif
        </span>
       </span>
      </a>
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      <strong>
       <span style="background-color:#efedf6;">
        Q:
       </span>
      </strong>
     </span>
     <span style="color:#000000;">
      <strong>
       <span style="background-color:#efedf6;">
        图像逐渐从噪声中转变而来，这一过程是如何实现的呢？
       </span>
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      <strong>
       <span style="background-color:#efedf6;">
        A:
       </span>
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       <span style="background-color:#efedf6;">
        训练噪声预测器
       </span>
      </strong>
     </span>
    </p>
    <p>
    </p>
    <p>
    </p>
    <h5 id="(1)%20%E5%89%8D%E5%90%91%E6%89%A9%E6%95%A3%EF%BC%9A%E5%8A%A0%E5%99%AA">
     (1) 前向扩散：加噪
    </h5>
    <p>
     <span style="color:#000000;">
      向图像逐渐添加
     </span>
     <span style="color:#ff0000;">
      <strong>
       高斯噪声以破坏图像原始的特征
      </strong>
     </span>
     <span style="color:#000000;">
      ，直到图像
     </span>
     <span style="color:#ff0000;">
      <strong>
       完全无法识别
      </strong>
     </span>
     <span style="color:#000000;">
      。这个过程就像
     </span>
     <span style="color:#000000;">
      一滴
     </span>
     <span style="color:#000000;">
      墨水滴入了一杯水中，墨水在水里
     </span>
     <span style="color:#000000;">
      diffuses(
     </span>
     <span style="color:#000000;">
      扩散）
     </span>
     <span style="color:#000000;">
      .
     </span>
     <span style="color:#000000;">
      几分钟后，墨水会随机分散并融入水里。数据通过逐步添加噪声，
     </span>
     <u>
      <span style="color:#000000;">
       从一个真实图像的
      </span>
     </u>
     <u>
      <span style="color:#ff0000;">
       <strong>
        复杂分布逐渐过渡到
       </strong>
      </span>
     </u>
     <u>
      <span style="color:#000000;">
       噪点图的
      </span>
     </u>
     <u>
      <span style="color:#ff0000;">
       <strong>
        简单分布（符合高斯分布）
       </strong>
      </span>
     </u>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <ul>
     <li>
      <strong>
       <span style="color:#000000;">
        为什么添加高斯噪声？
       </span>
      </strong>
     </li>
    </ul>
    <p>
     <span style="color:#000000;">
      高斯噪声是一种正太分布的噪声，正态分布在自然界中广泛存在，如人的身高、体重、智商等都可以用正态分布来描述。因此使用高斯噪声可以称为一个正确的基本假设，符合真实世界规律。
     </span>
    </p>
    <p>
     <img alt="" height="192" src="https://i-blog.csdnimg.cn/blog_migrate/a1439baa36d67ed030bf34e20eb258dd.png" width="788"/>
    </p>
    <p>
    </p>
    <ul>
     <li>
      <strong>
       如何添加噪声？
      </strong>
     </li>
    </ul>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/6d8e40d37abe64c29a98560928e84272.jpeg"/>
    </p>
    <ul>
     <li>
      <strong>
       如何反向去除噪声？
      </strong>
     </li>
    </ul>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      与
     </span>
     <span style="color:#000000;">
      正向过程不同，不能使用
     </span>
     <span style="color:#000000;">
      q(xₜ₋₁|xₜ)
     </span>
     <span style="color:#000000;">
      来反转噪声，因为它是难以处理的
     </span>
     <span style="color:#000000;">
      (
     </span>
     <span style="color:#000000;">
      无法计算
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      。所以我们需要训练神经网络
     </span>
     <span style="color:#000000;">
      pθ
     </span>
     <span style="color:#000000;">
      (xₜ₋₁|xₜ)
     </span>
     <span style="color:#000000;">
      来近似
     </span>
     <span style="color:#000000;">
      q(xₜ₋₁|xₜ)
     </span>
     <span style="color:#000000;">
      。近似
     </span>
     <span style="color:#000000;">
      pθ
     </span>
     <span style="color:#000000;">
      (xₜ₋₁|xₜ)
     </span>
     <span style="color:#000000;">
      服从
     </span>
     <span style="color:#000000;">
      正态分布。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <strong>
      <span style="color:#fe2c24;">
       小结
      </span>
     </strong>
     ：
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      1. LDM前
     </span>
     <span style="color:#000000;">
      向扩散可以用封闭形式的固定公式计算
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      2. LDM反向扩散可以用
     </span>
     <u>
      <span style="color:#000000;">
       <strong>
        训练好的神经网络
       </strong>
      </span>
     </u>
     <span style="color:#000000;">
      来完成（
     </span>
     <span style="color:#000000;">
      Noising Predictor, U-Net)
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      为了近似所需的去噪步骤，我们只需要使用神经网络推理结果近似预期
     </span>
     <span style="color:#000000;">
      噪声
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#ff0000;">
      <strong>
       因此
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       噪声预测
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       器实际训练的就是反向扩散的
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       U-Net
      </strong>
     </span>
     <span style="color:#ff0000;">
      <strong>
       网络 ！！
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p>
    </p>
    <h5 id="(2)%20%E5%8F%8D%E5%90%91%E6%89%A9%E6%95%A3%EF%BC%9A%E5%8E%BB%E5%99%AA">
     (2) 反向扩散：去噪
    </h5>
    <p>
     前面已经讲了，反向扩散要想像时光倒流一样，将噪点图逐渐恢复到原图，需要教会该神经网络预测（1）中添加的噪声，然后从前向扩散得到的符合高斯分布的噪声矩阵中，连续减去预测噪声，最终恢复到原图。这个神经网络就是U-Net网络。
    </p>
    <p>
     <img alt="" height="178" src="https://i-blog.csdnimg.cn/blog_migrate/6e4e18a5c5832a958b7c71a947f8de56.png" width="328"/>
    </p>
    <p>
     <img alt="" height="274" src="https://i-blog.csdnimg.cn/blog_migrate/5c99750637f04349cd1c380f1236759d.png" width="579"/>
    </p>
    <h6 id="a)%C2%A0SD%E6%A0%B8%E5%BF%83%EF%BC%9AU-Net">
     a) SD核心：U-Net
    </h6>
    <p>
     从上图中放大UNet模块：
    </p>
    <p>
     <img alt="" height="280" src="https://i-blog.csdnimg.cn/blog_migrate/77d6bcc965af1bfea55cc4b9021b919b.png" width="640"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      U-Net主要对
     </span>
     <span style="color:#000000;">
      Forward Diffusion
     </span>
     <span style="color:#000000;">
      输出的高斯噪声矩阵进行迭代降噪，并且每次都使用
     </span>
     <span style="color:#000000;">
      CLIP Text-encoder
     </span>
     <span style="color:#000000;">
      的文本特征向量
     </span>
     <span style="color:#000000;">
      +
     </span>
     <span style="color:#000000;">
      timesteps
     </span>
     <span style="color:#000000;">
      作为条件控制来预测噪声，然后在高斯噪声矩阵上去除预测噪声，经过多次迭代后，将高斯噪声矩阵转换成图片的
     </span>
     <span style="color:#000000;">
      Latent特征。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      U-Net原本的结构
     </span>
    </p>
    <p style="margin-left:0in;text-align:center;">
     <span style="color:#000000;">
      <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/aa7594d49c229a13ba7dd3901aac476b.png"/>
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      SD中的
     </span>
     <span style="color:#000000;">
      U-Net
     </span>
     <span style="color:#000000;">
      ，在原本
     </span>
     <span style="color:#000000;">
      Encoder-Decoder（下采样-上采样）
     </span>
     <span style="color:#000000;">
      结构基础上，增加了新模块：
     </span>
    </p>
    <ol>
     <li style="margin-left:0in;text-align:left;">
      <span style="color:#000000;">
       Time Embedding 时间编码
      </span>
      <br/>
      <span style="color:#000000;">
       将时间信息映射到一
      </span>
      <span style="color:#000000;">
       个连续的向量空间，使得时间之间的关系可以被模型学习和利用。这些
      </span>
      <span style="color:#000000;">
       时间嵌入帮助神经网络获得图像当前处于哪个状态（步骤）的某些信息。这对于了解图像中当前是否存在更多或更少的噪声很有用，从而使模型减去更多或更少的噪声。
      </span>
     </li>
     <li style="margin-left:0in;text-align:left;">
      <span style="color:#000000;">
       Cross
      </span>
      <span style="color:#000000;">
       Attenion
      </span>
      <span style="color:#000000;">
       模块（交叉注意力机制）
      </span>
     </li>
     <li style="margin-left:0in;text-align:left;">
      <span style="color:#000000;">
       Self-Attention模块（自注意力机制）
      </span>
     </li>
    </ol>
    <p style="margin-left:0in;text-align:left;">
     <img alt="" height="356" src="https://i-blog.csdnimg.cn/blog_migrate/25b12743d041d2ead0375e0a06c81a4c.png" width="661"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <h6 id="b)%C2%A0Transformer2DModel%3A%20%E5%8A%A0%E5%85%A5%E6%9D%A1%E4%BB%B6%E6%8E%A7%E5%88%B6" style="margin-left:0in;text-align:left;">
     b)
     <span style="color:#000000;">
      <strong>
       Transformer2DModel:
      </strong>
     </span>
     <span style="color:#000000;">
      <strong>
       加入条件控制
      </strong>
     </span>
    </h6>
    <p>
     <span style="color:#000000;">
      先说一下注意力机制中QKV的作用：
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      举个例子，我在网页上输入关键字“Query”来搜索某些信息，网站会根据Q去数据库查询相关联的“Key”, 然后返回给我”Key”对应的“Value”。
     </span>
    </p>
    <p>
     <span style="color:#24292f;">
      更通俗的说法可以是：
     </span>
     <span style="color:#24292f;">
      QKV
     </span>
     <span style="color:#24292f;">
      模式就像是你在找答案时，先提出问题（
     </span>
     <span style="color:#24292f;">
      Query
     </span>
     <span style="color:#24292f;">
      ），然后根据问题找到相关的关键信息（
     </span>
     <span style="color:#24292f;">
      Key
     </span>
     <span style="color:#24292f;">
      ），最终得到你想要的具体答案或内容（
     </span>
     <span style="color:#24292f;">
      Value
     </span>
     <span style="color:#24292f;">
      ）。
     </span>
    </p>
    <p>
     详见：
     <a href="https://blog.csdn.net/zmj1582188592/article/details/135389511?spm=1001.2014.3001.5501" title="Transformer_transformer qkv不同源-CSDN博客">
      Transformer_transformer qkv不同源-CSDN博客
     </a>
    </p>
    <p>
    </p>
    <p>
     <img alt="" height="580" src="https://i-blog.csdnimg.cn/blog_migrate/378601dadbb7652e9811e9ce769d823e.png" width="594"/>
    </p>
    <p style="margin-left:0;">
     <span style="color:#191b1f;">
      SD的U-Net既用到了自注意力，也用到了交叉注意力。
     </span>
    </p>
    <ul>
     <li style="margin-left:0px;">
      <strong>
       <span style="color:#ff0000;">
        Self-Attention
       </span>
      </strong>
      <strong>
       <span style="color:#191b1f;">
        用于图像特征自己内部信息聚合
       </span>
      </strong>
     </li>
     <li style="margin-left:0px;">
      <strong>
       <span style="color:#ff0000;">
        Cross-Attention
       </span>
      </strong>
      <strong>
       <span style="color:#191b1f;">
        用于让生成图像对齐文本，其
       </span>
      </strong>
      <strong>
       <span style="color:#191b1f;">
        Q
       </span>
      </strong>
      <strong>
       <span style="color:#191b1f;">
        来自图像特征，
       </span>
      </strong>
      <strong>
       <span style="color:#191b1f;">
        K,V
       </span>
      </strong>
      <strong>
       <span style="color:#191b1f;">
        来自文本编码。
       </span>
      </strong>
      <span style="color:#191b1f;">
       我们知道
      </span>
      <span style="color:#191b1f;">
       CLIP Text Encoder
      </span>
      <span style="color:#191b1f;">
       编码出来的文本编码跟其对应的图像向量是近似的，所以这个
      </span>
      <strong>
       <span style="color:#191b1f;">
        文本编码本身也对应表示
       </span>
      </strong>
      <strong>
       <u>
        <span style="color:#191b1f;">
         一幅图像
        </span>
       </u>
      </strong>
      <span style="color:#191b1f;">
       。
      </span>
      <span style="color:#191b1f;">
       Cross-attention
      </span>
      <span style="color:#191b1f;">
       将
      </span>
      <span style="color:#191b1f;">
       KV
      </span>
      <span style="color:#191b1f;">
       替换成来自文本编码的
      </span>
      <span style="color:#191b1f;">
       KV,
      </span>
      <span style="color:#191b1f;">
       就可以关联原图像和关键字，生成的图片会既和原本生成的图相似，也会和参考图像相似。（个人理解）
      </span>
     </li>
    </ul>
    <h5 id="(3)%20%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83%E5%99%AA%E5%A3%B0%E9%A2%84%E6%B5%8B%E5%99%A8%EF%BC%9F">
     (3) 如何训练噪声预测器？
    </h5>
    <p>
     <img alt="" height="364" src="https://i-blog.csdnimg.cn/blog_migrate/c15d27ea515a82c41ddefc0b4172d85d.png" width="566"/>
    </p>
    <p>
     （不知道为啥，上传清晰的图片总是失败。。。图只能这么糊了，大致也能分清字）
    </p>
    <p>
    </p>
    <p>
    </p>
    <hr/>
    <p>
    </p>
    <h4 id="step4%20LDM%E4%B9%8BVAE%20Decoder">
     <span style="color:#fe2c24;">
      step4 LDM之VAE Decoder
     </span>
    </h4>
    <p>
     看看step2就行了，知道这一步是将 去噪后的矩阵解码回像素空间就行。具体VAE的原理没有过多研究。
    </p>
    <p>
    </p>
    <p style="margin-left:0in;">
    </p>
    <h2 id="%E4%B8%89.%20%E5%8F%82%E8%80%83" style="margin-left:0in;">
     三. 参考
    </h2>
    <ol>
     <li>
      <a href="https://pdf.dfcfw.com/pdf/H3_AP202303021583976226_1.pdf?1677783639000.pdf" rel="nofollow" title="中国信通院-京东探索研究所-人工智能生成内容（AIGC）白皮书（2022年）.pdf">
       中国信通院-京东探索研究所-人工智能生成内容（AIGC）白皮书（2022年）.pdf
      </a>
     </li>
     <li>
      <a href="https://pdf.dfcfw.com/pdf/H3_AP202303021583976226_1.pdf?1677783639000.pdf" rel="nofollow" title="AIGC深度报告：新一轮内容生产力革命的起点（国海证券）.pdf">
       AIGC深度报告：新一轮内容生产力革命的起点（国海证券）.pdf
      </a>
     </li>
     <li>
      <a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQ1NjAzNg==&amp;mid=2247563864&amp;idx=1&amp;sn=917c047706bf7f7d78aa6c0cc5e19b5c&amp;chksm=fb3b5f53cc4cd645f5448d84e9ab42c41eb286f173e0bdc9e7da4b717be0d5285fa4465a48e3&amp;scene=27" rel="nofollow" title="硬核解读Stable Diffusion（完整版）">
       硬核解读Stable Diffusion（完整版）
      </a>
     </li>
     <li>
      <a href="https://zh.wikipedia.org/wiki/Stable_Diffusion" rel="nofollow" title="https://zh.wikipedia.org/wiki/Stable_Diffusion">
       https://zh.wikipedia.org/wiki/Stable_Diffusion
      </a>
     </li>
     <li>
      <a href="https://www.zhangzhenhu.com/aigc/%E7%A8%B3%E5%AE%9A%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B.html" rel="nofollow" title="7. 稳定扩散模型（Stable diffusion model） — 张振虎的博客 张振虎 文档">
       7. 稳定扩散模型（Stable diffusion model） — 张振虎的博客 张振虎 文档
      </a>
     </li>
     <li>
      <a href="https://zhuanlan.zhihu.com/p/632809634" rel="nofollow" title="深入浅出完整解析Stable Diffusion（SD）核心基础知识">
       深入浅出完整解析Stable Diffusion（SD）核心基础知识
      </a>
     </li>
     <li>
      <a href="https://blog.csdn.net/m0_51976564/article/details/134356352" title="CLIP：用文本作为监督信号训练可迁移的视觉模型">
       CLIP：用文本作为监督信号训练可迁移的视觉模型
      </a>
     </li>
     <li>
      <a href="https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2" rel="nofollow" title="OpenAI CLIP模型的简单实现：教程">
       OpenAI CLIP模型的简单实现：教程
      </a>
     </li>
     <li>
      <a href="https://zhuanlan.zhihu.com/p/493489688" rel="nofollow" title="神器CLIP：连接文本和图像，打造可迁移的视觉模型">
       神器CLIP：连接文本和图像，打造可迁移的视觉模型
      </a>
     </li>
     <li>
      <a href="https://zhuanlan.zhihu.com/p/618862789" rel="nofollow" title="【Stable Diffusion】之原理篇">
       【Stable Diffusion】之原理篇
      </a>
     </li>
     <li>
      <a href="https://zhuanlan.zhihu.com/p/683237596" rel="nofollow" title="LDM（Latent Diffusion Model）详解">
       LDM（Latent Diffusion Model）详解
      </a>
     </li>
     <li>
      <a href="https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&amp;mid=2247617834&amp;idx=1&amp;sn=2b8f0f56b8b5b25e5ba1240e5705a6a8&amp;chksm=e9e004a1de978db79be4b5d01829959efb839852f818fabc14e5020c0f26b132e5f044c3bf8f&amp;scene=27" rel="nofollow" title="stable diffusion原理解读通俗易懂，史诗级万字爆肝长文！">
       stable diffusion原理解读通俗易懂，史诗级万字爆肝长文！
      </a>
     </li>
     <li>
      <a href="https://blog.csdn.net/mingzai624/article/details/132486118" title="Stable Diffusion 文生图技术原理_stable diffusion csdn-CSDN博客">
       Stable Diffusion 文生图技术原理_stable diffusion csdn-CSDN博客
      </a>
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f7a6d6a313538323138383539322f:61727469636c652f64657461696c732f313337313534313635" class_="artid" style="display:none">
 </p>
</div>


