---
layout: post
title: 2024-12-26-多模态机器学习概述及其音视频融合总结
date: 2024-12-26 10:07:27 +08:00
categories: ['Research']
tags: ['机器学习', '多模态融合阶段总结', '神经网络', '多模态融合综述', '多模态融合']
image:
  path: https://img-blog.csdnimg.cn/2020071318203386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXBlbmcxOTk3MDExOQ==,size_16,color_FFFFFF,t_70,image/resize,m_fixed,h_150
  alt: 多模态机器学习概述及其音视频融合总结
artid: 106504992
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=106504992
featuredImagePreview: https://bing.ee123.net/img/rand?artid=106504992
---

# 多模态机器学习概述及其音视频融合总结

#### 文章目录

* [前言](#_3)
* [综述总结](#_48)
* + [摘要](#_52)
  + - [1.1 介绍](#11__54)
    - [1.2 多模态机器学习研究方向](#12__71)
    - [1.3 多模态机器学习发展和应用](#13__91)
  + [多模态表示](#_111)
  + - [联合表示](#_122)
    - * [神经网络](#_127)
      * [图形模型](#_131)
      * [序列模型](#_142)
    - [协同表示](#_150)
    - [章节小结](#_162)
  + [翻译](#_170)
  + [对齐挑战](#_176)
  + [融合](#_184)
  + [协同学习](#_190)
  + [综述总结](#_198)
  + [参考文献](#_202)
* [多模态融合应用场景论文分析](#_249)
* + [音视频双模态情感识别融合框架研究](#_252)
  + - [摘要](#_258)
    - [内容](#_266)
    - [总结](#_282)
  + [音视频双模态情感识别融合框架研究](#_292)
  + - [摘要](#_298)
    - [内容](#_306)
    - [总结](#_328)
  + [基于情绪基调的音视频双模态情绪识别算法](#_338)
  + - [摘要](#_340)
    - [内容](#_348)
    - [总结](#_360)
  + [基于音视频特征融合的暴力镜头识别](#_369)
  + - [摘要](#_375)
    - [内容](#_381)
    - [总结](#_408)
  + [End-to-End Multimodal Emotion Recognition using Deep Neural Networks](#EndtoEnd_Multimodal_Emotion_Recognition_using_Deep_Neural_Networks_415)
  + - [摘要](#_416)
    - [内容](#_420)
    - [总结](#_426)
* [总结](#_435)

## 前言

阅读了不少专题参考文献和综述后，自己对多模态的理解进一步加深。同时我针对多模态中的音视频融合也有了更深的理解。在此我做个多模态融合和音视频融合总结，概述我阅读的几个文献的核心内容，同时针对一些没有阅读到的，但比较有参考价值和重大意义的文献进行引用。我希望通过这篇文章总结能开拓我以后做研究的视野和方向，希望投稿论文别再出现如对该领域理解不够深刻的修改意见。

文章主要内容来源：
  
知乎：

多模态机器学习的知乎最高赞回答：
[添加链接描述](https://zhuanlan.zhihu.com/p/53511144)

论文：

1. Baltrušaitis T, Ahuja C, Morency L P. Multimodal machine learning: A survey and taxonomy[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(2): 423-443.
2. Guo W, Wang J, Wang S. Deep multimodal representation learning: A survey[J]. IEEE Access, 2019, 7: 63373-63394.
3. 陈鹏,李擎,张德政,杨宇航,蔡铮,陆子怡.多模态学习方法综述[J].工程科学学报,2020,42(05):557-569.
4. 何俊,张彩庆,李小珍,张德海.面向深度学习的多模态融合技术研究综述[J].计算机工程,2020,46(05):1-11.
5. 宋冠军,张树东,卫飞高.音视频双模态情感识别融合框架研究[J].计算机工程与应用,2020,56(06):140-146.
6. 吴晓雨,顾超男,王生进.多模态特征融合与多任务学习的特种视频分类[J].光学精密工程,2020,28(05):1177-1186.
7. 邵晨智. 基于音视频特征融合的暴力镜头识别方法研究[D].哈尔滨工业大学,2019.
8. Tzirakis P, Trigeorgis G, Nicolaou M A, et al. End-to-end multimodal emotion recognition using deep neural networks[J]. IEEE Journal of Selected Topics in Signal Processing, 2017, 11(8): 1301-1309.
9. Katsaggelos A K, Bahaadini S, Molina R. Audiovisual fusion: Challenges and new approaches[J]. Proceedings of the IEEE, 2015, 103(9): 1635-1653.
10. Hazarika D, Poria S, Mihalcea R, et al. ICON: interactive conversational memory network for multimodal emotion detection[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2018: 2594-2604.
11. 卫飞高,张树东,付晓慧.基于情绪基调的音视频双模态情绪识别算法[J].计算机应用与软件,2018,35(08):238-242.
12. 苏育挺,王蒙蒙,刘婧,费云鹏,何旭.基于多运动特征融合的微表情识别算法[J/OL].激光与光电子学进展:1-11[2020-06-02].

综述1 主要探讨多模态机器学习的一些研究，并按照相关研究挑战进行分类。全文翻译参照
[this](https://zhuanlan.zhihu.com/p/63143789)
.

综述6 是探讨音视频融合中一个模态如何影响另外一个模态，并讨论了时序不同步问题，实际应用时有一个模态丢失问题解决方法，深度学习在av融合中的发展。

综述2 主要总结多模态融合架构、融合方法和对齐技术。重点分析了联合、协同、编解码器三种融合架构在深度学习中的应用情况和优缺点，以及多核学习、图像模型和神经网络等具体融合方法和对齐技术。

## 综述总结

自己研究多模态机器学习的时候对一些经典的工作和主要研究方向了解不够深入。不同的综述论文对这个领域的分类不同，这里我主要参照【1】和其他的综述资料，写下自己的多模态研究综述，希望对自己以后的研究有一个较大的指导。

### 摘要

#### 1.1 介绍

首先，什么叫做模态（Modality）呢？

每一种信息的来源或者形式，都可以称为一种模态。例如，人有触觉，听觉，视觉，嗅觉；信息的媒介，有语音、视频、文字等；多种多样的传感器，如雷达、红外、加速度计等。以上的每一种都可以称为一种模态。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/7bcdef357a2835a2c88874d8b6c405b2.png)
  
同时，模态也可以有非常广泛的定义，比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集（多视角），亦可认为是两种模态。

因此，多模态机器学习，英文全称 MultiModal Machine Learning (MMML)，旨在通过机器学习的方法实现处理和理解多源模态信息的能力。目前比较热门的研究方向是图像、视频、音频、语义之间的多模态学习。

多模态学习从1970年代起步，经历了几个发展阶段，在2010后全面步入Deep Learning阶段。

本文我们将介绍多模态的发展历史和应用、一些主要技术的介绍和经典论文。

#### 1.2 多模态机器学习研究方向

**多模态学习可以划分为以下五个研究方向：**
  
多模态表示学习 Multimodal Representation
  
模态转化 Translation
  
对齐 Alignment
  
多模态融合 Multimodal Fusion
  
协同学习 Co-learning

1）表示基本挑战是学习如何以利用多种模态的互补性和冗余性的方式表示和总结多模态数据。多模态数据的异质性使得构建这种表示具有挑战性。例如，语言通常是符号，而音频和视觉模态将表示为信号。

2）翻译第二个挑战涉及如何将数据从一种模态转换（映射）到另一种模态。不仅数据是异质的，而且模态之间的关系通常是开放式的或主观的。例如，存在许多描述图像的正确方式，并且可能不存在一个完美的翻译。

3）对齐第三个挑战是识别来自两种或更多种不同形式的（子）元素之间的直接关系。例如，我们可能希望将配方中的步骤与显示正在制作的菜肴的视频对齐。为了应对这一挑战，我们需要测量不同模态之间的相似性，并处理可能的长期依赖性和模糊性。

4）融合第四个挑战是加入来自两个或多个模态的信息以执行预测。例如，对于视听语音识别，唇部运动的视觉描述与语音信号融合以预测说出的话语。来自不同模态的信息可能具有变化的预测能力和噪声拓扑，在至少一种模态中可能缺少数据。

5）共同学习第五个挑战是在模态，表示和预测模型之间传递知识。这可以通过小样本学习算法来举例说明。共同学习探索了如何从一种模态学习知识可以帮助计算模型在不同的模态下训练。当其中一种模式具有有限的资源（例如，注释数据）时，该挑战尤其相关。

#### 1.3 多模态机器学习发展和应用

**多模式研究最早的例子之一是视听语音识别（AVSR）[243]。**
  
它的动机是麦格劳克效应[138] - 在言语感知过程中听觉和视觉之间的相互作用。当人类受试者听到音节/ ba-ba /同时观看一个人的嘴唇说/ ga-ga /时，他们会感觉到第三种声音：/ da-da /。这些结果促使来自语言社区的许多研究人员用视觉信息扩展他们的方法。鉴于当时语音社区隐藏马尔可夫模型（HMM）的突出，AVSR的许多早期模型基于各种HMM扩展[24]并不奇怪。虽然目前对AVSR的研究并不常见，但多模态在深度学习社区再次引起了人们的兴趣[151]。虽然AVSR的最初愿景是在所有情况下提高语音识别性能（例如，文字错误率），但实验结果表明视觉信息的主要优点是当语音信号有噪声时（即，低信噪比）比例）[151]，换句话说，模式之间捕获的相互作用是补充而非互补。两者都捕获了相同的信息，提高了多模式模型的稳健性，但没有提高无噪声场景下的语音识别性能。

**第二个重要的多模式应用类别来自多媒体内容索引和检索领域。早期的索引和搜索这些多媒体视频的方法都是基于关键词的[188]，但在尝试直接搜索视觉和多模式内容时出现了新的研究问题。这导致了多媒体内容分析的新研究课题，如自动镜头边界检测视频摘要[53]。这些研究项目得到了美国国家标准与技术研究所的TrecVid计划的支持，该计划引入了许多高质量的数据集，包括2011年开始的多媒体事件检测（MED）任务[1]。

第三类应用是在21世纪初围绕多模式互动的新兴领域建立的，其目标是在社会交往中理解人类多模态行为。视听情绪挑战AVEC的挑战随后每年都在继续，后来的事例包括医疗应用，如自动评估抑郁和焦虑。D’Mello等人发表了关于多模式情感识别最近进展的一个很好的总结。 [50]。他们的分析显示，最近大多数关于多模态情绪识别的研究表明，当使用多模态时，准确率有所改善，但在识别自然发生的情绪时，这种改善会减少。

最近，出现了一种新的多模式应用，强调语言和愿景：媒体描述。最具代表性的应用之一是图像字幕，其任务是生成输入图像的文本描述[83]。这是因为这种系统能够帮助视障人士完成日常工作[20]。媒体描述的主要挑战是评估：如何评估预测描述的质量。最近提出了视觉问答（VQA）的任务，以解决一些评估挑战[9]，其目标是回答关于图像的特定问题。

为了将一些上述应用程序带入现实世界，我们需要解决多模式机器学习所面临的许多技术挑战。我们总结了表1中上述应用领域的相关技术挑战。最重要的挑战之一是多模式表示，这是我们下一部分的重点。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c2675aab8113c4c4c18193ae2b154921.png)

### 多模态表示

好的特征表示对于模型来说是非常关键的，好的特征表示能够大大地提升模型精度。【18】好的特征表示应该具有平滑，空间分开等性质。

特征表示发展时间久，早期是提取手工特征，例如描述图片有人设计sift特征【127】，近年来关于大规模数据很多人用CNN。音频方面之前的MFCC手工特征被dnn和rnn取代了【207】。自然语言中用词嵌入取代之前的方法去学习上下文。然而这些只是单模态，多模态表示有新的架构创新：联合表示和协同表示。如下图所示联合表示多模态的输入共同作用于一个共享空间，协同表示每个模态都有一个表示，但是它和另外的模态有一定的关系和结构约束。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9a463faf1f4219ea60e66ea503245629.png)

#### 联合表示

**联合表示：**
联合表示的最简单示例是各个模态特征的串联（也称为早期融合[50]）。在本节中，我们将讨论从神经网络开始创建联合表示的更高级方法，然后是图形模型和循环神经网络（表2中可以看到代表性的工作）。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/61ecf155d8697536acd186481449668f.png)

##### 神经网络

由于深度神经网络的多层性质，通常使用最终或倒数第二个神经层作为数据表示的形式。为了使用神经网络构建多模态表示，每种模态都从几个单独的神经层开始，然后是隐藏层，将模态投射到关节空间[9]，[145]，[156]。联合多模态表示通过多个隐藏层本身或直接用于预测。这些模型可以端到端地进行训练 - 学习既可以表示数据又可以执行特定任务。当使用神经网络时，**这导致多模态表示学习和多模态融合之间的密切关系。**由于神经网络需要大量标记的训练数据，因此通常在无监督数据上使用自动编码器预训这样的表示。Ngiam等人提出的模型。 [151]扩展了使用自动编码器到多模式域的想法。他们使用堆叠去噪自动编码器分别表示每个模态，然后使用另一个自动编码器层将它们融合成多模态表示。
  
基于神经网络的联合表示的主要优点来自于它们通常优越的性能以及以无人监督的方式预先训练表示的能力。但是，性能增益取决于可用于培训的数据量。其中一个缺点来自于模型无法自然处理缺失的数据 。

##### 图形模型

概率图形模型是通过使用潜在随机变量构建表示的另一种流行方式。在本节中，我们将描述概率图形模型如何用于表示单模态和多模态数据。 欧阳等人[156]探索多模式DBM用于从多视图数据进行人体姿态估计的任务。他们证明，在单峰数据经历非线性变换之后，在稍后阶段整合数据对于模型是有益的。在来自 NIPS 2012 的 《Multimodal learning with deep boltzmann machines》一文中提出将 deep boltzmann machines（DBM） 结构扩充到多模态领域，通过 Multimodal DBM，可以学习到多模态的联合概率分布。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/3bc8316a1438a74a2c24dcbd5f3a4c87.png)
  
论文中的实验通过 Bimodal DBM，学习图片和文本的联合概率分布 P(图片，文本)。在应用阶段，输入图片，利用条件概率 P(文本|图片)，生成文本特征，可以得到图片相应的文本描述；而输入文本，利用条件概率 P(图片|文本)，可以生成图片特征，通过检索出最靠近该特征向量的两个图片实例，可以得到符合文本描述的图片。如下图所示：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/40cd1c1dabd83cbefb07ca269c3d4e43.png)

使用多模式DBM学习多模态表示的一大优势是它们的生成性质，它允许一种简单的方法来处理丢失的数据
**- 即使缺少整体模态，模型也有一种自然的应对方式。它还可以用于在存在另一个模态的情况下生成一种模态的样本，或者从表示中生成两种模态。**
与自动编码器类似，可以以无人监督的方式训练表示，使得能够使用未标记的数据。DBM的主要缺点是难以训练 - 计算成本高。

##### 序列模型

到目前为止，我们已经讨论了可以表示固定长度数据的模型，但是，我们经常需要表示不同长度的序列，例如句子，视频或音频流。在本节中，我们描述了可用于表示此类序列的模型。

递归神经网络（RNN）及其变体（如长短期记忆（LSTM）网络，最近因其在各种任务中顺序建模的成功而受到欢迎[12]，[213]。到目前为止，RNN主要用于表示单词，音频或图像的单峰序列，在语言域中取得最大成功。类似于传统的神经网络，RNN的隐藏状态可被视为数据的表示，即，在时间步t的RNN的隐藏状态可被视为直到该时间步长的序列的概括。这在RNN解码器框架中尤其明显。RNN表示的使用不限于单峰域。使用RNN构建多模态表示的早期使用来自Cosi等人关于AVSR的工作。 它们还被用于多模态情感识别[37]。

#### 协同表示

协同多模表示是通过每个输入模态都对应有一个输出的表示。但是他们的表示之间有一个约定也就是相似地表示不同模态之间的表示距离要满足一个相关性最大，距离最小。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c3ca476c619842eede72f782db1934ad.png)
  
协同表示学习一个比较经典且有趣的应用是来自于《Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models 》这篇文章【105】。利用协同学习到的特征向量之间满足加减算数运算这一特性，可以搜索出与给定图片满足“指定的转换语义”的图片。例如：

狗的图片特征向量 - 狗的文本特征向量 + 猫的文本特征向量 = 猫的图片特征向量 -> 在特征向量空间，根据最近邻距离，检索得到猫的图片：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/881dbd2395ea52883fd3612d56a80334.png)

#### 章节小结

在本节中，我们确定了两种主要类型的多模式表示 - 联合和协调。联合代表将多模态数据投射到一个公共空间，最适合在推理期间存在所有模态的情况。它们已被广泛用于AVSR，情感和多模态手势识别。另一方面，协调表示将每种模态投射到一个单独但协调的空间，使其适用于在测试时只有一种模态的应用，例如：多模式检索和翻译（第4节），接地（第7.2节）和小样本学习（第7.2节）。最后，虽然在情境中使用联合表示来构建两种以上模态的表示，但到目前为止，协调空间主要限于两种模态，三种模态非常困难。

### 翻译

### 对齐挑战

多模态对齐是两个模态或者多个模态间查找实例子组件之间的关系和对应。一般有两种方式，一种是显性对齐，另外一种是隐性对齐，隐性对齐主要有注意力机制。这个领域面临问题是显性对齐标注的数据量较少，两种模态之间的相似制度难以设计。一种模式的元素不一定在另一种模式中有对应的关系。因此在这个领域中它依赖于手工定义的模型直接相似性的度量，或者在无监督的情况下学习他们。

### 融合

多模态融合和多模态表示是非常像的。在我看来他们之间最本质的区别可能就是多模态融合方法，后面有一个分类而多模态表示只需要用到表示。在这一部分我们将多模态分为两种，一种是模型不可知的方法，另外一种是模型可知的方法，针对模型不可知的方法，我们将多模态融合分为，早期融合，中期融合，混合融合。在这里面你可以使用任意的方法，而模型可知的方法中我们需要控制特定的模型去满足特定的需求，例如在多模态中多核学习和图形模型是天生适合多模态数据进行融合的，而且由于他们的数学可控性强，因此适合于可解释性比较强的任务中。此外还有一种模型:神经网络。适合在大数据上的分析。尽管多模态融合取得了这些进展，但是他仍面临着以下的挑战，一个是信息可能不是对齐的，另外一个挑战是每个模态之间可能偶尔会出现一定的噪声。最后一点是难以利用其他的补充模型。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/bee88b4dc6de5e74fd7f2788e132b1ef.png)

### 协同学习

。多模态协同学习是通过一个数据量较好的模态去影响另一个模态。需要注意的是联合学习是独立于任务的，经过处理的数据可以用于更好的融合转换对齐。在去年的顶级会议上有很多论文都是关于这方面的。

### 综述总结

文章中作者按照5个挑战去分类了每个应用所面临的挑战。总的来说，这篇文章具有很高的参照价值。根据这篇文章我理解了我自己的研究点在多模态机器学习的整个地位以及分布。也明白了我下一个研究的方向在多模态中机器学习中属于哪一部分。我将针对从这章学习的内容对我以前看到的一些应用型的论文进行分类。

### 参考文献

[243] B. P. Yuhas, M. H. Goldstein, and T. J. Sejnowski, "Integration of Acoustic and Visual Speech Signals Using Neural Networks,"IEEE Communications Magazine, 1989.

[24] H. Bourlard and S. Dupont,“A mew ASR approach based on independent processing and recombination of partial frequency bands," in International Conference on Spoken Language, 1996.
  
[37]S. Chen and Q. Jin, “Multi-modal Dimensional Emotion Recognition Using Recurrent Neural Networks,” in Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, 2015.
  
[53]G. Evangelopoulos, A. Zlatintsi, A. Potamianos, P. Maragos,
  
K. Rapantzikos, G. Skoumas, and Y. Avrithis, “Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention,” IEEE Trans. Multimedia, 2013.
  
[50]S. K. D’mello and J. Kory, “A Review and Meta-Analysis of Multimodal Affect Detection Systems,” ACM Computing Surveys,2015.
  
[9]S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh, “VQA: Visual question answering,” in ICCV, 2015.
  
[20]J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller, A. Tatarowicz, B. White, S. White, and T. Yeh, “VizWiz: Nearly Real-Time Answers to Vvisual Questions,” in UIST, 2010.
  
[18]Y. Bengio, A. Courville, and P. Vincent, “Representation learning: A review and new perspectives,” TPAMI, 2013.

[105]R. Kiros, R. Salakhutdinov, and R. S. Zemel, “Unifying Visual- Semantic Embeddings with Multimodal Neural Language Models,” 2014.
  
[127]D. G. Lowe, “Distinctive image features from scale-invariant keypoints,” IJCV, 2004.

[138]H. McGurk and J. Macdonald, "Hearing lips and seeing voices.“Nature, 1976.
  
[145]Y. Mroueh, E. Marcheret, and V. Goel, “Deep multimodal learning for Audio-Visual Speech Recognition,” in ICASSP, 2015.
  
[151]J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, “Multimodal Deep Learning,” ICML, 2011.
  
[156]W. Ouyang, X. Chu, and X. Wang, “Multi-source Deep Learning for Human Pose Estimation,” in CVPR, 2014.

[198]N. Srivastava and R. R. Salakhutdinov, “Multimodal Learning with Deep Boltzmann Machines," in NIPS, 2012.
  
[207]G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nico- laou, B. Schuller, and S. Zafeiriou, “Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network," in ICASSP, 2016.

## 多模态融合应用场景论文分析

### 音视频双模态情感识别融合框架研究

#### 摘要

针对双模态情感识别框架识别率低、可靠性差的问题，对情感识别最重要的两个模态语音和面部表情进行 了双模态情感识别特征层融合的研究。采用基于先验知识的特征提取方法和VGGNet-19网络分别对预处理后的音 视频信号进行特征提取，以直接级联的方式并通过PCA进行降维来达到特征融合的目的，使用BLSTM网络进行模 型构建以完成情感识别。将该框架应用到AViD-Corpus和SEMAINE数据库上进行测试，并和传统情感识别特征层 融合框架以及基于VGGNet-19或BLSTM的框架进行了对比。实验结果表明，情感识别的均方根误差（RMSE）得到 降低，皮尔逊相关系数（PCC）得到提高，验证了文中提出方法的有效性。

#### 内容

这篇文章开始主要介绍了情感识别的研究意义并提出了多模态融合在情感分析的一些研究或者指出特征层融合框架仍有一些不足。主要是不能直接级联。融合之后的网络没有考虑双向时效性，因此作者提出了双向长短期记忆网络和特征降维。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/8f93866628d5e173b153797c1555e72c.png)

我那整个架构是参照上图，首先在音频信号作者将音频进行了参数统一化，高通滤波窗函数双门限端点检测法以及特征提取通过以上的方法提取了很多的语音的特征。。对于视频中的话采用了参数统一，通过68个特征点将人脸翻转到同一个角度后使用vgg网络去提取相应的特征。以上步骤处理完毕之后，作者将特征直接进行了级联，也就是每三秒的音频片段对应三秒内的90个面部表情数据维度。完成连接之后级联，特征数据进行了降温。由于输入的是连续信号，作者使用了双向长短期记忆网络去训练这段数据最后得出来的效果如下图，我做了对比和分析，得出了论文的结果。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ceacbec8e0261b496f868e21ced09ac4.png)

#### 总结

这篇文章给我的启发就是针对音视频的一个处理以及PC降维和双向长短期记忆网络这几个点都是非常有用的，但是作者在吉林的时候介绍得很笼统。而且作者在音频当中并没有采用深度学习的方法，也就是一个深度学习方法和一个传统方法他们进行相连。感觉实验对比还是不够详细，如果将音频加入深度神经网络的话，效果会怎么样呢？这一点值得思考。我希望把双向长短期记忆网络和PCA降维方法纳入我的论文当中

### 音视频双模态情感识别融合框架研究

#### 摘要

特种视频（本文特指暴力视频）的智能分类技术有助于实现网络信息内容安全的智能监控。针对现有特种视频多 模态特征融合时未考虑语义一致性等问题，本文提出了一种基于音视频多模态特征融合与多任务学习的特种视频识别 方法。首先，提取特种视频的表观信息和运动信息随时空变化的视觉语义特征及音频信息语义特征；然 后，构 建 具 有 语 义保持的共享特征子空间，以实现音视频多种模态特征的融合；最后，提出基于音视频特征的语义一致性度量和特种视 频分类的多任务学习特种视频分类理论框架，设计了对应的损失函数，实现了端到端的特种视频智能识别。实验结果表 明，本文提出的算法在 ＶｉｏｌｅｎｔＦｌｏｗ 和 ＭｅｄｉａＥｖａｌＶＳＤ２０１５两个数据集上平均精度分别为９７．９７％和３９．７６％，优 于 已 有研究。结果证明了该算法的有效性，有助于提升特种视频监控的智能化水平。

#### 内容

这篇文章专门用来检测暴力视频，和我的婴儿哭声有异曲同工之妙。作者在之前介绍了暴力的概念以及暴力检测的作用之后，介绍了相关的工作有人通过静态视频帧和光流图作为两路简单融合。作者强调之前的算法，暴力地将经典特征和深度网络自动提取的特征进行简单的拼接，就认为这种多模态是非常好的，这是不可取的。作者又提到了早期融合也就是特征融合和决策融合的一个对比决策融合的话没有提取共有信息，因此本文提出静态图运动和音频多种特征有效的特征融合结构。该方法可以很好解决共享子空间与一致性。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/8cbaf09ee5355e342955c9b912c566dc.png)

作者的整个网络架构如上图所示，首先在原始的视频帧和光流中使用p3d长短期记忆网络提取具有时空相关特性的视觉语义特征。然后使用vgg网络去提取音频的语义特征。在融合的过程中音视频语义的一致性设置loss监督,可以自动学习并求取具有语义保持的特征映射变换矩阵。之后呢，作者介绍了每一步的音频提取时，直接将梅尔频谱图放入其中，并使用训练好的模型去进行微调。视频方面采用了类三D+长短期记忆网络提取了对应的特征。以上就是所有模型的内容，而且主要的创新点在于添加了语义一致性损失函数。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/48e2146995289b8c22eaeb6ba4c0511b.png)

作者还描述了其他的约束条件具有一定的缺点。但是我并不认为如此，可能会有新的方法。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4d91471afaa0086a5c84b9dac4f09b0a.png)

上图是作者在公开数据中得到的结果。由于数据集正样本很少，作者采用了ap方式进行比对。

#### 总结

这篇文章对我的收获也很大，自己从这里面了解如何进行共享此空间的设置以及如何保持共享此空间的语义一致性。针对数据正样本和反样本差距过大的方法，作者也提出了一些公开的标准用来评价这种分布的识别效果。最后作者关于图像采集的部分让我学到了光流可以捕捉运动信息。但是文中的语义一致性标签怎么做并没有介绍。这里我暂且认为和分类标签是一致的。。将这篇论文结合到我的婴幼儿哭声分析中，我可以做以下的改进，一个是增加共享子空间另外可以通过对共享子空间的约束性或者动态规划调整特征。

### 基于情绪基调的音视频双模态情绪识别算法

#### 摘要

在音视频决策层融合过程中，当单模态间的情绪识别结果不一致时，融合后的识别结果不准确。针对 这类问题，提出一种基于情绪基调的音视频双模态情绪识别算法。采用连续混合高斯分布的隐马尔科夫模型 ( GMM-HMM) 和随机森林( ＲF) 分别对音频和视频进行情绪识别; 基于情绪基调对音频和视频的情绪识别结果进 行修正; 在不同情绪基调下运用线性相关性分析方法进行决策层融合。实验结果表明: 将该算法应用到 SEMA- INE 数据库上，情绪识别的均方根误差( ＲMSE) 得到降低，皮尔逊相关系数( PCC) 得到了提高。

#### 内容

针对单模态间情绪识别结果不一致导致识别结果 不准确的问题，本文将情绪基调考虑在内，提出了一种 基于情绪基调的音视频双模态情绪识别算法。首先对 音频和视频进行单模态情绪识别;其次对音频和视频 的单模态识别结果进行线性加权和零均值归一化处理，得到音视频双模态的情绪基调;然后基于不同的情 绪基调对单模态间不一致的识别结果进行修正;最后， 基于情绪基调对音视频双模态进行决策层融合，得到 最终的情绪识别结果。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/00439916871c76ed742a26f09ec5d746.png)

在音视频双模态决策层融合过程中，当单模态间 情绪识别结果不一致时，融合后的识别结果不准确。 本文将单模态间识别结果不一致情况予以考虑，提出 一种基于情绪基调的音视频双模态的情绪识别算法。 当音频和视频的单模态情绪识别结果不一致时，本文 创新性地使用情绪基调对两个模态的识别结果进行修 正，解决了单模态间识别结果不一致导致融合后识别 结果准确率不高的问题。决策层融合阶段，在不同音 视频情绪基调下，使用线性相关性分析算法进行音视 频双模态决策层融合，识别结果的准确率也有了一定 提升。使用 SEMAINE 数据库对该算法进行验证，结果 表明，音视频双模态情绪识别的 ＲMSE 得到下降，PCC 得到提升

#### 总结

这篇文章是介绍决策融合的，决策融合的方法有多种，而这篇文章的创新点在于他并没有修改决策融合的机制还是去修正了音频和视频表示结果不一样的情况。这篇文章创新的使用了情绪基调对两各模态的识别结果进行了修正，这样方便以后的决策融合，但是这篇文章我没怎么看懂他是怎么修正的。以后涉及到决策融合的话，可能还需要再看看一下这篇文章，但是这篇文章主要不是传统的决策融合修正，而是基于自己的想法去修改了模型。。

### 基于音视频特征融合的暴力镜头识别

方法研究（我在这过滤镜头捕捉）

#### 摘要

，本文分别从视觉通道、听觉通道、视听双通道对于单个镜头的暴力 成分进行了深入分析。在视觉通道上，本文比较了视频行为分析领域效果最好 的密集轨迹特征方法和目前业界使用较广泛的深度学习方法。在深度学习方法 中，本文将相邻两帧图像的帧间差分图作为卷积神经网络（Convolutional Neural Network，CNN）的输入，之后将 CNN 学习到的每个帧间差分图的特征送入长 短时记忆（Long Short-Term Memory，LSTM）网络中，对时序信号进行建模。 本文在 LSTM 结构中，使用卷积操作进行了改进，改进后的 ConvLSTM 网络ᨀ 取到了更高层的空间特征。在听觉通道上，本文针对目前暴力音频数据集稀缺 问题，基于 MediaEval 电影数据构建了一个 VioAudio 数据集，然后比较了传统 的声学特征方法和分别用原始音频波形图和音频语谱图作为网络输入的深度学 习方法。最后，本文基于视觉通道和听觉通道上结果最好的深度学习模型进行 了融合实验。我们将视频中相邻图像帧的帧间差分图及其对应的音频波形图分 别送入两个 CNN 网络中进行特征的ᨀ取，之后对特征进行融合送入 LSTM 网 络中，利用长短时记忆网络对时序信息进行建模与分类。实验表明了该音视频 融合方法的有效性。

#### 内容

单模态音频网络：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/739cf519602edbe60539526a98a2e03a.png)

单模态视频网络：

![](https://i-blog.csdnimg.cn/blog_migrate/d883d1f474ab7228604e75ac44daff19.png)

融合网络：本文使用预训练的 AlexNet网络ᨀ取相邻帧间差分图的特征；

同时在听觉通道上利用 AlexNet 网络ᨀ取每帧音频信号原始波形的特征。最后，
  
将每个时刻的两类特征融合，送入 LSTM 网络中，对时序信息进行建模，进而
  
判断在该片段中是否存在暴力事件。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4852d84ba96c95f6c3a9069f9571174e.png)

本文不足： （1）在音视频特征融合中，可能存在音频和视频信息不同步的问题，针对 该问题，可考虑先将音频特征和视频特征进行对齐，然后将对齐后的特征进行 建模，进而构建暴力镜头检测系统。 （2）扩大暴力检测数据集，现有的数据集主要针对电影片段，可构建真实结论的监控视频数据，对于监控中的暴力行为进行及时地检测，这将具有更大的现 实意义和应用价值

#### 总结

整体来说这篇论文相比较之前来看的论文细节比较多，但是创新方法可能有所不足。因为这是一篇硕士论文，作者的工作量很多，而他的创新点在镜头检测中也有不少，所以这一点也是可以理解的，但是作为一篇优秀的硕士论文是值得我去参考的，它里面的很多环节，例如如何撰写音视频相关的一些理论以及如何撰写基础理论都是值得我以后写论文进行参考。我将这篇论文定义为硕士论文的标准。其次作者也提到了他没有对数据进行很好的融合以及对其这是一个比较关键的点，而在之前的暴利检测中有一篇文章是对该方法进行了研究，因此可以结合起来。总的来说，这篇文章参考价值在于工作细节的描述至于方法，则只能基于他这个在改进了。参考的创新点比较少。

### End-to-End Multimodal Emotion Recognition using Deep Neural Networks

#### 摘要

摘要情感自动识别是一项具有挑战性的任务，因为情感表达的方式多种多样。在多媒体检索和人机交互等领域都有应用。近年来，深度神经网络已被成功地用于确定情绪状态。受此启发，我们提出了一个使用听觉和视觉模式的情感识别系统。为了捕捉各种说话风格的情感内容，需要提取健壮的特征。为了达到这个目的，我们使用卷积神经网络(CNN)从语音中提取特征，而视觉模态使用50层的深度残差网络(ResNet)。除了特征提取的重要性，机器学习算法还需要对离群值不敏感，同时能够建模上下文。为了解决这个问题，使用了长短期内存(LSTM)网络。然后训练系统在一个端到端的时尚——也利用每个流的相关性,我们设法表现明显优于传统方法基于听觉和视觉手工特性预测的自然和自然情感的RECOLA数据库用2016情感识别研究的挑战。

#### 内容

文中作者总结了之前工作不怎么考虑时间信息，本文提出一种LSTM方法去捕捉时间信息。

L0310在本文中，我们提出了一个多模态系统操作的原始信号，以执行端到端的自发情绪预测任务从语音和视觉数据。为了考虑数据中的上下文信息，使用了LSTM网络。为了加快模型的训练，我们分别预先训练了语音网络和视觉网络。此外，我们研究了语言模态中循环层的门激活，并发现了与韵律特征高度相关的细胞，这些韵律特征通常被认为是引起觉醒的原因。我们在单峰模态上的实验表明，与使用RECOLA数据库的其他模型(包括提交给AVEC2016挑战的模型)相比，我们的模型在测试集上取得了明显更好的性能，从而证明了更好地适应当前任务的学习特性的有效性。另一方面，与其他模型相比，我们的多模态模型在价态维度上比在唤醒维度上表现得更好。

#### 总结

这篇文章我是一个个单词的看过来了，做作者在目标函数上做了一定的处理，将两个模特的相关性以及个性都体现了出来。从作者这篇论文中我学到最多的知识是如何介绍你的模型参数？如何介绍你的对比实验和步骤？还有一个小窍门是，如何使用长短期网络的激活细胞去证明你的方法是正确的，如下图。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/14be353d3b20b47b23e22e5d3bb36035.png)

之前我对这个方向了解不深，看起来比较吃力，现在理解这个方向之后来看感觉还好，确实以后要经常读英文的论文了，收获很多。我在文中需要标注的地方都笔记了，这篇文章中就不重复介绍了，下一阶段就是将该方法应用到我的论文中以及跑通这个作者提供的代码。

## 总结

可以增加光流，共享子空间，loss保持语义一致性，lstm激活响应，pca降维到我的论文中。

68747470733a2f2f626c6f672e:6373646e2e6e65742f6c697570656e6731393937303131392f:61727469636c652f64657461696c732f313036353034393932