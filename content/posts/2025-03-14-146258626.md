---
layout: post
title: "DeepSeek-linux服务器CentOS部署命令笔记"
date: 2025-03-14 15:50:43 +0800
description: "Linux（CentOS）+FinalShell+Ollama+远程访问，本地部署deepseek自备CentOS服务器，并且已经使用FinalShell连接到服务器。"
keywords: "DeepSeek linux服务器（CentOS）部署命令笔记"
categories: ['Llm', 'Linux']
tags: ['本地部署', 'Linux', 'Deepseek', 'Centos']
artid: "146258626"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146258626
    alt: "DeepSeek-linux服务器CentOS部署命令笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146258626
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146258626
cover: https://bing.ee123.net/img/rand?artid=146258626
image: https://bing.ee123.net/img/rand?artid=146258626
img: https://bing.ee123.net/img/rand?artid=146258626
---

# DeepSeek linux服务器（CentOS）部署命令笔记

Linux（CentOS）+FinalShell+Ollama+远程访问，本地部署deepseek

自备CentOS服务器，并且已经使用FinalShell连接到服务器

## 一、准备工作

### 1.更新服务器

    
    
    apt-get update-y

### 2.下载Ollama

    
    
    curl -fsSL https://ollama.com/install.sh  | sh

### 3.测试ollama是否安装完成

    
    
    ollama

### 4.按照正常的命令下载Ollama模型。根据个人服务器硬件配置

    
    
    ollama run deepseek-r1:14b

## 二、放行端口

    
    
    #放行端口
    ufw allow 11434
    
    #查看端口状态
    ufw status

## 三、设置环境变量：

### 1.编辑ollama.service

    
    
    vim /etc/systemd/system/ollama.service

### 2.在[Service]部分，Environment下面添加：

    
    
    Environment="OLLAMA_HOST=0.0.0.0"
    Environment="OLLAMA_ORIGINS=*"
    

![](https://i-blog.csdnimg.cn/direct/0110c685f3744e8c921e8c82b022bec6.png)

使用箭头移动光标到Environment下方，

键盘[i]键，进入编辑模式（左下角显示INSERT代表进入编辑模式）

输入以上两个配置

按[ESC]键，退出编辑模式

输入:wq保存并退出

### 3.重新加载systemd并重启Ollama

    
    
    systemctl deamon-reload
    systemctl restart ollama

## 查看Ollama运行状态

    
    
    systemctl status ollama
    按【q】退出

## 查看显存占用状态

    
    
    #英伟达
    nvidia-smi
    
    #其他软件
    apt-get install nvtop -y
    nvtop

## 四、使用Ollama

使用PageAssist、ChatBox、CherryStudio、AnythingLLM等方式连接即可



