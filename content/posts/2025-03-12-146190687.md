---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35353136383832372f:61727469636c652f64657461696c732f313436313930363837"
layout: post
title: "决策树,Laplace-剪枝与感知机"
date: 2025-03-12 00:17:01 +08:00
description: "决策树，Laplace 剪枝与感知机"
keywords: "决策树，Laplace 剪枝与感知机"
categories: ['人工智能']
tags: ['算法', '剪枝', '决策树']
artid: "146190687"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146190687
    alt: "决策树,Laplace-剪枝与感知机"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146190687
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146190687
cover: https://bing.ee123.net/img/rand?artid=146190687
image: https://bing.ee123.net/img/rand?artid=146190687
img: https://bing.ee123.net/img/rand?artid=146190687
---

# 决策树，Laplace 剪枝与感知机

## 1.决策树

决策树是一种用于分类任务的监督学习算法。它基于特征的划分来做出决策，每个节点表示一个特征，每条分支代表该特征的可能取值，每个叶子节点代表分类结果。

用通俗的话来说，决策树就像一个\*\*"如果……那么……"\*\*的游戏，用来帮助我们做决定。可以把它想象成一棵倒过来的树，
**从根部开始，一层层往下分支**
，每个分支代表不同的选择，最终会到达一个结果。

> 举个简单的例子：
>
> 假设你想决定
> **今天要不要带伞**
> ，你可以用一个决策树来帮你分析：
>
> 1. **第一步（根节点）：**
>    今天会下雨吗？
>    * **如果是（Yes）**
>      →
>      **带伞**
>    * **如果不是（No）**
>      →
>      **看下一步**
> 2. **第二步（分支）：**
>    天气预报说有可能下雨吗？
>    * **如果是（Yes）**
>      →
>      **带伞**
>    * **如果不是（No）**
>      →
>      **不带伞**
>
> 这样，我们通过一个
> **简单的二叉决策树**
> ，一步步推导出了是否带伞的答案。

先给大家po一道题目

![](https://i-blog.csdnimg.cn/direct/15f553ffe2cf46f1af0ab8db53126f2c.png)

首先列出关于本题的所有知识点：

> #### **(a) 计算信息增益（Information Gain）**
>
> 信息增益衡量一个特征在划分数据集后，减少数据不确定性的程度。计算公式如下：
>
> ![](https://i-blog.csdnimg.cn/direct/305a8ea5c8ee411897c25ed60fd8b3d7.png)
>
> 信息增益就像是\*\*"整理书桌的过程"
> **，它衡量的是**
> 我们在某个问题上获得了多少有用的信息
>
> 举个例子：
>
> 假设你有一堆书，里面既有
> **小说**
> ，也有
> **教科书**
> ，现在你想把它们整理好，让书桌变得更整洁。
>
> * **情况 1（未分类前）**
>   ：所有书都混在一起，显得很乱，我们不知道哪些是小说，哪些是教科书（信息很杂乱，不确定性很高）。
> * **情况 2（按类别分类）**
>   ：如果我们按照“
>   **小说**
>   ”和“
>   **教科书**
>   ”分成两堆，那就一目了然了（信息更清晰，不确定性降低）。
>
> 信息增益，就是
> **分类前的混乱程度（信息不确定性） - 分类后的混乱程度**
> 。
>
> ---
>
> #### 具体到决策树：
>
> 在构建决策树时，我们希望每一步分类都能
> **尽量减少混乱**
> ，也就是说，
> **让每个分支里的数据尽可能相似**
> 。
>   
> 信息增益就是
> **衡量某个特征（分类标准）在多大程度上减少了数据的混乱程度**
> 。
>
> **高信息增益的特征 → 说明它对分类非常有帮助，应该优先使用！**
>
> #### 
>
> #### **(b) 构造决策树**
>
> 使用信息增益最大的特征作为根节点，递归划分数据集，直到所有数据属于同一类别或信息增益趋于零。
>
> #### **(c) 预测**
>
> 利用构建的决策树，对
> **short + red + brown eyes**
> 的人进行分类，判断是否会被录取。

> #### 问题解答：
>
> ##### a. 计算各属性的信息增益
>
> 1. **父节点熵计算**
>    ：
>      
>    总样本数 N=8N=8，其中+类3个，-类5个。
>
> ![](https://i-blog.csdnimg.cn/direct/2e70b84336d54e2da1000e1503b39074.png)
>
> **2.属性划分后的熵**
> ：
>
> * **height属性**
>   ：
>
> ![](https://i-blog.csdnimg.cn/direct/efbc569177044f728147c7f6907191d5.png)
>
> **hair属性**
> ：
>
> ![](https://i-blog.csdnimg.cn/direct/267c2286c3db4c1da9d0d8520348dca2.png)
>
> **eyes属性**
> ：
>
> ![](https://i-blog.csdnimg.cn/direct/53b5a5e2e82448f598d94f8fc84af73c.png)
>
> **结果**
> ：
>   
> 信息增益最大为
> **hair（0.4544）**
> ，其次是 eyes（0.3475），最后是 height（0.0031）。
>
> ---
>
> ##### b. 构建决策树
>
> 1. **根节点**
>    ：选择信息增益最大的属性
>    **hair**
>    。
>
>    * **hair=red**
>      → 直接分类为+（1个样本）。
>    * **hair=dark**
>      → 直接分类为-（3个样本）。
>    * **hair=blond**
>      → 需进一步分裂。
> 2. **子节点分裂（hair=blond）**
>    ：
>
>    * 剩余属性为
>      **eyes**
>      和
>      **height**
>      。
>    * 在hair=blond的4个样本中，按
>      **eyes**
>      划分：
>
>      + eyes=blue（2个样本，均为+） → 分类为+。
>      + eyes=brown（2个样本，均为-） → 分类为-。
> 3. **最终决策树结构**
>
> ![](https://i-blog.csdnimg.cn/direct/c090e5d5e2824170bbbcf8730bbfd400.jpeg)
>
> ##### c. 预测样本
>
> 样本属性：
> **short**
> （height）、
> **red**
> （hair）、
> **brown**
> （eyes）。
>   
> 根据决策树：
>
> 1. 检查
>    **hair**
>    ：
>
>    * hair=red → 直接分类为
>      **+**
>      （无需检查其他属性）。
>
> **预测结果**
> ：该候选人会被雇佣（+）。

## **2. 拉普拉斯剪枝 (Laplace Pruning)**

决策树容易过拟合，Laplace 剪枝是一种防止过拟合的技术，利用
**Laplace 误差估计公式**
：

![](https://i-blog.csdnimg.cn/direct/d86a7356ce4c487f843fa8c1e3d72821.png)

> #### **什么是拉普拉斯剪枝？**
>
> 拉普拉斯剪枝（Laplace Smoothing Pruning）是一种防止
> **决策树过拟合**
> 的方法，主要用于修正
> **样本较少时容易出现的极端情况**
> 。
>
> 你可以把它
> **想象成考试时的“保底分”**
> ，就算你不会做题，老师也不会给你零分，而是至少给你一点分数，以免误判你的能力。
>
> ---
>
> #### **为什么需要拉普拉斯剪枝？**
>
> 假设你在训练决策树时，有一个类别的数据非常少，比如：
>
> * 你做了 10 次实验，其中有 9 次“成功”，1 次“失败”。
> * 你的决策树可能会认为\*\*“只要遇到类似情况，几乎100%会成功”\*\*。
>
> 但现实中，如果我们再多做几次实验，可能会出现不同的结果。因此，我们不应该
> **过度相信少量样本的统计结果**
> ，需要
> **让概率更平滑一点**
> ，避免“绝对化”的判断。
>
> ---
>
> #### **拉普拉斯修正是怎么做的？**
>
> 它的原理是
> **在每个类别的计数上加上一个小的“先验”值**
> ，通常是
> **1**
> ，就像给每个类别
> **都分配一点“保底权重”**
> 。
>
> 计算公式：
>
> ![](https://i-blog.csdnimg.cn/direct/753d31d2f9dc49e1bfd3e749813c065f.png)
>
> 其中：
>
> * **k**
>   是类别的总数（比如有 2 类：成功和失败，k=2）。
> * **+1**
>   就是给每个类别
>   **都加上一点权重**
>   ，防止概率变成 0 或 1。
>
> ---
>
> #### **举个例子**
>
> 假设你统计某种天气下是否会下雨：
>
> * 你观测了 4 次，其中
>   **3 次下雨，1 次没下雨**
>   。
> * 传统方法计算概率：
>   + 下雨概率 =
>     **3 / 4 = 0.75**
>   + 不下雨概率 =
>     **1 / 4 = 0.25**
>
> 但这个数据太少了，可能并不可靠。
>   
> 用
> **拉普拉斯修正**
> （k=2，+1调整）：
>
> ![](https://i-blog.csdnimg.cn/direct/d4c89c91f18b4b1abcaed134be174c68.png)
>
> 可以看到，修正后
> **概率更平滑，更不容易被少量数据误导**
> 。
>
> ---
>
> #### **总结**
>
> * **作用**
>   ：避免决策树过拟合，防止少量样本导致极端判断。
> * **方法**
>   ：在计算概率时，每个类别的计数都 +1，同时分母加上类别总数 k，使概率更平滑。
> * **类比**
>   ：像考试的“保底分”或者“菜鸟保护机制”，即使没见过某个情况，也不会直接判断为 0% 或 100%。
>
> 这样，决策树在处理少量数据时会更稳健，不会因为数据不足而产生极端的决策！

首先来po一道例题

![](https://i-blog.csdnimg.cn/direct/991b23e342804866b98bf0e29eb4263c.png)

> #### 问题解答：
>
> ##### 步骤1：计算父节点的Laplace误差
>
> 父节点为
> **[4,7]**
> ，表示有4个正类样本和7个负类样本：
>
> * 总样本数 N=11
> * 多数类为负类（7个），故 n=7
> * 类别数 k=2
>
> 代入公式：
> ![](https://i-blog.csdnimg.cn/direct/f993f969c3e04bc5b2674cfb6e13790a.png)
>
> ##### 步骤2：计算子节点的Laplace误差
>
> 子节点分别为
> **[2,1]**
> 和
> **[2,6]**
> ：
>
> 1. **左子节点 [2,1]**
>    ：
>
>    * 总样本数 N=3
>    * 多数类为正类（2个），故 n=2
>    * 类别数 k=2
>        
>      误差：
>      ![](https://i-blog.csdnimg.cn/direct/5bf1bbb41df64567aacd8cdc8f31bfa0.png)
>
> **2.右子节点 [2,6]**
> ：
>
> 1. 总样本数 N=8
>
> 2.多数类为负类（6个），故 n=6
>
> 3.类别数 k=2
>   
> 误差：
> ![](https://i-blog.csdnimg.cn/direct/7a3caa7fd95948b4933c45637c6d3894.png)
>
> ##### 步骤3：计算保留子节点的加权总误差
>
> * 左子节点权重：3/11​
> * 右子节点权重：8/11
>     
>   总误差：
>   ![](https://i-blog.csdnimg.cn/direct/bf81b07c6f9143beb6c632d433e96749.png)
>
> ##### 步骤4：比较误差并决定是否剪枝
>
> * 父节点误差：0.38460.3846
> * 子节点总误差：0.32730.3273
>
> **结论**
> ：由于子节点总误差（0.3273）小于父节点误差（0.3846），剪枝后误差反而增大，因此
> **不应剪枝**
> 。

> **计算父节点和子节点的拉普拉斯误差，然后比较，是为了决定是否进行剪枝！**
>
> **🌱 什么是剪枝？**
>
> 在决策树中，
> **剪枝**
> （Pruning）是指
> **去掉一些不必要的分支**
> ，让树变得更简单，以防止过拟合。
>
> 拉普拉斯误差（Laplace Error）是一种用于
> **评估决策节点质量**
> 的方法，我们可以计算
> **父节点的误差**
> 和
> **子节点的误差总和**
> ，然后比较它们，来决定是否要剪掉这个分支。
>
> ---
>
> **🎯 为什么要比较父节点和子节点的拉普拉斯误差？**
>
> 1. **如果子节点的误差之和比父节点大**
>    ，说明
>    **划分后效果反而更差**
>    ，分支不值得保留，应该剪枝。
> 2. **如果子节点的误差之和比父节点小**
>    ，说明
>    **划分确实提高了预测效果**
>    ，分支值得保留。
>
> 换句话说，我们是在
> **衡量这个划分是否真的有效**
> ，如果划分后误差反而增加了，那就不如直接用父节点的预测！

## 3.感知机

> #### **🌟 什么是感知机？（Perceptron）**
>
> 感知机就像是\*\*"一个简单的大脑"
> **，可以**
> 根据输入做出二选一的决策\*\*（比如“是”或“否”）。它是最基础的人工神经网络模型之一，专门用来
> **处理二分类问题**
> 。
>
> ---
>
> #### **🤔 怎么理解感知机？**
>
> 可以把感知机想象成一个
> **面试官**
> ，他要决定一个求职者是\*\*"录取"（1）\*\* 还是
> **"不录取"（0）**
> 。
>
> 面试官会根据求职者的
> **学历、工作经验、技能**
> 等因素，给这些特征分配不同的
> **权重**
> ，然后计算一个分数。
>
> * 如果分数高于某个标准（阈值），就
>   **录取（1）**
>   。
> * 如果分数低于标准，就
>   **不录取（0）**
>   。
>
> ---
>
> #### **🛠 感知机的工作原理**
>
> 感知机的计算过程可以简单拆解成三步：
>
> 1️⃣
> **特征加权求和（加权输入）**
>
> * 设 x1,x2,x3​ 代表求职者的学历、经验、技能（输入）。
> * 设 w1,w2,w3​ 代表这些因素的重要性（权重）。
> * 计算总评分： S=w1x1+w2x2+w3x3+b其中 b 是偏置项（相当于面试官的个人倾向）。
>
> 2️⃣
> **通过激活函数做判断**
>
> * 如果 S
>   **大于某个阈值**
>   （比如 0），输出 1（录取）。
> * 否则，输出 0（不录取）。
>     
>   这里常用的
>   **激活函数**
>   就是
>   **阶跃函数**
>   ：
> * ![](https://i-blog.csdnimg.cn/direct/61823a38df8c429db9fe0dad92427856.png)
>
> 3️⃣
> **学习和调整权重（训练过程）**
>
> * 如果决策错误，就调整权重 w，让模型在下次预测时更准确。
> * 这个调整规则就是感知机的
>   **学习算法**
>   。
>
> ---
>
> #### **🎯 举个例子**
>
> 假设感知机要判断一封邮件是
> **垃圾邮件（1）**
> 还是
> **正常邮件（0）**
> ，它会根据一些特征来计算分数，比如：
>
> * **包含“免费”这个词**
>   （x₁ = 1）
> * **包含“中奖”这个词**
>   （x₂ = 1）
> * **发送者是否是陌生人**
>   （x₃ = 1）
>
> 假设感知机学到的权重是：
>
> * **w₁ = 0.8**
>   （“免费”很重要）
> * **w₂ = 0.6**
>   （“中奖”也重要）
> * **w₃ = 0.7**
>   （陌生人发的邮件风险大）
> * **偏置 b = -1.2**
>   （防止过于轻易判定垃圾邮件）
>
> 那么，计算分数：
>
> S=(0.8×1)+(0.6×1)+(0.7×1)−1.2=0.9
>
> 由于 S>0，所以感知机判断：
> **这封邮件是垃圾邮件（1）**
> ！
>
> ---
>
> #### **📌 感知机的特点**
>
> ✅
> **简单高效**
> ：计算快、容易理解。
>   
> ✅
> **适用于线性可分问题**
> （能用一条直线区分的情况）。
>   
> ❌
> **无法解决非线性问题**
> （比如 XOR 异或问题）。
>   
> ❌
> **只能做二分类**
> ，不能处理多类别。
>
> ---
>
> #### **📈 总结**
>
> 感知机就像一个简单的“人工大脑”，它通过
> **给特征赋权重、计算分数，并用阈值做决策**
> ，来判断某个输入属于哪一类。
>
> 你可以把它理解成：
>
> * 面试官（判断是否录取）。
> * 邮件过滤系统（判断是否垃圾邮件）。
> * 简单的 AI 判断器（比如黑白图像分类）。
>
> 虽然感知机很基础，但它是神经网络的“祖师爷”，现代深度学习中的
> **神经元**
> 概念就源自它！ 🚀

po一道例题，具体讲解也可以看
[几道感知机算法（PLA）的例题](https://blog.csdn.net/qq_55168827/article/details/145808909 "几道感知机算法（PLA）的例题")

![](https://i-blog.csdnimg.cn/direct/daf7f37820164886be698a59139e137e.png)

> 解答；
>
> ![](https://i-blog.csdnimg.cn/direct/d47a6e215aca449ca81d95a2986c779a.png)
>
> ![](https://i-blog.csdnimg.cn/direct/6badbfc066734482a91c49b92a45fec5.png)
> 从第一步开始看，w0=-1.5，w1=0，w2=2，x1=0，x2=1，可以计算得到s=0.5>0，class应该为1，但是class分类却为-，这时候的数字应该减少，w0=w0-n（学习率）=-2.5，w1=w1-nx1=0，w2= w2-nx2=1，这时候可以得到第二步结果，然后继续运算，直到符合a,b,c三个样本为止。

## 4.多层神经网络构建逻辑函数

在之前的文章中也有讲解：
[逻辑函数的神经网络实现](https://blog.csdn.net/qq_55168827/article/details/145816152 "逻辑函数的神经网络实现")

> #### **🌟 什么是多层神经网络（MLP）？**
>
> 多层神经网络（MLP，全称 Multi-Layer Perceptron）就是由
> **多个感知机叠加**
> 而成的网络，它能解决单个感知机无法解决的复杂问题，比如
> **异或（XOR）逻辑函数**
> 。
>
> 你可以把它想象成：
>
> * **感知机是一个人**
>   ，只能做简单的判断。
> * **多层神经网络是一支团队**
>   ，可以一起合作，完成更复杂的任务！🎯
>
> ---
>
> #### **🤔 为什么感知机解决不了 XOR？**
>
> ##### **❌ 单层感知机的局限**
>
> 感知机只能处理
> **线性可分**
> 的问题，比如
> **AND**
> 和
> **OR**
> ：
>
> ![](https://i-blog.csdnimg.cn/direct/0a572f6ff8b04a3fb7a5fe5b3705611f.png)
>
> 但
> **XOR（异或）**
> 不能用一条直线分开：
>
> * ![](https://i-blog.csdnimg.cn/direct/eb4904f7f44f4a778f6b4a3bf5e8d8db.png)
> * **无法用一条直线划分 0 和 1！**
>   🚫
>
> 👉
> **解决方案？**
> **加隐藏层！**
> 让网络学会更复杂的决策。
>
> ---
>
> #### **🛠 多层神经网络如何构建 XOR？**
>
> 我们用
> **三层神经网络**
> （输入层 → 隐藏层 → 输出层）来解决 XOR：
>
> ##### **📌 结构**
>
> ```
> 输入层（x₁, x₂）
> ↓
> 隐藏层（h₁, h₂） 
> ↓
> 输出层（y） 
> ```
>
> 1. **输入层**
>    ：有两个输入 x1,x2​（即 XOR 的两个变量）。
> 2. **隐藏层**
>    ：有两个神经元 h1,h2，它们的作用是
>    **创造新特征**
>    ，让 XOR 变得线性可分。
> 3. **输出层**
>    ：根据隐藏层的输出，决定最终结果。
>
> ---
>
> #### **📊 计算 XOR**
>
> ##### **第一步：计算隐藏层**
>
> 我们用两个隐藏神经元 h1,h2h\_1, h\_2h1​,h2​ 来创建 AND 和 OR：
>
> h1=AND(x1,x2)=x1⋅x2​       h2=OR(x1,x2)=x1+x2
>
> | x1 | x2​ | h1=x1⋅x2 | h2=x1+x2 |
> | --- | --- | --- | --- |
> | 0 | 0 | 0 | 0 |
> | 0 | 1 | 0 | 1 |
> | 1 | 0 | 0 | 1 |
> | 1 | 1 | 1 | 1 |
>
> ##### **第二步：计算输出层**
>
> XOR 公式：
>
> y=h2−h1=(x1+x2)−(x1⋅x2)
>
> | x1​ | x2 | h1​ | h2 | y=h2−h1 |
> | --- | --- | --- | --- | --- |
> | 0 | 0 | 0 | 0 | 0 |
> | 0 | 1 | 0 | 1 | 1 |
> | 1 | 0 | 0 | 1 | 1 |
> | 1 | 1 | 1 | 1 | 0 |
>
> 💡
> **结果完全符合 XOR 的逻辑！**
> 🎉
>
> ---
>
> #### **🎯 关键点总结**
>
> 1. **单层感知机无法解决 XOR**
>    ，因为 XOR
>    **不是线性可分**
>    。
> 2. **多层神经网络可以通过隐藏层来学习新的特征**
>    ，比如 AND 和 OR 的组合。
> 3. **隐藏层让神经网络可以处理更复杂的逻辑问题**
>    ，这就是深度学习的核心思想！
>
> 🚀
> **多层神经网络 = 多个感知机合作，解决更复杂的问题！**

> 解决问题：
>
> ![](https://i-blog.csdnimg.cn/direct/6410bdad528e46ad95836ea991999179.png)
> ![](https://i-blog.csdnimg.cn/direct/63a413908c6846238bda6d25ae12ef34.png)
>
> ![](https://i-blog.csdnimg.cn/direct/32aa8bd6f2f3471086cbd02a7b848e5a.png)

> 以表格形式说明偏置项 w0​ 的求解过程 ：
>
> 对 ¬B∨C∨¬D而言：
>
> ![](https://i-blog.csdnimg.cn/direct/b67204f2de984fea846d8a7b72b57813.png)