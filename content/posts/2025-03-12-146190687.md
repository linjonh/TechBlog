---
layout: post
title: "决策树,Laplace-剪枝与感知机"
date: 2025-03-12 00:17:01 +0800
description: "决策树，Laplace 剪枝与感知机"
keywords: "决策树，Laplace 剪枝与感知机"
categories: ['人工智能']
tags: ['算法', '剪枝', '决策树']
artid: "146190687"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146190687
    alt: "决策树,Laplace-剪枝与感知机"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146190687
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146190687
cover: https://bing.ee123.net/img/rand?artid=146190687
image: https://bing.ee123.net/img/rand?artid=146190687
img: https://bing.ee123.net/img/rand?artid=146190687
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     决策树，Laplace 剪枝与感知机
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     1.决策树
    </h2>
    <p>
     决策树是一种用于分类任务的监督学习算法。它基于特征的划分来做出决策，每个节点表示一个特征，每条分支代表该特征的可能取值，每个叶子节点代表分类结果。
    </p>
    <p>
     用通俗的话来说，决策树就像一个**"如果……那么……"**的游戏，用来帮助我们做决定。可以把它想象成一棵倒过来的树，
     <strong>
      从根部开始，一层层往下分支
     </strong>
     ，每个分支代表不同的选择，最终会到达一个结果。
    </p>
    <blockquote>
     <p style="background-color:transparent">
      举个简单的例子：
     </p>
     <p>
      假设你想决定
      <strong>
       今天要不要带伞
      </strong>
      ，你可以用一个决策树来帮你分析：
     </p>
     <ol>
      <li>
       <strong>
        第一步（根节点）：
       </strong>
       今天会下雨吗？
       <ul>
        <li>
         <strong>
          如果是（Yes）
         </strong>
         →
         <strong>
          带伞
         </strong>
        </li>
        <li>
         <strong>
          如果不是（No）
         </strong>
         →
         <strong>
          看下一步
         </strong>
        </li>
       </ul>
      </li>
      <li>
       <strong>
        第二步（分支）：
       </strong>
       天气预报说有可能下雨吗？
       <ul>
        <li>
         <strong>
          如果是（Yes）
         </strong>
         →
         <strong>
          带伞
         </strong>
        </li>
        <li>
         <strong>
          如果不是（No）
         </strong>
         →
         <strong>
          不带伞
         </strong>
        </li>
       </ul>
      </li>
     </ol>
     <p>
      这样，我们通过一个
      <strong>
       简单的二叉决策树
      </strong>
      ，一步步推导出了是否带伞的答案。
     </p>
    </blockquote>
    <p>
     先给大家po一道题目
    </p>
    <p>
     <img alt="" height="776" src="https://i-blog.csdnimg.cn/direct/15f553ffe2cf46f1af0ab8db53126f2c.png" width="891"/>
    </p>
    <p>
     首先列出关于本题的所有知识点：
    </p>
    <blockquote>
     <h4>
      <strong>
       (a) 计算信息增益（Information Gain）
      </strong>
     </h4>
     <p>
      信息增益衡量一个特征在划分数据集后，减少数据不确定性的程度。计算公式如下：
     </p>
     <p>
      <img alt="" height="918" src="https://i-blog.csdnimg.cn/direct/305a8ea5c8ee411897c25ed60fd8b3d7.png" width="1110"/>
     </p>
     <p>
      信息增益就像是**"整理书桌的过程"
      <strong>
       ，它衡量的是
      </strong>
      我们在某个问题上获得了多少有用的信息
     </p>
     <p>
      举个例子：
     </p>
     <p>
      假设你有一堆书，里面既有
      <strong>
       小说
      </strong>
      ，也有
      <strong>
       教科书
      </strong>
      ，现在你想把它们整理好，让书桌变得更整洁。
     </p>
     <ul>
      <li>
       <strong>
        情况 1（未分类前）
       </strong>
       ：所有书都混在一起，显得很乱，我们不知道哪些是小说，哪些是教科书（信息很杂乱，不确定性很高）。
      </li>
      <li>
       <strong>
        情况 2（按类别分类）
       </strong>
       ：如果我们按照“
       <strong>
        小说
       </strong>
       ”和“
       <strong>
        教科书
       </strong>
       ”分成两堆，那就一目了然了（信息更清晰，不确定性降低）。
      </li>
     </ul>
     <p>
      信息增益，就是
      <strong>
       分类前的混乱程度（信息不确定性） - 分类后的混乱程度
      </strong>
      。
     </p>
     <hr/>
     <h4>
      具体到决策树：
     </h4>
     <p>
      在构建决策树时，我们希望每一步分类都能
      <strong>
       尽量减少混乱
      </strong>
      ，也就是说，
      <strong>
       让每个分支里的数据尽可能相似
      </strong>
      。
      <br/>
      信息增益就是
      <strong>
       衡量某个特征（分类标准）在多大程度上减少了数据的混乱程度
      </strong>
      。
     </p>
     <p>
      <strong>
       高信息增益的特征 → 说明它对分类非常有帮助，应该优先使用！
      </strong>
     </p>
     <h4>
     </h4>
     <h4>
      <strong>
       (b) 构造决策树
      </strong>
     </h4>
     <p>
      使用信息增益最大的特征作为根节点，递归划分数据集，直到所有数据属于同一类别或信息增益趋于零。
     </p>
     <h4>
      <strong>
       (c) 预测
      </strong>
     </h4>
     <p>
      利用构建的决策树，对
      <strong>
       short + red + brown eyes
      </strong>
      的人进行分类，判断是否会被录取。
     </p>
    </blockquote>
    <blockquote>
     <h4>
      问题解答：
     </h4>
     <h5>
      a. 计算各属性的信息增益
     </h5>
     <ol>
      <li>
       <p>
        <strong>
         父节点熵计算
        </strong>
        ：
        <br/>
        总样本数 N=8N=8，其中+类3个，-类5个。
       </p>
      </li>
     </ol>
     <p>
      <img alt="" height="125" src="https://i-blog.csdnimg.cn/direct/2e70b84336d54e2da1000e1503b39074.png" width="824"/>
     </p>
     <p>
      <strong>
       2.属性划分后的熵
      </strong>
      ：
     </p>
     <ul>
      <li>
       <p>
        <strong>
         height属性
        </strong>
        ：
       </p>
      </li>
     </ul>
     <p>
      <img alt="" height="422" src="https://i-blog.csdnimg.cn/direct/efbc569177044f728147c7f6907191d5.png" width="808"/>
     </p>
     <p>
      <strong>
       hair属性
      </strong>
      ：
     </p>
     <p>
      <img alt="" height="446" src="https://i-blog.csdnimg.cn/direct/267c2286c3db4c1da9d0d8520348dca2.png" width="772"/>
     </p>
     <p>
      <strong>
       eyes属性
      </strong>
      ：
     </p>
     <p>
      <img alt="" height="376" src="https://i-blog.csdnimg.cn/direct/53b5a5e2e82448f598d94f8fc84af73c.png" width="796"/>
     </p>
     <p>
      <strong>
       结果
      </strong>
      ：
      <br/>
      信息增益最大为
      <strong>
       hair（0.4544）
      </strong>
      ，其次是 eyes（0.3475），最后是 height（0.0031）。
     </p>
     <hr/>
     <h5>
      b. 构建决策树
     </h5>
     <ol>
      <li>
       <p>
        <strong>
         根节点
        </strong>
        ：选择信息增益最大的属性
        <strong>
         hair
        </strong>
        。
       </p>
       <ul>
        <li>
         <p>
          <strong>
           hair=red
          </strong>
          → 直接分类为+（1个样本）。
         </p>
        </li>
        <li>
         <p>
          <strong>
           hair=dark
          </strong>
          → 直接分类为-（3个样本）。
         </p>
        </li>
        <li>
         <p>
          <strong>
           hair=blond
          </strong>
          → 需进一步分裂。
         </p>
        </li>
       </ul>
      </li>
      <li>
       <p>
        <strong>
         子节点分裂（hair=blond）
        </strong>
        ：
       </p>
       <ul>
        <li>
         <p>
          剩余属性为
          <strong>
           eyes
          </strong>
          和
          <strong>
           height
          </strong>
          。
         </p>
        </li>
        <li>
         <p>
          在hair=blond的4个样本中，按
          <strong>
           eyes
          </strong>
          划分：
         </p>
         <ul>
          <li>
           <p>
            eyes=blue（2个样本，均为+） → 分类为+。
           </p>
          </li>
          <li>
           <p>
            eyes=brown（2个样本，均为-） → 分类为-。
           </p>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <p>
        <strong>
         最终决策树结构
        </strong>
       </p>
      </li>
     </ol>
     <p>
      <img alt="" height="384" src="https://i-blog.csdnimg.cn/direct/c090e5d5e2824170bbbcf8730bbfd400.jpeg" width="455"/>
     </p>
     <h5>
      c. 预测样本
     </h5>
     <p>
      样本属性：
      <strong>
       short
      </strong>
      （height）、
      <strong>
       red
      </strong>
      （hair）、
      <strong>
       brown
      </strong>
      （eyes）。
      <br/>
      根据决策树：
     </p>
     <ol>
      <li>
       <p>
        检查
        <strong>
         hair
        </strong>
        ：
       </p>
       <ul>
        <li>
         <p>
          hair=red → 直接分类为
          <strong>
           +
          </strong>
          （无需检查其他属性）。
         </p>
        </li>
       </ul>
      </li>
     </ol>
     <p>
      <strong>
       预测结果
      </strong>
      ：该候选人会被雇佣（+）。
     </p>
    </blockquote>
    <h2>
     <strong>
      2. 拉普拉斯剪枝 (Laplace Pruning)
     </strong>
    </h2>
    <p>
     决策树容易过拟合，Laplace 剪枝是一种防止过拟合的技术，利用
     <strong>
      Laplace 误差估计公式
     </strong>
     ：
    </p>
    <p>
     <img alt="" height="314" src="https://i-blog.csdnimg.cn/direct/d86a7356ce4c487f843fa8c1e3d72821.png" width="562">
     </img>
    </p>
    <blockquote>
     <h4>
      <strong>
       什么是拉普拉斯剪枝？
      </strong>
     </h4>
     <p>
      拉普拉斯剪枝（Laplace Smoothing Pruning）是一种防止
      <strong>
       决策树过拟合
      </strong>
      的方法，主要用于修正
      <strong>
       样本较少时容易出现的极端情况
      </strong>
      。
     </p>
     <p>
      你可以把它
      <strong>
       想象成考试时的“保底分”
      </strong>
      ，就算你不会做题，老师也不会给你零分，而是至少给你一点分数，以免误判你的能力。
     </p>
     <hr/>
     <h4>
      <strong>
       为什么需要拉普拉斯剪枝？
      </strong>
     </h4>
     <p>
      假设你在训练决策树时，有一个类别的数据非常少，比如：
     </p>
     <ul>
      <li>
       你做了 10 次实验，其中有 9 次“成功”，1 次“失败”。
      </li>
      <li>
       你的决策树可能会认为**“只要遇到类似情况，几乎100%会成功”**。
      </li>
     </ul>
     <p>
      但现实中，如果我们再多做几次实验，可能会出现不同的结果。因此，我们不应该
      <strong>
       过度相信少量样本的统计结果
      </strong>
      ，需要
      <strong>
       让概率更平滑一点
      </strong>
      ，避免“绝对化”的判断。
     </p>
     <hr/>
     <h4>
      <strong>
       拉普拉斯修正是怎么做的？
      </strong>
     </h4>
     <p>
      它的原理是
      <strong>
       在每个类别的计数上加上一个小的“先验”值
      </strong>
      ，通常是
      <strong>
       1
      </strong>
      ，就像给每个类别
      <strong>
       都分配一点“保底权重”
      </strong>
      。
     </p>
     <p>
      计算公式：
     </p>
     <img alt="" height="118" src="https://i-blog.csdnimg.cn/direct/753d31d2f9dc49e1bfd3e749813c065f.png" width="382"/>
     <p>
      其中：
     </p>
     <ul>
      <li>
       <strong>
        k
       </strong>
       是类别的总数（比如有 2 类：成功和失败，k=2）。
      </li>
      <li>
       <strong>
        +1
       </strong>
       就是给每个类别
       <strong>
        都加上一点权重
       </strong>
       ，防止概率变成 0 或 1。
      </li>
     </ul>
     <hr/>
     <h4 style="background-color:transparent">
      <strong>
       举个例子
      </strong>
     </h4>
     <p>
      假设你统计某种天气下是否会下雨：
     </p>
     <ul>
      <li>
       你观测了 4 次，其中
       <strong>
        3 次下雨，1 次没下雨
       </strong>
       。
      </li>
      <li>
       传统方法计算概率：
       <ul>
        <li>
         下雨概率 =
         <strong>
          3 / 4 = 0.75
         </strong>
        </li>
        <li>
         不下雨概率 =
         <strong>
          1 / 4 = 0.25
         </strong>
        </li>
       </ul>
      </li>
     </ul>
     <p>
      但这个数据太少了，可能并不可靠。
      <br/>
      用
      <strong>
       拉普拉斯修正
      </strong>
      （k=2，+1调整）：
     </p>
     <p>
      <img alt="" height="159" src="https://i-blog.csdnimg.cn/direct/d4c89c91f18b4b1abcaed134be174c68.png" width="506"/>
     </p>
     <p>
      可以看到，修正后
      <strong>
       概率更平滑，更不容易被少量数据误导
      </strong>
      。
     </p>
     <hr/>
     <h4>
      <strong>
       总结
      </strong>
     </h4>
     <ul>
      <li>
       <strong>
        作用
       </strong>
       ：避免决策树过拟合，防止少量样本导致极端判断。
      </li>
      <li>
       <strong>
        方法
       </strong>
       ：在计算概率时，每个类别的计数都 +1，同时分母加上类别总数 k，使概率更平滑。
      </li>
      <li>
       <strong>
        类比
       </strong>
       ：像考试的“保底分”或者“菜鸟保护机制”，即使没见过某个情况，也不会直接判断为 0% 或 100%。
      </li>
     </ul>
     <p>
      这样，决策树在处理少量数据时会更稳健，不会因为数据不足而产生极端的决策！
     </p>
    </blockquote>
    <p>
     首先来po一道例题
    </p>
    <p>
     <img alt="" height="508" src="https://i-blog.csdnimg.cn/direct/991b23e342804866b98bf0e29eb4263c.png" width="895"/>
    </p>
    <blockquote>
     <h4>
      问题解答：
     </h4>
     <h5>
      步骤1：计算父节点的Laplace误差
     </h5>
     <p>
      父节点为
      <strong>
       [4,7]
      </strong>
      ，表示有4个正类样本和7个负类样本：
     </p>
     <ul>
      <li>
       <p>
        总样本数 N=11
       </p>
      </li>
      <li>
       <p>
        多数类为负类（7个），故 n=7
       </p>
      </li>
      <li>
       <p>
        类别数 k=2
       </p>
      </li>
     </ul>
     <p>
      代入公式：
      <img alt="" height="98" src="https://i-blog.csdnimg.cn/direct/f993f969c3e04bc5b2674cfb6e13790a.png" width="710"/>
     </p>
     <h5>
      步骤2：计算子节点的Laplace误差
     </h5>
     <p>
      子节点分别为
      <strong>
       [2,1]
      </strong>
      和
      <strong>
       [2,6]
      </strong>
      ：
     </p>
     <ol>
      <li>
       <p>
        <strong>
         左子节点 [2,1]
        </strong>
        ：
       </p>
       <ul>
        <li>
         <p>
          总样本数 N=3
         </p>
        </li>
        <li>
         <p>
          多数类为正类（2个），故 n=2
         </p>
        </li>
        <li>
         <p>
          类别数 k=2
          <br/>
          误差：
          <img alt="" height="55" src="https://i-blog.csdnimg.cn/direct/5bf1bbb41df64567aacd8cdc8f31bfa0.png" width="253"/>
         </p>
        </li>
       </ul>
      </li>
     </ol>
     <p>
      <strong>
       2.右子节点 [2,6]
      </strong>
      ：
     </p>
     <p>
      1. 总样本数 N=8
     </p>
     <p>
      2.多数类为负类（6个），故 n=6
     </p>
     <p>
      3.类别数 k=2
      <br/>
      误差：
      <img alt="" height="53" src="https://i-blog.csdnimg.cn/direct/7a3caa7fd95948b4933c45637c6d3894.png" width="264"/>
     </p>
     <h5>
      步骤3：计算保留子节点的加权总误差
     </h5>
     <ul>
      <li>
       <p>
        左子节点权重：3/11​
       </p>
      </li>
      <li>
       <p>
        右子节点权重：8/11
        <br/>
        总误差：
        <img alt="" height="66" src="https://i-blog.csdnimg.cn/direct/bf81b07c6f9143beb6c632d433e96749.png" width="763"/>
       </p>
      </li>
     </ul>
     <h5>
      步骤4：比较误差并决定是否剪枝
     </h5>
     <ul>
      <li>
       <p>
        父节点误差：0.38460.3846
       </p>
      </li>
      <li>
       <p>
        子节点总误差：0.32730.3273
       </p>
      </li>
     </ul>
     <p>
      <strong>
       结论
      </strong>
      ：由于子节点总误差（0.3273）小于父节点误差（0.3846），剪枝后误差反而增大，因此
      <strong>
       不应剪枝
      </strong>
      。
     </p>
    </blockquote>
    <blockquote>
     <p style="background-color:transparent">
      <strong>
       计算父节点和子节点的拉普拉斯误差，然后比较，是为了决定是否进行剪枝！
      </strong>
     </p>
     <p style="background-color:transparent">
      <strong>
       🌱 什么是剪枝？
      </strong>
     </p>
     <p style="background-color:transparent">
     </p>
     <p>
      在决策树中，
      <strong>
       剪枝
      </strong>
      （Pruning）是指
      <strong>
       去掉一些不必要的分支
      </strong>
      ，让树变得更简单，以防止过拟合。
     </p>
     <p>
      拉普拉斯误差（Laplace Error）是一种用于
      <strong>
       评估决策节点质量
      </strong>
      的方法，我们可以计算
      <strong>
       父节点的误差
      </strong>
      和
      <strong>
       子节点的误差总和
      </strong>
      ，然后比较它们，来决定是否要剪掉这个分支。
     </p>
     <hr/>
     <p>
      <strong>
       🎯 为什么要比较父节点和子节点的拉普拉斯误差？
      </strong>
     </p>
     <ol>
      <li>
       <strong>
        如果子节点的误差之和比父节点大
       </strong>
       ，说明
       <strong>
        划分后效果反而更差
       </strong>
       ，分支不值得保留，应该剪枝。
      </li>
      <li>
       <strong>
        如果子节点的误差之和比父节点小
       </strong>
       ，说明
       <strong>
        划分确实提高了预测效果
       </strong>
       ，分支值得保留。
      </li>
     </ol>
     <p>
      换句话说，我们是在
      <strong>
       衡量这个划分是否真的有效
      </strong>
      ，如果划分后误差反而增加了，那就不如直接用父节点的预测！
     </p>
    </blockquote>
    <h2>
     3.感知机
    </h2>
    <blockquote>
     <h4>
      <strong>
       🌟 什么是感知机？（Perceptron）
      </strong>
     </h4>
     <p>
      感知机就像是**"一个简单的大脑"
      <strong>
       ，可以
      </strong>
      根据输入做出二选一的决策**（比如“是”或“否”）。它是最基础的人工神经网络模型之一，专门用来
      <strong>
       处理二分类问题
      </strong>
      。
     </p>
     <hr/>
     <h4>
      <strong>
       🤔 怎么理解感知机？
      </strong>
     </h4>
     <p>
      可以把感知机想象成一个
      <strong>
       面试官
      </strong>
      ，他要决定一个求职者是**"录取"（1）** 还是
      <strong>
       "不录取"（0）
      </strong>
      。
     </p>
     <p>
      面试官会根据求职者的
      <strong>
       学历、工作经验、技能
      </strong>
      等因素，给这些特征分配不同的
      <strong>
       权重
      </strong>
      ，然后计算一个分数。
     </p>
     <ul>
      <li>
       如果分数高于某个标准（阈值），就
       <strong>
        录取（1）
       </strong>
       。
      </li>
      <li>
       如果分数低于标准，就
       <strong>
        不录取（0）
       </strong>
       。
      </li>
     </ul>
     <hr/>
     <h4>
      <strong>
       🛠 感知机的工作原理
      </strong>
     </h4>
     <p>
      感知机的计算过程可以简单拆解成三步：
     </p>
     <p>
      1️⃣
      <strong>
       特征加权求和（加权输入）
      </strong>
     </p>
     <ul>
      <li>
       设 x1,x2,x3​ 代表求职者的学历、经验、技能（输入）。
      </li>
      <li>
       设 w1,w2,w3​ 代表这些因素的重要性（权重）。
      </li>
      <li>
       计算总评分： S=w1x1+w2x2+w3x3+b其中 b 是偏置项（相当于面试官的个人倾向）。
      </li>
     </ul>
     <p>
      2️⃣
      <strong>
       通过激活函数做判断
      </strong>
     </p>
     <ul>
      <li>
       如果 S
       <strong>
        大于某个阈值
       </strong>
       （比如 0），输出 1（录取）。
      </li>
      <li>
       否则，输出 0（不录取）。
       <br/>
       这里常用的
       <strong>
        激活函数
       </strong>
       就是
       <strong>
        阶跃函数
       </strong>
       ：
      </li>
      <li>
       <img alt="" height="132" src="https://i-blog.csdnimg.cn/direct/61823a38df8c429db9fe0dad92427856.png" width="293"/>
      </li>
     </ul>
     <p>
      3️⃣
      <strong>
       学习和调整权重（训练过程）
      </strong>
     </p>
     <ul>
      <li>
       如果决策错误，就调整权重 w，让模型在下次预测时更准确。
      </li>
      <li>
       这个调整规则就是感知机的
       <strong>
        学习算法
       </strong>
       。
      </li>
     </ul>
     <hr/>
     <h4>
      <strong>
       🎯 举个例子
      </strong>
     </h4>
     <p>
      假设感知机要判断一封邮件是
      <strong>
       垃圾邮件（1）
      </strong>
      还是
      <strong>
       正常邮件（0）
      </strong>
      ，它会根据一些特征来计算分数，比如：
     </p>
     <ul>
      <li>
       <strong>
        包含“免费”这个词
       </strong>
       （x₁ = 1）
      </li>
      <li>
       <strong>
        包含“中奖”这个词
       </strong>
       （x₂ = 1）
      </li>
      <li>
       <strong>
        发送者是否是陌生人
       </strong>
       （x₃ = 1）
      </li>
     </ul>
     <p>
      假设感知机学到的权重是：
     </p>
     <ul>
      <li>
       <strong>
        w₁ = 0.8
       </strong>
       （“免费”很重要）
      </li>
      <li>
       <strong>
        w₂ = 0.6
       </strong>
       （“中奖”也重要）
      </li>
      <li>
       <strong>
        w₃ = 0.7
       </strong>
       （陌生人发的邮件风险大）
      </li>
      <li>
       <strong>
        偏置 b = -1.2
       </strong>
       （防止过于轻易判定垃圾邮件）
      </li>
     </ul>
     <p>
      那么，计算分数：
     </p>
     S=(0.8×1)+(0.6×1)+(0.7×1)−1.2=0.9
     <p>
      由于 S&gt;0，所以感知机判断：
      <strong>
       这封邮件是垃圾邮件（1）
      </strong>
      ！
     </p>
     <hr/>
     <h4>
      <strong>
       📌 感知机的特点
      </strong>
     </h4>
     <p>
      ✅
      <strong>
       简单高效
      </strong>
      ：计算快、容易理解。
      <br/>
      ✅
      <strong>
       适用于线性可分问题
      </strong>
      （能用一条直线区分的情况）。
      <br/>
      ❌
      <strong>
       无法解决非线性问题
      </strong>
      （比如 XOR 异或问题）。
      <br/>
      ❌
      <strong>
       只能做二分类
      </strong>
      ，不能处理多类别。
     </p>
     <hr/>
     <h4>
      <strong>
       📈 总结
      </strong>
     </h4>
     <p>
      感知机就像一个简单的“人工大脑”，它通过
      <strong>
       给特征赋权重、计算分数，并用阈值做决策
      </strong>
      ，来判断某个输入属于哪一类。
     </p>
     <p>
      你可以把它理解成：
     </p>
     <ul>
      <li>
       面试官（判断是否录取）。
      </li>
      <li>
       邮件过滤系统（判断是否垃圾邮件）。
      </li>
      <li>
       简单的 AI 判断器（比如黑白图像分类）。
      </li>
     </ul>
     <p>
      虽然感知机很基础，但它是神经网络的“祖师爷”，现代深度学习中的
      <strong>
       神经元
      </strong>
      概念就源自它！ 🚀
     </p>
    </blockquote>
    <p>
     po一道例题，具体讲解也可以看
     <a class="link-info" href="https://blog.csdn.net/qq_55168827/article/details/145808909" title="几道感知机算法（PLA）的例题">
      几道感知机算法（PLA）的例题
     </a>
    </p>
    <p>
     <img alt="" height="830" src="https://i-blog.csdnimg.cn/direct/daf7f37820164886be698a59139e137e.png" width="925"/>
    </p>
    <blockquote>
     <p>
      解答；
     </p>
     <p>
      <img alt="" height="431" src="https://i-blog.csdnimg.cn/direct/d47a6e215aca449ca81d95a2986c779a.png" width="902"/>
     </p>
     <p>
      <img alt="" height="452" src="https://i-blog.csdnimg.cn/direct/6badbfc066734482a91c49b92a45fec5.png" width="1126"/>
      从第一步开始看，w0=-1.5，w1=0，w2=2，x1=0，x2=1，可以计算得到s=0.5&gt;0，class应该为1，但是class分类却为-，这时候的数字应该减少，w0=w0-n（学习率）=-2.5，w1=w1-nx1=0，w2= w2-nx2=1，这时候可以得到第二步结果，然后继续运算，直到符合a,b,c三个样本为止。
     </p>
    </blockquote>
    <h2>
     4.多层神经网络构建逻辑函数
    </h2>
    <p>
     在之前的文章中也有讲解：
     <a class="link-info" href="https://blog.csdn.net/qq_55168827/article/details/145816152" title="逻辑函数的神经网络实现">
      逻辑函数的神经网络实现
     </a>
    </p>
    <blockquote>
     <h4>
      <strong>
       🌟 什么是多层神经网络（MLP）？
      </strong>
     </h4>
     <p>
      多层神经网络（MLP，全称 Multi-Layer Perceptron）就是由
      <strong>
       多个感知机叠加
      </strong>
      而成的网络，它能解决单个感知机无法解决的复杂问题，比如
      <strong>
       异或（XOR）逻辑函数
      </strong>
      。
     </p>
     <p>
      你可以把它想象成：
     </p>
     <ul>
      <li>
       <strong>
        感知机是一个人
       </strong>
       ，只能做简单的判断。
      </li>
      <li>
       <strong>
        多层神经网络是一支团队
       </strong>
       ，可以一起合作，完成更复杂的任务！🎯
      </li>
     </ul>
     <hr/>
     <h4 style="background-color:transparent">
      <strong>
       🤔 为什么感知机解决不了 XOR？
      </strong>
     </h4>
     <h5>
      <strong>
       ❌ 单层感知机的局限
      </strong>
     </h5>
     <p>
      感知机只能处理
      <strong>
       线性可分
      </strong>
      的问题，比如
      <strong>
       AND
      </strong>
      和
      <strong>
       OR
      </strong>
      ：
     </p>
     <p>
      <img alt="" height="107" src="https://i-blog.csdnimg.cn/direct/0a572f6ff8b04a3fb7a5fe5b3705611f.png" width="750"/>
     </p>
     <p>
      但
      <strong>
       XOR（异或）
      </strong>
      不能用一条直线分开：
     </p>
     <ul>
      <li>
       <img alt="" height="53" src="https://i-blog.csdnimg.cn/direct/eb4904f7f44f4a778f6b4a3bf5e8d8db.png" width="699"/>
      </li>
      <li>
       <strong>
        无法用一条直线划分 0 和 1！
       </strong>
       🚫
      </li>
     </ul>
     <p>
      👉
      <strong>
       解决方案？
      </strong>
      <strong>
       加隐藏层！
      </strong>
      让网络学会更复杂的决策。
     </p>
     <hr/>
     <h4>
      <strong>
       🛠 多层神经网络如何构建 XOR？
      </strong>
     </h4>
     <p>
      我们用
      <strong>
       三层神经网络
      </strong>
      （输入层 → 隐藏层 → 输出层）来解决 XOR：
     </p>
     <h5>
      <strong>
       📌 结构
      </strong>
     </h5>
     <pre><code>输入层（x₁, x₂）
↓
隐藏层（h₁, h₂） 
↓
输出层（y） </code></pre>
     <ol>
      <li>
       <strong>
        输入层
       </strong>
       ：有两个输入 x1,x2​（即 XOR 的两个变量）。
      </li>
      <li>
       <strong>
        隐藏层
       </strong>
       ：有两个神经元 h1,h2，它们的作用是
       <strong>
        创造新特征
       </strong>
       ，让 XOR 变得线性可分。
      </li>
      <li>
       <strong>
        输出层
       </strong>
       ：根据隐藏层的输出，决定最终结果。
      </li>
     </ol>
     <hr/>
     <h4>
      <strong>
       📊 计算 XOR
      </strong>
     </h4>
     <h5>
      <strong>
       第一步：计算隐藏层
      </strong>
     </h5>
     <p>
      我们用两个隐藏神经元 h1,h2h_1, h_2h1​,h2​ 来创建 AND 和 OR：
     </p>
     h1=AND(x1,x2)=x1⋅x2​       h2=OR(x1,x2)=x1+x2
     <table>
      <thead>
       <tr>
        <th>
         x1
        </th>
        <th>
         x2​
        </th>
        <th>
         h1=x1⋅x2
        </th>
        <th>
         h2=x1+x2
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
       </tr>
       <tr>
        <td>
         0
        </td>
        <td>
         1
        </td>
        <td>
         0
        </td>
        <td>
         1
        </td>
       </tr>
       <tr>
        <td>
         1
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         1
        </td>
       </tr>
       <tr>
        <td>
         1
        </td>
        <td>
         1
        </td>
        <td>
         1
        </td>
        <td>
         1
        </td>
       </tr>
      </tbody>
     </table>
     <h5>
      <strong>
       第二步：计算输出层
      </strong>
     </h5>
     <p>
      XOR 公式：
     </p>
     y=h2−h1=(x1+x2)−(x1⋅x2)
     <table>
      <thead>
       <tr>
        <th>
         x1​
        </th>
        <th>
         x2
        </th>
        <th>
         h1​
        </th>
        <th>
         h2
        </th>
        <th>
         y=h2−h1
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
       </tr>
       <tr>
        <td>
         0
        </td>
        <td>
         1
        </td>
        <td>
         0
        </td>
        <td>
         1
        </td>
        <td>
         1
        </td>
       </tr>
       <tr>
        <td>
         1
        </td>
        <td>
         0
        </td>
        <td>
         0
        </td>
        <td>
         1
        </td>
        <td>
         1
        </td>
       </tr>
       <tr>
        <td>
         1
        </td>
        <td>
         1
        </td>
        <td>
         1
        </td>
        <td>
         1
        </td>
        <td>
         0
        </td>
       </tr>
      </tbody>
     </table>
     <p>
      💡
      <strong>
       结果完全符合 XOR 的逻辑！
      </strong>
      🎉
     </p>
     <hr/>
     <h4>
      <strong>
       🎯 关键点总结
      </strong>
     </h4>
     <ol>
      <li>
       <strong>
        单层感知机无法解决 XOR
       </strong>
       ，因为 XOR
       <strong>
        不是线性可分
       </strong>
       。
      </li>
      <li>
       <strong>
        多层神经网络可以通过隐藏层来学习新的特征
       </strong>
       ，比如 AND 和 OR 的组合。
      </li>
      <li>
       <strong>
        隐藏层让神经网络可以处理更复杂的逻辑问题
       </strong>
       ，这就是深度学习的核心思想！
      </li>
     </ol>
     <p>
      🚀
      <strong>
       多层神经网络 = 多个感知机合作，解决更复杂的问题！
      </strong>
     </p>
    </blockquote>
    <blockquote>
     <p>
      解决问题：
     </p>
     <p>
      <img alt="" height="706" src="https://i-blog.csdnimg.cn/direct/6410bdad528e46ad95836ea991999179.png" width="883"/>
      <img alt="" height="556" src="https://i-blog.csdnimg.cn/direct/63a413908c6846238bda6d25ae12ef34.png" width="880"/>
     </p>
     <p>
      <img alt="" height="784" src="https://i-blog.csdnimg.cn/direct/32aa8bd6f2f3471086cbd02a7b848e5a.png" width="938"/>
     </p>
    </blockquote>
    <blockquote>
     <p>
      以表格形式说明偏置项 w0​ 的求解过程 ：
     </p>
     <p>
      对 ¬B∨C∨¬D而言：
     </p>
     <p>
      <img alt="" height="1121" src="https://i-blog.csdnimg.cn/direct/b67204f2de984fea846d8a7b72b57813.png" width="945"/>
     </p>
    </blockquote>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35353136383832372f:61727469636c652f64657461696c732f313436313930363837" class_="artid" style="display:none">
 </p>
</div>


