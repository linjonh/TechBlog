---
layout: post
title: "KubernetesK8s集群中使用-GPU"
date: 2025-03-10 22:28:52 +0800
description: "在 Kubernetes（K8s）集群中使用 GPU，需要完成安装驱动、部署插件、配置 containerd、实现 GPU 虚拟化及部分使用等一系列步骤，下面为你详细介绍。"
keywords: "Kubernetes（K8s）集群中使用 GPU"
categories: ['未分类']
tags: ['容器', 'Kubernetes']
artid: "146165743"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146165743
    alt: "KubernetesK8s集群中使用-GPU"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146165743
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146165743
cover: https://bing.ee123.net/img/rand?artid=146165743
image: https://bing.ee123.net/img/rand?artid=146165743
img: https://bing.ee123.net/img/rand?artid=146165743
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Kubernetes（K8s）集群中使用 GPU
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     在 Kubernetes（K8s）集群中使用 GPU，需要完成安装驱动、部署插件、配置 containerd、实现 GPU 虚拟化及部分使用等一系列步骤，下面为你详细介绍。
    </p>
    <h4>
     <a id="1__GPU__3">
     </a>
     1. 安装 GPU 驱动
    </h4>
    <p>
     以 NVIDIA GPU 为例，因为在深度学习和机器学习场景中 NVIDIA GPU 应用广泛，以下是在 Linux 系统上安装 NVIDIA 驱动的步骤：
    </p>
    <h5>
     <a id="11__GPU__6">
     </a>
     1.1 检查系统和 GPU 信息
    </h5>
    <p>
     首先需要确认系统的内核版本和 GPU 型号，使用以下命令：
    </p>
    <pre><code class="prism language-bash"><span class="token function">uname</span> <span class="token parameter variable">-r</span>  <span class="token comment"># 查看内核版本</span>
lspci <span class="token operator">|</span> <span class="token function">grep</span> <span class="token parameter variable">-i</span> nvidia  <span class="token comment"># 查看 GPU 型号</span>
</code></pre>
    <h5>
     <a id="12__Nouveau__13">
     </a>
     1.2 禁用 Nouveau 驱动
    </h5>
    <p>
     Nouveau 是 Linux 系统默认的开源 NVIDIA 驱动，需要先禁用它，以免和 NVIDIA 官方驱动冲突。编辑
     <code>
      /etc/modprobe.d/blacklist-nouveau.conf
     </code>
     文件：
    </p>
    <pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">nano</span> /etc/modprobe.d/blacklist-nouveau.conf
</code></pre>
    <p>
     添加以下内容：
    </p>
    <pre><code class="prism language-plaintext">blacklist nouveau
options nouveau modeset=0
</code></pre>
    <p>
     保存文件后，更新 initramfs：
    </p>
    <pre><code class="prism language-bash"><span class="token function">sudo</span> update-initramfs <span class="token parameter variable">-u</span>
</code></pre>
    <h5>
     <a id="13__NVIDIA__28">
     </a>
     1.3 安装 NVIDIA 驱动
    </h5>
    <p>
     可以从 NVIDIA 官方网站下载适合你 GPU 型号和系统内核版本的驱动程序，也可以使用包管理器进行安装。以 Ubuntu 系统为例：
    </p>
    <pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">apt</span> update
<span class="token function">sudo</span> <span class="token function">apt</span> <span class="token function">install</span> nvidia-driver-<span class="token operator">&lt;</span>version<span class="token operator">&gt;</span>  <span class="token comment"># &lt;version&gt; 替换为适合的驱动版本号</span>
</code></pre>
    <p>
     安装完成后，重启系统：
    </p>
    <pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">reboot</span>
</code></pre>
    <p>
     重启后，使用
     <code>
      nvidia-smi
     </code>
     命令验证驱动是否安装成功，如果能正常显示 GPU 信息，则说明驱动安装成功。
    </p>
    <h4>
     <a id="2__NVIDIA__40">
     </a>
     2. 部署 NVIDIA 设备插件
    </h4>
    <p>
     NVIDIA 设备插件是一个 Kubernetes 插件，用于向 K8s 集群暴露 GPU 资源。可以使用以下 YAML 文件进行部署：
    </p>
    <pre><code class="prism language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> apps/v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> DaemonSet
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> nvidia<span class="token punctuation">-</span>device<span class="token punctuation">-</span>plugin<span class="token punctuation">-</span>daemonset
  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> kube<span class="token punctuation">-</span>system
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">selector</span><span class="token punctuation">:</span>
    <span class="token key atrule">matchLabels</span><span class="token punctuation">:</span>
      <span class="token key atrule">name</span><span class="token punctuation">:</span> nvidia<span class="token punctuation">-</span>device<span class="token punctuation">-</span>plugin<span class="token punctuation">-</span>ds
  <span class="token key atrule">template</span><span class="token punctuation">:</span>
    <span class="token key atrule">metadata</span><span class="token punctuation">:</span>
      <span class="token key atrule">annotations</span><span class="token punctuation">:</span>
        <span class="token key atrule">scheduler.alpha.kubernetes.io/critical-pod</span><span class="token punctuation">:</span> <span class="token string">""</span>
      <span class="token key atrule">labels</span><span class="token punctuation">:</span>
        <span class="token key atrule">name</span><span class="token punctuation">:</span> nvidia<span class="token punctuation">-</span>device<span class="token punctuation">-</span>plugin<span class="token punctuation">-</span>ds
    <span class="token key atrule">spec</span><span class="token punctuation">:</span>
      <span class="token key atrule">tolerations</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> CriticalAddonsOnly
        <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists
      <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> nvidia.com/gpu
        <span class="token key atrule">operator</span><span class="token punctuation">:</span> Exists
        <span class="token key atrule">effect</span><span class="token punctuation">:</span> NoSchedule
      <span class="token key atrule">containers</span><span class="token punctuation">:</span>
      <span class="token punctuation">-</span> <span class="token key atrule">image</span><span class="token punctuation">:</span> nvcr.io/nvidia/k8s<span class="token punctuation">-</span>device<span class="token punctuation">-</span>plugin<span class="token punctuation">:</span>v0.14.1
        <span class="token key atrule">name</span><span class="token punctuation">:</span> nvidia<span class="token punctuation">-</span>device<span class="token punctuation">-</span>plugin<span class="token punctuation">-</span>ctr
        <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>
          <span class="token key atrule">allowPrivilegeEscalation</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>
          <span class="token key atrule">capabilities</span><span class="token punctuation">:</span>
            <span class="token key atrule">drop</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"ALL"</span><span class="token punctuation">]</span>
        <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>
          <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> device<span class="token punctuation">-</span>plugin
            <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /var/lib/kubelet/device<span class="token punctuation">-</span>plugins
      <span class="token key atrule">volumes</span><span class="token punctuation">:</span>
        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> device<span class="token punctuation">-</span>plugin
          <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>
            <span class="token key atrule">path</span><span class="token punctuation">:</span> /var/lib/kubelet/device<span class="token punctuation">-</span>plugins
</code></pre>
    <p>
     使用以下命令部署该 DaemonSet：
    </p>
    <pre><code class="prism language-bash">kubectl apply <span class="token parameter variable">-f</span> nvidia-device-plugin.yaml
</code></pre>
    <h4>
     <a id="3__containerd_85">
     </a>
     3. 配置 containerd
    </h4>
    <h5>
     <a id="31__containerd__86">
     </a>
     3.1 修改 containerd 配置文件
    </h5>
    <p>
     通常，containerd 的配置文件位于
     <code>
      /etc/containerd/config.toml
     </code>
     。可以通过以下命令编辑该文件：
    </p>
    <pre><code class="prism language-bash"><span class="token function">sudo</span> <span class="token function">nano</span> /etc/containerd/config.toml
</code></pre>
    <p>
     在文件中添加或修改以下内容：
    </p>
    <pre><code class="prism language-toml">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
  BinaryName = "runc"

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
  privileged_without_host_devices = false
  runtime_engine = ""
  runtime_root = ""
  runtime_type = "io.containerd.runc.v2"
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
    BinaryName = "/usr/bin/nvidia-container-runtime"
    SystemdCgroup = true

[plugins."io.containerd.grpc.v1.cri"]
  default_runtime_name = "nvidia"
</code></pre>
    <p>
     上述配置中，我们添加了一个名为
     <code>
      nvidia
     </code>
     的运行时，其对应的二进制文件为
     <code>
      /usr/bin/nvidia-container-runtime
     </code>
     ，并将其设置为默认运行时。
    </p>
    <h5>
     <a id="32__nvidiacontainertoolkit_111">
     </a>
     3.2 安装
     <code>
      nvidia-container-toolkit
     </code>
    </h5>
    <p>
     <code>
      nvidia-container-toolkit
     </code>
     能让容器运行时（如 containerd）支持 GPU 设备。可以使用包管理器进行安装，例如在 Ubuntu 系统上：
    </p>
    <pre><code class="prism language-bash"><span class="token assign-left variable">distribution</span><span class="token operator">=</span><span class="token variable"><span class="token variable">$(</span><span class="token builtin class-name">.</span> /etc/os-release<span class="token punctuation">;</span><span class="token builtin class-name">echo</span> $ID$VERSION_ID<span class="token variable">)</span></span> <span class="token punctuation">\</span>
   <span class="token operator">&amp;&amp;</span> <span class="token function">curl</span> <span class="token parameter variable">-fsSL</span> https://nvidia.github.io/libnvidia-container/gpgkey <span class="token operator">|</span> <span class="token function">sudo</span> gpg <span class="token parameter variable">--dearmor</span> <span class="token parameter variable">-o</span> /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span class="token punctuation">\</span>
   <span class="token operator">&amp;&amp;</span> <span class="token function">curl</span> <span class="token parameter variable">-s</span> <span class="token parameter variable">-L</span> https://nvidia.github.io/libnvidia-container/<span class="token variable">$distribution</span>/libnvidia-container.list <span class="token operator">|</span> <span class="token punctuation">\</span>
         <span class="token function">sed</span> <span class="token string">'s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g'</span> <span class="token operator">|</span> <span class="token punctuation">\</span>
         <span class="token function">sudo</span> <span class="token function">tee</span> /etc/apt/sources.list.d/nvidia-container-toolkit.list
<span class="token function">sudo</span> <span class="token function">apt-get</span> update
<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> nvidia-container-toolkit
</code></pre>
    <h5>
     <a id="33__containerd__123">
     </a>
     3.3 重启 containerd 服务
    </h5>
    <p>
     修改配置文件后，需要重启 containerd 服务使配置生效：
    </p>
    <pre><code class="prism language-bash"><span class="token function">sudo</span> systemctl restart containerd
</code></pre>
    <h4>
     <a id="4_GPU__129">
     </a>
     4. GPU 虚拟化及部分使用
    </h4>
    <h5>
     <a id="41_GPU__130">
     </a>
     4.1 GPU 虚拟化
    </h5>
    <p>
     在 K8s 中，NVIDIA 提供了一些技术来实现 GPU 虚拟化，例如 NVIDIA Multi - Process Service（MPS）和 NVIDIA vGPU。
    </p>
    <ul>
     <li>
      <strong>
       NVIDIA MPS
      </strong>
      ：允许一个物理 GPU 被多个进程同时使用，通过时间片轮转的方式实现资源共享。要使用 MPS，需要在节点上启动 MPS 服务，然后在容器中配置使用 MPS。
     </li>
     <li>
      <strong>
       NVIDIA vGPU
      </strong>
      ：将一个物理 GPU 分割成多个虚拟 GPU 实例，每个实例有独立的显存和计算资源。使用 vGPU 需要特定的硬件和软件许可证支持。
     </li>
    </ul>
    <h5>
     <a id="42_GPU__136">
     </a>
     4.2 GPU 部分使用
    </h5>
    <p>
     在 K8s 中，可以通过请求部分 GPU 资源来实现 GPU 的部分使用。在 Pod 的 YAML 文件中，可以通过
     <code>
      resources.requests
     </code>
     和
     <code>
      resources.limits
     </code>
     字段来请求和限制 GPU 资源。例如：
    </p>
    <pre><code class="prism language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> gpu<span class="token punctuation">-</span>partial<span class="token punctuation">-</span>use<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> gpu<span class="token punctuation">-</span>partial<span class="token punctuation">-</span>use<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nvcr.io/nvidia/cuda<span class="token punctuation">:</span>11.0<span class="token punctuation">-</span>base
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token number">0.5</span>  <span class="token comment"># 请求 0.5 个 GPU</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token number">0.5</span>  <span class="token comment"># 限制最多使用 0.5 个 GPU</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"nvidia-smi"</span><span class="token punctuation">]</span>
</code></pre>
    <p>
     需要注意的是，部分 GPU 资源请求的支持依赖于 NVIDIA 驱动和设备插件的版本，某些版本可能不支持分数形式的 GPU 资源请求。
    </p>
    <h4>
     <a id="5__156">
     </a>
     5. 验证配置
    </h4>
    <p>
     部署完成后，可以创建一个简单的 Pod 来验证 GPU 是否可以正常使用。以下是一个示例 Pod 的 YAML 文件：
    </p>
    <pre><code class="prism language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1
<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod
<span class="token key atrule">metadata</span><span class="token punctuation">:</span>
  <span class="token key atrule">name</span><span class="token punctuation">:</span> gpu<span class="token punctuation">-</span>test<span class="token punctuation">-</span>pod
<span class="token key atrule">spec</span><span class="token punctuation">:</span>
  <span class="token key atrule">containers</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> gpu<span class="token punctuation">-</span>test<span class="token punctuation">-</span>container
    <span class="token key atrule">image</span><span class="token punctuation">:</span> nvcr.io/nvidia/cuda<span class="token punctuation">:</span>11.0<span class="token punctuation">-</span>base
    <span class="token key atrule">resources</span><span class="token punctuation">:</span>
      <span class="token key atrule">requests</span><span class="token punctuation">:</span>
        <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token number">1</span>
      <span class="token key atrule">limits</span><span class="token punctuation">:</span>
        <span class="token key atrule">nvidia.com/gpu</span><span class="token punctuation">:</span> <span class="token number">1</span>
    <span class="token key atrule">command</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"nvidia-smi"</span><span class="token punctuation">]</span>
</code></pre>
    <p>
     使用以下命令创建该 Pod：
    </p>
    <pre><code class="prism language-bash">kubectl apply <span class="token parameter variable">-f</span> gpu-test-pod.yaml
</code></pre>
    <p>
     然后查看 Pod 的日志：
    </p>
    <pre><code class="prism language-bash">kubectl logs gpu-test-pod
</code></pre>
    <p>
     如果能看到 NVIDIA GPU 的信息，则说明配置成功。
    </p>
    <p>
     通过以上步骤，你可以在使用 containerd 作为容器运行时的 K8s 集群中正确配置和使用 GPU 资源，并实现一定程度的 GPU 虚拟化和部分使用。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f5261696e7369726975732f:61727469636c652f64657461696c732f313436313635373433" class_="artid" style="display:none">
 </p>
</div>


