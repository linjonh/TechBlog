---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f5261696e7369726975732f:61727469636c652f64657461696c732f313436313635373433"
layout: post
title: "KubernetesK8s集群中使用-GPU"
date: 2025-03-10 22:28:52 +0800
description: "在 Kubernetes（K8s）集群中使用 GPU，需要完成安装驱动、部署插件、配置 containerd、实现 GPU 虚拟化及部分使用等一系列步骤，下面为你详细介绍。"
keywords: "Kubernetes（K8s）集群中使用 GPU"
categories: ['未分类']
tags: ['容器', 'Kubernetes']
artid: "146165743"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146165743
    alt: "KubernetesK8s集群中使用-GPU"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146165743
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146165743
cover: https://bing.ee123.net/img/rand?artid=146165743
image: https://bing.ee123.net/img/rand?artid=146165743
img: https://bing.ee123.net/img/rand?artid=146165743
---

# Kubernetes（K8s）集群中使用 GPU

在 Kubernetes（K8s）集群中使用 GPU，需要完成安装驱动、部署插件、配置 containerd、实现 GPU 虚拟化及部分使用等一系列步骤，下面为你详细介绍。

#### 1. 安装 GPU 驱动

以 NVIDIA GPU 为例，因为在深度学习和机器学习场景中 NVIDIA GPU 应用广泛，以下是在 Linux 系统上安装 NVIDIA 驱动的步骤：

##### 1.1 检查系统和 GPU 信息

首先需要确认系统的内核版本和 GPU 型号，使用以下命令：

```bash
uname -r  # 查看内核版本
lspci | grep -i nvidia  # 查看 GPU 型号

```

##### 1.2 禁用 Nouveau 驱动

Nouveau 是 Linux 系统默认的开源 NVIDIA 驱动，需要先禁用它，以免和 NVIDIA 官方驱动冲突。编辑
`/etc/modprobe.d/blacklist-nouveau.conf`
文件：

```bash
sudo nano /etc/modprobe.d/blacklist-nouveau.conf

```

添加以下内容：

```plaintext
blacklist nouveau
options nouveau modeset=0

```

保存文件后，更新 initramfs：

```bash
sudo update-initramfs -u

```

##### 1.3 安装 NVIDIA 驱动

可以从 NVIDIA 官方网站下载适合你 GPU 型号和系统内核版本的驱动程序，也可以使用包管理器进行安装。以 Ubuntu 系统为例：

```bash
sudo apt update
sudo apt install nvidia-driver-<version>  # <version> 替换为适合的驱动版本号

```

安装完成后，重启系统：

```bash
sudo reboot

```

重启后，使用
`nvidia-smi`
命令验证驱动是否安装成功，如果能正常显示 GPU 信息，则说明驱动安装成功。

#### 2. 部署 NVIDIA 设备插件

NVIDIA 设备插件是一个 Kubernetes 插件，用于向 K8s 集群暴露 GPU 资源。可以使用以下 YAML 文件进行部署：

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nvidia-device-plugin-daemonset
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: nvidia-device-plugin-ds
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
      labels:
        name: nvidia-device-plugin-ds
    spec:
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      containers:
      - image: nvcr.io/nvidia/k8s-device-plugin:v0.14.1
        name: nvidia-device-plugin-ctr
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
        volumeMounts:
          - name: device-plugin
            mountPath: /var/lib/kubelet/device-plugins
      volumes:
        - name: device-plugin
          hostPath:
            path: /var/lib/kubelet/device-plugins

```

使用以下命令部署该 DaemonSet：

```bash
kubectl apply -f nvidia-device-plugin.yaml

```

#### 3. 配置 containerd

##### 3.1 修改 containerd 配置文件

通常，containerd 的配置文件位于
`/etc/containerd/config.toml`
。可以通过以下命令编辑该文件：

```bash
sudo nano /etc/containerd/config.toml

```

在文件中添加或修改以下内容：

```toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
  BinaryName = "runc"

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
  privileged_without_host_devices = false
  runtime_engine = ""
  runtime_root = ""
  runtime_type = "io.containerd.runc.v2"
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
    BinaryName = "/usr/bin/nvidia-container-runtime"
    SystemdCgroup = true

[plugins."io.containerd.grpc.v1.cri"]
  default_runtime_name = "nvidia"

```

上述配置中，我们添加了一个名为
`nvidia`
的运行时，其对应的二进制文件为
`/usr/bin/nvidia-container-runtime`
，并将其设置为默认运行时。

##### 3.2 安装 `nvidia-container-toolkit`

`nvidia-container-toolkit`
能让容器运行时（如 containerd）支持 GPU 设备。可以使用包管理器进行安装，例如在 Ubuntu 系统上：

```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
   && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
   && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
         sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
         sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit

```

##### 3.3 重启 containerd 服务

修改配置文件后，需要重启 containerd 服务使配置生效：

```bash
sudo systemctl restart containerd

```

#### 4. GPU 虚拟化及部分使用

##### 4.1 GPU 虚拟化

在 K8s 中，NVIDIA 提供了一些技术来实现 GPU 虚拟化，例如 NVIDIA Multi - Process Service（MPS）和 NVIDIA vGPU。

* **NVIDIA MPS**
  ：允许一个物理 GPU 被多个进程同时使用，通过时间片轮转的方式实现资源共享。要使用 MPS，需要在节点上启动 MPS 服务，然后在容器中配置使用 MPS。
* **NVIDIA vGPU**
  ：将一个物理 GPU 分割成多个虚拟 GPU 实例，每个实例有独立的显存和计算资源。使用 vGPU 需要特定的硬件和软件许可证支持。

##### 4.2 GPU 部分使用

在 K8s 中，可以通过请求部分 GPU 资源来实现 GPU 的部分使用。在 Pod 的 YAML 文件中，可以通过
`resources.requests`
和
`resources.limits`
字段来请求和限制 GPU 资源。例如：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-partial-use-pod
spec:
  containers:
  - name: gpu-partial-use-container
    image: nvcr.io/nvidia/cuda:11.0-base
    resources:
      requests:
        nvidia.com/gpu: 0.5  # 请求 0.5 个 GPU
      limits:
        nvidia.com/gpu: 0.5  # 限制最多使用 0.5 个 GPU
    command: ["nvidia-smi"]

```

需要注意的是，部分 GPU 资源请求的支持依赖于 NVIDIA 驱动和设备插件的版本，某些版本可能不支持分数形式的 GPU 资源请求。

#### 5. 验证配置

部署完成后，可以创建一个简单的 Pod 来验证 GPU 是否可以正常使用。以下是一个示例 Pod 的 YAML 文件：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-test-pod
spec:
  containers:
  - name: gpu-test-container
    image: nvcr.io/nvidia/cuda:11.0-base
    resources:
      requests:
        nvidia.com/gpu: 1
      limits:
        nvidia.com/gpu: 1
    command: ["nvidia-smi"]

```

使用以下命令创建该 Pod：

```bash
kubectl apply -f gpu-test-pod.yaml

```

然后查看 Pod 的日志：

```bash
kubectl logs gpu-test-pod

```

如果能看到 NVIDIA GPU 的信息，则说明配置成功。

通过以上步骤，你可以在使用 containerd 作为容器运行时的 K8s 集群中正确配置和使用 GPU 资源，并实现一定程度的 GPU 虚拟化和部分使用。