---
layout: post
title: "Deepseek基础篇-v3基本架构"
date: 2025-03-12 21:12:04 +0800
description: "DeepSeek-V3 是一款采用 Mixture-of-Experts（MoE）架构的大型：61 层：7168：18432：128：129280：163840该模型通过精细的架构设计，实现了在计算效率和性能上的平衡。"
keywords: "【Deepseek基础篇】--v3基本架构"
categories: ['Deepseek']
tags: ['架构']
artid: "146052478"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146052478
    alt: "Deepseek基础篇-v3基本架构"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146052478
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146052478
cover: https://bing.ee123.net/img/rand?artid=146052478
image: https://bing.ee123.net/img/rand?artid=146052478
img: https://bing.ee123.net/img/rand?artid=146052478
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【Deepseek基础篇】--v3基本架构
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
    </p>
    <hr id="hr-toc" name="tableOfContents"/>
    <p>
    </p>
    <p>
     论文地址：
     <a href="https://arxiv.org/pdf/2412.19437" rel="nofollow" title="https://arxiv.org/pdf/2412.19437">
      https://arxiv.org/pdf/2412.19437
     </a>
    </p>
    <p>
     DeepSeek-V3 是一款采用 Mixture-of-Experts（MoE）架构的大型
     <a href="https://so.csdn.net/so/search?q=%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B&amp;spm=1001.2101.3001.7020" title="语言模型">
      语言模型
     </a>
     ，其核心参数配置如下：
     <a href="https://blog.csdn.net/qq_58602552/article/details/146072460?spm=1001.2014.3001.5501" title="【大模型理论篇】--Mixture of Experts架构-CSDN博客">
      【大模型理论篇】--Mixture of Experts架构-CSDN博客
     </a>
    </p>
    <ul>
     <li>
      <p>
       <strong>
        模型层数
       </strong>
       ：61 层
      </p>
     </li>
     <li>
      <p>
       <strong>
        隐藏层维度
       </strong>
       ：7168
      </p>
     </li>
     <li>
      <p>
       <strong>
        前馈网络维度
       </strong>
       ：18432
      </p>
     </li>
     <li>
      <p>
       <strong>
        注意力头数
       </strong>
       ：128
      </p>
     </li>
     <li>
      <p>
       <strong>
        词汇表大小
       </strong>
       ：129280
      </p>
     </li>
     <li>
      <p>
       <strong>
        最大位置嵌入
       </strong>
       ：163840
      </p>
      <p>
       该模型通过精细的架构设计，实现了在计算效率和性能上的平衡。
      </p>
     </li>
    </ul>
    <h2 id="MOE%E5%8F%82%E6%95%B0" name="MOE%E5%8F%82%E6%95%B0">
     MOE参数
    </h2>
    <blockquote>
     <table>
      <thead>
       <tr>
        <th>
         参数
        </th>
        <th>
         描述
        </th>
       </tr>
      </thead>
      <tbody>
       <tr>
        <td>
         MoE 层频率
        </td>
        <td>
         1--每一层都是 MoE 层
        </td>
       </tr>
       <tr>
        <td>
         共享专家数
        </td>
        <td>
         1
        </td>
       </tr>
       <tr>
        <td>
         路由专家数
        </td>
        <td>
         256
        </td>
       </tr>
       <tr>
        <td>
         每个 Token 选择的专家数
        </td>
        <td>
         8
        </td>
       </tr>
       <tr>
        <td>
         MoE 专家前馈网络维度
        </td>
        <td>
         2048
        </td>
       </tr>
       <tr>
        <td>
         总 MoE 层数
        </td>
        <td>
         58 层（第 4 层至第 61 层）
        </td>
       </tr>
       <tr>
        <td>
         每层专家总数
        </td>
        <td>
         257 个（1 个共享专家 + 256 个路由专家）
        </td>
       </tr>
       <tr>
        <td>
         模型总专家数
        </td>
        <td>
         14,906 个（257 个专家 × 58 层）
        </td>
       </tr>
       <tr>
        <td>
         每层活跃专家
        </td>
        <td>
         9 个（1 个共享专家 + 8 个路由专家）
        </td>
       </tr>
       <tr>
        <td>
         整个模型的活跃专家
        </td>
        <td>
         522 个（9 个活跃专家 × 58 层）
        </td>
       </tr>
       <tr>
        <td>
         计算效率高
        </td>
        <td>
         每个 Token 只需计算少量专家，降低了计算成本
        </td>
       </tr>
       <tr>
        <td>
         参数利用率高
        </td>
        <td>
         总参数量 6,710 亿，实际计算的激活参数仅约 370 亿
        </td>
       </tr>
       <tr>
        <td>
         专家专精化
        </td>
        <td>
         路由机制使得专家专注于特定特征，提高模型性能
        </td>
       </tr>
       <tr>
        <td>
         路由专家（Routed Experts）
        </td>
        <td>
        </td>
       </tr>
       <tr>
        <td>
         - 选择性激活
        </td>
        <td>
         按需激活，利用门控机制（如基于亲和度分数的 Top-K 选择）决定哪些专家处理当前 Token
        </td>
       </tr>
       <tr>
        <td>
         - 专精化处理
        </td>
        <td>
         每个路由专家擅长处理特定类型的输入或特征，实现专精化
        </td>
       </tr>
       <tr>
        <td>
         - 稀疏计算
        </td>
        <td>
         仅激活部分专家，提高计算效率
        </td>
       </tr>
       <tr>
        <td>
         - 负载均衡
        </td>
        <td>
         确保不同专家在不同输入上均衡被激活，避免过载
        </td>
       </tr>
       <tr>
        <td>
         共享专家（Shared Experts）
        </td>
        <td>
        </td>
       </tr>
       <tr>
        <td>
         - 全局参与
        </td>
        <td>
         始终参与所有输入的处理，贡献通用知识
        </td>
       </tr>
       <tr>
        <td>
         - 促进泛化
        </td>
        <td>
         捕捉数据中的普遍模式，减少过拟合风险
        </td>
       </tr>
       <tr>
        <td>
         - 提高稳定性
        </td>
        <td>
         提供稳定的基础，即使路由机制不完美时，也能有可靠的输出
        </td>
       </tr>
      </tbody>
     </table>
    </blockquote>
    <p>
    </p>
    <h2 id="1.%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84" name="1.%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84">
     1.基本架构
    </h2>
    <p>
     DeepSeek-V3模型的架构，包括
    </p>
    <ol>
     <li>
      基本架构、
     </li>
     <li>
      多头潜在注意力机制(MLA)、
     </li>
     <li>
      DeepSeekMoE
     </li>
     <li>
      多token预测(MTP)训练目标。
     </li>
    </ol>
    <p>
     DeepSeek-V3的架构设计在高效推理和经济高效的训练之间取得了良好的平衡，并通过引入辅助损失免费负载均衡策略和多token预测训练目标，进一步提升了模型的性能。
    </p>
    <blockquote>
     <p>
      DeepSeek-V3模型架构的核心特点是：
      <span style="color:#fe2c24">
       采用Multi-head Latent Attention (MLA) 以提高推理效率，采用DeepSeekMoE以降低训练成本
      </span>
      。此外，还引入了多token预测(MTP)训练目标，以提升模型性能。
     </p>
    </blockquote>
    <p class="img-center">
     <img alt="" height="428" src="https://i-blog.csdnimg.cn/direct/2512571877a14b48adee9d84640fc436.png" width="544"/>
    </p>
    <h3 id="1.1.%C2%A0Multi-Head%20Latent%20Attention%E5%A4%9A%E5%A4%B4%E6%BD%9C%E5%9C%A8%E6%B3%A8%E6%84%8F%E5%8A%9B" name="1.1.%C2%A0Multi-Head%20Latent%20Attention%E5%A4%9A%E5%A4%B4%E6%BD%9C%E5%9C%A8%E6%B3%A8%E6%84%8F%E5%8A%9B">
     1.1.
     <strong>
      <strong>
       <strong>
        Multi-Head Latent Attention多头潜在注意力
       </strong>
      </strong>
     </strong>
    </h3>
    <p class="img-center">
     <img alt="" height="299" src="https://i-blog.csdnimg.cn/direct/efba5fa10f914212a3d87ad288b709cf.png" width="423"/>
    </p>
    <ul>
     <li>
      &gt;&gt;核心思想：通过低秩联合压缩注意力键和值来减少推理过程中的键值缓存 (KV cache)。
     </li>
     <li>
      &gt;&gt; 具体方法：对注意力键和值进行低秩压缩，生成压缩的潜在向量 (
      <img alt="" height="29" src="https://i-blog.csdnimg.cn/direct/fbfd1536f06f4f888028df39e29b16fa.png" width="17">
       )。 对注意力查询也进行低秩压缩，生成压缩的潜在向量 (
       <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/c85ac32009684c6398ee70da36e5eb5e.png" width="28">
        )。 利用旋转位置编码 (RoPE) 生成解耦的键和查询 (
        <img alt="" height="34" src="https://i-blog.csdnimg.cn/direct/c6d1713a915b475c852cb001cb746456.png" width="29">
         ,
         <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/1fc5690c1c1144faaa1ac9816569f606.png" width="27">
          )。最终将查询、键和值结合起来生成最终的注意力输出 (
          <img alt="" height="32" src="https://i-blog.csdnimg.cn/direct/06ee80378634406b84549a3320916f58.png" width="25">
           )。
          </img>
         </img>
        </img>
       </img>
      </img>
     </li>
     <li>
      &gt;&gt; 优势：仅需缓存压缩的潜在向量和解耦的键/查询，显著减少了KV缓存，同时保持了与标准多头注意力 (MHA) 相当的性能。
     </li>
    </ul>
    <p>
     <span style="background-color:#ffd900">
      具体参数：
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="147" src="https://i-blog.csdnimg.cn/direct/45240297d95d46b59dbde9a2ce8f5e8d.png" width="317"/>
    </p>
    <blockquote>
     <p>
      <img alt="" height="243" src="https://i-blog.csdnimg.cn/direct/273f275264c54f0b8a735d358e57ea00.png" width="279"/>
      <img alt="" height="176" src="https://i-blog.csdnimg.cn/direct/c222d635df53483ab4ac373174407812.png" width="351"/>
     </p>
    </blockquote>
    <p>
     <span style="color:#000000">
      对 于 注 意 力 查 询 ， 我 们 也 执 行 低 秩 压 缩 ， 这 可 以 在 训 练 期 间 减 少 激 活 内 存：
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="131" src="https://i-blog.csdnimg.cn/direct/1d35f9b628834614885d9e9a5122f2ac.png" width="420"/>
    </p>
    <table>
     <thead>
      <tr>
      </tr>
     </thead>
     <tbody>
      <tr>
      </tr>
     </tbody>
    </table>
    <blockquote>
     <p class="img-center">
      <img alt="" height="286" src="https://i-blog.csdnimg.cn/direct/f7eaeca380c6407b9bac7afcb9a5e4be.png" width="399"/>
     </p>
     <p class="img-center">
      <img alt="" height="242" src="https://i-blog.csdnimg.cn/direct/cdf635c1d30745e89ebdfd4af0c9c539.png" width="400"/>
     </p>
    </blockquote>
    <p>
    </p>
    <p>
     <span style="color:#000000">
      最 终 ， 注 意 力 查 询
     </span>
     <span style="color:#000000">
     </span>
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/265774d4f489476abcac220f720adf5f.png" width="21"/>
     <span style="color:#000000">
      、 键 （
     </span>
     <img alt="" height="25" src="https://i-blog.csdnimg.cn/direct/894b14f0c11e4182afffe2c1539e2d46.png" width="35"/>
     <span style="color:#000000">
      ） 和 值 (
     </span>
     <img alt="" height="36" src="https://i-blog.csdnimg.cn/direct/f28f796081734861896fc244074a335f.png" width="42"/>
     <span style="color:#000000">
      ) 被 组 合 起 来 以 生 成 最 终 的 注 意 力 输 出
      <strong>
       u
      </strong>
      T
     </span>
     <span style="color:#000000">
      :
     </span>
     <span style="color:#000000">
      。
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="109" src="https://i-blog.csdnimg.cn/direct/ad42564004304dbe824f8dffb8344459.png" width="309"/>
    </p>
    <p>
    </p>
    <h3 id="1.2.%E6%97%A0%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%20DeepSeekMoE" name="1.2.%E6%97%A0%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%9A%84%20DeepSeekMoE">
     1.2.无辅助损失负载均衡的 DeepSeekMoE
    </h3>
    <p class="img-center">
     <img alt="" height="270" src="https://i-blog.csdnimg.cn/direct/74b732428e154679b18ba800cf39ae42.png" width="469"/>
    </p>
    <p>
     对于前馈网络（FFNs），DeepSeek-V3 采用 DeepSeekMoE 架构（Dai et al., 2024）。与 GShard（Lepikhin et al., 2021）等传统 MoE 架构相比，DeepSeekMoE 使用更细粒度的专家（experts），并将部分专家隔离为共享专家。设  为第 𝑡 个 token 的 FFN 输入，我们计算其 FFN 输出
     <img alt="" height="25" src="https://i-blog.csdnimg.cn/direct/74c2710c16fe4bef92c3c716c743e7c6.png" width="24"/>
     如下：
    </p>
    <p class="img-center">
     <img alt="" height="288" src="https://i-blog.csdnimg.cn/direct/8a62817868b44044a7fa887ce8665919.png" width="497"/>
    </p>
    <blockquote>
     <p class="img-center">
      <img alt="" height="367" src="https://i-blog.csdnimg.cn/direct/6070bc62d7ca413a82047ed045f80b6d.png" width="504"/>
     </p>
     <p>
     </p>
    </blockquote>
    <p>
     <strong>
      <span style="color:#000000">
       无 辅 助 损 失
      </span>
      <span style="color:#000000">
       的
      </span>
      <span style="color:#000000">
       负 载
      </span>
      <span style="color:#000000">
       均 衡：
      </span>
     </strong>
    </p>
    <p style="text-align:justify">
     对于 MoE 模型，专家负载的不均衡会导致路由崩溃（Shazeer et al., 2017），并在专家并行的场景中降低计算效率。传统的解决方案通常依赖于辅助损失（Fedus et al., 2021；Lepikhin et al., 2021）来避免负载不均衡。然而，过大的辅助损失会损害模型的性能（Wang et al., 2024a）。为了在负载均衡和模型性能之间实现更好的平衡，我们开创了一种无辅助损失的负载均衡策略（Wang et al., 2024a），以确保负载均衡。具体来说，我们为每个专家引入一个偏置项
     <img alt="" height="17" src="https://i-blog.csdnimg.cn/direct/19982a0af5a5461384ad875ab0d89edb.png" width="13"/>
     ，并将其添加到对应的亲和度分数
     <img alt="" height="15" src="https://i-blog.csdnimg.cn/direct/0ac9a6ee42c144d095ea69ceeb87ad47.png" width="22"/>
     中，以确定 top-K 路由：
    </p>
    <p class="img-center">
     <img alt="" height="73" src="https://i-blog.csdnimg.cn/direct/0b7a035038504cd2bf44cd32ca87ba8c.png" width="528"/>
    </p>
    <blockquote>
     <p style="text-align:justify">
      注意，偏置项仅用于路由。门控值（gating value），即将与 FFN 输出相乘的值，仍然来自于原始的亲和度分数
      <img alt="" height="15" src="https://i-blog.csdnimg.cn/direct/e1b36ec056334a5f9d7b9ec6636a4dc2.png" width="22"/>
      。 在训练过程中，我们会持续监控每个训练步骤中整个批次的专家负载。在每个步骤结束时，如果对应的专家超载，我们将偏置项减少 𝛾；如果对应的专家负载不足，我们将其增加 𝛾，其中 𝛾 是一个超参数，称为偏置更新速度（bias update speed）。通过这种动态调整，DeepSeek-V3 在训练过程中保持专家负载平衡，并且相比于仅通过纯辅助损失来鼓励负载平衡的模型，能够实现更好的性能。
     </p>
    </blockquote>
    <p>
     <strong>
      <span style="color:#000000">
       互 补 序 列 级 辅 助 损 失：
      </span>
     </strong>
     尽管 DeepSeek-V3 主要依赖于无辅助损失策略来实现负载平衡，但为了防止单个序列内部出现极端的不平衡，我们还使用了一个互补的序列级平衡损失：
    </p>
    <p class="img-center">
     <img alt="" height="284" src="https://i-blog.csdnimg.cn/direct/1d016e2c7b2749cc9fec94cca025fece.png" width="514"/>
    </p>
    <ol>
     <li style="text-align: justify;">
      平衡因子 𝛼 是一个超参数，对于 DeepSeek-V3 来说将赋予一个极小的值；
     </li>
     <li style="text-align: justify;">
      1(·) 表示指示函数；
     </li>
     <li style="text-align: justify;">
      𝑇 表示序列中的标记数。序列级平衡损失鼓励每个序列的专家负载保持平衡。
     </li>
    </ol>
    <p class="img-center">
     <img alt="" height="268" src="https://i-blog.csdnimg.cn/direct/f312660b43ec40f9904b31cb81362eec.png" width="591"/>
    </p>
    <p>
     <span style="color:#000000">
      多 标 记 预 测 （
     </span>
     <span style="color:#000000">
      MTP
     </span>
     <span style="color:#000000">
      ） 实 现 方 式 的 示 意 图 。 在 每 个 深 度 ， 我 们 为 每 个 标 记 的 预 测 保 留 完 整 的 因 果 链 。
     </span>
    </p>
    <blockquote>
     <p>
      <span style="background-color:#ff9900">
       节点限制路由：
      </span>
      与 DeepSeek-V2 使用的设备限制路由类似，DeepSeek-V3 也使用了一种受限路由机制，以限制训练过程中的通信成本。简而言之，我们确保每个标记最多只会发送到 𝑀 个节点，这些节点是根据分布在每个节点上的专家的前  个亲和度分数的总和来选择的。在这个约束下，我们的 MoE 训练框架几乎可以实现完全的计算-通信重叠。
     </p>
     <p>
     </p>
     <p>
      <span style="background-color:#fe2c24">
       无Token丢弃：
      </span>
      由于有效的负载平衡策略，DeepSeek-V3 在整个训练过程中保持良好的负载平衡。因此，DeepSeek-V3 在训练期间不会丢弃任何Token。此外，我们还实施了特定的部署策略，以确保推理过程中的负载平衡，因此 DeepSeek-V3 在推理过程中也不会丢弃Token。
     </p>
    </blockquote>
    <hr/>
    <h2 id="2.%E5%A4%9A%E6%A0%87%E8%AE%B0%E9%A2%84%E6%B5%8B" name="2.%E5%A4%9A%E6%A0%87%E8%AE%B0%E9%A2%84%E6%B5%8B">
     2.多标记预测
    </h2>
    <p>
     受到 Gloeckle 等人（2024）的启发，我们为 DeepSeek-V3 设定了一个多标记预测（MTP）目标，将预测范围扩展到每个位置的多个未来标记。一方面，MTP 目标可以密集化训练信号，可能提高数据效率。另一方面，MTP 可以使模型预先规划其表示，以便更好地预测未来的标记。图 3 展示了我们的 MTP 实现。与 Gloeckle 等人（2024）通过独立输出头并行预测 𝐷 个额外标记不同，我们依次预测额外的标记，并在每个预测深度保持完整的因果链条。在本节中，我们介绍了 MTP 实现的细节。
    </p>
    <h3 id="2.1.%C2%A0MTP%20%E6%A8%A1%E5%9D%97" name="2.1.%C2%A0MTP%20%E6%A8%A1%E5%9D%97">
     2.1.
     <strong>
      MTP 模块
     </strong>
    </h3>
    <p style="text-align:justify">
     MTP 实现使用 𝐷 个顺序模块来预测 𝐷 个额外的标记。第 𝑘 个 MTP 模块由一个共享嵌入层 Emb(·)、一个共享输出头 OutHead(·)、一个 Transformer 块 TRM𝑘 (·) 和一个投影矩阵 𝑀𝑘 ∈
     <img alt="" height="17" src="https://i-blog.csdnimg.cn/direct/5e44ec5a697b4cb583c41a772d68562d.png" width="47"/>
     组成。对于第 𝑖 个输入标记 ，在第 𝑘 个预测深度，我们首先将第 𝑖 个标记在第 (𝑘−1) 深度的表示  ∈
     <img alt="" height="21" src="https://i-blog.csdnimg.cn/direct/484cedccd98d4025af2e4bd1f97f1e2a.png" width="82"/>
     与第 (𝑖+𝑘) 个标记的嵌入
     <img alt="" height="43" src="https://i-blog.csdnimg.cn/direct/71e6873c0afc4fa7a496618481deaba1.png" width="144"/>
     结合，并进行线性投影：
    </p>
    <p class="img-center">
     <img alt="" height="50" src="https://i-blog.csdnimg.cn/direct/546d6dc90ee443f4b38f07d9c7021bde.png" width="414"/>
    </p>
    <blockquote>
     <p>
      其中，[·; ·] 表示拼接操作。特别地，当 𝑘 = 1 时，
      <img alt="h^{k-1}_{i}" src="https://latex.csdn.net/eq?h%5E%7Bk-1%7D_%7Bi%7D"/>
      指的是由主模型给出的表示。需要注意的是，对于每个 MTP 模块，它的嵌入层与主模型共享。组合后的
      <img alt="h'^{k}_{i}" src="https://latex.csdn.net/eq?h%27%5E%7Bk%7D_%7Bi%7D"/>
      作为 Transformer 块第 𝑘 深度的输入，以生成当前深度的输出表示
      <img alt="h^{k}_{i}" src="https://latex.csdn.net/eq?h%5E%7Bk%7D_%7Bi%7D"/>
      ：
     </p>
     <p class="img-center">
      <img alt="" height="93" src="https://i-blog.csdnimg.cn/direct/99ca5ec4c3a6424d9590999ffb88f215.png" width="412"/>
     </p>
    </blockquote>
    <p>
     其中，𝑇 表示输入序列的长度，𝑖:𝑗 表示切片操作（包括左右边界）。最后，以
     <img alt="h^{k}_{i}" src="https://latex.csdn.net/eq?h%5E%7Bk%7D_%7Bi%7D"/>
     作为输入，共享的输出头将计算第 𝑘 个附加预测 token 的概率分布
     <img alt="p^{k}_{i+1+k}" src="https://latex.csdn.net/eq?p%5E%7Bk%7D_%7Bi&amp;plus;1&amp;plus;k%7D"/>
     ∈
     <img alt="\mathbb{R}^{V}" src="https://latex.csdn.net/eq?%5Cmathbb%7BR%7D%5E%7BV%7D"/>
     ，其中 𝑉 是词汇表的大小：
    </p>
    <p class="img-center">
     <img alt="" height="36" src="https://i-blog.csdnimg.cn/direct/d7bcd90e91ff4417aa8d4d88c27a9011.png" width="228"/>
    </p>
    <p>
     输出头 OutHead(·) 将表示映射到 logits，并随后应用 Softmax(·) 函数来计算第 𝑘 个附加 token 的预测概率。同时，对于每个 MTP 模块，它的输出头与主模型共享。我们保持预测的因果链的原则与 EAGLE (Li et al., 2024b) 类似，但它的主要目标是推测解码 (Leviathan et al., 2023; Xia et al., 2023)，而我们利用 MTP 来改进训练。
    </p>
    <p>
     MTP 训练目标。在每个预测深度，我们计算交叉熵损失
     <img alt="" height="35" src="https://i-blog.csdnimg.cn/direct/b01d523aca064b40a01f72bdbbe62e2c.png" width="43"/>
     ：
    </p>
    <p class="img-center">
     <img alt="" height="73" src="https://i-blog.csdnimg.cn/direct/fadd10af19d54a859f7c83c80ad3e630.png" width="509"/>
    </p>
    <p>
     其中，𝑇 表示输入序列的长度，
     <img alt="t_{i}" src="https://latex.csdn.net/eq?t_%7Bi%7D"/>
     表示第 𝑖 个位置的真实标记，
     <img alt="p^{k}_{i}[t_{i}]" src="https://latex.csdn.net/eq?p%5E%7Bk%7D_%7Bi%7D%5Bt_%7Bi%7D%5D"/>
     表示给定 𝑘-th MTP 模块的预测概率。最后，我们计算所有深度的 MTP 损失的平均值，并将其乘以一个权重因子 𝜆，得到最终的 MTP 损失 LMTP，该损失作为 DeepSeek-V3 的附加训练目标：
    </p>
    <p class="img-center">
     <img alt="" height="76" src="https://i-blog.csdnimg.cn/direct/3497e29a1f124bc081d31719f3df6ee1.png" width="216"/>
    </p>
    <blockquote>
     <p>
      MTP 在推理中的应用。我们的 MTP 策略主要旨在提升主模型的性能，因此，在推理过程中，我们可以直接丢弃 MTP 模块，主模型可以独立且正常地运行。此外，我们还可以将这些 MTP 模块重新用于推测解码，以进一步提升生成的延迟表现。
     </p>
    </blockquote>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f35383630323535322f:61727469636c652f64657461696c732f313436303532343738" class_="artid" style="display:none">
 </p>
</div>


