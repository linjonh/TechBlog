---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f33373937303232342f:61727469636c652f64657461696c732f313436313434303734"
layout: post
title: "机器学习李宏毅Auto-Encoder"
date: 2025-03-10 10:50:21 +0800
description: "机器学习（李宏毅）——Auto-Encoder"
keywords: "机器学习（李宏毅）——Auto-Encoder"
categories: ['未分类']
tags: ['机器学习', '人工智能']
artid: "146144074"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146144074
    alt: "机器学习李宏毅Auto-Encoder"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146144074
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146144074
cover: https://bing.ee123.net/img/rand?artid=146144074
image: https://bing.ee123.net/img/rand?artid=146144074
img: https://bing.ee123.net/img/rand?artid=146144074
---

# 机器学习（李宏毅）——Auto-Encoder

#### 一、前言

本文章作为学习2023年《李宏毅机器学习课程》的笔记，感谢台湾大学李宏毅教授的课程，respect！！！

#### 二、大纲

1. Basic Idea of Auto-encoder
2. Feature Disentanglement
3. Discrete Latent Representation
4. More Applications

#### 三、Basic Idea of Auto-encoder

***Self-supervised Learning Framework***

在讲Auto-encoder之前，先再温习一下Self-supervised Learning Framework，即自监督学习框架。

最典型的就是BERT模型了，可参照本人BERT博客。框架如下图所示：

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/25f23d436f12434eb8d86cece9aaf141.png)
  
一言以蔽之：有一堆没有标注的资料data，把资料的部分遮住，丢到预训练模型里面，期待输出和遮住部分越接近越好。换句话说就是在做填空题。然后预训练模型可针对不同下游任务进行微调和应用。

***Auto-encoder***
  
Auto-encoder也可以看成是Self-supervised Learning的一种，虽然Auto-encoder比Self-supervised Learning提出的还要早。

举例而言，将Auto-encoder应用在图像生成领域，那就是类似于Cycle GAN的思想，如下图：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9142d0bfbef84ddb93e6a35ef522108d.png)
  
说明：有一堆未标注的图片，经过NN Encoder变成一个向量vector，再经过NN Decoder还原至原来的输入图片，期待输入输出越接近越好，这就是Auto-encoder的基本思想。

那为什么这招能够work呢？
  
我个人的理解这是一种空间映射，将图像空间和矩阵空间映射在一起，而这个映射关系完全可以自己定义。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/13c82d5e67fc46c0a6a471a6683f2981.png)
  
什么叫自己定义，举例而言，假设矩阵就两列，如果图片中看到光头的，矩阵第一维值就是1，其他为0。没看到光头，矩阵第一维值就是0，其他为1。这是一种方法，当然还可以是其他方法，比如男女分开、黑发黄发等。

这让我想到了一个游戏（没玩过的可以pass）：这是开，这是闭，这是开还是闭。文字“开”、“闭”完全可以映射到手势、嘴巴等部分，但是要符合规律即可，起初没人告诉我“开”、“闭”应该对应什么，好似Auto-encoder是Self-supervised一样，需要去找一种映射关系，满足输出结果规律。

***De-noising Auto-encoder***
  
这也是Auto-encoder的一种，只不过是加入了noises，如下图所示：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7f38c2f8e82f4725b2056c859e08e587.png)
  
说明：先对原始图片加入杂讯，杂讯图片再丢入Auto-encoder中，期待输入的结果和原始图片越像越好，这不就和BERT一模一样吗！BERT是在做填空题，它是在去杂讯。（不禁让我想起了去马的操作，想跃跃欲试了~）

以下是把de-noising auto-encoder思想套进BERT：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/00c2057641c44f2fbe862fe430a28087.png)

#### 四、Feature Disentanglement

什么叫做Feature Disentanglement？
  
就是不要让Feature 纠结在一起，要把它们分开来。

举例而言，语音辨识经过Encoder的向量既有语者的资讯，又有声音的内容，如下图：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e13aec61a54e453aa829f81fcd707584.png)
  
那怎么把这两者分开呢？参考左下角的链接。

分开后又有啥应用呢？
  
柯南变声器，或者是可以伪造别人的声音，听着这操作很骚啊~

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/337e3b91bcda46c1a1e0f93d1b1d3ee5.png)
  
说明：
  
将新垣结衣的语者特征和李宏毅老师的文字内容拼接，就可以以新垣结衣的声音来上课了。

#### 五、Discrete Latent Representation

啥是Discrete Latent Representation，翻译成中文是离散潜在表示，其实这里就是针对Encoder得到的向量而言，向量这一块的表示方法，上面也提到了这块内容，就是你爱咋定义映射关系就咋定义，于是乎有了各种各样的定义方式，如下图：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6d5b57ce545c4ab6bc892ea7ede137ac.png)
  
说明：可以是Real numbers、Binary、One-hot等。

也可以设定一个索引表（codebook），看这个编码向量和索引表中哪个向量越接近（类似于self-attention中的q、k），如下图：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/422a8c78683e4e11a818ce02f52a32b2.png)
  
更狂一点的就是Encoder出来的不一样要是向量，直接就是一段文字：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/cf68b965015f4e95bac630c728366ff3.png)
  
直接一段文字，那不就是摘要吗？对，没错！
  
为了让模型学会说人话，还需要一个老师（Discriminator），如下图：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/42b4d0d9eb5a42ab90913ceb47a5bd04.png)

#### 六、More Applications

不得不说，真是博大精深啊！
  
当然，还有其他应用，比如：
  
1、Generator：把Decoder拿出来，直接当成图片生成器来用。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3739c83f043f43d1b9be983273ce2c59.png)
  
2、Compression（压缩）
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2db796d683914800bb8858dd2c42402d.png)
  
NN Encoder就是在做压缩Compression，可以直接拿来做图片压缩相关工作，再通过NN Decoder解压缩回去（Decompression），虽然会有一点图片失真。

3、Anomaly Detection（异常检测）
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9af2d3797d324c6791ba522c1d5ff106.png)
  
思想很直接，就是如果今天是一张不同于训练集的图片送入Auto-encoder中，那是还原不回去的，domain不一样嘞，从而可以检测出异常。

可以应用在Fraud Detection（欺诈检测）、Network Intrusion Detection（网络入侵检测）、Cancer Detection（癌症检测）等领域。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/660e52d0fd924291950a752f74bd94bd.png)