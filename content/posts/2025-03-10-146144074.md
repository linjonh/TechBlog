---
layout: post
title: "机器学习李宏毅Auto-Encoder"
date: 2025-03-10 10:50:21 +0800
description: "机器学习（李宏毅）——Auto-Encoder"
keywords: "机器学习（李宏毅）——Auto-Encoder"
categories: ['未分类']
tags: ['机器学习', '人工智能']
artid: "146144074"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146144074
    alt: "机器学习李宏毅Auto-Encoder"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146144074
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146144074
cover: https://bing.ee123.net/img/rand?artid=146144074
image: https://bing.ee123.net/img/rand?artid=146144074
img: https://bing.ee123.net/img/rand?artid=146144074
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     机器学习（李宏毅）——Auto-Encoder
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h4>
     <a id="_0">
     </a>
     一、前言
    </h4>
    <p>
     本文章作为学习2023年《李宏毅机器学习课程》的笔记，感谢台湾大学李宏毅教授的课程，respect！！！
    </p>
    <h4>
     <a id="_2">
     </a>
     二、大纲
    </h4>
    <ol>
     <li>
      Basic Idea of Auto-encoder
     </li>
     <li>
      Feature Disentanglement
     </li>
     <li>
      Discrete Latent Representation
     </li>
     <li>
      More Applications
     </li>
    </ol>
    <h4>
     <a id="Basic_Idea_of_Autoencoder_9">
     </a>
     三、Basic Idea of Auto-encoder
    </h4>
    <p>
     <em>
      <strong>
       Self-supervised Learning Framework
      </strong>
     </em>
    </p>
    <p>
     在讲Auto-encoder之前，先再温习一下Self-supervised Learning Framework，即自监督学习框架。
    </p>
    <p>
     最典型的就是BERT模型了，可参照本人BERT博客。框架如下图所示：
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/25f23d436f12434eb8d86cece9aaf141.png">
      <br/>
      一言以蔽之：有一堆没有标注的资料data，把资料的部分遮住，丢到预训练模型里面，期待输出和遮住部分越接近越好。换句话说就是在做填空题。然后预训练模型可针对不同下游任务进行微调和应用。
     </img>
    </p>
    <p>
     <em>
      <strong>
       Auto-encoder
      </strong>
     </em>
     <br/>
     Auto-encoder也可以看成是Self-supervised Learning的一种，虽然Auto-encoder比Self-supervised Learning提出的还要早。
    </p>
    <p>
     举例而言，将Auto-encoder应用在图像生成领域，那就是类似于Cycle GAN的思想，如下图：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9142d0bfbef84ddb93e6a35ef522108d.png">
      <br/>
      说明：有一堆未标注的图片，经过NN Encoder变成一个向量vector，再经过NN Decoder还原至原来的输入图片，期待输入输出越接近越好，这就是Auto-encoder的基本思想。
     </img>
    </p>
    <p>
     那为什么这招能够work呢？
     <br/>
     我个人的理解这是一种空间映射，将图像空间和矩阵空间映射在一起，而这个映射关系完全可以自己定义。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/13c82d5e67fc46c0a6a471a6683f2981.png">
      <br/>
      什么叫自己定义，举例而言，假设矩阵就两列，如果图片中看到光头的，矩阵第一维值就是1，其他为0。没看到光头，矩阵第一维值就是0，其他为1。这是一种方法，当然还可以是其他方法，比如男女分开、黑发黄发等。
     </img>
    </p>
    <p>
     这让我想到了一个游戏（没玩过的可以pass）：这是开，这是闭，这是开还是闭。文字“开”、“闭”完全可以映射到手势、嘴巴等部分，但是要符合规律即可，起初没人告诉我“开”、“闭”应该对应什么，好似Auto-encoder是Self-supervised一样，需要去找一种映射关系，满足输出结果规律。
    </p>
    <p>
     <em>
      <strong>
       De-noising Auto-encoder
      </strong>
     </em>
     <br/>
     这也是Auto-encoder的一种，只不过是加入了noises，如下图所示：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/7f38c2f8e82f4725b2056c859e08e587.png">
      <br/>
      说明：先对原始图片加入杂讯，杂讯图片再丢入Auto-encoder中，期待输入的结果和原始图片越像越好，这不就和BERT一模一样吗！BERT是在做填空题，它是在去杂讯。（不禁让我想起了去马的操作，想跃跃欲试了~）
     </img>
    </p>
    <p>
     以下是把de-noising auto-encoder思想套进BERT：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/00c2057641c44f2fbe862fe430a28087.png"/>
    </p>
    <h4>
     <a id="Feature_Disentanglement_41">
     </a>
     四、Feature Disentanglement
    </h4>
    <p>
     什么叫做Feature Disentanglement？
     <br/>
     就是不要让Feature 纠结在一起，要把它们分开来。
    </p>
    <p>
     举例而言，语音辨识经过Encoder的向量既有语者的资讯，又有声音的内容，如下图：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/e13aec61a54e453aa829f81fcd707584.png">
      <br/>
      那怎么把这两者分开呢？参考左下角的链接。
     </img>
    </p>
    <p>
     分开后又有啥应用呢？
     <br/>
     柯南变声器，或者是可以伪造别人的声音，听着这操作很骚啊~
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/337e3b91bcda46c1a1e0f93d1b1d3ee5.png">
      <br/>
      说明：
      <br/>
      将新垣结衣的语者特征和李宏毅老师的文字内容拼接，就可以以新垣结衣的声音来上课了。
     </img>
    </p>
    <h4>
     <a id="Discrete_Latent_Representation_55">
     </a>
     五、Discrete Latent Representation
    </h4>
    <p>
     啥是Discrete Latent Representation，翻译成中文是离散潜在表示，其实这里就是针对Encoder得到的向量而言，向量这一块的表示方法，上面也提到了这块内容，就是你爱咋定义映射关系就咋定义，于是乎有了各种各样的定义方式，如下图：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/6d5b57ce545c4ab6bc892ea7ede137ac.png"/>
     <br/>
     说明：可以是Real numbers、Binary、One-hot等。
    </p>
    <p>
     也可以设定一个索引表（codebook），看这个编码向量和索引表中哪个向量越接近（类似于self-attention中的q、k），如下图：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/422a8c78683e4e11a818ce02f52a32b2.png"/>
     <br/>
     更狂一点的就是Encoder出来的不一样要是向量，直接就是一段文字：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/cf68b965015f4e95bac630c728366ff3.png"/>
     <br/>
     直接一段文字，那不就是摘要吗？对，没错！
     <br/>
     为了让模型学会说人话，还需要一个老师（Discriminator），如下图：
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/42b4d0d9eb5a42ab90913ceb47a5bd04.png"/>
    </p>
    <h4>
     <a id="More_Applications_67">
     </a>
     六、More Applications
    </h4>
    <p>
     不得不说，真是博大精深啊！
     <br/>
     当然，还有其他应用，比如：
     <br/>
     1、Generator：把Decoder拿出来，直接当成图片生成器来用。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/3739c83f043f43d1b9be983273ce2c59.png"/>
     <br/>
     2、Compression（压缩）
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/2db796d683914800bb8858dd2c42402d.png"/>
     <br/>
     NN Encoder就是在做压缩Compression，可以直接拿来做图片压缩相关工作，再通过NN Decoder解压缩回去（Decompression），虽然会有一点图片失真。
    </p>
    <p>
     3、Anomaly Detection（异常检测）
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9af2d3797d324c6791ba522c1d5ff106.png"/>
     <br/>
     思想很直接，就是如果今天是一张不同于训练集的图片送入Auto-encoder中，那是还原不回去的，domain不一样嘞，从而可以检测出异常。
    </p>
    <p>
     可以应用在Fraud Detection（欺诈检测）、Network Intrusion Detection（网络入侵检测）、Cancer Detection（癌症检测）等领域。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/660e52d0fd924291950a752f74bd94bd.png"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f33373937303232342f:61727469636c652f64657461696c732f313436313434303734" class_="artid" style="display:none">
 </p>
</div>


