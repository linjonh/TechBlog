---
layout: post
title: "Prompt-Learning-Awesome"
date: 2025-03-13 22:48:21 +0800
description: "prompt learning领域顶会论文总结"
keywords: "Prompt Learning Awesome"
categories: ['未分类']
tags: ['深度学习', '机器学习', '人工智能', 'Prompt']
artid: "146244158"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146244158
    alt: "Prompt-Learning-Awesome"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146244158
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146244158
cover: https://bing.ee123.net/img/rand?artid=146244158
image: https://bing.ee123.net/img/rand?artid=146244158
img: https://bing.ee123.net/img/rand?artid=146244158
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Prompt Learning Awesome
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p id="ua4450704">
    </p>
    <p>
     <em>
      <strong>
       提示调整是一种将预训练的视觉语言模型 (VLM) 适应各种下游任务的宝贵技术
      </strong>
     </em>
    </p>
    <hr id="hr-toc" name="tableOfContents"/>
    <p>
    </p>
    <h4 id="aXutz" name="aXutz">
     COOP: Prompt Learning for Vision-Language Models
    </h4>
    <p id="uc83cdea5">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u039b9f73">
     🔹
     <strong>
      Prompt Engineering 难以手工优化
     </strong>
     ：
    </p>
    <ul>
     <li id="u0f83405c">
      现有的
      <strong>
       CLIP
      </strong>
      依赖于
      <strong>
       手工设计的 Prompt（Prompt Engineering）
      </strong>
      ，但
      <strong>
       稍微改变 Prompt 的措辞可能会严重影响模型表现
      </strong>
      ，因此需要
      <strong>
       大量试错
      </strong>
      。
     </li>
     <li id="u663d32aa">
      设计合适的 Prompt 需要
      <strong>
       领域知识
      </strong>
      ，并且不同任务（如医学、遥感）可能需要不同的 Prompt 设计，
      <strong>
       泛化性较差
      </strong>
      。
     </li>
    </ul>
    <p id="u544e98f8">
     🔹
     <strong>
      Zero-shot 任务的 Prompt 泛化能力受限
     </strong>
     ：
    </p>
    <ul>
     <li id="u9abf6ac5">
      由于 CLIP 的 Prompt 主要是
      <strong>
       固定的文本模板
      </strong>
      ，无法适应不同的数据分布，在特定
      <strong>
       下游任务（如图像分类）
      </strong>
      时，手工 Prompt
      <strong>
       难以在所有类别上泛化
      </strong>
      。
     </li>
    </ul>
    <hr id="ywzGK"/>
    <p id="uc7637fac">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u15335084">
     ✅
     <strong>
      提出 CoOp（Context Optimization），用可训练的 Soft Prompt 代替手工 Prompt
     </strong>
    </p>
    <ul>
     <li id="uac526c97">
      通过
      <strong>
       可训练的 Prompt 向量
      </strong>
      （Soft Prompt）替换 CLIP 的手工文本模板：
     </li>
    </ul>
    <pre id="IS35Z"><code>[P1, P2, P3, ..., Pn, {class}]</code></pre>
    <ul>
     <li id="ud64b1f92">
      <strong>
       Prompt 的 Context 部分可训练
      </strong>
      ，而 CLIP 本身的权重保持不变，提高适应性。
     </li>
    </ul>
    <p id="uf7b3a9ca">
     ✅
     <strong>
      提出两种 Prompt 结构，提升适配性
     </strong>
    </p>
    <ul>
     <li id="u78b11ba1">
      <strong>
       Unified Context（统一 Prompt）
      </strong>
      ：所有类别共享一个
      <strong>
       Soft Prompt
      </strong>
      ，适用于小规模任务。
     </li>
     <li id="u8a6813a8">
      <strong>
       Class-Specific Context（类别特定 Prompt）
      </strong>
      ：不同类别拥有不同的 Prompt，提高类别区分度。
     </li>
    </ul>
    <p id="ud66e0f96">
     ✅
     <strong>
      Few-shot 场景下显著超越手工 Prompt
     </strong>
    </p>
    <ul>
     <li id="ubb0c334b">
      <strong>
       在 1-2 Shot 学习下，CoOp 已经优于人工设计的 Prompt
      </strong>
      。
     </li>
     <li id="ua603214e">
      <strong>
       在 16 Shot 任务下，平均提升 15%（最高可达 45%）
      </strong>
      ，远超 Zero-shot CLIP。
     </li>
    </ul>
    <p id="ub826dc04">
     ✅
     <strong>
      无需手动调整 Prompt，且具有良好的领域泛化能力
     </strong>
    </p>
    <ul>
     <li id="u7132da51">
      <strong>
       即使是在新领域的数据集上，CoOp 仍能比手工 Prompt 取得更好的泛化性能
      </strong>
      。
     </li>
     <li id="ua70a9b74">
      解决了
      <strong>
       CLIP 迁移到下游任务时对 Prompt 设计的依赖
      </strong>
      ，提高了
      <strong>
       Zero-shot 任务的稳健性
      </strong>
      。
     </li>
    </ul>
    <p id="u745ab1da">
    </p>
    <p class="img-center">
     <img alt="" height="506" id="u4d78c38d" src="https://i-blog.csdnimg.cn/img_convert/8597ce633f9a62035b92eecf5b19e7c5.png" width="1320"/>
    </p>
    <p id="uf8a88320">
    </p>
    <p class="img-center">
     <img alt="" height="573" id="u59989ede" src="https://i-blog.csdnimg.cn/img_convert/8f98ccd6dd73ea98a2dc15442a2ade0a.png" width="1320"/>
    </p>
    <h4 id="wYMol" name="wYMol" style="background-color:transparent">
     COCOOP: Conditional Prompt Learning for Vision-Language Models
    </h4>
    <p id="u9d246c16">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u12057d92">
     🔹
     <strong>
      CoOp 存在泛化性问题，容易过拟合训练类别
     </strong>
    </p>
    <ul>
     <li id="u60fd244c">
      CoOp 通过学习
      <strong>
       固定的 Soft Prompt
      </strong>
      适配 CLIP，但其
      <strong>
       学习到的 Prompt 对未见类别泛化能力差
      </strong>
      ，仅在训练类别上表现良好。
     </li>
     <li id="u428e83d2">
      在 Zero-shot 场景中，CoOp
      <strong>
       无法有效适应未见类别，导致性能下降
      </strong>
      。
     </li>
    </ul>
    <p id="u985f8ac3">
     🔹
     <strong>
      静态 Prompt 无法适应不同图像实例
     </strong>
    </p>
    <ul>
     <li id="u0c823307">
      CoOp 采用
      <strong>
       静态的 Soft Prompt
      </strong>
      ，所有类别共享相同的 Prompt，无法根据输入图像动态调整。
     </li>
     <li id="u96b83d9a">
      这种固定的 Prompt 使得模型在类别分布变化时难以适应，影响跨任务迁移能力。
     </li>
    </ul>
    <hr id="ucTDR"/>
    <p id="uc0a274d9">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="udbefb5ba">
     ✅
     <strong>
      提出 CoCoOp（Conditional Context Optimization），引入动态 Prompt 机制
     </strong>
    </p>
    <ul>
     <li id="u40f0c7bb">
      在 CoOp 基础上，使用
      <strong>
       轻量级神经网络
      </strong>
      生成
      <strong>
       基于输入图像的动态 Prompt
      </strong>
      ，使 Prompt
      <strong>
       可以根据不同的图像实例自适应调整
      </strong>
      。
     </li>
    </ul>
    <p id="u574ae14e">
     ✅
     <strong>
      提升 Zero-shot 泛化能力
     </strong>
    </p>
    <ul>
     <li id="ua137361d">
      由于 Prompt 不再是静态的，而是
      <strong>
       针对每张图像动态变化
      </strong>
      ，CoCoOp
      <strong>
       能更好地适应未见类别
      </strong>
      ，减少 CoOp 过拟合训练类别的问题。
     </li>
    </ul>
    <p id="uc6656964">
     ✅
     <strong>
      增强跨数据集的迁移能力，提高 Domain Generalization 表现
     </strong>
    </p>
    <ul>
     <li id="u244d6c11">
      通过动态 Prompt 生成，CoCoOp 在多个数据集上展示出
      <strong>
       更强的泛化能力
      </strong>
      ，不仅适用于 Zero-shot 学习，还能在跨领域任务上保持较好的适配性。
     </li>
    </ul>
    <p id="u9c95ca51">
    </p>
    <p class="img-center">
     <img alt="" height="421" id="uebfbdeeb" src="https://i-blog.csdnimg.cn/img_convert/6731450ae887487ff95865375803016c.png" width="1219"/>
    </p>
    <p id="u6d1fb7e5">
    </p>
    <p class="img-center">
     <img alt="" height="498" id="ueac22c7c" src="https://i-blog.csdnimg.cn/img_convert/f6544d6a165d4cebac03014dd56f6b80.png" width="1002"/>
    </p>
    <p id="ud493a269">
    </p>
    <h4 id="BjRKq" name="BjRKq">
     TextRefiner: Internal Visual Feature as Efficient Refiner for Vision-Language Models Prompt Tuning
    </h4>
    <p id="ue0295778">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u76de317d">
     🔹
     <strong>
      现有 Prompt Tuning 采用粗粒度方法，缺乏类别特异性
     </strong>
    </p>
    <ul>
     <li id="ufdcdbe61">
      现有方法（如 CoOp）学习到的 Prompt
      <strong>
       是全类别共享的
      </strong>
      ，无法针对
      <strong>
       类别特定的视觉概念进行区分
      </strong>
      ，导致
      <strong>
       对于共享相似视觉特征的类别（如动物品种）区分能力不足
      </strong>
      。
     </li>
    </ul>
    <p id="u33231267">
     🔹
     <strong>
      现有改进方法依赖外部知识（如 LLMs），计算成本高
     </strong>
    </p>
    <ul>
     <li id="u51ffced0">
      近期研究尝试通过
      <strong>
       大语言模型（LLMs）
      </strong>
      生成类别描述增强 Prompt，但这种方法在推理时需要额外的 LLM 调用，
      <strong>
       推理成本高、效率低
      </strong>
      。
     </li>
    </ul>
    <p id="ue6bdb31a">
    </p>
    <p class="img-center">
     <img alt="" height="473" id="u81f4797f" src="https://i-blog.csdnimg.cn/img_convert/1f545bbf28da58c5cffa9971e6b7f3f8.png" width="1075"/>
    </p>
    <hr id="iZW93"/>
    <p id="u68127496">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u1a4cb9f0">
     ✅
     <strong>
      提出 TextRefiner，结合 VLM 内部知识提升 Prompt 表达能力
     </strong>
    </p>
    <ul>
     <li id="u804b2bbf">
      通过
      <strong>
       视觉分支中的局部 Token 提取细粒度视觉概念
      </strong>
      ，存入一个
      <strong>
       本地缓存模块（Local Cache Module）
      </strong>
      ，然后与文本分支对齐，
      <strong>
       增强 Prompt 细粒度信息
      </strong>
      ，不依赖外部知识。
     </li>
    </ul>
    <p id="u90cd7203">
     ✅
     <strong>
      无外部依赖，高效优化现有 Prompt 学习方法
     </strong>
    </p>
    <ul>
     <li id="u0645be30">
      <strong>
       作为 Plug-and-Play 模块
      </strong>
      ，可直接应用于现有 Prompt 方法（如 CoOp、CoCoOp），无需额外训练外部模型，提高泛化能力。
     </li>
    </ul>
    <p id="u47bc9979">
     ✅
     <strong>
      提升 Prompt 质量，显著提高下游任务性能
     </strong>
    </p>
    <ul>
     <li id="u68fb8295">
      在
      <strong>
       11 个基准测试
      </strong>
      上，将 CoOp
      <strong>
       从 71.66% 提高到 76.94%
      </strong>
      ，超越 CoCoOp 的
      <strong>
       实例级 Prompt 生成方法
      </strong>
      。
     </li>
     <li id="uaed9a3c3">
      <strong>
       结合 PromptKD 可达到 SOTA 级别
      </strong>
      ，且推理效率更高。
     </li>
    </ul>
    <p id="u7c522c28">
    </p>
    <p class="img-center">
     <img alt="" height="525" id="u201e6275" src="https://i-blog.csdnimg.cn/img_convert/90ff0f6ac2486aad785c68c6dea122ed.png" width="1475"/>
    </p>
    <p id="u093f0f67">
    </p>
    <h4 id="A0ziR" name="A0ziR">
     KgCoOp : Visual-Language Prompt Tuning with Knowledge-guided Context Optimization
    </h4>
    <p id="ue74054e6">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u990d271c">
     🔹
     <strong>
      CoOp 生成的可训练 Prompt 泛化能力较差
     </strong>
    </p>
    <ul>
     <li id="u40c8f504">
      CoOp 结合可学习的文本 Token 和类别 Token，但其
      <strong>
       学习到的特定任务文本知识对未见类别的泛化能力较差
      </strong>
      ，导致 Zero-shot 任务表现不佳。
     </li>
     <li id="u8335d5e0">
      由于模型只关注可学习 Prompt，
      <strong>
       会遗忘预训练模型中原本具备的通用文本知识
      </strong>
      ，进一步降低泛化能力。
     </li>
    </ul>
    <p id="u8ec0f6ea">
     🔹
     <strong>
      现有 Prompt Tuning 方法未能充分保留通用语言知识
     </strong>
    </p>
    <ul>
     <li id="ubb632487">
      由于 Prompt 经过训练后偏向特定任务，
      <strong>
       会与 CLIP 预训练时的手工 Prompt 产生较大偏差
      </strong>
      ，影响在 Zero-shot 任务上的稳定性。
     </li>
    </ul>
    <hr id="TT3vI"/>
    <p id="u9c1943f8">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="ud87c115c">
     ✅
     <strong>
      提出 KgCoOp（Knowledge-guided Context Optimization），结合手工 Prompt 以增强泛化能力
     </strong>
    </p>
    <ul>
     <li id="u1d0741b1">
      通过
      <strong>
       减少可学习 Prompt 与手工 Prompt 之间的差异
      </strong>
      ，避免模型遗忘通用文本知识，提升 Zero-shot 任务表现。
     </li>
    </ul>
    <p id="ufea1be43">
     ✅
     <strong>
      引入知识约束，平衡任务特定性与通用性
     </strong>
    </p>
    <ul>
     <li id="uc23e5dff">
      在对比学习损失上加入
      <strong>
       Prompt 语义对齐约束
      </strong>
      ，让
      <strong>
       可训练 Prompt 既能学习任务特定信息，又保留 CLIP 预训练的文本泛化能力
      </strong>
      。
     </li>
    </ul>
    <p id="u6608b682">
     ✅
     <strong>
      提升泛化能力，减少训练成本
     </strong>
    </p>
    <ul>
     <li id="u145cc5f9">
      在多个基准测试上，KgCoOp
      <strong>
       在 seen/unseen 任务中均取得更好的性能
      </strong>
      ，且相比 CoOp
      <strong>
       训练时间更短
      </strong>
      ，提高了 Prompt Tuning 的效率。
     </li>
    </ul>
    <p id="ua867f848">
    </p>
    <p class="img-center">
     <img alt="" height="498" id="ue5f44017" src="https://i-blog.csdnimg.cn/img_convert/3e0d656c341942d2819dfafb86830c26.png" width="890"/>
    </p>
    <p id="ub6d60bef">
    </p>
    <h4 id="P9Ro8" name="P9Ro8">
     MaPLe: Multi-modal Prompt Learning
    </h4>
    <p id="u1b6d2afd">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="uf0e81078">
     🔹
     <strong>
      现有 CLIP 适配方法仅调整单模态（语言或视觉）分支，优化不充分
     </strong>
    </p>
    <ul>
     <li id="ucbeb88e9">
      现有 Prompt Tuning 方法（如 CoOp、CoCoOp）
      <strong>
       仅对 CLIP 的文本分支进行优化
      </strong>
      ，忽略了视觉分支的动态调整，导致
      <strong>
       跨模态表示不够协调
      </strong>
      。
     </li>
     <li id="ue0945bc6">
      这种单模态调整方式
      <strong>
       缺乏对视觉和语言信息的联合优化
      </strong>
      ，无法充分利用 VLM 的多模态对齐能力。
     </li>
    </ul>
    <p id="u47a08b37">
     🔹
     <strong>
      现有方法缺乏对不同层级特征的建模，影响泛化能力
     </strong>
    </p>
    <ul>
     <li id="u21a76be2">
      现有方法大多在固定层级应用 Prompt，
      <strong>
       未能逐步建模不同层级的特征交互
      </strong>
      ，限制了对复杂上下文信息的捕捉能力。
     </li>
    </ul>
    <p id="u450614cf">
    </p>
    <p class="img-center">
     <img alt="" height="564" id="uf769f835" src="https://i-blog.csdnimg.cn/img_convert/5a1c457a71986bd99ced47b8a9031ba4.png" width="616"/>
    </p>
    <hr id="TmrLy"/>
    <p id="u1c1730a3">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u80d8bd1e">
     ✅
     <strong>
      提出 Multi-modal Prompt Learning (MaPLe)，同时优化视觉和语言分支
     </strong>
    </p>
    <ul>
     <li id="udd52cee6">
      <strong>
       不同于 CoOp/CoCoOp 仅优化文本分支，MaPLe 同时学习视觉和文本 Prompt
      </strong>
      ，确保两者协同优化，从而提升跨模态对齐能力。
     </li>
     <li id="u11b9715a">
      通过双模态联合优化，增强 CLIP 在下游任务中的适配能力，提高泛化性。
     </li>
    </ul>
    <p id="ud633f0ce">
     ✅
     <strong>
      引入层级化 Prompt 机制，逐步建模不同层级的特征关系
     </strong>
    </p>
    <ul>
     <li id="ufba8065e">
      在 CLIP 的
      <strong>
       不同早期阶段分别学习独立的 Prompt
      </strong>
      ，逐步建模层级特征，使模型能够捕捉更丰富的上下文信息，提升分类性能。
     </li>
    </ul>
    <p id="ubeab9826">
     ✅
     <strong>
      显著提升 Zero-shot 泛化能力，在多个任务上优于 CoCoOp
     </strong>
    </p>
    <ul>
     <li id="u3cc0c7ab">
      在
      <strong>
       新类别泛化（novel classes）
      </strong>
      任务上比 CoCoOp 提高
      <strong>
       3.45%
      </strong>
      ，在
      <strong>
       整体泛化任务（harmonic mean）
      </strong>
      上提高
      <strong>
       2.72%
      </strong>
      ，
      <strong>
       在 11 个图像分类数据集上均表现更优
      </strong>
      。
     </li>
     <li id="u17f3a2cf">
      在
      <strong>
       新数据集泛化、新领域迁移
      </strong>
      任务上展现更好的适配能力。
     </li>
    </ul>
    <p id="uab78ba07">
    </p>
    <p class="img-center">
     <img alt="" height="619" id="u6d2e8efd" src="https://i-blog.csdnimg.cn/img_convert/571070a71f60e8f102857ad40d00763a.png" width="1359"/>
    </p>
    <p id="u79a301f4">
    </p>
    <h4 id="BMUwB" name="BMUwB">
     DePT: Decoupled Prompt Tuning
    </h4>
    <p id="u79c492ba">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u200337a5">
     🔹
     <strong>
      Prompt Tuning 存在 Base-New Tradeoff（BNT）问题，影响泛化能力
     </strong>
    </p>
    <ul>
     <li id="uc57207e4">
      现有 Prompt Tuning 方法在适配
      <strong>
       Base 任务（训练任务）
      </strong>
      时，会导致模型在
      <strong>
       New 任务（未见任务）
      </strong>
      上的泛化能力下降，即
      <strong>
       适配能力和泛化能力存在权衡
      </strong>
      。
     </li>
     <li id="u7a56a970">
      这种现象的根本原因在于
      <strong>
       模型在 Prompt Tuning 过程中会偏向 Base 任务，导致跨任务共享知识被削弱
      </strong>
      ，影响 Zero-shot 识别能力。
     </li>
    </ul>
    <p id="uc1d8c0be">
     🔹
     <strong>
      BNT 现象源于通道偏置（Channel Bias），影响任务共享特征的保持
     </strong>
    </p>
    <ul>
     <li id="uae19d43b">
      研究发现，在 Prompt Tuning 过程中，
      <strong>
       大部分特征通道会被 Base 任务的特定知识占据
      </strong>
      ，导致
      <strong>
       重要的任务共享特征被压缩甚至丢失
      </strong>
      ，进而降低模型在新任务上的泛化能力。
     </li>
    </ul>
    <hr id="lqUmc"/>
    <p id="u2937d50a">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="uf06b334c">
     ✅
     <strong>
      提出 Decoupled Prompt Tuning (DePT) 解决 Base-New Tradeoff（BNT）问题
     </strong>
    </p>
    <ul>
     <li id="u78532cc3">
      通过
      <strong>
       特征空间解耦
      </strong>
      ，在 Prompt Tuning 过程中
      <strong>
       将 Base 任务的特定知识从通道中隔离
      </strong>
      ，避免其过度占用模型的特征表示能力。
     </li>
     <li id="u5270ef37">
      使模型在适配 Base 任务的同时，
      <strong>
       最大程度保留跨任务共享知识
      </strong>
      ，提高 Zero-shot 任务的泛化能力。
     </li>
    </ul>
    <p id="uc6e9d0ad">
     ✅
     <strong>
      DePT 作为通用框架，可与现有 Prompt Tuning 方法结合
     </strong>
    </p>
    <ul>
     <li id="udc9de076">
      DePT
      <strong>
       与现有的 Prompt Tuning 方法（如 CoOp、CoCoOp、MaPLe）正交
      </strong>
      ，可以无缝集成到这些方法中，提升它们的泛化能力。
     </li>
    </ul>
    <p id="u627d1f83">
     ✅
     <strong>
      无需额外计算成本，即可显著提升 Zero-shot 任务表现
     </strong>
    </p>
    <ul>
     <li id="u789ca137">
      在多个数据集上验证，DePT
      <strong>
       在不增加显著计算开销的情况下，增强 Prompt Tuning 任务的灵活性和泛化性
      </strong>
      ，有效缓解 BNT 问题，提高模型在新任务上的适配能力。
     </li>
    </ul>
    <p id="u287cce18">
    </p>
    <p class="img-center">
     <img alt="" height="473" id="u029420bb" src="https://i-blog.csdnimg.cn/img_convert/5b11444521e41aa4b0ce974887b37ce7.png" width="1314"/>
    </p>
    <p id="u88c59cfb">
    </p>
    <h4 id="yLpDT" name="yLpDT">
     QNet: PROMPT LEARNING WITH QUATERNION NETWORKS
    </h4>
    <p id="u4e0e2522">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="ubf8481d8">
     🔹
     <strong>
      现有多模态融合策略结构单一，难以捕捉多样化特征模式
     </strong>
    </p>
    <ul>
     <li id="uf6af44d5">
      目前的多模态预训练模型在融合不同模态（如文本和视觉）时，主要依赖
      <strong>
       显式交互结构
      </strong>
      ，但这些方法在处理
      <strong>
       细粒度分类和抽象语义对齐
      </strong>
      时表现不佳。
     </li>
     <li id="u21d38192">
      由于缺乏对多模态间复杂特征关系的建模能力，现有方法在
      <strong>
       Zero-shot 场景下的泛化能力有限
      </strong>
      。
     </li>
    </ul>
    <p id="udfff8599">
     🔹
     <strong>
      多模态特征融合效率低，参数量大，影响模型适配性
     </strong>
    </p>
    <ul>
     <li id="ubc5f210a">
      现有方法在跨模态学习时通常涉及大量参数，导致计算效率低，尤其在跨数据集迁移和域泛化任务中难以保持稳定性能。
     </li>
    </ul>
    <hr id="cfJK6"/>
    <p id="u42b4f57f">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u5f23c41a">
     ✅
     <strong>
      提出 QNet（Prompt Learning with Quaternion Networks），利用四元数网络增强多模态语义对齐
     </strong>
    </p>
    <ul>
     <li id="u854716d1">
      通过
      <strong>
       四元数隐藏空间（Quaternion Hidden Space）
      </strong>
      ，利用
      <strong>
       互相正交的虚部轴（Imaginary Axes）
      </strong>
      进行多视角特征建模，捕捉多模态间丰富的语义空间关系。
     </li>
    </ul>
    <p id="u9147c780">
     ✅
     <strong>
      跨层级特征建模，增强不同模态间的语义依赖
     </strong>
    </p>
    <ul>
     <li id="u004f4960">
      采用
      <strong>
       多层级特征编码机制
      </strong>
      ，整合跨层级特征，增强不同模态之间的深度交互能力，提高 Zero-shot 任务中的细粒度分类能力。
     </li>
    </ul>
    <p id="u3ba03f19">
     ✅
     <strong>
      更少参数、更强泛化能力，在多个任务上超越 SOTA
     </strong>
    </p>
    <ul>
     <li id="ue94a4116">
      <strong>
       在 11 个数据集上测试，QNet 在 Base-to-Novel 泛化、跨数据集迁移、领域泛化任务上均优于现有 Prompt Learning 方法
      </strong>
      ，同时
      <strong>
       减少了可训练参数量，提高计算效率
      </strong>
      。
     </li>
    </ul>
    <p id="ua041c62d">
    </p>
    <p class="img-center">
     <img alt="" height="707" id="u0c82b7bb" src="https://i-blog.csdnimg.cn/img_convert/82c6b32783effc1a58042fcb43f09a21.png" width="1209"/>
    </p>
    <p id="u2c332b5f">
    </p>
    <h4 id="enQzO" name="enQzO">
     TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model
    </h4>
    <p id="ue6b4a783">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u3aa98da5">
     🔹
     <strong>
      现有 CoOp 及其改进方法在未见领域（Unseen Domains）上的泛化能力有限
     </strong>
    </p>
    <ul>
     <li id="ua2e13953">
      现有基于 CoOp 的 Prompt Tuning 方法
      <strong>
       依赖可学习的文本 Token
      </strong>
      ，但这些 Token 在
      <strong>
       新类别分布上无法动态调整
      </strong>
      ，导致在 Unseen Domains 任务上表现不佳。
     </li>
     <li id="u39c5a363">
      由于 Prompt 设计未考虑
      <strong>
       类别级别的可区分性（Class Discriminability）
      </strong>
      ，使得不同类别之间的区分度降低，影响模型泛化能力。
     </li>
    </ul>
    <p id="ub882727e">
     🔹
     <strong>
      缺乏类别感知的文本表示，影响 Zero-shot 任务中的判别能力
     </strong>
    </p>
    <ul>
     <li id="u08c430ec">
      现有方法主要学习
      <strong>
       共享或图像条件的 Prompt
      </strong>
      ，未能显式利用类别级别的先验知识，导致对于
      <strong>
       相似类别（如不同品种的动物）
      </strong>
      ，Prompt 难以准确区分。
     </li>
    </ul>
    <p id="u41eeacba">
    </p>
    <p class="img-center">
     <img alt="" height="1014" id="AXzR3" src="https://i-blog.csdnimg.cn/img_convert/fbf73516c745ab4f2f88da3364b8fc7b.png" width="1459"/>
    </p>
    <hr id="An1CA"/>
    <p id="u8c41d4ae">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="ue7f3a099">
     ✅
     <strong>
      提出 TCP（Textual-based Class-aware Prompt Tuning），引入类别感知的 Prompt 机制
     </strong>
    </p>
    <ul>
     <li id="ue72c4c85">
      通过
      <strong>
       Textual Knowledge Embedding (TKE)
      </strong>
      ，映射类别级别的文本知识，使 Prompt 具备类别特异性，提高不同类别的区分能力。
     </li>
    </ul>
    <p id="u0ca6f681">
     ✅
     <strong>
      利用 TKE 生成动态类别感知 Prompt，提高 Unseen Domain 任务泛化性
     </strong>
    </p>
    <ul>
     <li id="ue5b42c71">
      在推理阶段，TKE 可
      <strong>
       动态生成针对未见类别的 Class-aware Prompt
      </strong>
      ，增强 Prompt 适应不同类别分布的能力，
      <strong>
       提升 Zero-shot 任务表现
      </strong>
      。
     </li>
    </ul>
    <p id="u17a57820">
     ✅
     <strong>
      TCP 作为 Plug-and-Play 模块，可无缝集成至现有 Prompt Tuning 方法
     </strong>
    </p>
    <ul>
     <li id="u29930f07">
      可轻松结合现有的 Prompt Tuning 方法（如 CoOp、CoCoOp），且
      <strong>
       减少训练时间的同时提升性能
      </strong>
      ，在多个数据集上实现
      <strong>
       更优的泛化能力
      </strong>
      。
     </li>
    </ul>
    <p id="u40ca009d">
    </p>
    <p id="u84f76f12">
    </p>
    <p class="img-center">
     <img alt="" height="526" id="u921c109e" src="https://i-blog.csdnimg.cn/img_convert/0ba49f0b738f6b0e010203adc1ce2d25.png" width="800"/>
    </p>
    <p id="u3766bdd2">
    </p>
    <h4 id="fBYAh" name="fBYAh">
     MMA: Multi-Modal Adapter for Vision-Language Models
    </h4>
    <p id="u1d6db725">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="ueb9e3de0">
     🔹
     <strong>
      VLM（视觉-语言模型）在 Few-shot 任务中面临判别性与泛化性的权衡（Discrimination-Generalization Dilemma）
     </strong>
    </p>
    <ul>
     <li id="ua72e87b4">
      在适配下游任务时，
      <strong>
       需要同时保持模型的通用知识（General Knowledge）和任务特定知识（Task-Specific Knowledge）
      </strong>
      ，但目前缺乏有效的方法来区分和优化这两种表征。
     </li>
     <li id="u7afdc0aa">
      过度优化任务特定知识会导致泛化能力下降，而保留过多通用知识又可能影响分类精度。
     </li>
    </ul>
    <p id="u033518de">
     🔹
     <strong>
      视觉与语言分支之间的特征对齐存在语义鸿沟
     </strong>
    </p>
    <ul>
     <li id="u60aab72d">
      研究发现，
      <strong>
       语言特征比视觉特征更具判别性
      </strong>
      ，但两者在 Transformer 低层的特征差异较大，导致跨模态对齐困难，影响 Few-shot 任务性能。
     </li>
    </ul>
    <hr id="T5EWn"/>
    <p id="u163f6b92">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u958cadb2">
     ✅
     <strong>
      提出 Multi-Modal Adapter (MMA)，优化视觉与语言特征对齐
     </strong>
    </p>
    <ul>
     <li id="u62358020">
      MMA
      <strong>
       聚合视觉和语言分支的特征
      </strong>
      ，将它们映射到一个共享特征空间，使梯度能够在不同模态间有效传播，增强跨模态交互能力。
     </li>
    </ul>
    <p id="ub4ad939a">
     ✅
     <strong>
      基于层级分析，选择性优化高层 Transformer 层，提高判别性与泛化性平衡
     </strong>
    </p>
    <ul>
     <li id="u8347a3e1">
      通过分析不同层的信息分布，发现
      <strong>
       高层特征偏向任务特定知识，低层特征更具通用性
      </strong>
      ，因此 MMA 仅在高层 Transformer 中加入适配模块，避免影响模型的通用能力。
     </li>
    </ul>
    <p id="ud76c5e6d">
     ✅
     <strong>
      在多个泛化任务上超越 SOTA，提升 Zero-shot 适应能力
     </strong>
    </p>
    <ul>
     <li id="u5a7ec908">
      在
      <strong>
       新类别泛化（Novel Classes）、新目标数据集（New Target Datasets）和领域泛化（Domain Generalization）
      </strong>
      任务中，MMA
      <strong>
       在多个数据集上均优于现有 Prompt Tuning 方法
      </strong>
      ，实现更优的判别性与泛化能力平衡。
     </li>
    </ul>
    <p id="u35c42002">
    </p>
    <p class="img-center">
     <img alt="" height="396" id="uabe64219" src="https://i-blog.csdnimg.cn/img_convert/8b8431c827f81a90aaad5a94b28d6b94.png" width="1311"/>
    </p>
    <p id="u384858b4">
    </p>
    <p class="img-center">
     <img alt="" height="260" id="ud03ab438" src="https://i-blog.csdnimg.cn/img_convert/eb2163fe8a4b2c5123daadd6812afe1b.png" width="555"/>
    </p>
    <p id="uf7abd122">
    </p>
    <h4 id="RlHJU" name="RlHJU">
     CoPrompt: Consistency-guided Prompt Learning for Vision-Language Models
    </h4>
    <p id="ub5a38fec">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u5f3d31b0">
     🔹
     <strong>
      VLM 在 Few-shot Fine-tuning 过程中容易过拟合，导致泛化能力下降
     </strong>
    </p>
    <ul>
     <li id="ua4be5669">
      在小样本（Few-shot）任务中，对 VLM（视觉-语言模型）进行 Fine-tuning 容易导致
      <strong>
       对训练集的过拟合
      </strong>
      ，从而影响
      <strong>
       Zero-shot 任务和跨域泛化能力
      </strong>
      。
     </li>
     <li id="u2723b966">
      现有 Prompt Tuning 方法
      <strong>
       缺乏有效的正则化策略
      </strong>
      来防止 Fine-tuning 过程中模型对下游任务数据的过拟合。
     </li>
    </ul>
    <p id="u87c02f05">
     🔹
     <strong>
      现有方法无法同时兼顾 Prompt Tuning 和 Adapter Tuning 的优势
     </strong>
    </p>
    <ul>
     <li id="ubd5a9fbf">
      Prompt Tuning 在输入空间进行调整，而 Adapter Tuning
      <strong>
       通过在模型内部插入小型可训练模块
      </strong>
      进行调整，但目前的研究很少考虑如何
      <strong>
       结合两者的优势
      </strong>
      ，提升泛化能力和适配性。
     </li>
    </ul>
    <p id="uff9dd9d3">
    </p>
    <p class="img-center">
     <img alt="" height="584" id="ytIo1" src="https://i-blog.csdnimg.cn/img_convert/8a8eafbc145503404f873b0d399223ed.png" width="697"/>
    </p>
    <hr id="UnMX5"/>
    <p id="ud53efa1e">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u62168bf1">
     ✅
     <strong>
      提出 CoPrompt（Consistency-guided Prompt Learning），引入一致性约束以防止过拟合
     </strong>
    </p>
    <ul>
     <li id="u06e6fb56">
      在 Fine-tuning 过程中
      <strong>
       引入一致性约束（Consistency Constraint）
      </strong>
      ，确保
      <strong>
       可训练 Prompt 的预测结果与预训练模型的预测保持一致
      </strong>
      ，防止模型过度拟合 Few-shot 任务数据。
     </li>
    </ul>
    <p id="u4b56ce43">
     ✅
     <strong>
      结合输入扰动一致性（Perturbation Consistency），提升泛化能力
     </strong>
    </p>
    <ul>
     <li id="u3677392e">
      通过对输入数据添加不同扰动（Perturbation），
      <strong>
       让 Fine-tuned 模型在不同的输入条件下保持预测一致
      </strong>
      ，进一步提升模型的鲁棒性和 Zero-shot 泛化能力。
     </li>
    </ul>
    <p id="u77479a9e">
     ✅
     <strong>
      融合 Prompt Tuning 和 Adapter Tuning，提高 Few-shot 任务适配能力
     </strong>
    </p>
    <ul>
     <li id="u881044fb">
      <strong>
       在输入端使用 Prompt 调整输入表征，在模型内部使用 Adapter 进行特征增强
      </strong>
      ，同时优化输入和输出空间，使 VLM 适应不同的下游任务。
     </li>
     <li id="ub9dda531">
      这种结合方式
      <strong>
       提升了 Zero-shot 任务表现，同时增强 Base-to-New 泛化能力
      </strong>
      。
     </li>
    </ul>
    <p id="u3dc6073d">
     ✅
     <strong>
      在多个任务上超越 SOTA，显著提升泛化能力
     </strong>
    </p>
    <ul>
     <li id="u8ca4f9fa">
      <strong>
       在 Zero-shot 任务和 11 个数据集的 Harmonic Mean 指标上均取得 SOTA 结果
      </strong>
      ，并在
      <strong>
       Base-to-Novel 泛化、领域泛化（Domain Generalization）、跨数据集迁移
      </strong>
      等任务中超越现有方法。
     </li>
    </ul>
    <p id="u102cb242">
    </p>
    <p class="img-center">
     <img alt="" height="655" id="uc53431ad" src="https://i-blog.csdnimg.cn/img_convert/9649f29b2f6a9e07c05b66a720f489c8.png" width="795"/>
    </p>
    <p id="u56825f95">
    </p>
    <h4 id="FLR1B" name="FLR1B">
     CasPL: Cascade Prompt Learning for Vision-Language Model Adaptation
    </h4>
    <p id="u3e788d19">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u276e231f">
     🔹
     <strong>
      现有 Prompt Learning 方法仅适用于单阶段适配，容易导致过拟合
     </strong>
    </p>
    <ul>
     <li id="udcc368ce">
      目前的可训练 Prompt 主要用于
      <strong>
       任务适配（Adapting Prompt）
      </strong>
      ，
      <strong>
       缺乏对通用知识的学习机制
      </strong>
      ，导致模型在下游任务中容易过拟合目标数据分布，影响泛化能力。
     </li>
     <li id="u817abd6e">
      由于 Prompt 直接在目标任务上训练，
      <strong>
       无法充分利用无标签数据进行知识提取
      </strong>
      ，导致任务特定信息的学习受限。
     </li>
    </ul>
    <p id="u4d7f4af7">
     🔹
     <strong>
      Prompt Tuning 缺乏层次化知识提取，难以同时兼顾通用性和任务特异性
     </strong>
    </p>
    <ul>
     <li id="u3d6ae7dc">
      现有方法大多采用
      <strong>
       单一 Prompt 训练方式
      </strong>
      ，难以同时优化通用特征（Domain-General Knowledge）和任务特定特征（Task-Specific Knowledge），影响 Zero-shot 任务的稳定性。
     </li>
    </ul>
    <p id="u624829ac">
    </p>
    <p class="img-center">
     <img alt="" height="353" id="u1a3be50d" src="https://i-blog.csdnimg.cn/img_convert/7135d2f2323eec438344d267f9b0193a.png" width="1164"/>
    </p>
    <hr id="FmoX3"/>
    <p id="u274f782a">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u08135a63">
     ✅
     <strong>
      提出 Cascade Prompt Learning (CasPL)，采用双阶段 Prompt 结构
     </strong>
    </p>
    <ul>
     <li id="uecc9a063">
      <strong>
       第一阶段：Boosting Prompt
      </strong>
      —— 通过一个
      <strong>
       更强大的 CLIP 教师模型
      </strong>
      提取
      <strong>
       无标签数据中的通用知识
      </strong>
      ，并对其预测进行对齐，确保 Prompt 具备广泛的领域泛化能力。
     </li>
     <li id="ue1e6f51b">
      <strong>
       第二阶段：Adapting Prompt
      </strong>
      —— 在
      <strong>
       冻结第一阶段 Prompt
      </strong>
      的基础上，对目标任务进行 Fine-tuning，以学习任务特定特征，从而降低过拟合风险。
     </li>
    </ul>
    <p id="ua7dd4eb2">
     ✅
     <strong>
      实现通用特征与任务特定特征的解耦，提高泛化能力
     </strong>
    </p>
    <ul>
     <li id="uea814c6e">
      采用
      <strong>
       逐步学习的 Prompt 结构
      </strong>
      ，确保模型在任务适配的同时，仍然保留通用知识，提高 Zero-shot 任务的稳健性。
     </li>
     <li id="u17fdf866">
      这种两阶段 Prompt 设计
      <strong>
       有效缓解了传统单阶段 Prompt Learning 方法的过拟合问题
      </strong>
      。
     </li>
    </ul>
    <p id="u3144c323">
     ✅
     <strong>
      Plug-and-Play 设计，可集成到现有 Prompt Learning 方法中，提升小型 VLM 适配能力
     </strong>
    </p>
    <ul>
     <li id="u0c8897b8">
      CasPL 可无缝集成至现有的 Prompt Learning 方法，如 CoOp 和 CoCoOp，
      <strong>
       在不增加显著计算开销的情况下提升适配能力
      </strong>
      ，适用于
      <strong>
       资源受限环境的小型 VLM 部署
      </strong>
      。
     </li>
     <li id="u7a13fe86">
      在
      <strong>
       11 个图像分类数据集
      </strong>
      上，CasPL
      <strong>
       比 PromptSRC 提高 1.85%（Base Classes）、3.44%（Novel Classes）、2.72%（Harmonic Mean）
      </strong>
      ，在泛化能力与推理效率之间取得更优平衡。
     </li>
    </ul>
    <p id="u1556a646">
    </p>
    <p class="img-center">
     <img alt="" height="589" id="uafb3f626" src="https://i-blog.csdnimg.cn/img_convert/20c47c1288ca0dc884855779fa26c141.png" width="1296"/>
    </p>
    <p id="uf5874aa9">
    </p>
    <h4 id="X7Yuh" name="X7Yuh">
     PromptKD: Unsupervised Prompt Distillation for Vision-Language Models
    </h4>
    <p id="ua5ac5535">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u2d9b05da">
     🔹
     <strong>
      现有 Prompt Learning 主要关注 Prompt 设计，忽略了 Prompt 作为蒸馏器的潜力
     </strong>
    </p>
    <ul>
     <li id="u416533f0">
      目前的研究大多集中在
      <strong>
       如何优化 Prompt 本身
      </strong>
      ，而
      <strong>
       缺乏利用 Prompt 进行知识蒸馏（Knowledge Distillation, KD）
      </strong>
      的方法，使得小模型难以高效继承大模型的知识。
     </li>
     <li id="ucd77e8b8">
      在资源受限的环境下，
      <strong>
       小型 VLM 需要高效从大型模型中学习，以提升下游任务适应能力
      </strong>
      ，但现有方法很少关注如何在 Prompt 机制下进行模型蒸馏。
     </li>
    </ul>
    <p id="uc1a65d48">
     🔹
     <strong>
      知识蒸馏通常依赖有标签数据，限制了无监督领域适配能力
     </strong>
    </p>
    <ul>
     <li id="u3e1d5c77">
      传统蒸馏方法通常需要
      <strong>
       有标签数据
      </strong>
      来指导学生模型的学习，而在许多领域（如医学、遥感）获取高质量标注数据的成本极高。
     </li>
     <li id="u9660c0e0">
      现有 VLM 适配方法未充分利用
      <strong>
       大量无标签数据
      </strong>
      ，导致在特定领域任务上的泛化能力受限。
     </li>
    </ul>
    <p id="ua69499b2">
    </p>
    <p class="img-center">
     <img alt="" height="438" id="u6859b487" src="https://i-blog.csdnimg.cn/img_convert/68470a5d3996e42870323b00badb9fd3.png" width="1198"/>
    </p>
    <hr id="QZUrw"/>
    <p id="u1c6cb7b1">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="uf9ea1778">
     ✅
     <strong>
      提出无监督领域 Prompt 蒸馏（Unsupervised Domain Prompt Distillation），利用 Prompt 进行知识迁移
     </strong>
    </p>
    <ul>
     <li id="uefcf821b">
      采用
      <strong>
       Prompt 作为蒸馏桥梁
      </strong>
      ，通过 Prompt Learning 让
      <strong>
       小型 VLM 直接模仿大型 CLIP 教师模型的预测分布
      </strong>
      ，提高学生模型的适应能力。
     </li>
     <li id="uf9ef0e06">
      <strong>
       消除对标注数据的依赖
      </strong>
      ，仅使用
      <strong>
       无标签领域数据
      </strong>
      进行 Prompt 蒸馏，极大提升领域适配性。
     </li>
    </ul>
    <p id="ude9dc9f2">
     ✅
     <strong>
      采用两阶段蒸馏框架，高效传递教师模型的知识
     </strong>
    </p>
    <ul>
     <li id="u1dd71dfc">
      <strong>
       第一阶段
      </strong>
      ：对
      <strong>
       大型 CLIP 教师模型
      </strong>
      进行少量标注数据预训练，并
      <strong>
       预存教师模型文本编码器生成的类别向量
      </strong>
      ，减少推理阶段的计算开销。
     </li>
     <li id="uac3748b6">
      <strong>
       第二阶段
      </strong>
      ：在无监督环境下，使用
      <strong>
       KL 散度（KL Divergence）
      </strong>
      使
      <strong>
       学生模型的预测概率分布尽可能匹配教师模型
      </strong>
      ，从而提升学生模型的泛化能力。
     </li>
    </ul>
    <p id="u81c11ed3">
     ✅
     <strong>
      首创预存文本特征机制，优化 Prompt 蒸馏的计算效率
     </strong>
    </p>
    <ul>
     <li id="ue7728146">
      <strong>
       仅需计算一次文本特征（Class Vectors）并存储
      </strong>
      ，在后续蒸馏过程中，
      <strong>
       学生模型可直接使用已存储的类别向量进行学习
      </strong>
      ，大幅减少计算成本。
     </li>
     <li id="u9242949b">
      这种方法有效
      <strong>
       提升小型 VLM 的适配性，同时降低推理时的资源消耗
      </strong>
      ，适用于
      <strong>
       低算力场景
      </strong>
      。
     </li>
    </ul>
    <p id="u0ce22d47">
    </p>
    <p class="img-center">
     <img alt="" height="726" id="u63ef1d7c" src="https://i-blog.csdnimg.cn/img_convert/96e3c68da7e008c01e468c59adbb7e40.png" width="1361"/>
    </p>
    <p id="uf184e93c">
    </p>
    <h4 id="CHcPs" name="CHcPs">
     <strong>
      KAPT:
     </strong>
     Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models
    </h4>
    <p id="uccdcb966">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u5f0574da">
     🔹
     <strong>
      现有可训练 Prompt 方法容易过拟合已见类别，泛化能力不足
     </strong>
    </p>
    <ul>
     <li id="u2d8c96ff">
      现有 Prompt Tuning 方法（如 CoOp、CoCoOp）尽管在已见类别上表现良好，但
      <strong>
       在未见类别（Unseen Classes）上的泛化能力较差
      </strong>
      ，限制了 Zero-shot 任务的适用性。
     </li>
     <li id="u08bcd36d">
      这种过拟合现象源于 Prompt 学习时
      <strong>
       缺乏外部知识的指导，仅依赖任务内数据
      </strong>
      ，导致对新类别的理解能力受限。
     </li>
    </ul>
    <p id="u7fe7ab68">
     🔹
     <strong>
      现有方法缺乏知识引导，未充分利用类别相关信息
     </strong>
    </p>
    <ul>
     <li id="u7c129e9b">
      人类在识别新类别时通常会结合
      <strong>
       外部知识（如文本描述、先验知识）
      </strong>
      ，但当前 Prompt 学习仅利用训练数据，
      <strong>
       忽略了类别外部信息的潜在作用
      </strong>
      。
     </li>
     <li id="ua29f37f6">
      视觉特征与文本特征的匹配未充分利用类别的区分性特征，导致模型在细粒度分类任务上的表现有限。
     </li>
    </ul>
    <hr id="gHkZ5"/>
    <p id="u728f6c6c">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u7d30b5e4">
     ✅
     <strong>
      提出 KAPT（Knowledge-Aware Prompt Tuning），结合外部知识增强 Prompt 泛化能力
     </strong>
    </p>
    <ul>
     <li id="u8dae8227">
      通过
      <strong>
       引入类别相关的外部知识
      </strong>
      ，构建更丰富的文本表征，增强 Prompt 对新类别的适应能力，
      <strong>
       减少对已见类别的过拟合
      </strong>
      。
     </li>
    </ul>
    <p id="uc530db18">
     ✅
     <strong>
      设计两种互补的知识感知 Prompt，提升文本编码器的知识表达能力
     </strong>
    </p>
    <ul>
     <li id="u24c00783">
      <strong>
       离散 Prompt（Discrete Prompt）
      </strong>
      ：从类别描述文本中提取关键信息，增强模型的类别感知能力。
     </li>
     <li id="u2ae29013">
      <strong>
       连续 Prompt（Continuous Prompt）
      </strong>
      ：可训练的 Soft Prompt，捕捉全局上下文信息，使 Prompt 具备更好的适配性。
     </li>
    </ul>
    <p id="uc815c85b">
     ✅
     <strong>
      引入自适应视觉注意力模块，增强视觉特征的判别能力
     </strong>
    </p>
    <ul>
     <li id="u19e0edc0">
      在视觉编码器中加入
      <strong>
       自适应特征聚合模块（Adaptation Head）
      </strong>
      ，提取关键视觉特征，使视觉表征更加
      <strong>
       区分性强且任务相关
      </strong>
      ，提高跨类别泛化能力。
     </li>
    </ul>
    <p id="u94f52038">
     ✅
     <strong>
      在 Few-shot 任务和未见类别泛化任务上超越 SOTA
     </strong>
    </p>
    <ul>
     <li id="ud6473815">
      在
      <strong>
       11 个基准数据集
      </strong>
      上进行实验，在
      <strong>
       Few-shot 任务和 Zero-shot 泛化
      </strong>
      任务中优于 CoCoOp，
      <strong>
       在新类别任务上提升 3.22%
      </strong>
      ，展现更优的跨任务适应能力。
     </li>
    </ul>
    <p id="u0375e21d">
    </p>
    <p class="img-center">
     <img alt="" height="626" id="uba029a09" src="https://i-blog.csdnimg.cn/img_convert/e399892defbd9acc99f44cc78ebebeca.png" width="1691"/>
    </p>
    <p id="u548c8db0">
    </p>
    <h4 id="BIZHi" name="BIZHi">
     Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition
    </h4>
    <p id="u1e05421b">
     prompt learning结合clip预训练
    </p>
    <p id="u7701faec">
    </p>
    <p class="img-center">
     <img alt="" height="671" id="u4138685f" src="https://i-blog.csdnimg.cn/img_convert/80555c6b1532d05162087ec00f2c2c10.png" width="1375"/>
    </p>
    <p id="u9132ebd6">
    </p>
    <h4 id="AGTwF" name="AGTwF">
     AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation
    </h4>
    <p id="u70920c49">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u1bd34583">
     🔹
     <strong>
      VLM 在适应新概念（New Concept Understanding）时受限于有限类别信息
     </strong>
    </p>
    <ul>
     <li id="u2bc8c051">
      由于
      <strong>
       新类别缺乏足够的视觉和文本描述
      </strong>
      ，现有 VLM（如 CLIP）在 Zero-shot 任务中的泛化能力受限，难以充分理解和区分新概念。
     </li>
     <li id="u0f7fab5a">
      传统 Prompt Tuning 方法
      <strong>
       未能充分利用外部信息来增强类别表征
      </strong>
      ，导致新类别适应性较差。
     </li>
    </ul>
    <p id="uf73078cf">
     🔹
     <strong>
      缺乏有效的特征对齐机制，影响视觉-语言模型的跨模态泛化能力
     </strong>
    </p>
    <ul>
     <li id="ubc454b36">
      视觉和语言模态的特征分布可能存在差异，导致
      <strong>
       模型难以在多模态空间中发现语义相关性
      </strong>
      ，影响 Zero-shot 和 Few-shot 任务的准确性。
     </li>
    </ul>
    <p id="ud08c6391">
    </p>
    <p class="img-center">
     <img alt="" height="489" id="u9558412e" src="https://i-blog.csdnimg.cn/img_convert/9437531a1518b132b342fa8e9fb5f4e6.png" width="914"/>
    </p>
    <hr id="RRKFT"/>
    <p id="u92feeaf7">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u08299cb7">
     ✅
     <strong>
      提出 AWT（Augment, Weight, then Transport），增强 VLM 在新概念上的适配能力
     </strong>
    </p>
    <ul>
     <li id="u329e7eea">
      <strong>
       Augment（增强输入）：
      </strong>
      通过
      <strong>
       图像变换（Image Transformations）
      </strong>
      和
      <strong>
       语言模型生成类别描述（LLM-based Class Descriptions）
      </strong>
      ，扩充视觉和文本信息，提高新类别的可识别性。
     </li>
    </ul>
    <p id="u49d72717">
     ✅
     <strong>
      引入预测熵加权机制（Weighting），动态优化输入贡献
     </strong>
    </p>
    <ul>
     <li id="ue96c4201">
      <strong>
       基于预测熵（Prediction Entropy）对输入进行动态加权
      </strong>
      ，赋予更可靠的输入更高权重，从而减少低置信度输入的干扰，提高模型的稳定性。
     </li>
    </ul>
    <p id="u68d18153">
     ✅
     <strong>
      采用最优传输（Optimal Transport）方法，优化视觉-语言对齐
     </strong>
    </p>
    <ul>
     <li id="ud731c7a8">
      通过
      <strong>
       最优传输（Optimal Transport, OT）
      </strong>
      挖掘
      <strong>
       视觉和文本模态之间的语义关联
      </strong>
      ，确保 VLM 在不同模态之间建立更紧密的联系，提高跨模态泛化能力。
     </li>
    </ul>
    <p id="uaffcedf0">
     ✅
     <strong>
      无需额外训练，可无缝集成到不同 VLM 中，提升 Zero-shot 能力
     </strong>
    </p>
    <ul>
     <li id="u8ff3bab8">
      AWT
      <strong>
       作为通用框架
      </strong>
      ，可直接集成到不同规模和架构的 VLM（如 CLIP），增强其
      <strong>
       Zero-shot 图像分类、Few-shot 识别、视频动作识别和 OOD 泛化能力
      </strong>
      ，在多个任务上超越 SOTA 方法。
     </li>
    </ul>
    <p id="uac2ed402">
    </p>
    <p class="img-center">
     <img alt="" height="575" id="u458a6694" src="https://i-blog.csdnimg.cn/img_convert/77393cf910972ba67a57049e8d0672a8.png" width="1496"/>
    </p>
    <p id="ucf8bbd1f">
    </p>
    <h4 id="a14AP" name="a14AP">
     ProText: Learning to Prompt with Text Only Supervision for Vision-Language Models
    </h4>
    <p id="u18c78e76">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u365b0dd3">
     🔹
     <strong>
      现有基于视觉信息的 Prompt Tuning 依赖标注数据，泛化能力受限
     </strong>
    </p>
    <ul>
     <li id="u1fcd254d">
      许多现有方法通过
      <strong>
       视觉信息优化 Prompt
      </strong>
      ，但这需要
      <strong>
       大量标注数据
      </strong>
      ，在实际应用中不够高效。
     </li>
     <li id="uf5bb7f35">
      由于这些方法主要在
      <strong>
       训练数据集上进行优化
      </strong>
      ，容易
      <strong>
       过拟合源数据
      </strong>
      ，导致在新数据集上的泛化能力下降。
     </li>
    </ul>
    <p id="ufd29744b">
     🔹
     <strong>
      基于 LLM 生成的 Prompt 方式存在高成本问题
     </strong>
    </p>
    <ul>
     <li id="u560840ae">
      另一类方法使用
      <strong>
       大语言模型（LLMs）
      </strong>
      生成类别描述，并通过
      <strong>
       Prompt Ensembling
      </strong>
      提高泛化能力，但这些
      <strong>
       类别特定（Class-specific）的 Prompt 无法直接迁移到其他类别
      </strong>
      ，导致每个新类别都需要单独生成描述，
      <strong>
       计算成本高
      </strong>
      。
     </li>
    </ul>
    <hr id="tWK9V"/>
    <p id="u7fd63c08">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="ue5bc0812">
     ✅
     <strong>
      提出基于文本学习的 Prompt Tuning 方法，无需依赖标注数据
     </strong>
    </p>
    <ul>
     <li id="u5d2091be">
      通过
      <strong>
       仅使用 LLM 生成的文本数据
      </strong>
      进行 Prompt 训练，
      <strong>
       避免对图像数据的依赖
      </strong>
      ，从而
      <strong>
       减少人工标注成本
      </strong>
      ，提高泛化能力。
     </li>
     <li id="u38bfb81c">
      通过新颖的训练方法，使 Prompt 能够
      <strong>
       从 LLM 生成的数据中提取丰富的上下文知识
      </strong>
      ，提高 Zero-shot 适配能力。
     </li>
    </ul>
    <p id="uff322d8d">
     ✅
     <strong>
      实现 Prompt 的 Zero-shot 迁移能力，减少 LLM Prompt 生成成本
     </strong>
    </p>
    <ul>
     <li id="u6b703039">
      由于 Prompt 通过文本数据学习，不依赖特定类别的视觉信息，因此可以
      <strong>
       直接迁移到新类别和新数据集
      </strong>
      ，相比基于 LLM 逐类别生成 Prompt 的方法
      <strong>
       减少计算开销
      </strong>
      。
     </li>
    </ul>
    <p id="u58398643">
     ✅
     <strong>
      在多个基准测试中超越现有 Prompt Ensembling 方法，并与监督学习方法竞争
     </strong>
    </p>
    <ul>
     <li id="u10b3dfd7">
      在
      <strong>
       4 个基准数据集
      </strong>
      上进行测试，
      <strong>
       比现有的 LLM-based Prompt Ensembling 方法表现更优
      </strong>
      ，且在
      <strong>
       无标注图像的情况下能接近有监督图像 Prompt Tuning 方法的性能
      </strong>
      ，展现强大的 Zero-shot 泛化能力。
     </li>
    </ul>
    <p id="u10dfe2af">
    </p>
    <p class="img-center">
     <img alt="" height="502" id="ua76a8157" src="https://i-blog.csdnimg.cn/img_convert/0960ff37563057501c8453ec92eca85e.png" width="1452"/>
    </p>
    <p id="u0085bdb2">
    </p>
    <p id="u43f22c9f">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u43764f68">
     🔹
     <strong>
      现有视觉 Prompt Tuning 方法缺乏对最佳视觉 Prompt 设计的系统分析
     </strong>
    </p>
    <ul>
     <li id="udfa314da">
      目前在视觉模态（如 CLIP）上的 Prompt Tuning 主要基于
      <strong>
       可学习的视觉 Token 或手工设计的文本 Prompt
      </strong>
      ，但对于
      <strong>
       如何选择最佳视觉 Prompt 以提升模型性能
      </strong>
      研究较少。
     </li>
     <li id="u389799bb">
      现有方法
      <strong>
       缺乏对 Prompt 质量的量化分析
      </strong>
      ，导致 Prompt 设计往往依赖经验，泛化能力受限。
     </li>
    </ul>
    <p id="u42cab387">
     🔹
     <strong>
      视觉 Prompt Tuning 泛化能力较弱，低于纯文本 Prompt 方法
     </strong>
    </p>
    <ul>
     <li id="uad7c9bef">
      现有基于视觉 Prompt 的方法相比
      <strong>
       文本 Prompt（Text-only Prompt）
      </strong>
      泛化能力较差，尤其在 Few-shot 任务上，难以有效适配新类别。
     </li>
     <li id="uc31a80d6">
      由于视觉模态的信息表达方式不同，现有方法难以让视觉 Prompt 具备和文本 Prompt 相同的可迁移性。
     </li>
    </ul>
    <p id="uabd3793c">
    </p>
    <p class="img-center">
     <img alt="" height="442" id="u2de30b9a" src="https://i-blog.csdnimg.cn/img_convert/57a1e72f463a80b303cae5e6435374d1.png" width="967"/>
    </p>
    <hr id="sONr1"/>
    <p id="u13ff6af9">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="ud02b2e92">
     ✅
     <strong>
      提出 LoGoPrompt，使用合成文本图像（Synthetic Text Images）作为视觉 Prompt
     </strong>
    </p>
    <ul>
     <li id="u48c331f6">
      发现
      <strong>
       合成的文本图像（Synthetic Text Images）可以成为有效的视觉 Prompt
      </strong>
      ，让 VLM 以图像形式理解类别信息，
      <strong>
       提高模型的分类能力
      </strong>
      。
     </li>
     <li id="uafbcd15c">
      通过在图像输入中添加合成文本图像，使 CLIP
      <strong>
       直接匹配视觉类别信息
      </strong>
      ，提高分类准确性。
     </li>
    </ul>
    <p id="ua1d59b66">
     ✅
     <strong>
      提出新的 Prompt 选择策略，避免先有 Prompt 还是先分类的“鸡生蛋”问题
     </strong>
    </p>
    <ul>
     <li id="u3718fd7f">
      通过
      <strong>
       重新定义分类目标，使其转换为最佳视觉 Prompt 的选择问题
      </strong>
      ，从而解决
      <strong>
       先选择 Prompt 还是先分类的逻辑悖论
      </strong>
      。
     </li>
    </ul>
    <p id="ud20f2474">
     ✅
     <strong>
      在 Zero-shot 和 Few-shot 任务上超越现有 SOTA 方法
     </strong>
    </p>
    <ul>
     <li id="u184441d5">
      在
      <strong>
       16 个数据集上进行实验
      </strong>
      ，
      <strong>
       无需训练任何视觉 Prompt 参数
      </strong>
      ，仍能
      <strong>
       超越当前 Few-shot 视觉 Prompt 方法
      </strong>
      ，证明 LoGoPrompt 作为视觉 Prompt 的有效性和泛化能力。
     </li>
    </ul>
    <p id="u41eb3681">
    </p>
    <p class="img-center">
     <img alt="" height="392" id="ua422c799" src="https://i-blog.csdnimg.cn/img_convert/6d535b752c6c5aef4d40e4f8b0eb257d.png" width="1801"/>
    </p>
    <h4 id="NtTUb" name="NtTUb">
     TPT: Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models
    </h4>
    <p id="u2242de8a">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u24a2769b">
     🔹
     <strong>
      现有 Prompt Tuning 依赖下游任务数据，降低模型泛化能力
     </strong>
    </p>
    <ul>
     <li id="uefbd9c90">
      现有方法通常使用
      <strong>
       下游任务的标注数据
      </strong>
      来训练可学习 Prompt，尽管这提高了特定任务的性能，但会
      <strong>
       降低模型在新领域和未见类别上的泛化能力
      </strong>
      。
     </li>
     <li id="uaa1f2a26">
      在
      <strong>
       跨数据集或自然分布变化（Natural Distribution Shift）
      </strong>
      场景中，这种依赖任务数据的 Prompt Tuning 方法表现不稳定，影响 Zero-shot 适应能力。
     </li>
    </ul>
    <p id="ua31507ce">
     🔹
     <strong>
      现有方法无法在测试时动态调整 Prompt，提高适应性
     </strong>
    </p>
    <ul>
     <li id="u7c702673">
      传统 Prompt 学习方法
      <strong>
       在训练阶段固定 Prompt
      </strong>
      ，导致测试时
      <strong>
       无法根据新数据自适应调整
      </strong>
      ，难以适应
      <strong>
       未见类别或不同数据分布
      </strong>
      。
     </li>
     <li id="ucf380c1a">
      目前缺乏
      <strong>
       无需额外训练数据的 Prompt 适配方案
      </strong>
      ，使得模型在 Zero-shot 任务中的泛化能力受限。
     </li>
    </ul>
    <hr id="ok0Yg"/>
    <p id="ua3db2e8f">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u6b594295">
     ✅
     <strong>
      提出 Test-time Prompt Tuning (TPT)，在测试时动态调整 Prompt，提高泛化能力
     </strong>
    </p>
    <ul>
     <li id="u57ff0d2b">
      在
      <strong>
       测试阶段
      </strong>
      直接优化 Prompt，而无需额外的训练数据，使得 Prompt
      <strong>
       能够在推理时根据新样本自适应调整
      </strong>
      ，提升 Zero-shot 任务的稳定性。
     </li>
    </ul>
    <p id="u12ee3b8d">
     ✅
     <strong>
      利用熵最小化（Entropy Minimization）和置信度选择优化 Prompt
     </strong>
    </p>
    <ul>
     <li id="u542f8e7b">
      通过
      <strong>
       最小化测试样本的预测熵
      </strong>
      ，确保模型在不同增强视角下的预测结果一致，从而动态调整 Prompt，使其适应不同数据分布。
     </li>
     <li id="ub8878a2f">
      结合
      <strong>
       置信度选择（Confidence Selection）
      </strong>
      ，确保优化后的 Prompt 能够提高模型的稳定性和准确性。
     </li>
    </ul>
    <p id="u8e854661">
     ✅
     <strong>
      在 Zero-shot 任务中超越现有 Prompt Tuning 方法，提升跨数据集泛化能力
     </strong>
    </p>
    <ul>
     <li id="u3e07c200">
      <strong>
       在自然分布变化任务上，Zero-shot Top-1 准确率提升 3.6%
      </strong>
      ，超越所有需要额外训练数据的 Prompt 方法。
     </li>
     <li id="u66abb9cc">
      <strong>
       在未见类别的跨数据集泛化任务上，TPT 的表现与当前 SOTA 方法相当
      </strong>
      ，但不需要额外的训练数据，展现更高的适配性和部署优势。
     </li>
    </ul>
    <p id="u503eb52e">
    </p>
    <p class="img-center">
     <img alt="" height="606" id="u8e5bdbc1" src="https://i-blog.csdnimg.cn/img_convert/fc16a0b5c6d9c2a3d7b71c37909d24d0.png" width="1176"/>
    </p>
    <p id="uc45f7955">
    </p>
    <h4 id="oJM3V" name="oJM3V">
     Unsupervised Prompt Learning for Vision-Language Models
    </h4>
    <p id="u0e764a21">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u3d9e2106">
     🔹
     <strong>
      现有 Prompt Tuning 方法依赖目标数据集的标注数据，限制了扩展性
     </strong>
    </p>
    <ul>
     <li id="u68e1f80f">
      方法如 CoOp、CLIP-Adapter、Tip-Adapter 需要
      <strong>
       少量标注数据（Few-shot Learning）
      </strong>
      来学习优化的 Prompt，但在
      <strong>
       无标签（Unlabeled）环境下无法使用
      </strong>
      ，影响其在大规模数据上的适用性。
     </li>
     <li id="u68d07c23">
      依赖标注数据使这些方法在
      <strong>
       Zero-shot 任务和跨数据集泛化（Transfer Learning）
      </strong>
      方面受限，降低了 VLM（如 CLIP）的适配能力。
     </li>
    </ul>
    <p id="ue9af895e">
     🔹
     <strong>
      缺乏无监督 Prompt 学习（Unsupervised Prompt Learning）的方法
     </strong>
    </p>
    <ul>
     <li id="u6fbdf6b4">
      目前的 Prompt Learning 方法大多采用
      <strong>
       监督学习（Supervised Learning）
      </strong>
      ，
      <strong>
       未能探索如何在无监督场景下优化 Prompt
      </strong>
      ，以充分释放 CLIP 的 Zero-shot 能力。
     </li>
     <li id="u33995881">
      现有 Zero-shot 方法依赖
      <strong>
       手工 Prompt 设计（Prompt Engineering）
      </strong>
      ，
      <strong>
       无法自动适配不同数据分布
      </strong>
      ，需要大量人工干预。
     </li>
    </ul>
    <p id="u2c33a7b9">
    </p>
    <p class="img-center">
     <img alt="" height="443" id="u4036f34b" src="https://i-blog.csdnimg.cn/img_convert/d78b71ef94eddb4ab9d3cdb685a2dd54.png" width="767"/>
    </p>
    <hr id="dWPVF"/>
    <p id="u02859819">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u4f12fbb8">
     ✅
     <strong>
      提出 Unsupervised Prompt Learning (UPL)，在无标签数据上优化 Prompt
     </strong>
    </p>
    <ul>
     <li id="u43cb5a05">
      <strong>
       UPL 是首个引入无监督学习（Unsupervised Learning）到 Prompt Tuning 的方法
      </strong>
      ，无需目标数据集的标注数据即可提升 CLIP 的迁移性能。
     </li>
     <li id="udd8620e3">
      通过无监督优化策略，使得 CLIP
      <strong>
       能够自动学习最优 Prompt，而不依赖手工设计
      </strong>
      ，减少人工干预。
     </li>
    </ul>
    <p id="ucceb3738">
     ✅
     <strong>
      避免 Prompt Engineering，同时提升 CLIP 的 Zero-shot 泛化能力
     </strong>
    </p>
    <ul>
     <li id="u0e58243a">
      通过
      <strong>
       优化 Prompt 使其适应不同数据分布
      </strong>
      ，UPL
      <strong>
       在 Zero-shot 任务中超越 CLIP+手工 Prompt
      </strong>
      ，提高跨数据集适配能力。
     </li>
     <li id="u40fe4469">
      <strong>
       在 ImageNet 及 10 个其他数据集上均超过 CLIP 的手工 Prompt 结果
      </strong>
      ，表明 UPL 能够有效提升 Zero-shot 识别能力。
     </li>
    </ul>
    <p id="ubdd9ddf1">
     ✅
     <strong>
      UPL 在无监督环境下的表现接近 Few-shot Prompt Tuning 方法
     </strong>
    </p>
    <ul>
     <li id="u831bcfca">
      <strong>
       UPL 的增强版本在多数数据集上可匹敌 8-shot CoOp 和 8-shot TIP-Adapter
      </strong>
      ，表明即使
      <strong>
       没有任何标注数据
      </strong>
      ，UPL 仍能在 Few-shot 级别的任务中达到接近 SOTA 的效果，具有更高的扩展性和实用价值。
     </li>
    </ul>
    <p id="uf8cc7786">
    </p>
    <p class="img-center">
     <img alt="" height="483" id="u716106c6" src="https://i-blog.csdnimg.cn/img_convert/b3e3d3e2ea38972c28f35eecc05418ff.png" width="1559"/>
    </p>
    <p id="u52ff3730">
    </p>
    <h4 id="Y9b8d" name="Y9b8d">
     Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models
    </h4>
    <p id="u9257494b">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="u27a66975">
     🔹
     <strong>
      现有高效迁移学习（ETL）方法未考虑不同任务的迁移难度差异
     </strong>
    </p>
    <ul>
     <li id="u212f8cff">
      目前的 Prompt Learning 和 Adapter 方法在迁移 VLM（如 CLIP）到下游任务时，
      <strong>
       未针对不同任务的难度进行优化
      </strong>
      ，导致方法在某些任务上的适应能力受限。
     </li>
     <li id="u22b8d7e3">
      <strong>
       高迁移难度任务（如细粒度分类、跨领域任务）
      </strong>
      需要更复杂的调整，而
      <strong>
       低迁移难度任务
      </strong>
      则可以直接利用预训练模型的知识，但现有方法无法动态适配。
     </li>
    </ul>
    <p id="u2a1243fc">
     🔹
     <strong>
      缺乏优化适应不同迁移难度的 Prompt 和 Adapter 组合策略
     </strong>
    </p>
    <ul>
     <li id="u09a7d38f">
      现有方法通常采用
      <strong>
       单一方式（仅使用 Prompt 或 Adapter）
      </strong>
      进行 VLM 适配，而没有考虑
      <strong>
       如何在不同难度任务之间合理分配 Prompt 和 Adapter 资源
      </strong>
      ，导致泛化能力受限。
     </li>
     <li id="u2fe25f8a">
      在低难度任务上，过度依赖任务特定适配可能导致
      <strong>
       信息损失
      </strong>
      ，而在高难度任务上，单独使用 Prompt 或 Adapter 可能
      <strong>
       不足以提升泛化能力
      </strong>
      。
     </li>
    </ul>
    <hr id="UEUgr"/>
    <p id="u62def31e">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="ua99e3696">
     ✅
     <strong>
      提出自适应集成方法（Adaptive Ensemble），根据迁移难度优化 Prompt 和 Adapter 组合策略
     </strong>
    </p>
    <ul>
     <li id="u70394033">
      <strong>
       在高迁移难度任务中
      </strong>
      ，强调
      <strong>
       视觉 Prompt（Vision Prompt）+ 文本 Adapter（Text Adapter）
      </strong>
      的协同作用，提高 VLM 的适配能力。
     </li>
     <li id="u2d31d5a1">
      <strong>
       在低迁移难度任务中
      </strong>
      ，更倾向于利用
      <strong>
       原始预训练模型的通用知识
      </strong>
      ，减少不必要的适配，保留模型原生的泛化能力。
     </li>
    </ul>
    <p id="u6c159aba">
     ✅
     <strong>
      采用动态融合策略，自适应调整任务特定知识与通用知识的比例
     </strong>
    </p>
    <ul>
     <li id="u25e8ae3d">
      通过
      <strong>
       自适应集成（Adaptive Ensemble）
      </strong>
      机制，在不同任务上动态调整
      <strong>
       任务适配模型（Task-adapted VLMs）
      </strong>
      和
      <strong>
       原始 VLMs
      </strong>
      之间的融合权重。
     </li>
     <li id="u9c604f57">
      <strong>
       在低难度任务上更多依赖 VLM 原生能力，在高难度任务上强化任务特定适配
      </strong>
      ，确保模型在不同任务间的最优表现。
     </li>
    </ul>
    <p id="u3bcd905f">
     ✅
     <strong>
      在多个基准数据集上超越现有方法，尤其在未见任务（Unseen Tasks）上提升显著
     </strong>
    </p>
    <ul>
     <li id="u2f3b0a3a">
      该方法在
      <strong>
       Zero-shot 任务和跨数据集迁移任务上均超过所有基线方法
      </strong>
      ，尤其在
      <strong>
       高迁移难度任务上展现出更优的泛化能力
      </strong>
      ，证明其在不同任务中的适应性和有效性。
     </li>
    </ul>
    <p id="u70993730">
    </p>
    <p class="img-center">
     <img alt="" height="508" id="u0b383541" src="https://i-blog.csdnimg.cn/img_convert/b17d5ac11d66fa081947a321cc31098b.png" width="1468"/>
    </p>
    <p id="u8d4ae6a7">
    </p>
    <h4 id="vwRHR" name="vwRHR" style="background-color:transparent">
     Waffling around for Performance: Visual Classification with Random Words and Broad Concepts
    </h4>
    <p id="u2ecc283e">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="ue6cdea77">
     🔹
     <strong>
      现有 VLM（如 CLIP）结合 LLM 生成的类别描述提升分类性能，但计算成本高
     </strong>
    </p>
    <ul>
     <li id="ube8ad10c">
      研究表明，使用 LLM（如 GPT-3）生成类别描述（如
      <em>
       “waffle, which has a round shape”
      </em>
      ）可以提高 CLIP 在 Zero-shot 视觉分类任务上的泛化能力。
     </li>
     <li id="uac67e980">
      但
      <strong>
       依赖 LLM 生成文本的方式计算成本高，需要额外的 API 调用
      </strong>
      ，且 LLM 生成的描述未必始终可靠。
     </li>
    </ul>
    <p id="u5f362d74">
     🔹
     <strong>
      缺乏对 LLM 生成的语义描述影响的深入分析
     </strong>
    </p>
    <ul>
     <li id="ud885fe7b">
      目前的研究主要关注 LLM 生成的类别描述对视觉分类性能的提升效果，但
      <strong>
       尚未系统性分析 LLM 生成的语义信息是否真正有效
      </strong>
      ，或者是否可以用更简单的方法替代。
     </li>
    </ul>
    <p id="u3f6b0a56">
    </p>
    <p class="img-center">
     <img alt="" height="563" id="u994a7227" src="https://i-blog.csdnimg.cn/img_convert/d02adf2c254b68c90f7ccadeefa53379.png" width="769"/>
    </p>
    <hr id="ZJPUH"/>
    <p id="uaebc105c">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="u609448b0">
     ✅
     <strong>
      提出 WaffleCLIP，用随机字符和单词替代 LLM 生成的类别描述，达到类似提升效果
     </strong>
    </p>
    <ul>
     <li id="u79e263e2">
      <strong>
       无需调用外部 LLM
      </strong>
      ，仅使用
      <strong>
       随机字符或无关词汇
      </strong>
      作为类别描述，即可在 Zero-shot 视觉分类任务中获得与 LLM 生成描述相近的性能提升。
     </li>
     <li id="u6bcef9cc">
      证明
      <strong>
       LLM 生成的类别描述可能并非真正带来了额外的语义信息，而可能仅仅是增加了类别的文本信息量
      </strong>
      。
     </li>
    </ul>
    <p id="ube736864">
     ✅
     <strong>
      对 LLM 生成的类别描述进行系统性分析，揭示其局限性
     </strong>
    </p>
    <ul>
     <li id="u6a039a05">
      通过实验分析，发现 LLM 生成的类别描述
      <strong>
       可能并未有效利用高层语义
      </strong>
      ，部分情况下仅相当于一种
      <strong>
       数据增强
      </strong>
      。
     </li>
     <li id="u7b3bf549">
      进一步研究
      <strong>
       在类别名称存在歧义时，如何更有效地利用 LLM 提供的高层语义
      </strong>
      ，如使用 LLM 来生成更具区分度的概念描述。
     </li>
    </ul>
    <p id="ubb418c8a">
     ✅
     <strong>
      提供低成本替代方案，并作为未来 VLM + LLM 研究的基准
     </strong>
    </p>
    <ul>
     <li id="u83fe8e02">
      WaffleCLIP 既是一个
      <strong>
       低成本、无需 LLM 依赖的 Prompt 扩展方法
      </strong>
      ，也可以作为
      <strong>
       评估未来 LLM 扩展 VLM 方法有效性的基准
      </strong>
      ，防止不必要的 LLM 计算开销。
     </li>
    </ul>
    <p id="ub9ef4261">
    </p>
    <p class="img-center">
     <img alt="" height="772" id="u839c2295" src="https://i-blog.csdnimg.cn/img_convert/53853a0ab05339fd83ed14f1f97ea56a.png" width="1543"/>
    </p>
    <p id="udc54858b">
    </p>
    <h4 id="igfeN" name="igfeN" style="background-color:transparent">
     Weak Distribution Detectors Lead to Stronger Generalizability of Vision-Language Prompt Tuning
    </h4>
    <p id="u94fce194">
     <strong>
      1️⃣
     </strong>
     <strong>
      论文试图解决的核心难点
     </strong>
    </p>
    <p id="ue09b2a21">
     🔹
     <strong>
      现有 VLM（如 CLIP）在 Few-shot Fine-tuning 过程中难以兼顾已见类别（Base Classes）和新类别（Novel Classes）的泛化能力
     </strong>
    </p>
    <ul>
     <li id="u128976e9">
      传统 Few-shot Fine-tuning 方法（如 CoOp、ProGrad）
      <strong>
       在训练集类别（Base Classes）上表现良好，但对未见类别（Novel Classes）泛化能力有限
      </strong>
      ，即
      <strong>
       Base-to-Novel Generalization（BNT）问题
      </strong>
      。
     </li>
     <li id="ucd6f0673">
      由于 Fine-tuning 过程中模型的表示会逐渐偏向 Base 类别，导致模型在 Zero-shot 任务中难以有效利用预训练知识。
     </li>
    </ul>
    <p id="uf35b3078">
     🔹
     <strong>
      缺乏有效的方法来动态调整 Zero-shot 预测与 Fine-tuned 预测的权重
     </strong>
    </p>
    <ul>
     <li id="u68519906">
      现有方法往往
      <strong>
       固定使用 Fine-tuned 分类器
      </strong>
      ，而没有在测试时考虑
      <strong>
       当前样本是否更接近预训练分布（Zero-shot）或下游任务分布（Fine-tuned）
      </strong>
      ，导致在新类别上的识别能力不足。
     </li>
    </ul>
    <hr id="EsEpe"/>
    <p id="ub0e0d8d3">
     <strong>
      2️⃣
     </strong>
     <strong>
      论文提出的创新点
     </strong>
    </p>
    <p id="uef88a0e0">
     ✅
     <strong>
      基于 OOD 检测（Out-of-Distribution Detection）判断样本是否属于 Base 类别或 Novel 类别
     </strong>
    </p>
    <ul>
     <li id="ue7dfd9fb">
      在测试阶段，引入 OOD 检测方法，
      <strong>
       预测当前样本是来自预训练数据分布（Base）还是新的数据分布（Novel）
      </strong>
      ，从而动态调整分类器的使用方式。
     </li>
    </ul>
    <p id="ufa2ad874">
     ✅
     <strong>
      提出基于竞争得分的动态分类器融合策略，提高 Base-to-Novel 泛化能力
     </strong>
    </p>
    <ul>
     <li id="u4aab14a2">
      <strong>
       竞争得分（Competition-based Scoring Function）
      </strong>
      计算
      <strong>
       Zero-shot 预测结果与 Fine-tuned 预测结果的偏向性
      </strong>
      ，并利用该得分进行加权融合：
     </li>
    </ul>
    <ul>
     <li>
      <ul>
       <li id="u5001c1bb">
        若样本更可能属于预训练分布（Base Classes），则更偏向
        <strong>
         Zero-shot 分类器
        </strong>
        。
       </li>
       <li id="u8c3fef04">
        若样本可能属于 Novel 类别，则更偏向
        <strong>
         Fine-tuned 分类器
        </strong>
        。
       </li>
      </ul>
     </li>
    </ul>
    <ul>
     <li id="u1ec3f866">
      这种方法
      <strong>
       在不影响 Base 类别性能的前提下，提高了新类别的泛化能力
      </strong>
      。
     </li>
    </ul>
    <p id="ud9bfe745">
     ✅
     <strong>
      方法仅在测试阶段执行，无需额外训练，可直接提升现有 Few-shot Fine-tuning 方法的泛化能力
     </strong>
    </p>
    <ul>
     <li id="u28326b66">
      <strong>
       无需重新训练模型
      </strong>
      ，可直接应用于现有 Fine-tuning 方法，如
      <strong>
       CoOp 和 ProGrad
      </strong>
      ，在 11 个数据集上
      <strong>
       提升 CoOp 2.6% 和 ProGrad 1.5% 的 Harmonic Mean 指标
      </strong>
      ，证明其在 Base-to-Novel 泛化任务上的有效性。
     </li>
    </ul>
    <p id="u78159721">
    </p>
    <p class="img-center">
     <img alt="" height="592" id="ua8f7b41f" src="https://i-blog.csdnimg.cn/img_convert/f88ceba4162c2ca4c163acd3d82004ac.png" width="1009"/>
    </p>
    <p id="ua4450704">
     <a href="https://github.com/zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs" title="参考仓库：GitHub - zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs: A curated list of awesome prompt/adapter learning methods for vision-language models like CLIP.">
      参考仓库：GitHub - zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs: A curated list of awesome prompt/adapter learning methods for vision-language models like CLIP.
     </a>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f36343432363934372f:61727469636c652f64657461696c732f313436323434313538" class_="artid" style="display:none">
 </p>
</div>


