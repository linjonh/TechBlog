---
layout: post
title: "Prompt-Learning-Awesome"
date: 2025-03-13 22:48:21 +0800
description: "prompt learning领域顶会论文总结"
keywords: "textrefiner: internal visual feature as efficient refiner for vision-languag"
categories: ['未分类']
tags: ['深度学习', '机器学习', '人工智能', 'Prompt']
artid: "146244158"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146244158
    alt: "Prompt-Learning-Awesome"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146244158
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146244158
cover: https://bing.ee123.net/img/rand?artid=146244158
image: https://bing.ee123.net/img/rand?artid=146244158
img: https://bing.ee123.net/img/rand?artid=146244158
---

# Prompt Learning Awesome

_**提示调整是一种将预训练的视觉语言模型 (VLM) 适应各种下游任务的宝贵技术**_

* * *

#### COOP: Prompt Learning for Vision-Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **Prompt Engineering 难以手工优化** ：

  * 现有的 **CLIP** 依赖于 **手工设计的 Prompt（Prompt Engineering）** ，但 **稍微改变 Prompt 的措辞可能会严重影响模型表现** ，因此需要 **大量试错** 。
  * 设计合适的 Prompt 需要 **领域知识** ，并且不同任务（如医学、遥感）可能需要不同的 Prompt 设计，**泛化性较差** 。

🔹 **Zero-shot 任务的 Prompt 泛化能力受限** ：

  * 由于 CLIP 的 Prompt 主要是 **固定的文本模板** ，无法适应不同的数据分布，在特定 **下游任务（如图像分类）** 时，手工 Prompt **难以在所有类别上泛化** 。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 CoOp（Context Optimization），用可训练的 Soft Prompt 代替手工 Prompt**

  * 通过 **可训练的 Prompt 向量** （Soft Prompt）替换 CLIP 的手工文本模板：

    
    
    [P1, P2, P3, ..., Pn, {class}]

  * **Prompt 的 Context 部分可训练** ，而 CLIP 本身的权重保持不变，提高适应性。

✅ **提出两种 Prompt 结构，提升适配性**

  * **Unified Context（统一 Prompt）** ：所有类别共享一个 **Soft Prompt** ，适用于小规模任务。
  * **Class-Specific Context（类别特定 Prompt）** ：不同类别拥有不同的 Prompt，提高类别区分度。

✅ **Few-shot 场景下显著超越手工 Prompt**

  * **在 1-2 Shot 学习下，CoOp 已经优于人工设计的 Prompt** 。
  * **在 16 Shot 任务下，平均提升 15%（最高可达 45%）** ，远超 Zero-shot CLIP。

✅ **无需手动调整 Prompt，且具有良好的领域泛化能力**

  * **即使是在新领域的数据集上，CoOp 仍能比手工 Prompt 取得更好的泛化性能** 。
  * 解决了 **CLIP 迁移到下游任务时对 Prompt 设计的依赖** ，提高了 **Zero-shot 任务的稳健性** 。

![](https://i-blog.csdnimg.cn/img_convert/8597ce633f9a62035b92eecf5b19e7c5.png)

![](https://i-blog.csdnimg.cn/img_convert/8f98ccd6dd73ea98a2dc15442a2ade0a.png)

#### COCOOP: Conditional Prompt Learning for Vision-Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **CoOp 存在泛化性问题，容易过拟合训练类别**

  * CoOp 通过学习 **固定的 Soft Prompt** 适配 CLIP，但其 **学习到的 Prompt 对未见类别泛化能力差** ，仅在训练类别上表现良好。
  * 在 Zero-shot 场景中，CoOp **无法有效适应未见类别，导致性能下降** 。

🔹 **静态 Prompt 无法适应不同图像实例**

  * CoOp 采用 **静态的 Soft Prompt** ，所有类别共享相同的 Prompt，无法根据输入图像动态调整。
  * 这种固定的 Prompt 使得模型在类别分布变化时难以适应，影响跨任务迁移能力。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 CoCoOp（Conditional Context Optimization），引入动态 Prompt 机制**

  * 在 CoOp 基础上，使用 **轻量级神经网络** 生成 **基于输入图像的动态 Prompt** ，使 Prompt **可以根据不同的图像实例自适应调整** 。

✅ **提升 Zero-shot 泛化能力**

  * 由于 Prompt 不再是静态的，而是 **针对每张图像动态变化** ，CoCoOp **能更好地适应未见类别** ，减少 CoOp 过拟合训练类别的问题。

✅ **增强跨数据集的迁移能力，提高 Domain Generalization 表现**

  * 通过动态 Prompt 生成，CoCoOp 在多个数据集上展示出 **更强的泛化能力** ，不仅适用于 Zero-shot 学习，还能在跨领域任务上保持较好的适配性。

![](https://i-blog.csdnimg.cn/img_convert/6731450ae887487ff95865375803016c.png)

![](https://i-blog.csdnimg.cn/img_convert/f6544d6a165d4cebac03014dd56f6b80.png)

#### TextRefiner: Internal Visual Feature as Efficient Refiner for Vision-
Language Models Prompt Tuning

**1️⃣****论文试图解决的核心难点**

🔹 **现有 Prompt Tuning 采用粗粒度方法，缺乏类别特异性**

  * 现有方法（如 CoOp）学习到的 Prompt **是全类别共享的** ，无法针对 **类别特定的视觉概念进行区分** ，导致 **对于共享相似视觉特征的类别（如动物品种）区分能力不足** 。

🔹 **现有改进方法依赖外部知识（如 LLMs），计算成本高**

  * 近期研究尝试通过 **大语言模型（LLMs）** 生成类别描述增强 Prompt，但这种方法在推理时需要额外的 LLM 调用，**推理成本高、效率低** 。

![](https://i-blog.csdnimg.cn/img_convert/1f545bbf28da58c5cffa9971e6b7f3f8.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 TextRefiner，结合 VLM 内部知识提升 Prompt 表达能力**

  * 通过 **视觉分支中的局部 Token 提取细粒度视觉概念** ，存入一个 **本地缓存模块（Local Cache Module）** ，然后与文本分支对齐，**增强 Prompt 细粒度信息** ，不依赖外部知识。

✅ **无外部依赖，高效优化现有 Prompt 学习方法**

  * **作为 Plug-and-Play 模块** ，可直接应用于现有 Prompt 方法（如 CoOp、CoCoOp），无需额外训练外部模型，提高泛化能力。

✅ **提升 Prompt 质量，显著提高下游任务性能**

  * 在 **11 个基准测试** 上，将 CoOp **从 71.66% 提高到 76.94%** ，超越 CoCoOp 的 **实例级 Prompt 生成方法** 。
  * **结合 PromptKD 可达到 SOTA 级别** ，且推理效率更高。

![](https://i-blog.csdnimg.cn/img_convert/90ff0f6ac2486aad785c68c6dea122ed.png)

#### KgCoOp : Visual-Language Prompt Tuning with Knowledge-guided Context
Optimization

**1️⃣****论文试图解决的核心难点**

🔹 **CoOp 生成的可训练 Prompt 泛化能力较差**

  * CoOp 结合可学习的文本 Token 和类别 Token，但其 **学习到的特定任务文本知识对未见类别的泛化能力较差** ，导致 Zero-shot 任务表现不佳。
  * 由于模型只关注可学习 Prompt，**会遗忘预训练模型中原本具备的通用文本知识** ，进一步降低泛化能力。

🔹 **现有 Prompt Tuning 方法未能充分保留通用语言知识**

  * 由于 Prompt 经过训练后偏向特定任务，**会与 CLIP 预训练时的手工 Prompt 产生较大偏差** ，影响在 Zero-shot 任务上的稳定性。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 KgCoOp（Knowledge-guided Context Optimization），结合手工 Prompt 以增强泛化能力**

  * 通过 **减少可学习 Prompt 与手工 Prompt 之间的差异** ，避免模型遗忘通用文本知识，提升 Zero-shot 任务表现。

✅ **引入知识约束，平衡任务特定性与通用性**

  * 在对比学习损失上加入 **Prompt 语义对齐约束** ，让 **可训练 Prompt 既能学习任务特定信息，又保留 CLIP 预训练的文本泛化能力** 。

✅ **提升泛化能力，减少训练成本**

  * 在多个基准测试上，KgCoOp **在 seen/unseen 任务中均取得更好的性能** ，且相比 CoOp **训练时间更短** ，提高了 Prompt Tuning 的效率。

![](https://i-blog.csdnimg.cn/img_convert/3e0d656c341942d2819dfafb86830c26.png)

#### MaPLe: Multi-modal Prompt Learning

**1️⃣****论文试图解决的核心难点**

🔹 **现有 CLIP 适配方法仅调整单模态（语言或视觉）分支，优化不充分**

  * 现有 Prompt Tuning 方法（如 CoOp、CoCoOp）**仅对 CLIP 的文本分支进行优化** ，忽略了视觉分支的动态调整，导致 **跨模态表示不够协调** 。
  * 这种单模态调整方式 **缺乏对视觉和语言信息的联合优化** ，无法充分利用 VLM 的多模态对齐能力。

🔹 **现有方法缺乏对不同层级特征的建模，影响泛化能力**

  * 现有方法大多在固定层级应用 Prompt，**未能逐步建模不同层级的特征交互** ，限制了对复杂上下文信息的捕捉能力。

![](https://i-blog.csdnimg.cn/img_convert/5a1c457a71986bd99ced47b8a9031ba4.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 Multi-modal Prompt Learning (MaPLe)，同时优化视觉和语言分支**

  * **不同于 CoOp/CoCoOp 仅优化文本分支，MaPLe 同时学习视觉和文本 Prompt** ，确保两者协同优化，从而提升跨模态对齐能力。
  * 通过双模态联合优化，增强 CLIP 在下游任务中的适配能力，提高泛化性。

✅ **引入层级化 Prompt 机制，逐步建模不同层级的特征关系**

  * 在 CLIP 的 **不同早期阶段分别学习独立的 Prompt** ，逐步建模层级特征，使模型能够捕捉更丰富的上下文信息，提升分类性能。

✅ **显著提升 Zero-shot 泛化能力，在多个任务上优于 CoCoOp**

  * 在 **新类别泛化（novel classes）** 任务上比 CoCoOp 提高 **3.45%** ，在 **整体泛化任务（harmonic mean）** 上提高 **2.72%** ，**在 11 个图像分类数据集上均表现更优** 。
  * 在 **新数据集泛化、新领域迁移** 任务上展现更好的适配能力。

![](https://i-blog.csdnimg.cn/img_convert/571070a71f60e8f102857ad40d00763a.png)

#### DePT: Decoupled Prompt Tuning

**1️⃣****论文试图解决的核心难点**

🔹 **Prompt Tuning 存在 Base-New Tradeoff（BNT）问题，影响泛化能力**

  * 现有 Prompt Tuning 方法在适配 **Base 任务（训练任务）** 时，会导致模型在 **New 任务（未见任务）** 上的泛化能力下降，即 **适配能力和泛化能力存在权衡** 。
  * 这种现象的根本原因在于 **模型在 Prompt Tuning 过程中会偏向 Base 任务，导致跨任务共享知识被削弱** ，影响 Zero-shot 识别能力。

🔹 **BNT 现象源于通道偏置（Channel Bias），影响任务共享特征的保持**

  * 研究发现，在 Prompt Tuning 过程中，**大部分特征通道会被 Base 任务的特定知识占据** ，导致 **重要的任务共享特征被压缩甚至丢失** ，进而降低模型在新任务上的泛化能力。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 Decoupled Prompt Tuning (DePT) 解决 Base-New Tradeoff（BNT）问题**

  * 通过 **特征空间解耦** ，在 Prompt Tuning 过程中 **将 Base 任务的特定知识从通道中隔离** ，避免其过度占用模型的特征表示能力。
  * 使模型在适配 Base 任务的同时，**最大程度保留跨任务共享知识** ，提高 Zero-shot 任务的泛化能力。

✅ **DePT 作为通用框架，可与现有 Prompt Tuning 方法结合**

  * DePT **与现有的 Prompt Tuning 方法（如 CoOp、CoCoOp、MaPLe）正交** ，可以无缝集成到这些方法中，提升它们的泛化能力。

✅ **无需额外计算成本，即可显著提升 Zero-shot 任务表现**

  * 在多个数据集上验证，DePT **在不增加显著计算开销的情况下，增强 Prompt Tuning 任务的灵活性和泛化性** ，有效缓解 BNT 问题，提高模型在新任务上的适配能力。

![](https://i-blog.csdnimg.cn/img_convert/5b11444521e41aa4b0ce974887b37ce7.png)

#### QNet: PROMPT LEARNING WITH QUATERNION NETWORKS

**1️⃣****论文试图解决的核心难点**

🔹 **现有多模态融合策略结构单一，难以捕捉多样化特征模式**

  * 目前的多模态预训练模型在融合不同模态（如文本和视觉）时，主要依赖**显式交互结构** ，但这些方法在处理**细粒度分类和抽象语义对齐** 时表现不佳。
  * 由于缺乏对多模态间复杂特征关系的建模能力，现有方法在 **Zero-shot 场景下的泛化能力有限** 。

🔹 **多模态特征融合效率低，参数量大，影响模型适配性**

  * 现有方法在跨模态学习时通常涉及大量参数，导致计算效率低，尤其在跨数据集迁移和域泛化任务中难以保持稳定性能。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 QNet（Prompt Learning with Quaternion Networks），利用四元数网络增强多模态语义对齐**

  * 通过**四元数隐藏空间（Quaternion Hidden Space）** ，利用**互相正交的虚部轴（Imaginary Axes）** 进行多视角特征建模，捕捉多模态间丰富的语义空间关系。

✅ **跨层级特征建模，增强不同模态间的语义依赖**

  * 采用**多层级特征编码机制** ，整合跨层级特征，增强不同模态之间的深度交互能力，提高 Zero-shot 任务中的细粒度分类能力。

✅ **更少参数、更强泛化能力，在多个任务上超越 SOTA**

  * **在 11 个数据集上测试，QNet 在 Base-to-Novel 泛化、跨数据集迁移、领域泛化任务上均优于现有 Prompt Learning 方法** ，同时**减少了可训练参数量，提高计算效率** 。

![](https://i-blog.csdnimg.cn/img_convert/82c6b32783effc1a58042fcb43f09a21.png)

#### TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model

**1️⃣****论文试图解决的核心难点**

🔹 **现有 CoOp 及其改进方法在未见领域（Unseen Domains）上的泛化能力有限**

  * 现有基于 CoOp 的 Prompt Tuning 方法 **依赖可学习的文本 Token** ，但这些 Token 在**新类别分布上无法动态调整** ，导致在 Unseen Domains 任务上表现不佳。
  * 由于 Prompt 设计未考虑**类别级别的可区分性（Class Discriminability）** ，使得不同类别之间的区分度降低，影响模型泛化能力。

🔹 **缺乏类别感知的文本表示，影响 Zero-shot 任务中的判别能力**

  * 现有方法主要学习**共享或图像条件的 Prompt** ，未能显式利用类别级别的先验知识，导致对于**相似类别（如不同品种的动物）** ，Prompt 难以准确区分。

![](https://i-blog.csdnimg.cn/img_convert/fbf73516c745ab4f2f88da3364b8fc7b.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 TCP（Textual-based Class-aware Prompt Tuning），引入类别感知的 Prompt 机制**

  * 通过 **Textual Knowledge Embedding (TKE)** ，映射类别级别的文本知识，使 Prompt 具备类别特异性，提高不同类别的区分能力。

✅ **利用 TKE 生成动态类别感知 Prompt，提高 Unseen Domain 任务泛化性**

  * 在推理阶段，TKE 可**动态生成针对未见类别的 Class-aware Prompt** ，增强 Prompt 适应不同类别分布的能力，**提升 Zero-shot 任务表现** 。

✅ **TCP 作为 Plug-and-Play 模块，可无缝集成至现有 Prompt Tuning 方法**

  * 可轻松结合现有的 Prompt Tuning 方法（如 CoOp、CoCoOp），且**减少训练时间的同时提升性能** ，在多个数据集上实现**更优的泛化能力** 。

![](https://i-blog.csdnimg.cn/img_convert/0ba49f0b738f6b0e010203adc1ce2d25.png)

#### MMA: Multi-Modal Adapter for Vision-Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **VLM（视觉-语言模型）在 Few-shot 任务中面临判别性与泛化性的权衡（Discrimination-Generalization
Dilemma）**

  * 在适配下游任务时，**需要同时保持模型的通用知识（General Knowledge）和任务特定知识（Task-Specific Knowledge）** ，但目前缺乏有效的方法来区分和优化这两种表征。
  * 过度优化任务特定知识会导致泛化能力下降，而保留过多通用知识又可能影响分类精度。

🔹 **视觉与语言分支之间的特征对齐存在语义鸿沟**

  * 研究发现，**语言特征比视觉特征更具判别性** ，但两者在 Transformer 低层的特征差异较大，导致跨模态对齐困难，影响 Few-shot 任务性能。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 Multi-Modal Adapter (MMA)，优化视觉与语言特征对齐**

  * MMA **聚合视觉和语言分支的特征** ，将它们映射到一个共享特征空间，使梯度能够在不同模态间有效传播，增强跨模态交互能力。

✅ **基于层级分析，选择性优化高层 Transformer 层，提高判别性与泛化性平衡**

  * 通过分析不同层的信息分布，发现**高层特征偏向任务特定知识，低层特征更具通用性** ，因此 MMA 仅在高层 Transformer 中加入适配模块，避免影响模型的通用能力。

✅ **在多个泛化任务上超越 SOTA，提升 Zero-shot 适应能力**

  * 在 **新类别泛化（Novel Classes）、新目标数据集（New Target Datasets）和领域泛化（Domain Generalization）** 任务中，MMA **在多个数据集上均优于现有 Prompt Tuning 方法** ，实现更优的判别性与泛化能力平衡。

![](https://i-blog.csdnimg.cn/img_convert/8b8431c827f81a90aaad5a94b28d6b94.png)

![](https://i-blog.csdnimg.cn/img_convert/eb2163fe8a4b2c5123daadd6812afe1b.png)

#### CoPrompt: Consistency-guided Prompt Learning for Vision-Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **VLM 在 Few-shot Fine-tuning 过程中容易过拟合，导致泛化能力下降**

  * 在小样本（Few-shot）任务中，对 VLM（视觉-语言模型）进行 Fine-tuning 容易导致 **对训练集的过拟合** ，从而影响 **Zero-shot 任务和跨域泛化能力** 。
  * 现有 Prompt Tuning 方法 **缺乏有效的正则化策略** 来防止 Fine-tuning 过程中模型对下游任务数据的过拟合。

🔹 **现有方法无法同时兼顾 Prompt Tuning 和 Adapter Tuning 的优势**

  * Prompt Tuning 在输入空间进行调整，而 Adapter Tuning **通过在模型内部插入小型可训练模块** 进行调整，但目前的研究很少考虑如何**结合两者的优势** ，提升泛化能力和适配性。

![](https://i-blog.csdnimg.cn/img_convert/8a8eafbc145503404f873b0d399223ed.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 CoPrompt（Consistency-guided Prompt Learning），引入一致性约束以防止过拟合**

  * 在 Fine-tuning 过程中 **引入一致性约束（Consistency Constraint）** ，确保 **可训练 Prompt 的预测结果与预训练模型的预测保持一致** ，防止模型过度拟合 Few-shot 任务数据。

✅ **结合输入扰动一致性（Perturbation Consistency），提升泛化能力**

  * 通过对输入数据添加不同扰动（Perturbation），**让 Fine-tuned 模型在不同的输入条件下保持预测一致** ，进一步提升模型的鲁棒性和 Zero-shot 泛化能力。

✅ **融合 Prompt Tuning 和 Adapter Tuning，提高 Few-shot 任务适配能力**

  * **在输入端使用 Prompt 调整输入表征，在模型内部使用 Adapter 进行特征增强** ，同时优化输入和输出空间，使 VLM 适应不同的下游任务。
  * 这种结合方式 **提升了 Zero-shot 任务表现，同时增强 Base-to-New 泛化能力** 。

✅ **在多个任务上超越 SOTA，显著提升泛化能力**

  * **在 Zero-shot 任务和 11 个数据集的 Harmonic Mean 指标上均取得 SOTA 结果** ，并在 **Base-to-Novel 泛化、领域泛化（Domain Generalization）、跨数据集迁移** 等任务中超越现有方法。

![](https://i-blog.csdnimg.cn/img_convert/9649f29b2f6a9e07c05b66a720f489c8.png)

#### CasPL: Cascade Prompt Learning for Vision-Language Model Adaptation

**1️⃣****论文试图解决的核心难点**

🔹 **现有 Prompt Learning 方法仅适用于单阶段适配，容易导致过拟合**

  * 目前的可训练 Prompt 主要用于 **任务适配（Adapting Prompt）** ，**缺乏对通用知识的学习机制** ，导致模型在下游任务中容易过拟合目标数据分布，影响泛化能力。
  * 由于 Prompt 直接在目标任务上训练，**无法充分利用无标签数据进行知识提取** ，导致任务特定信息的学习受限。

🔹 **Prompt Tuning 缺乏层次化知识提取，难以同时兼顾通用性和任务特异性**

  * 现有方法大多采用**单一 Prompt 训练方式** ，难以同时优化通用特征（Domain-General Knowledge）和任务特定特征（Task-Specific Knowledge），影响 Zero-shot 任务的稳定性。

![](https://i-blog.csdnimg.cn/img_convert/7135d2f2323eec438344d267f9b0193a.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 Cascade Prompt Learning (CasPL)，采用双阶段 Prompt 结构**

  * **第一阶段：Boosting Prompt** —— 通过一个 **更强大的 CLIP 教师模型** 提取 **无标签数据中的通用知识** ，并对其预测进行对齐，确保 Prompt 具备广泛的领域泛化能力。
  * **第二阶段：Adapting Prompt** —— 在 **冻结第一阶段 Prompt** 的基础上，对目标任务进行 Fine-tuning，以学习任务特定特征，从而降低过拟合风险。

✅ **实现通用特征与任务特定特征的解耦，提高泛化能力**

  * 采用**逐步学习的 Prompt 结构** ，确保模型在任务适配的同时，仍然保留通用知识，提高 Zero-shot 任务的稳健性。
  * 这种两阶段 Prompt 设计**有效缓解了传统单阶段 Prompt Learning 方法的过拟合问题** 。

✅ **Plug-and-Play 设计，可集成到现有 Prompt Learning 方法中，提升小型 VLM 适配能力**

  * CasPL 可无缝集成至现有的 Prompt Learning 方法，如 CoOp 和 CoCoOp，**在不增加显著计算开销的情况下提升适配能力** ，适用于**资源受限环境的小型 VLM 部署** 。
  * 在 **11 个图像分类数据集** 上，CasPL **比 PromptSRC 提高 1.85%（Base Classes）、3.44%（Novel Classes）、2.72%（Harmonic Mean）** ，在泛化能力与推理效率之间取得更优平衡。

![](https://i-blog.csdnimg.cn/img_convert/20c47c1288ca0dc884855779fa26c141.png)

#### PromptKD: Unsupervised Prompt Distillation for Vision-Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **现有 Prompt Learning 主要关注 Prompt 设计，忽略了 Prompt 作为蒸馏器的潜力**

  * 目前的研究大多集中在 **如何优化 Prompt 本身** ，而**缺乏利用 Prompt 进行知识蒸馏（Knowledge Distillation, KD）** 的方法，使得小模型难以高效继承大模型的知识。
  * 在资源受限的环境下，**小型 VLM 需要高效从大型模型中学习，以提升下游任务适应能力** ，但现有方法很少关注如何在 Prompt 机制下进行模型蒸馏。

🔹 **知识蒸馏通常依赖有标签数据，限制了无监督领域适配能力**

  * 传统蒸馏方法通常需要 **有标签数据** 来指导学生模型的学习，而在许多领域（如医学、遥感）获取高质量标注数据的成本极高。
  * 现有 VLM 适配方法未充分利用 **大量无标签数据** ，导致在特定领域任务上的泛化能力受限。

![](https://i-blog.csdnimg.cn/img_convert/68470a5d3996e42870323b00badb9fd3.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出无监督领域 Prompt 蒸馏（Unsupervised Domain Prompt Distillation），利用 Prompt
进行知识迁移**

  * 采用 **Prompt 作为蒸馏桥梁** ，通过 Prompt Learning 让 **小型 VLM 直接模仿大型 CLIP 教师模型的预测分布** ，提高学生模型的适应能力。
  * **消除对标注数据的依赖** ，仅使用 **无标签领域数据** 进行 Prompt 蒸馏，极大提升领域适配性。

✅ **采用两阶段蒸馏框架，高效传递教师模型的知识**

  * **第一阶段** ：对 **大型 CLIP 教师模型** 进行少量标注数据预训练，并**预存教师模型文本编码器生成的类别向量** ，减少推理阶段的计算开销。
  * **第二阶段** ：在无监督环境下，使用 **KL 散度（KL Divergence）** 使**学生模型的预测概率分布尽可能匹配教师模型** ，从而提升学生模型的泛化能力。

✅ **首创预存文本特征机制，优化 Prompt 蒸馏的计算效率**

  * **仅需计算一次文本特征（Class Vectors）并存储** ，在后续蒸馏过程中，**学生模型可直接使用已存储的类别向量进行学习** ，大幅减少计算成本。
  * 这种方法有效**提升小型 VLM 的适配性，同时降低推理时的资源消耗** ，适用于 **低算力场景** 。

![](https://i-blog.csdnimg.cn/img_convert/96e3c68da7e008c01e468c59adbb7e40.png)

#### **KAPT:** Knowledge-Aware Prompt Tuning for Generalizable Vision-Language
Models

**1️⃣****论文试图解决的核心难点**

🔹 **现有可训练 Prompt 方法容易过拟合已见类别，泛化能力不足**

  * 现有 Prompt Tuning 方法（如 CoOp、CoCoOp）尽管在已见类别上表现良好，但 **在未见类别（Unseen Classes）上的泛化能力较差** ，限制了 Zero-shot 任务的适用性。
  * 这种过拟合现象源于 Prompt 学习时**缺乏外部知识的指导，仅依赖任务内数据** ，导致对新类别的理解能力受限。

🔹 **现有方法缺乏知识引导，未充分利用类别相关信息**

  * 人类在识别新类别时通常会结合**外部知识（如文本描述、先验知识）** ，但当前 Prompt 学习仅利用训练数据，**忽略了类别外部信息的潜在作用** 。
  * 视觉特征与文本特征的匹配未充分利用类别的区分性特征，导致模型在细粒度分类任务上的表现有限。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 KAPT（Knowledge-Aware Prompt Tuning），结合外部知识增强 Prompt 泛化能力**

  * 通过 **引入类别相关的外部知识** ，构建更丰富的文本表征，增强 Prompt 对新类别的适应能力，**减少对已见类别的过拟合** 。

✅ **设计两种互补的知识感知 Prompt，提升文本编码器的知识表达能力**

  * **离散 Prompt（Discrete Prompt）** ：从类别描述文本中提取关键信息，增强模型的类别感知能力。
  * **连续 Prompt（Continuous Prompt）** ：可训练的 Soft Prompt，捕捉全局上下文信息，使 Prompt 具备更好的适配性。

✅ **引入自适应视觉注意力模块，增强视觉特征的判别能力**

  * 在视觉编码器中加入 **自适应特征聚合模块（Adaptation Head）** ，提取关键视觉特征，使视觉表征更加**区分性强且任务相关** ，提高跨类别泛化能力。

✅ **在 Few-shot 任务和未见类别泛化任务上超越 SOTA**

  * 在 **11 个基准数据集** 上进行实验，在 **Few-shot 任务和 Zero-shot 泛化** 任务中优于 CoCoOp，**在新类别任务上提升 3.22%** ，展现更优的跨任务适应能力。

![](https://i-blog.csdnimg.cn/img_convert/e399892defbd9acc99f44cc78ebebeca.png)

#### Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary
Visual Recognition

prompt learning结合clip预训练

![](https://i-blog.csdnimg.cn/img_convert/80555c6b1532d05162087ec00f2c2c10.png)

#### AWT: Transferring Vision-Language Models via Augmentation, Weighting, and
Transportation

**1️⃣****论文试图解决的核心难点**

🔹 **VLM 在适应新概念（New Concept Understanding）时受限于有限类别信息**

  * 由于 **新类别缺乏足够的视觉和文本描述** ，现有 VLM（如 CLIP）在 Zero-shot 任务中的泛化能力受限，难以充分理解和区分新概念。
  * 传统 Prompt Tuning 方法 **未能充分利用外部信息来增强类别表征** ，导致新类别适应性较差。

🔹 **缺乏有效的特征对齐机制，影响视觉-语言模型的跨模态泛化能力**

  * 视觉和语言模态的特征分布可能存在差异，导致 **模型难以在多模态空间中发现语义相关性** ，影响 Zero-shot 和 Few-shot 任务的准确性。

![](https://i-blog.csdnimg.cn/img_convert/9437531a1518b132b342fa8e9fb5f4e6.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 AWT（Augment, Weight, then Transport），增强 VLM 在新概念上的适配能力**

  * **Augment（增强输入）：** 通过 **图像变换（Image Transformations）** 和 **语言模型生成类别描述（LLM-based Class Descriptions）** ，扩充视觉和文本信息，提高新类别的可识别性。

✅ **引入预测熵加权机制（Weighting），动态优化输入贡献**

  * **基于预测熵（Prediction Entropy）对输入进行动态加权** ，赋予更可靠的输入更高权重，从而减少低置信度输入的干扰，提高模型的稳定性。

✅ **采用最优传输（Optimal Transport）方法，优化视觉-语言对齐**

  * 通过 **最优传输（Optimal Transport, OT）** 挖掘 **视觉和文本模态之间的语义关联** ，确保 VLM 在不同模态之间建立更紧密的联系，提高跨模态泛化能力。

✅ **无需额外训练，可无缝集成到不同 VLM 中，提升 Zero-shot 能力**

  * AWT **作为通用框架** ，可直接集成到不同规模和架构的 VLM（如 CLIP），增强其 **Zero-shot 图像分类、Few-shot 识别、视频动作识别和 OOD 泛化能力** ，在多个任务上超越 SOTA 方法。

![](https://i-blog.csdnimg.cn/img_convert/77393cf910972ba67a57049e8d0672a8.png)

#### ProText: Learning to Prompt with Text Only Supervision for Vision-
Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **现有基于视觉信息的 Prompt Tuning 依赖标注数据，泛化能力受限**

  * 许多现有方法通过**视觉信息优化 Prompt** ，但这需要 **大量标注数据** ，在实际应用中不够高效。
  * 由于这些方法主要在 **训练数据集上进行优化** ，容易 **过拟合源数据** ，导致在新数据集上的泛化能力下降。

🔹 **基于 LLM 生成的 Prompt 方式存在高成本问题**

  * 另一类方法使用 **大语言模型（LLMs）** 生成类别描述，并通过 **Prompt Ensembling** 提高泛化能力，但这些**类别特定（Class-specific）的 Prompt 无法直接迁移到其他类别** ，导致每个新类别都需要单独生成描述，**计算成本高** 。

* * *

**2️⃣****论文提出的创新点**

✅ **提出基于文本学习的 Prompt Tuning 方法，无需依赖标注数据**

  * 通过 **仅使用 LLM 生成的文本数据** 进行 Prompt 训练，**避免对图像数据的依赖** ，从而**减少人工标注成本** ，提高泛化能力。
  * 通过新颖的训练方法，使 Prompt 能够**从 LLM 生成的数据中提取丰富的上下文知识** ，提高 Zero-shot 适配能力。

✅ **实现 Prompt 的 Zero-shot 迁移能力，减少 LLM Prompt 生成成本**

  * 由于 Prompt 通过文本数据学习，不依赖特定类别的视觉信息，因此可以 **直接迁移到新类别和新数据集** ，相比基于 LLM 逐类别生成 Prompt 的方法 **减少计算开销** 。

✅ **在多个基准测试中超越现有 Prompt Ensembling 方法，并与监督学习方法竞争**

  * 在 **4 个基准数据集** 上进行测试，**比现有的 LLM-based Prompt Ensembling 方法表现更优** ，且在 **无标注图像的情况下能接近有监督图像 Prompt Tuning 方法的性能** ，展现强大的 Zero-shot 泛化能力。

![](https://i-blog.csdnimg.cn/img_convert/0960ff37563057501c8453ec92eca85e.png)

**1️⃣****论文试图解决的核心难点**

🔹 **现有视觉 Prompt Tuning 方法缺乏对最佳视觉 Prompt 设计的系统分析**

  * 目前在视觉模态（如 CLIP）上的 Prompt Tuning 主要基于 **可学习的视觉 Token 或手工设计的文本 Prompt** ，但对于 **如何选择最佳视觉 Prompt 以提升模型性能** 研究较少。
  * 现有方法 **缺乏对 Prompt 质量的量化分析** ，导致 Prompt 设计往往依赖经验，泛化能力受限。

🔹 **视觉 Prompt Tuning 泛化能力较弱，低于纯文本 Prompt 方法**

  * 现有基于视觉 Prompt 的方法相比 **文本 Prompt（Text-only Prompt）** 泛化能力较差，尤其在 Few-shot 任务上，难以有效适配新类别。
  * 由于视觉模态的信息表达方式不同，现有方法难以让视觉 Prompt 具备和文本 Prompt 相同的可迁移性。

![](https://i-blog.csdnimg.cn/img_convert/57a1e72f463a80b303cae5e6435374d1.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 LoGoPrompt，使用合成文本图像（Synthetic Text Images）作为视觉 Prompt**

  * 发现**合成的文本图像（Synthetic Text Images）可以成为有效的视觉 Prompt** ，让 VLM 以图像形式理解类别信息，**提高模型的分类能力** 。
  * 通过在图像输入中添加合成文本图像，使 CLIP **直接匹配视觉类别信息** ，提高分类准确性。

✅ **提出新的 Prompt 选择策略，避免先有 Prompt 还是先分类的“鸡生蛋”问题**

  * 通过**重新定义分类目标，使其转换为最佳视觉 Prompt 的选择问题** ，从而解决 **先选择 Prompt 还是先分类的逻辑悖论** 。

✅ **在 Zero-shot 和 Few-shot 任务上超越现有 SOTA 方法**

  * 在 **16 个数据集上进行实验** ，**无需训练任何视觉 Prompt 参数** ，仍能**超越当前 Few-shot 视觉 Prompt 方法** ，证明 LoGoPrompt 作为视觉 Prompt 的有效性和泛化能力。

![](https://i-blog.csdnimg.cn/img_convert/6d535b752c6c5aef4d40e4f8b0eb257d.png)

#### TPT: Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-
Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **现有 Prompt Tuning 依赖下游任务数据，降低模型泛化能力**

  * 现有方法通常使用 **下游任务的标注数据** 来训练可学习 Prompt，尽管这提高了特定任务的性能，但会**降低模型在新领域和未见类别上的泛化能力** 。
  * 在 **跨数据集或自然分布变化（Natural Distribution Shift）** 场景中，这种依赖任务数据的 Prompt Tuning 方法表现不稳定，影响 Zero-shot 适应能力。

🔹 **现有方法无法在测试时动态调整 Prompt，提高适应性**

  * 传统 Prompt 学习方法 **在训练阶段固定 Prompt** ，导致测试时**无法根据新数据自适应调整** ，难以适应 **未见类别或不同数据分布** 。
  * 目前缺乏 **无需额外训练数据的 Prompt 适配方案** ，使得模型在 Zero-shot 任务中的泛化能力受限。

* * *

**2️⃣****论文提出的创新点**

✅ **提出 Test-time Prompt Tuning (TPT)，在测试时动态调整 Prompt，提高泛化能力**

  * 在 **测试阶段** 直接优化 Prompt，而无需额外的训练数据，使得 Prompt **能够在推理时根据新样本自适应调整** ，提升 Zero-shot 任务的稳定性。

✅ **利用熵最小化（Entropy Minimization）和置信度选择优化 Prompt**

  * 通过 **最小化测试样本的预测熵** ，确保模型在不同增强视角下的预测结果一致，从而动态调整 Prompt，使其适应不同数据分布。
  * 结合 **置信度选择（Confidence Selection）** ，确保优化后的 Prompt 能够提高模型的稳定性和准确性。

✅ **在 Zero-shot 任务中超越现有 Prompt Tuning 方法，提升跨数据集泛化能力**

  * **在自然分布变化任务上，Zero-shot Top-1 准确率提升 3.6%** ，超越所有需要额外训练数据的 Prompt 方法。
  * **在未见类别的跨数据集泛化任务上，TPT 的表现与当前 SOTA 方法相当** ，但不需要额外的训练数据，展现更高的适配性和部署优势。

![](https://i-blog.csdnimg.cn/img_convert/fc16a0b5c6d9c2a3d7b71c37909d24d0.png)

#### Unsupervised Prompt Learning for Vision-Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **现有 Prompt Tuning 方法依赖目标数据集的标注数据，限制了扩展性**

  * 方法如 CoOp、CLIP-Adapter、Tip-Adapter 需要 **少量标注数据（Few-shot Learning）** 来学习优化的 Prompt，但在 **无标签（Unlabeled）环境下无法使用** ，影响其在大规模数据上的适用性。
  * 依赖标注数据使这些方法在 **Zero-shot 任务和跨数据集泛化（Transfer Learning）** 方面受限，降低了 VLM（如 CLIP）的适配能力。

🔹 **缺乏无监督 Prompt 学习（Unsupervised Prompt Learning）的方法**

  * 目前的 Prompt Learning 方法大多采用 **监督学习（Supervised Learning）** ，**未能探索如何在无监督场景下优化 Prompt** ，以充分释放 CLIP 的 Zero-shot 能力。
  * 现有 Zero-shot 方法依赖 **手工 Prompt 设计（Prompt Engineering）** ，**无法自动适配不同数据分布** ，需要大量人工干预。

![](https://i-blog.csdnimg.cn/img_convert/d78b71ef94eddb4ab9d3cdb685a2dd54.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 Unsupervised Prompt Learning (UPL)，在无标签数据上优化 Prompt**

  * **UPL 是首个引入无监督学习（Unsupervised Learning）到 Prompt Tuning 的方法** ，无需目标数据集的标注数据即可提升 CLIP 的迁移性能。
  * 通过无监督优化策略，使得 CLIP **能够自动学习最优 Prompt，而不依赖手工设计** ，减少人工干预。

✅ **避免 Prompt Engineering，同时提升 CLIP 的 Zero-shot 泛化能力**

  * 通过 **优化 Prompt 使其适应不同数据分布** ，UPL **在 Zero-shot 任务中超越 CLIP+手工 Prompt** ，提高跨数据集适配能力。
  * **在 ImageNet 及 10 个其他数据集上均超过 CLIP 的手工 Prompt 结果** ，表明 UPL 能够有效提升 Zero-shot 识别能力。

✅ **UPL 在无监督环境下的表现接近 Few-shot Prompt Tuning 方法**

  * **UPL 的增强版本在多数数据集上可匹敌 8-shot CoOp 和 8-shot TIP-Adapter** ，表明即使 **没有任何标注数据** ，UPL 仍能在 Few-shot 级别的任务中达到接近 SOTA 的效果，具有更高的扩展性和实用价值。

![](https://i-blog.csdnimg.cn/img_convert/b3e3d3e2ea38972c28f35eecc05418ff.png)

#### Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-
Language Models

**1️⃣****论文试图解决的核心难点**

🔹 **现有高效迁移学习（ETL）方法未考虑不同任务的迁移难度差异**

  * 目前的 Prompt Learning 和 Adapter 方法在迁移 VLM（如 CLIP）到下游任务时，**未针对不同任务的难度进行优化** ，导致方法在某些任务上的适应能力受限。
  * **高迁移难度任务（如细粒度分类、跨领域任务）** 需要更复杂的调整，而 **低迁移难度任务** 则可以直接利用预训练模型的知识，但现有方法无法动态适配。

🔹 **缺乏优化适应不同迁移难度的 Prompt 和 Adapter 组合策略**

  * 现有方法通常采用 **单一方式（仅使用 Prompt 或 Adapter）** 进行 VLM 适配，而没有考虑**如何在不同难度任务之间合理分配 Prompt 和 Adapter 资源** ，导致泛化能力受限。
  * 在低难度任务上，过度依赖任务特定适配可能导致**信息损失** ，而在高难度任务上，单独使用 Prompt 或 Adapter 可能**不足以提升泛化能力** 。

* * *

**2️⃣****论文提出的创新点**

✅ **提出自适应集成方法（Adaptive Ensemble），根据迁移难度优化 Prompt 和 Adapter 组合策略**

  * **在高迁移难度任务中** ，强调 **视觉 Prompt（Vision Prompt）+ 文本 Adapter（Text Adapter）** 的协同作用，提高 VLM 的适配能力。
  * **在低迁移难度任务中** ，更倾向于利用 **原始预训练模型的通用知识** ，减少不必要的适配，保留模型原生的泛化能力。

✅ **采用动态融合策略，自适应调整任务特定知识与通用知识的比例**

  * 通过 **自适应集成（Adaptive Ensemble）** 机制，在不同任务上动态调整 **任务适配模型（Task-adapted VLMs）** 和 **原始 VLMs** 之间的融合权重。
  * **在低难度任务上更多依赖 VLM 原生能力，在高难度任务上强化任务特定适配** ，确保模型在不同任务间的最优表现。

✅ **在多个基准数据集上超越现有方法，尤其在未见任务（Unseen Tasks）上提升显著**

  * 该方法在 **Zero-shot 任务和跨数据集迁移任务上均超过所有基线方法** ，尤其在**高迁移难度任务上展现出更优的泛化能力** ，证明其在不同任务中的适应性和有效性。

![](https://i-blog.csdnimg.cn/img_convert/b17d5ac11d66fa081947a321cc31098b.png)

#### Waffling around for Performance: Visual Classification with Random Words
and Broad Concepts

**1️⃣****论文试图解决的核心难点**

🔹 **现有 VLM（如 CLIP）结合 LLM 生成的类别描述提升分类性能，但计算成本高**

  * 研究表明，使用 LLM（如 GPT-3）生成类别描述（如 _“waffle, which has a round shape”_ ）可以提高 CLIP 在 Zero-shot 视觉分类任务上的泛化能力。
  * 但 **依赖 LLM 生成文本的方式计算成本高，需要额外的 API 调用** ，且 LLM 生成的描述未必始终可靠。

🔹 **缺乏对 LLM 生成的语义描述影响的深入分析**

  * 目前的研究主要关注 LLM 生成的类别描述对视觉分类性能的提升效果，但**尚未系统性分析 LLM 生成的语义信息是否真正有效** ，或者是否可以用更简单的方法替代。

![](https://i-blog.csdnimg.cn/img_convert/d02adf2c254b68c90f7ccadeefa53379.png)

* * *

**2️⃣****论文提出的创新点**

✅ **提出 WaffleCLIP，用随机字符和单词替代 LLM 生成的类别描述，达到类似提升效果**

  * **无需调用外部 LLM** ，仅使用**随机字符或无关词汇** 作为类别描述，即可在 Zero-shot 视觉分类任务中获得与 LLM 生成描述相近的性能提升。
  * 证明 **LLM 生成的类别描述可能并非真正带来了额外的语义信息，而可能仅仅是增加了类别的文本信息量** 。

✅ **对 LLM 生成的类别描述进行系统性分析，揭示其局限性**

  * 通过实验分析，发现 LLM 生成的类别描述 **可能并未有效利用高层语义** ，部分情况下仅相当于一种**数据增强** 。
  * 进一步研究 **在类别名称存在歧义时，如何更有效地利用 LLM 提供的高层语义** ，如使用 LLM 来生成更具区分度的概念描述。

✅ **提供低成本替代方案，并作为未来 VLM + LLM 研究的基准**

  * WaffleCLIP 既是一个 **低成本、无需 LLM 依赖的 Prompt 扩展方法** ，也可以作为 **评估未来 LLM 扩展 VLM 方法有效性的基准** ，防止不必要的 LLM 计算开销。

![](https://i-blog.csdnimg.cn/img_convert/53853a0ab05339fd83ed14f1f97ea56a.png)

#### Weak Distribution Detectors Lead to Stronger Generalizability of Vision-
Language Prompt Tuning

**1️⃣****论文试图解决的核心难点**

🔹 **现有 VLM（如 CLIP）在 Few-shot Fine-tuning 过程中难以兼顾已见类别（Base Classes）和新类别（Novel
Classes）的泛化能力**

  * 传统 Few-shot Fine-tuning 方法（如 CoOp、ProGrad）**在训练集类别（Base Classes）上表现良好，但对未见类别（Novel Classes）泛化能力有限** ，即 **Base-to-Novel Generalization（BNT）问题** 。
  * 由于 Fine-tuning 过程中模型的表示会逐渐偏向 Base 类别，导致模型在 Zero-shot 任务中难以有效利用预训练知识。

🔹 **缺乏有效的方法来动态调整 Zero-shot 预测与 Fine-tuned 预测的权重**

  * 现有方法往往**固定使用 Fine-tuned 分类器** ，而没有在测试时考虑**当前样本是否更接近预训练分布（Zero-shot）或下游任务分布（Fine-tuned）** ，导致在新类别上的识别能力不足。

* * *

**2️⃣****论文提出的创新点**

✅ **基于 OOD 检测（Out-of-Distribution Detection）判断样本是否属于 Base 类别或 Novel 类别**

  * 在测试阶段，引入 OOD 检测方法，**预测当前样本是来自预训练数据分布（Base）还是新的数据分布（Novel）** ，从而动态调整分类器的使用方式。

✅ **提出基于竞争得分的动态分类器融合策略，提高 Base-to-Novel 泛化能力**

  * **竞争得分（Competition-based Scoring Function）** 计算 **Zero-shot 预测结果与 Fine-tuned 预测结果的偏向性** ，并利用该得分进行加权融合：

  *     * 若样本更可能属于预训练分布（Base Classes），则更偏向 **Zero-shot 分类器** 。
    * 若样本可能属于 Novel 类别，则更偏向 **Fine-tuned 分类器** 。

  * 这种方法**在不影响 Base 类别性能的前提下，提高了新类别的泛化能力** 。

✅ **方法仅在测试阶段执行，无需额外训练，可直接提升现有 Few-shot Fine-tuning 方法的泛化能力**

  * **无需重新训练模型** ，可直接应用于现有 Fine-tuning 方法，如 **CoOp 和 ProGrad** ，在 11 个数据集上 **提升 CoOp 2.6% 和 ProGrad 1.5% 的 Harmonic Mean 指标** ，证明其在 Base-to-Novel 泛化任务上的有效性。

![](https://i-blog.csdnimg.cn/img_convert/f88ceba4162c2ca4c163acd3d82004ac.png)

[参考仓库：GitHub - zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs: A curated
list of awesome prompt/adapter learning methods for vision-language models
like CLIP.](https://github.com/zhengli97/Awesome-Prompt-Adapter-Learning-for-
VLMs "参考仓库：GitHub - zhengli97/Awesome-Prompt-Adapter-Learning-for-VLMs: A
curated list of awesome prompt/adapter learning methods for vision-language
models like CLIP.")



