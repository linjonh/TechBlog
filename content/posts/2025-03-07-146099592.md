---
layout: post
title: "mindie运行Qwen2.5-7B-Instruct正常,量化版Qwen2.5-72B-Instruct-GPTQ-Int4报错"
date: 2025-03-07 16:46:37 +0800
description: "(Python310) root@huawei:/usr/local/Ascend/atb-models# torchrun --nproc_per_node 8 --master_port 20030 -m examples.run_pa --model_path \"/models/Qwen/Qwen2___5-72B-Instruct-GPTQ-Int4\" --input_texts \"你好，请介绍一下武汉\" --max_batch_size 8。"
keywords: "mindie运行Qwen2.5-7B-Instruct正常，量化版Qwen2.5-72B-Instruct-GPTQ-Int4报错"
categories: ['未分类']
tags: ['Mindie']
artid: "146099592"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146099592
    alt: "mindie运行Qwen2.5-7B-Instruct正常,量化版Qwen2.5-72B-Instruct-GPTQ-Int4报错"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146099592
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146099592
cover: https://bing.ee123.net/img/rand?artid=146099592
image: https://bing.ee123.net/img/rand?artid=146099592
img: https://bing.ee123.net/img/rand?artid=146099592
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     mindie运行Qwen2.5-7B-Instruct正常，量化版Qwen2.5-72B-Instruct-GPTQ-Int4报错
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     (base) root@huawei:/disk1/models# pwd
     <br/>
     /disk1/models
     <br/>
     (base) root@huawei:/disk1/models# cat /etc/issue
     <br/>
     Ubuntu 20.04 LTS \n \l
    </p>
    <p>
     (base) root@huawei:/disk1/models# free -g
     <br/>
     total        used        free      shared  buff/cache   available
     <br/>
     Mem:            754           8         389           0         356         741
     <br/>
     Swap:             7           0           7
     <br/>
     (base) root@huawei:/disk1/models# lscpu|grep CPU
     <br/>
     CPU op-mode(s):                  64-bit
     <br/>
     CPU(s):                          192
     <br/>
     On-line CPU(s) list:             0-191
     <br/>
     NUMA node0 CPU(s):               0-23
     <br/>
     NUMA node1 CPU(s):               24-47
     <br/>
     NUMA node2 CPU(s):               48-71
     <br/>
     NUMA node3 CPU(s):               72-95
     <br/>
     NUMA node4 CPU(s):               96-119
     <br/>
     NUMA node5 CPU(s):               120-143
     <br/>
     NUMA node6 CPU(s):               144-167
     <br/>
     NUMA node7 CPU(s):               168-191
     <br/>
     (base) root@huawei:/disk1/models# df -hT
     <br/>
     Filesystem                Type      Size  Used Avail Use% Mounted on
     <br/>
     udev                      devtmpfs  377G     0  377G   0% /dev
     <br/>
     tmpfs                     tmpfs      76G  4.6M   76G   1% /run
     <br/>
     /dev/sda2                 ext4      439G  159G  258G  39% /
     <br/>
     tmpfs                     tmpfs     378G  4.3M  378G   1% /dev/shm
     <br/>
     tmpfs                     tmpfs     5.0M     0  5.0M   0% /run/lock
     <br/>
     tmpfs                     tmpfs     378G     0  378G   0% /sys/fs/cgroup
     <br/>
     /dev/sda1                 vfat      511M  3.5M  508M   1% /boot/efi
     <br/>
     /dev/loop7                squashfs   49M   49M     0 100% /snap/core18/2848
     <br/>
     /dev/loop0                squashfs   69M   69M     0 100% /snap/core22/1720
     <br/>
     /dev/loop6                squashfs  100M  100M     0 100% /snap/lxd/31572
     <br/>
     /dev/loop2                squashfs  101M  101M     0 100% /snap/lxd/31822
     <br/>
     /dev/loop3                squashfs   39M   39M     0 100% /snap/snapd/23546
     <br/>
     /dev/loop4                squashfs   69M   69M     0 100% /snap/core22/1752
     <br/>
     /dev/loop5                squashfs   49M   49M     0 100% /snap/core18/2857
     <br/>
     overlay                   overlay   439G  159G  258G  39% /var/lib/docker/overlay2/3fb838ad167298740a56ca0038f073f7e3a212a7b4d5e7f295b85bd7130428aa/merged
     <br/>
     /dev/loop1                squashfs   39M   39M     0 100% /snap/snapd/23772
     <br/>
     /dev/mapper/testvg-testlv ext4      1.5T  226G  1.2T  17% /disk1
     <br/>
     overlay                   overlay   439G  159G  258G  39% /var/lib/docker/overlay2/27007413f47cdafb51bbef36aa09298d95f6f9870d2ba16f3f74dfcbf1d7f5a9/merged
     <br/>
     tmpfs                     tmpfs      76G     0   76G   0% /run/user/0
     <br/>
     (base) root@huawei:/disk1/models# npu-smi info
     <br/>
     +------------------------------------------------------------------------------------------------+
     <br/>
     | npu-smi 23.0.0                   Version: 23.0.0                                               |
     <br/>
     +---------------------------+---------------+----------------------------------------------------+
     <br/>
     | NPU   Name                | Health        | Power(W)    Temp(C)           Hugepages-Usage(page)|
     <br/>
     | Chip                      | Bus-Id        | AICore(%)   Memory-Usage(MB)  HBM-Usage(MB)        |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 0     910PremiumA         | OK            | 98.6        75                0    / 0             |
     <br/>
     | 0                         | 0000:C1:00.0  | 0           1225 / 13553      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 1     910PremiumA         | OK            | 102.6       75                0    / 0             |
     <br/>
     | 0                         | 0000:81:00.0  | 0           1973 / 15665      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 2     910PremiumA         | OK            | 102.4       75                0    / 0             |
     <br/>
     | 0                         | 0000:41:00.0  | 0           2237 / 15665      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 3     910PremiumA         | OK            | 100.0       75                0    / 0             |
     <br/>
     | 0                         | 0000:01:00.0  | 0           2944 / 15567      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 4     910PremiumA         | OK            | 100.4       74                0    / 0             |
     <br/>
     | 0                         | 0000:C2:00.0  | 0           1415 / 13553      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 5     910PremiumA         | OK            | 104.7       75                0    / 0             |
     <br/>
     | 0                         | 0000:82:00.0  | 0           1708 / 15665      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 6     910PremiumA         | OK            | 101.1       75                0    / 0             |
     <br/>
     | 0                         | 0000:42:00.0  | 0           2342 / 15665      0    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | 7     910PremiumA         | OK            | 99.3        75                0    / 0             |
     <br/>
     | 0                         | 0000:02:00.0  | 0           2898 / 15567      1    / 32768         |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     +---------------------------+---------------+----------------------------------------------------+
     <br/>
     | NPU     Chip              | Process id    | Process name             | Process memory(MB)      |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 0                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 1                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 2                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 3                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 4                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 5                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 6                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     | No running processes found in NPU 7                                                            |
     <br/>
     +===========================+===============+====================================================+
     <br/>
     <br/>
     (base) root@huawei:/disk1/models# ll /disk1/models
     <br/>
     total 220140
     <br/>
     drwxrwxrwx 5 root root     4096 Mar  7 07:37 ./
     <br/>
     drwxr-xr-x 4 root root     4096 Mar  7 06:11 ../
     <br/>
     -rw-r--r-- 1 root root  4807602 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py310_torch2.1.0-abi0.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4944832 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py310_torch2.1.0-abi1.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4813371 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py310_torch2.3.1-abi0.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4734426 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py310_torch2.3.1-abi1.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4808762 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py311_torch2.1.0-abi0.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4945450 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py311_torch2.1.0-abi1.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4813791 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py311_torch2.3.1-abi0.tar.gz
     <br/>
     -rw-r--r-- 1 root root  4734373 Mar  7 01:59 Ascend-mindie-atb-models_1.0.0_linux-aarch64_py311_torch2.3.1-abi1.tar.gz
     <br/>
     drwxrwxrwx 3 root root     4096 Mar  6 00:56 deepseek-ai/
     <br/>
     -rw------- 1 root root      368 Mar  7 07:36 .msc
     <br/>
     drwxrwxrwx 7 root root     4096 Mar  7 07:38 Qwen/
     <br/>
     drwxrwxrwx 4 root root     4096 Mar  7 07:36 ._____temp/
     <br/>
     -rw-r--r-- 1 root root 84138364 Oct  6  2023 torch-2.1.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
     <br/>
     -rw-r--r-- 1 root root 89791945 Jul 24  2024 torch-2.4.0-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
     <br/>
     -rw-r--r-- 1 root root 12845038 Mar  7 01:30 torch_npu-2.4.0.post2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl
     <br/>
     (base) root@huawei:/disk1/models#
     <br/>
    </p>
    <p>
     运行容器：
     <br/>
     docker run -it -d --name mindie-910a-t71 --ipc=host --net=host --shm-size=200g \
     <br/>
     --device=/dev/davinci0 \
     <br/>
     --device=/dev/davinci1 \
     <br/>
     --device=/dev/davinci2 \
     <br/>
     --device=/dev/davinci3 \
     <br/>
     --device=/dev/davinci4 \
     <br/>
     --device=/dev/davinci5 \
     <br/>
     --device=/dev/davinci6 \
     <br/>
     --device=/dev/davinci7 \
     <br/>
     --device=/dev/davinci_manager \
     <br/>
     --device=/dev/hisi_hdc \
     <br/>
     --device=/dev/devmm_svm \
     <br/>
     --entrypoint=bash \
     <br/>
     -w /usr/local/Ascend/mindie/latest/mindie-llm/logs \
     <br/>
     -v /usr/local/dcmi:/usr/local/dcmi \
     <br/>
     -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
     <br/>
     -v /usr/local/sbin:/usr/local/sbin \
     <br/>
     -v /usr/local/Ascend/driver/lib64/common:/usr/local/Ascend/driver/lib64/common \
     <br/>
     -v /usr/local/Ascend/driver/lib64/driver:/usr/local/Ascend/driver/lib64/driver \
     <br/>
     -v /etc/hccn.conf:/etc/hccn.conf \
     <br/>
     -v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \
     <br/>
     -v /etc/ascend_install.info:/etc/ascend_install.info \
     <br/>
     -v /etc/vnpu.cfg:/etc/vnpu.cfg \
     <br/>
     -v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \
     <br/>
     -v /disk1/models:/models \
     <br/>
     swr.cn-central-221.ovaijisuan.com/wh-aicc-fae/mindie:910A-ascend_24.1.rc3-cann_8.0.t63-py_3.10-ubuntu_20.04-aarch64-mindie_1.0.T71.02
     <br/>
     进入容器测试
     <br/>
     docker exec -it mindie-910a-t71 bash
    </p>
    <p>
     另外一个模型也可运行：
    </p>
    <pre><code>docker run -it -d --name mindie-910a-t65 --ipc=host --net=host --shm-size=200g \
--device=/dev/davinci0 \
--device=/dev/davinci1 \
--device=/dev/davinci2 \
--device=/dev/davinci3 \
--device=/dev/davinci4 \
--device=/dev/davinci5 \
--device=/dev/davinci6 \
--device=/dev/davinci7 \
--device=/dev/davinci_manager \
--device=/dev/hisi_hdc \
--device=/dev/devmm_svm \
--entrypoint=bash \
-w /usr/local/Ascend/mindie/latest/mindie-llm/logs \
-v /usr/local/dcmi:/usr/local/dcmi \
-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
-v /usr/local/sbin:/usr/local/sbin \
-v /usr/local/Ascend/driver/lib64/common:/usr/local/Ascend/driver/lib64/common \
-v /usr/local/Ascend/driver/lib64/driver:/usr/local/Ascend/driver/lib64/driver \
-v /etc/hccn.conf:/etc/hccn.conf \
-v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \
-v /etc/ascend_install.info:/etc/ascend_install.info \
-v /etc/vnpu.cfg:/etc/vnpu.cfg \
-v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \
-v /disk1/models:/models \
swr.cn-central-221.ovaijisuan.com/wh-aicc-fae/mindie:910a-ascend_23.0.0-cann_8.0.rc3-py_3.10-ubuntu_22.04-aarch64-mindie_1.0.t65

docker exec -it mindie-910a-t65 bash

torchrun --nproc_per_node 2 --master_port 20030 -m examples.run_pa --model_path /models/Qwen/Qwen2___5-7B-Instruct --input_texts "你好，请介绍一下武汉" --max_batch_size 2
</code></pre>
    <p>
     <img alt="" height="512" src="https://i-blog.csdnimg.cn/direct/4f66e20802ed498cb3e7ea5c43a9725e.png" width="1767"/>
    </p>
    <p>
     <img alt="" height="433" src="https://i-blog.csdnimg.cn/direct/68c4affaba8c47e7b3af3aba4f8e1e91.png" width="1023"/>
    </p>
    <p>
     测试结果：
     <br/>
     1.运行Qwen2.5-7B-Instruct正常：
    </p>
    <p>
     (Python310) root@huawei:/usr/local/Ascend/atb-models# torchrun --nproc_per_node 2 --master_port 20030 -m examples.run_pa --model_path /models/Qwen/Qwen2___5-7B-Instruct --input_texts "你好，请介绍一下武汉" --max_batch_size 2
     <br/>
     [2025-03-07 16:32:36,351] torch.distributed.run: [WARNING]
     <br/>
     [2025-03-07 16:32:36,351] torch.distributed.run: [WARNING] *****************************************
     <br/>
     [2025-03-07 16:32:36,351] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
     <br/>
     [2025-03-07 16:32:36,351] torch.distributed.run: [WARNING] *****************************************
     <br/>
     [2025-03-07 16:32:46,307] [22204] [281473125748752] [llm] [INFO][logging.py-227] : Skip binding cpu.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     [2025-03-07 16:32:46,855] [22204] [281473125748752] [llm] [INFO][logging.py-227] : model_runner.quantize: None, model_runner.kv_quant_type: None, model_runner.fa_quant_type: None, model_runner.dtype: torch.float16
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     [2025-03-07 16:32:54,824] [22204] [281473125748752] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:32:54,826] [22204] [281473125748752] [llm] [INFO][logging.py-227] : init tokenizer done: Qwen2TokenizerFast(name_or_path='/models/Qwen/Qwen2___5-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '&lt;|im_end|&gt;', 'pad_token': '&lt;|endoftext|&gt;', 'additional_special_tokens': ['&lt;|im_start|&gt;', '&lt;|im_end|&gt;', '&lt;|object_ref_start|&gt;', '&lt;|object_ref_end|&gt;', '&lt;|box_start|&gt;', '&lt;|box_end|&gt;', '&lt;|quad_start|&gt;', '&lt;|quad_end|&gt;', '&lt;|vision_start|&gt;', '&lt;|vision_end|&gt;', '&lt;|vision_pad|&gt;', '&lt;|image_pad|&gt;', '&lt;|video_pad|&gt;']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
     <!-- -->
     <br/>
     151643: AddedToken("&lt;|endoftext|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151644: AddedToken("&lt;|im_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151645: AddedToken("&lt;|im_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151646: AddedToken("&lt;|object_ref_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151647: AddedToken("&lt;|object_ref_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151648: AddedToken("&lt;|box_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151649: AddedToken("&lt;|box_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151650: AddedToken("&lt;|quad_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151651: AddedToken("&lt;|quad_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151652: AddedToken("&lt;|vision_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151653: AddedToken("&lt;|vision_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151654: AddedToken("&lt;|vision_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151655: AddedToken("&lt;|image_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151656: AddedToken("&lt;|video_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151657: AddedToken("&lt;tool_call&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151658: AddedToken("&lt;/tool_call&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151659: AddedToken("&lt;|fim_prefix|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151660: AddedToken("&lt;|fim_middle|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151661: AddedToken("&lt;|fim_suffix|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151662: AddedToken("&lt;|fim_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151663: AddedToken("&lt;|repo_name|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151664: AddedToken("&lt;|file_sep|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     }
     <br/>
     [2025-03-07 16:32:54,834] [22204] [281473125748752] [llm] [INFO][logging.py-227] : NPUSocInfo(soc_name='', soc_version=100, need_nz=True, matmul_nd_nz=False)
     <br/>
     [2025-03-07 16:32:55,027] [22204] [281473125748752] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:32:55,130] [22205] [281472994160656] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:32:55,324] [22205] [281472994160656] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:33:33,519] [22205] [281472994160656] [llm] [INFO][cache.py-98] : kv cache will allocate 0.0615234375GB memory
     <br/>
     [2025-03-07 16:33:34,434] [22205] [281472994160656] [llm] [INFO][flash_causal_qwen2.py-435] : &lt;&lt;&lt;&lt;&lt;&lt;&lt;after transdata k_caches[0].shape=torch.Size([18, 16, 128, 16])
     <br/>
     [2025-03-07 16:33:41,789] [22204] [281473125748752] [llm] [INFO][logging.py-227] : model:
     <br/>
     FlashQwen2ForCausalLM(
     <br/>
     (rotary_embedding): PositionRotaryEmbedding()
     <br/>
     (attn_mask): AttentionMask()
     <br/>
     (transformer): FlashQwenModel(
     <br/>
     (wte): TensorParallelEmbedding()
     <br/>
     (h): ModuleList(
     <br/>
     (0-27): 28 x FlashQwenLayer(
     <br/>
     (attn): FlashQwenAttention(
     <br/>
     (rotary_emb): PositionRotaryEmbedding()
     <br/>
     (c_attn): TensorParallelColumnLinear(
     <br/>
     (linear): FastLinear()
     <br/>
     )
     <br/>
     (c_proj): TensorParallelRowLinear(
     <br/>
     (linear): FastLinear()
     <br/>
     )
     <br/>
     )
     <br/>
     (mlp): QwenMLP(
     <br/>
     (act): SiLU()
     <br/>
     (w2_w1): TensorParallelColumnLinear(
     <br/>
     (linear): FastLinear()
     <br/>
     )
     <br/>
     (c_proj): TensorParallelRowLinear(
     <br/>
     (linear): FastLinear()
     <br/>
     )
     <br/>
     )
     <br/>
     (ln_1): QwenRMSNorm()
     <br/>
     (ln_2): QwenRMSNorm()
     <br/>
     )
     <br/>
     )
     <br/>
     (ln_f): QwenRMSNorm()
     <br/>
     )
     <br/>
     (lm_head): TensorParallelHead(
     <br/>
     (linear): FastLinear()
     <br/>
     )
     <br/>
     )
     <br/>
     [2025-03-07 16:33:43,496] [22204] [281473125748752] [llm] [INFO][logging.py-227] : hbm_capacity(GB): 13.2353515625, init_memory(GB): 1.323535155504942
     <br/>
     [2025-03-07 16:33:43,496] [22204] [281473125748752] [llm] [INFO][logging.py-227] : pa_runner: PARunner(model_path=/models/Qwen/Qwen2___5-7B-Instruct, input_text=None, max_position_embeddings=None, max_input_length=1024, max_output_length=20, max_prefill_tokens=-1, load_tokenizer=True, enable_atb_torch=False, max_prefill_batch_size=None, max_batch_size=2, dtype=torch.float16, block_size=128, model_config=ModelConfig(num_heads=14, num_kv_heads=2, num_kv_heads_origin=4, head_size=128, k_head_size=128, v_head_size=128, num_layers=28, device=npu:0, dtype=torch.float16, soc_info=NPUSocInfo(soc_name='', soc_version=100, need_nz=True, matmul_nd_nz=False), kv_quant_type=None, fa_quant_type=None, mapping=Mapping(world_size=2, rank=0, pp_rank=0, pp_groups=[[0], [1]], micro_batch_size=2) ), cla_share_factor=1, , max_memory=14211350528,
     <br/>
     [2025-03-07 16:33:43,497] [22204] [281473125748752] [llm] [INFO][logging.py-227] : ---------------begin warm_up---------------
     <br/>
     [2025-03-07 16:33:43,497] [22204] [281473125748752] [llm] [INFO][cache.py-98] : kv cache will allocate 0.0615234375GB memory
     <br/>
     [2025-03-07 16:33:43,499] [22204] [281473125748752] [llm] [INFO][logging.py-227] : ------total req num: 2, infer start--------
     <br/>
     [2025-03-07 16:33:43,504] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,505] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,505] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,505] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,506] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,506] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,507] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,507] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,508] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,508] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,508] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,508] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,509] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,509] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,509] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,510] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,510] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,510] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,511] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,511] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,511] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,512] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,512] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,512] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,513] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,513] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,513] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,514] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,514] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,514] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,514] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,515] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,515] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,515] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,516] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,516] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,516] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,517] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,517] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,517] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,518] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,518] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,518] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,519] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,519] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,519] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,520] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,520] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,520] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,521] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,521] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,521] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,522] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,522] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,522] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,522] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,523] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,523] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,523] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,524] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,524] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,524] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,525] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,525] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,525] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,526] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,526] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,526] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,527] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,527] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,527] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,528] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,528] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,528] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,529] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,529] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,529] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,530] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,530] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,530] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,531] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,531] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,531] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,531] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,532] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,532] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,532] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,533] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,533] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,533] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,534] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,534] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,534] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,535] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,535] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,535] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,536] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,536] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,536] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,536] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,537] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,537] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,537] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,538] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,538] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,538] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,539] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,539] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,539] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,540] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,540] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,540] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:43,541] [22204] [281473125748752] [llm] [INFO][logging.py-227] : trans to 29
     <br/>
     [2025-03-07 16:33:44,367] [22204] [281473125748752] [llm] [INFO][logging.py-227] : &lt;&lt;&lt;&lt;&lt;&lt;&lt; ori k_caches[0].shape=torch.Size([18, 16, 128, 16])
     <br/>
     [2025-03-07 16:33:44,376] [22204] [281473125748752] [llm] [INFO][flash_causal_qwen2.py-435] : &lt;&lt;&lt;&lt;&lt;&lt;&lt;after transdata k_caches[0].shape=torch.Size([18, 16, 128, 16])
     <br/>
     [2025-03-07 16:33:44,376] [22204] [281473125748752] [llm] [INFO][logging.py-227] : &gt;&gt;&gt;&gt;&gt;&gt;id of kcache is 281470252742704 id of vcache is 281470252742784
     <br/>
     [2025-03-07 16:33:46,979] [22204] [281473125748752] [llm] [INFO][logging.py-227] : warmup_memory(GB):  1.32
     <br/>
     [2025-03-07 16:33:46,979] [22204] [281473125748752] [llm] [INFO][logging.py-227] : ---------------end warm_up---------------
     <br/>
     [2025-03-07 16:33:46,979] [22204] [281473125748752] [llm] [INFO][logging.py-227] : ---------------begin inference---------------
     <br/>
     [2025-03-07 16:33:47,060] [22204] [281473125748752] [llm] [INFO][logging.py-227] : ------total req num: 2, infer start--------
     <br/>
     [2025-03-07 16:33:48,480] [22204] [281473125748752] [llm] [INFO][logging.py-227] : ---------------end inference---------------
     <br/>
     [2025-03-07 16:33:48,480] [22204] [281473125748752] [llm] [INFO][logging.py-227] : Answer[0]: 大学的历史和特色。
     <br/>
     武汉大学是中国著名的高等学府之一，位于湖北省武汉市，创建于
     <br/>
     [2025-03-07 16:33:48,480] [22204] [281473125748752] [llm] [INFO][logging.py-227] : Generate[0] token num: (0, 20)
     <br/>
     [2025-03-07 16:33:48,480] [22204] [281473125748752] [llm] [INFO][logging.py-227] : Answer[1]: 大学的历史和特色。
     <br/>
     武汉大学是中国著名的高等学府之一，位于湖北省武汉市，创建于
     <br/>
     [2025-03-07 16:33:48,480] [22204] [281473125748752] [llm] [INFO][logging.py-227] : Generate[1] token num: (1, 40)
     <br/>
     (Python310) root@huawei:/usr/local/Ascend/atb-models# ll /models/Qwen/
     <br/>
     total 40
     <br/>
     drwxrwxrwx 7 root root 4096 Mar  7 15:38 ./
     <br/>
     drwxrwxrwx 5 root root 4096 Mar  7 15:37 ../
     <br/>
     drwxr-xr-x 3 root root 4096 Mar  7 15:38 Qwen2.5-72B-Instruct/
     <br/>
     lrwxrwxrwx 1 root root   72 Mar  6 00:57 Qwen2.5-72B-Instruct-GPTQ-Int4 -&gt; /root/.cache/modelscope/hub/models/Qwen/Qwen2___5-72B-Instruct-GPTQ-Int4
     <br/>
     lrwxrwxrwx 1 root root   61 Mar  6 14:19 Qwen2.5-7B-Instruct -&gt; /root/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B-Instruct
     <br/>
     lrwxrwxrwx 1 root root   65 Mar  6 05:42 Qwen2.5-VL-72B-Instruct -&gt; /root/.cache/modelscope/hub/models/Qwen/Qwen2___5-VL-72B-Instruct
     <br/>
     drwxr-xr-x 2 root root 4096 Mar  7 14:04 Qwen2___5-72B-Instruct/
     <br/>
     drwxr-x--- 2 root root 4096 Mar  7 15:28 Qwen2___5-72B-Instruct-GPTQ-Int4/
     <br/>
     drwxr-x--- 2 root root 4096 Mar  6 16:18 Qwen2___5-7B-Instruct/
     <br/>
     drwxr-x--- 2 root root 4096 Mar  6 05:42 Qwen2___5-VL-72B-Instruct/
    </p>
    <p>
     2.运行Qwen2.5-72B-Instruct-GPTQ-Int4报错：
    </p>
    <p>
     <br/>
     (Python310) root@huawei:/usr/local/Ascend/atb-models# torchrun --nproc_per_node 8 --master_port 20030 -m examples.run_pa --model_path "/models/Qwen/Qwen2___5-72B-Instruct-GPTQ-Int4" --input_texts "你好，请介绍一下武汉" --max_batch_size 8
     <br/>
     [2025-03-07 16:36:38,408] torch.distributed.run: [WARNING]
     <br/>
     [2025-03-07 16:36:38,408] torch.distributed.run: [WARNING] *****************************************
     <br/>
     [2025-03-07 16:36:38,408] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
     <br/>
     [2025-03-07 16:36:38,408] torch.distributed.run: [WARNING] *****************************************
     <br/>
     [2025-03-07 16:36:49,200] [24163] [281473876656144] [llm] [INFO][logging.py-227] : Skip binding cpu.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
     <br/>
     [2025-03-07 16:36:49,770] [24163] [281473876656144] [llm] [INFO][logging.py-227] : model_runner.quantize: None, model_runner.kv_quant_type: None, model_runner.fa_quant_type: None, model_runner.dtype: torch.float16
     <br/>
     [2025-03-07 16:36:57,840] [24166] [281473450606608] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:58,085] [24167] [281473718341648] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:58,084] [24170] [281472927670288] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:58,111] [24168] [281473527169040] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:58,285] [24166] [281473450606608] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:36:58,472] [24167] [281473718341648] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:36:58,579] [24170] [281472927670288] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:36:58,598] [24168] [281473527169040] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:36:58,637] [24164] [281473344917520] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:58,698] [24169] [281472867508240] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:58,975] [24163] [281473876656144] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:59,001] [24163] [281473876656144] [llm] [INFO][logging.py-227] : init tokenizer done: Qwen2TokenizerFast(name_or_path='/models/Qwen/Qwen2___5-72B-Instruct-GPTQ-Int4', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '&lt;|im_end|&gt;', 'pad_token': '&lt;|endoftext|&gt;', 'additional_special_tokens': ['&lt;|im_start|&gt;', '&lt;|im_end|&gt;', '&lt;|object_ref_start|&gt;', '&lt;|object_ref_end|&gt;', '&lt;|box_start|&gt;', '&lt;|box_end|&gt;', '&lt;|quad_start|&gt;', '&lt;|quad_end|&gt;', '&lt;|vision_start|&gt;', '&lt;|vision_end|&gt;', '&lt;|vision_pad|&gt;', '&lt;|image_pad|&gt;', '&lt;|video_pad|&gt;']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
     <!-- -->
     <br/>
     151643: AddedToken("&lt;|endoftext|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151644: AddedToken("&lt;|im_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151645: AddedToken("&lt;|im_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151646: AddedToken("&lt;|object_ref_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151647: AddedToken("&lt;|object_ref_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151648: AddedToken("&lt;|box_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151649: AddedToken("&lt;|box_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151650: AddedToken("&lt;|quad_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151651: AddedToken("&lt;|quad_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151652: AddedToken("&lt;|vision_start|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151653: AddedToken("&lt;|vision_end|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151654: AddedToken("&lt;|vision_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151655: AddedToken("&lt;|image_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151656: AddedToken("&lt;|video_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
     <br/>
     151657: AddedToken("&lt;tool_call&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151658: AddedToken("&lt;/tool_call&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151659: AddedToken("&lt;|fim_prefix|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151660: AddedToken("&lt;|fim_middle|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151661: AddedToken("&lt;|fim_suffix|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151662: AddedToken("&lt;|fim_pad|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151663: AddedToken("&lt;|repo_name|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     151664: AddedToken("&lt;|file_sep|&gt;", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
     <br/>
     }
     <br/>
     [2025-03-07 16:36:59,018] [24164] [281473344917520] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     [2025-03-07 16:36:58,930] [24165] [281473872347152] [llm] [INFO][dist.py-81] : initialize_distributed has been Set
     <br/>
     [2025-03-07 16:36:59,066] [24163] [281473876656144] [llm] [INFO][logging.py-227] : NPUSocInfo(soc_name='', soc_version=100, need_nz=True, matmul_nd_nz=False)
     <br/>
     [2025-03-07 16:36:59,212] [24169] [281472867508240] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     [2025-03-07 16:36:59,423] [24163] [281473876656144] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     [2025-03-07 16:36:59,662] [24165] [281473872347152] [llm] [INFO][flash_causal_qwen2.py-122] : &gt;&gt;&gt;&gt; qwen_QwenDecoderModel is called.
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 196, in _run_module_as_main
     <br/>
     return _run_code(code, main_globals, None,
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/runpy.py", line 86, in _run_code
     <br/>
     exec(code, run_globals)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 500, in &lt;module&gt;
     <br/>
     pa_runner = PARunner(**input_dict)
     <br/>
     File "/usr/local/Ascend/atb-models/examples/run_pa.py", line 97, in __init__
     <br/>
     self.model.load_weights(**kw_args)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/runner/model_runner.py", line 161, in load_weights
     <br/>
     self.model = self.model_cls(self.config,
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/flash_causal_qwen2.py", line 32, in __init__
     <br/>
     self.transformer = FlashQwenModel(config, weights, model_prefix=model_prefix, lmhead_prefix=lmhead_prefix)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 407, in __init__
     <br/>
     [
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 408, in &lt;listcomp&gt;
     <br/>
     FlashQwenLayer(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 278, in __init__
     <br/>
     self.attn = FlashQwenAttention(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/models/qwen2/modeling_qwen2.py", line 149, in __init__
     <br/>
     self.c_attn = load_column_multi(
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/layers/__init__.py", line 48, in load_column_multi
     <br/>
     weight = weights.get_multi_weights_col(prefixes, quantize=quantize, dim=0, gqa_size=head_size)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in get_multi_weights_col
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 657, in &lt;listcomp&gt;
     <br/>
     w = [self.get_sharded(f"{p}.weight", dim=dim, gqa_size=gqa_size) for p in prefixes]
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 331, in get_sharded
     <br/>
     slice_ = self._get_slice(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 718, in _get_slice
     <br/>
     filename, tensor_name = self.get_filename(tensor_name)
     <br/>
     File "/usr/local/Ascend/atb-models/atb_llm/utils/weights.py", line 146, in get_filename
     <br/>
     raise AssertionError(f"weight {tensor_name} does not exist")
     <br/>
     AssertionError: weight model.layers.0.self_attn.q_proj.weight does not exist
     <br/>
     [ERROR] 2025-03-07-16:37:05 (PID:24166, Device:3, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:05 (PID:24168, Device:5, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:05 (PID:24170, Device:7, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:05 (PID:24167, Device:4, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:06 (PID:24164, Device:1, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:06 (PID:24163, Device:0, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:06 (PID:24169, Device:6, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [ERROR] 2025-03-07-16:37:07 (PID:24165, Device:2, RankID:-1) ERR99999 UNKNOWN application exception
     <br/>
     [2025-03-07 16:37:13,455] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 24163 closing signal SIGTERM
     <br/>
     [2025-03-07 16:37:13,487] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 24164) of binary: /root/miniconda3/envs/Python310/bin/python
     <br/>
     Traceback (most recent call last):
     <br/>
     File "/root/miniconda3/envs/Python310/bin/torchrun", line 8, in &lt;module&gt;
     <br/>
     sys.exit(main())
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
     <br/>
     return f(*args, **kwargs)
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/site-packages/torch/distributed/run.py", line 806, in main
     <br/>
     run(args)
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/site-packages/torch/distributed/run.py", line 797, in run
     <br/>
     elastic_launch(
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
     <br/>
     return launch_agent(self._config, self._entrypoint, list(args))
     <br/>
     File "/root/miniconda3/envs/Python310/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
     <br/>
     raise ChildFailedError(
     <br/>
     torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
     <br/>
     ============================================================
     <br/>
     examples.run_pa FAILED
     <br/>
     ------------------------------------------------------------
     <br/>
     Failures:
     <br/>
     [1]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 2 (local_rank: 2)
     <br/>
     exitcode  : 1 (pid: 24165)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     [2]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 3 (local_rank: 3)
     <br/>
     exitcode  : 1 (pid: 24166)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     [3]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 4 (local_rank: 4)
     <br/>
     exitcode  : 1 (pid: 24167)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     [4]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 5 (local_rank: 5)
     <br/>
     exitcode  : 1 (pid: 24168)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     [5]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 6 (local_rank: 6)
     <br/>
     exitcode  : 1 (pid: 24169)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     [6]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 7 (local_rank: 7)
     <br/>
     exitcode  : 1 (pid: 24170)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     ------------------------------------------------------------
     <br/>
     Root Cause (first observed failure):
     <br/>
     [0]:
     <br/>
     time      : 2025-03-07_16:37:13
     <br/>
     host      : huawei
     <br/>
     rank      : 1 (local_rank: 1)
     <br/>
     exitcode  : 1 (pid: 24164)
     <br/>
     error_file: &lt;N/A&gt;
     <br/>
     traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
     <br/>
     ============================================================
     <br/>
     (Python310) root@huawei:/usr/local/Ascend/atb-models#
    </p>
    <p>
     执行量化版报错：
    </p>
    <p>
     <img alt="" height="548" src="https://i-blog.csdnimg.cn/direct/8d4cff136fdc4086934863e989f2afd4.png" width="1779"/>
    </p>
    <p>
     <img alt="" height="808" src="https://i-blog.csdnimg.cn/direct/7196c4fa97dd4411822f3bca4f53bafd.png" width="1433"/>
    </p>
    <p>
     <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/163.html" rel="nofollow" title="镜像仓库网">
      镜像仓库网
     </a>
    </p>
    <p>
     <a href="http://mirrors.cn-central-221.ovaijisuan.com/mirrors.html" rel="nofollow" title="镜像仓库网">
      镜像仓库网
     </a>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470:733a2f2f626c6f672e6373646e2e6e65742f6a79636a79632f:61727469636c652f64657461696c732f313436303939353932" class_="artid" style="display:none">
 </p>
</div>


