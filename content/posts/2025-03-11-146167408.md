---
layout: post
title: "深度学习subword分词BPE"
date: 2025-03-11 21:57:40 +0800
description: "BPE：这是一种流行分词算法，可以有效的平衡词汇表大小和步数，分词采用共现性。步骤：1.准备足够大的训练语料2.确定期望的subword词表大小（超参）3.将单词拆分为字符序列并在末尾添加后缀\"</w>\"，这样就可以统计单词频率。比如一开始有一个l字母，现在编程l</w>5，就说明出现了五次。停止符</w>的意义在于表示subword是词后缀。每次合并后词表可能出现三种变化：+1：表明加入合并后的新字词，同时原来在的2个子词还保留；0：如果一个字词不是单独出现的，就被消解；"
keywords: "深度学习subword分词BPE"
categories: ['未分类']
tags: ['深度学习', '人工智能']
artid: "146167408"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146167408
    alt: "深度学习subword分词BPE"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146167408
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146167408
cover: https://bing.ee123.net/img/rand?artid=146167408
image: https://bing.ee123.net/img/rand?artid=146167408
img: https://bing.ee123.net/img/rand?artid=146167408
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     深度学习subword分词BPE
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     首先要安装：pip install subword-nmt
    </p>
    <p>
     'subword-nmt'是一种用于处理自然语言处理任务中的词汇外问题的工具，支持几种不同的算法
    </p>
    <h2>
     一、介绍
    </h2>
    <p>
     BPE：这是一种流行分词算法，可以有效的平衡词汇表大小和步数，分词采用共现性。
    </p>
    <p>
     步骤：
    </p>
    <p>
     1.准备足够大的训练语料
    </p>
    <p>
     2.确定期望的subword词表大小（超参）
    </p>
    <p>
     3.将单词拆分为字符序列并在末尾添加后缀"&lt;/w&gt;"，这样就可以统计单词频率。 比如一开始有一个l字母，现在编程l&lt;/w&gt;5，就说明出现了五次。停止符&lt;/w&gt;的意义在于表示subword是词后缀。每次合并后词表可能出现三种变化：
    </p>
    <p>
     +1：表明加入合并后的新字词，同时原来在的2个子词还保留；
    </p>
    <p>
     0：如果一个字词不是单独出现的，就被消解；
    </p>
    <p>
     -1：表明加入合并后的新词，同时原来2个字词都被消解。实际上，随着合并次数增加，词表大小通常先增加后减小。
    </p>
    <p>
     4.统计每一个连续字节对出现的频率，选择最高频的配对比如lo&lt;/w&gt;5
    </p>
    <p>
     5.重复
    </p>
    <p>
     “low” 和 “lower” 合并后，可能生成子词 “low” 和 “er”
    </p>
    <h2>
     二、代码
    </h2>
    <h3>
     导包：
    </h3>
    <p>
     随机种子：
    </p>
    <ol>
     <li>
      确保实验可复现性
     </li>
     <li>
      控制以下随机过程的一致性：
      <ul>
       <li>
        神经网络权重初始化
       </li>
       <li>
        数据加载顺序
       </li>
       <li>
        数据增强中的随机变换
       </li>
       <li>
        Dropout等随机操作
       </li>
      </ul>
     </li>
    </ol>
    <pre><code class="language-python">import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import sklearn
import pandas as pd
import os
import sys
import time
from tqdm.auto import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F

print(sys.version_info)
for module in mpl, np, pd, sklearn, torch:
    print(module.__name__, module.__version__)
    
device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
print(device)

seed = 42
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)</code></pre>
    <h3>
     准备数据：
    </h3>
    <pre><code class="language-python">cleaned_df = pd.read_csv("imdb_processed.csv")
print(cleaned_df.shape) # (50000, 2), 50000条评论, 2列

# 随机打乱数据，取训练集和测试集
np.random.seed(seed) #随机
cleaned_df = cleaned_df.sample(frac=1).reset_index(drop=True)#打乱，frac=1表示全部打乱（frac是比例，reset_index(drop=True)是重新索引
with open("imdb_train.txt", "w", encoding="utf8") as file:# 保存训练集
    for line in cleaned_df.processed.values[:25000]:#只保存了processed列，即评论文本，没有保存label列
        file.write(line.lower() + "\n") #变为小写，token数量少一些

with open("imdb_test.txt", "w", encoding="utf8") as file:# 保存测试集
    for line in cleaned_df.processed.values[25000:]:
        file.write(line.lower() + "\n")</code></pre>
    <ul>
     <li>
      <code>
       np.random.seed(seed)
      </code>
      ：固定随机种子，确保每次打乱顺序一致（可复现性）
     </li>
     <li>
      <code>
       sample(frac=1)
      </code>
      ：按100%比例随机采样（即打乱数据顺序）
     </li>
     <li>
      <code>
       reset_index(drop=True)
      </code>
      ：重置索引并丢弃旧索引
     </li>
    </ul>
    <pre><code class="language-python"># 学习bpe分词(很慢,学一次就好)
# -i 选择学习的文件
# -o 核心输出文件,分词需要用到imdb_bpe_code,生成的 imdb_bpe_code 文件包含了学习到的 BPE 操作规则。这些规则用于将单词分割成子词单元
# --write-vocabulary 字典输出文件，imdb_bpe_vocab 文件包含了根据 BPE 规则生成的词汇表，列出了所有子词单元及其频率
# -s 词表大小
!subword-nmt learn-joint-bpe-and-vocab -i ./imdb_train.txt -o ./imdb_bpe_code --write-vocabulary ./imdb_bpe_vocab -s 8000
</code></pre>
    <p>
     使用
     <code>
      subword-nmt
     </code>
     工具包进行
     <strong>
      BPE（Byte Pair Encoding）子词分词训练
     </strong>
    </p>
    <pre><code class="language-python"># 应用bpe分词,-c 指定 BPE 编码的配置文件,就是上面生成的 imdb_bpe_code 文件
!subword-nmt apply-bpe -c ./imdb_bpe_code -i ./imdb_train.txt -o ./imdb_train_bpe.txt
!subword-nmt apply-bpe -c ./imdb_bpe_code -i ./imdb_test.txt -o ./imdb_test_bpe.txt
#-c ./imdb_bpe_code   # 指定 BPE 合并规则配置文件（学习阶段生成）
#-i ./imdb_train.txt  # 输入：原始训练集文本
#-o ./imdb_train_bpe.txt  # 输出：子词化后的训练文本</code></pre>
    <h5>
     <strong>
      工作流程
     </strong>
    </h5>
    <ol>
     <li>
      <strong>
       加载规则
      </strong>
      ：读取
      <code>
       imdb_bpe_code
      </code>
      中的合并优先级（如
      <code>
       e s → es
      </code>
      ）
     </li>
     <li>
      <strong>
       贪婪匹配
      </strong>
      ：对每个单词从长到短尝试应用已有合并规则
     </li>
     <li>
      <strong>
       添加分隔符
      </strong>
      ：通常用
      <code>
       @@
      </code>
      标记子词边界（可配置）
     </li>
    </ol>
    <p>
     <img alt="" height="160" src="https://i-blog.csdnimg.cn/direct/0f65a512a6d3468e888fe953d2c249c5.png" width="508"/>
    </p>
    <pre><code class="language-python">from torch.utils.data import Dataset, DataLoader

# 随后加载数据集就从bpe分词的文件里加载
class IMDBDataset(Dataset):
    def __init__(self, mode="train"):
        df = pd.read_csv("imdb_subwords.csv").query("split == '{}'".format(mode)) # 加载训练集或测试集，query语句筛选
        self.texts = df["subwords10k"].values # 评论文本
        self.labels = df["label"].values # 评论标签
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return self.texts[idx].split(), self.labels[idx] # 返回文本和标签
    
    
train_ds = IMDBDataset("train")
test_ds = IMDBDataset("test")</code></pre>
    <p>
     df = pd.read_csv("imdb_subwords.csv").query("split == '{}'".format(mode))详解：
    </p>
    <ol>
     <li>
      <p>
       <code>
        pd.read_csv("imdb_subwords.csv")
       </code>
      </p>
      <ul>
       <li>
        这是Pandas库读取CSV文件的标准方法
       </li>
       <li>
        会返回一个DataFrame对象
       </li>
       <li>
        文件"imdb_subwords.csv"应位于当前工作目录或指定路径下
       </li>
      </ul>
     </li>
    </ol>
    <ol>
     <li>
      <p>
       <code>
        .query("split == '{}'".format(mode))
       </code>
      </p>
      <ul>
       <li>
        使用DataFrame的query()方法进行数据过滤
       </li>
       <li>
        <code>
         split
        </code>
        是DataFrame中的一个列名（应该表示数据集划分类型）
       </li>
       <li>
        <code>
         '{}'
        </code>
        是字符串格式化占位符，会被
        <code>
         mode
        </code>
        变量的值替换
       </li>
       <li>
        例如：当
        <code>
         mode = "train"
        </code>
        时，实际执行的查询条件是
        <code>
         split == 'train'
        </code>
       </li>
      </ul>
     </li>
    </ol>
    <ul>
     <li>
      文本：BPE分词字符串（如
      <code>
       "I love this movie"
      </code>
      ）→ 通过
      <code>
       .split()
      </code>
      转为列表（
      <code>
       ["I", "love", ...]
      </code>
      ）
     </li>
    </ul>
    <h3>
     构造字典：
    </h3>
    <pre><code class="language-python">#载入词表，看下词表长度，词表就像英语字典
word2idx = {
    "[PAD]": 0,     # 填充 token
    "[BOS]": 1,     # begin of sentence
    "[UNK]": 2,     # 未知 token
    "[EOS]": 3,     # end of sentence
}
idx2word = {value: key for key, value in word2idx.items()}
print(idx2word)
index = len(idx2word) # 词表长度，现在为4

#%%
threshold = 1  # 出现次数低于此的token舍弃
with open("imdb_bpe_vocab", "r", encoding="utf8") as file:
    for line in tqdm(file.readlines()):
        token, counts = line.strip().split()#左右空格去掉
        if int(counts) &gt;= threshold: #词频大于等于1就要
            word2idx[token] = index # 加入词表
            idx2word[index] = token # 加入反向词典
            index += 1

vocab_size = len(word2idx)
print("vocab_size: {}".format(vocab_size))
#%%
# 选择 max_length
length_collect = {}
for text, label in train_ds:
    length = len(text)
    length_collect[length] = length_collect.get(length, 0) + 1
    
MAX_LENGTH = 500
plt.bar(length_collect.keys(), length_collect.values())
plt.axvline(MAX_LENGTH, label="max length", c="gray", ls=":")
plt.legend()
plt.show()</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35353834333932312f:61727469636c652f64657461696c732f313436313637343038" class_="artid" style="display:none">
 </p>
</div>


