---
layout: post
title: "理解知识如何在大型Vision-Language-Models-中演化"
date: 2025-03-10 20:47:11 +0800
description: "这项工作研究了多模态知识如何在lvlm中发展。使用early exit 和降维技术，我们设计了几种策略来跟踪模型中的知识，并在三个层次上探索这个主题：单个令牌概率、令牌概率分布和特征编码。基于两个关键节点，即关键层和突变层，我们首次深入了解了lvlm中的知识进化过程。此外，根据不同进化阶段的特点，我们还在模型压缩和幻觉消除等问题上探索了新的视角。"
keywords: "理解知识如何在大型Vision-Language Models 中演化"
categories: ['1500深度学习笔记']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "146125862"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146125862
    alt: "理解知识如何在大型Vision-Language-Models-中演化"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146125862
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146125862
cover: https://bing.ee123.net/img/rand?artid=146125862
image: https://bing.ee123.net/img/rand?artid=146125862
img: https://bing.ee123.net/img/rand?artid=146125862
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     理解知识如何在大型Vision-Language Models 中演化
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     原论文:Towards Understanding How Knowledge Evolves in Large Vision-Language Models
    </p>
    <p>
     <span style="color:#38d8f0">
      第一次读这种纯分析和讨论向的
     </span>
    </p>
    <h2>
     Abstract
    </h2>
    <p>
     大型视觉语言模型（LVLMs）正逐渐成为许多人工智能应用的基础。然而，了解它们的内部工作机制一直困扰着研究人员，这反过来又限制了它们能力的进一步提高。在本文中，我们试图研究多模态知识如何演变并最终在lvlm中诱导自然语言。我们设计了一系列新的lvlm内部知识分析策略，并从single token probabilities, token probability distributions, feature encodings三个层面深入研究了多模态知识的演化。在此过程中，我们确定了知识进化的两个关键节点：critical layers 关键层和mutation layers 突变层，将知识进化过程分为快速进化、稳定和突变三个阶段。我们的研究首次揭示了LVLMs的知识演化轨迹，为理解其潜在机制提供了新的视角。
    </p>
    <p>
     <strong>
      Contribution
     </strong>
    </p>
    <p>
     •    We
     <span style="color:#ff9900">
      deconstruct the token generation process in LVLMs
     </span>
     and, for the first time, investigate deeply the evolution of multimodal knowledge within the model in multiple levels, thereby building a bridge between multimodal inputs and natural language descriptions.
     <br/>
     •    We design various effective strategies to
     <span style="color:#ff9900">
      analyze the evolution at different levels
     </span>
     , and observe several novel and intriguing phenomena from extensive experiments.
     <br/>
     •    We creatively
     <span style="color:#ff9900">
      reveal the patterns of knowledge evolution in LVLMs
     </span>
     , which has great significance for illuminating the mechanisms underlying the black-box models and de-signing more efficient and accurate LVLMs.
    </p>
    <p>
     <img alt="" height="528" src="https://i-blog.csdnimg.cn/direct/90874c1290b046fdbfcd7713011b077a.png" width="778"/>
    </p>
    <p>
    </p>
    <h2>
     Backgrounds
    </h2>
    <p>
     an LVLM consists of three modules:
    </p>
    <p>
     1.
     <span style="color:#ff9900">
      modality encoder
     </span>
     , which generally includes a visual encoder and a text en-coder, extracting features from the input modalities. Pre-trained encoders are usually used here, such as
     <span style="color:#ff9900">
      CLIP
     </span>
     [32] and
     <span style="color:#ff9900">
      BLIP
     </span>
     [24].
    </p>
    <p>
    </p>
    <p>
     2.
     <span style="color:#ff9900">
      feature alignment module
     </span>
     , en-suring effective interaction between multimodal inputs in the same feature space.
    </p>
    <p>
     feature alignment methods, mainly categorized into two types:
    </p>
    <p>
     1) linear projections that directly transform image features into the word embedding space [26, 27], and 2) sampling methods that learn to represent visual-textual alignments [13, 43, 50].
    </p>
    <p>
     Regardless of the method used by the alignment module, the fused multimodal features do not affect the subsequent language generation process.
    </p>
    <p>
    </p>
    <p>
     3.
     <span style="color:#ff9900">
      LLM
     </span>
     , using the fused features to generate natural language descriptions in an autoregressive manner, progressively predicting the next word. Pretrained open-source LLMs, such as
     <span style="color:#ff9900">
      LLaMA
     </span>
     [38] and
     <span style="color:#ff9900">
      Vicuna
     </span>
     [10], are typically used, which are then finetuned with visual-language instructions.
    </p>
    <p>
     <span style="color:#38d8f0">
      本工作重点关注模块2-3
     </span>
    </p>
    <p>
    </p>
    <p>
     backend lannguage module: LLaVA-1.5 with a 32-layer vicuna-1.5 model
    </p>
    <p>
     <strong>
      标记（Token）定义
     </strong>
    </p>
    <p>
     aligned visual tokens:
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/fffd63ea060f4f36966d2921d07a6799.png" width="289"/>
    </p>
    <p>
     <span style="color:#ff9900">
      M is the length of the visual tokens
     </span>
     and it is usually a fixed number
    </p>
    <p>
     input text tokens:
     <img alt="" height="28" src="https://i-blog.csdnimg.cn/direct/816e39fb13e548a78b2e8c231ad5d866.png" width="296"/>
    </p>
    <p>
     <span style="color:#ff9900">
      N is the length of the text tokens,
     </span>
     which depends on the user prompt.
    </p>
    <p>
     <span style="color:#38d8f0">
      这里没看懂，为什么xM 重复那么多次？
     </span>
    </p>
    <p>
    </p>
    <p>
     <strong>
      模型预测
     </strong>
    </p>
    <p>
     LLM predicts tokens in an
     <span style="color:#a2e043">
      autoregressive 自回归
     </span>
     manner based on previous tokens, assuming K tokens have been predicted, they can be denoted as
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/ed75056b4b454ce6afc9e6fe577f3653.png" width="239">
      （最终生成的文本序列）
     </img>
    </p>
    <p>
     the final input sequence of the LLM can be denoted as
     <img alt="" height="31" src="https://i-blog.csdnimg.cn/direct/26caa736b9ab46e293ee316daaee0200.png" width="165">
      , whose length is M + N + K.（模型将同时处理视觉和文本输入）
     </img>
    </p>
    <p>
     When predicting the (K+1)-th token, the input is first embedded into a sequence of vectors
    </p>
    <p>
     H0 =
     <img alt="" height="41" src="https://i-blog.csdnimg.cn/direct/7b4f3a2a6f364ac6962a8dc0a6c885a9.png" width="206"/>
    </p>
    <p>
     After that, H0 is processed by
     <span style="color:#ff9900">
      L stacked transformer blocks
     </span>
     successively(for LLaVA-1.5-7B, L = 32)
    </p>
    <p>
     <strong>
      Transformer 处理
     </strong>
    </p>
    <p>
     then the
     <span style="color:#a2e043">
      language head ϕ(·)
     </span>
     is applied on the output of the final layer (HL)
     <span style="color:#ff9900">
      to predict the probability of the next token over the vocabulary set χ
     </span>
     , formulated as
    </p>
    <p>
     <img alt="" height="52" src="https://i-blog.csdnimg.cn/direct/39c7bbb039084b55b7b9d38e90cb880b.png" width="485"/>
    </p>
    <p>
    </p>
    <p>
     <strong>
      标记解码
     </strong>
    </p>
    <p>
     After obtaining all the
     <span style="color:#ff9900">
      logits of token
     </span>
     s in χ, the predicted token is selected by a decoding method, then the selected token is concatenated to the last of xp for the next-round generation, until all tokens are predicted.
    </p>
    <p>
     一旦得到每个标记的 logits（预测概率），通过解码方法将其转化为实际标记，并选择概率最大的标记作为下一个预测。这个过程会持续进行，直到所有标记都被预测完
    </p>
    <p>
    </p>
    <p>
     <strong>
      中间层的知识表示
     </strong>
     <br/>
     To analyze the inner knowledge representations of the model, we apply the function
     <span style="color:#a2e043">
      ϕ(·)
     </span>
     on each intermediate layer, and predict next tokens with these hidden features. Denoting the output of the j-th layer as Hj , the probability of the next token predicted with it can be calculated as
    </p>
    <p>
     <img alt="" height="93" src="https://i-blog.csdnimg.cn/direct/cdc5fae3b3414d35b675b7084d09ffd4.png" width="488"/>
    </p>
    <p>
     Our study starts from the probabilities of the output tokens in the intermediate layers, and then analyzes the prob-ability distribution of all tokens over the vocabulary set, and finally, explores further the feature vectors that used for calculate token probabilities.
    </p>
    <p>
     模型首先为每个标记生成概率分布，并分析这些分布，以确定最可能的下一个标记,重点是使用 transformer 各层的中间特征来改进下一个标记的预测
    </p>
    <p>
    </p>
    <h2>
     Token Probability Analyses
    </h2>
    <p>
     the probability of individual tokens varies across different layers
    </p>
    <p>
     1) typical token probability trends during normal inferences
    </p>
    <p>
     2) probability changes for key entities when the model generates hallucinations (幻觉)
    </p>
    <p>
    </p>
    <h3>
     Normal Inference
    </h3>
    <p>
     AMBER dataset
    </p>
    <p>
     prompt: "Please describe this image in detail"  to prompt LLaVA-1.5 to generate descriptions
    </p>
    <p>
     compute the probabilities for each token across different layers
    </p>
    <p>
     <img alt="" height="733" src="https://i-blog.csdnimg.cn/direct/a466bb4fa89f49ccab167eeb82af535f.png" width="1080"/>
    </p>
    <p>
     first 24 tokens together with some others with high probabilities are shown. The observations are as follows
    </p>
    <p>
     1.
     <span style="color:#ff9900">
      all tokens exhibit very low probabilities in the shallow layers of the model
     </span>
     , nearing zero
     <br/>
     around the 20-th layer, the probabilities of these token sharply increase (for ease of description, we refer to these layers as critical layers)
     <span style="color:#38d8f0">
      跟前面那篇剪枝论文对于信息的分布还挺吻合
     </span>
     <span style="color:null">
      standing out from the vocabulary set to gain a prob-abilistic advantage.Some tokens maintain this advantage through to the output layer
     </span>
    </p>
    <p>
     2.
     <span style="color:#ff9900">
      some tokens that are easy to predict
     </span>
     , such as punctuation marks, words directly copied from the in-put question like “image”, subsequent words in continuous phrases like “field”, “soccer”, and “ball”, as well as some functional words like “a”, their probabilities tend to stabilize after the critical layers.
    </p>
    <p>
     when
     <span style="color:#ff9900">
      predicting tokens that contain important information
     </span>
     —such as certain verbs, nouns, and prepositions describing object attributes (e.g., “standing”, “in”, “boy”, “he”, “appear”)—the number of candidate tokens （候选令牌）increases, and these tokens often have high probabilities
    </p>
    <p>
     both the predicted tokens and other can-didates
     <span style="color:#ff9900">
      experience abrupt changes in probability
     </span>
     , with the
     <span style="color:#ff9900">
      predicted token only gaining an advantage after this probability surge
     </span>
     , eventually becoming the final selection
     <span style="color:#38d8f0">
      确实logits在很多layers都会突然交叉然后改变优势
     </span>
    </p>
    <p>
    </p>
    <h3>
     Hallucinations 幻觉
    </h3>
    <p>
     we examine the probability changes of hallucinated tokens when the model generates faithless descriptions
    </p>
    <p>
     we exam-ine the probability changes of hallucinated tokens when the model generates faithless descriptions, and several exam-ples are placed in Figure 3.
    </p>
    <p>
     experience probability mutations after the critical layers, and the layers where the probabilities mutate are referred as the
     <span style="color:#a2e043">
      mutation layers.
     </span>
     <span style="color:#ff9900">
      Most of these tokens do not exhibit probability advantages before the mutation layers
     </span>
     <span style="color:null">
      ; instead, the correct tokens, such as “water”, “be-hind”, “black”, and “sheep”, often have higher probabil-ities.
     </span>
     <span style="color:#38d8f0">
      所以通过什么标准判读是correct的呢？
     </span>
     <span style="color:null">
      After the mutation layers, the probabilities of cor-rect tokens rapidly decline, while those of hallucinated ones rise sharply, enabling them to become the dominant tokens and ultimately be output.
     </span>
    </p>
    <p>
     <span style="color:null">
      probability mutations in deep layers may be associated with hallucinations.
     </span>
     <span style="color:#38d8f0">
      (所以有的时候正确的也会更高啊）
     </span>
    </p>
    <p>
     <img alt="" height="740" src="https://i-blog.csdnimg.cn/direct/a1d6f96d7dc84e7aa4db49516fd9fa4c.png" width="1318"/>
    </p>
    <p>
     possible reason:
    </p>
    <p>
     the language model’s capability—significantly large in both pa-rameters and training data—far surpasses that of the visual model [22, 41], resulting in that the
     <span style="color:#ff9900">
      language generation process is dominated by the language model
     </span>
     . Once the knowledge from the visual model stabilizes, it becomes difficult for subsequent layers to acquire new knowledge from the input.
     <span style="color:#ff9900">
      language model may inject its own prior knowledge,
     </span>
     <span style="color:null">
      constitutes
     </span>
     <span style="color:#ff9900">
      external knowledge
     </span>
    </p>
    <p>
     <span style="color:#38d8f0">
      总之就是语言模型的先验去影响了视觉模型，那么文生图是不是反过来，vision的先验会影响image
     </span>
    </p>
    <p>
    </p>
    <h2>
     Token Distribution Analyses
    </h2>
    <p>
     we analyze the probability distribution of all tokens in the vocabulary set across each layer and calculate the
     <span style="color:#a2e043">
      Jensen-Shannon (JS) divergence
     </span>
     between adjacent layers, which can be formulated as
    </p>
    <p>
     <img alt="" height="149" src="https://i-blog.csdnimg.cn/direct/78df57f6b98e4d39941ab5d92ed479b2.png" width="624"/>
    </p>
    <p>
     where i and j are two adjacent layers, A is the average distribution of pi and pj .
    </p>
    <p>
    </p>
    <h3>
     4.1. Normal Inference
    </h3>
    <p>
     we still take the image in Figure 2 as an example to illustrate our findings, which is shown in Figure 4
    </p>
    <p>
     <img alt="" height="521" src="https://i-blog.csdnimg.cn/direct/50d3a243aa7649d38461c81004315e91.png" width="1297"/>
    </p>
    <p>
     1.
    </p>
    <p>
     Around the 18-th layer, there is a clear distinction in the JS divergence values between the shallow and deep layers.
     <span style="color:#38d8f0">
      浅层divergence 大，深层小
     </span>
    </p>
    <p>
     critical layers of
     <span style="color:#ff9900">
      divergences correspond to the critical layers of token probabilities approximately
     </span>
    </p>
    <p>
     Large divergences in shallow layers indicate that
     <span style="color:#ff9900">
      knowledge evolves rapidly across these layers
     </span>
     , causing dramatic shifts in token probability distributions.
    </p>
    <p>
     shallow layers primarily focus on the local features, it is difficult for a token to gain a probability advantage at this stage. Instead, many tokens may have similar probabilities, thus their probabilities re-main near zero. When knowledge evolves into deeper lay-ers, progressing from low-level to high-level feature aggre-gation,
     <span style="color:#ff9900">
      small divergences values mean that the pace of evo-lution obviously slow down
     </span>
     . Consequently, tokens associ-ated with deep knowledge start to gain probability advan-tages and emerge from the vocabulary set.
     <span style="color:#38d8f0">
      所以可以说divergence大的时候是模型正在寻找正确答案的过程
     </span>
     ？
    </p>
    <p>
    </p>
    <p>
     2.
    </p>
    <p>
     When pre-dicting entities containing important information, divergence values in deep layers often
     <span style="color:#ff9900">
      mutate,
     </span>
     <span style="color:null">
      token distributions un-dergo significant changes in these layers, further suggesting that the
     </span>
     <span style="color:#ff9900">
      knowledge within the model also experiences sud-den shifts
     </span>
     <span style="color:null">
      .
     </span>
    </p>
    <p>
     In the mutation layers,
     <span style="color:#a2e043">
      new knowledge is in-jected into the model
     </span>
     ,
     <span style="color:#38d8f0">
      (这个怎么说明呢？)
     </span>
     causing a secondary evolution of the knowledge and leading to oscillations in the token proba-bility distributions. If the injected knowledge conflicts with the image-derived knowledge already present in the model, it may lead to the emergence of a new dominant token, potentially caus-ing the model to fall into hallucinations.
     <span style="color:#38d8f0">
      新进入的知识从哪来的呢？
     </span>
    </p>
    <p>
    </p>
    <p>
    </p>
    <h3>
     4.2. Skip Connection
    </h3>
    <p>
     <img alt="" height="521" src="https://i-blog.csdnimg.cn/direct/50d3a243aa7649d38461c81004315e91.png" width="1297"/>
    </p>
    <p>
     According to Figure 4, the critical layers and mutation lay-ers divide knowledge evolution into three stages:
     <span style="color:#ff9900">
      rapid evolution
     </span>
     ,
     <span style="color:#ff9900">
      stabilization
     </span>
     (slow evolution), and
     <span style="color:#ff9900">
      mutation
     </span>
     .
    </p>
    <p>
     we explore the impact of the latter two stages through skip connections:
    </p>
    <p>
     <span style="color:#ff9900">
      jumping from the critical layers to the mutation layers (skip.1)
     </span>
     ,
     <span style="color:#ff9900">
      skipping only the mutation layers (skip.2)
     </span>
     , and
     <span style="color:#ff9900">
      jumping directly from the critical layers to the last few layers (skip.3)
     </span>
     . Some examples of the experimental results are shown in Figure 5.
    </p>
    <p>
     <img alt="" height="821" src="https://i-blog.csdnimg.cn/direct/7cd4cc2d75c74d55b74d942f550b1bd3.png" width="963"/>
    </p>
    <p>
     when
     <span style="color:#ff9900">
      skipping the layers before the mutation layers
     </span>
     , the
     <span style="color:#ff9900">
      model’s outputs closely resemble the original
     </span>
     , including object descriptions and attributes, and even the hallucinations in the original descriptions appear in the skip.1 output. This indicates that during the phase from the criti-cal layers to the mutation layers, knowledge changes mini-mally between layers, which means the
     <span style="color:#ff9900">
      evolution process of multimodal knowledge tends to stabilize
     </span>
     .
    </p>
    <p>
     In the skip.2 setup, the model
     <span style="color:#ff9900">
      not only retains the original description but also cor-rects some hallucinated information
     </span>
     , for example, changing “standing” to “playing” and “toothbrush” to “food”.
    </p>
    <p>
     knowledge injected into the mutation lay-ers only affects the probabilities of certain tokens, while leaving the predictions of most tokens unchanged.
     <span style="color:#ff9900">
      If the injected knowledge misaligns with image features, it will leads to hallucinations.
     </span>
     <span style="color:null">
      handling the mutation layers can improve model’s accuracy.
     </span>
    </p>
    <p>
     When
     <span style="color:#ff9900">
      jumping directly from the critical layers to the last few layers
     </span>
     , the outputs retain some similarities to the original results, but exhibit more hallucinations, indicating that knowledge continues to evolve after the critical layers, though at a slower pace, potentially involving processes of local knowledge aggregation and global knowledge construction.
     <span style="color:#ff9900">
      Therefore, skipping too many layers may cause some knowledge to be omitted, consequently affecting the outputs.
     </span>
    </p>
    <p>
    </p>
    <h2>
     Feature Encoding Analyses
    </h2>
    <p>
     we use t-SNE to reduce their dimensions
    </p>
    <h3>
     Features in a Single Image
    </h3>
    <p>
     Assuming the number of predicted tokens is T, the size of the feature matrix to be analyzed is T × 32 × 4096. Then the compressed matrix after t-SNE is:
    </p>
    <p>
     <img alt="" height="52" src="https://i-blog.csdnimg.cn/direct/0c205c6964624390a80bc0d54adec6fd.png" width="499"/>
    </p>
    <p>
     <span style="color:#a2e043">
      T-SNE（t-Distributed Stochastic Neighbor Embedding）
     </span>
     主要适用于
     <strong>
      高维数据的可视化
     </strong>
     ，特别是在以下情况下表现良好：
    </p>
    <ol>
     <li>
      <p>
       <strong>
        非线性结构的数据
       </strong>
       ：T-SNE能够很好地捕捉数据的
       <strong>
        局部结构
       </strong>
       ，适用于存在非线性关系的数据降维，可用于发现数据中的簇结构。
      </p>
     </li>
     <li>
      <p>
       <strong>
        数据类别较多且复杂
       </strong>
       ：T-SNE在展示高维数据中的不同类别时，能够将相似的数据点聚集在一起，同时保持不同类别的数据分离，使得数据的分类结构更加明显。
      </p>
     </li>
     <li>
      <p>
       <strong>
        数据特征空间维度较高（通常 &gt; 50 维）
       </strong>
       ：在高维数据中，欧几里得距离可能无法很好地表示数据点之间的关系，T-SNE可以通过基于概率分布的方法找到更合适的低维表示。
      </p>
     </li>
     <li>
      <p>
       <strong>
        用于数据探索和可视化
       </strong>
       ：T-SNE通常用于将高维数据降至
       <strong>
        2D 或 3D
       </strong>
       ，以便进行直观的可视化。例如，在自然语言处理（NLP）中，可以用 T-SNE 来可视化词向量（如 Word2Vec, BERT）在低维空间的分布情况。
      </p>
     </li>
    </ol>
    <p style="background-color:transparent">
     需要注意的限制：
    </p>
    <ul>
     <li>
      <strong>
       计算复杂度高
      </strong>
      ：T-SNE的计算复杂度较高（O(N²)），在大规模数据集（&gt;10,000个样本）上效率较低。
     </li>
     <li>
      <strong>
       难以保持全局结构
      </strong>
      ：T-SNE主要关注局部结构，可能会导致不同类群的相对位置失真。
     </li>
     <li>
      <strong>
       随机性较大
      </strong>
      ：T-SNE的结果对超参数（如 perplexity）和初始化较敏感，每次运行可能会得到不同的结果。
     </li>
    </ul>
    <p>
    </p>
    <p>
     We take the four images shown in Figure 3 as examples, and visualizes their feature encodings in Figure 6.
    </p>
    <p>
     <img alt="" height="672" src="https://i-blog.csdnimg.cn/direct/4c15397e52e0420e9da08c6d9ccc68d3.png" width="636"/>
    </p>
    <p>
     #1:
     <strong>
      For all predicted to-kens, their feature encodings exhibit a progressively diverging trend
     </strong>
     .
    </p>
    <p>
     Specifically, the features of all tokens are closely clustered together in the first two layers, indicating that
     <span style="color:#ff9900">
      in the initial stage, the features for different tokens are similar
     </span>
     .
    </p>
    <p>
     informa-tion propagates forward from shallow to deep layers, knowl-edge continues to evolve, gradually revealing
     <span style="color:#ff9900">
      token-specific characteristics
     </span>
    </p>
    <p>
     <span style="color:#38d8f0">
      由低维到高维度逐渐发散，也符合前文的分布
     </span>
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     #2:
     <strong>
      An approximately linear distribution emerges for the feature encodings of a single token across different layers
     </strong>
     .
    </p>
    <p>
     Starting from the initial layer’s feature, the features in subsequent layers diverge gradually along a near-linear path, with each layer’s feature located just next to the previous one 线性diverge，参考图中的连续线
    </p>
    <p>
     for some tokens,
     <span style="color:#ff9900">
      a hierarchical phenomenon is observed
     </span>
     (due to the large number of tokens in the figure, we do not label them to avoid making the figure overly cluttered)（ some clusters of green points (shallow features) are separate from clusters of blue points (deep features)）the deep features and shallow features for these tokens differ significantly
    </p>
    <p>
    </p>
    <h3>
     Features in Different Images
    </h3>
    <p>
     We further explore the knowledge evolution patterns with different input images.
    </p>
    <p>
     For two images, Img1 and Img2, if the predicted token for Img1 is a functional word such as “a” or “an”, while the token for Img2 corresponds to a key object in the image, the knowl-edge required for these two types of tokens differs signif-icantly.
    </p>
    <p>
     To make the output tokens more controllable and con-centrated on the image contents, we perform this experi-ment on the
     <span style="color:#ff9900">
      visual question-answering (VQA) task
     </span>
     .In our setting, the model is prompted to
     <span style="color:#ff9900">
      answer simple questions about the input image with a single word
     </span>
     , such as “What is the color of the flower?” or “What is the main object of the image?”
    </p>
    <p>
     <span style="color:#38d8f0">
      之前用的是图生文，这里采用VQA
     </span>
    </p>
    <p>
     Images with the same question are grouped together, with a total count of NI . The t-SNE processed matrix containing the features of all layers across all images is represented as:
    </p>
    <p>
     <img alt="" height="63" src="https://i-blog.csdnimg.cn/direct/37e52659f3de4fb6b84bc8b563fc273d.png" width="513"/>
    </p>
    <p>
     Two examples are shown in Figure 7, each being ob-served on 12 different images.
    </p>
    <p>
     <img alt="" height="432" src="https://i-blog.csdnimg.cn/direct/2154ee9a955f4576b5e8a25c991e5fde.png" width="491"/>
    </p>
    <p>
     features are closely clustered, forming the “neck” of the “guitar”, while deep features diverge in different directions, form-ing the “body” of the “guitar”.
    </p>
    <p>
     <span style="color:#38d8f0">
      针对同一问题浅层的特征分布也相对一致
     </span>
    </p>
    <p>
     the final layer’s features exhibit a distinct behavior com-pared to other layers, with some features showing a ten-dency to re-cluster. A possi-ble reason is that the attributes of the predicted tokens are relatively similar (e.g., animals, colors), which leads to
     <span style="color:#ff9900">
      similar encodings
     </span>
     in the feature space,
     <span style="color:#ff9900">
      especially for the final transformer block used for generating the output tokens
     </span>
     .
    </p>
    <p>
    </p>
    <h2>
     Further Discussions
    </h2>
    <h3>
     LVLM vs. LLM
    </h3>
    <p>
     LLM no clear evidence of hierarchy or mutations (那那篇剪枝（？））
    </p>
    <p>
     This may be because LVLMs require in-struction finetuning using vision-language data, which de-mands strict faithfulness to the input image, resulting in characteristics distinct from those of LLMs.
    </p>
    <h3>
     Knowledge Evolution
    </h3>
    <p>
     with the criti-cal layers and mutation layers as two key nodes, the evolu-tion process can be divided into three stages: rapid evolu-tion, stabilization, and mutation. The rapid evolution stage occurs in the shallow layers of the model, where knowledge undergoes swift updates during forward propagation. As the layers deepen, the process transitions to the stabilization stage, where knowledge exhibits characteristics specific to tokens, and evolution slows. In the mutation stage, external knowledge not derived from the input image (e.g., general priors obtained through the LLM’s pretraining) is injected, prompting a secondary evolution. This newly introduced knowledge likely persists through to the output layer, ulti-mately influencing the selection of predicted tokens.
    </p>
    <p>
    </p>
    <h2>
     Conclusion
    </h2>
    <p>
     这项工作研究了多模态知识如何在lvlm中发展。使用early exit 和降维技术，我们设计了几种策略来跟踪模型中的知识，并在三个层次上探索这个主题：单个令牌概率、令牌概率分布和特征编码。基于两个关键节点，即关键层和突变层，我们首次深入了解了lvlm中的知识进化过程。此外，根据不同进化阶段的特点，我们还在模型压缩和幻觉消除等问题上探索了新的视角。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f5363616262617264735f2f:61727469636c652f64657461696c732f313436313235383632" class_="artid" style="display:none">
 </p>
</div>


