---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f753031303334323231332f:61727469636c652f64657461696c732f313436313030383136"
layout: post
title: "Flinkæ·±å…¥æµ…å‡ºä¹‹03çŠ¶æ€çª—å£checkpointä¸¤é˜¶æ®µæäº¤"
date: 2025-03-07 18:15:42 +0800
description: "Flink æ˜¯ä¸€ä¸ªé»˜è®¤å°±æœ‰çŠ¶æ€çš„åˆ†æå¼•æ“ï¼Œå‰é¢çš„WordCount æ¡ˆä¾‹å¯ä»¥åšåˆ°å•è¯çš„æ•°é‡çš„ç´¯åŠ ï¼Œå…¶å®æ˜¯å› ä¸ºåœ¨å†…å­˜ä¸­ä¿è¯äº†æ¯ä¸ªå•è¯çš„å‡ºç°çš„æ¬¡æ•°ï¼Œè¿™äº›æ•°æ®å…¶å®å°±æ˜¯çŠ¶æ€æ•°æ®ã€‚ä½†æ˜¯å¦‚æœä¸€ä¸ª Task åœ¨å¤„ç†è¿‡ç¨‹ä¸­æŒ‚æ‰äº†ï¼Œé‚£ä¹ˆå®ƒåœ¨å†…å­˜ä¸­çš„çŠ¶æ€éƒ½ä¼šä¸¢å¤±ï¼Œæ‰€æœ‰çš„æ•°æ®éƒ½éœ€è¦é‡æ–°è®¡ç®—ã€‚ä»å®¹é”™å’Œæ¶ˆæ¯å¤„ç†çš„è¯­ä¹‰ï¼ˆAt -least-once å’Œ Exactly-onceï¼‰ä¸Šæ¥è¯´ï¼ŒFlinkå¼•å…¥äº†State å’Œ CheckPointã€‚"
keywords: "Flinkæ·±å…¥æµ…å‡ºä¹‹03ï¼šçŠ¶æ€ã€çª—å£ã€checkpointã€ä¸¤é˜¶æ®µæäº¤"
categories: ['å¤§æ•°æ®æŠ€æœ¯å­¦ä¹ ']
tags: ['å¤§æ•°æ®', 'Flink']
artid: "146100816"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146100816
    alt: "Flinkæ·±å…¥æµ…å‡ºä¹‹03çŠ¶æ€çª—å£checkpointä¸¤é˜¶æ®µæäº¤"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146100816
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146100816
cover: https://bing.ee123.net/img/rand?artid=146100816
image: https://bing.ee123.net/img/rand?artid=146100816
img: https://bing.ee123.net/img/rand?artid=146100816
---

# Flinkæ·±å…¥æµ…å‡ºä¹‹03ï¼šçŠ¶æ€ã€çª—å£ã€checkpointã€ä¸¤é˜¶æ®µæäº¤

## Flinkæ˜¯ä¸€ä¸ªæœ‰çŠ¶æ€çš„æµï¼ŒğŸ‘…ä¸€èµ·æ·±å…¥äº†è§£è¿™ä¸ªæœ‰çŠ¶æ€çš„æµ

### 3ï¸âƒ£ ç›®æ ‡

1. æŒæ¡StateçŸ¥è¯†
2. æŒæ¡Flinkä¸‰ç§State Backend
3. æŒæ¡Flink checkpointå’ŒsavepointåŸç†
4. äº†è§£Flinkçš„é‡å¯ç­–ç•¥
5. checkpoint+two phase commitä¿è¯E-Oè¯­ä¹‰

### 4ï¸âƒ£ è¦ç‚¹

### ğŸ“– 1. Flinkçš„State

##### 1.1 stateæ¦‚è¿°

**Apache FlinkÂ® â€” Stateful Computations over Data Streams**

```
	Flink æ˜¯ä¸€ä¸ªé»˜è®¤å°±æœ‰çŠ¶æ€çš„åˆ†æå¼•æ“ï¼Œå‰é¢çš„WordCount æ¡ˆä¾‹å¯ä»¥åšåˆ°å•è¯çš„æ•°é‡çš„ç´¯åŠ ï¼Œ
	å…¶å®æ˜¯å› ä¸ºåœ¨å†…å­˜ä¸­ä¿è¯äº†æ¯ä¸ªå•è¯çš„å‡ºç°çš„æ¬¡æ•°ï¼Œè¿™äº›æ•°æ®å…¶å®å°±æ˜¯çŠ¶æ€æ•°æ®ã€‚
	ä½†æ˜¯å¦‚æœä¸€ä¸ª Task åœ¨å¤„ç†è¿‡ç¨‹ä¸­æŒ‚æ‰äº†ï¼Œé‚£ä¹ˆå®ƒåœ¨å†…å­˜ä¸­çš„çŠ¶æ€éƒ½ä¼šä¸¢å¤±ï¼Œ
	æ‰€æœ‰çš„æ•°æ®éƒ½éœ€è¦é‡æ–°è®¡ç®—ã€‚
	ä»å®¹é”™å’Œæ¶ˆæ¯å¤„ç†çš„è¯­ä¹‰ï¼ˆAt -least-once å’Œ Exactly-onceï¼‰ä¸Šæ¥è¯´ï¼Œ
	Flinkå¼•å…¥äº†State å’Œ CheckPointã€‚
	
	Stateä¸€èˆ¬æŒ‡ä¸€ä¸ªå…·ä½“çš„ Task/Operator çš„çŠ¶æ€ï¼ŒStateæ•°æ®é»˜è®¤ä¿å­˜åœ¨ Java çš„å †å†…å­˜ä¸­ã€‚

```

* å›é¡¾å•è¯è®¡æ•°çš„ä¾‹å­

  ```scala
  package com.kaikeba.demo1

  import org.apache.flink.api.java.tuple.Tuple
  import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, WindowedStream}
  import org.apache.flink.streaming.api.windowing.time.Time
  import org.apache.flink.streaming.api.windowing.windows.TimeWindow

  /**
    * ä½¿ç”¨æ»‘åŠ¨çª—å£
    * æ¯éš”1ç§’é’Ÿç»Ÿè®¡æœ€è¿‘2ç§’é’Ÿçš„æ¯ä¸ªå•è¯å‡ºç°çš„æ¬¡æ•°
    */
  object FlinkStream {

    def main(args: Array[String]): Unit = {
        //æ„å»ºæµå¤„ç†çš„ç¯å¢ƒ
        val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    //ä»socketè·å–æ•°æ®
  val sourceStream: DataStream[String] = env.socketTextStream("node01",9999)
   //å¯¼å…¥éšå¼è½¬æ¢çš„åŒ…
    import org.apache.flink.api.scala._
   
   //å¯¹æ•°æ®è¿›è¡Œå¤„ç†
    val result: DataStream[(String, Int)] = sourceStream
                                  .flatMap(x => x.split(" ")) //æŒ‰ç…§ç©ºæ ¼åˆ‡åˆ†
                                  .map(x => (x, 1))  //æ¯ä¸ªå•è¯è®¡ä¸º1
                                  .keyBy(0)         //æŒ‰ç…§ä¸‹æ ‡ä¸º0çš„å•è¯è¿›è¡Œåˆ†ç»„
                                  .sum(1)          //æŒ‰ç…§ä¸‹æ ‡ä¸º1ç´¯åŠ ç›¸åŒå•è¯å‡ºç°çš„1
          //å¯¹æ•°æ®è¿›è¡Œæ‰“å°
    result.print()

    //å¼€å¯ä»»åŠ¡
     env.execute("FlinkStream")
  }
        }

  ```
* è¾“å…¥

  ```
    hadoop hadoop
    hadoop
    hive hadoop 

  ```
* è¾“å‡º

  ```
    8> (hadoop,1)
    1> (hive,1)
    8> (hadoop,2)
    8> (hadoop,3)
    8> (hadoop,4)

  ```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/0c1b9b542ab5462ab4c48c53fabc6e30.png)

##### 1.2 stateç±»å‹

* Flinkä¸­æœ‰ä¸¤ç§åŸºæœ¬ç±»å‹çš„State, ï¼Œä»–ä»¬ä¸¤ç§éƒ½å¯ä»¥ä»¥ä¸¤ç§å½¢å¼å­˜åœ¨ï¼š
  + åŸç”ŸçŠ¶æ€(raw state)
    - ç”±ç®—å­è‡ªå·±ç®¡ç†æ•°æ®ç»“æ„ï¼Œå½“è§¦å‘Checkpointæ“ä½œè¿‡ç¨‹ä¸­ï¼ŒFlinkå¹¶ä¸çŸ¥é“çŠ¶æ€æ•°æ®å†…éƒ¨çš„æ•°æ®ç»“æ„ï¼Œåªæ˜¯å°†æ•°æ®è½¬æ¢æˆbytesæ•°æ®å­˜å‚¨åœ¨Checkpointsä¸­ï¼Œå½“ä»Checkpointsæ¢å¤ä»»åŠ¡æ—¶ï¼Œç®—å­è‡ªå·±å†ååºåˆ—åŒ–å‡ºçŠ¶æ€çš„æ•°æ®ç»“æ„ã€‚
  + æ‰˜ç®¡çŠ¶æ€(managed state)
    - ç”±
      **Flink Runtime**
      æ§åˆ¶å’Œç®¡ç†çŠ¶æ€æ•°æ®ï¼Œå¹¶å°†çŠ¶æ€æ•°æ®è½¬æ¢æˆä¸ºå†…å­˜çš„Hash tablesæˆ– RocksDBçš„å¯¹è±¡å­˜å‚¨ï¼Œç„¶åå°†è¿™äº›æ•°æ®é€šè¿‡å†…éƒ¨çš„æ¥å£æŒä¹…åŒ–åˆ°checkpointsä¸­ï¼Œä»»åŠ¡å¼‚å¸¸æ—¶å¯ä»¥é€šè¿‡è¿™äº›çŠ¶æ€æ•°æ®æ¢å¤ä»»åŠ¡ã€‚
    - æ¨èä½¿ç”¨ManagedStateç®¡ç†çŠ¶æ€æ•°æ®ï¼ŒManagedStateæ›´å¥½çš„æ”¯æŒçŠ¶æ€æ•°æ®çš„é‡å¹³è¡¡ä»¥åŠæ›´åŠ å®Œå–„çš„å†…å­˜ç®¡ç†

|  | Managed State | Raw State |
| --- | --- | --- |
| çŠ¶æ€ç®¡ç†æ–¹å¼ | Flink Runtimeæ‰˜ç®¡ï¼Œè‡ªåŠ¨å­˜å‚¨ã€è‡ªåŠ¨æ¢å¤ã€è‡ªåŠ¨ä¼¸ç¼© | ç”¨æˆ·è‡ªå·±ç®¡ç† |
| çŠ¶æ€æ•°æ®ç»“æ„ | Flinkæä¾›çš„å¸¸ç”¨æ•°æ®ç»“æ„ï¼Œå¦‚ListStateã€MapStateç­‰ | å­—èŠ‚æ•°ç»„ï¼šbyte[] |
| ä½¿ç”¨åœºæ™¯ | ç»å¤§å¤šæ•°Flinkç®—å­ | ç”¨æˆ·è‡ªå®šä¹‰ç®—å­ |

###### 1.2.1 Operator State(ç®—å­çŠ¶æ€)

* operator stateæ˜¯taskçº§åˆ«çš„stateï¼Œè¯´ç™½äº†å°±æ˜¯æ¯ä¸ªtaskå¯¹åº”ä¸€ä¸ªstateã€‚
* Kafka Connector sourceä¸­çš„æ¯ä¸ªåˆ†åŒºï¼ˆtaskï¼‰éƒ½éœ€è¦è®°å½•æ¶ˆè´¹çš„topicçš„partitionå’Œoffsetç­‰ä¿¡æ¯ã€‚
* å¯¹äºOperator Stateï¼Œæˆ‘ä»¬è¿˜éœ€è¿›ä¸€æ­¥å®ç°
  `CheckpointedFunction`
  æ¥å£ã€‚
* Operator Stateçš„å®é™…åº”ç”¨åœºæ™¯ä¸å¦‚Keyed Stateå¤šï¼Œå®ƒç»å¸¸è¢«ç”¨åœ¨Sourceæˆ–Sinkç­‰ç®—å­ä¸Šï¼Œç”¨æ¥ä¿å­˜æµå…¥æ•°æ®çš„åç§»é‡æˆ–å¯¹è¾“å‡ºæ•°æ®åšç¼“å­˜ï¼Œä»¥ä¿è¯Flinkåº”ç”¨çš„Exactly-Onceè¯­ä¹‰ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/4d8c796bff2f4f37a2d2561ff3b38310.png)

###### 1.2.2 keyed State(é”®æ§çŠ¶æ€)

* `Keyed Stateï¼š`

  + é¡¾åæ€ä¹‰å°±æ˜¯åŸºäºKeyedStreamä¸Šçš„çŠ¶æ€ï¼Œè¿™ä¸ªçŠ¶æ€æ˜¯è·Ÿç‰¹å®šçš„Key ç»‘å®šçš„ã€‚KeyedStreamæµä¸Šçš„æ¯ä¸€ä¸ªKeyï¼Œéƒ½å¯¹åº”ä¸€ä¸ªStateã€‚Flinké’ˆå¯¹ Keyed State æä¾›äº†ä»¥ä¸‹å¯ä»¥ä¿å­˜Stateçš„æ•°æ®ç»“æ„.
* `Keyed stateæ‰˜ç®¡çŠ¶æ€æœ‰å…­ç§ç±»å‹ï¼š`

  + 1ã€
    ValueState

    ```
     ä¿å­˜ä¸€ä¸ªå¯ä»¥æ›´æ–°å’Œæ£€ç´¢çš„å€¼ï¼ˆå¦‚ä¸Šæ‰€è¿°ï¼Œæ¯ä¸ªå€¼éƒ½å¯¹åº”åˆ°å½“å‰çš„è¾“å…¥æ•°æ®çš„keyï¼Œå› æ­¤ç®—å­æ¥æ”¶åˆ°çš„æ¯ä¸ªkeyéƒ½å¯èƒ½å¯¹åº”ä¸€ä¸ªå€¼ï¼‰ã€‚ è¿™ä¸ªå€¼å¯ä»¥é€šè¿‡update(T) è¿›è¡Œæ›´æ–°ï¼Œé€šè¿‡ T value() è¿›è¡Œæ£€ç´¢	

    ```
  + 2ã€
    ListState

    ```
    	ä¿å­˜ä¸€ä¸ªå…ƒç´ çš„åˆ—è¡¨ã€‚å¯ä»¥å¾€è¿™ä¸ªåˆ—è¡¨ä¸­è¿½åŠ æ•°æ®ï¼Œå¹¶åœ¨å½“å‰çš„åˆ—è¡¨ä¸Šè¿›è¡Œæ£€ç´¢ã€‚å¯ä»¥é€šè¿‡ add(T) æˆ–è€… addAll(List<T>) è¿›è¡Œæ·»åŠ å…ƒç´ ï¼Œé€šè¿‡ Iterable<T> get() è·å¾—æ•´ä¸ªåˆ—è¡¨ã€‚è¿˜å¯ä»¥é€šè¿‡ update(List<T>) è¦†ç›–å½“å‰çš„åˆ—è¡¨	ã€‚

    ```
  + 3ã€
    MapState

    ```
    	ç»´æŠ¤äº†ä¸€ä¸ªæ˜ å°„åˆ—è¡¨ã€‚ ä½ å¯ä»¥æ·»åŠ é”®å€¼å¯¹åˆ°çŠ¶æ€ä¸­ï¼Œä¹Ÿå¯ä»¥è·å¾— åæ˜ å½“å‰æ‰€æœ‰æ˜ å°„çš„è¿­ä»£å™¨ã€‚ä½¿ç”¨ put(UKï¼ŒUV) æˆ–è€… putAll(Map<UKï¼ŒUV>) æ·»åŠ æ˜ å°„ã€‚ ä½¿ç”¨ get(UK) æ£€ç´¢ç‰¹å®š keyã€‚ ä½¿ç”¨ entries()ï¼Œkeys() å’Œ values() åˆ†åˆ«æ£€ç´¢æ˜ å°„ã€ é”®å’Œå€¼çš„å¯è¿­ä»£è§†å›¾ã€‚

    ```
  + 4ã€ReducingState

    ```
    	ä¿å­˜ä¸€ä¸ªå•å€¼ï¼Œè¡¨ç¤ºæ·»åŠ åˆ°çŠ¶æ€çš„æ‰€æœ‰å€¼çš„èšåˆã€‚æ¥å£ä¸ListStateç±»ä¼¼ï¼Œä½†ä½¿ç”¨add(T) å¢åŠ å…ƒç´ ï¼Œä¼šä½¿ç”¨æä¾›çš„ ReduceFunction è¿›è¡Œèšåˆã€‚

    ```
  + 5ã€AggregatingState

    ```
    	AggregatingState<IN, OUT>: ä¿ç•™ä¸€ä¸ªå•å€¼ï¼Œè¡¨ç¤ºæ·»åŠ åˆ°çŠ¶æ€çš„æ‰€æœ‰å€¼çš„èšåˆã€‚å’Œ ReducingState ç›¸åçš„æ˜¯, èšåˆç±»å‹å¯èƒ½ä¸æ·»åŠ åˆ°çŠ¶æ€çš„å…ƒç´ çš„ç±»å‹ä¸åŒã€‚ æ¥å£ä¸ ListStateç±»ä¼¼ï¼Œä½†ä½¿ç”¨ add(IN) æ·»åŠ çš„å…ƒç´ ä¼šç”¨æŒ‡å®šçš„ AggregateFunction è¿›è¡Œèšåˆ

    ```
* **keyedStateä½¿ç”¨æ–¹æ³•**

  + 1ã€åªèƒ½ç”¨äº
    `RichFunction`
  + 2ã€å°†
    `State`
    å£°æ˜ä¸ºå®ä¾‹å˜é‡
  + 3ã€åœ¨
    `open()`
    æ–¹æ³•ä¸­ä¸ºStateèµ‹å€¼
    - åˆ›å»ºä¸€ä¸ª
      `StateDescriptor`
    - åˆ©ç”¨
      `getRuntimeContext().getXXState(...)`
      æ„å»ºä¸åŒçš„State
  + 4ã€è°ƒç”¨Stateçš„æ–¹æ³•è¿›è¡Œ
    `è¯»å†™`
    - ä¾‹å¦‚ state.value()ã€state.update(â€¦)ç­‰ç­‰

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/45e7bd0992e64ea6afae3b797ad5e5bc.png)

##### 1.3 Keyed Stateæ¡ˆä¾‹æ¼”ç¤º

###### 1.3.1 ValueState

* ä½œç”¨

  + ä¿å­˜ä¸€ä¸ªå¯ä»¥æ›´æ–°å’Œæ£€ç´¢çš„å€¼
* éœ€æ±‚

  + ä½¿ç”¨valueStateå®ç°å¹³å‡å€¼æ±‚å–
* ä»£ç å¼€å‘

  ```scala
  package com.kaikeba.keystate

  import org.apache.flink.api.common.functions.RichFlatMapFunction
  import org.apache.flink.api.common.state.{ValueState, ValueStateDescriptor}
  import org.apache.flink.configuration.Configuration
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import org.apache.flink.util.Collector

  /**
    * ä½¿ç”¨valueStateå®ç°å¹³å‡å€¼æ±‚å–
    */
  object ValueStateOperate {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      import org.apache.flink.api.scala._

        env.fromCollection(List(
          (1L, 3d),
          (1L, 5d),
          (1L, 7d),
          (1L, 4d),
          (1L, 2d)
        ))
          .keyBy(_._1)
          .flatMap(new CountAverageWithValue())
          .print()
      env.execute()
    }
  }

  class CountAverageWithValue extends RichFlatMapFunction[(Long, Double), (Long, Double)] {
    //å®šä¹‰ValueStateç±»å‹çš„å˜é‡
    private var sum: ValueState[(Long, Double)] = _
      override def open(parameters: Configuration): Unit = {
    //åˆå§‹åŒ–è·å–å†å²çŠ¶æ€çš„å€¼
    sum = getRuntimeContext.getState(
      new ValueStateDescriptor[(Long, Double)]("average", classOf[(Long, Double)])
    )
  }

  override def flatMap(input: (Long, Double), out: Collector[(Long, Double)]): Unit = {
    // access the state value
    val tmpCurrentSum = sum.value
    // If it hasn't been used before, it will be null
    val currentSum = if (tmpCurrentSum != null) {
      tmpCurrentSum
    } else {
      (0L, 0d)
    }
    // update the count
    val newSum = (currentSum._1 + 1, currentSum._2 + input._2)

    // update the state
    sum.update(newSum)

    // if the count reaches 2, emit the average and clear the state
    if (newSum._1 >= 2) {
      out.collect((input._1, newSum._2 / newSum._1))
      //å°†çŠ¶æ€æ¸…é™¤
      //sum.clear()
    		}
  	}
    }

  ```

###### 1.3.2 ListState

* ä½œç”¨

  + ç”¨äºä¿å­˜æ¯ä¸ªkeyçš„å†å²æ•°æ®æ•°æ®æˆä¸ºä¸€ä¸ªåˆ—è¡¨
* éœ€æ±‚

  + ä½¿ç”¨ListStateæ±‚å–æ•°æ®å¹³å‡å€¼
* ä»£ç å¼€å‘

  ```scala
  package com.kaikeba.keystate

    import java.lang
    import java.util.Collections

    import org.apache.flink.api.common.functions.RichFlatMapFunction
    import org.apache.flink.api.common.state.{ListState, ListStateDescriptor}
    import org.apache.flink.configuration.Configuration
    import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    import org.apache.flink.util.Collector

    /**
      * ä½¿ç”¨ListStateå®ç°å¹³å‡å€¼æ±‚å–
      * ListState<T> ï¼šè¿™ä¸ªçŠ¶æ€ä¸ºæ¯ä¸€ä¸ª key ä¿å­˜é›†åˆçš„å€¼
      *      get() è·å–çŠ¶æ€å€¼
      *      add() / addAll() æ›´æ–°çŠ¶æ€å€¼ï¼Œå°†æ•°æ®æ”¾åˆ°çŠ¶æ€ä¸­
      *      clear() æ¸…é™¤çŠ¶æ€
      */
    object ListStateOperate {

      def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        import org.apache.flink.api.scala._
        env.fromCollection(List(
          (1L, 3d),
          (1L, 5d),
          (1L, 7d),
          (2L, 4d),
          (2L, 2d),
          (2L, 6d)
        )).keyBy(_._1)
          .flatMap(new CountAverageWithList)
          .print()
        env.execute()
      }
    }



    class CountAverageWithList extends RichFlatMapFunction[(Long,Double),(Long,Double)]{
      //å®šä¹‰æˆ‘ä»¬å†å²æ‰€æœ‰çš„æ•°æ®è·å–
      private var elementsByKey: ListState[(Long,Double)] = _

      override def open(parameters: Configuration): Unit = {
        //åˆå§‹åŒ–è·å–å†å²çŠ¶æ€çš„å€¼ï¼Œæ¯ä¸ªkeyå¯¹åº”çš„æ‰€æœ‰å†å²å€¼ï¼Œéƒ½å­˜å‚¨åœ¨listé›†åˆé‡Œé¢äº†
        val listState = new ListStateDescriptor[(Long,Double)]("listState",classOf[(Long,Double)])
        elementsByKey = getRuntimeContext.getListState(listState)

      }

      override def flatMap(element: (Long, Double), out: Collector[(Long, Double)]): Unit = {
        //è·å–å½“å‰keyçš„çŠ¶æ€å€¼
       val currentState: lang.Iterable[(Long, Double)] = elementsByKey.get()

        //å¦‚æœåˆå§‹çŠ¶æ€ä¸ºç©ºï¼Œé‚£ä¹ˆå°±è¿›è¡Œåˆå§‹åŒ–ï¼Œæ„é€ ä¸€ä¸ªç©ºçš„é›†åˆå‡ºæ¥ï¼Œå‡†å¤‡ç”¨äºå­˜å‚¨åç»­çš„æ•°æ®
        if(currentState == null){
          elementsByKey.addAll(Collections.emptyList())
        }
        //æ·»åŠ å…ƒç´ 
        elementsByKey.add(element)
        import scala.collection.JavaConverters._
        val allElements: Iterator[(Long, Double)] = elementsByKey.get().iterator().asScala
          
        val allElementList: List[(Long, Double)] = allElements.toList
        if(allElementList.size >= 3){
          var count = 0L
          var sum = 0d
          for(eachElement <- allElementList){
            count +=1
            sum += eachElement._2
          }
          out.collect((element._1,sum/count))
        }
      }
    }

  ```

###### 1.3.3 MapState

* ä½œç”¨

  + ç”¨äºå°†æ¯ä¸ªkeyå¯¹åº”çš„æ•°æ®éƒ½ä¿å­˜æˆä¸€ä¸ªmapé›†åˆ
* éœ€æ±‚

  + ä½¿ç”¨MapStateæ±‚å–æ¯ä¸ªkeyå¯¹åº”çš„å¹³å‡å€¼
* ä»£ç å¼€å‘

  ```scala
  package com.kaikeba.keystate

  import java.util.UUID

  import org.apache.flink.api.common.functions.RichFlatMapFunction
  import org.apache.flink.api.common.state.{MapState, MapStateDescriptor}
  import org.apache.flink.configuration.Configuration
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import org.apache.flink.util.Collector
  import org.apache.flink.api.scala._

  /**
    * ä½¿ç”¨MapStateæ±‚å–æ¯ä¸ªkeyå¯¹åº”çš„å¹³å‡å€¼
    */
  object MapStateOperate {

    def main(args: Array[String]): Unit = {

      val env = StreamExecutionEnvironment.getExecutionEnvironment

      env.fromCollection(List(
        (1L, 3d),
        (1L, 5d),
        (1L, 7d),
        (2L, 4d),
        (2L, 2d),
        (2L, 6d)
      )).keyBy(_._1)
        .flatMap(new CountAverageMapState)
        .print()
      env.execute()
    }
  }

  class CountAverageMapState extends RichFlatMapFunction[(Long,Double),(Long,Double)]{
    private var mapState:MapState[String,Double] = _

    //åˆå§‹åŒ–è·å–mapStateå¯¹è±¡
    override def open(parameters: Configuration): Unit = {
      val mapStateOperate = new MapStateDescriptor[String,Double]("mapStateOperate",classOf[String],classOf[Double])
      mapState = getRuntimeContext.getMapState(mapStateOperate)
    }

    override def flatMap(input: (Long, Double), out: Collector[(Long, Double)]): Unit = {
      //å°†ç›¸åŒçš„keyå¯¹åº”çš„æ•°æ®æ”¾åˆ°ä¸€ä¸ªmapé›†åˆå½“ä¸­å»ï¼Œå°±æ˜¯è¿™ç§å¯¹åº”  key -> Map((key1, value1),(key2, value2)) 
      //æ¯æ¬¡éƒ½æ„å»ºä¸€ä¸ªmapé›†åˆ
      mapState.put(UUID.randomUUID().toString,input._2)
      import scala.collection.JavaConverters._

      //è·å–mapé›†åˆå½“ä¸­æ‰€æœ‰çš„valueï¼Œæˆ‘ä»¬æ¯æ¬¡å°†æ•°æ®çš„valueç»™æ”¾åˆ°mapçš„valueé‡Œé¢å»
      val listState: List[Double] = mapState.values().iterator().asScala.toList
      if(listState.size >=3){
        var count = 0L
        var sum = 0d
        for(eachState <- listState){
          count +=1
          sum += eachState
        }
        out.collect(input._1,sum/count)
      }
    }
  }

  ```

###### 1.3.4 ReducingState

* ä½œç”¨

  + ç”¨äºæ•°æ®çš„èšåˆ
* éœ€æ±‚

  + ä½¿ç”¨ReducingStateæ±‚å–æ¯ä¸ªkeyå¯¹åº”çš„å¹³å‡å€¼
* ä»£ç å¼€å‘

  ```scala
  package com.kaikeba.keystate

  import org.apache.flink.api.common.functions.{ReduceFunction, RichFlatMapFunction}
  import org.apache.flink.api.common.state.{ReducingState, ReducingStateDescriptor}
  import org.apache.flink.configuration.Configuration
  import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
  import org.apache.flink.util.Collector

  /**
    * ReducingState<T> ï¼šè¿™ä¸ªçŠ¶æ€ä¸ºæ¯ä¸€ä¸ª key ä¿å­˜ä¸€ä¸ªèšåˆä¹‹åçš„å€¼
    * get() è·å–çŠ¶æ€å€¼
    * add()  æ›´æ–°çŠ¶æ€å€¼ï¼Œå°†æ•°æ®æ”¾åˆ°çŠ¶æ€ä¸­
    * clear() æ¸…é™¤çŠ¶æ€
    */

  object ReduceingStateOperate {
    def main(args: Array[String]): Unit = {
      val env = StreamExecutionEnvironment.getExecutionEnvironment
      import org.apache.flink.api.scala._
      env.fromCollection(List(
        (1L, 3d),
        (1L, 5d),
        (1L, 7d),
        (2L, 4d),
        (2L, 2d),
        (2L, 6d)
      )).keyBy(_._1)
        .flatMap(new CountAverageReduceStage)
        .print()

      env.execute()
    }
  }
    class CountAverageReduceStage extends RichFlatMapFunction[(Long,Double),(Long,Double)]{
        
         //å®šä¹‰ReducingState
          private var reducingState:ReducingState[Double] = _

          //å®šä¹‰ä¸€ä¸ªè®¡æ•°å™¨
          var counter=0L
    override def open(parameters: Configuration): Unit = {

    val reduceSum = new ReducingStateDescriptor[Double]("reduceSum", new ReduceFunction[ Double] {
      override def reduce(value1: Double, value2: Double): Double = {
        value1+ value2
      }
    }, classOf[Double])

    //åˆå§‹åŒ–è·å–reducingStateå¯¹è±¡
    reducingState = getRuntimeContext.getReducingState[Double](reduceSum)

  }
  override def flatMap(input: (Long, Double), out: Collector[(Long, Double)]): Unit = {
    //è®¡æ•°å™¨+1
    counter+=1

    //æ·»åŠ æ•°æ®åˆ°reducingState
    reducingState.add(input._2)

    out.collect(input._1,reducingState.get()/counter)
  	}
  }

  ```

###### 1.3.5 AggregatingState

* ä½œç”¨

  + å°†ç›¸åŒkeyçš„æ•°æ®è¿›è¡Œèšåˆ
* éœ€æ±‚

  + å°†ç›¸åŒkeyçš„æ•°æ®èšåˆæˆä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
* ä»£ç å¼€å‘

  ```scala
  package com.kaikeba.keystate

    import org.apache.flink.api.common.functions.{AggregateFunction, RichFlatMapFunction}
    import org.apache.flink.api.common.state.{AggregatingState, AggregatingStateDescriptor}
    import org.apache.flink.configuration.Configuration
    import org.apache.flink.runtime.state.memory.MemoryStateBackend
    import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
    import org.apache.flink.util.Collector
    import org.apache.flink.api.scala._
    /**
      * å°†ç›¸åŒkeyçš„æ•°æ®èšåˆæˆä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
      */
    object AggregrageStateOperate {

      def main(args: Array[String]): Unit = {

          val env = StreamExecutionEnvironment.getExecutionEnvironment

          env.fromCollection(List(
            (1L, 3d),
            (1L, 5d),
            (1L, 7d),
            (2L, 4d),
            (2L, 2d),
            (2L, 6d)
          )).keyBy(_._1)
            .flatMap(new AggregrageState)
            .print()

          env.execute()
        }
      }

      /**
        *   (1L, 3d),
            (1L, 5d),
            (1L, 7d),   æŠŠç›¸åŒkeyçš„valueæ‹¼æ¥å­—ç¬¦ä¸²ï¼šContains-3-5-7
        */
      class AggregrageState extends RichFlatMapFunction[(Long,Double),(Long,String)]{

         //å®šä¹‰AggregatingState
        private var aggregateTotal:AggregatingState[Double, String] = _

        override def open(parameters: Configuration): Unit = {
          /**
            * name: String,
            * aggFunction: AggregateFunction[IN, ACC, OUT],
            * stateType: Class[ACC]
            */
          val aggregateStateDescriptor = new AggregatingStateDescriptor[Double, String, String]("aggregateState", new AggregateFunction[Double, String, String] {
                 //åˆ›å»ºä¸€ä¸ªåˆå§‹å€¼
                override def createAccumulator(): String = {
                  "Contains"
                }

                //å¯¹æ•°æ®è¿›è¡Œç´¯åŠ 
                override def add(value: Double, accumulator: String): String = {
                  accumulator + "-" + value
            }

               //è·å–ç´¯åŠ çš„ç»“æœ
              override def getResult(accumulator: String): String = {
                  accumulator
              }

              //æ•°æ®åˆå¹¶çš„è§„åˆ™
              override def merge(a: String, b: String): String = {
                  a + "-" + b
              }
          }, classOf[String])

          //è·å–AggregatingStateå¯¹è±¡
            aggregateTotal = getRuntimeContext.getAggregatingState(aggregateStateDescriptor)
        }

        override def flatMap(input: (Long, Double), out: Collector[(Long, String)]): Unit = {
            aggregateTotal.add(input._2)
            out.collect(input._1,aggregateTotal.get())
        }

    }


  ```

##### 1.4 Operator Stateæ¡ˆä¾‹æ¼”ç¤º

* éœ€æ±‚
  + å®ç°æ¯ä¸¤æ¡æ•°æ®è¿›è¡Œè¾“å‡ºæ‰“å°ä¸€æ¬¡ï¼Œä¸ç”¨åŒºåˆ†æ•°æ®çš„key
  + è¿™é‡Œä½¿ç”¨ListStateå®ç°

    ```scala
    package com.kaikeba.operatorstate

    import org.apache.flink.streaming.api.functions.sink.SinkFunction
    import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}

    import scala.collection.mutable.ListBuffer

    /**
      * å®ç°æ¯ä¸¤æ¡æ•°æ®è¿›è¡Œè¾“å‡ºæ‰“å°ä¸€æ¬¡ï¼Œä¸ç”¨åŒºåˆ†æ•°æ®çš„key
      */
    object OperatorListState {
      def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        import org.apache.flink.api.scala._
        val sourceStream: DataStream[(String, Int)] = env.fromCollection(List(
            ("spark", 3),
            ("hadoop", 5),
            ("hive", 7),
            ("flume", 9)
        ))

        sourceStream.addSink(new OperateTaskState).setParallelism(1)

        env.execute()
      }

    }

    class OperateTaskState extends SinkFunction[(String,Int)]{
      //å®šä¹‰ä¸€ä¸ªlist ç”¨äºæˆ‘ä»¬æ¯ä¸¤æ¡æ•°æ®æ‰“å°ä¸€ä¸‹
      private var listBuffer:ListBuffer[(String,Int)] = new ListBuffer[(String, Int)]

      override def invoke(value: (String, Int), context: SinkFunction.Context[_]): Unit = {
        listBuffer.+=(value)

        if(listBuffer.size ==2){
           println(listBuffer)

          //æ¸…ç©ºstateçŠ¶æ€
          listBuffer.clear()
        }
      }

    }




    ```

### ğŸ“– 2. Flinkçš„çŠ¶æ€ç®¡ç†ä¹‹State Backend

* é»˜è®¤æƒ…å†µä¸‹ï¼Œstateä¼šä¿å­˜åœ¨taskmanagerçš„å†…å­˜ä¸­ï¼Œcheckpointä¼šå­˜å‚¨åœ¨JobManagerçš„å†…å­˜ä¸­ã€‚state çš„å­˜å‚¨å’Œcheckpointçš„ä½ç½®å–å†³äºState Backendçš„é…ç½®ã€‚
* Flinkä¸€å…±æä¾›äº†3ç§StateBackend
  + MemoryStateBackend
    - åŸºäºå†…å­˜å­˜å‚¨
  + FsStateBackend
    - åŸºäºæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨
  + RocksDBStateBackend
    - åŸºäºæ•°æ®åº“å­˜å‚¨
* å¯ä»¥é€šè¿‡ ==StreamExecutionEnvironment.setStateBackend(â€¦)==æ¥è®¾ç½®stateå­˜å‚¨çš„ä½ç½®

##### 2.1 MemoryStateBackend

```
	å°†æ•°æ®æŒä¹…åŒ–çŠ¶æ€å­˜å‚¨åˆ°å†…å­˜å½“ä¸­ï¼Œstateæ•°æ®ä¿å­˜åœ¨javaå †å†…å­˜ä¸­ï¼Œ
	æ‰§è¡Œcheckpointçš„æ—¶å€™ï¼Œä¼šæŠŠstateçš„å¿«ç…§æ•°æ®ä¿å­˜åˆ°jobmanagerçš„å†…å­˜ä¸­ã€‚
	åŸºäºå†…å­˜çš„state backendåœ¨ç”Ÿäº§ç¯å¢ƒä¸‹ä¸å»ºè®®ä½¿ç”¨ã€‚

```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/27e4b9cc4ddc4517b0eb43b1ee5c883b.png)

* ä»£ç é…ç½®ï¼š

```scala
environment.setStateBackend(new MemoryStateBackend())

```

* ä½¿ç”¨åœºæ™¯ï¼š

```
ï¼ˆ1ï¼‰æœ¬åœ°è°ƒè¯•
ï¼ˆ2ï¼‰flinkä»»åŠ¡çŠ¶æ€æ•°æ®é‡è¾ƒå°çš„åœºæ™¯

```

##### 2.2 FsStateBackend

```
	stateæ•°æ®ä¿å­˜åœ¨taskmanagerçš„å†…å­˜ä¸­ï¼Œ
	æ‰§è¡Œcheckpointçš„æ—¶å€™ï¼Œä¼šæŠŠstateçš„å¿«ç…§æ•°æ®ä¿å­˜åˆ°é…ç½®çš„æ–‡ä»¶ç³»ç»Ÿä¸­ã€‚
	å¯ä»¥ä½¿ç”¨hdfsç­‰åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ.
	
	FsStateBackend é€‚åˆåœºæ™¯ï¼šçŠ¶æ€æ•°æ®ç‰¹åˆ«çš„å¤šï¼Œè¿˜æœ‰é•¿æ—¶é—´çš„windowç®—å­ç­‰ï¼Œå®ƒå¾ˆå®‰å…¨ï¼Œå› ä¸ºåŸºäºhdfsï¼Œæ‰€ä»¥æ•°æ®æœ‰å¤‡ä»½å¾ˆå®‰å…¨ã€‚

```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/09e2a888ee9e496688f236c6fcd139d7.png)

* ä»£ç é…ç½®ï¼š

```scala
environment.setStateBackend(new FsStateBackend("hdfs://node01:8020/flink/checkDir"))

```

* é€‚ç”¨åœºæ™¯ï¼š

```
ï¼ˆ1ï¼‰å¤§çŠ¶æ€ã€é•¿çª—å£ã€å¤§key/valueçŠ¶æ€çš„çš„ä»»åŠ¡
ï¼ˆ2ï¼‰å…¨é«˜å¯ç”¨é…ç½®

```

##### 2.3 RocksDBStateBackend ï¼ˆç”Ÿäº§ä¸­æ¨èï¼‰

```
RocksDBä»‹ç»ï¼š
	RocksDBä½¿ç”¨ä¸€å¥—æ—¥å¿—ç»“æ„çš„æ•°æ®åº“å¼•æ“ï¼Œ
	å®ƒæ˜¯Flinkä¸­å†…ç½®çš„ç¬¬ä¸‰æ–¹çŠ¶æ€ç®¡ç†å™¨,ä¸ºäº†æ›´å¥½çš„æ€§èƒ½ï¼Œè¿™å¥—å¼•æ“æ˜¯ç”¨C++ç¼–å†™çš„ã€‚ 
	Keyå’Œvalueæ˜¯ä»»æ„å¤§å°çš„å­—èŠ‚æµã€‚
	RocksDBè·Ÿä¸Šé¢çš„éƒ½ç•¥æœ‰ä¸åŒï¼Œå®ƒä¼šåœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­ç»´æŠ¤çŠ¶æ€ï¼Œstateä¼šç›´æ¥å†™å…¥æœ¬åœ°rocksdbä¸­ã€‚
	åŒæ—¶å®ƒéœ€è¦é…ç½®ä¸€ä¸ªè¿œç«¯çš„filesystem uriï¼ˆä¸€èˆ¬æ˜¯HDFSï¼‰ï¼Œåœ¨åšcheckpointçš„æ—¶å€™ï¼Œä¼šæŠŠæœ¬åœ°çš„æ•°æ®ç›´æ¥å¤åˆ¶åˆ°fileSystemä¸­ã€‚
	fail overçš„æ—¶å€™ä»fileSystemä¸­æ¢å¤åˆ°æœ¬åœ°RocksDBå…‹æœäº†stateå—å†…å­˜é™åˆ¶çš„ç¼ºç‚¹ï¼ŒåŒæ—¶åˆèƒ½å¤ŸæŒä¹…åŒ–åˆ°è¿œç«¯æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œæ¯”è¾ƒé€‚åˆåœ¨ç”Ÿäº§ä¸­ä½¿ç”¨.

```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b4675ce8b5ca485ca906877aadce5600.png)

* ä»£ç é…ç½®ï¼šå¯¼å…¥jaråŒ…ç„¶åé…ç½®ä»£ç 

```xml
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-statebackend-rocksdb_2.11</artifactId>
    <version>1.9.2</version>
</dependency>

```

* é…ç½®ä»£ç 

```scala
environment.setStateBackend(new RocksDBStateBackend("hdfs://node01:8020/flink/checkDir",true))

```

* ä½¿ç”¨åœºæ™¯

```
ï¼ˆ1ï¼‰å¤§çŠ¶æ€ã€é•¿çª—å£ã€å¤§key/valueçŠ¶æ€çš„çš„ä»»åŠ¡
ï¼ˆ2ï¼‰å…¨é«˜å¯ç”¨é…ç½®

    ç”±äºRocksDBStateBackendå°†å·¥ä½œçŠ¶æ€å­˜å‚¨åœ¨taskMangerçš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿï¼ŒçŠ¶æ€æ•°é‡ä»…ä»…å—é™äºæœ¬åœ°ç£ç›˜å®¹é‡é™åˆ¶ï¼Œå¯¹æ¯”äºFsStateBackendä¿å­˜å·¥ä½œçŠ¶æ€åœ¨å†…å­˜ä¸­ï¼ŒRocksDBStateBackendèƒ½é¿å…flinkä»»åŠ¡æŒç»­è¿è¡Œå¯èƒ½å¯¼è‡´çš„çŠ¶æ€æ•°é‡æš´å¢è€Œå†…å­˜ä¸è¶³çš„æƒ…å†µï¼Œå› æ­¤é€‚åˆåœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚


```

##### 2.4 ä¿®æ”¹state-backendçš„ä¸¤ç§æ–¹å¼

* ç¬¬ä¸€ç§ï¼šå•ä»»åŠ¡è°ƒæ•´
  + ä¿®æ”¹å½“å‰ä»»åŠ¡ä»£ç 

```scala
env.setStateBackend(
new FsStateBackend("hdfs://node01:8020/flink/checkDir"))
æˆ–è€…new MemoryStateBackend()
æˆ–è€…new RocksDBStateBackend(filebackend, true);ã€éœ€è¦æ·»åŠ ç¬¬ä¸‰æ–¹ä¾èµ–ã€‘

```

* ç¬¬äºŒç§ï¼šå…¨å±€è°ƒæ•´

  + ä¿®æ”¹flink-conf.yaml

  ```yaml
  state.backend: filesystem
  state.checkpoints.dir: hdfs://node01:8020/flink/checkDir

  ```

  + æ³¨æ„ï¼šstate.backendçš„å€¼å¯ä»¥æ˜¯ä¸‹é¢å‡ ç§

  ```
  (1) jobmanager    è¡¨ç¤ºä½¿ç”¨ MemoryStateBackend
  (2) filesystem    è¡¨ç¤ºä½¿ç”¨ FsStateBackend
  (3) rocksdb       è¡¨ç¤ºä½¿ç”¨ RocksDBStateBackend

  ```

### ğŸ“– 3. Flinkçš„checkPointä¿å­˜æ•°æ®å®ç°å®¹é”™

##### 3.1 checkPointçš„åŸºæœ¬æ¦‚å¿µ

```
ä¸ºäº†ä¿è¯stateçš„å®¹é”™æ€§ï¼ŒFlinkéœ€è¦å¯¹stateè¿›è¡Œcheckpointã€‚

	Checkpointæ˜¯Flinkå®ç°å®¹é”™æœºåˆ¶æœ€æ ¸å¿ƒçš„åŠŸèƒ½ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®é…ç½®å‘¨æœŸæ€§åœ°åŸºäºStreamä¸­å„ä¸ªOperator/taskçš„çŠ¶æ€æ¥ç”Ÿæˆå¿«ç…§ï¼Œä»è€Œå°†è¿™äº›çŠ¶æ€æ•°æ®å®šæœŸæŒä¹…åŒ–å­˜å‚¨ä¸‹æ¥ï¼Œå½“Flinkç¨‹åºä¸€æ—¦æ„å¤–å´©æºƒæ—¶ï¼Œé‡æ–°è¿è¡Œç¨‹åºæ—¶å¯ä»¥æœ‰é€‰æ‹©åœ°ä»è¿™äº›å¿«ç…§è¿›è¡Œæ¢å¤ï¼Œä»è€Œä¿®æ­£å› ä¸ºæ•…éšœå¸¦æ¥çš„ç¨‹åºæ•°æ®å¼‚å¸¸ã€‚


```

##### 3.2 checkPointçš„å‰æ

* Flinkçš„checkpointæœºåˆ¶å¯ä»¥ä¸(streamå’Œstate)çš„æŒä¹…åŒ–å­˜å‚¨äº¤äº’çš„å‰æ
  + 1ã€æŒä¹…åŒ–çš„sourceï¼Œå®ƒéœ€è¦æ”¯æŒåœ¨ä¸€å®šæ—¶é—´å†…é‡æ”¾äº‹ä»¶ã€‚è¿™ç§sourcesçš„å…¸å‹ä¾‹å­æ˜¯æŒä¹…åŒ–çš„æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆæ¯”å¦‚Apache Kafkaï¼ŒRabbitMQç­‰ï¼‰æˆ–æ–‡ä»¶ç³»ç»Ÿï¼ˆæ¯”å¦‚HDFSï¼ŒS3ï¼ŒGFSç­‰ï¼‰
  + 2ã€ç”¨äºstateçš„æŒä¹…åŒ–å­˜å‚¨ï¼Œä¾‹å¦‚åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼ˆæ¯”å¦‚HDFSï¼ŒS3ï¼ŒGFSç­‰ï¼‰

##### 3.3 Flinkè¿›è¡Œcheckpointæ­¥éª¤

* ï¼ˆ1ï¼‰æš‚åœæ–°æ•°æ®çš„è¾“å…¥
* ï¼ˆ2ï¼‰ç­‰å¾…æµä¸­on-the-flyçš„æ•°æ®è¢«å¤„ç†å¹²å‡€ï¼Œæ­¤æ—¶å¾—åˆ°flink graphçš„ä¸€ä¸ªsnapshot
* ï¼ˆ3ï¼‰å°†æ‰€æœ‰Taskä¸­çš„Stateæ‹·è´åˆ°State Backendä¸­ï¼Œå¦‚HDFSã€‚æ­¤åŠ¨ä½œç”±å„ä¸ªTask Managerå®Œæˆ
* ï¼ˆ4ï¼‰å„ä¸ªTask Managerå°†Task Stateçš„ä½ç½®ä¸ŠæŠ¥ç»™Job Managerï¼Œå®Œæˆcheckpoint
* ï¼ˆ5ï¼‰æ¢å¤æ•°æ®çš„è¾“å…¥

> å¦‚ä¸Šæ‰€è¿°ï¼Œè¿™é‡Œæ‰éœ€è¦â€œæš‚åœè¾“å…¥ + æ’å¹²on-the-fly æ•°æ®â€çš„æ“ä½œï¼Œè¿™æ ·æ‰èƒ½æ‹¿åˆ°åŒä¸€æ—¶åˆ»ä¸‹æ‰€æœ‰subtaskçš„state

##### 3.4 é…ç½®checkPoint

* é»˜è®¤checkpointåŠŸèƒ½æ˜¯disabledçš„ï¼Œæƒ³è¦ä½¿ç”¨çš„æ—¶å€™éœ€è¦å…ˆå¯ç”¨
* checkpointå¼€å¯ä¹‹åï¼Œé»˜è®¤çš„checkPointModeæ˜¯Exactly-once
* checkpointçš„checkPointModeæœ‰ä¸¤ç§

  + Exactly-once: æ•°æ®å¤„ç†ä¸”åªè¢«å¤„ç†ä¸€æ¬¡
  + At-least-onceï¼šæ•°æ®è‡³å°‘è¢«å¤„ç†ä¸€æ¬¡

Exactly-onceå¯¹äºå¤§å¤šæ•°åº”ç”¨æ¥è¯´æ˜¯æœ€åˆé€‚çš„ã€‚At-least-onceå¯èƒ½ç”¨åœ¨æŸäº›å»¶è¿Ÿè¶…ä½çš„åº”ç”¨ç¨‹åºï¼ˆå§‹ç»ˆå»¶è¿Ÿä¸ºå‡ æ¯«ç§’ï¼‰

```scala
//é»˜è®¤checkpointåŠŸèƒ½æ˜¯disabledçš„ï¼Œæƒ³è¦ä½¿ç”¨çš„æ—¶å€™éœ€è¦å…ˆå¯ç”¨
// æ¯éš”1000 msè¿›è¡Œå¯åŠ¨ä¸€ä¸ªæ£€æŸ¥ç‚¹ã€è®¾ç½®checkpointçš„å‘¨æœŸã€‘
environment.enableCheckpointing(1000);
// é«˜çº§é€‰é¡¹ï¼š
// è®¾ç½®æ¨¡å¼ä¸ºexactly-once ï¼ˆè¿™æ˜¯é»˜è®¤å€¼ï¼‰
environment.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
// ç¡®ä¿æ£€æŸ¥ç‚¹ä¹‹é—´æœ‰è‡³å°‘500 msçš„é—´éš”ã€checkpointæœ€å°é—´éš”ã€‘
environment.getCheckpointConfig.setMinPauseBetweenCheckpoints(500);
// æ£€æŸ¥ç‚¹å¿…é¡»åœ¨ä¸€åˆ†é’Ÿå†…å®Œæˆï¼Œæˆ–è€…è¢«ä¸¢å¼ƒã€checkpointçš„è¶…æ—¶æ—¶é—´ã€‘
environment.getCheckpointConfig.setCheckpointTimeout(60000);
// åŒä¸€æ—¶é—´åªå…è®¸è¿›è¡Œä¸€ä¸ªæ£€æŸ¥ç‚¹
environment.getCheckpointConfig.setMaxConcurrentCheckpoints(1);
// è¡¨ç¤ºä¸€æ—¦Flinkå¤„ç†ç¨‹åºè¢«cancelåï¼Œä¼šä¿ç•™Checkpointæ•°æ®ï¼Œä»¥ä¾¿æ ¹æ®å®é™…éœ€è¦æ¢å¤åˆ°æŒ‡å®šçš„Checkpointã€è¯¦ç»†è§£é‡Šè§å¤‡æ³¨ã€‘

/**
  * ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION:è¡¨ç¤ºä¸€æ—¦Flinkå¤„ç†ç¨‹åºè¢«cancelåï¼Œä¼šä¿ç•™Checkpointæ•°æ®ï¼Œä»¥ä¾¿æ ¹æ®å®é™…éœ€è¦æ¢å¤åˆ°æŒ‡å®šçš„Checkpoint
  * ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION: è¡¨ç¤ºä¸€æ—¦Flinkå¤„ç†ç¨‹åºè¢«cancelåï¼Œä¼šåˆ é™¤Checkpointæ•°æ®ï¼Œåªæœ‰jobæ‰§è¡Œå¤±è´¥çš„æ—¶å€™æ‰ä¼šä¿å­˜checkpoint
  */
environment.getCheckpointConfig.enableExternalizedCheckpoints(ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);


```

##### â­ï¸3.5 é‡å¯ç­–ç•¥æ¦‚è¿°

* Flinkæ”¯æŒä¸åŒçš„é‡å¯ç­–ç•¥ï¼Œä»¥åœ¨æ•…éšœå‘ç”Ÿæ—¶æ§åˆ¶ä½œä¸šå¦‚ä½•é‡å¯ï¼Œé›†ç¾¤åœ¨å¯åŠ¨æ—¶ä¼šä¼´éšä¸€ä¸ªé»˜è®¤çš„é‡å¯ç­–ç•¥ï¼Œåœ¨æ²¡æœ‰å®šä¹‰å…·ä½“é‡å¯ç­–ç•¥æ—¶ä¼šä½¿ç”¨è¯¥é»˜è®¤ç­–ç•¥ã€‚
* å¦‚æœåœ¨å·¥ä½œæäº¤æ—¶æŒ‡å®šäº†ä¸€ä¸ªé‡å¯ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä¼šè¦†ç›–é›†ç¾¤çš„é»˜è®¤ç­–ç•¥ï¼Œé»˜è®¤çš„é‡å¯ç­–ç•¥å¯ä»¥é€šè¿‡ Flink çš„é…ç½®æ–‡ä»¶ flink-conf.yaml æŒ‡å®šã€‚é…ç½®å‚æ•° restart-strategy å®šä¹‰äº†å“ªä¸ªç­–ç•¥è¢«ä½¿ç”¨ã€‚
* å¸¸ç”¨çš„é‡å¯ç­–ç•¥
  + ï¼ˆ1ï¼‰å›ºå®šé—´éš” (Fixed delay)
  + ï¼ˆ2ï¼‰å¤±è´¥ç‡ (Failure rate)
  + ï¼ˆ3ï¼‰æ— é‡å¯ (No restart)
* å¦‚æœæ²¡æœ‰å¯ç”¨ checkpointingï¼Œåˆ™ä½¿ç”¨æ— é‡å¯ (no restart) ç­–ç•¥ã€‚
* å¦‚æœå¯ç”¨äº† checkpointingï¼Œé‡å¯ç­–ç•¥å¯ä»¥åœ¨
  flink-conf.yaml
  ä¸­é…ç½®ï¼Œè¡¨ç¤ºå…¨å±€çš„é…ç½®ã€‚ä¹Ÿå¯ä»¥åœ¨åº”ç”¨ä»£ç ä¸­åŠ¨æ€æŒ‡å®šï¼Œä¼šè¦†ç›–å…¨å±€é…ç½®
  + ä½†æ²¡æœ‰é…ç½®é‡å¯ç­–ç•¥ï¼Œåˆ™ä½¿ç”¨å›ºå®šé—´éš” (fixed-delay) ç­–ç•¥ï¼Œ å°è¯•é‡å¯æ¬¡æ•°é»˜è®¤å€¼æ˜¯ï¼šInteger.MAX\_VALUEï¼Œã€‚

##### 3.6 é‡å¯ç­–ç•¥é…ç½®å®ç°

* å›ºå®šé—´éš” (Fixed delay)

```properties
ç¬¬ä¸€ç§ï¼šå…¨å±€é…ç½® flink-conf.yaml
restart-strategy: fixed-delay
restart-strategy.fixed-delay.attempts: 3
restart-strategy.fixed-delay.delay: 10 s

ç¬¬äºŒç§ï¼šåº”ç”¨ä»£ç è®¾ç½®
	//é‡å¯æ¬¡æ•°ã€é‡å¯æ—¶é—´é—´éš”
environment.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,10000))


```

* å¤±è´¥ç‡ (Failure rate)

```properties
ç¬¬ä¸€ç§ï¼šå…¨å±€é…ç½® flink-conf.yaml
//5åˆ†é’Ÿå†…è‹¥å¤±è´¥äº†3æ¬¡åˆ™è®¤ä¸ºè¯¥jobå¤±è´¥ï¼Œé‡è¯•é—´éš”ä¸º10s
restart-strategy: failure-rate
restart-strategy.failure-rate.max-failures-per-interval: 3
restart-strategy.failure-rate.failure-rate-interval: 5 min
restart-strategy.failure-rate.delay: 10 s


ç¬¬äºŒç§ï¼šåº”ç”¨ä»£ç è®¾ç½®
environment.setRestartStrategy(RestartStrategies.failureRateRestart(3, org.apache.flink.api.common.time.Time.seconds(100), org.apache.flink.api.common.time.Time.seconds(10)))


```

* æ— é‡å¯ (No restart)

```properties
ç¬¬ä¸€ç§ï¼šå…¨å±€é…ç½® flink-conf.yaml
restart-strategy: none

ç¬¬äºŒç§ï¼šåº”ç”¨ä»£ç è®¾ç½®
environment.setRestartStrategy(RestartStrategies.noRestart())


```

### â­ï¸ğŸ“– 4. ä»checkPointæ¢å¤æ•°æ®ä»¥åŠcheckPointä¿å­˜å¤šä¸ªå†å²ç‰ˆæœ¬

##### 4.1 ä¿å­˜å¤šä¸ªå†å²ç‰ˆæœ¬

* é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœè®¾ç½®äº†Checkpointé€‰é¡¹ï¼Œåˆ™Flinkåªä¿ç•™æœ€è¿‘æˆåŠŸç”Ÿæˆçš„1ä¸ªCheckpointï¼Œè€Œå½“Flinkç¨‹åºå¤±è´¥æ—¶ï¼Œå¯ä»¥ä»æœ€è¿‘çš„è¿™ä¸ªCheckpointæ¥è¿›è¡Œæ¢å¤ã€‚
* å¦‚æœæˆ‘ä»¬å¸Œæœ›ä¿ç•™å¤šä¸ªCheckpointï¼Œå¹¶èƒ½å¤Ÿæ ¹æ®å®é™…éœ€è¦
  é€‰æ‹©å…¶ä¸­ä¸€ä¸ªè¿›è¡Œæ¢å¤
  ï¼Œè¿™æ ·ä¼šæ›´åŠ çµæ´»ï¼Œæ¯”å¦‚ï¼Œæˆ‘ä»¬å‘ç°æœ€è¿‘4ä¸ªå°æ—¶æ•°æ®è®°å½•å¤„ç†æœ‰é—®é¢˜ï¼Œå¸Œæœ›å°†æ•´ä¸ªçŠ¶æ€è¿˜åŸåˆ°4å°æ—¶ä¹‹å‰
* Flinkå¯ä»¥æ”¯æŒä¿ç•™å¤šä¸ªCheckpointï¼Œéœ€è¦åœ¨Flinkçš„é…ç½®æ–‡ä»¶
  conf/flink-conf.yaml
  ä¸­ï¼Œæ·»åŠ å¦‚ä¸‹é…ç½®ï¼ŒæŒ‡å®šæœ€å¤šéœ€è¦ä¿å­˜Checkpointçš„ä¸ªæ•°ã€‚

```yaml
state.checkpoints.num-retained: 20

```

* è¿™æ ·è®¾ç½®ä»¥åå°±æŸ¥çœ‹å¯¹åº”çš„Checkpointåœ¨HDFSä¸Šå­˜å‚¨çš„æ–‡ä»¶ç›®å½•

```yaml
hdfs dfs -ls hdfs://node01:8020/flink/checkpoints

```

* å¦‚æœå¸Œæœ›å›é€€åˆ°æŸä¸ªCheckpointç‚¹ï¼Œåªéœ€è¦æŒ‡å®šå¯¹åº”çš„æŸä¸ªCheckpointè·¯å¾„å³å¯å®ç°

##### 4.2 æ¢å¤å†å²æŸä¸ªç‰ˆæœ¬æ•°æ®

* å¦‚æœFlinkç¨‹åºå¼‚å¸¸å¤±è´¥ï¼Œæˆ–è€…æœ€è¿‘ä¸€æ®µæ—¶é—´å†…æ•°æ®å¤„ç†é”™è¯¯ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç¨‹åºä»æŸä¸€ä¸ªCheckpointç‚¹è¿›è¡Œæ¢å¤

```scala
flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 -s hdfs://node01:8020/fsStateBackend/971ae7ac4d5f20e704747ea7c549b356/chk-50/_metadata -c com.kaikeba.checkpoint.TestCheckPoint original-flink_study-1.0-SNAPSHOT.jar

```

* ç¨‹åºæ­£å¸¸è¿è¡Œåï¼Œè¿˜ä¼šæŒ‰ç…§Checkpointé…ç½®è¿›è¡Œè¿è¡Œï¼Œç»§ç»­ç”ŸæˆCheckpointæ•°æ®

### ğŸ“– 5. Flinkçš„savePointä¿å­˜æ•°æ®

##### 5.1 savePointçš„ä»‹ç»

* savePointæ˜¯æ£€æŸ¥ç‚¹ä¸€ç§ç‰¹æ®Šå®ç°ï¼Œåº•å±‚å…¶å®ä¹Ÿæ˜¯ä½¿ç”¨Checkpointsçš„æœºåˆ¶ã€‚
* savePointæ˜¯ç”¨æˆ·ä»¥æ‰‹å·¥å‘½ä»¤çš„æ–¹å¼è§¦å‘checkpointï¼Œå¹¶å°†ç»“æœæŒä¹…åŒ–åˆ°æŒ‡å®šçš„å­˜å‚¨ç›®å½•ä¸­
* ä½œç”¨

  + 1ã€åº”ç”¨ç¨‹åºä»£ç å‡çº§
    - é€šè¿‡è§¦å‘ä¿å­˜ç‚¹å¹¶ä»è¯¥ä¿å­˜ç‚¹å¤„è¿è¡Œæ–°ç‰ˆæœ¬ï¼Œä¸‹æ¸¸çš„åº”ç”¨ç¨‹åºå¹¶ä¸ä¼šå¯Ÿè§‰åˆ°ä¸åŒ
  + 2ã€Flinkç‰ˆæœ¬æ›´æ–°
    - Flink è‡ªèº«çš„æ›´æ–°ä¹Ÿå˜å¾—ç®€å•ï¼Œå› ä¸ºå¯ä»¥é’ˆå¯¹æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡è§¦å‘ä¿å­˜ç‚¹ï¼Œå¹¶ä»ä¿å­˜ç‚¹å¤„ç”¨æ–°ç‰ˆæœ¬çš„ Flink é‡å¯ä»»åŠ¡ã€‚
  + 3ã€ç»´æŠ¤å’Œè¿ç§»
    - ä½¿ç”¨ä¿å­˜ç‚¹ï¼Œå¯ä»¥è½»æ¾åœ°â€œæš‚åœå’Œæ¢å¤â€åº”ç”¨ç¨‹åº

##### 5.2 savePointçš„ä½¿ç”¨

* 1ï¼šåœ¨flink-conf.yamlä¸­é…ç½®Savepointå­˜å‚¨ä½ç½®

ä¸æ˜¯å¿…é¡»è®¾ç½®ï¼Œä½†æ˜¯è®¾ç½®åï¼Œåé¢åˆ›å»ºæŒ‡å®šJobçš„Savepointæ—¶ï¼Œå¯ä»¥ä¸ç”¨åœ¨æ‰‹åŠ¨æ‰§è¡Œå‘½ä»¤æ—¶æŒ‡å®šSavepointçš„ä½ç½®

```yaml
state.savepoints.dir: hdfs://node01:8020/flink/savepoints

```

* 2ï¼šè§¦å‘ä¸€ä¸ªsavepoint

  + ï¼ˆ1ï¼‰æ‰‹åŠ¨è§¦å‘savepoint

    ```shell

    #ã€é’ˆå¯¹on standAloneæ¨¡å¼ã€‘
    bin/flink savepoint jobId [targetDirectory] 

    #ã€é’ˆå¯¹on yarnæ¨¡å¼éœ€è¦æŒ‡å®š-yidå‚æ•°ã€‘
    bin/flink savepoint jobId [targetDirectory] [-yid yarnAppId]

    #jobId 				éœ€è¦è§¦å‘savepointçš„jobIdç¼–å·
    #targetDirectory     æŒ‡å®šsavepointå­˜å‚¨æ•°æ®ç›®å½•
    #-yid                æŒ‡å®šyarnAppId 

    ##ä¾‹å¦‚ï¼š
    flink savepoint 8d1bb7f88a486815f9b9cf97c304885b  -yid application_1594807273214_0004

    ```
  + ï¼ˆ2ï¼‰å–æ¶ˆä»»åŠ¡å¹¶æ‰‹åŠ¨è§¦å‘savepoint

    ```shell
    ##ã€é’ˆå¯¹on standAloneæ¨¡å¼ã€‘
    bin/flink cancel -s [targetDirectory] jobId 

    ##ã€é’ˆå¯¹on yarnæ¨¡å¼éœ€è¦æŒ‡å®š-yidå‚æ•°ã€‘
    bin/flink cancel -s [targetDirectory] jobId [-yid yarnAppId]

    ##ä¾‹å¦‚ï¼š
    flink cancel 8d1bb7f88a486815f9b9cf97c304885b -yid application_1594807273214_0004

    ```
* 3ï¼šä»æŒ‡å®šçš„savepointå¯åŠ¨job

  ```shell
  bin/flink run -s savepointPath [runArgs]

  ##ä¾‹å¦‚ï¼š
  flink run -m yarn-cluster -yn 2 -yjm 1024 -ytm 1024 -s hdfs://node01:8020/flink/savepoints/savepoint-8d1bb7-c9187993ca94 -c com.kaikeba.checkpoint.TestCheckPoint original-flink_study-1.0-SNAPSHOT.jar


  ```
* 4ã€æ¸…é™¤savepointæ•°æ®

  ```shell
  bin/flink savepoint -d savepointPath

  ```

### ğŸ“– 6. Flinkæµå¼å¤„ç†é›†æˆkafka

* å¯¹äºå®æ—¶å¤„ç†å½“ä¸­ï¼Œæˆ‘ä»¬å®é™…å·¥ä½œå½“ä¸­çš„æ•°æ®æºä¸€èˆ¬éƒ½æ˜¯ä½¿ç”¨kafkaï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¦‚ä½•é€šè¿‡Flinkæ¥é›†æˆkafka
* Flinkæä¾›äº†ä¸€ä¸ªç‰¹æœ‰çš„kafka connectorå»è¯»å†™kafka topicçš„æ•°æ®ã€‚flinkæ¶ˆè´¹kafkaæ•°æ®ï¼Œå¹¶ä¸æ˜¯å®Œå…¨é€šè¿‡è·Ÿè¸ªkafkaæ¶ˆè´¹ç»„çš„offsetæ¥å®ç°å»ä¿è¯exactly-onceçš„è¯­ä¹‰ï¼Œè€Œæ˜¯flinkå†…éƒ¨å»è·Ÿè¸ªoffsetå’Œåšcheckpointå»å®ç°exactly-onceçš„è¯­ä¹‰ï¼Œè€Œä¸”å¯¹äºkafkaçš„partitionï¼ŒFlinkä¼šå¯åŠ¨å¯¹åº”çš„å¹¶è¡Œåº¦å»å¤„ç†kafkaå½“ä¸­çš„æ¯ä¸ªåˆ†åŒºçš„æ•°æ®
* Flinkæ•´åˆkafkaå®˜ç½‘ä»‹ç»

  + https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html

##### 6.1 å¯¼å…¥pomä¾èµ–

```xml
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka_2.11</artifactId>
    <version>1.9.2</version>
</dependency>
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-statebackend-rocksdb_2.11</artifactId>
    <version>1.9.2</version>
</dependency>
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>1.1.0</version>
</dependency>
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-api</artifactId>
    <version>1.7.25</version>
</dependency>
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>slf4j-log4j12</artifactId>
    <version>1.7.25</version>
</dependency>

```

##### 6.2 å°†kafkaä½œä¸ºflinkçš„sourceæ¥ä½¿ç”¨

* å®é™…å·¥ä½œå½“ä¸­ä¸€èˆ¬éƒ½æ˜¯å°†kafkaä½œä¸ºflinkçš„sourceæ¥ä½¿ç”¨

###### 6.2.1 åˆ›å»ºkafkaçš„topic

* å®‰è£…å¥½kafkaé›†ç¾¤ï¼Œå¹¶å¯åŠ¨kafkaé›†ç¾¤ï¼Œç„¶ååœ¨node01æ‰§è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºkafkaçš„topicä¸ºtest

```shell
kafka-topics.sh --create --partitions 3 --topic test --replication-factor 1 --zookeeper node01:2181,node02:2181,node03:2181

```

###### 6.2.2 ä»£ç å®ç°ï¼š

```scala
package com.kaikeba.kafka

import java.util.Properties

import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import org.apache.flink.streaming.util.serialization.SimpleStringSchema

/**
  *  å°†kafkaä½œä¸ºflinkçš„sourceæ¥ä½¿ç”¨
  */
object FlinkKafkaSource {

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    //**éšå¼è½¬æ¢
    import org.apache.flink.api.scala._
    //checkpoint**é…ç½®
    env.enableCheckpointing(100)
    env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)
    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)
    env.getCheckpointConfig.setCheckpointTimeout(60000)
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)
    env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)
    //è®¾ç½®statebackend
    env.setStateBackend(new RocksDBStateBackend("hdfs://node01:8020/flink_kafka_sink/checkpoints",true));

    val topic = "test"
    val prop = new Properties()
    prop.setProperty("bootstrap.servers","node01:9092,node02:9092,node03:9092")
    prop.setProperty("group.id","con1")
    prop.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    prop.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
    val kafkaConsumer = new FlinkKafkaConsumer[String]("test",new SimpleStringSchema,prop)
    kafkaConsumer.setCommitOffsetsOnCheckpoints(true)
    val kafkaSource: DataStream[String] = env.addSource(kafkaConsumer)
    kafkaSource.print()
    env.execute()


  }
}


```

###### 6.2.3 kafkaç”Ÿäº§æ•°æ®

* node01æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œé€šè¿‡shellå‘½ä»¤è¡Œæ¥ç”Ÿäº§æ•°æ®åˆ°kafkaå½“ä¸­å»

```shell
##åˆ›å»ºtopic
 kafka-topics.sh --create --topic test --partitions 3 --replication-factor 2 --zookeeper node01:2181,node02:2181,node03:2181 

##å‘é€æ•°æ®
kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 --topic  test

```

##### 6.3 å°†kafkaä½œä¸ºflinkçš„sinkæ¥ä½¿ç”¨

* æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†kafkaä½œä¸ºflinkçš„sinkæ¥ä½¿ç”¨ï¼Œå°±æ˜¯å°†flinkå¤„ç†å®Œæˆä¹‹åçš„æ•°æ®å†™å…¥åˆ°kafkaå½“ä¸­å»

###### 6.3.1 socketå‘é€æ•°æ®

* node01æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œä»socketå½“ä¸­å‘é€æ•°æ®

```
 nc -lk 9999

```

###### 6.3.2 ä»£ç å®ç°

```scala
package com.kaikeba.kafka

import java.util.Properties

import org.apache.flink.contrib.streaming.state.RocksDBStateBackend
import org.apache.flink.streaming.api.CheckpointingMode
import org.apache.flink.streaming.api.environment.CheckpointConfig
import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer
import org.apache.flink.streaming.connectors.kafka.internals.KeyedSerializationSchemaWrapper
import org.apache.flink.streaming.util.serialization.SimpleStringSchema

/**
  * å°†kafkaä½œä¸ºflinkçš„sinkæ¥ä½¿ç”¨
  */
object FlinkKafkaSink {

  def main(args: Array[String]): Unit = {
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    //éšå¼è½¬æ¢
    import org.apache.flink.api.scala._
      
    //checkpointé…ç½®
    env.enableCheckpointing(5000);
    env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
    env.getCheckpointConfig.setMinPauseBetweenCheckpoints(500);
    env.getCheckpointConfig.setCheckpointTimeout(60000);
    env.getCheckpointConfig.setMaxConcurrentCheckpoints(1);
    env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);
    //è®¾ç½®statebackend
    env.setStateBackend(new RocksDBStateBackend("hdfs://node01:8020/flink_kafka_sink/checkpoints",true));
    val socketStream = env.socketTextStream("node01",9999)
    val topic = "test"
    val prop = new Properties()
    prop.setProperty("bootstrap.servers","node01:9092,node02:9092,node03:9092")
    prop.setProperty("group.id","kafka_group1")
    //ç¬¬ä¸€ç§è§£å†³æ–¹æ¡ˆï¼Œè®¾ç½®FlinkKafkaProduceré‡Œé¢çš„äº‹åŠ¡è¶…æ—¶æ—¶é—´
    //è®¾ç½®äº‹åŠ¡è¶…æ—¶æ—¶é—´
    prop.setProperty("transaction.timeout.ms",60000*15+"");
      
    //ç¬¬äºŒç§è§£å†³æ–¹æ¡ˆï¼Œè®¾ç½®kafkaçš„æœ€å¤§äº‹åŠ¡è¶…æ—¶æ—¶é—´
    //FlinkKafkaProducer011<String> myProducer = new FlinkKafkaProducer<>(brokerList, topic, new SimpleStringSchema());
    
      //ä½¿ç”¨æ”¯æŒä»…ä¸€æ¬¡è¯­ä¹‰çš„å½¢å¼
    /**
      * defaultTopic: String,
      * serializationSchema: KafkaSerializationSchema[IN],
      * producerConfig: Properties,
      * semantic: FlinkKafkaProducer.Semantic
      */
    val kafkaSink = new FlinkKafkaProducer[String](topic,new KeyedSerializationSchemaWrapper[String](new SimpleStringSchema()), prop,FlinkKafkaProducer.Semantic.EXACTLY_ONCE)
    socketStream.addSink(kafkaSink)

    env.execute("StreamingFromCollectionScala")

  }
}


```

###### 6.3.3 å¯åŠ¨kafkaæ¶ˆè´¹è€…

* node01æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨kafkaæ¶ˆè´¹è€…ï¼Œæ¶ˆè´¹æ•°æ®

```shell
kafka-console-consumer.sh --bootstrap-server node01:9092,node02:9092,node03:9092 --topic test

```

### ğŸ“– 7. Flinkå½“ä¸­çš„windowçª—å£

* å¯¹äºæµå¼å¤„ç†ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦æ±‚å–æ€»å’Œï¼Œå¹³å‡å€¼ï¼Œæˆ–è€…æœ€å¤§å€¼ï¼Œæœ€å°å€¼ç­‰ï¼Œæ˜¯åšä¸åˆ°çš„ï¼Œå› ä¸ºæ•°æ®ä¸€ç›´åœ¨æºæºä¸æ–­çš„äº§ç”Ÿï¼Œå³æ•°æ®æ˜¯æ²¡æœ‰è¾¹ç•Œçš„ï¼Œæ‰€ä»¥æ²¡æ³•æ±‚æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼ç­‰ï¼Œæ‰€ä»¥ä¸ºäº†ä¸€äº›æ•°å€¼ç»Ÿè®¡çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šæ—¶é—´æ®µï¼Œå¯¹æŸä¸€æ®µæ—¶é—´çš„æ•°æ®æ±‚å–ä¸€äº›æ•°æ®å€¼æ˜¯å¯ä»¥åšåˆ°çš„ã€‚æˆ–è€…å¯¹æŸä¸€äº›æ•°æ®æ±‚å–æ•°æ®å€¼ä¹Ÿæ˜¯å¯ä»¥åšåˆ°çš„
* æ‰€ä»¥ï¼Œæµä¸Šçš„èšåˆéœ€è¦ç”± window æ¥åˆ’å®šèŒƒå›´ï¼Œæ¯”å¦‚ â€œè®¡ç®—è¿‡å»çš„5åˆ†é’Ÿâ€ ï¼Œæˆ–è€… â€œæœ€å100ä¸ªå…ƒç´ çš„å’Œâ€ ã€‚
* windowæ˜¯ä¸€ç§å¯ä»¥æŠŠæ— é™æ•°æ®åˆ‡å‰²ä¸ºæœ‰é™æ•°æ®å—çš„æ‰‹æ®µ

  + çª—å£å¯ä»¥æ˜¯ æ—¶é—´é©±åŠ¨çš„ ã€Time Windowã€‘ï¼ˆæ¯”å¦‚ï¼šæ¯30ç§’ï¼‰
  + æˆ–è€… æ•°æ®é©±åŠ¨çš„ã€Count Windowã€‘ ï¼ˆæ¯”å¦‚ï¼šæ¯100ä¸ªå…ƒç´ ï¼‰
      
    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/4227af9b6b1649c1ae6cf7d42ff85515.png)
* çª—å£ç±»å‹æ±‡æ€»ï¼š
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/673373c734034a11913cb93e065daf57.png)

##### 7.1 çª—å£çš„åŸºæœ¬ç±»å‹ä»‹ç»

* çª—å£é€šå¸¸è¢«åŒºåˆ†ä¸ºä¸åŒçš„ç±»å‹:
  + tumbling windowsï¼šæ»šåŠ¨çª—å£ ã€æ²¡æœ‰é‡å ã€‘

    - æ»šåŠ¨çª—å£ä¸‹çª—å£ä¹‹é—´ä¹‹é—´ä¸é‡å ï¼Œä¸”çª—å£é•¿åº¦æ˜¯å›ºå®šçš„

    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b1fc82b047bf4ea39da2ba30fe5f0112.png)
  + sliding windowsï¼šæ»‘åŠ¨çª—å£ ã€æœ‰é‡å ã€‘

    - æ»‘åŠ¨çª—å£ä»¥ä¸€ä¸ªæ­¥é•¿ï¼ˆSlideï¼‰ä¸æ–­å‘å‰æ»‘åŠ¨ï¼Œçª—å£çš„é•¿åº¦å›ºå®š

    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e8696432522643728c428c75c4c3d797.png)
  + session windowsï¼šä¼šè¯çª—å£ ï¼Œä¸€èˆ¬æ²¡äººç”¨

    - Session windowçš„çª—å£å¤§å°ï¼Œåˆ™æ˜¯ç”±æ•°æ®æœ¬èº«å†³å®šï¼Œå®ƒæ²¡æœ‰å›ºå®šçš„å¼€å§‹å’Œç»“æŸæ—¶é—´ã€‚
    - ä¼šè¯çª—å£æ ¹æ®Session gapåˆ‡åˆ†ä¸åŒçš„çª—å£ï¼Œå½“ä¸€ä¸ªçª—å£åœ¨å¤§äºSession gapçš„æ—¶é—´å†…æ²¡æœ‰æ¥æ”¶åˆ°æ–°æ•°æ®æ—¶ï¼Œçª—å£å°†å…³é—­

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/eb12d356819d475b8677ce6ee2b6c2bb.png)

##### 7.2 Flinkçš„çª—å£ä»‹ç»

###### 7.2.1 Time Windowçª—å£çš„åº”ç”¨

* time windowåˆåˆ†ä¸ºæ»šåŠ¨çª—å£å’Œæ»‘åŠ¨çª—å£ï¼Œè¿™ä¸¤ç§çª—å£è°ƒç”¨æ–¹æ³•éƒ½æ˜¯ä¸€æ ·çš„ï¼Œéƒ½æ˜¯è°ƒç”¨timeWindowè¿™ä¸ªæ–¹æ³•ï¼Œå¦‚æœä¼ å…¥
  ä¸€ä¸ªå‚æ•°å°±æ˜¯æ»šåŠ¨çª—å£
  ï¼Œå¦‚æœä¼ å…¥
  ä¸¤ä¸ªå‚æ•°å°±æ˜¯æ»‘åŠ¨çª—å£

â€‹
![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e05acaddbc254232833567e978da277e.png)

* éœ€æ±‚ï¼šæ¯éš”5sæ—¶é—´ï¼Œç»Ÿè®¡æœ€è¿‘10så‡ºç°çš„æ•°æ®
* ä»£ç å®ç°ï¼š

```scala
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.windowing.time.Time

object TestTimeWindow {

  def main(args: Array[String]): Unit = {
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._
    val socketSource: DataStream[String] = environment.socketTextStream("node01",9999)

    socketSource
      .flatMap(x => x.split(" "))
      .map(x =>(x,1))
      .keyBy(0)
      .timeWindow(Time.seconds(10),Time.seconds(5))
      .sum(1).print()
    environment.execute()

  }

}

```

###### 7.2.2 Count Windosçª—å£çš„åº”ç”¨

* ä¸timeWindowç±»å‹ï¼ŒCountWinodwä¹Ÿå¯ä»¥åˆ†ä¸ºæ»šåŠ¨çª—å£å’Œæ»‘åŠ¨çª—å£ï¼Œè¿™ä¸¤ä¸ªçª—å£è°ƒç”¨æ–¹æ³•ä¸€æ ·ï¼Œéƒ½æ˜¯è°ƒç”¨countWindowï¼Œå¦‚æœä¼ å…¥ä¸€ä¸ªå‚æ•°å°±æ˜¯æ»šåŠ¨çª—å£ï¼Œå¦‚æœä¼ å…¥ä¸¤ä¸ªå‚æ•°å°±æ˜¯æ»‘åŠ¨çª—å£

  â€‹
    
  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/295036358afe42f8b06a515448580f96.png)
* éœ€æ±‚ï¼šä½¿ç”¨count Window ç»Ÿè®¡æœ€è¿‘5æ¡æ•°çš„æœ€å¤§å€¼

```scala
import org.apache.flink.api.common.functions.AggregateFunction
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}

/**
  * ä½¿ç”¨countWindowç»Ÿè®¡æœ€è¿‘5æ¡æ•°æ®çš„æœ€å¤§å€¼
  */
object TestCountWindow {

  def main(args: Array[String]): Unit = {
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    import org.apache.flink.api.scala._
    val socketSource: DataStream[String] = environment.socketTextStream("node01",9999)

    /**
      * å‘é€æ•°æ®
      * spark 1
      * spark 2
      * spark 3
      * spark 4
      * spark 5
      * hello 100
      * hello 90
      * hello 80
      * hello 70
      * hello 60
      * hello 10
      */
    socketSource.map(x => (x.split(" ")(0),x.split(" ")(1).toInt))
    .keyBy(0).countWindow(5)
        .aggregate(new AggregateFunction[(String,Int),Int,Double]{
          var initAccumulator :Int = 0
          override def createAccumulator(): Int = {
            initAccumulator
          }

          override def add(value: (String, Int), accumulator: Int): Int = {
            if(accumulator >= value._2){
              accumulator
            }else{
              value._2
            }
          }

          override def getResult(accumulator: Int): Double = {
            accumulator

          }

          override def merge(a: Int, b: Int): Int = {
            if(a>=b){
              a
            }else{
              b
            }
          }
        }).print()

    environment.execute()
  }
}

```

###### 7.2.3 è‡ªå®šä¹‰windowçš„åº”ç”¨

* å¦‚æœtime window å’Œ countWindow è¿˜ä¸å¤Ÿç”¨çš„è¯ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰windowæ¥å®ç°æ•°æ®çš„ç»Ÿè®¡ç­‰åŠŸèƒ½ã€‚

  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/f3f1c51d1509402d979d2e9231718518.png)

##### 7.3 windowçª—å£æ•°æ®çš„é›†åˆç»Ÿè®¡

* å‰é¢æˆ‘ä»¬å¯ä»¥é€šè¿‡aggregrateå®ç°æ•°æ®çš„èšåˆï¼Œå¯¹äºæ±‚æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼ç­‰æ“ä½œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡processæ–¹æ³•æ¥å®ç°
* å¯¹äºæŸä¸€ä¸ªwindowå†…çš„æ•°å€¼ç»Ÿè®¡ï¼Œæˆ‘ä»¬å¯ä»¥å¢é‡çš„èšåˆç»Ÿè®¡æˆ–è€…å…¨é‡çš„èšåˆç»Ÿè®¡

###### 7.3.1 å¢é‡èšåˆç»Ÿè®¡

* çª—å£å½“ä¸­æ¯åŠ å…¥ä¸€æ¡æ•°æ®ï¼Œå°±è¿›è¡Œä¸€æ¬¡ç»Ÿè®¡
* å¸¸ç”¨çš„èšåˆç®—å­
  + reduce(reduceFunction)
  + aggregate(aggregateFunction)

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/fae9c823a64447a3804410681419a2fe.png)

* éœ€æ±‚

  + é€šè¿‡æ¥æ”¶socketå½“ä¸­è¾“å…¥çš„æ•°æ®ï¼Œç»Ÿè®¡æ¯5ç§’é’Ÿæ•°æ®çš„ç´¯è®¡çš„å€¼
* ä»£ç å®ç°

```scala
package com.kaikeba.window

import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.windowing.time.Time

object FlinkTimeCount {

  def main(args: Array[String]): Unit = {

      val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
      import org.apache.flink.api.scala._

      val socketStream: DataStream[String] = environment.socketTextStream("node01",9999)

      socketStream.map(x => (1, x.toInt))
                  .keyBy(0)
                  .timeWindow(Time.seconds(5))
                  .reduce((c1,c2)=>(c1._1,c1._2+c2._2))
                  .print()

      environment.execute("FlinkTimeCount")
    }

}



```

###### 7.3.2 å…¨é‡èšåˆç»Ÿè®¡

* ç­‰åˆ°çª—å£æˆªæ­¢ï¼Œæˆ–è€…çª—å£å†…çš„æ•°æ®å…¨éƒ¨åˆ°é½ï¼Œç„¶åå†è¿›è¡Œç»Ÿè®¡ï¼Œå¯ä»¥ç”¨äºæ±‚çª—å£å†…çš„æ•°æ®çš„æœ€å¤§å€¼ï¼Œæˆ–è€…æœ€å°å€¼ï¼Œå¹³å‡å€¼ç­‰
* ç­‰å±äºçª—å£çš„æ•°æ®åˆ°é½ï¼Œæ‰å¼€å§‹è¿›è¡Œèšåˆè®¡ç®—ã€å¯ä»¥å®ç°å¯¹çª—å£å†…çš„æ•°æ®è¿›è¡Œæ’åºç­‰éœ€æ±‚ã€‘

  + apply(windowFunction)
  + process(processWindowFunction)
  + processWindowFunctionæ¯”windowFunctionæä¾›äº†æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
* éœ€æ±‚

  + é€šè¿‡å…¨é‡èšåˆç»Ÿè®¡ï¼Œæ±‚å–æ¯3æ¡æ•°æ®çš„å¹³å‡å€¼
* ä»£ç å®ç°

```scala
package com.kaikeba.window

import org.apache.flink.api.java.tuple.Tuple
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment}
import org.apache.flink.streaming.api.windowing.windows.GlobalWindow
import org.apache.flink.util.Collector

/**
  * æ±‚å–æ¯3æ¡æ•°æ®çš„å¹³å‡å€¼
  */
object FlinkCountWindowAvg {
  /**
    * è¾“å…¥æ•°æ®
    * 1
    * 2
    * 3
    * 4
    * 5
    * 6
    * @param args
    */
  def main(args: Array[String]): Unit = {
    val environment: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

    import org.apache.flink.api.scala._

    val socketStream: DataStream[String] = environment.socketTextStream("node01",9999)

    //ç»Ÿè®¡ä¸€ä¸ªçª—å£å†…çš„æ•°æ®çš„å¹³å‡å€¼
    socketStream.map(x => (1, x.toInt))
                .keyBy(0)
                .countWindow(3)
                //é€šè¿‡processæ–¹æ³•æ¥ç»Ÿè®¡çª—å£çš„å¹³å‡å€¼
                .process(new MyProcessWindowFunctionclass)
                .print()

    //å¿…é¡»è°ƒç”¨executeæ–¹æ³•ï¼Œå¦åˆ™ç¨‹åºä¸ä¼šæ‰§è¡Œ
    environment.execute("count avg")
  }
}

/**ProcessWindowFunction éœ€è¦è·Ÿå››ä¸ªå‚æ•°
  * è¾“å…¥å‚æ•°ç±»å‹ï¼Œè¾“å‡ºå‚æ•°ç±»å‹ï¼Œèšåˆçš„keyçš„ç±»å‹ï¼Œwindowçš„ä¸‹ç•Œ
  *
  */
class MyProcessWindowFunctionclass extends ProcessWindowFunction[(Int , Int) , Double , Tuple , GlobalWindow]{

  override def process(key: Tuple, context: Context, elements: Iterable[(Int, Int)], out: Collector[Double]): Unit = {
    var totalNum = 0;
    var countNum = 0;
    for(data <-  elements){
      totalNum +=1
      countNum += data._2
    }
    out.collect(countNum/totalNum)
  }
}

```

### ğŸ“– â­ï¸8. checkpointæœºåˆ¶åŸç†æ·±åº¦å‰–æ

* checkpointæ˜¯flinkä¸ºäº†è§£å†³stateä¸€è‡´æ€§å’Œå®¹é”™æ€§å¼•å…¥çš„ä¸€ç§åˆ†å¸ƒå¼çš„çŠ¶æ€å¿«ç…§æœºåˆ¶ã€‚

##### 8.1 Flinkåˆ†å¸ƒå¼å¿«ç…§æµç¨‹

* é¦–å…ˆæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä¸€ä¸ªç®€å•çš„Checkpointçš„å¤§è‡´æµç¨‹ï¼š
  1. æš‚åœå¤„ç†æ–°æµå…¥æ•°æ®ï¼Œå°†æ–°æ•°æ®ç¼“å­˜èµ·æ¥ã€‚
  2. å°†ç®—å­å­ä»»åŠ¡çš„æœ¬åœ°çŠ¶æ€æ•°æ®æ‹·è´åˆ°ä¸€ä¸ªè¿œç¨‹çš„æŒä¹…åŒ–å­˜å‚¨ä¸Šã€‚
  3. ç»§ç»­å¤„ç†æ–°æµå…¥çš„æ•°æ®ï¼ŒåŒ…æ‹¬åˆšæ‰ç¼“å­˜èµ·æ¥çš„æ•°æ®ã€‚

##### 8.2 Barrieræœºåˆ¶

> â€‹ flinkæ˜¯å¦‚ä½•æ¥å®ç°åˆ†å¸ƒå¼çŠ¶æ€å¿«ç…§çš„å‘¢ï¼Œç”±äºflinkæ˜¯æµå¼çš„è®¡ç®—å¼•æ“ï¼ŒåŸºäºè¿™ç§ç‰¹å®šçš„åœºæ™¯ï¼ŒFlinké€šè¿‡å‘æµæ•°æ®ä¸­æ³¨å…¥ç‰¹æ®Šçš„äº‹ä»¶æ¥ä½œä¸ºå¿«ç…§çš„ä¿¡å·ï¼Œè¿™ç§ç‰¹æ®Šäº‹ä»¶å°±å«Barrierï¼ˆå±éšœï¼Œæ …æ ï¼‰ã€‚å½“ç®—å­ä»»åŠ¡å¤„ç†åˆ°Barrier nçš„æ—¶å€™å°±ä¼šæ‰§è¡ŒçŠ¶æ€çš„å¿«ç…§å¹¶æŠŠå®ƒæ ‡è®°ä¸ºnçš„çŠ¶æ€å¿«ç…§ã€‚
>   
> ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/30033d9a206c412b8801b58f510ed6f5.png)

* checkpointçš„è°ƒç”¨æµç¨‹ï¼š

  + 1. é¦–å…ˆæ˜¯JobManagerä¸­çš„checkpoint Coordinator(åè°ƒå™¨) å‘ä»»åŠ¡ä¸­çš„æ‰€æœ‰source Taskå‘¨æœŸæ€§å‘é€barrierï¼ˆæ …æ ï¼‰è¿›è¡Œå¿«ç…§è¯·æ±‚ã€‚
  + 2. source Taskæ¥å—åˆ°barrieråï¼Œ ä¼šæŠŠå½“å‰è‡ªå·±çš„çŠ¶æ€è¿›è¡Œsnapshot(å¯ä»¥ä¿å­˜åœ¨HDFSä¸Š)ã€‚
  + 3. sourceå‘checkpoint coordinatorç¡®è®¤snapshotå·²ç»å®Œæˆã€‚
  + 4. sourceç»§ç»­å‘ä¸‹æ¸¸transformation operatorå‘é€ barrierã€‚
  + 5. transformation operatoré‡å¤sourceçš„æ“ä½œï¼Œç›´åˆ°sink operatorå‘åè°ƒå™¨ç¡®è®¤snapshotå®Œæˆã€‚
  + 6. coordinatorç¡®è®¤å®Œæˆæœ¬å‘¨æœŸçš„snapshotå·²ç»å®Œæˆã€‚

  ```scala

  // 5ç§’å¯åŠ¨ä¸€æ¬¡checkpoint
  env.enableCheckpointing(5000)

  // è®¾ç½®checkpointåªcheckpointä¸€æ¬¡
  env.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)

  // è®¾ç½®ä¸¤æ¬¡checkpointçš„æœ€å°æ—¶é—´é—´éš”
  env.getCheckpointConfig.setMinPauseBetweenCheckpoints(1000)

  // checkpointè¶…æ—¶çš„æ—¶é•¿
  env.getCheckpointConfig.setCheckpointTimeout(60000)

  // å…è®¸çš„æœ€å¤§checkpointå¹¶è¡Œåº¦
  env.getCheckpointConfig.setMaxConcurrentCheckpoints(1)

  // å½“ç¨‹åºå…³é—­çš„æ—¶ï¼Œè§¦å‘é¢å¤–çš„checkpoint
  env.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)

  // è®¾ç½®checkpointçš„åœ°å€
  env.setStateBackend(new FsStateBackend("hdfs://node01:8020/flink-checkpoint/"))

  ```
* æ³¨æ„

  > Checkpoint Barrierè¢«æ’å…¥åˆ°æ•°æ®æµä¸­ï¼Œå®ƒå°†æ•°æ®æµåˆ‡åˆ†æˆæ®µã€‚Flinkçš„Checkpointé€»è¾‘æ˜¯ï¼Œä¸€æ®µæ–°æ•°æ®æµå…¥å¯¼è‡´çŠ¶æ€å‘ç”Ÿäº†å˜åŒ–ï¼ŒFlinkçš„ç®—å­æ¥æ”¶åˆ°Checkpoint Barrieråï¼Œå¯¹çŠ¶æ€è¿›è¡Œå¿«ç…§ã€‚æ¯ä¸ªCheckpoint Barrieræœ‰ä¸€ä¸ªIDï¼Œè¡¨ç¤ºè¯¥æ®µæ•°æ®å±äºå“ªæ¬¡Checkpointã€‚å¦‚å›¾æ‰€ç¤ºï¼Œå½“IDä¸ºnçš„Checkpoint Barrieråˆ°è¾¾æ¯ä¸ªç®—å­åï¼Œè¡¨ç¤ºè¦å¯¹n-1å’Œnä¹‹é—´çŠ¶æ€çš„æ›´æ–°åšå¿«ç…§ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/070e9923546e4e48adba64b29161da8c.png)

##### 8.3 å¤šä»»åŠ¡å¹¶è¡Œä¸‹çš„checkpoint

* æˆ‘ä»¬æ„å»ºä¸€ä¸ªå¹¶è¡Œæ•°æ®æµå›¾ï¼Œç”¨è¿™ä¸ªå¹¶è¡Œæ•°æ®æµå›¾æ¥æ¼”ç¤ºFlinkçš„åˆ†å¸ƒå¼å¿«ç…§æœºåˆ¶ã€‚è¿™ä¸ªæ•°æ®æµå›¾æœ‰ä¸¤ä¸ªSourceå­ä»»åŠ¡ï¼Œæ•°æ®æµä¼šåœ¨è¿™äº›å¹¶è¡Œç®—å­ä¸Šä»SourceæµåŠ¨åˆ°Sinkã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/e43b5c7090af425091e5acc610971ae5.png)

* é¦–å…ˆï¼ŒFlinkçš„æ£€æŸ¥ç‚¹åè°ƒå™¨ï¼ˆCheckpoint Coordinatorï¼‰è§¦å‘ä¸€æ¬¡Checkpointï¼ˆTrigger Checkpointï¼‰ï¼Œè¿™ä¸ªè¯·æ±‚ä¼šå‘é€ç»™Sourceçš„å„ä¸ªå­ä»»åŠ¡ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/40c05f2982ce4b0f8a4d480b59384a59.png)

* å„Sourceç®—å­å­ä»»åŠ¡æ¥æ”¶åˆ°è¿™ä¸ªCheckpointè¯·æ±‚ä¹‹åï¼Œä¼šå°†è‡ªå·±çš„çŠ¶æ€å†™å…¥åˆ°çŠ¶æ€åç«¯ï¼Œç”Ÿæˆä¸€æ¬¡å¿«ç…§ï¼Œå¹¶ä¸”ä¼šå‘ä¸‹æ¸¸å¹¿æ’­Checkpoint Barrierã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/b5e3ddd23f684c0e929319fc1ba95101.png)

* Sourceç®—å­åšå®Œå¿«ç…§åï¼Œè¿˜ä¼šç»™Checkpoint Coodinatorå‘é€ä¸€ä¸ªç¡®è®¤ï¼Œå‘ŠçŸ¥è‡ªå·±å·²ç»åšå®Œäº†ç›¸åº”çš„å·¥ä½œã€‚è¿™ä¸ªç¡®è®¤ä¸­åŒ…æ‹¬äº†ä¸€äº›å…ƒæ•°æ®ï¼Œå…¶ä¸­å°±åŒ…æ‹¬åˆšæ‰å¤‡ä»½åˆ°State Backendçš„çŠ¶æ€å¥æŸ„ï¼Œæˆ–è€…è¯´æ˜¯æŒ‡å‘çŠ¶æ€çš„æŒ‡é’ˆã€‚è‡³æ­¤ï¼ŒSourceå®Œæˆäº†ä¸€æ¬¡Checkpointã€‚è·ŸWatermarkçš„ä¼ æ’­ä¸€æ ·ï¼Œä¸€ä¸ªç®—å­å­ä»»åŠ¡è¦æŠŠCheckpoint Barrierå‘é€ç»™æ‰€è¿æ¥çš„æ‰€æœ‰ä¸‹æ¸¸ç®—å­å­ä»»åŠ¡ã€‚
* å¯¹äºä¸‹æ¸¸ç®—å­æ¥è¯´ï¼Œå¯èƒ½æœ‰å¤šä¸ªä¸ä¹‹ç›¸è¿çš„ä¸Šæ¸¸è¾“å…¥ï¼Œæˆ‘ä»¬å°†ç®—å­ä¹‹é—´çš„è¾¹ç§°ä¸ºé€šé“ã€‚
  Sourceè¦å°†ä¸€ä¸ªIDä¸ºnçš„Checkpoint Barrierå‘æ‰€æœ‰ä¸‹æ¸¸ç®—å­å¹¿æ’­ï¼Œè¿™ä¹Ÿæ„å‘³ç€ä¸‹æ¸¸ç®—å­çš„å¤šä¸ªè¾“å…¥é‡Œéƒ½æœ‰åŒä¸€ä¸ªCheckpoint Barrierï¼Œè€Œä¸”ä¸åŒè¾“å…¥é‡ŒCheckpoint Barrierçš„æµå…¥è¿›åº¦å¯èƒ½ä¸åŒã€‚
  **Checkpoint Barrierä¼ æ’­çš„è¿‡ç¨‹éœ€è¦è¿›è¡Œå¯¹é½ï¼ˆBarrier Alignmentï¼‰ï¼Œæˆ‘ä»¬ä»æ•°æ®æµå›¾ä¸­æˆªå–ä¸€å°éƒ¨åˆ†æ¥åˆ†æCheckpoint Barrieræ˜¯å¦‚ä½•åœ¨ç®—å­é—´ä¼ æ’­å’Œå¯¹é½çš„**
  ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/ff547ddccc704691ab2354274ad0189f.png)

> å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¯¹é½åˆ†ä¸ºå››æ­¥ï¼š
>
> (1). ç®—å­å­ä»»åŠ¡åœ¨æŸä¸ªè¾“å…¥é€šé“ä¸­æ”¶åˆ°ç¬¬ä¸€ä¸ªIDä¸ºnçš„Checkpoint Barrierï¼Œä½†æ˜¯å…¶ä»–è¾“å…¥é€šé“ä¸­IDä¸ºnçš„Checkpoint Barrierè¿˜æœªåˆ°è¾¾ï¼Œè¯¥ç®—å­å­ä»»åŠ¡å¼€å§‹å‡†å¤‡è¿›è¡Œå¯¹é½ã€‚
>
> (2). ç®—å­å­ä»»åŠ¡å°†ç¬¬ä¸€ä¸ªè¾“å…¥é€šé“çš„æ•°æ®ç¼“å­˜ä¸‹æ¥ï¼ŒåŒæ—¶ç»§ç»­å¤„ç†å…¶ä»–è¾“å…¥é€šé“çš„æ•°æ®ï¼Œè¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸ºå¯¹é½ã€‚
>
> (3). ç¬¬äºŒä¸ªè¾“å…¥é€šé“çš„Checkpoint BarrieræŠµè¾¾è¯¥ç®—å­å­ä»»åŠ¡ï¼Œè¯¥ç®—å­å­ä»»åŠ¡æ‰§è¡Œå¿«ç…§ï¼Œå°†çŠ¶æ€å†™å…¥State Backendï¼Œç„¶åå°†IDä¸ºnçš„Checkpoint Barrierå‘ä¸‹æ¸¸æ‰€æœ‰è¾“å‡ºé€šé“å¹¿æ’­ã€‚
>
> (4). å¯¹äºè¿™ä¸ªç®—å­å­ä»»åŠ¡ï¼Œå¿«ç…§æ‰§è¡Œç»“æŸï¼Œç»§ç»­å¤„ç†å„ä¸ªé€šé“ä¸­æ–°æµå…¥æ•°æ®ï¼ŒåŒ…æ‹¬åˆšæ‰ç¼“å­˜èµ·æ¥çš„æ•°æ®ã€‚

* æ•°æ®æµå›¾ä¸­çš„æ¯ä¸ªç®—å­å­ä»»åŠ¡éƒ½è¦å®Œæˆä¸€éä¸Šè¿°çš„å¯¹é½ã€å¿«ç…§ã€ç¡®è®¤çš„å·¥ä½œï¼Œå½“æœ€åæ‰€æœ‰Sinkç®—å­ç¡®è®¤å®Œæˆå¿«ç…§ä¹‹åï¼Œè¯´æ˜IDä¸ºnçš„Checkpointæ‰§è¡Œç»“æŸï¼ŒCheckpoint Coordinatorå‘State Backendå†™å…¥ä¸€äº›æœ¬æ¬¡Checkpointçš„å…ƒæ•°æ®ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/d14c9c7502e848d090455b2a131ce267.png)

> â€‹ ä¹‹æ‰€ä»¥è¦è¿›è¡Œbarrierå¯¹é½ï¼Œä¸»è¦æ˜¯ä¸ºäº†ä¿è¯ä¸€ä¸ªFlinkä½œä¸šæ‰€æœ‰ç®—å­çš„çŠ¶æ€æ˜¯ä¸€è‡´çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒæŸä¸ªIDä¸ºnçš„Checkpoint Barrierä»å‰åˆ°åæµå…¥æ‰€æœ‰ç®—å­å­ä»»åŠ¡åï¼Œæ‰€æœ‰ç®—å­å­ä»»åŠ¡éƒ½èƒ½å°†åŒæ ·çš„ä¸€æ®µæ•°æ®å†™å…¥å¿«ç…§ã€‚

##### 8.4 å¿«ç…§æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

* ä¸Šé¢è®²åˆ°äº†ä¸€è‡´æ€§å¿«ç…§çš„å…·ä½“æµç¨‹ï¼Œè¿™ç§æ–¹å¼ä¿è¯äº†æ•°æ®çš„ä¸€è‡´æ€§ï¼Œä½†æœ‰ä¸€äº›æ½œåœ¨çš„é—®é¢˜

  + ï¼ˆ1ï¼‰æ¯æ¬¡è¿›è¡ŒCheckpointå‰ï¼Œéƒ½éœ€è¦æš‚åœå¤„ç†æ–°æµå…¥æ•°æ®ï¼Œç„¶åå¼€å§‹æ‰§è¡Œå¿«ç…§ï¼Œå‡å¦‚çŠ¶æ€æ¯”è¾ƒå¤§ï¼Œä¸€æ¬¡å¿«ç…§å¯èƒ½é•¿è¾¾å‡ ç§’ç”šè‡³å‡ åˆ†é’Ÿã€‚
  + ï¼ˆ2ï¼‰Checkpoint Barrierå¯¹é½æ—¶ï¼Œå¿…é¡»ç­‰å¾…æ‰€æœ‰ä¸Šæ¸¸é€šé“éƒ½å¤„ç†å®Œï¼Œå‡å¦‚æŸä¸ªä¸Šæ¸¸é€šé“å¤„ç†å¾ˆæ…¢ï¼Œè¿™å¯èƒ½é€ æˆæ•´ä¸ªæ•°æ®æµå µå¡ã€‚
* ä¼˜åŒ–æ–¹æ¡ˆ

  > + ï¼ˆ1ï¼‰å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼ŒFlinkæä¾›äº†å¼‚æ­¥å¿«ç…§ï¼ˆAsynchronous Snapshotï¼‰çš„æœºåˆ¶ã€‚å½“å®é™…æ‰§è¡Œå¿«ç…§æ—¶ï¼ŒFlinkå¯ä»¥ç«‹å³å‘ä¸‹å¹¿æ’­Checkpoint Barrierï¼Œè¡¨ç¤ºè‡ªå·±å·²ç»æ‰§è¡Œå®Œè‡ªå·±éƒ¨åˆ†çš„å¿«ç…§ã€‚ä¸€æ—¦æ•°æ®åŒæ­¥å®Œæˆï¼Œå†ç»™Checkpoint Coordinatorå‘é€ç¡®è®¤ä¿¡æ¯
  > + ï¼ˆ2ï¼‰å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼ŒFlinkå…è®¸è·³è¿‡å¯¹é½è¿™ä¸€æ­¥ï¼Œæˆ–è€…è¯´ä¸€ä¸ªç®—å­å­ä»»åŠ¡ä¸éœ€è¦ç­‰å¾…æ‰€æœ‰ä¸Šæ¸¸é€šé“çš„Checkpoint Barrierï¼Œç›´æ¥å°†Checkpoint Barrierå¹¿æ’­ï¼Œæ‰§è¡Œå¿«ç…§å¹¶ç»§ç»­å¤„ç†åç»­æµå…¥æ•°æ®ã€‚ä¸ºäº†ä¿è¯æ•°æ®ä¸€è‡´æ€§ï¼ŒFlinkå¿…é¡»å°†é‚£äº›è¾ƒæ…¢çš„æ•°æ®æµä¸­çš„å…ƒç´ ä¹Ÿä¸€èµ·å¿«ç…§ï¼Œä¸€æ—¦é‡å¯ï¼Œè¿™äº›å…ƒç´ ä¼šè¢«é‡æ–°å¤„ç†ä¸€éã€‚

â­ï¸
  
[Barrierå¯¹é½ä¼šå½±å“æ‰§è¡Œæ•ˆç‡ï¼Œæ€ä¹ˆè·³è¿‡Barrierå¯¹é½ï¼Œè·³è¿‡åè¿˜èƒ½ä¿è¯â€ŒExactly-Onceè¯­ä¹‰å—ï¼Ÿ](https://blog.csdn.net/u010342213/article/details/146116566?sharetype=blogdetail&sharerId=146116566&sharerefer=PC&sharesource=u010342213&spm=1011.2480.3001.8118)

##### 8.5 ä»»åŠ¡é‡å¯æ¢å¤æµç¨‹

* Flinkçš„é‡å¯æ¢å¤é€»è¾‘ç›¸å¯¹æ¯”è¾ƒç®€å•ï¼š

  > + 1ã€é‡å¯åº”ç”¨ï¼Œåœ¨é›†ç¾¤ä¸Šé‡æ–°éƒ¨ç½²æ•°æ®æµå›¾ã€‚
  > + 2ã€ä»æŒä¹…åŒ–å­˜å‚¨ä¸Šè¯»å–æœ€è¿‘ä¸€æ¬¡çš„Checkpointæ•°æ®ï¼ŒåŠ è½½åˆ°å„ç®—å­å­ä»»åŠ¡ä¸Šã€‚
  > + 3ã€ç»§ç»­å¤„ç†æ–°æµå…¥çš„æ•°æ®ã€‚
* è¿™æ ·çš„æœºåˆ¶å¯ä»¥ä¿è¯Flinkå†…éƒ¨çŠ¶æ€çš„Excatly-Onceä¸€è‡´æ€§ã€‚è‡³äºç«¯åˆ°ç«¯çš„Exactly-Onceä¸€è‡´æ€§ï¼Œè¦æ ¹æ®Sourceå’ŒSinkçš„å…·ä½“å®ç°è€Œå®šã€‚å½“å‘ç”Ÿæ•…éšœæ—¶ï¼Œä¸€éƒ¨åˆ†æ•°æ®æœ‰å¯èƒ½å·²ç»æµå…¥ç³»ç»Ÿï¼Œä½†è¿˜æœªè¿›è¡ŒCheckpointï¼ŒSourceçš„Checkpointè®°å½•äº†è¾“å…¥çš„Offsetï¼›å½“é‡å¯æ—¶ï¼ŒFlinkèƒ½æŠŠæœ€è¿‘ä¸€æ¬¡çš„Checkpointæ¢å¤åˆ°å†…å­˜ä¸­ï¼Œå¹¶æ ¹æ®Offsetï¼Œè®©Sourceä»è¯¥ä½ç½®é‡æ–°å‘é€ä¸€éæ•°æ®ï¼Œä»¥ä¿è¯æ•°æ®ä¸ä¸¢ä¸é‡ã€‚åƒKafkaç­‰æ¶ˆæ¯é˜Ÿåˆ—æ˜¯æä¾›é‡å‘åŠŸèƒ½çš„ï¼Œ
  `socketTextStream`
  å°±ä¸å…·æœ‰è¿™ç§åŠŸèƒ½ï¼Œä¹Ÿæ„å‘³ç€ä¸èƒ½ä¿è¯Exactly-OnceæŠ•é€’ä¿éšœã€‚

### ğŸ“– 9. Flinkä¸¤é˜¶æ®µæäº¤ TwoPhaseCommit

##### 9.1 EXACTLY\_ONCEè¯­ä¹‰æ¦‚è¿°

* ä½•ä¸ºEXACTLY\_ONCEï¼Ÿ
  + EXACTLY\_ONCEç®€ç§°EOSï¼Œæ¯æ¡è¾“å…¥æ¶ˆæ¯åªä¼šå½±å“æœ€ç»ˆç»“æœä¸€æ¬¡ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å½±å“ä¸€æ¬¡ï¼Œè€Œéå¤„ç†ä¸€æ¬¡ï¼ŒFlinkä¸€ç›´å®£ç§°è‡ªå·±æ”¯æŒEOSï¼Œå®é™…ä¸Šä¸»è¦æ˜¯å¯¹äºFlinkåº”ç”¨å†…éƒ¨æ¥è¯´çš„ï¼Œå¯¹äºå¤–éƒ¨ç³»ç»Ÿ(ç«¯åˆ°ç«¯)åˆ™æœ‰æ¯”è¾ƒå¼ºçš„é™åˆ¶
  + Flinkå®ç°ç«¯åˆ°ç«¯çš„EXACTLY\_ONCEè¯­ä¹‰éœ€è¦æ»¡è¶³ï¼š
    - 1.å¤–éƒ¨ç³»ç»Ÿå†™å…¥æ”¯æŒå¹‚ç­‰æ€§
    - 2.å¤–éƒ¨ç³»ç»Ÿæ”¯æŒä»¥äº‹åŠ¡çš„æ–¹å¼å†™å…¥
* Flinkçš„åŸºæœ¬æ€è·¯å°±æ˜¯å°†çŠ¶æ€å®šæ—¶åœ°checkpiontåˆ°hdfsä¸­å»ï¼Œå½“å‘ç”Ÿfailureçš„æ—¶å€™æ¢å¤ä¸Šä¸€æ¬¡çš„çŠ¶æ€ï¼Œç„¶åå°†è¾“å‡ºupdateåˆ°å¤–éƒ¨ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯è¾“å…¥æµçš„offsetä¹Ÿæ˜¯çŠ¶æ€çš„ä¸€éƒ¨åˆ†ï¼Œå› æ­¤ä¸€æ—¦å‘ç”Ÿfailureå°±èƒ½ä»æœ€åä¸€æ¬¡çŠ¶æ€æ¢å¤ï¼Œä»è€Œä¿è¯è¾“å‡ºçš„ç»“æœæ˜¯exactly onceã€‚è¿™æ˜¯Flink1.4ä¹‹å‰çš„å®ç°ã€‚
* Flinkåœ¨1.4.0ç‰ˆæœ¬å¼•å…¥äº†TwoPhaseCommitSinkFunctionæ¥å£ï¼Œå¹¶åœ¨Kafka Producerçš„connectorä¸­å®ç°äº†å®ƒï¼Œæ”¯æŒäº†å¯¹å¤–éƒ¨Kafka Sinkçš„EXACTLY\_ONCEè¯­ä¹‰ï¼Œæ¥è®©å¼€å‘è€…ç”¨æ›´å°‘çš„ä»£ç æ¥å®ç°ç«¯åˆ°ç«¯çš„exactly-onceè¯­ä¹‰

##### 9.2 ä¸¤é˜¶æ®µæäº¤åè®®ä»‹ç»

* ä¸¤é˜¶æ®µæäº¤åè®®æ˜¯åè°ƒæ‰€æœ‰åˆ†å¸ƒå¼åŸå­äº‹åŠ¡å‚ä¸è€…ï¼Œå¹¶å†³å®šæäº¤æˆ–å–æ¶ˆï¼ˆå›æ»šï¼‰çš„åˆ†å¸ƒå¼ç®—æ³•
* åè®®å‚ä¸è€…

> ä¸¤é˜¶æ®µæäº¤æŒ‡çš„æ˜¯ä¸€ç§åè®®ï¼Œç»å¸¸ç”¨æ¥å®ç°åˆ†å¸ƒå¼äº‹åŠ¡ï¼Œå¯ä»¥ç®€å•ç†è§£ä¸ºé¢„æäº¤+å®é™…æäº¤ï¼Œä¸€èˆ¬åˆ†ä¸ºåè°ƒå™¨Coordinator(ä»¥ä¸‹ç®€ç§°C)å’Œè‹¥å¹²äº‹åŠ¡å‚ä¸è€…Participant(ä»¥ä¸‹ç®€ç§°P)ä¸¤ç§è§’è‰²ã€‚

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/fecdc9c7a1ca4fc6b59db0573c67c339.png)

* ä¸¤ä¸ªé˜¶æ®µçš„æ‰§è¡Œ

  + 1.è¯·æ±‚é˜¶æ®µï¼ˆcommit-request phaseï¼Œæˆ–ç§°è¡¨å†³é˜¶æ®µï¼Œvoting phaseï¼‰

    ```
    åœ¨è¯·æ±‚é˜¶æ®µï¼Œåè°ƒè€…å°†é€šçŸ¥äº‹åŠ¡å‚ä¸è€…å‡†å¤‡æäº¤æˆ–å–æ¶ˆäº‹åŠ¡ï¼Œç„¶åè¿›å…¥è¡¨å†³è¿‡ç¨‹ã€‚
    åœ¨è¡¨å†³è¿‡ç¨‹ä¸­ï¼Œå‚ä¸è€…å°†å‘ŠçŸ¥åè°ƒè€…è‡ªå·±çš„å†³ç­–ï¼šåŒæ„ï¼ˆäº‹åŠ¡å‚ä¸è€…æœ¬åœ°ä½œä¸šæ‰§è¡ŒæˆåŠŸï¼‰æˆ–å–æ¶ˆï¼ˆæœ¬åœ°ä½œä¸šæ‰§è¡Œæ•…éšœï¼‰ã€‚

    ```
  + 2. æäº¤é˜¶æ®µï¼ˆcommit phaseï¼‰

    ```
    åœ¨è¯¥é˜¶æ®µï¼Œåè°ƒè€…å°†åŸºäºç¬¬ä¸€ä¸ªé˜¶æ®µçš„æŠ•ç¥¨ç»“æœè¿›è¡Œå†³ç­–ï¼šæäº¤æˆ–å–æ¶ˆã€‚
    å½“ä¸”ä»…å½“æ‰€æœ‰çš„å‚ä¸è€…åŒæ„æäº¤äº‹åŠ¡åè°ƒè€…æ‰é€šçŸ¥æ‰€æœ‰çš„å‚ä¸è€…æäº¤äº‹åŠ¡ï¼Œå¦åˆ™åè°ƒè€…å°†é€šçŸ¥æ‰€æœ‰çš„å‚ä¸è€…å–æ¶ˆäº‹åŠ¡ã€‚
    å‚ä¸è€…åœ¨æ¥æ”¶åˆ°åè°ƒè€…å‘æ¥çš„æ¶ˆæ¯åå°†æ‰§è¡Œå“åº”çš„æ“ä½œã€‚

    ```

##### 9.3 ä¸¤é˜¶æ®µæäº¤å®ç°åŸç†æœºåˆ¶

* Flinkå’Œå¤–éƒ¨ç³»ç»Ÿ(å¦‚Kafka)ä¹‹é—´çš„æ¶ˆæ¯ä¼ é€’å¦‚ä½•åšåˆ°exactly onceå‘¢?
* å…ˆçœ‹ä¸‹é¢è¿™å¹…å›¾ä¼šå‡ºç°çš„é—®é¢˜

  ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/68818f70a3014eca94c71d72b4fe6885.png)

  + å½“sink Aå·²ç»å¾€Kafkaå†™å…¥äº†æ•°æ®,è€Œsink B fail.
  + æ ¹æ®Flinkçš„exactly onceä¿è¯,ç³»ç»Ÿä¼šå›æ»šåˆ°æœ€è¿‘çš„checkpoint,
  + ä½†æ˜¯sink Aå·²ç»æŠŠæ•°æ®å†™å…¥åˆ°kafkaäº†.
  + Flinkæ— æ³•å›æ»škafkaçš„state.å› æ­¤,kafkaå°†åœ¨ä¹‹åå†æ¬¡æ¥æ”¶åˆ°ä¸€ä»½åŒæ ·çš„æ¥è‡ªsink Açš„æ•°æ®,
  + è¿™æ ·çš„message deliveryä¾¿æˆä¸ºäº†at least once
* Flinké‡‡ç”¨Two phase commitæ¥è§£å†³è¿™ä¸ªé—®é¢˜.

  + Two phase commit

    > - Phase 1: Pre-commit é¢„æäº¤
    >   * Flinkçš„JobManagerå‘sourceæ³¨å…¥checkpoint barrierä»¥å¼€å¯è¿™snapshotï¼Œbarrierä»sourceæµå‘sink,
    >   * æ¯ä¸ªè¿›è¡Œsnapshotçš„ç®—å­æˆåŠŸsnapshotå,éƒ½ä¼šå‘JobManagerå‘é€ACK.
    >   * å½“sinkå®Œæˆsnapshotå, å‘JobManagerå‘é€ACKçš„åŒæ—¶å‘kafkaè¿›è¡Œpre-commit.

    > - Phase 2: Commit å®é™…æäº¤
    >   * å½“JobManageræ¥æ”¶åˆ°æ‰€æœ‰ç®—å­çš„ACKå, å°±ä¼šé€šçŸ¥æ‰€æœ‰çš„ç®—å­è¿™æ¬¡checkpointå·²ç»å®Œæˆ
    >   * Sinkæ¥æ”¶åˆ°è¿™ä¸ªé€šçŸ¥å, å°±å‘kafkaè¿›è¡Œcommit, æ­£å¼æŠŠæ•°æ®å†™å…¥åˆ°kafka
* ä¸‹é¢æˆ‘ä»¬æ¥çœ‹çœ‹flinkæ¶ˆè´¹å¹¶å†™å…¥kafkaçš„ä¾‹å­æ˜¯å¦‚ä½•é€šè¿‡ä¸¤éƒ¨æäº¤æ¥ä¿è¯exactly-onceè¯­ä¹‰çš„ã€‚

  + kafkaä»0.11å¼€å§‹æ”¯æŒäº‹ç‰©æ“ä½œï¼Œè‹¥è¦ä½¿ç”¨flinkç«¯åˆ°ç«¯exactly-onceè¯­ä¹‰éœ€è¦flinkçš„sinkçš„kafkaæ˜¯0.11ç‰ˆæœ¬ä»¥ä¸Šçš„
  + è¿™ä¸ªä¾‹å­åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

    - ä»kafkaè¯»å–æ•°æ®
    - ä¸€ä¸ªèšåˆçª—æ“ä½œ
    - å‘kafkaå†™å…¥æ•°æ®

    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/8e94144ad5fe4014ac68bfeedb8157a9.png)
  + 1ã€JobManagerå‘Sourceå‘é€Barrierï¼Œå¼€å§‹è¿›å…¥pre-Commité˜¶æ®µï¼Œå½“Sourceæ”¶åˆ°Barrieråï¼Œå°†è‡ªèº«çš„çŠ¶æ€è¿›è¡Œä¿å­˜ï¼Œåç«¯å¯ä»¥æ ¹æ®é…ç½®è¿›è¡Œé€‰æ‹©ï¼Œ
    è¿™é‡Œçš„çŠ¶æ€æ˜¯æŒ‡æ¶ˆè´¹çš„æ¯ä¸ªåˆ†åŒºå¯¹åº”çš„offset
    ã€‚ç„¶åå°†Barrierå‘é€ç»™ä¸‹ä¸€ä¸ªOperatorã€‚

    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/378bbea756f748999c49b5bbe4825cc4.png)
  + 2ã€å½“Windowè¿™ä¸ªOperatoræ”¶åˆ°Barrierä¹‹åï¼Œå¯¹è‡ªå·±çš„çŠ¶æ€è¿›è¡Œä¿å­˜ï¼Œè¿™é‡Œçš„çŠ¶æ€æ˜¯æŒ‡èšåˆçš„ç»“æœ(sumæˆ–countçš„ç»“æœ)ï¼Œç„¶åå°†Barrierå‘é€ç»™Sinkã€‚Sinkæ”¶åˆ°åä¹Ÿå¯¹è‡ªå·±çš„çŠ¶æ€è¿›è¡Œä¿å­˜ï¼Œä¹‹åä¼šè¿›è¡Œä¸€æ¬¡é¢„æäº¤ã€‚

    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/4de6759ad3d24d679b9c432fbb8dc095.png)
  + 3ã€é¢„æäº¤æˆåŠŸåï¼ŒJobManageré€šçŸ¥æ¯ä¸ªOperatorï¼Œè¿™ä¸€è½®æ£€æŸ¥ç‚¹å·²ç»å®Œæˆï¼Œè¿™ä¸ªæ—¶å€™ï¼ŒKafka Sinkä¼šå‘Kafkaè¿›è¡ŒçœŸæ­£çš„äº‹åŠ¡Commitã€‚

    ![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/a335c777a07448af9d494ac23acc44b0.png)

> ä»¥ä¸Šä¾¿æ˜¯ä¸¤é˜¶æ®µçš„å®Œæ•´æµç¨‹ï¼Œä¸åŒé˜¶æ®µfail overçš„recoveryä¸¾æª:
>
> â€‹ (1) åœ¨pre-commitå‰fail over, ç³»ç»Ÿæ¢å¤åˆ°æœ€è¿‘çš„checkponit
>
> â€‹ (2) åœ¨pre-commitå,commitå‰fail over,ç³»ç»Ÿæ¢å¤åˆ°åˆšå®Œæˆpre-commitæ—¶çš„çŠ¶æ€
>
> å› æ­¤ï¼Œæ‰€æœ‰opeartorå¿…é¡»å¯¹checkpointæœ€ç»ˆç»“æœè¾¾æˆå…±è¯†ï¼š
>
> â€‹ å³æ‰€æœ‰operatoréƒ½å¿…é¡»è®¤å®šæ•°æ®æäº¤è¦ä¹ˆæˆåŠŸæ‰§è¡Œï¼Œè¦ä¹ˆè¢«ç»ˆæ­¢ç„¶åå›æ»šã€‚

##### 9.4 ä¸¤é˜¶æ®µæäº¤çš„TwoPhaseCommitSinkFunctionç±»

* åœ¨ä½¿ç”¨ä¸¤æ­¥æäº¤ç®—å­æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç»§æ‰¿TwoPhaseCommitSinkFunctionè¿™ä¸ªç±»ã€‚

  + TwoPhaseCommitSinkFunctionæœ‰4ä¸ªæ–¹æ³•

    - 1. beginTransaction()

         ```
         å¼€å¯äº‹åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªä¸´æ—¶æ–‡ä»¶.åç»­æŠŠåŸè¦å†™å…¥åˆ°å¤–éƒ¨ç³»ç»Ÿçš„æ•°æ®å†™å…¥åˆ°è¿™ä¸ªä¸´æ—¶æ–‡ä»¶

         ```
    - 2. preCommit()

      ```
      é¢„æäº¤ï¼šflushå¹¶closeè¿™ä¸ªæ–‡ä»¶,ä¹‹åä¾¿ä¸å†å¾€å…¶ä¸­å†™æ•°æ®.åŒæ—¶å¼€å¯ä¸€ä¸ªæ–°çš„äº‹åŠ¡ä¾›ä¸‹ä¸ªcheckponitä½¿ç”¨

      ```
    - 3. commit()

         ```
         æ­£å¼æäº¤: æŠŠpre-committedçš„ä¸´æ—¶æ–‡ä»¶ç§»åŠ¨åˆ°æŒ‡å®šç›®å½•

         ```
    - 4. abort()

         ```
         ä¸¢å¼ƒ: åˆ é™¤æ‰pre-committedçš„ä¸´æ—¶æ–‡ä»¶

         ```

### 7ï¸âƒ£ æŠŠæ‰€æœ‰çš„ä»£ç éƒ½æ•²ä¸€é