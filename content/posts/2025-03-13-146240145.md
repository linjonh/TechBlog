---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f32323834313338372f:61727469636c652f64657461696c732f313436323430313435"
layout: post
title: "线性回归中的最小二乘法直接法与梯度下降的比较"
date: 2025-03-13 19:29:37 +08:00
description: "最小二乘法（Least Squares Method）是一种用于数据拟合的方法，它的核心思想是“最小化误差的平方和”。假设我们有一组数据点，并希望找到一条最优的直线或曲线来尽可能贴合这些点。由于数据通常存在噪声或者误差，无法完美拟合所有点，因此最小二乘法的目标是找到一个最佳拟合，使得所有点到拟合曲线的垂直距离的平方和最小。通俗地说，就像是在一堆散落的数据点中找一根“最合理”的线，使得数据点到这条线的总体偏差最小。"
keywords: "线性回归中的最小二乘法：直接法与梯度下降的比较"
categories: ['Ai']
tags: ['线性回归', '机器学习', '最小二乘法']
artid: "146240145"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146240145
    alt: "线性回归中的最小二乘法直接法与梯度下降的比较"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146240145
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146240145
cover: https://bing.ee123.net/img/rand?artid=146240145
image: https://bing.ee123.net/img/rand?artid=146240145
img: https://bing.ee123.net/img/rand?artid=146240145
---

# 线性回归中的最小二乘法：直接法与梯度下降的比较

## 最小二乘法（Least Squares Method）学习博客

## 0. 直观理解最小二乘法

最小二乘法（Least Squares Method）是一种用于数据拟合的方法，它的核心思想是“最小化误差的平方和”。

假设我们有一组数据点，并希望找到一条最优的直线或曲线来尽可能贴合这些点。由于数据通常存在噪声或者误差，无法完美拟合所有点，因此最小二乘法的目标是找到一个最佳拟合，使得所有点到拟合曲线的垂直距离的平方和最小。

通俗地说，就像是在一堆散落的数据点中找一根“最合理”的线，使得数据点到这条线的总体偏差最小。

### 1. 最小二乘法解决的问题

在数据拟合或机器学习任务中，我们经常需要找到一个最优的模型，使其能够最好地描述数据的规律。例如，给定一组数据点

(
x
i
,
y
i
)
(x_i, y_i)





(


x









i

​


,




y









i

​


)
，希望找到一个线性模型：

y
=
w
x
+
b
y = w x + b





y



=





w

x



+





b

或者更一般的线性回归模型：

y
=
w
1
x
1
+
w
2
x
2
+
.
.
.
+
w
n
x
n
+
b
y = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b





y



=






w









1

​



x









1

​




+






w









2

​



x









2

​




+





...



+






w









n

​



x









n

​




+





b

最小二乘法的目标是找到参数

w
\mathbf{w}





w
和

b
b





b
，使得模型预测值与真实值之间的误差平方和最小，即：

min
⁡
w
,
b
∑
i
=
1
m
(
y
i
−
(
w
1
x
i
1
+
w
2
x
i
2
+
.
.
.
+
w
n
x
i
n
+
b
)
)
2
\min_{\mathbf{w}, b} \sum_{i=1}^{m} (y_i - (w_1 x_{i1} + w_2 x_{i2} + ... + w_n x_{in} + b))^2














w

,

b





min

​













i

=

1





∑






m

​


(


y









i

​




−





(


w









1

​



x










i

1

​




+






w









2

​



x










i

2

​




+





...



+






w









n

​



x










in

​




+





b

)


)









2

这里，

m
m





m
是样本数，

n
n





n
是特征数。

### 2. 最小二乘法的求解方法

#### 2.1 直接法（正规方程）

假设我们用矩阵形式表示线性回归模型：

y
=
X
w
+
ε
\mathbf{y} = X \mathbf{w} + \varepsilon





y



=





X

w



+





ε

其中，

* y
  ∈
  R
  m
  ×
  1
  \mathbf{y} \in \mathbb{R}^{m \times 1}





  y



  ∈






  R










  m

  ×

  1
  是目标值向量。
* X
  ∈
  R
  m
  ×
  (
  n
  +
  1
  )
  X \in \mathbb{R}^{m \times (n+1)}





  X



  ∈






  R










  m

  ×

  (

  n

  +

  1

  )
  是输入数据矩阵（包括一个全 1 的列表示偏置项）。
* w
  ∈
  R
  (
  n
  +
  1
  )
  ×
  1
  \mathbf{w} \in \mathbb{R}^{(n+1) \times 1}





  w



  ∈






  R










  (

  n

  +

  1

  )

  ×

  1
  是待求参数向量。
* ε
  \varepsilon





  ε
  是误差项。

最小二乘法的目标是最小化误差平方和，即最小化：

J
(
w
)
=
∣
∣
X
w
−
y
∣
∣
2
J(\mathbf{w}) = ||X\mathbf{w} - \mathbf{y}||^2





J

(

w

)



=





∣∣

X

w



−





y

∣


∣









2

通过求导并设导数为 0，可以推导出最优解的封闭形式：

w
=
(
X
T
X
)
−
1
X
T
y
\mathbf{w} = (X^T X)^{-1} X^T \mathbf{y}





w



=





(


X









T

X


)










−

1


X









T

y

#### 2.2 迭代法（梯度下降）

当数据规模很大时，直接求解

w
=
(
X
T
X
)
−
1
X
T
y
\mathbf{w} = (X^T X)^{-1} X^T \mathbf{y}





w



=





(


X









T

X


)










−

1


X









T

y
可能会遇到计算问题：

* 计算

  X
  T
  X
  X^T X






  X









  T

  X
  需要

  O
  (
  n
  2
  m
  )
  O(n^2m)





  O

  (


  n









  2

  m

  )
  的时间复杂度，求逆

  O
  (
  n
  3
  )
  O(n^3)





  O

  (


  n









  3

  )
  ，当

  n
  n





  n
  很大时计算成本极高。
* X
  T
  X
  X^T X






  X









  T

  X
  可能是病态矩阵（接近奇异矩阵），导致数值计算不稳定。

梯度下降是一种优化方法，通过迭代更新参数来逐步找到最优解。其基本思想是计算损失函数

J
(
w
)
J(\mathbf{w})





J

(

w

)
对

w
\mathbf{w}





w
的梯度，并沿着梯度的负方向更新参数：

w
:
=
w
−
α
∇
J
(
w
)
\mathbf{w} := \mathbf{w} - \alpha \nabla J(\mathbf{w})





w



:=





w



−





α

∇

J

(

w

)

其中，学习率

α
\alpha





α
控制每次更新的步长。

梯度计算如下：

∇
J
(
w
)
=
1
m
X
T
(
X
w
−
y
)
\nabla J(\mathbf{w}) = \frac{1}{m} X^T (X \mathbf{w} - \mathbf{y})





∇

J

(

w

)



=
















m











1

​



X









T

(

X

w



−





y

)

因此，更新规则为：

w
:
=
w
−
α
m
X
T
(
X
w
−
y
)
\mathbf{w} := \mathbf{w} - \frac{\alpha}{m} X^T (X \mathbf{w} - \mathbf{y})





w



:=





w



−
















m











α

​



X









T

(

X

w



−





y

)

### 3. 直接法 vs 迭代法

| 方法 | 计算复杂度 | 适用情况 |
| --- | --- | --- |
| 直接法（正规方程） | O ( n 2 m + n 3 ) O(n^2 m + n^3)      O  (   n          2  m    +       n          3  ) | 适用于特征维度较小的数据集（如  n < 1 0 3 n < 10^3      n    <      1   0          3 ） |
| 迭代法（梯度下降） | 每次迭代  O ( n m ) O(nm)      O  (  nm  ) | 适用于大规模数据集（如  n > 1 0 4 n > 10^4      n    >      1   0          4 ） |

在

n
n





n
很大时，直接求逆是不可行的，因此我们使用梯度下降等迭代方法来近似求解。

### 4. 代码实现

#### 4.1 直接法（正规方程）

```python
import numpy as np

def least_squares_normal_eq(X, y):
    return np.linalg.inv(X.T @ X) @ X.T @ y

# 示例数据
X = np.array([[1, 1], [1, 2], [1, 3]])  # 增加一列1表示偏置项
y = np.array([[1], [2], [3]])

w = least_squares_normal_eq(X, y)
print("最优参数:", w)

```

#### 4.2 迭代法（梯度下降）

```python
import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    w = np.zeros((n, 1))  # 初始化权重
    for _ in range(epochs):
        gradient = (1/m) * X.T @ (X @ w - y)
        w -= lr * gradient
    return w

# 示例数据
X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([[1], [2], [3]])

w = gradient_descent(X, y)
print("最优参数:", w)

```

### 5. 结论

最小二乘法是解决回归问题的一种基本方法，正规方程适用于小规模数据，梯度下降适用于大规模数据。在实际应用中，需要根据数据规模选择合适的求解方法，以实现高效、稳定的计算。