---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f71715f3338373936363336:2f61727469636c652f64657461696c732f3935323233323637"
layout: post
title: "Python用requests库爬取网页内容,返回为为空的解决办法"
date: 2024-12-09 11:13:33 +08:00
description: "首先介紹一下我們用360搜索派取城市排名前20。我们爬取的网址：https://baike.so.c"
keywords: "python中的request到目标网址为什么返回未登录的页面"
categories: ['学习', 'Python']
tags: ['网络爬虫', 'Requests', 'Python', 'Bs']
artid: "95223267"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=95223267
    alt: "Python用requests库爬取网页内容,返回为为空的解决办法"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=95223267
featuredImagePreview: https://bing.ee123.net/img/rand?artid=95223267
---

# Python用requests库爬取网页内容，返回为‘’（为空）的解决办法。

首先介紹一下我們用360搜索派取城市排名前20。
  
我们爬取的网址：
<https://baike.so.com/doc/24368318-25185095.html>

我们要爬取的内容：
![爬取的内容](https://i-blog.csdnimg.cn/blog_migrate/31f99f7ba7e09d177a92a15133e89534.png)
  
html字段：
  
![源代码](https://i-blog.csdnimg.cn/blog_migrate/840e58392524613e18570454951cf685.png)
  
robots协议：
  
![360搜索的robots协议](https://i-blog.csdnimg.cn/blog_migrate/8cf12618b142d7092c5505b02e63fbfb.png)
  
现在我们开始用python IDLE 爬取
  
![爬取得到的是‘’空返回](https://i-blog.csdnimg.cn/blog_migrate/30fdeff4d9cfe14a3e27bb7acc3719cf.png)

```
import requests
r = requests.get("https://baike.so.com/doc/24368318-25185095.html")
r.status_code
r.text

```

结果分析，我们可以成功访问到该网页，但是得不到网页的结果。被360搜索识别，我们将headers修改。转自：链接: [link(
<https://blog.csdn.net/SL_World/article/details/84893957>
)

![修改headers后得到成功的返回](https://i-blog.csdnimg.cn/blog_migrate/3f6b580436d9a0245d38bb7e48d49f2d.png)
  
输出有个小插曲，网页内容很多，我是想将前500个字符输出，第一次格式错了

```
import requests
headers = {
    'Cookie':'OCSSID=4df0bjva6j7ejussu8al3eqo03',
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                 '(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
}
r = requests.get("https://baike.so.com/doc/24368318-25185095.html"， headers = headers)
r.status_code
r.text

```

接着我们对需要的内容进行爬取，用(.find)方法找到我们内容位置，用(.children)下行遍历的方法对内容进行爬取，用(isinstance)方法对内容进行筛选：

```
import requests
from bs4 import BeautifulSoup
import bs4
headers = {
    'Cookie':'OCSSID=4df0bjva6j7ejussu8al3eqo03',
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                 '(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
}
r = requests.get("https://baike.so.com/doc/24368318-25185095.html",  headers = headers)
r.status_code
r.encoding = r.apparent_encoding
soup = BeautifulSoup(r.text, "html.parser")
for tr in soup.find('tbody').children:
	if isinstance(tr, bs4.element.Tag):
		tds = tr('td')
		print([tds[0].string, tds[1].string, tds[2].string])

```

得到结果如下：
  
![输出结果](https://i-blog.csdnimg.cn/blog_migrate/977bac7b071eceadfa5787a31413a3be.png)
  
修改输出的数目，我们用Clist列表来存取所有城市的排名，将前20个输出代码如下：

```
import requests
from bs4 import BeautifulSoup
import bs4
Clist = list()  #存所有城市的列表
headers = {
    'Cookie':'OCSSID=4df0bjva6j7ejussu8al3eqo03',
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                 '(KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
}
r = requests.get("https://baike.so.com/doc/24368318-25185095.html", headers = headers)
r.encoding = r.apparent_encoding #将html的编码解码为utf-8格式
soup = BeautifulSoup(r.text, "html.parser") #重新排版
for tr in soup.find('tbody').children:     #将tbody标签的子列全部读取
	if isinstance(tr, bs4.element.Tag):    #筛选tb列表，将有内容的筛选出啦
	    tds = tr('td')
	    Clist.append([tds[0].string, tds[1].string, tds[2].string])
for i in range(21):
    print(Clist[i])

```

最终结果：
  
![最终的结果](https://i-blog.csdnimg.cn/blog_migrate/c250fbe550579c8ab546826035b2748a.png)
  
第一次写博客，如果错的地方希望您能指出。互相学习！！！