---
layout: post
title: "基于PyTorch的深度学习机器学习3"
date: 2025-03-10 22:19:16 +0800
description: "在多分类任务中，经常采用softmax激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。此时一般不宜选择sigmoid、tanh激活函数，因它们的导数都小于1，尤其是sigmoid的导数在[0,1/4]之间，多层叠加后，根据微积分链式法则，随着层数增多，导数或偏导将指数级变小。回归问题预测的不是类别，而是一个任意实数。PyTorch中已集成多种损失函数，这里介绍两个经典的损失函数，其他损失函数基本上是在它们的基础上的变种或延伸。"
keywords: "基于PyTorch的深度学习——机器学习3"
categories: ['未分类']
tags: ['深度学习', '机器学习', 'Pytorch']
artid: "146165553"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146165553
    alt: "基于PyTorch的深度学习机器学习3"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146165553
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146165553
cover: https://bing.ee123.net/img/rand?artid=146165553
image: https://bing.ee123.net/img/rand?artid=146165553
img: https://bing.ee123.net/img/rand?artid=146165553
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     基于PyTorch的深度学习——机器学习3
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     激活函数在神经网络中作用有很多，主要作用是给神经网络提供非线性建模能力。如果没有激活函数，那么再多层的神经网络也只能处理线性可分问题。
    </p>
    <p>
     在搭建神经网络时，如何选择激活函数？如果搭建的神经网络层数不多，选择sigmoid、tanh、relu、softmax都可以；而如果搭建的网络层次较多，那就需要小心，选择不当就可导致梯度消失问题。此时一般不宜选择sigmoid、tanh激活函数，因它们的导数都小于1，尤其是sigmoid的导数在[0,1/4]之间，多层叠加后，根据微积分链式法则，随着层数增多，导数或偏导将指数级变小。所以层数较多的激活函数需要考虑其导数不宜小于1当然也不能大于1，大于1将导致梯度爆炸，导数为1最好，而激活函数relu正好满足这个条件。所以，搭建比较深的神经网络时，一般使用relu激活函数，当然一般神经网络也可使用。
    </p>
    <p>
     ————————————————————
    </p>
    <p>
     损失函数(Loss Function)在机器学习中非常重要，因为训练模型的过程实际就是优化损失函数的过程。损失函数对每个参数的偏导数就是梯度下降中提到的梯度，防止过拟合时添加的正则化项也是加在损失函数后面。损失函数用来衡量模型的好坏，损失函数越小说明模型和参数越符合训练样本。任何能够衡量模型预测值与真实值之间的差异的函数都可以叫作损失函数。在机器学习中常用的损失函数有两种，即交叉熵(Cross Entropy)和均方误差(Mean squared error，MSE)，分别对应机器学习中的
     <strong>
      分类问题和回归问题
     </strong>
     。
    </p>
    <p>
     分类问题的损失函数一般采用交叉熵，交叉熵反应的两个概率分布的距离（不是欧氏距离）​。分类问题进一步又可分为多目标分类，如一次要判断100张图是否包含10种动物，或单目标分类。回归问题预测的不是类别，而是一个任意实数。在神经网络中一般只有一个输出节点，该输出值就是预测值。反应的预测值与实际值之间的距离可以用欧氏距离来表示，所以对这类问题通常使用均方差作为损失函数
    </p>
    <p>
     PyTorch中已集成多种损失函数，这里介绍两个经典的损失函数，其他损失函数基本上是在它们的基础上的变种或延伸。
    </p>
    <p>
     1.torch.nn.MSELoss
    </p>
    <p>
     2.torch.nn.CrossEntropyLoss交叉熵损失(Cross-Entropy Loss)又称对数似然损失(Log-likelihood Loss)、对数损失；二分类时还可称之为逻辑回归损失(Logistic Loss)。在PyTroch里，它不是严格意义上的交叉熵损失函数，而是先将Input经过softmax激活函数，将向量“归一化”成概率形式，然后再与target计算严格意义上的交叉熵损失。在多分类任务中，经常采用softmax激活函数+交叉熵损失函数，因为交叉熵描述了两个概率分布的差异，然而神经网络输出的是向量，并不是概率分布的形式。所以需要softmax激活函数将一个向量进行“归一化”成概率分布的形式，再采用交叉熵损失函数计算loss。
    </p>
   </div>
  </div>
 </article>
 <p alt="687474:70733a2f2f626c6f672e6373646e2e6e65742f57697334652f:61727469636c652f64657461696c732f313436313635353533" class_="artid" style="display:none">
 </p>
</div>


