---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6c7a6d31323237383832382f:61727469636c652f64657461696c732f313436323336303937"
layout: post
title: "通过特征值和特征向量实现的图像压缩和特征提取"
date: 2025-03-14 00:45:00 +0800
description: "图像压缩：通过SVD分解图像矩阵，保留最大的几个奇异值及其对应的奇异向量，重构图像以实现压缩。将图像矩阵分解为。保留前 k 个奇异值及其对应的奇异向量。通过近似重构图像。评估压缩效果。特征提取：通过PCA计算数据的协方差矩阵的特征值和特征向量，选择最重要的特征向量作为新的特征空间，实现降维。标准化数据。计算协方差矩阵并求解特征值和特征向量。选择前 k 个主成分。将数据投影到主成分空间。评估特征提取效果。"
keywords: "通过特征值和特征向量实现的图像压缩和特征提取"
categories: ['人工智能原理']
tags: ['计算机视觉', '目标检测', '深度学习', '机器学习', '图像处理', '人工智能']
artid: "146236097"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146236097
    alt: "通过特征值和特征向量实现的图像压缩和特征提取"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146236097
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146236097
cover: https://bing.ee123.net/img/rand?artid=146236097
image: https://bing.ee123.net/img/rand?artid=146236097
img: https://bing.ee123.net/img/rand?artid=146236097
---

# 通过特征值和特征向量实现的图像压缩和特征提取

前文，我们在学习人工智能的线性代数基础的时候，就了解到，矩阵在人工智能中被广泛使用，接下来我们就从大家非常常见的图像开始，深度理解矩阵在人工智能中的应用。有关线性代数基础的文章可以看的我CSDN:
[人工智能中的线性代数基础详解-CSDN博客](https://blog.csdn.net/lzm12278828/article/details/146227363 "人工智能中的线性代数基础详解-CSDN博客")

在图像处理和机器学习中，特征值和特征向量（尤其是奇异值分解，SVD）被广泛用于图像压缩和特征提取。接下来我们详细讲解
图像压缩（通过SVD）和特征提取（通过PCA）
的每一个步骤，包括数学原理、具体操作和示例。

## 一、图像压缩（通过奇异值分解，SVD）

图像压缩的目标是
减少图像数据的存储空间
，同时尽量保留图像的主要信息。奇异值分解（SVD）是一种强大的工具，可以实现高效的图像压缩。SVD将A矩阵分解成三个其他矩阵的示意图如下（分两种情况）：

![](https://i-blog.csdnimg.cn/direct/6b5b478b03ad457c81addf0efc5c8506.png)

### 1.数学原理

一张图像可以表示为一个m×n 的矩阵 A
，其中每个元素对应一个像素的
灰度值或颜色值（注意这个不是彩色图像）
。SVD将图像
矩阵 A 分解为三个矩阵的乘积
：

![](https://i-blog.csdnimg.cn/direct/6ed7bbc9ffa24ec8b7c43c2d6ba5413a.png)

![](https://i-blog.csdnimg.cn/direct/3e60f5b9d70d48e39be7f27b4e4cec13.png)

其中：

* U 是一个 m×m 的
  正交矩阵
  （即
  ![U^{T}U=I](https://latex.csdn.net/eq?U%5E%7BT%7DU%3DI)
  ），
  其列向量是 A 的左奇异向量
  ，表示图像的行空间的基。

![](https://i-blog.csdnimg.cn/direct/5f3f3777ff24452f8ffe37c258f7f900.png)

* Σ 是一个 m×n 的
  对角矩阵
  ，
  **对角线上的元素是奇异值**
  σ1​,σ2​,…,σk​，且 σ1​≥σ2​≥⋯≥σk，通常按从大到小的顺序排列​，表示每个基向量的重要性。
* V 是一个 n×n 的
  正交矩阵
  （即
  ![V^{T}V=I](https://latex.csdn.net/eq?V%5E%7BT%7DV%3DI)
  ），其列向量是 A 的右奇异向量，表示图像的列空间的基。

![](https://i-blog.csdnimg.cn/direct/962026ea58ae409e9ca824d235b33a75.png)

通过保留最大的几个奇异值及其对应的奇异向量，可以近似重构图像，从而实现压缩。例如：假设我们有
一个 1080×1920 的图像
矩阵 A。通过SVD分解后，我们发现前10个奇异值占据了大部分信息。因此，可以只保留前10个奇异值及其对应的奇异向量，
将图像压缩为一个 1080×10 和 10×1920 的矩阵
，大大减少了存储空间。

### 2.图像压缩的具体步骤

#### 步骤1：图像矩阵化

将图像数据表示为一个矩阵 A。对于灰度图像，每个像素的灰度值构成矩阵的一个元素；对于彩色图像，可以分别对RGB三个通道进行处理。

**示例**
：假设有一张 5×5 的灰度图像，其矩阵表示为：

![](https://i-blog.csdnimg.cn/direct/ae5e18839cfb43c1837c632ab7c0fa4f.png)

#### 步骤2：SVD分解

对矩阵 A 进行SVD分解，得到 U、Σ 和
![V^{T}](https://latex.csdn.net/eq?V%5E%7BT%7D)
。分解的过程参照下图（网上下载的），其中的M为本文中的A。

![](https://i-blog.csdnimg.cn/direct/361fb5a91abf410d874ac20bf7560371.png)

**如何通过 SVD 分解得到奇异矩阵，以下是分解步骤：**

**（1）计算
![A^{T}A](https://latex.csdn.net/eq?A%5E%7BT%7DA)
和
![AA^{T}](https://latex.csdn.net/eq?AA%5E%7BT%7D)**
：

**![A^{T}A](https://latex.csdn.net/eq?A%5E%7BT%7DA)**
和
**![AA^{T}](https://latex.csdn.net/eq?AA%5E%7BT%7D)**
是对称矩阵，且它们的特征值和特征向量与 A 的奇异值和奇异向量有关。

**（2）求
![A^{T}A](https://latex.csdn.net/eq?A%5E%7BT%7DA)
和
![AA^{T}](https://latex.csdn.net/eq?AA%5E%7BT%7D)
的特征值和特征向量**
：

* 计算
  **![A^{T}A](https://latex.csdn.net/eq?A%5E%7BT%7DA)**
  的特征值和特征向量，
  得到矩阵 V 和奇异值的平方
  。
* 计算
  **![AA^{T}](https://latex.csdn.net/eq?AA%5E%7BT%7D)**
  的特征值和特征向量，
  得到矩阵 U 和奇异值的平方
  。

**（3）
构造
奇异值矩阵 Σ（注意是构造出来的，不是计算得到的）**
：

* 奇异值是
  **![A^{T}A](https://latex.csdn.net/eq?A%5E%7BT%7DA)**

  或
  **![AA^{T}](https://latex.csdn.net/eq?AA%5E%7BT%7D)**

  的特征值的平方根。
* 将奇异值按从大到小的顺序排列在对角矩阵 Σ 中。

**（4）构造正交矩阵 U 和 V**
：

* V 的列是
  **![A^{T}A](https://latex.csdn.net/eq?A%5E%7BT%7DA)**

  的特征向量。
* U 的列是
  **![AA^{T}](https://latex.csdn.net/eq?AA%5E%7BT%7D)**

  的特征向量。

**（5）验证分解结果**
：

* 通过
  ![](https://i-blog.csdnimg.cn/direct/b2d6d8dbf27f4644a67cf2ddadda7c12.png)
  验证分解的正确性。

![](https://i-blog.csdnimg.cn/direct/ca13136d0853426683caf8263fe0ece2.png)

**以下是示例**
：假设

分解结果
为：

![](https://i-blog.csdnimg.cn/direct/b2d6d8dbf27f4644a67cf2ddadda7c12.png)

其中三个矩阵分别为：

![](https://i-blog.csdnimg.cn/direct/3e485851c5a840c788f39de94d28553e.png)

![](https://i-blog.csdnimg.cn/direct/922d2840d4934eedae0923bbb3812032.png)

#### 步骤3：选择重要的奇异值

保留前 k 个最大的奇异值及其对应的奇异向量
，其中 k 远小于 min(m,n)。这一步可以显著减少数据量。

**示例**
：假设我们选择 k=2（原本有5个），则新的矩阵为：

![](https://i-blog.csdnimg.cn/direct/fa409543aab140d48d745bf194ff219d.png)

其中：

![](https://i-blog.csdnimg.cn/direct/f8e6c96fc801471db54b7f8c3294ac2c.png)

注意：
Uk的列数跟Σk的列数相同，Vk的行数跟Σk的行数相同
。

**以下为补充内容：**

在SVD分解后，
确定保留的奇异值数量 k 是一个关键步骤
，因为它直接影响到数据压缩或降维的效果。以下是几种常用的方法来确定 k 的值：

##### （1） **累积能量百分比**

奇异值的平方通常表示矩阵的能量分布。通过计算累积能量百分比，可以选择一个 k，使得保留的奇异值能够解释大部分的能量（例如90%或95%）。

**累积能量百分比的步骤：**

1）计算所有奇异值的平方和
![](https://i-blog.csdnimg.cn/direct/b9618674924443c8b33f344ec45c30ba.png)
。

2）计算每个奇异值的累积能量百分比：

![](https://i-blog.csdnimg.cn/direct/8caa596323664ac6a50878b717701cfb.png)

3）选择 k，使得累积能量百分比达到一个阈值（如90%）。

**示例：**
假设奇异值为 σ1​,σ2​,…,σr​，当 k=10 时，累积能量百分比为92%，则可以选择 k=10。

##### （2） **奇异值分布曲线**

通过绘制奇异值的分布曲线（通常是按降序排列的奇异值大小），观察奇异值的衰减情况。通常，奇异值会快速下降，形成一个“肘部”（elbow point），选择肘部位置作为 k 的值。

**示例：**
在奇异值分布曲线上，当 k=20 时，奇异值的下降速度明显减缓，可以将 k 设为20。

##### （3） **重构误差**

通过尝试不同的 k 值，计算重构矩阵与原始矩阵之间的误差（如均方误差MSE或Frobenius范数）。选择一个 k，使得重构误差在可接受范围内。

**重构误差的步骤：**

1）对于不同的 k，计算重构矩阵
![](https://i-blog.csdnimg.cn/direct/bd00d950dfdb4987a7d04358728e53e9.png)
。

2）计算重构误差：
![](https://i-blog.csdnimg.cn/direct/efa274c4b16a4aee8c93a13edda0c0c9.png)

3）选择一个 k，使得MSE小于某个阈值。

##### （4） **基于应用需求**

在某些应用场景中，可以根据实际需求选择 k。例如：

* 在图像压缩中，选择较小的 k 可以显著减少存储空间，但可能会丢失一些细节。
* 在图像去噪中，选择较小的 k 可以去除噪声，但可能会丢失一些高频细节。

#### 步骤4：重构图像

通过 Ak​ 近似重构图像。虽然 Ak​ 的维度比原始矩阵小，但可以通过以下公式重构近似图像：

![](https://i-blog.csdnimg.cn/direct/9e9fbd00d60d48358013acc71ae34e41.png)

**示例**
：重构后的图像矩阵为：

![](https://i-blog.csdnimg.cn/direct/05ce4aeeb2764c42a2919cf298737121.png)

其中
![\tilde{a}_{ij}](https://latex.csdn.net/eq?%5Ctilde%7Ba%7D_%7Bij%7D)
是近似值。

#### 步骤5：评估压缩效果

通过比较原始图像和重构图像的差异（如均方误差MSE或峰值信噪比PSNR），评估压缩效果。

## 二、特征提取（通过主成分分析，PCA）

特征提取是
从原始数据中提取有意义的特征
，以减少数据维度并提高模型性能。主成分分析（PCA）是一种基于特征值和特征向量的特征提取方法。

假设我们有一组图像数据，每张
图像有1000个像素
。通过PCA，我们计算出协方差矩阵的特征值和特征向量，发现前50个特征值占据了大部分方差。因此，
可以将每张图像投影到这50个特征向量上，将图像的维度从1000降为50
，同时保留主要信息。

### 1.数学原理

PCA通过将数据投影到方差最大的方向上，提取数据的主要特征，从而实现降维。其核心是
通过协方差矩阵的特征值和特征向量来确定主成分
。

PCA通过以下步骤实现特征提取：

#### 步骤1：数据预处理（ **标准化数据** ）

将数据标准化，
使每个特征的均值为0，方差为1
。对于图像数据，可以将像素值归一化到 [0, 1] 或 [-1, 1]。

**示例**
：假设有一组图像数据 X，其中每一行是一个图像的像素向量。

#### 步骤2：计算协方差矩阵

协方差矩阵 C 表示数据特征之间的相关性：

![](https://i-blog.csdnimg.cn/direct/d5753d163b57445bb0af59b6cca2529c.png)

其中 n 是样本数量。

#### 步骤3：求解特征值和特征向量

计算协方差矩阵 C 的特征值 λi​ 和特征向量 vi​。
特征值表示每个方向上的方差大小，特征向量表示数据的主要方向
。

**示例**
：假设特征值按大小排序为 λ1​≥λ2​≥⋯≥λd​，对应的特征向量为 v1​,v2​,…,vd​。

#### 步骤4：选择主成分

选择前 k 个特征值最大的特征向量作为主成分，构成投影矩阵 Vk​。

**示例**
：假设选择前2个主成分，则投影矩阵为：

![](https://i-blog.csdnimg.cn/direct/586b681b93b94eb08b8e67b9624d1d31.png)

#### 步骤5：数据投影

将原始数据 X 投影到主成分空间，得到降维后的数据 Y：

![](https://i-blog.csdnimg.cn/direct/4ec2e268a03f44a2b4c366205cda1417.png)

**示例**
：假设原始数据 X 是 m×d 的矩阵，投影后得到 m×k 的矩阵 Y。

#### 步骤6：评估特征提取效果

通过比较降维前后的数据，评估特征提取的效果。例如，可以通过重构误差或分类任务的性能来评估。

##### 

## 总结

* **图像压缩**
  ：通过SVD分解图像矩阵，保留最大的几个奇异值及其对应的奇异向量，重构图像以实现压缩。

  + 将图像矩阵分解为
    ![](https://i-blog.csdnimg.cn/direct/dda83b18277f4f02b468685eb741f732.png)
    。
  + 保留前 k 个奇异值及其对应的奇异向量。
  + 通过
    ![](https://i-blog.csdnimg.cn/direct/10aed694b6bc4c0d8e51eadcac67388f.png)
    近似重构图像。
  + 评估压缩效果。
* **特征提取**
  ：通过PCA计算数据的协方差矩阵的特征值和特征向量，选择最重要的特征向量作为新的特征空间，实现降维。

  + 标准化数据。
  + 计算协方差矩阵并求解特征值和特征向量。
  + 选择前 k 个主成分。
  + 将数据投影到主成分空间。
  + 评估特征提取效果。

这两种方法都利用了特征值和特征向量的性质，分别在图像压缩和特征提取中发挥了重要作用。