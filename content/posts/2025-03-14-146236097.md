---
layout: post
title: "通过特征值和特征向量实现的图像压缩和特征提取"
date: 2025-03-14 00:45:00 +0800
description: "图像压缩：通过SVD分解图像矩阵，保留最大的几个奇异值及其对应的奇异向量，重构图像以实现压缩。将图像矩阵分解为。保留前 k 个奇异值及其对应的奇异向量。通过近似重构图像。评估压缩效果。特征提取：通过PCA计算数据的协方差矩阵的特征值和特征向量，选择最重要的特征向量作为新的特征空间，实现降维。标准化数据。计算协方差矩阵并求解特征值和特征向量。选择前 k 个主成分。将数据投影到主成分空间。评估特征提取效果。"
keywords: "通过特征值和特征向量实现的图像压缩和特征提取"
categories: ['人工智能原理']
tags: ['计算机视觉', '目标检测', '深度学习', '机器学习', '图像处理', '人工智能']
artid: "146236097"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146236097
    alt: "通过特征值和特征向量实现的图像压缩和特征提取"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146236097
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146236097
cover: https://bing.ee123.net/img/rand?artid=146236097
image: https://bing.ee123.net/img/rand?artid=146236097
img: https://bing.ee123.net/img/rand?artid=146236097
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     通过特征值和特征向量实现的图像压缩和特征提取
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     前文，我们在学习人工智能的线性代数基础的时候，就了解到，矩阵在人工智能中被广泛使用，接下来我们就从大家非常常见的图像开始，深度理解矩阵在人工智能中的应用。有关线性代数基础的文章可以看的我CSDN:
     <a href="https://blog.csdn.net/lzm12278828/article/details/146227363" title="人工智能中的线性代数基础详解-CSDN博客">
      人工智能中的线性代数基础详解-CSDN博客
     </a>
    </p>
    <p>
     在图像处理和机器学习中，特征值和特征向量（尤其是奇异值分解，SVD）被广泛用于图像压缩和特征提取。接下来我们详细讲解
     <span style="color:#fe2c24">
      图像压缩（通过SVD）和特征提取（通过PCA）
     </span>
     的每一个步骤，包括数学原理、具体操作和示例。
    </p>
    <h2>
     一、图像压缩（通过奇异值分解，SVD）
    </h2>
    <p>
     图像压缩的目标是
     <span style="color:#fe2c24">
      减少图像数据的存储空间
     </span>
     ，同时尽量保留图像的主要信息。奇异值分解（SVD）是一种强大的工具，可以实现高效的图像压缩。SVD将A矩阵分解成三个其他矩阵的示意图如下（分两种情况）：
    </p>
    <p>
     <img alt="" height="465" src="https://i-blog.csdnimg.cn/direct/6b5b478b03ad457c81addf0efc5c8506.png" width="823"/>
    </p>
    <h3>
     1.数学原理
    </h3>
    <p>
     <span style="color:#fe2c24">
      一张图像可以表示为一个m×n 的矩阵 A
     </span>
     ，其中每个元素对应一个像素的
     <span style="color:#fe2c24">
      灰度值或颜色值（注意这个不是彩色图像）
     </span>
     。SVD将图像
     <span style="background-color:#d7d8d9">
      矩阵 A 分解为三个矩阵的乘积
     </span>
     ：
    </p>
    <p style="text-align:center">
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/6ed7bbc9ffa24ec8b7c43c2d6ba5413a.png" width="119"/>
    </p>
    <p class="img-center">
     <img alt="" height="140" src="https://i-blog.csdnimg.cn/direct/3e60f5b9d70d48e39be7f27b4e4cec13.png" width="429"/>
    </p>
    <p>
     其中：
    </p>
    <ul>
     <li>
      <p>
       U 是一个 m×m 的
       <span style="color:#4da8ee">
        正交矩阵
       </span>
       （即
       <img alt="U^{T}U=I" class="mathcode" src="https://latex.csdn.net/eq?U%5E%7BT%7DU%3DI">
        ），
        <span style="color:#fe2c24">
         其列向量是 A 的左奇异向量
        </span>
        ，表示图像的行空间的基。
       </img>
      </p>
     </li>
    </ul>
    <p class="img-center">
     <img alt="" height="137" src="https://i-blog.csdnimg.cn/direct/5f3f3777ff24452f8ffe37c258f7f900.png" width="371"/>
    </p>
    <ul>
     <li>
      <p>
       Σ 是一个 m×n 的
       <span style="color:#4da8ee">
        对角矩阵
       </span>
       ，
       <span style="color:#fe2c24">
        <strong>
         对角线上的元素是奇异值
        </strong>
       </span>
       σ1​,σ2​,…,σk​，且 σ1​≥σ2​≥⋯≥σk，通常按从大到小的顺序排列​，表示每个基向量的重要性。
      </p>
     </li>
     <li>
      <p>
       V 是一个 n×n 的
       <span style="color:#4da8ee">
        正交矩阵
       </span>
       （即
       <img alt="V^{T}V=I" class="mathcode" src="https://latex.csdn.net/eq?V%5E%7BT%7DV%3DI">
        ），其列向量是 A 的右奇异向量，表示图像的列空间的基。
       </img>
      </p>
     </li>
    </ul>
    <p class="img-center">
     <img alt="" height="131" src="https://i-blog.csdnimg.cn/direct/962026ea58ae409e9ca824d235b33a75.png" width="396"/>
    </p>
    <p>
     通过保留最大的几个奇异值及其对应的奇异向量，可以近似重构图像，从而实现压缩。例如：假设我们有
     <span style="color:#a2e043">
      一个 1080×1920 的图像
     </span>
     矩阵 A。通过SVD分解后，我们发现前10个奇异值占据了大部分信息。因此，可以只保留前10个奇异值及其对应的奇异向量，
     <span style="color:#a2e043">
      将图像压缩为一个 1080×10 和 10×1920 的矩阵
     </span>
     ，大大减少了存储空间。
    </p>
    <h3>
     2.图像压缩的具体步骤
    </h3>
    <h4>
     步骤1：图像矩阵化
    </h4>
    <p>
     将图像数据表示为一个矩阵 A。对于灰度图像，每个像素的灰度值构成矩阵的一个元素；对于彩色图像，可以分别对RGB三个通道进行处理。
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：假设有一张 5×5 的灰度图像，其矩阵表示为：
    </p>
    <p class="img-center">
     <img alt="" height="133" src="https://i-blog.csdnimg.cn/direct/ae5e18839cfb43c1837c632ab7c0fa4f.png" width="329"/>
    </p>
    <h4>
     步骤2：SVD分解
    </h4>
    <p>
     对矩阵 A 进行SVD分解，得到 U、Σ 和
     <img alt="V^{T}" class="mathcode" src="https://latex.csdn.net/eq?V%5E%7BT%7D"/>
     。分解的过程参照下图（网上下载的），其中的M为本文中的A。
    </p>
    <p>
     <img alt="" height="657" src="https://i-blog.csdnimg.cn/direct/361fb5a91abf410d874ac20bf7560371.png" width="1350"/>
    </p>
    <p>
     <strong>
     </strong>
    </p>
    <p>
     <strong>
      <span style="color:#fe2c24">
       如何通过 SVD 分解得到奇异矩阵，以下是分解步骤：
      </span>
     </strong>
    </p>
    <p>
     <strong>
      （1）计算
      <img alt="A^{T}A" class="mathcode" src="https://latex.csdn.net/eq?A%5E%7BT%7DA"/>
      和
      <img alt="AA^{T}" class="mathcode" src="https://latex.csdn.net/eq?AA%5E%7BT%7D"/>
     </strong>
     ：
    </p>
    <p>
     <strong>
      <img alt="A^{T}A" class="mathcode" src="https://latex.csdn.net/eq?A%5E%7BT%7DA"/>
     </strong>
     和
     <strong>
      <img alt="AA^{T}" class="mathcode" src="https://latex.csdn.net/eq?AA%5E%7BT%7D"/>
     </strong>
     是对称矩阵，且它们的特征值和特征向量与 A 的奇异值和奇异向量有关。
    </p>
    <p>
     <strong>
      （2）求
      <img alt="A^{T}A" class="mathcode" src="https://latex.csdn.net/eq?A%5E%7BT%7DA"/>
      和
      <img alt="AA^{T}" class="mathcode" src="https://latex.csdn.net/eq?AA%5E%7BT%7D"/>
      的特征值和特征向量
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       计算
       <strong>
        <img alt="A^{T}A" class="mathcode" src="https://latex.csdn.net/eq?A%5E%7BT%7DA"/>
       </strong>
       的特征值和特征向量，
       <span style="color:#fe2c24">
        得到矩阵 V 和奇异值的平方
       </span>
       。
      </p>
     </li>
     <li>
      <p>
       计算
       <strong>
        <img alt="AA^{T}" class="mathcode" src="https://latex.csdn.net/eq?AA%5E%7BT%7D"/>
       </strong>
       的特征值和特征向量，
       <span style="color:#fe2c24">
        得到矩阵 U 和奇异值的平方
       </span>
       。
      </p>
     </li>
    </ul>
    <p>
     <strong>
      （3）
      <span style="color:#fe2c24">
       构造
      </span>
      奇异值矩阵 Σ（注意是构造出来的，不是计算得到的）
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       <span style="background-color:#d7d8d9">
        奇异值是
       </span>
       <strong>
        <img alt="A^{T}A" class="mathcode" src="https://latex.csdn.net/eq?A%5E%7BT%7DA"/>
       </strong>
       <span style="background-color:#d7d8d9">
        或
       </span>
       <strong>
        <img alt="AA^{T}" class="mathcode" src="https://latex.csdn.net/eq?AA%5E%7BT%7D"/>
       </strong>
       <span style="background-color:#d7d8d9">
        的特征值的平方根。
       </span>
      </p>
     </li>
     <li>
      <p>
       <span style="background-color:#d7d8d9">
        将奇异值按从大到小的顺序排列在对角矩阵 Σ 中。
       </span>
      </p>
     </li>
    </ul>
    <p>
     <strong>
      （4）构造正交矩阵 U 和 V
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       <span style="background-color:#d7d8d9">
        V 的列是
       </span>
       <strong>
        <img alt="A^{T}A" class="mathcode" src="https://latex.csdn.net/eq?A%5E%7BT%7DA"/>
       </strong>
       <span style="background-color:#d7d8d9">
        的特征向量。
       </span>
      </p>
     </li>
     <li>
      <p>
       <span style="background-color:#d7d8d9">
        U 的列是
       </span>
       <strong>
        <img alt="AA^{T}" class="mathcode" src="https://latex.csdn.net/eq?AA%5E%7BT%7D"/>
       </strong>
       <span style="background-color:#d7d8d9">
        的特征向量。
       </span>
      </p>
     </li>
    </ul>
    <p>
     <strong>
      （5）验证分解结果
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       通过
       <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/b2d6d8dbf27f4644a67cf2ddadda7c12.png" width="119"/>
       验证分解的正确性。
      </p>
     </li>
    </ul>
    <p class="img-center">
     <img alt="" height="657" src="https://i-blog.csdnimg.cn/direct/ca13136d0853426683caf8263fe0ece2.png" width="603"/>
    </p>
    <p>
     <strong>
      以下是示例
     </strong>
     ：假设
     <span style="color:#fe2c24">
      <span style="background-color:#d7d8d9">
       分解结果
      </span>
     </span>
     为：
    </p>
    <p style="text-align:center">
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/b2d6d8dbf27f4644a67cf2ddadda7c12.png" width="119"/>
    </p>
    <p>
     其中三个矩阵分别为：
    </p>
    <p style="text-align:center">
     <img alt="" height="133" src="https://i-blog.csdnimg.cn/direct/3e485851c5a840c788f39de94d28553e.png" width="560"/>
    </p>
    <p style="text-align:center">
     <img alt="" height="120" src="https://i-blog.csdnimg.cn/direct/922d2840d4934eedae0923bbb3812032.png" width="264"/>
    </p>
    <h4>
     步骤3：选择重要的奇异值
    </h4>
    <p>
     <span style="color:#fe2c24">
      保留前 k 个最大的奇异值及其对应的奇异向量
     </span>
     ，其中 k 远小于 min(m,n)。这一步可以显著减少数据量。
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：假设我们选择 k=2（原本有5个），则新的矩阵为：
    </p>
    <p class="img-center">
     <img alt="" height="31" src="https://i-blog.csdnimg.cn/direct/fa409543aab140d48d745bf194ff219d.png" width="153"/>
    </p>
    <p>
     其中：
    </p>
    <p class="img-center">
     <img alt="" height="123" src="https://i-blog.csdnimg.cn/direct/f8e6c96fc801471db54b7f8c3294ac2c.png" width="640"/>
    </p>
    <p>
     注意：
     <span style="color:#fe2c24">
      Uk的列数跟Σk的列数相同，Vk的行数跟Σk的行数相同
     </span>
     。
    </p>
    <p>
    </p>
    <p>
     <span style="color:#fe2c24">
      <strong>
       <span style="background-color:#d7d8d9">
        以下为补充内容：
       </span>
      </strong>
     </span>
    </p>
    <p>
     在SVD分解后，
     <span style="color:#fe2c24">
      确定保留的奇异值数量 k 是一个关键步骤
     </span>
     ，因为它直接影响到数据压缩或降维的效果。以下是几种常用的方法来确定 k 的值：
    </p>
    <h5>
     （1）
     <strong>
      累积能量百分比
     </strong>
    </h5>
    <p>
     奇异值的平方通常表示矩阵的能量分布。通过计算累积能量百分比，可以选择一个 k，使得保留的奇异值能够解释大部分的能量（例如90%或95%）。
    </p>
    <p>
     <strong>
      累积能量百分比的步骤：
     </strong>
    </p>
    <p>
     1）计算所有奇异值的平方和
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/b9618674924443c8b33f344ec45c30ba.png" width="76"/>
     。
    </p>
    <p>
     2）计算每个奇异值的累积能量百分比：
    </p>
    <p class="img-center">
     <img alt="" height="64" src="https://i-blog.csdnimg.cn/direct/8caa596323664ac6a50878b717701cfb.png" width="340"/>
    </p>
    <p>
     3）选择 k，使得累积能量百分比达到一个阈值（如90%）。
    </p>
    <p>
     <strong>
      示例：
     </strong>
     假设奇异值为 σ1​,σ2​,…,σr​，当 k=10 时，累积能量百分比为92%，则可以选择 k=10。
    </p>
    <h5>
     （2）
     <strong>
      奇异值分布曲线
     </strong>
    </h5>
    <p>
     通过绘制奇异值的分布曲线（通常是按降序排列的奇异值大小），观察奇异值的衰减情况。通常，奇异值会快速下降，形成一个“肘部”（elbow point），选择肘部位置作为 k 的值。
    </p>
    <p>
     <strong>
      示例：
     </strong>
     在奇异值分布曲线上，当 k=20 时，奇异值的下降速度明显减缓，可以将 k 设为20。
    </p>
    <h5>
     （3）
     <strong>
      重构误差
     </strong>
    </h5>
    <p>
     通过尝试不同的 k 值，计算重构矩阵与原始矩阵之间的误差（如均方误差MSE或Frobenius范数）。选择一个 k，使得重构误差在可接受范围内。
    </p>
    <p>
     <strong>
      重构误差的步骤：
     </strong>
    </p>
    <p>
     1）对于不同的 k，计算重构矩阵
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/bd00d950dfdb4987a7d04358728e53e9.png" width="144"/>
     。
    </p>
    <p>
     2）计算重构误差：
     <img alt="" height="49" src="https://i-blog.csdnimg.cn/direct/efa274c4b16a4aee8c93a13edda0c0c9.png" width="225"/>
    </p>
    <p>
     3）选择一个 k，使得MSE小于某个阈值。
    </p>
    <h5>
     （4）
     <strong>
      基于应用需求
     </strong>
    </h5>
    <p>
     在某些应用场景中，可以根据实际需求选择 k。例如：
    </p>
    <ul>
     <li>
      <p>
       在图像压缩中，选择较小的 k 可以显著减少存储空间，但可能会丢失一些细节。
      </p>
     </li>
     <li>
      <p>
       在图像去噪中，选择较小的 k 可以去除噪声，但可能会丢失一些高频细节。
      </p>
     </li>
    </ul>
    <p>
    </p>
    <h4>
     步骤4：重构图像
    </h4>
    <p>
     通过 Ak​ 近似重构图像。虽然 Ak​ 的维度比原始矩阵小，但可以通过以下公式重构近似图像：
    </p>
    <p class="img-center">
     <img alt="" height="30" src="https://i-blog.csdnimg.cn/direct/9e9fbd00d60d48358013acc71ae34e41.png" width="151"/>
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：重构后的图像矩阵为：
    </p>
    <p class="img-center">
     <img alt="" height="123" src="https://i-blog.csdnimg.cn/direct/05ce4aeeb2764c42a2919cf298737121.png" width="263"/>
    </p>
    <p>
     其中
     <img alt="\tilde{a}_{ij}" class="mathcode" src="https://latex.csdn.net/eq?%5Ctilde%7Ba%7D_%7Bij%7D"/>
     是近似值。
    </p>
    <h4>
     步骤5：评估压缩效果
    </h4>
    <p>
     通过比较原始图像和重构图像的差异（如均方误差MSE或峰值信噪比PSNR），评估压缩效果。
    </p>
    <p>
    </p>
    <h2>
     二、特征提取（通过主成分分析，PCA）
    </h2>
    <p>
     特征提取是
     <span style="color:#fe2c24">
      从原始数据中提取有意义的特征
     </span>
     ，以减少数据维度并提高模型性能。主成分分析（PCA）是一种基于特征值和特征向量的特征提取方法。
    </p>
    <p>
     假设我们有一组图像数据，每张
     <span style="color:#a2e043">
      图像有1000个像素
     </span>
     。通过PCA，我们计算出协方差矩阵的特征值和特征向量，发现前50个特征值占据了大部分方差。因此，
     <span style="color:#a2e043">
      可以将每张图像投影到这50个特征向量上，将图像的维度从1000降为50
     </span>
     ，同时保留主要信息。
    </p>
    <h3>
     1.数学原理
    </h3>
    <p>
     PCA通过将数据投影到方差最大的方向上，提取数据的主要特征，从而实现降维。其核心是
     <span style="color:#fe2c24">
      通过协方差矩阵的特征值和特征向量来确定主成分
     </span>
     。
    </p>
    <p>
     PCA通过以下步骤实现特征提取：
    </p>
    <h4>
     步骤1：数据预处理（
     <strong>
      标准化数据
     </strong>
     ）
    </h4>
    <p>
     将数据标准化，
     <span style="color:#fe2c24">
      使每个特征的均值为0，方差为1
     </span>
     。对于图像数据，可以将像素值归一化到 [0, 1] 或 [-1, 1]。
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：假设有一组图像数据 X，其中每一行是一个图像的像素向量。
    </p>
    <h4>
     步骤2：计算协方差矩阵
    </h4>
    <p>
     协方差矩阵 C 表示数据特征之间的相关性：
    </p>
    <p class="img-center">
     <img alt="" height="50" src="https://i-blog.csdnimg.cn/direct/d5753d163b57445bb0af59b6cca2529c.png" width="159"/>
    </p>
    <p>
     其中 n 是样本数量。
    </p>
    <h4>
     步骤3：求解特征值和特征向量
    </h4>
    <p>
     计算协方差矩阵 C 的特征值 λi​ 和特征向量 vi​。
     <span style="color:#fe2c24">
      特征值表示每个方向上的方差大小，特征向量表示数据的主要方向
     </span>
     。
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：假设特征值按大小排序为 λ1​≥λ2​≥⋯≥λd​，对应的特征向量为 v1​,v2​,…,vd​。
    </p>
    <h4>
     步骤4：选择主成分
    </h4>
    <p>
     选择前 k 个特征值最大的特征向量作为主成分，构成投影矩阵 Vk​。
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：假设选择前2个主成分，则投影矩阵为：
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="" height="36" src="https://i-blog.csdnimg.cn/direct/586b681b93b94eb08b8e67b9624d1d31.png" width="145"/>
    </p>
    <h4>
     步骤5：数据投影
    </h4>
    <p>
     将原始数据 X 投影到主成分空间，得到降维后的数据 Y：
    </p>
    <p style="text-align:center">
     <img alt="" height="32" src="https://i-blog.csdnimg.cn/direct/4ec2e268a03f44a2b4c366205cda1417.png" width="105"/>
    </p>
    <p>
     <strong>
      示例
     </strong>
     ：假设原始数据 X 是 m×d 的矩阵，投影后得到 m×k 的矩阵 Y。
    </p>
    <h4>
     步骤6：评估特征提取效果
    </h4>
    <p>
     通过比较降维前后的数据，评估特征提取的效果。例如，可以通过重构误差或分类任务的性能来评估。
    </p>
    <p>
    </p>
    <h5>
    </h5>
    <h2>
     总结
    </h2>
    <ul>
     <li>
      <p>
       <strong>
        图像压缩
       </strong>
       ：通过SVD分解图像矩阵，保留最大的几个奇异值及其对应的奇异向量，重构图像以实现压缩。
      </p>
      <ul>
       <li>
        <p>
         将图像矩阵分解为
         <img alt="" height="24" src="https://i-blog.csdnimg.cn/direct/dda83b18277f4f02b468685eb741f732.png" width="68"/>
         。
        </p>
       </li>
       <li>
        <p>
         保留前 k 个奇异值及其对应的奇异向量。
        </p>
       </li>
       <li>
        <p>
         通过
         <img alt="" height="26" src="https://i-blog.csdnimg.cn/direct/10aed694b6bc4c0d8e51eadcac67388f.png" width="88"/>
         近似重构图像。
        </p>
       </li>
       <li>
        <p>
         评估压缩效果。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        特征提取
       </strong>
       ：通过PCA计算数据的协方差矩阵的特征值和特征向量，选择最重要的特征向量作为新的特征空间，实现降维。
      </p>
      <ul>
       <li>
        <p>
         标准化数据。
        </p>
       </li>
       <li>
        <p>
         计算协方差矩阵并求解特征值和特征向量。
        </p>
       </li>
       <li>
        <p>
         选择前 k 个主成分。
        </p>
       </li>
       <li>
        <p>
         将数据投影到主成分空间。
        </p>
       </li>
       <li>
        <p>
         评估特征提取效果。
        </p>
       </li>
      </ul>
     </li>
    </ul>
    <p>
     这两种方法都利用了特征值和特征向量的性质，分别在图像压缩和特征提取中发挥了重要作用。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6c7a6d31323237383832382f:61727469636c652f64657461696c732f313436323336303937" class_="artid" style="display:none">
 </p>
</div>


