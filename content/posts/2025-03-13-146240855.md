---
layout: post
title: "深度学习中学习率调整策略"
date: 2025-03-13 20:05:51 +0800
description: "不同学习率策略的一些记录和整理"
keywords: "深度学习中学习率调整策略"
categories: ['深度学习']
tags: ['深度学习']
artid: "146240855"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146240855
    alt: "深度学习中学习率调整策略"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146240855
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146240855
cover: https://bing.ee123.net/img/rand?artid=146240855
image: https://bing.ee123.net/img/rand?artid=146240855
img: https://bing.ee123.net/img/rand?artid=146240855
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     深度学习中学习率调整策略
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <blockquote>
     <p>
      学习率衰减策略是深度学习优化过程中的一个关键因素，它决定了训练过程中学习率的调整方式，从而影响模型收敛的速度和效果。不同的衰减策略在不同的任务和模型上可能有不同的表现，下面从我用到过的几个衰减策略进行记录，后续慢慢跟进。
     </p>
    </blockquote>
    <h4>
     <a id="_2">
     </a>
     <strong>
      为什么需要学习率衰减？
     </strong>
    </h4>
    <p>
     在训练深度学习模型时，学习率的选择至关重要。通常，我们希望模型能够在训练的早期阶段较快地收敛（即较大的学习率），而在训练的后期阶段逐渐减小学习率，以便模型能够更精细地调整参数，避免错过局部最优点。
    </p>
    <h4>
     <a id="Step_Decay_6">
     </a>
     阶梯衰减Step Decay
    </h4>
    <p>
     这是最简单常用的学习率调整方法，每过step_size轮，将此前的学习率乘以gamma。
    </p>
    <pre><code class="prism language-dart"><span class="token class-name"><span class="token namespace">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span></span>StepLR</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_CosineAnnealingLR_13">
     </a>
     <strong>
      余弦退火调整学习率 CosineAnnealingLR
     </strong>
    </h4>
    <p>
     在每个周期内，学习率从初始学习率（
     <code>
      lr0
     </code>
     ）开始，经过余弦函数的逐渐衰减，最后衰减到最小学习率（
     <code>
      eta_min
     </code>
     ）。
    </p>
    <pre><code class="prism language-dart"><span class="token class-name"><span class="token namespace">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span></span>CosineAnnealingLR</span><span class="token punctuation">(</span>
optimizer<span class="token punctuation">,</span> 
<span class="token class-name">T_max</span><span class="token punctuation">,</span>     
eta_min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
last_epoch<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     T_max：指定一个完整的周期（例如，多少个训练步长或 epoch）。在
     <code>
      T_max
     </code>
     的周期内，学习率会从初始学习率衰减到
     <code>
      eta_min
     </code>
     。
    </p>
    <p>
     eta_min=0：最终学习率
    </p>
    <p>
     last_epoch=-1：指定上次训练的最后一个epoch，-1就是从头开始训练
    </p>
    <h4>
     <a id="_CosineAnnealingWarmRestarts_31">
     </a>
     <strong>
      余弦退火重启学习率 CosineAnnealingWarmRestarts
     </strong>
    </h4>
    <p>
     <code>
      CosineAnnealingWarmRestarts
     </code>
     通过将学习率沿着一个余弦曲线逐渐衰减到最小值（
     <code>
      eta_min
     </code>
     ），然后在周期结束时重启并回升到初始学习率（
     <code>
      lr0
     </code>
     ），这种周期性的重启和余弦退火帮助模型在每个阶段进行不同范围的探索，避免陷入局部最优。
    </p>
    <pre><code class="prism language-dart">scheduler <span class="token operator">=</span> <span class="token class-name"><span class="token namespace">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span></span>CosineAnnealingWarmRestarts</span><span class="token punctuation">(</span>
optimizer<span class="token punctuation">,</span> 
T_0<span class="token punctuation">,</span> 
<span class="token class-name">T_mult</span><span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> 
eta_min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
last_epoch<span class="token operator">=</span><span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">,</span> 
verbose<span class="token operator">=</span><span class="token class-name">False</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     T_0：第一个周期的长度，后续的长度基于T0和T_mult调整
    </p>
    <p>
     T_mult：控制后续周期的长度，例如设置为2，那则第二个周期长度会是第一个周期的2倍，第三个周期的长度会是第二个周期的2倍，设置为1则每个周期一样长。
    </p>
    <p>
     eta_min：这是学习率的最小值。在每个周期中，学习率会从
     <code>
      eta_max
     </code>
     （初始学习率）衰减到
     <code>
      eta_min
     </code>
     ，然后在每个周期结束时重新回升到初始学习率。
     <br/>
     verbose：如果设置为
     <code>
      True
     </code>
     ，则每次学习率重启时，调度器会输出日志信息，显示学习率调整的情况。例如，输出每次重启时的学习率和当前的训练状态。
    </p>
    <h4>
     <a id="_Linear_Decay_51">
     </a>
     <strong>
      线性衰减 Linear Decay
     </strong>
    </h4>
    <p>
     在线性衰减中，学习率随着训练的进展而线性地减小。学习率的减少是均匀的，每个时间步减少固定的量，直到达到预定的最小值
    </p>
    <pre><code class="prism language-dart"><span class="token class-name"><span class="token namespace">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span></span>LinearLR</span><span class="token punctuation">(</span>
optimizer<span class="token punctuation">,</span>
start_factor<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
end_factor<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
total_iters<span class="token operator">=</span><span class="token number">80</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     LinearLR是线性学习率，给定起始factor和最终的factor，LinearLR会在中间阶段做线性插值，比如学习率为0.1，起始factor为1，最终的factor为0.1，那么第0次迭代，学习率将为0.1，最终轮学习率为0.01。下面设置的总轮数total_iters为80,所以超过80时，学习率恒为0.01。
    </p>
    <h4>
     <a id="_Exponential_Decay_64">
     </a>
     指数衰减 Exponential Decay
    </h4>
    <p>
     学习率初期迅速下降，适合早期跳出局部最优，但可能导致后期过早收敛，适合大规模训练任务或对收敛速度有要求的情况。
    </p>
    <pre><code class="prism language-dart"><span class="token class-name"><span class="token namespace">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span></span>ExponentialLR</span><span class="token punctuation">(</span>
optimizer<span class="token punctuation">,</span>
gamma<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     ExponentialLR是指数型下降的学习率调节器，每一轮会将学习率乘以gamma，所以这里千万注意gamma不要设置的太小，不然几轮之后学习率就会降到0。
    </p>
    <h4>
     <a id="_75">
     </a>
     可视化
    </h4>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/77bf66edbe06489299dac61b1d3a4857.png"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34303933383231372f:61727469636c652f64657461696c732f313436323430383535" class_="artid" style="display:none">
 </p>
</div>


