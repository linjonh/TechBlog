---
arturl_encode: "6874:7470733a2f2f626c6f672e6373646e2e6e65742f6d616f7069:672f61727469636c652f64657461696c732f36373636313634"
layout: post
title: "用FFMPEG-SDK进行视频转码压缩时解决音视频不同步问题的方法转-PTS-DTS"
date: 2023-12-20 17:19:45 +08:00
description: "本文探讨了使用FFMPEGSDK进行视频转码压缩时遇到的音视频不同步问题。重点介绍了AvPacket"
keywords: "ffmpeg(dts is greater than pts)"
categories: ['X', 'H', 'Ffmpeg']
tags: ['Video', 'Image', 'Flv', 'Filter', 'File', 'C']
artid: "6766164"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=6766164
    alt: "用FFMPEG-SDK进行视频转码压缩时解决音视频不同步问题的方法转-PTS-DTS"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=6766164
featuredImagePreview: https://bing.ee123.net/img/rand?artid=6766164
---

# 用FFMPEG SDK进行视频转码压缩时解决音视频不同步问题的方法(转) PTS DTS

用FFMPEG SDK进行视频转码压缩的时候，转码成功后去看视频的内容，发现音视频是不同步的。这个的确是一个恼火的事情。我在用FFMPEG SDK做h264格式的FLV文件编码Filter的时候就碰到了这个问题。
  
  
经过研究发现，FFMPEG SDK写入视频的时候有两个地方用来控制写入的时间戳，一个是AvPacket, 一个是AvFrame。 在调用avcodec\_encode\_video的时候需要传入AvFrame的对象指针，也就是传入一帧未压缩的视频进行压缩处理，AvFrame包含一个pts的参数，这个参数就是当前帧将来在还原播放的时候的时间戳。而AvPacket里面也有pts，还有dts。说起这个就必须要说明一下I,P,B三种视频压缩帧。I帧就是关键帧，不依赖于其他视频帧，P帧是向前预测的帧，只依赖于前面的视频帧，而B帧是双向预测视频帧，依赖于前后视频帧。由于B帧的存在，因为它是双向的，必须知道前面的视频帧和后面的视频帧的详细内容后，才能知道本B帧最终该呈现什么图像。而pts和dts两个参数就是用来控制视频帧的显示和解码的顺序。
  
  
pts就是帧显示的顺序。
  
  
dts就是帧被读取进行解码的顺序。
  
  
如果没有B帧存在，dts和pts是相同的。反之，则是不相同的。关于这个的详细介绍可以参考一下mpeg的原理。
  
  
再说说AvPacket中包含的pts和dts两个到底该设置什么值？
  
  
pts和dts需要设置的就是视频帧解码和显示的顺序。每增加一帧就加一，并不是播放视频的时间戳。
  
  
但是实践证明经过rmvb解码的视频有时候并不是固定帧率的，而是变帧率的，这样，如果每压缩一帧，pts和dts加一的方案为导致音视频不同步。
  
  
那怎么来解决音视频同步的问题呢？
  
  
请看如下代码段。
  
  
lTimeStamp 是通过directshow 获取的当前的视频帧的时间戳。
  
  
m\_llframe\_index为当前已经经过压缩处理的帧的数量。
  
  
首先av\_rescale计算得到当前压缩处理已经需要处理什么时间戳的视频帧，如果该时间戳尚未到达directshow当前提供的视频帧的时间戳，则将该帧丢弃掉。
  
  
否则进行压缩操作。并设置AVPacket的pts和dts。这里假设B帧不存在。
  
  
因为在将来播放的时候视频以我们设定的固定播放帧率进行播放，所以需要根据设定的播放帧率计算得到的视频帧时间戳和directshow提供的当前视频帧的时间戳进行比较，设定是否需要进行实施延缓播放的策略。如果需要延缓播放，则将pts增加步长2，否则以普通速度播放，则设置为1.dts与之相同。
  
\_\_int64 x = av\_rescale(m\_llframe\_index,AV\_TIME\_BASE\*(int64\_t)c->time\_base.num,c->time\_base.den);
  
  
if( x > lTimeStamp )
  
{
  
return TRUE;
  
}
  
m\_pVideoFrame2->pts = lTimeStamp;
  
m\_pVideoFrame2->pict\_type = 0;
  
  
int out\_size = avcodec\_encode\_video( c, m\_pvideo\_outbuf, video\_outbuf\_size, m\_pVideoFrame2 );
  
/\* if zero size, it means the image was buffered \*/
  
if (out\_size > 0)
  
{
  
AVPacket pkt;
  
av\_init\_packet(&pkt);
  
  
if( x > lTimeStamp )
  
{
  
pkt.pts = pkt.dts = m\_llframe\_index;
  
pkt.duration = 0;
  
}
  
else
  
{
  
pkt.duration = (lTimeStamp - x)\*c->time\_base.den/1000000 + 1;
  
pkt.pts = m\_llframe\_index;
  
pkt.dts = pkt.pts;
  
m\_llframe\_index += pkt.duration;
  
}
  
  
//pkt.pts = lTimeStamp \* (\_\_int64)frame\_rate.den / 1000;
  
if( c->coded\_frame && c->coded\_frame->key\_frame )
  
{
  
pkt.flags |= PKT\_FLAG\_KEY;
  
}
  
  
pkt.stream\_index= m\_pVideoStream->index;
  
pkt.data= m\_pvideo\_outbuf;
  
pkt.size= out\_size;
  
  
/\* write the compressed frame in the media file \*/
  
ret = av\_interleaved\_write\_frame( m\_pAvFormatContext, &pkt );
  
}
  
else
  
{
  
ret = 0;
  
}