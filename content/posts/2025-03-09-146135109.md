---
layout: post
title: "李沐动手学深度学习14.9.-用于预训练BERT的数据集wiki数据集问题以及存在的其他问题"
date: 2025-03-09 17:22:41 +0800
description: "原因是链接已经失效。解决方法：打开下面链接自行下载，需要魔法。下载完解压到特定位置。"
keywords: "李沐《动手学深度学习》——14.9. 用于预训练BERT的数据集——wiki数据集问题以及存在的其他问题"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Bert']
artid: "146135109"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146135109
    alt: "李沐动手学深度学习14.9.-用于预训练BERT的数据集wiki数据集问题以及存在的其他问题"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146135109
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146135109
cover: https://bing.ee123.net/img/rand?artid=146135109
image: https://bing.ee123.net/img/rand?artid=146135109
img: https://bing.ee123.net/img/rand?artid=146135109
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     李沐《动手学深度学习》——14.9. 用于预训练BERT的数据集——wiki数据集问题以及存在的其他问题
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     问题1：出现"file is not a zip file"
    </h2>
    <p>
     原因是链接已经失效。
     <br/>
     解决方法：打开下面链接自行下载，需要魔法。下载完解压到特定位置。
    </p>
    <p>
     下载链接：
     <a href="https://gitcode.com/open-source-toolkit/db1ec/?utm_source=tools_gitcode&amp;index=top&amp;type=card&amp;&amp;isLogin=1" title="项目首页 - Wikitext-2-v1数据包下载:Wikitext-2-v1 数据包下载本仓库提供了一份Wikitext-2-v1的标准数据包，方便无法通过亚马逊网址下载的用户获取 - GitCode">
      项目首页 - Wikitext-2-v1数据包下载:Wikitext-2-v1 数据包下载本仓库提供了一份Wikitext-2-v1的标准数据包，方便无法通过亚马逊网址下载的用户获取 - GitCode
     </a>
    </p>
    <p>
     <img alt="" height="316" src="https://i-blog.csdnimg.cn/direct/776254f7f4b94a529ca5932e7e437b16.png" width="1041"/>
    </p>
    <p>
     修改load_data_wiki函数中data_dir的路径，如下：
    </p>
    <pre><code class="hljs">#@save
def load_data_wiki(batch_size, max_len):
    """加载WikiText-2数据集"""
    num_workers = d2l.get_dataloader_workers()
    #data_dir = = d2l.download_extract('wikitext-2', 'wikitext-2')  
    data_dir = 'D:\data\wikitext-2-v1\wikitext-2'  # 使用正斜杠避免转义问题
    
    paragraphs = _read_wiki(data_dir)
    train_set = _WikiTextDataset(paragraphs, max_len)
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                        shuffle=True, num_workers=0)
    return train_iter, train_set.vocab</code></pre>
    <p>
    </p>
    <h2>
     问题2：'gbk' codec can't decode byte 0xae in position 96: illegal multibyte sequence
    </h2>
    <p>
     原因是读取文件的时候编码方式不一样。
    </p>
    <p>
     解决方法：修改
     <strong>
      def _read_wiki(data_dir)
     </strong>
     函数，open里添加encoding = "utf-8"编码方式。
    </p>
    <pre><code class="hljs">#@save
def _read_wiki(data_dir):
    file_name = os.path.join(data_dir, 'wiki.train.tokens')
    with open(file_name, 'r',encoding = "utf-8") as f:
        lines = f.readlines()
    # 大写字母转换为小写字母
    paragraphs = [line.strip().lower().split(' . ')
                  for line in lines if len(line.split(' . ')) &gt;= 2]
    random.shuffle(paragraphs)
    return paragraphs</code></pre>
    <p>
    </p>
    <h2>
     问题3 ：一直卡在load_data_wiki运行不下去
    </h2>
    <p>
     原因是上面函数load_data_wiki的多线程问题，在load_data_wiki函数里令num_workers=0（如下），即可解决。
    </p>
    <pre><code class="hljs">batch_size, max_len = 512, 64
train_iter, vocab = load_data_wiki(batch_size, max_len)

for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,
     mlm_Y, nsp_y) in train_iter:
    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,
          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,
          nsp_y.shape)
    break</code></pre>
    <pre><code class="hljs">#@save
def load_data_wiki(batch_size, max_len):
    """加载WikiText-2数据集"""
    num_workers = d2l.get_dataloader_workers()
    #data_dir = = d2l.download_extract('wikitext-2', 'wikitext-2-v1')
    data_dir = 'D:\data\wikitext-2-v1\wikitext-2'
    
    paragraphs = _read_wiki(data_dir)
    train_set = _WikiTextDataset(paragraphs, max_len)
    train_iter = torch.utils.data.DataLoader(train_set, batch_size,
                                        shuffle=True, num_workers=0)
    return train_iter, train_set.vocab</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35373032313234392f:61727469636c652f64657461696c732f313436313335313039" class_="artid" style="display:none">
 </p>
</div>


