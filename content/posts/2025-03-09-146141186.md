---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f33383733343437322f:61727469636c652f64657461696c732f313436313431313836"
layout: post
title: "llama.cpp编译"
date: 2025-03-09 23:32:34 +0800
description: "【代码】llama.cpp编译。"
keywords: "llama cpp编译"
categories: ['未分类']
tags: ['Llama']
artid: "146141186"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146141186
    alt: "llama.cpp编译"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146141186
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146141186
cover: https://bing.ee123.net/img/rand?artid=146141186
image: https://bing.ee123.net/img/rand?artid=146141186
img: https://bing.ee123.net/img/rand?artid=146141186
---

# llama.cpp编译

## llam.cpp编译

### 1. 下载&编译

```
git clone https://github.com/ggml-org/llama.cpp
cmake -S . -B build

```

### 2. 下载模型验证

```
# 下载地址
https://huggingface.co/filipealmeida/open-llama-7b-v2-open-instruct-GGUF/blob/main/ggml-model-Q4_0.gguf

# 验证
./llama-cli.exe -m .\models\7B\ggml-model-Q4_0.gguf -p "Tell me a joke." --n_predict 100

```