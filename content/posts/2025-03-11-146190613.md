---
layout: post
title: "DeepSeek-R1-论文阅读总结"
date: 2025-03-11 22:21:52 +0800
description: "通过构建冷启动数据（数千条长CoT数据）微调基础模型，结合多阶段训练流程（RL训练、拒绝采样生成SFT数据），并优化输出格式（如特殊标记分隔），显著提升可读性。相比仅用RL的Zero版本，改进后的R1保持了推理能力且输出更易读。-R1-Zero：纯RL训练，无监督数据，输出存在语言混杂、可读性差-R1：引入监督学习阶段冷启动阶段用高质量CoT数据微调拒绝采样生成600K过滤数据（移除混合语言/冗余内容）二阶段RL（推理任务用规则奖励，通用任务用人类偏好奖励）"
keywords: "DeepSeek-R1 论文阅读总结"
categories: ['未分类']
tags: ['论文阅读', '人工智能']
artid: "146190613"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146190613
    alt: "DeepSeek-R1-论文阅读总结"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146190613
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146190613
cover: https://bing.ee123.net/img/rand?artid=146190613
image: https://bing.ee123.net/img/rand?artid=146190613
img: https://bing.ee123.net/img/rand?artid=146190613
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     DeepSeek-R1 论文阅读总结
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     1. QA问答（我的笔记）
    </h2>
    <p>
     <strong>
      Q1: DeepSeek如何处理可读性问题？
     </strong>
    </p>
    <p>
     通过构建冷启动数据（数千条长CoT数据）微调基础模型，结合多阶段训练流程（RL训练、拒绝采样生成SFT数据），并优化输出格式（如特殊标记分隔），显著提升可读性。相比仅用RL的Zero版本，改进后的R1保持了推理能力且输出更易读。
    </p>
    <p>
    </p>
    <p>
     <strong>
      Q2: DeepSeek-R1-Zero与R1的核心区别？
     </strong>
    </p>
    <p>
     -R1-Zero：纯RL训练，无监督数据，输出存在语言混杂、可读性差
    </p>
    <p>
     -R1：引入监督学习阶段
    </p>
    <p>
     冷启动阶段用高质量CoT数据微调
    </p>
    <p>
     拒绝采样生成600K过滤数据（移除混合语言/冗余内容）
    </p>
    <p>
     二阶段RL（推理任务用规则奖励，通用任务用人类偏好奖励）
    </p>
    <p>
    </p>
    <p>
     <strong>
      Q3: 如何验证推理能力蒸馏效果？
     </strong>
    </p>
    <p>
     在标准评测网站（如LiveCodeBench/Codeforces）测试，经蒸馏的小模型性能超越直接用RL训练的同规模模型。
    </p>
    <p>
    </p>
    <p>
     <strong>
      Q4: 成本节约方法？
     </strong>
    </p>
    <p>
     自进化RL减少监督数据需求
    </p>
    <p>
     GRPO算法优化RL训练效率
    </p>
    <p>
     复用V3训练集生成思维链
    </p>
    <p>
    </p>
    <h2>
     2. 论文核心贡献（做了什么）
    </h2>
    <p>
     方法论创新：提出四阶段训练框架（冷启动→推理RL→数据生成→通用能力RL）
    </p>
    <p>
     性能突破：在数学（MATH-500 97.3%）知识任务（MMLU 90.8%）达到SOTA
    </p>
    <p>
     工程实践：解决纯RL训练的可读性缺陷，构建首个支持人类友好CoT的RL优化模型
    </p>
    <p>
     技术验证：证明RL可通过自我进化提升推理能力，且该能力可蒸馏至小模型
    </p>
    <h2>
     3. 关键技术路径
    </h2>
    <h3>
     3.1 混合奖励机制
    </h3>
    <table>
     <tbody>
      <tr>
       <td>
        <p>
         任务类型
        </p>
       </td>
       <td>
        <p>
         奖励构成
        </p>
       </td>
       <td>
        <p>
         目标特性
        </p>
       </td>
      </tr>
      <tr>
       <td>
        <p>
         推理任务
        </p>
       </td>
       <td>
        <p>
         准确性(70%)+过程合规性(30%)
        </p>
       </td>
       <td>
        <p>
         严谨性
        </p>
       </td>
      </tr>
      <tr>
       <td>
        <p>
         通用任务
        </p>
       </td>
       <td>
        <p>
         有用性(50%)+无害性(30%)+可读性(20%)
        </p>
       </td>
       <td>
        <p>
         安全性
        </p>
       </td>
      </tr>
     </tbody>
    </table>
    <h3>
     3.2 数据生产管线
    </h3>
    <p>
     ​
     <img alt="" height="761" src="https://i-blog.csdnimg.cn/direct/d8ea3f2a3cb747aab373457dcd58b8d7.png" width="626"/>
    </p>
    <h2>
     4. 当前局限性
    </h2>
    <h3>
     4.1 技术瓶颈
    </h3>
    <p>
     MCTS应用失败：语言生成空间离散性导致搜索复杂度爆炸（相比围棋增长10^3倍）
    </p>
    <p>
     过程奖励困境：
    </p>
    <p>
     原子步骤定义模糊（如数学证明中间态）
    </p>
    <p>
     需人工标注百万级步骤数据（成本$380K+）
    </p>
    <p>
     奖励黑客问题频发（模型学会伪造合规步骤）
    </p>
    <h3>
     4.2 实践缺陷
    </h3>
    <p>
     <img alt="" height="674" src="https://i-blog.csdnimg.cn/direct/6a03a1277b1b44088eff8f9e2022378a.png" width="1269"/>
    </p>
    <h2>
     5. 未来方向
    </h2>
    <h3>
     短期重点
    </h3>
    <p>
     蒸馏优化：探索RL+蒸馏联合框架（当前仅用SFT）
    </p>
    <p>
     架构改进：
    </p>
    <p>
     动态上下文窗口（当前固定4K）
    </p>
    <p>
     混合专家系统（MoE）提升工程能力
    </p>
    <h3>
     长期愿景
    </h3>
    <p>
     自进化系统：构建完全闭环的RL训练生态（人工标注量&lt;1%）
    </p>
    <p>
     多模态推理：扩展至视觉-语言联合推理场景
    </p>
    <p>
     安全增强：研发可解释的奖励模型（当前黑盒率&gt;92%）
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f33333738313934362f:61727469636c652f64657461696c732f313436313930363133" class_="artid" style="display:none">
 </p>
</div>


