---
layout: post
title: "Pytorch系列教程可视化Pytorch模型训练过程"
date: 2025-03-11 21:41:18 +0800
description: "深度学习和理解训练过程中的学习和进步机制对于优化性能、诊断欠拟合或过拟合等问题至关重要。将训练过程可视化的过程为学习的动态提供了有价值的见解，使我们能够做出合理的决策。训练进度必须可视化的两种方法是：使用Matplotlib和Tensor Board。在本文中，我们将学习如何在Pytorch中可视化模型训练进度。"
keywords: "Pytorch系列教程：可视化Pytorch模型训练过程"
categories: ['人工智能', 'Python']
tags: ['人工智能', 'Pytorch', 'Python']
artid: "146189809"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146189809
    alt: "Pytorch系列教程可视化Pytorch模型训练过程"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146189809
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146189809
cover: https://bing.ee123.net/img/rand?artid=146189809
image: https://bing.ee123.net/img/rand?artid=146189809
img: https://bing.ee123.net/img/rand?artid=146189809
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Pytorch系列教程：可视化Pytorch模型训练过程
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <blockquote>
     <p>
      深度学习和理解训练过程中的学习和进步机制对于优化性能、诊断欠拟合或过拟合等问题至关重要。将训练过程可视化的过程为学习的动态提供了有价值的见解，使我们能够做出合理的决策。训练进度必须可视化的两种方法是：使用Matplotlib和Tensor Board。在本文中，我们将学习如何在Pytorch中可视化模型训练进度。
     </p>
    </blockquote>
    <h3>
     <a id="MatplotlibPyTorch_4">
     </a>
     使用Matplotlib在PyTorch中可视化训练进度
    </h3>
    <p>
     Matplotlib是Python中广泛使用的绘图库，它为在Python中创建静态，动画和交互式可视化提供了灵活而强大的工具。它特别适合于创建出版质量的图表。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/dc17dbfcf0764904b5c850c18c99e017.png"/>
    </p>
    <p>
     **步骤1：**导入必要的库并生成样本数据集
    </p>
    <p>
     在这一步中，我们将导入必要的库并生成样本数据集。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token comment"># Sample data</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Sample features</span>
y <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> X <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Sample labels with noise</span>
</code></pre>
    <p>
     **步骤2：**定义模型
    </p>
    <ol>
     <li>
      PyTorch中的
      <code>
       LinearRegression
      </code>
      类定义了一个简单的线性回归模型。它继承自nn。模块的类，使其成为一个神经网络模型。
     </li>
     <li>
      构造函数(
      <code>
       __init__
      </code>
      方法)初始化模型的结构，创建具有一个输入特征和一个输出特征的单一线性层(‘nn.Linear’)。
     </li>
     <li>
      这个线性层被存储为名为
      <code>
       self.linear
      </code>
      的属性。“forward”方法定义了如何通过这个线性层处理输入数据“x”以产生模型的输出。
     </li>
     <li>
      具体来说，输入
      <code>
       x
      </code>
      是通过
      <code>
       self.linear
      </code>
      ，并返回结果输出。该方法封装了神经网络的前向传递计算，决定了模型如何将输入转换为输出。
     </li>
    </ol>
    <pre><code class="prism language-python"><span class="token comment"># Define a simple linear regression model</span>
<span class="token keyword">class</span> <span class="token class-name">LinearRegression</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearRegression<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># One input feature, one output</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

model <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     **步骤3：**定义损失函数、优化器和训练循环
    </p>
    <p>
     在下面的代码中，我们将均方误差定义为损失函数，将随机梯度下降(SGD)优化器定义为优化器，该优化器通过使用学习率为0.01的计算梯度来修改模型的参数。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Define loss function and optimizer</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     这段代码运行了一个神经网络模型在多个时代的训练循环，使用梯度下降计算和优化损失。损失值被存储以进行绘图，进度每10次打印一次。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Training loop</span>
num_epochs <span class="token operator">=</span> <span class="token number">100</span>
losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Forward pass</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

    <span class="token comment"># Backward pass and optimization</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Print progress</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>num_epochs<span class="token punctuation">}</span></span><span class="token string">], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

    <span class="token comment"># Store loss for plotting</span>
    losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     **步骤4：**使用Matplotlib在PyTorch中可视化训练进度
    </p>
    <p>
     使用下面的代码，我们可以使用matplotlib可视化训练损失曲线。
    </p>
    <ul>
     <li>
      plot(损失)线根据epoch号绘制存储在损失列表中的损失值。
     </li>
     <li>
      x轴表示历元数，y轴表示相应的损失值。
     </li>
     <li>
      plt.xlabel(‘Epoch’), plt.ylabel(‘Loss‘)和plt.xlabel(’Epoch’).title()‘Training Loss’)行设置情节的标签和标题。
     </li>
     <li>
      最后，plot .show()显示该图，允许您可视化地分析损失如何在训练期间减少(或收敛)。
     </li>
    </ul>
    <pre><code class="prism language-python"><span class="token comment"># Plot the loss curve</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>losses<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     通常，您会期望在损失曲线中看到下降的趋势，这表明模型正在随着时间的推移而学习和改进。
    </p>
    <p>
     完整的代码:
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># Sample data</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Sample features</span>
y <span class="token operator">=</span> <span class="token number">3</span> <span class="token operator">*</span> X <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">+</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># Sample labels with noise</span>

<span class="token comment"># Define a simple linear regression model</span>
<span class="token keyword">class</span> <span class="token class-name">LinearRegression</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LinearRegression<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># One input feature, one output</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

model <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># Define loss function and optimizer</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token comment"># Training loop</span>
num_epochs <span class="token operator">=</span> <span class="token number">100</span>
losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Forward pass</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

    <span class="token comment"># Backward pass and optimization</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token comment"># Print progress</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>num_epochs<span class="token punctuation">}</span></span><span class="token string">], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>

    <span class="token comment"># Store loss for plotting</span>
    losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    
<span class="token comment"># Plot the loss curve</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>losses<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Training Loss'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
  
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/dfde2df97c1749f7949f46806486016f.png"/>
    </p>
    <p>
     输出图显示了训练损失如何随时间变化，并根据迭代次数绘制。这种可视化使人们能够看到模型在训练时是如何减少损失的。此外，Matplotlib图还有其他东西，如轴标签、标题，可能还有标记或线条，表示特定事件，如最小实现损失或损失急剧下降。
    </p>
    <h3>
     <a id="TensorBoard_159">
     </a>
     使用TensorBoard可视化训练进度
    </h3>
    <p>
     为了在深度学习模型中可视化训练过程，我们可以使用torch.utils.tensorboard模块中的SummaryWriter类，该模块与TensorFlow开发的可视化工具TensorBoard无缝集成。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/35761f7771ba44438e4817f25338d3ec.png"/>
    </p>
    <ul>
     <li>
      集成：PyTorch在torch.utils.tensorboard模块中提供了一个SummaryWriter类，它与TensorBoard无缝集成以实现可视化。
     </li>
     <li>
      日志记录：在训练循环中，您可以使用SummaryWriter记录各种指标，如损失，准确性等，以实现可视化。
     </li>
     <li>
      可视化：TensorBoard提供了记录指标的交互式和实时可视化，允许您动态监控训练进度。
     </li>
     <li>
      监控：TensorBoard使您能够监控训练的多个方面，例如学习曲线，模型图和权重直方图，为优化您的模型提供见解。
     </li>
    </ul>
    <p>
     使用以下命令安装TensorBoard库：
    </p>
    <pre><code>pip install tensorboard
</code></pre>
    <p>
     <strong>
      步骤1：导入库
     </strong>
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter
</code></pre>
    <p>
     <strong>
      步骤2：定义简单的神经网络
     </strong>
    </p>
    <p>
     让我们定义SimpleNN一个简单神经网络的类声明，它包含两个完全连接的层，以及定义网络前向传递的forward函数。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Define a simple neural network</span>
<span class="token keyword">class</span> <span class="token class-name">SimpleNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre>
    <p>
     <strong>
      步骤3：加载MNIST数据集
     </strong>
    </p>
    <p>
     让我们加载用于训练的MINST数据，将其分成批次并使用一些预处理技术进行转换。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Load a smaller subset of MNIST dataset</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
small_train_dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Subset<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Subset of first 1000 samples</span>
train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>small_train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      步骤4：初始化模型、损失函数和优化器
     </strong>
    </p>
    <p>
     现在，初始化模型。与此同时，我们将使用交叉熵损失函数和adam优化器来更新模型参数。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Initialize model, loss function, and optimizer</span>
model <span class="token operator">=</span> SimpleNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      步骤5：初始化用于日志记录的SummaryWriter
     </strong>
    </p>
    <p>
     SummaryWriter是导入模块的对象，用于编写要在TensorBoard中可视化的日志。
    </p>
    <pre><code class="prism language-python"><span class="token comment"># Initialize SummaryWriter for logging</span>
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token string">'logs_small'</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      第六步：循环训练
     </strong>
    </p>
    <ul>
     <li>
      训练循环：通过时代和批次，执行向前传递，计算损失，向后传递和更新模型参数。
     </li>
     <li>
      日志损失和准确性：记录划时代的训练损失和准确性。
     </li>
    </ul>
    <pre><code class="prism language-python"><span class="token comment"># Training loop</span>
epochs <span class="token operator">=</span> <span class="token number">5</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>
    correct <span class="token operator">=</span> <span class="token number">0</span>
    total <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Calculate accuracy</span>
        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Log loss</span>
        writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/train'</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> epoch <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span> <span class="token operator">+</span> i<span class="token punctuation">)</span>

    <span class="token comment"># Log accuracy</span>
    accuracy <span class="token operator">=</span> <span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Accuracy/train'</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epochs<span class="token punctuation">}</span></span><span class="token string">], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>running_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">, Accuracy: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>accuracy<span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     完整代码：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
<span class="token keyword">import</span> torchvision
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>tensorboard <span class="token keyword">import</span> SummaryWriter

<span class="token comment"># Define a simple neural network</span>
<span class="token keyword">class</span> <span class="token class-name">SimpleNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
      
<span class="token comment"># Load a smaller subset of MNIST dataset</span>
transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>
    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>

train_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>MNIST<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'./data'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
small_train_dataset <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Subset<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># Subset of first 1000 samples</span>
train_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>small_train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># Initialize model, loss function, and optimizer</span>
model <span class="token operator">=</span> SimpleNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>

<span class="token comment"># Initialize SummaryWriter for logging</span>
writer <span class="token operator">=</span> SummaryWriter<span class="token punctuation">(</span><span class="token string">'logs_small'</span><span class="token punctuation">)</span>


<span class="token comment"># Training loop</span>
epochs <span class="token operator">=</span> <span class="token number">5</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    running_loss <span class="token operator">=</span> <span class="token number">0.0</span>
    correct <span class="token operator">=</span> <span class="token number">0</span>
    total <span class="token operator">=</span> <span class="token number">0</span>

    <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        running_loss <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Calculate accuracy</span>
        _<span class="token punctuation">,</span> predicted <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        total <span class="token operator">+=</span> labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        correct <span class="token operator">+=</span> <span class="token punctuation">(</span>predicted <span class="token operator">==</span> labels<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># Log loss</span>
        writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Loss/train'</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> epoch <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span> <span class="token operator">+</span> i<span class="token punctuation">)</span>

    <span class="token comment"># Log accuracy</span>
    accuracy <span class="token operator">=</span> <span class="token number">100</span> <span class="token operator">*</span> correct <span class="token operator">/</span> total
    writer<span class="token punctuation">.</span>add_scalar<span class="token punctuation">(</span><span class="token string">'Accuracy/train'</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Epoch [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epochs<span class="token punctuation">}</span></span><span class="token string">], Loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>running_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>train_loader<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">, Accuracy: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>accuracy<span class="token punctuation">}</span></span><span class="token string">%'</span></span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Finished Training'</span><span class="token punctuation">)</span>
writer<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre>
    <p>
     运行示例，输出如下：
    </p>
    <pre><code>Epoch [1/5], Loss: 1.8145772516727448, Accuracy: 47.1%
Epoch [2/5], Loss: 1.0121613591909409, Accuracy: 78.8%
Epoch [3/5], Loss: 0.6829517856240273, Accuracy: 84.1%
Epoch [4/5], Loss: 0.5442189555615187, Accuracy: 85.4%
Epoch [5/5], Loss: 0.46599634923040867, Accuracy: 87.0%
Finished Training
</code></pre>
    <p>
     TensorBoard提供了一个基于web的仪表板，其中包含代表各种培训方面的选项卡和可视化。标量度量将损失或准确度等值可视化，为训练动态提供了不同的视角。此外，TensorBoard可以显示直方图、嵌入和基于日志信息的专门可视化。
    </p>
    <p>
     <strong>
      在PyTorch中可视化训练进度
     </strong>
    </p>
    <p>
     为了运行TensorBoard，你应该打开终端，然后运行tensorboard use命令：
    </p>
    <pre><code>tensorboard --logdir=./logs_small
</code></pre>
    <p>
     注意，这里logdir指定上节示例的路径，采用相对路径表示。访问TensorBoard需要：打开浏览器，输入TensorBoard提供的网址（通常为http://localhost:6006/）。
    </p>
    <table>
     <thead>
      <tr>
       <th>
       </th>
       <th>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <img alt="a" src="https://i-blog.csdnimg.cn/direct/2aef932485df4ccc82d813bb2399a3b9.png"/>
       </td>
       <td>
       </td>
      </tr>
      <tr>
       <td>
        <img alt="b" src="https://i-blog.csdnimg.cn/direct/3a1a251e141c466e8b0dce2830c4210c.png"/>
       </td>
       <td>
       </td>
      </tr>
      <tr>
       <td>
       </td>
       <td>
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     TensorBoard提供了一个基于web的仪表板，其中包含代表各种培训方面的选项卡和可视化。标量度量将损失或准确度等值可视化，为训练动态提供了不同的视角。此外，TensorBoard可以显示直方图、嵌入和基于日志信息的专门可视化。
    </p>
    <p>
     在这篇博客中，我们介绍了如何使用matplotlib和tensorboard来可视化深度学习框架的训练过程。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f6e65776561737473756e2f:61727469636c652f64657461696c732f313436313839383039" class_="artid" style="display:none">
 </p>
</div>


