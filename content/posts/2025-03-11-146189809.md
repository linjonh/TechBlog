---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f6e65776561737473756e2f:61727469636c652f64657461696c732f313436313839383039"
layout: post
title: "Pytorch系列教程可视化Pytorch模型训练过程"
date: 2025-03-11 21:41:18 +0800
description: "深度学习和理解训练过程中的学习和进步机制对于优化性能、诊断欠拟合或过拟合等问题至关重要。将训练过程可视化的过程为学习的动态提供了有价值的见解，使我们能够做出合理的决策。训练进度必须可视化的两种方法是：使用Matplotlib和Tensor Board。在本文中，我们将学习如何在Pytorch中可视化模型训练进度。"
keywords: "如何对litehrnet的训练结果进行可视化"
categories: ['人工智能', 'Python']
tags: ['人工智能', 'Pytorch', 'Python']
artid: "146189809"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146189809
    alt: "Pytorch系列教程可视化Pytorch模型训练过程"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146189809
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146189809
cover: https://bing.ee123.net/img/rand?artid=146189809
image: https://bing.ee123.net/img/rand?artid=146189809
img: https://bing.ee123.net/img/rand?artid=146189809
---

# Pytorch系列教程：可视化Pytorch模型训练过程

> 深度学习和理解训练过程中的学习和进步机制对于优化性能、诊断欠拟合或过拟合等问题至关重要。将训练过程可视化的过程为学习的动态提供了有价值的见解，使我们能够做出合理的决策。训练进度必须可视化的两种方法是：使用Matplotlib和Tensor Board。在本文中，我们将学习如何在Pytorch中可视化模型训练进度。

### 使用Matplotlib在PyTorch中可视化训练进度

Matplotlib是Python中广泛使用的绘图库，它为在Python中创建静态，动画和交互式可视化提供了灵活而强大的工具。它特别适合于创建出版质量的图表。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dc17dbfcf0764904b5c850c18c99e017.png)

\*\*步骤1：\*\*导入必要的库并生成样本数据集

在这一步中，我们将导入必要的库并生成样本数据集。

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
# Sample data
X = torch.randn(100, 1)  # Sample features
y = 3 * X + 2 + torch.randn(100, 1)  # Sample labels with noise

```

\*\*步骤2：\*\*定义模型

1. PyTorch中的
   `LinearRegression`
   类定义了一个简单的线性回归模型。它继承自nn。模块的类，使其成为一个神经网络模型。
2. 构造函数(
   `__init__`
   方法)初始化模型的结构，创建具有一个输入特征和一个输出特征的单一线性层(‘nn.Linear’)。
3. 这个线性层被存储为名为
   `self.linear`
   的属性。“forward”方法定义了如何通过这个线性层处理输入数据“x”以产生模型的输出。
4. 具体来说，输入
   `x`
   是通过
   `self.linear`
   ，并返回结果输出。该方法封装了神经网络的前向传递计算，决定了模型如何将输入转换为输出。

```python
# Define a simple linear regression model
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # One input feature, one output

    def forward(self, x):
        return self.linear(x)

model = LinearRegression()

```

\*\*步骤3：\*\*定义损失函数、优化器和训练循环

在下面的代码中，我们将均方误差定义为损失函数，将随机梯度下降(SGD)优化器定义为优化器，该优化器通过使用学习率为0.01的计算梯度来修改模型的参数。

```python
# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

```

这段代码运行了一个神经网络模型在多个时代的训练循环，使用梯度下降计算和优化损失。损失值被存储以进行绘图，进度每10次打印一次。

```python
# Training loop
num_epochs = 100
losses = []
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)

    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print progress
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

    # Store loss for plotting
    losses.append(loss.item())

```

\*\*步骤4：\*\*使用Matplotlib在PyTorch中可视化训练进度

使用下面的代码，我们可以使用matplotlib可视化训练损失曲线。

* plot(损失)线根据epoch号绘制存储在损失列表中的损失值。
* x轴表示历元数，y轴表示相应的损失值。
* plt.xlabel(‘Epoch’), plt.ylabel(‘Loss‘)和plt.xlabel(’Epoch’).title()‘Training Loss’)行设置情节的标签和标题。
* 最后，plot .show()显示该图，允许您可视化地分析损失如何在训练期间减少(或收敛)。

```python
# Plot the loss curve
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()

```

通常，您会期望在损失曲线中看到下降的趋势，这表明模型正在随着时间的推移而学习和改进。

完整的代码:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Sample data
X = torch.randn(100, 1)  # Sample features
y = 3 * X + 2 + torch.randn(100, 1)  # Sample labels with noise

# Define a simple linear regression model
class LinearRegression(nn.Module):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(1, 1)  # One input feature, one output

    def forward(self, x):
        return self.linear(x)

model = LinearRegression()

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Training loop
num_epochs = 100
losses = []
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)

    # Backward pass and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    # Print progress
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

    # Store loss for plotting
    losses.append(loss.item())
    
# Plot the loss curve
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.show()
  

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dfde2df97c1749f7949f46806486016f.png)

输出图显示了训练损失如何随时间变化，并根据迭代次数绘制。这种可视化使人们能够看到模型在训练时是如何减少损失的。此外，Matplotlib图还有其他东西，如轴标签、标题，可能还有标记或线条，表示特定事件，如最小实现损失或损失急剧下降。

### 使用TensorBoard可视化训练进度

为了在深度学习模型中可视化训练过程，我们可以使用torch.utils.tensorboard模块中的SummaryWriter类，该模块与TensorFlow开发的可视化工具TensorBoard无缝集成。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/35761f7771ba44438e4817f25338d3ec.png)

* 集成：PyTorch在torch.utils.tensorboard模块中提供了一个SummaryWriter类，它与TensorBoard无缝集成以实现可视化。
* 日志记录：在训练循环中，您可以使用SummaryWriter记录各种指标，如损失，准确性等，以实现可视化。
* 可视化：TensorBoard提供了记录指标的交互式和实时可视化，允许您动态监控训练进度。
* 监控：TensorBoard使您能够监控训练的多个方面，例如学习曲线，模型图和权重直方图，为优化您的模型提供见解。

使用以下命令安装TensorBoard库：

```
pip install tensorboard

```

**步骤1：导入库**

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

```

**步骤2：定义简单的神经网络**

让我们定义SimpleNN一个简单神经网络的类声明，它包含两个完全连接的层，以及定义网络前向传递的forward函数。

```python
# Define a simple neural network
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.flatten(x, 1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

```

**步骤3：加载MNIST数据集**

让我们加载用于训练的MINST数据，将其分成批次并使用一些预处理技术进行转换。

```python
# Load a smaller subset of MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
small_train_dataset = torch.utils.data.Subset(train_dataset, range(1000))  # Subset of first 1000 samples
train_loader = DataLoader(small_train_dataset, batch_size=64, shuffle=True)

```

**步骤4：初始化模型、损失函数和优化器**

现在，初始化模型。与此同时，我们将使用交叉熵损失函数和adam优化器来更新模型参数。

```python
# Initialize model, loss function, and optimizer
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

```

**步骤5：初始化用于日志记录的SummaryWriter**

SummaryWriter是导入模块的对象，用于编写要在TensorBoard中可视化的日志。

```python
# Initialize SummaryWriter for logging
writer = SummaryWriter('logs_small')

```

**第六步：循环训练**

* 训练循环：通过时代和批次，执行向前传递，计算损失，向后传递和更新模型参数。
* 日志损失和准确性：记录划时代的训练损失和准确性。

```python
# Training loop
epochs = 5
for epoch in range(epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    for i, (inputs, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Log loss
        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)

    # Log accuracy
    accuracy = 100 * correct / total
    writer.add_scalar('Accuracy/train', accuracy, epoch)

    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader)}, Accuracy: {accuracy}%')

print('Finished Training')
writer.close()

```

完整代码：

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter

# Define a simple neural network
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.flatten(x, 1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
      
# Load a smaller subset of MNIST dataset
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)
small_train_dataset = torch.utils.data.Subset(train_dataset, range(1000))  # Subset of first 1000 samples
train_loader = DataLoader(small_train_dataset, batch_size=64, shuffle=True)

# Initialize model, loss function, and optimizer
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Initialize SummaryWriter for logging
writer = SummaryWriter('logs_small')


# Training loop
epochs = 5
for epoch in range(epochs):
    running_loss = 0.0
    correct = 0
    total = 0

    for i, (inputs, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # Log loss
        writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)

    # Log accuracy
    accuracy = 100 * correct / total
    writer.add_scalar('Accuracy/train', accuracy, epoch)

    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader)}, Accuracy: {accuracy}%')

print('Finished Training')
writer.close()


```

运行示例，输出如下：

```
Epoch [1/5], Loss: 1.8145772516727448, Accuracy: 47.1%
Epoch [2/5], Loss: 1.0121613591909409, Accuracy: 78.8%
Epoch [3/5], Loss: 0.6829517856240273, Accuracy: 84.1%
Epoch [4/5], Loss: 0.5442189555615187, Accuracy: 85.4%
Epoch [5/5], Loss: 0.46599634923040867, Accuracy: 87.0%
Finished Training

```

TensorBoard提供了一个基于web的仪表板，其中包含代表各种培训方面的选项卡和可视化。标量度量将损失或准确度等值可视化，为训练动态提供了不同的视角。此外，TensorBoard可以显示直方图、嵌入和基于日志信息的专门可视化。

**在PyTorch中可视化训练进度**

为了运行TensorBoard，你应该打开终端，然后运行tensorboard use命令：

```
tensorboard --logdir=./logs_small

```

注意，这里logdir指定上节示例的路径，采用相对路径表示。访问TensorBoard需要：打开浏览器，输入TensorBoard提供的网址（通常为http://localhost:6006/）。

|  |  |
| --- | --- |
| a |  |
| b |  |
|  |  |

TensorBoard提供了一个基于web的仪表板，其中包含代表各种培训方面的选项卡和可视化。标量度量将损失或准确度等值可视化，为训练动态提供了不同的视角。此外，TensorBoard可以显示直方图、嵌入和基于日志信息的专门可视化。

在这篇博客中，我们介绍了如何使用matplotlib和tensorboard来可视化深度学习框架的训练过程。