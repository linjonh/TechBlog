---
layout: post
title: "前馈神经网络-参数学习梯度下降法-多分类任务"
date: 2025-03-07 18:55:44 +0800
description: "之前的博文中，对于前馈神经网络，我们学习过基于二分类任务的参数学习，本文我们来学习两个多分类任务的参数学习的例子，来进一步加深对反向传播算法的理解。"
keywords: "采用softmax激活函数构建前馈神经网络"
categories: ['Ai']
tags: ['神经网络', '深度学习', '机器学习', '学习', '分类', '人工智能']
artid: "146102309"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146102309
    alt: "前馈神经网络-参数学习梯度下降法-多分类任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146102309
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146102309
cover: https://bing.ee123.net/img/rand?artid=146102309
image: https://bing.ee123.net/img/rand?artid=146102309
img: https://bing.ee123.net/img/rand?artid=146102309
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     前馈神经网络 - 参数学习（梯度下降法 - 多分类任务）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     之前的博文中，对于前馈神经网络，我们学习过基于二分类任务的参数学习，本文我们来学习两个多分类任务的参数学习的例子，来进一步加深对反向传播算法的理解。
    </p>
    <h2>
     <strong>
      例子1：前馈神经网络在多分类任务中的参数学习（梯度下降法）
     </strong>
    </h2>
    <p>
     以下通过一个
     <strong>
      3分类任务
     </strong>
     的具体例子，详细说明前馈神经网络如何使用梯度下降法进行参数学习。假设网络结构如下：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        输入层
       </strong>
       ：2个神经元（输入特征 x1​,x2​）
      </p>
     </li>
     <li>
      <p>
       <strong>
        隐藏层
       </strong>
       ：3个神经元（使用 ReLU 激活函数）
      </p>
     </li>
     <li>
      <p>
       <strong>
        输出层
       </strong>
       ：3个神经元（使用 Softmax 激活函数）
      </p>
     </li>
    </ul>
    <h3>
     <strong>
      1. 网络参数初始化
     </strong>
    </h3>
    <p>
     假设输入样本为 x=[1,0]，真实标签为类别 2（标签编码为 one-hot 向量 y=[0,0,1]）。
    </p>
    <p>
     <strong>
      参数定义
     </strong>
     ：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        输入层到隐藏层
       </strong>
       ：
      </p>
      <img alt="" height="168" src="https://i-blog.csdnimg.cn/direct/d6d0ba86ef5d4cab96583cbcb6ddb3a0.png" width="1342"/>
     </li>
     <li>
      <p>
       <strong>
        隐藏层到输出层
       </strong>
       ：
      </p>
      <img alt="" height="172" src="https://i-blog.csdnimg.cn/direct/6c121735cbaa4c52ac60f6a11a5602ee.png" width="1380"/>
     </li>
    </ul>
    <h3>
     <strong>
      2. 前向传播
     </strong>
    </h3>
    <ol>
     <li>
      <p>
       <strong>
        隐藏层计算
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         线性变换：
        </p>
        <img alt="" height="170" src="https://i-blog.csdnimg.cn/direct/f22376f6690b4bbf875dfa6f1657eb7e.png" width="1432"/>
       </li>
       <li>
        <p>
         ReLU 激活：
        </p>
        <img alt="" height="186" src="https://i-blog.csdnimg.cn/direct/e06af27e4de24676abc2a727138f07d9.png" width="1408"/>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        输出层计算
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         线性变换：
        </p>
        <img alt="" height="164" src="https://i-blog.csdnimg.cn/direct/44ac24824b5541f980d82f0c04d47129.png" width="1464"/>
       </li>
       <li>
        <p>
         Softmax 激活（转换为概率）：
        </p>
        <img alt="" height="210" src="https://i-blog.csdnimg.cn/direct/e68af5e21d5c44bca4d1735acc87e5c0.png" width="1400"/>
       </li>
      </ul>
     </li>
    </ol>
    <h3>
     <strong>
      3. 损失计算（交叉熵损失）
     </strong>
    </h3>
    <p>
     真实标签为类别 2（y=[0,0,1]），预测概率为 y^≈[0.31,0.20,0.49]：
    </p>
    <p>
     <img alt="" height="138" src="https://i-blog.csdnimg.cn/direct/c8d0a1c94438491abb7e54a692dc2961.png" width="1550"/>
    </p>
    <h3>
     <strong>
      4. 反向传播计算梯度
     </strong>
    </h3>
    <h6>
     <strong>
      输出层梯度
     </strong>
    </h6>
    <ul>
     <li>
      <p>
       <strong>
        Softmax + 交叉熵的梯度简化
       </strong>
       ：
      </p>
      <img alt="" height="178" src="https://i-blog.csdnimg.cn/direct/42e05d9c62894e8f82deed40e3084edd.png" width="1584"/>
     </li>
     <li>
      <p>
       <strong>
        参数梯度
       </strong>
       ：
      </p>
      <img alt="" height="356" src="https://i-blog.csdnimg.cn/direct/ab9a0ad5aeb34b0d92db848cf2769d5d.png" width="1410"/>
     </li>
    </ul>
    <h6>
     <strong>
      隐藏层梯度
     </strong>
    </h6>
    <ul>
     <li>
      <p>
       <strong>
        误差信号传播
       </strong>
       ：
      </p>
      <img alt="" height="128" src="https://i-blog.csdnimg.cn/direct/3e176e343f4248f499565fef0b47a84c.png" width="1328"/>
      <ul>
       <li>
        <p>
         ReLU 导数：
        </p>
        <img alt="" height="170" src="https://i-blog.csdnimg.cn/direct/8f121ff471ab40b585f3b4e0c0b22c85.png" width="1446"/>
       </li>
       <li>
        <p>
         上游误差：
        </p>
        <img alt="" height="156" src="https://i-blog.csdnimg.cn/direct/b0f5dffee06244cd91b0fb05a046c281.png" width="822"/>
        <img alt="" height="162" src="https://i-blog.csdnimg.cn/direct/2c81443c4d8246afbf15474dd87b8a1b.png" width="1082"/>
       </li>
       <li>
        <p>
         逐元素相乘：
        </p>
        <img alt="" height="176" src="https://i-blog.csdnimg.cn/direct/514fc24355474f68b7b0f2af8a4de278.png" width="1416"/>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        参数梯度
       </strong>
       ：
      </p>
      <img alt="" height="340" src="https://i-blog.csdnimg.cn/direct/e6cf04b169144eb8b8f6851233e0be9d.png" width="1476"/>
     </li>
    </ul>
    <h3>
     <strong>
      5. 参数更新（学习率 η=0.1）
     </strong>
    </h3>
    <ul>
     <li>
      <p>
       <strong>
        输出层参数
       </strong>
       ：
      </p>
      <img alt="" height="160" src="https://i-blog.csdnimg.cn/direct/a5096bd1b2f04e0b95cc039c29a8c847.png" width="1156"/>
      <img alt="" height="170" src="https://i-blog.csdnimg.cn/direct/a56a86e5c7c9493d878422481794c903.png" width="436"/>
     </li>
     <li>
      <p>
       <strong>
        隐藏层参数
       </strong>
       ：
      </p>
      <img alt="" height="346" src="https://i-blog.csdnimg.cn/direct/aed39f8ff39d460ab48750876d62359e.png" width="1454"/>
     </li>
    </ul>
    <h3>
     <strong>
      6. 验证更新后的预测
     </strong>
    </h3>
    <p>
     更新参数后，重新进行前向传播：
    </p>
    <ul>
     <li>
      <p>
       隐藏层输出可能更接近真实类别 2，损失 L 应减小。例如，若新的预测概率为 [0.25,0.15,0.60]，则损失为 −log⁡(0.60)≈0.511&lt;0.713，表明参数学习有效。
      </p>
     </li>
    </ul>
    <h5>
     <strong>
      关键总结
     </strong>
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        Softmax + 交叉熵
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         多分类任务的标准组合，梯度计算简化为
         <img alt="" height="112" src="https://i-blog.csdnimg.cn/direct/bbcac144435c4570bed2c818369f0de5.png" width="194"/>
         。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        ReLU 导数特性
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         激活导数为 0 或 1，加速计算并缓解梯度消失问题。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        梯度下降步骤
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         通过链式法则逐层计算梯度，参数沿负梯度方向更新。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        实际应用注意点
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         学习率需调参（过大震荡，过小收敛慢）。
        </p>
       </li>
       <li>
        <p>
         参数初始化影响收敛（如 Xavier 初始化）。
        </p>
       </li>
      </ul>
     </li>
    </ol>
    <h2>
     <strong>
      例子2：
     </strong>
     一个简单的多层感知器（MLP）
    </h2>
    <p>
    </p>
    <p>
     下面给出一个基于多分类任务的前馈神经网络参数学习过程，展示如何使用梯度下降法（GD）结合反向传播计算梯度，逐步优化参数。我们以一个简单的多层感知器（MLP）来处理三分类问题为例。
    </p>
    <h3>
     1. 网络结构设定
    </h3>
    <p>
     假设我们的任务是将输入样本分为三个类别（类别1、类别2、类别3）。网络结构如下：
    </p>
    <ul>
     <li>
      <strong>
       输入层
      </strong>
      ：假设输入向量 x∈R^d（例如 d=4）。
     </li>
     <li>
      <strong>
       隐藏层
      </strong>
      ：设有一层隐藏层，包含 h 个神经元，激活函数使用 ReLU。
     </li>
     <li>
      <strong>
       输出层
      </strong>
      ：有 3 个神经元，对应三个类别，激活函数采用 Softmax，将输出转换为概率分布。
     </li>
    </ul>
    <p>
     具体数学描述：
    </p>
    <ul>
     <li>
      隐藏层：
      <img alt="" height="130" src="https://i-blog.csdnimg.cn/direct/f7b8b0c98024432c8896cd8353369cfd.png" width="1282"/>
     </li>
     <li>
      输出层：
      <img alt="" height="130" src="https://i-blog.csdnimg.cn/direct/56b7e7089bc143b29e32696dc8b35140.png" width="1232"/>
     </li>
    </ul>
    <p>
     Softmax 的定义为：
    </p>
    <p>
     <img alt="" height="174" src="https://i-blog.csdnimg.cn/direct/aa607d72332149868971099cbfd427e3.png" width="1506"/>
    </p>
    <h3>
     2. 损失函数
    </h3>
    <p>
     对于多分类任务，我们通常采用多类别交叉熵损失函数。假设真实标签 y 使用 one-hot 编码，交叉熵损失为：
    </p>
    <p>
     <img alt="" height="256" src="https://i-blog.csdnimg.cn/direct/e1e67eea65c64f18b868eb3f9fdad7d0.png" width="1460"/>
    </p>
    <h3>
     3. 具体例子
    </h3>
    <h4>
     网络参数设定（示例数值）
    </h4>
    <p>
     假设输入维度 d=4，隐藏层神经元数量 h=3：
    </p>
    <ul>
     <li>
      输入 x = [1.0, 0.5, -1.0, 2.0]^T。
     </li>
     <li>
      隐藏层权重：
      <img alt="" height="150" src="https://i-blog.csdnimg.cn/direct/4b1ce1fe9e4546a98825b76153f100ab.png" width="1456"/>
      隐藏层偏置：
      <img alt="" height="172" src="https://i-blog.csdnimg.cn/direct/6742ad2a6f45460285cb6819b5e6745c.png" width="1312"/>
     </li>
     <li>
      输出层权重：
      <img alt="" height="156" src="https://i-blog.csdnimg.cn/direct/02d118cf154c452c916901f5ae54b237.png" width="1330"/>
      输出层偏置：
      <img alt="" height="172" src="https://i-blog.csdnimg.cn/direct/20a57dc42f714647a1cfb2b4ce559113.png" width="1396"/>
     </li>
    </ul>
    <p>
     假设真实标签为类别3，即 one-hot 编码 y = [0, 0, 1]^T。
    </p>
    <h4>
     前向传播计算
    </h4>
    <p>
     <strong>
      隐藏层计算
     </strong>
     ：
    </p>
    <ol>
     <li>
      计算
      <img alt="" height="72" src="https://i-blog.csdnimg.cn/direct/56654fe687ed45aca84ed1bc1fdf8947.png" width="400"/>
     </li>
    </ol>
    <p>
     逐个神经元计算：
    </p>
    <ul>
     <li>
      <p>
       神经元1：
      </p>
      <img alt="" height="172" src="https://i-blog.csdnimg.cn/direct/08aaf815d6144d7b859889504a63130c.png" width="1418"/>
     </li>
     <li>
      <p>
       神经元2：
      </p>
      <img alt="" height="168" src="https://i-blog.csdnimg.cn/direct/e31d16cb4bed4c008bc5ae870bec2767.png" width="1480"/>
     </li>
     <li>
      <p>
       神经元3：
      </p>
      <img alt="" height="186" src="https://i-blog.csdnimg.cn/direct/22dbc7b2500d4159a93de55aa0486f3f.png" width="1406"/>
     </li>
    </ul>
    <p>
     得到
     <img alt="" height="70" src="https://i-blog.csdnimg.cn/direct/8f44b567174a4030aef01c059455a503.png" width="480"/>
     .
    </p>
    <p>
     2. 通过 ReLU 激活函数计算 a^{(1)}：
     <img alt="" height="266" src="https://i-blog.csdnimg.cn/direct/5347c1fa525b4110bb14441d3651d6a1.png" width="1518"/>
    </p>
    <p>
     <strong>
      输出层计算
     </strong>
     ：
    </p>
    <ol>
     <li>
      计算
      <img alt="" height="50" src="https://i-blog.csdnimg.cn/direct/72d14526d2fc4133826f49d424675f9b.png" width="476"/>
     </li>
    </ol>
    <p>
     对每个类别计算：
    </p>
    <ul>
     <li>
      <p>
       类别1：
      </p>
      <img alt="" height="176" src="https://i-blog.csdnimg.cn/direct/7e805a1832aa4f8aaefb28bab39adb46.png" width="1372"/>
     </li>
     <li>
      <p>
       类别2：
      </p>
      <img alt="" height="172" src="https://i-blog.csdnimg.cn/direct/591c2fc9513b4bd0bdc1b71458398293.png" width="1438"/>
     </li>
     <li>
      <p>
       类别3：
      </p>
      <img alt="" height="280" src="https://i-blog.csdnimg.cn/direct/eda1fa33947a4aaca54e3414d71fdc33.png" width="1500"/>
     </li>
    </ul>
    <p>
     2. 通过 Softmax 激活函数计算预测概率：
    </p>
    <p>
     <img alt="" height="732" src="https://i-blog.csdnimg.cn/direct/cb729deeb6bd450c9d174d5ccd743e11.png" width="1530"/>
    </p>
    <p>
     此时，模型预测概率为：
    </p>
    <ul>
     <li>
      类别1：42.1%，
     </li>
     <li>
      类别2：27.7%，
     </li>
     <li>
      类别3：30.2%。
     </li>
    </ul>
    <p>
     假设真实标签为类别3，则 one-hot 编码 y = [0,0,1]^T。
    </p>
    <h3>
     4. 损失计算
    </h3>
    <p>
     采用多类别交叉熵损失函数：
    </p>
    <p>
     <img alt="" height="152" src="https://i-blog.csdnimg.cn/direct/3c39141973214fe1a5f78a61e7124ca8.png" width="1430"/>
    </p>
    <p>
     由于 y=[0,0,1] ，损失为：
    </p>
    <p>
     <img alt="" height="116" src="https://i-blog.csdnimg.cn/direct/4835ec9d71de42d2902bcff09c74ebec.png" width="1504"/>
    </p>
    <h3>
     5. 反向传播与参数更新（简要描述）
    </h3>
    <ol>
     <li>
      <p>
       <strong>
        输出层梯度
       </strong>
       ：
      </p>
      <img alt="" height="526" src="https://i-blog.csdnimg.cn/direct/703e30c3076e4d7cb692135fad838b0d.png" width="1394"/>
     </li>
     <li>
      <p>
       <strong>
        隐藏层梯度
       </strong>
       ：
      </p>
      <img alt="" height="468" src="https://i-blog.csdnimg.cn/direct/eee40d67c4fb475a95c560303bb8c1d4.png" width="1520"/>
     </li>
     <li>
      <p>
       <strong>
        参数更新
       </strong>
       ： 使用梯度下降法（例如学习率 η），更新各层参数：
      </p>
      <img alt="" height="132" src="https://i-blog.csdnimg.cn/direct/5db31338dcb24f86b45510b8f9d4af8f.png" width="1414"/>
     </li>
    </ol>
    <p>
     经过多次迭代和大量样本训练，网络参数逐渐调整使得损失函数最小化，模型预测准确率不断提升。
    </p>
    <h3>
     总结
    </h3>
    <p>
     利用梯度下降法对前馈神经网络进行参数学习的过程包括：
    </p>
    <ol>
     <li>
      <strong>
       前向传播
      </strong>
      ：将输入数据通过网络各层计算，得到预测概率。
     </li>
     <li>
      <strong>
       损失计算
      </strong>
      ：利用多类别交叉熵损失函数衡量预测与真实标签之间的差距。
     </li>
     <li>
      <strong>
       反向传播
      </strong>
      ：使用链式法则，从输出层到隐藏层逐层计算梯度。
     </li>
     <li>
      <strong>
       参数更新
      </strong>
      ：依据计算得到的梯度，采用梯度下降（或其变种）更新各层权重和偏置。
     </li>
    </ol>
    <p>
     通过具体的多分类任务示例（例如一个三类别分类问题），我们可以看到如何从输入、前向传播、损失计算、反向传播到参数更新的整个流程，最终实现神经网络参数的优化和任务性能的提升。
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c:6f672e6373646e2e6e65742f6c697275697169616e6730352f:61727469636c652f64657461696c732f313436313032333039" class_="artid" style="display:none">
 </p>
</div>


