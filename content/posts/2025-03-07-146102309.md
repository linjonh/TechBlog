---
arturl_encode: "68747470733a2f2f626c:6f672e6373646e2e6e65742f6c697275697169616e6730352f:61727469636c652f64657461696c732f313436313032333039"
layout: post
title: "前馈神经网络-参数学习梯度下降法-多分类任务"
date: 2025-03-07 18:55:44 +0800
description: "之前的博文中，对于前馈神经网络，我们学习过基于二分类任务的参数学习，本文我们来学习两个多分类任务的参数学习的例子，来进一步加深对反向传播算法的理解。"
keywords: "采用softmax激活函数构建前馈神经网络"
categories: ['Ai']
tags: ['神经网络', '深度学习', '机器学习', '学习', '分类', '人工智能']
artid: "146102309"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146102309
    alt: "前馈神经网络-参数学习梯度下降法-多分类任务"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146102309
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146102309
cover: https://bing.ee123.net/img/rand?artid=146102309
image: https://bing.ee123.net/img/rand?artid=146102309
img: https://bing.ee123.net/img/rand?artid=146102309
---

# 前馈神经网络 - 参数学习（梯度下降法 - 多分类任务）

之前的博文中，对于前馈神经网络，我们学习过基于二分类任务的参数学习，本文我们来学习两个多分类任务的参数学习的例子，来进一步加深对反向传播算法的理解。

## **例子1：前馈神经网络在多分类任务中的参数学习（梯度下降法）**

以下通过一个
**3分类任务**
的具体例子，详细说明前馈神经网络如何使用梯度下降法进行参数学习。假设网络结构如下：

* **输入层**
  ：2个神经元（输入特征 x1​,x2​）
* **隐藏层**
  ：3个神经元（使用 ReLU 激活函数）
* **输出层**
  ：3个神经元（使用 Softmax 激活函数）

### **1. 网络参数初始化**

假设输入样本为 x=[1,0]，真实标签为类别 2（标签编码为 one-hot 向量 y=[0,0,1]）。

**参数定义**
：

* **输入层到隐藏层**
  ：

  ![](https://i-blog.csdnimg.cn/direct/d6d0ba86ef5d4cab96583cbcb6ddb3a0.png)
* **隐藏层到输出层**
  ：

  ![](https://i-blog.csdnimg.cn/direct/6c121735cbaa4c52ac60f6a11a5602ee.png)

### **2. 前向传播**

1. **隐藏层计算**
   ：

   * 线性变换：

     ![](https://i-blog.csdnimg.cn/direct/f22376f6690b4bbf875dfa6f1657eb7e.png)
   * ReLU 激活：

     ![](https://i-blog.csdnimg.cn/direct/e06af27e4de24676abc2a727138f07d9.png)
2. **输出层计算**
   ：

   * 线性变换：

     ![](https://i-blog.csdnimg.cn/direct/44ac24824b5541f980d82f0c04d47129.png)
   * Softmax 激活（转换为概率）：

     ![](https://i-blog.csdnimg.cn/direct/e68af5e21d5c44bca4d1735acc87e5c0.png)

### **3. 损失计算（交叉熵损失）**

真实标签为类别 2（y=[0,0,1]），预测概率为 y^≈[0.31,0.20,0.49]：

![](https://i-blog.csdnimg.cn/direct/c8d0a1c94438491abb7e54a692dc2961.png)

### **4. 反向传播计算梯度**

###### **输出层梯度**

* **Softmax + 交叉熵的梯度简化**
  ：

  ![](https://i-blog.csdnimg.cn/direct/42e05d9c62894e8f82deed40e3084edd.png)
* **参数梯度**
  ：

  ![](https://i-blog.csdnimg.cn/direct/ab9a0ad5aeb34b0d92db848cf2769d5d.png)

###### **隐藏层梯度**

* **误差信号传播**
  ：

  ![](https://i-blog.csdnimg.cn/direct/3e176e343f4248f499565fef0b47a84c.png)
  + ReLU 导数：

    ![](https://i-blog.csdnimg.cn/direct/8f121ff471ab40b585f3b4e0c0b22c85.png)
  + 上游误差：

    ![](https://i-blog.csdnimg.cn/direct/b0f5dffee06244cd91b0fb05a046c281.png)
    ![](https://i-blog.csdnimg.cn/direct/2c81443c4d8246afbf15474dd87b8a1b.png)
  + 逐元素相乘：

    ![](https://i-blog.csdnimg.cn/direct/514fc24355474f68b7b0f2af8a4de278.png)
* **参数梯度**
  ：

  ![](https://i-blog.csdnimg.cn/direct/e6cf04b169144eb8b8f6851233e0be9d.png)

### **5. 参数更新（学习率 η=0.1）**

* **输出层参数**
  ：

  ![](https://i-blog.csdnimg.cn/direct/a5096bd1b2f04e0b95cc039c29a8c847.png)
  ![](https://i-blog.csdnimg.cn/direct/a56a86e5c7c9493d878422481794c903.png)
* **隐藏层参数**
  ：

  ![](https://i-blog.csdnimg.cn/direct/aed39f8ff39d460ab48750876d62359e.png)

### **6. 验证更新后的预测**

更新参数后，重新进行前向传播：

* 隐藏层输出可能更接近真实类别 2，损失 L 应减小。例如，若新的预测概率为 [0.25,0.15,0.60]，则损失为 −log⁡(0.60)≈0.511<0.713，表明参数学习有效。

##### **关键总结**

1. **Softmax + 交叉熵**
   ：

   * 多分类任务的标准组合，梯度计算简化为
     ![](https://i-blog.csdnimg.cn/direct/bbcac144435c4570bed2c818369f0de5.png)
     。
2. **ReLU 导数特性**
   ：

   * 激活导数为 0 或 1，加速计算并缓解梯度消失问题。
3. **梯度下降步骤**
   ：

   * 通过链式法则逐层计算梯度，参数沿负梯度方向更新。
4. **实际应用注意点**
   ：

   * 学习率需调参（过大震荡，过小收敛慢）。
   * 参数初始化影响收敛（如 Xavier 初始化）。

## **例子2：** 一个简单的多层感知器（MLP）

下面给出一个基于多分类任务的前馈神经网络参数学习过程，展示如何使用梯度下降法（GD）结合反向传播计算梯度，逐步优化参数。我们以一个简单的多层感知器（MLP）来处理三分类问题为例。

### 1. 网络结构设定

假设我们的任务是将输入样本分为三个类别（类别1、类别2、类别3）。网络结构如下：

* **输入层**
  ：假设输入向量 x∈R^d（例如 d=4）。
* **隐藏层**
  ：设有一层隐藏层，包含 h 个神经元，激活函数使用 ReLU。
* **输出层**
  ：有 3 个神经元，对应三个类别，激活函数采用 Softmax，将输出转换为概率分布。

具体数学描述：

* 隐藏层：
  ![](https://i-blog.csdnimg.cn/direct/f7b8b0c98024432c8896cd8353369cfd.png)
* 输出层：
  ![](https://i-blog.csdnimg.cn/direct/56b7e7089bc143b29e32696dc8b35140.png)

Softmax 的定义为：

![](https://i-blog.csdnimg.cn/direct/aa607d72332149868971099cbfd427e3.png)

### 2. 损失函数

对于多分类任务，我们通常采用多类别交叉熵损失函数。假设真实标签 y 使用 one-hot 编码，交叉熵损失为：

![](https://i-blog.csdnimg.cn/direct/e1e67eea65c64f18b868eb3f9fdad7d0.png)

### 3. 具体例子

#### 网络参数设定（示例数值）

假设输入维度 d=4，隐藏层神经元数量 h=3：

* 输入 x = [1.0, 0.5, -1.0, 2.0]^T。
* 隐藏层权重：
  ![](https://i-blog.csdnimg.cn/direct/4b1ce1fe9e4546a98825b76153f100ab.png)
  隐藏层偏置：
  ![](https://i-blog.csdnimg.cn/direct/6742ad2a6f45460285cb6819b5e6745c.png)
* 输出层权重：
  ![](https://i-blog.csdnimg.cn/direct/02d118cf154c452c916901f5ae54b237.png)
  输出层偏置：
  ![](https://i-blog.csdnimg.cn/direct/20a57dc42f714647a1cfb2b4ce559113.png)

假设真实标签为类别3，即 one-hot 编码 y = [0, 0, 1]^T。

#### 前向传播计算

**隐藏层计算**
：

1. 计算
   ![](https://i-blog.csdnimg.cn/direct/56654fe687ed45aca84ed1bc1fdf8947.png)

逐个神经元计算：

* 神经元1：

  ![](https://i-blog.csdnimg.cn/direct/08aaf815d6144d7b859889504a63130c.png)
* 神经元2：

  ![](https://i-blog.csdnimg.cn/direct/e31d16cb4bed4c008bc5ae870bec2767.png)
* 神经元3：

  ![](https://i-blog.csdnimg.cn/direct/22dbc7b2500d4159a93de55aa0486f3f.png)

得到
![](https://i-blog.csdnimg.cn/direct/8f44b567174a4030aef01c059455a503.png)
.

2. 通过 ReLU 激活函数计算 a^{(1)}：
![](https://i-blog.csdnimg.cn/direct/5347c1fa525b4110bb14441d3651d6a1.png)

**输出层计算**
：

1. 计算
   ![](https://i-blog.csdnimg.cn/direct/72d14526d2fc4133826f49d424675f9b.png)

对每个类别计算：

* 类别1：

  ![](https://i-blog.csdnimg.cn/direct/7e805a1832aa4f8aaefb28bab39adb46.png)
* 类别2：

  ![](https://i-blog.csdnimg.cn/direct/591c2fc9513b4bd0bdc1b71458398293.png)
* 类别3：

  ![](https://i-blog.csdnimg.cn/direct/eda1fa33947a4aaca54e3414d71fdc33.png)

2. 通过 Softmax 激活函数计算预测概率：

![](https://i-blog.csdnimg.cn/direct/cb729deeb6bd450c9d174d5ccd743e11.png)

此时，模型预测概率为：

* 类别1：42.1%，
* 类别2：27.7%，
* 类别3：30.2%。

假设真实标签为类别3，则 one-hot 编码 y = [0,0,1]^T。

### 4. 损失计算

采用多类别交叉熵损失函数：

![](https://i-blog.csdnimg.cn/direct/3c39141973214fe1a5f78a61e7124ca8.png)

由于 y=[0,0,1] ，损失为：

![](https://i-blog.csdnimg.cn/direct/4835ec9d71de42d2902bcff09c74ebec.png)

### 5. 反向传播与参数更新（简要描述）

1. **输出层梯度**
   ：

   ![](https://i-blog.csdnimg.cn/direct/703e30c3076e4d7cb692135fad838b0d.png)
2. **隐藏层梯度**
   ：

   ![](https://i-blog.csdnimg.cn/direct/eee40d67c4fb475a95c560303bb8c1d4.png)
3. **参数更新**
   ： 使用梯度下降法（例如学习率 η），更新各层参数：

   ![](https://i-blog.csdnimg.cn/direct/5db31338dcb24f86b45510b8f9d4af8f.png)

经过多次迭代和大量样本训练，网络参数逐渐调整使得损失函数最小化，模型预测准确率不断提升。

### 总结

利用梯度下降法对前馈神经网络进行参数学习的过程包括：

1. **前向传播**
   ：将输入数据通过网络各层计算，得到预测概率。
2. **损失计算**
   ：利用多类别交叉熵损失函数衡量预测与真实标签之间的差距。
3. **反向传播**
   ：使用链式法则，从输出层到隐藏层逐层计算梯度。
4. **参数更新**
   ：依据计算得到的梯度，采用梯度下降（或其变种）更新各层权重和偏置。

通过具体的多分类任务示例（例如一个三类别分类问题），我们可以看到如何从输入、前向传播、损失计算、反向传播到参数更新的整个流程，最终实现神经网络参数的优化和任务性能的提升。