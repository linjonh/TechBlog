---
layout: post
title: "Ollama-已在本地部署了deepseek,如何用Python语言调用"
date: 2025-03-08 11:56:10 +0800
description: "OLLAMA_HOST = \"http://localhost:11434\"  # 默认地址。\"num_gpu\": 1,     # GPU层数（需要CUDA支持）\"stream\": False,  # 关闭流式输出。\"stream\": True,  # 启用流式输出。print(f\"API请求失败: {str(e)}\")print(f\"\\n请求失败: {str(e)}\")\"main_gpu\": 0,   # 主GPU索引。\"\"\"流式输出版本（实时显示生成结果）\"\"\"print(\"响应格式异常\")"
keywords: "Ollama 已在本地部署了deepseek，如何用Python语言调用"
categories: ['未分类']
tags: ['人工智能', 'Python', 'Llama']
artid: "146113793"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146113793
    alt: "Ollama-已在本地部署了deepseek,如何用Python语言调用"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146113793
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146113793
cover: https://bing.ee123.net/img/rand?artid=146113793
image: https://bing.ee123.net/img/rand?artid=146113793
img: https://bing.ee123.net/img/rand?artid=146113793
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Ollama 已在本地部署了deepseek，如何用Python语言调用
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     以下是使用 Python 调用部署在 Ollama 上的 DeepSeek 模型的步骤：
    </p>
    <h4>
     1. 安装依赖
    </h4>
    <p>
     bash
    </p>
    <p>
     复制
    </p>
    <pre>pip install requests</pre>
    <h4>
     2. 基础调用代码
    </h4>
    <p>
     python
    </p>
    <p>
     复制
    </p>
    <pre>import requests
import json

# 配置信息
OLLAMA_HOST = "http://localhost:11434"  # 默认地址
MODEL_NAME = "deepseek-chat"  # 确认模型名称（通过 ollama list 查看）

def generate(prompt: str, max_tokens: int = 512, temperature: float = 0.7):
    """
    调用 DeepSeek 模型生成文本
    
    参数：
    prompt: 输入的提示文本
    max_tokens: 最大生成token数（默认512）
    temperature: 温度参数（0-1，默认0.7）
    """
    try:
        response = requests.post(
            url=f"{OLLAMA_HOST}/api/generate",
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False,  # 关闭流式输出
                "options": {
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
            }
        )
        response.raise_for_status()  # 检查HTTP错误
        return json.loads(response.text)["response"]
    
    except requests.exceptions.RequestException as e:
        print(f"API请求失败: {str(e)}")
        return None
    except KeyError:
        print("响应格式异常")
        return None

# 使用示例
if __name__ == "__main__":
    result = generate("请用Python实现快速排序算法")
    if result:
        print("模型回复：\n", result)</pre>
    <h4>
     3. 流式输出版本（适合长文本）
    </h4>
    <p>
     python
    </p>
    <p>
     复制
    </p>
    <pre>def stream_generate(prompt: str, max_tokens: int = 1024):
    """流式输出版本（实时显示生成结果）"""
    try:
        response = requests.post(
            url=f"{OLLAMA_HOST}/api/generate",
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": True,  # 启用流式输出
                "options": {"max_tokens": max_tokens}
            },
            stream=True
        )
        response.raise_for_status()

        full_response = []
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode("utf-8"))
                if not chunk["done"]:
                    print(chunk["response"], end="", flush=True)
                    full_response.append(chunk["response"])
        
        return "".join(full_response)

    except requests.exceptions.RequestException as e:
        print(f"\n请求失败: {str(e)}")
        return None

# 使用示例
stream_generate("请详细解释量子计算的原理")</pre>
    <h4>
     4. 参数说明
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        参数
       </th>
       <th>
        说明
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <code>
         max_tokens
        </code>
       </td>
       <td>
        控制生成文本的最大长度（建议 512-4096）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         temperature
        </code>
       </td>
       <td>
        控制生成随机性（0-1，值越大输出越随机）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         top_p
        </code>
       </td>
       <td>
        核采样概率（0-1，通常与 temperature 二选一）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         stream
        </code>
       </td>
       <td>
        是否启用流式传输（适合长文本场景）
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     5. 常见问题处理
    </h4>
    <p>
     <strong>
      Q1: 出现连接错误
     </strong>
    </p>
    <p>
     python
    </p>
    <p>
     复制
    </p>
    <pre># 检查服务是否运行
import subprocess
subprocess.run(["ollama", "serve"], check=True)</pre>
    <p>
     <strong>
      Q2: 模型不存在错误
     </strong>
    </p>
    <p>
     bash
    </p>
    <p>
     复制
    </p>
    <pre># 查看可用模型列表
ollama list</pre>
    <p>
     <strong>
      Q3: 调整性能参数
     </strong>
    </p>
    <p>
     python
    </p>
    <p>
     复制
    </p>
    <pre>json={
    "model": MODEL_NAME,
    "prompt": prompt,
    "options": {
        "num_ctx": 4096,  # 上下文窗口大小
        "num_gpu": 1,     # GPU层数（需要CUDA支持）
        "main_gpu": 0,   # 主GPU索引
        "seed": 42       # 随机种子
    }
}</pre>
    <h4>
     6. 高级功能
    </h4>
    <p>
     python
    </p>
    <p>
     复制
    </p>
    <pre># 获取模型信息
requests.get(f"{OLLAMA_HOST}/api/show", json={"name": MODEL_NAME})

# 创建模型副本
requests.post(f"{OLLAMA_HOST}/api/create", json={"name": "my-deepseek-copy", "modelfile": f"FROM {MODEL_NAME}"})</pre>
    <p>
     建议根据实际部署情况调整：
    </p>
    <ol>
     <li>
      <p>
       如果服务部署在远程服务器，修改
       <code>
        OLLAMA_HOST
       </code>
      </p>
     </li>
     <li>
      <p>
       通过
       <code>
        ollama pull
       </code>
       确保模型已下载
      </p>
     </li>
     <li>
      <p>
       生产环境建议添加超时处理：
      </p>
     </li>
    </ol>
    <p>
     python
    </p>
    <p>
     复制
    </p>
    <pre>response = requests.post(..., timeout=(10, 30))  # 连接10秒，读取30秒超时</pre>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d6569736f6e6771696e672f:61727469636c652f64657461696c732f313436313133373933" class_="artid" style="display:none">
 </p>
</div>


