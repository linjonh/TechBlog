---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d6569736f6e6771696e672f:61727469636c652f64657461696c732f313436313133373933"
layout: post
title: "Ollama-已在本地部署了deepseek,如何用Python语言调用"
date: 2025-03-08 11:56:10 +0800
description: "OLLAMA_HOST = \"http://localhost:11434\"  # 默认地址。\"num_gpu\": 1,     # GPU层数（需要CUDA支持）\"stream\": False,  # 关闭流式输出。\"stream\": True,  # 启用流式输出。print(f\"API请求失败: {str(e)}\")print(f\"\\n请求失败: {str(e)}\")\"main_gpu\": 0,   # 主GPU索引。\"\"\"流式输出版本（实时显示生成结果）\"\"\"print(\"响应格式异常\")"
keywords: "Ollama 已在本地部署了deepseek，如何用Python语言调用"
categories: ['未分类']
tags: ['人工智能', 'Python', 'Llama']
artid: "146113793"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146113793
    alt: "Ollama-已在本地部署了deepseek,如何用Python语言调用"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146113793
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146113793
cover: https://bing.ee123.net/img/rand?artid=146113793
image: https://bing.ee123.net/img/rand?artid=146113793
img: https://bing.ee123.net/img/rand?artid=146113793
---

# Ollama 已在本地部署了deepseek，如何用Python语言调用

以下是使用 Python 调用部署在 Ollama 上的 DeepSeek 模型的步骤：

#### 1. 安装依赖

bash

复制

```
pip install requests
```

#### 2. 基础调用代码

python

复制

```
import requests
import json

# 配置信息
OLLAMA_HOST = "http://localhost:11434"  # 默认地址
MODEL_NAME = "deepseek-chat"  # 确认模型名称（通过 ollama list 查看）

def generate(prompt: str, max_tokens: int = 512, temperature: float = 0.7):
    """
    调用 DeepSeek 模型生成文本
    
    参数：
    prompt: 输入的提示文本
    max_tokens: 最大生成token数（默认512）
    temperature: 温度参数（0-1，默认0.7）
    """
    try:
        response = requests.post(
            url=f"{OLLAMA_HOST}/api/generate",
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": False,  # 关闭流式输出
                "options": {
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
            }
        )
        response.raise_for_status()  # 检查HTTP错误
        return json.loads(response.text)["response"]
    
    except requests.exceptions.RequestException as e:
        print(f"API请求失败: {str(e)}")
        return None
    except KeyError:
        print("响应格式异常")
        return None

# 使用示例
if __name__ == "__main__":
    result = generate("请用Python实现快速排序算法")
    if result:
        print("模型回复：\n", result)
```

#### 3. 流式输出版本（适合长文本）

python

复制

```
def stream_generate(prompt: str, max_tokens: int = 1024):
    """流式输出版本（实时显示生成结果）"""
    try:
        response = requests.post(
            url=f"{OLLAMA_HOST}/api/generate",
            headers={"Content-Type": "application/json"},
            json={
                "model": MODEL_NAME,
                "prompt": prompt,
                "stream": True,  # 启用流式输出
                "options": {"max_tokens": max_tokens}
            },
            stream=True
        )
        response.raise_for_status()

        full_response = []
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line.decode("utf-8"))
                if not chunk["done"]:
                    print(chunk["response"], end="", flush=True)
                    full_response.append(chunk["response"])
        
        return "".join(full_response)

    except requests.exceptions.RequestException as e:
        print(f"\n请求失败: {str(e)}")
        return None

# 使用示例
stream_generate("请详细解释量子计算的原理")
```

#### 4. 参数说明

| 参数 | 说明 |
| --- | --- |
| `max_tokens` | 控制生成文本的最大长度（建议 512-4096） |
| `temperature` | 控制生成随机性（0-1，值越大输出越随机） |
| `top_p` | 核采样概率（0-1，通常与 temperature 二选一） |
| `stream` | 是否启用流式传输（适合长文本场景） |

#### 5. 常见问题处理

**Q1: 出现连接错误**

python

复制

```
# 检查服务是否运行
import subprocess
subprocess.run(["ollama", "serve"], check=True)
```

**Q2: 模型不存在错误**

bash

复制

```
# 查看可用模型列表
ollama list
```

**Q3: 调整性能参数**

python

复制

```
json={
    "model": MODEL_NAME,
    "prompt": prompt,
    "options": {
        "num_ctx": 4096,  # 上下文窗口大小
        "num_gpu": 1,     # GPU层数（需要CUDA支持）
        "main_gpu": 0,   # 主GPU索引
        "seed": 42       # 随机种子
    }
}
```

#### 6. 高级功能

python

复制

```
# 获取模型信息
requests.get(f"{OLLAMA_HOST}/api/show", json={"name": MODEL_NAME})

# 创建模型副本
requests.post(f"{OLLAMA_HOST}/api/create", json={"name": "my-deepseek-copy", "modelfile": f"FROM {MODEL_NAME}"})
```

建议根据实际部署情况调整：

1. 如果服务部署在远程服务器，修改
   `OLLAMA_HOST`
2. 通过
   `ollama pull`
   确保模型已下载
3. 生产环境建议添加超时处理：

python

复制

```
response = requests.post(..., timeout=(10, 30))  # 连接10秒，读取30秒超时
```