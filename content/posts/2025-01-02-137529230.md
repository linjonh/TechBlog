---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f547736637936754b7944656138365a2f:61727469636c652f64657461696c732f313337353239323330"
layout: post
title: "干货-2024人工智能学习路线图-附链接"
date: 2025-01-02 17:50:58 +08:00
description: "如果觉得本文有帮助，请在推特和领英上关注我！我每周都会和朋友们分"
keywords: "人工智能学习路线"
categories: ['未分类']
tags: ['学习', '人工智能']
artid: "137529230"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=137529230
    alt: "干货-2024人工智能学习路线图-附链接"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=137529230
featuredImagePreview: https://bing.ee123.net/img/rand?artid=137529230
---

# 干货 ：2024人工智能学习路线图 （附链接）

如果觉得本文有帮助，请在推特和领英上关注我！我每周都会和朋友们分享有趣的链接，也可以及时订阅。

想学习人工智能吗？却不知道如何或从哪里开始？

早在2020年，我就在互联网上写下了前20大免费数据科学、ML和AI MOOCs（https://towardsdatascience.com/top-20-free-data-science-ml-and-ai-moocs-on-the-internet-4036bd0aac12），时至今日，很多课程已今非昔比。

为了摆脱“教程陷阱”，学习到真东西，必须动手实践，从头编写算法，复现论文，并通过真实项目用人工智能来解决实际问题。

本文设计了一套遵循该理念的免费课程，我自己也正在学习其中的一些课程，所以如果你想一起学习的话，请联系推特或领英！

另外，如果你认为课程内容不完整的话，敬请留言！

首先，需要做一些关于课程的笔记和一些学习建议。

![2ba0faeb0ad7762dc4d3da9b96040ff8.png](https://i-blog.csdnimg.cn/blog_migrate/4c292b792fcefcdec9fceb53863d13f7.png)

用excalidraw绘图

## **自上而下的方法**

本课程遵循一种自上而下的方法——首先是代码，然后是理论。

我倾向于根据实际需要进行学习。所以，如果需要弄清楚某个东西、解决问题或制作原型，我将广泛地搜集所需信息，深入研究并理解它，然后采取行动。

例如，我的目标是成为一名理解LLM的人工智能工程师（https://www.latent.space/p/ai-engineer），这需要拥有从零开始编写Transformer以及在GPU上微调LLM等技能。但由于存在知识缺漏，我目前无法做到这些。因此，我的目标是填补这些空白。

这个课程主要关注自然语言处理（Natural Language Processing，NLP），如果寻求学习其他人工智能专业，如计算机视觉或强化学习，请在文后评论或在推特或领英上DM我，我会给你一些建议。

在把一堆链接交给你之前，我希望在开始学习之前能有人告诉我两件重要的事情。

## 

## **公开学习**

有很多东西要学习，或许永远也学不完。尤其是人工智能，每周都有新的革命性论文和想法发布。

容易犯的最大错误是在私密状态下学习。这样无法为自己创造任何机会。除了能说你完成了一些事情之外，没有什么可以展示的。更重要的是，如何利用这些信息，将其转化为知识并与公众分享，对这些信息有什么新的想法和解决方案。

所以，应该公开学习（https://www.swyx.io/learn-in-public）。

这意味着要养成创造的习惯，例如：

* 写博客和教程
* 加入黑客马拉松，并与他人合作
* 在Discord社区询问和回答问题
* 从事你感兴趣的业余项目
* 在推特上发布一些你发现的有趣的东西

再来说说推特。

## **使用推特**

如果你关注了对的人，并能够正确地使用它，推特是当今任何人都能参与的价值最高的社交平台。

要关注谁？看看Suhail的人工智能列表（https://twitter.com/i/lists/1539497752140206080?s=20）。

如何使用推特？请阅读尼尔的《如何成功发布推特》（https://near.blog/how-to-twitter-successfully/?curius=1935）。

在推特上发送私信给别人时，要真诚、简洁，并提出具体的要求。Sriram Krishnan写的《如何写冷电子邮件》指南（https://sriramk.com/coldemail/）也适用于私信。

如何发推文？请阅读Instructor创始人Jason（https://twitter.com/jxnlco）写的《推文剖析》（https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg\_edit\_v2&action=edit&isNew=1&type=77&createType=0&token=837735478&lang=zh\_CN&timestamp=1712153003025#hook）一文。他在几个月内从0关注者成长到14k粉丝。

如果你正在读本文，请在推特上关注我！

给我发私信，告诉我你在做什么！我愿意和你合作一些很酷的项目。

现在让我们开始吧。

### 

## **内容列表**

* 1 数学
* 2 工具
    
  ∘2.1  Python（https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg\_edit\_v2&action=edit&isNew=1&type=77&createType=0&token=837735478&lang=zh\_CN&timestamp=1712153003025#6e37）

  ∘2.2  PyTorch
* 3 机器学习
    
  ∘ 3.1从零开始动手实践
    
  ∘3.2竞赛
    
  ∘3.3做项目
    
  ∘3.4部署项目
    
  ∘3.5补充
* 4 深度学习
    
  ∘4.1 Fast.ai
    
  ∘ 4.2参加更多竞赛
    
  ∘ 4.3论文复现

  ∘4.4计算机视觉

  ∘4.5强化学习

  ∘ 4.6 NLP
* 5 大型语言模型

∘ 5.1观看神经网络: 从零基础到高手

∘ 5.2免费 LLM 训练营

∘ 5.3使用LLMs构建应用

∘ 5.4参加黑客松比赛

∘ 5.5阅读论文

∘ 5.6从头开始写 Transformers（https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg\_edit\_v2&action=edit&isNew=1&type=77&createType=0&token=837735478&lang=zh\_CN&timestamp=1712153003025#4df0）

∘ 5.7一些优质博客

∘ 5.8观看 Umar Jamil的视频

∘5.9学习如何运行开源模型

∘5.10 提示工程

∘ 5.11 微调LLM

∘ 5.12 RAG

* 6如何追踪前沿动态
* 7其他有用的资源

## **1 数学**

![fc6479fe69eada6e406ac57552df4cfd.png](https://i-blog.csdnimg.cn/blog_migrate/c428c34d6ea2d683ef51cfc4652a69b6.png)

**DALL·E**

机器学习依赖于三大数学支柱：线性代数、微积分、概率和统计学。它们各自发挥着独特的作用，使算法能够有效地运行。

* 线性代数：用于数据表示和操作的数学工具包，其中矩阵和向量构成了算法解释和处理信息的语言；
* 微积分：机器学习的优化引擎，通过理解梯度和变化率来学习和改进算法；
* 概率和统计：不确定性下决策的基础，通过随机性和可变性模型，使算法能够从数据中预测结果。

从程序员的角度来看，这是一个关于机器学习数学的优秀视频系列：由Weight&Biases团队出品的《机器学习数学》（https://www.youtube.com/playlist?list=PLD80i8An1OEGZ2tYimemzwC3xqkU0jKUg））（代码）（https://github.com/wandb/edu/tree/main/math-for-ml）

如果想要学习从代码入手的线性代数方法，可以看看由fast.ai团队制作的《计算线性代数》（https://www.fast.ai/posts/2017-07-17-num-lin-alg.html）（视频，代码）（https://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY）

在学习过程中，可以阅读《应用机器学习的线性代数简介与Python》（https://pabloinsente.github.io/intro-linear-algebra）。

如果想学习一些更传统的东西，可以观看伦敦帝国理工学院的讲座——《线性代数》（https://www.youtube.com/playlist?list=PLiiljHvN6z1\_o1ztXTKWPrShrMrBLo5P3）和《多元微积分》（ https://www.youtube.com/playlist?list=PLiiljHvN6z193BBzS0Ln8NnqQmzimTW23）。

还可以观看3Blue1Brown的《线性代数的本质》和《微积分的本质》

《线性代数的本质》

https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE\_ab

《微积分的本质》

https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr

如果你想要学习统计学，可以观看StatQuest的《统计学基础》（https://www.youtube.com/playlist?list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9）

**补充**

书籍：《机器学习数学》（https://mml-book.github.io/book/mml-book.pdf）

论文：《深度学习所需的矩阵微积分》（https://arxiv.org/pdf/1802.01528.pdf）

## **2 工具**

![3a195306780ed0700edf7b543c5a1a92.png](https://i-blog.csdnimg.cn/blog_migrate/31997469129f8ccd3cfae71528283fc7.png)

**DALL·E**

### **2.1 Python**

面向初学者：《实用Python编程》

https://dabeaz-course.github.io/practical-python/Notes/Contents.html

有一定Python基础：《高级Python进阶》

https://github.com/dabeaz-course/python-mastery?tab=readme-ov-file

以上课程均由Python Cookbook作者David Beazley制作，制作精良。

之后，可以观看James Powell的一些演讲，以及阅读《Python设计模式》一书。

##### James Powell的一些演讲

##### https://www.youtube.com/playlist?list=PLdQruVCKu10lhVEnAQqRncVc8EIQN-x-X

##### 《Python设计模式》

##### https://python-patterns.guide/

##### 

##### **补充**

##### 

书籍：《Fluent Python》第二版（代码）

https://www.oreilly.com/library/view/fluent-python-2nd/9781492056348/

代码：

https://github.com/fluentpython/example-code-2es

播客：《Real Python》和《Talk Python》

### 

### 《Real Python》

https://realpython.com/podcasts/rpp/

《Talk Python》

https://talkpython.fm/episodes/all

### **2.2 PyTorch**

观看Aladdin Persson的《PyTorch教程》

Aladdin Persson

https://www.youtube.com/c/AladdinPersson

《PyTorch教程》

https://www.youtube.com/playlist?list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz

PyTorch官网同样提供了丰富的学习资源：

* PyTorch 实例教程（https://pytorch.org/examples/）
* PyTorch官方教程（https://pytorch.org/tutorials/index.html）
* 常见问题解答 (FAQ)（https://pytorch.org/docs/stable/notes/faq.html）

完成一些有趣的谜题来测试你的 PyTorch 知识：

lsrush/Tensor-Puzzles: 通过解决谜题提升你的 PyTorch 技能

（https://github.com/srush/Tensor-Puzzles）

##### 

##### **补充**

书籍：《深度学习PyTorch编程》

（https://www.oreilly.com/library/view/programming-pytorch-for/9781492045342/）

**3 机器学习**

![8a11f566f4b1856680f9a1fc91d841d7.png](https://i-blog.csdnimg.cn/blog_migrate/6d26c49cb4eee96ef6b4e6559f3e7449.png)

**DALL·E**

继续阅读《100页机器学习书》。

https://themlbook.com/

### **3.1 从零开始动手实践**

在阅读理论知识的同时，尝试从头开始实现算法。

参考以下代码库：

* leriklindernoren/ML-From-Scratch（https://github.com/eriklindernoren/ML-From-Scratch）
* lJeremyNixon/oracle（https://github.com/JeremyNixon/oracle）
* ltrekhleb/homemade-machine-learning（https://github.com/trekhleb/homemade-machine-learning）

如果想要接受挑战，可以跟随以下课程尝试从零开始搭建PyTorch 深度学习框架：

lMiniTorch: 机器学习工程 DIY 课程 (视频, 代码)

MiniTorch: 机器学习工程 DIY 课程

https://minitorch.github.io/

视频

https://www.youtube.com/playlist?list=PLO45-80-XKkQyROXXpn4PfjF1J2tH46w8

代码

https://github.com/minitorch

### **3.2 竞赛**

将所学的知识运用到竞赛中。

参加bitgrit和Kaggle等平台上的ML比赛；你可以阅读这篇文章获取更多信息。

bitgrit

https://bitgrit.net/competition/

Kaggle

https://www.kaggle.com/

这篇文章

https://towardsdatascience.com/12-data-science-ai-competitions-to-advance-your-skills-in-2021-32e3fcb95d8c

研究过去获奖的解决方案：学习并分析他们是如何解决问题的。

https://farid.one/kaggle-solutions

### **3.3 做项目**

阅读Vicki Boykis的《将机器学习投入生产》。

https://vickiboykis.com/2020/06/09/getting-machine-learning-to-production/

Vicki Boykis还写了一篇关于构建图书语义搜索引擎Viberary的心得体会，可以作为参考。

https://vickiboykis.com/2024/01/05/retro-on-viberary/

获取一个数据集并搭建模型（即，使用earthaccess获取NASA的地理数据）。

https://www.earthdata.nasa.gov/learn/blog/earthaccess

创建一个带有streamlit的UI，并在推特上分享它。

https://streamlit.io/

### **3.4 部署项目**

将模型部署到生产环境并追踪实验结果，学习如何监控模型，亲身体验数据和模型漂移。

以下是一些优质资源：

* Made With ML（https://madewithml.com/）
* DataTalksClub/mlops-zoomcamp: 免费 MLOps 教程（https://github.com/DataTalksClub/mlops-zoomcamp）
* chiphuyen/机器学习系统设计（https://github.com/chiphuyen/machine-learning-systems-design）
* Evidently AI — 机器学习系统设计300例（https://www.evidentlyai.com/ml-system-design）
* stas00/ml工程: 机器学习工程线上书（https://github.com/stas00/ml-engineering）

### **3.5 补充**

PyTorch 和Scikit-Learn 机器学习(代码)

PyTorch 和Scikit-Learn 机器学习

https://www.oreilly.com/library/view/machine-learning-with/9781801819312/

代码

https://github.com/rasbt/machine-learning-book

[1811.12808]机器学习模型评估, 模型选择和算法选择

https://arxiv.org/abs/1811.12808

机器学习面试攻略 · MLIB

https://huyenchip.com/ml-interviews-book/

### 

## **4 深度学习**

![4c42e50c0c70711fb86ed827a3b899ab.png](https://i-blog.csdnimg.cn/blog_migrate/974856fe57dc37e65c4e26a6d10a6914.png)

如果想要自上而下学习，从fast.ai开始。

### 

### **4.1 Fast.ai**

lfast.ai (part1, part2) + W&B 学习小组

part1

https://course.fast.ai/

part2

https://course.fast.ai/Lessons/part2.html

W&B 学习小组

https://wandb.ai/wandb\_fc/events/reports/W-B-Study-Group-Lectures-fast-ai-w-Hugging-Face--Vmlldzo4NDUzNDU?galleryTag=events

喜欢 fast.ai？看看《全栈深度学习》课程。

https://fullstackdeeplearning.com/course/2022

如果想要一个更全面的经典课程，可以考虑François Fleuret的《UNIGE 14x050-深度学习》课程。

François Fleuret

https://fleuret.org/francois/

《UNIGE 14x050-深度学习》

https://fleuret.org/dlc/

如果需要在学习过程中需要用到理论，这些都是很棒的书：

《深入深度学习》(附 PyTorch, NumPy/MXNet, JAX, and TensorFlow示例源代码)

https://d2l.ai/index.html

Ian Goodfellow、Yoshua Bengio和Aaron Courville的《深度学习》

https://www.deeplearningbook.org/

《神经网络和深度学习》

http://neuralnetworksanddeeplearning.com/

《理解深度学习》(附笔记)

https://udlbook.github.io/udlbook/

https://github.com/udlbook/udlbook/tree/main/Notebooks

与其刷社交媒体，不如在手机上阅读《深度学习小书》。

https://fleuret.org/francois/lbdl.html

利用神经网络训练的间隙，可以看看这些资源：

《训练神经网络的秘诀》

https://karpathy.github.io/2019/04/25/recipe

《深度神经网络：33年之前和33 年之后》

https://karpathy.github.io/2022/03/14/lecun1989/

### **4.2 参与更多的竞赛**

PlantTraits2024 — FGVC11 | Kaggle (计算机视觉)

https://www.kaggle.com/competitions/planttraits2024

### **4.3 复现论文**

查看labml.ai注释的PyTorch论文复现。

https://nn.labml.ai/index.html

Papers with Code是个很棒的论文资源网站，例如他们网站上这篇关于BERT的解释文章。

https://paperswithcode.com/method/bert

下面是一些深度学习的专业资源。

### 

### **4.4计算机视觉**

很多人推荐《CS231n：计算机视觉的深度学习》。这门课程很有挑战性，但值得一试。

http://cs231n.stanford.edu/

### **4.5强化学习**

对于强化学习领域，以下两个资源非常棒：

OpenAI 的《旋转式深度强化学习》

https://spinningup.openai.com/en/latest/

Hugging Face的《深度强化学习教程》

https://huggingface.co/learn/deep-rl-course/unit0/introduction

### 

### **4.6 NLP**

斯坦福课程的另一门精彩课程：《CS 224N |深度学习自然语言处理》

https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1234/

学习 《Hugging Face: Hugging Face NLP教程》

https://huggingface.co/learn/nlp-course/chapter1/1

看看Super Duper NLP Repo

https://notebooks.quantumstat.com/?trk=public\_post-text

**优质文章和解析**

《BERT研究 - 第1集 - 核心概念和资源》- Chris McCormick

https://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources

《图解Word2vec》- Jay Alammar

https://jalammar.github.io/illustrated-word2vec

《图解 BERT、ELMo 和其他NLP模型 (自然语言处理如何破解迁移学习)》

https://jalammar.github.io/illustrated-bert

《理解 LSTM 网络》 — colah的博客

https://colah.github.io/posts/2015-08-Understanding-LSTMs

《从零开始用PyTorch实现RNN》- Jake Tae

https://jaketae.github.io/study/pytorch-rnn/

**补充**

《Transformers 自然语言处理》一书

https://transformersbook.com/

### 

## **5 大型语言模型**

![29881b5e80ff37d12d3f421b9ff55dc3.png](https://i-blog.csdnimg.cn/blog_migrate/c9a0e0fab91ba8cb5a2c9745dbe85a98.png)

首先，观看Andrej的《一小时大型语言模型介绍》。

https://www.youtube.com/watch?v=zjkBMFhNj\_g

然后是Alexander Rush（康奈尔科技校区）的《用五个公式理解大型语言模型》

Alexander Rush

https://tech.cornell.edu/people/alexander-rush/

《用五个公式理解大型语言模型》

https://www.youtube.com/watch?v=KCXDr-UOb9A

### **5.1 观看《神经网络：从零基础到高手》**

它以解释和编写反向传播算法为开始，以从头开始编写GPT为结尾。

《神经网络：从零基础到高手》，作者：AndrejKarpathy

https://karpathy.ai/zero-to-hero.html

他刚刚发布了一个新的视频→《构建GPT分词器》

https://www.youtube.com/watch?v=zduSFxRajkE

你也可以看看Jay Mody的《用60行NumPy代码实现GPT》

https://jaykmody.com/blog/gpt-from-scratch

### **5.2 免费LLM训练营**

由Full Stack Deep Learning推出的一套原需付费的大型语言模型（LLM）训练营课程，现已免费开放。

https://fullstackdeeplearning.com/llm-bootcamp/

它教授提示工程、大型语言模型运维（LLMOps）、大型语言模型用户体验（UX）设计，以及如何在一个小时内发布LLM应用程序等内容。

训练营结束，你已跃跃欲试了吧？

### **5.3 使用LLMs构建应用**

想要用LLMs来构建应用程序吗？

观看吴恩达（Andrew Ng）的《利用LLM进行应用开发》课程。

https://nips.cc/virtual/2023/tutorial/73948

阅读Huyen Chip的《构建适用于生产环境的LLM应用》。

https://huyenchip.com/2023/04/11/llm-engineering.html

阅读Eugene Yan的《构建基于LLM的系统和产品的模式》。

https://eugeneyan.com/writing/llm-patterns

请查阅《OpenAI Cookbook》获取代码示例。

https://cookbook.openai.com/

可以使用Vercel AI模板快速上手。

https://vercel.com/templates/ai

### **5.4 参加黑客松比赛**

lablab.ai 每周都会有新的人工智能黑客松比赛。如有合作意向，请告诉我！

https://twitter.com/benxneo

如果你想更深入地研究理论并理解一切是如何运作的，请继续阅读。

### 

### **5.5 阅读论文**

Sebastian Raschka 写了一篇名为《理解大型语言模型》的精彩文章，他在文中列出了一些值得阅读的论文。

Sebastian Raschka

https://sebastianraschka.com/

《理解大型语言模型》

https://magazine.sebastianraschka.com/p/understanding-large-language-models

他最近还发表了另一篇文章，介绍了2024年1月值得阅读的论文，其中还包括了Mistral模型相关论文。

https://magazine.sebastianraschka.com/p/research-papers-in-january-2024

关注他的博客：“前瞻人工智能”，获取更多优质论文推荐。

https://magazine.sebastianraschka.com/

### 

### **5.6从头开始写Transformers**

建议阅读Lilian Weng的博客文章“The Transformer Family Version 2.0 | Lil’Log”，了解大型语言模型架构的概况。

https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/

选择最适合你的格式，并尝试从头开始实现一个Transformer模型。

**论文**

* Attention Is All You Need（https://arxiv.org/abs/1706.03762）
* The Illustrated Transformer（http://jalammar.github.io/illustrated-transformer/）
* The Annotated Transformer by Harvard（http://nlp.seas.harvard.edu/annotated-transformer/）
* Thinking like Transformer（https://srush.github.io/raspy/）

**博客**

《从头开始创建Transformer：一、注意力机制（第2部分）》（代码）

https://benjaminwarner.dev/2023/07/01/attention-mechanism

代码

https://github.com/warner-benjamin/commented-transformers

Sebastian Raschka博士的《从头开始理解和编写大型语言模型的自注意力机制》

Sebastian Raschka博士

https://www.linkedin.com/in/ACoAAAxqHaUBEa6zzXN--gv-wd8ih0vevPvr9eU

《从头开始理解和编写大型语言模型的自注意力机制》

https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html

《从头开始构建Transformers》

https://peterbloem.nl/blog/transformers

**视频**

《使用PyTorch从头开始编写 Transformer代码（包含完整的解释、训练和推理）》

https://www.youtube.com/watch?v=ISNdQcPhsts&t=7449s

《NLP：从零开始实现BERT和 Transformers》

https://www.youtube.com/watch?v=EPa98fyxZ-s&list=PLdM8d\_MWyPjV2vKl7Y2jnIIBRu522tiZc&index=9

你现在可以从头开始编写 Transformer了，但还有更多需要学习的地方。

观看斯坦福大学《cs25-Transformer United》课程的视频。

https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR\_Z27CM

### **5.7一些优质博客**

* 《从零构建大型语言模型：疯狂的梯度下降之旅》

  https://bclarkson-code.github.io/posts/llm-from-scratch-scalar-autograd/post.html
* 《图解Transformer》（作者：Jay Alammar）

  https://jalammar.github.io/illustrated-transformer/
* 《理解注意力机制与Transformer模型》（作者：Eugene Yan）

  https://eugeneyan.com/writing/attention/?curius=1935
* 《加速GPT-KV缓存|不败之路》

  https://www.dipkumar.dev/becoming-the-unbeatable/posts/gpt-kvcache/
* 《超越自注意力：小型语言模型如何预测下一个词元》

  https://shyam.blog/posts/beyond-self-attention/
* 《从头开始搭建Llama（或如何不掉一滴眼泪地复现论文）》（作者：Brian Kitano）

  https://blog.briankitano.com/llama-from-scratch/
* 《改进LoRA：从头开始实现权重分解低秩适应（DoRA）》

  https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch

### 

### **5.8观看 Umar Jamil的视频**

https://www.youtube.com/@umarjamilai/videos

他制作很棒的深度讲解论文的视频，并且展示了相应的代码。

* 《LoRA：低秩适应大型语言模型—可视化解释+从头开始的PyTorch代码》（https://www.youtube.com/watch?v=PXWYUTMt-AU）
* 《Mistral/Mixtral解释：滑动窗口注意力、稀疏化专家混合、滚动缓冲》（https://www.youtube.com/watch?v=UiX8K-xBUpE）
* 《你只需要注意力（Transformer）-模型解释（包括数学）、推理和训练》（https://www.youtube.com/watch?v=bCz4OMemCcA）
* 《LLaMA解释：KV缓存、旋转位置嵌入、RMS范数、分组查询注意力、SwiGLU》（https://www.youtube.com/watch?v=Mn\_9W1nCFLo）
* 《检索增强生成（RAG）解释：嵌入、Sentence BERT、向量数据库（HNSW）》（https://www.youtube.com/watch?v=rhZgXNdhWDY）
* 这些链接仅展示了部分与大型语言模型相关的资源。查看LLM教学大纲（https://github.com/mlabonne/llm-course），了解更全面的LLM知识。

### 

### **5.9学习如何运行开源模型**

可以参考《使用ollama：在本地运行Llama 2、Mistral和其他大型语言模型》（https://github.com/ollama/ollama）。他们提供了相关的Python和JavaScript库，帮助你轻松运行这些模型。

Python和JavaScript库

https://ollama.ai/blog/python-javascript-libraries

### 

### **5.10提示工程**

阅读Lilian Weng的博客：《提示工程》。

https://lilianweng.github.io/posts/2023-03-15-prompt-engineering

可以阅读由OpenAI的Ise Fulford和吴恩达（Andrew Ng）共同撰写的开发者指南：《ChatGPT开发者提示工程》。

https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/

DeepLearning.ai上还有其他免费的短期课程供你学习。

https://www.deeplearning.ai/short-courses/

### 

### 

### **5.11微调LLM**

阅读《Hugging Face的微调指南》

https://huggingface.co/docs/transformers/en/training

一本不错的指南：《微调-GenAI指南》

https://ravinkumar.com/GenAiGuidebook/language\_models/finetuning.html

了解axolotl。

https://github.com/OpenAccess-AI-Collective/axolotl?curius=2790

这是一篇很好的文章：《用直接偏好优化微调Mistral-7b模型》（作者：Maxime Labonne）

https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac

### 

### **5.12 RAG**

Anyscale的精彩文章：《构建基于 RAG的LLM应用程序》

https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1?curius=1144

Aman Chadha写的关于检索增强生成的概述。

Aman Chadha

https://aman.ai/

检索增强生成

https://aman.ai/primers/ai/RAG/

## **6 如何追踪前沿动态**

将时讯，播客和推特三者相结合

关于论文，可以关注AK（@\_akhaliq）

关于播客，最好的是Swyx & Alessio的 Latent Space

敬请加入他们的Discord社区。

他们还有一份时讯Smol Talk，它总结了所有主要的AI Discord讨论。

我喜欢的其他时讯有：

The Batch | DeepLearning.AI | AI新闻与见解（https://www.deeplearning.ai/the-batch/）

* 深度学习周刊
* Interconnects | Nathan Lambert
* AI Tidbits | Sahar Mor

更多信息请参见本文

https://towardsdatascience.com/20-must-subscribe-data-and-ai-newsletters-in-2021-7c5ddb9b3c19

## **7 其他有用的资源**

我的列表并非详尽，如果你还想找到更多资料，这里有一些推荐：

* openai/syllabus.md
* AI Canon | Andreessen Horowitz
* AI Learning Curation — LLM Utils
* Threshold to the AI Multiverse | Open DeepLearning
* louisfb01/start-llms：在2023年开始和提高LLM技能的完整指南

我花了不少时间撰写和整理这些内容，是时候开始学习和搭建模型了。

希望本文将有助你的人工智能之旅！

原文作者：Benedict Neo；转自：数据派THU 公众号；

****版权声明：本号内容部分来自互联网，转载请注明原文链接和作者，如有侵权或出处有误请和我们联系。****

---

**合作请加QQ：365242293**

**数据分析**
（ID : ecshujufenxi ）互联网科技与数据圈自己的微信，也是WeMedia自媒体联盟成员之一，WeMedia联盟覆盖5000万人群。

![a7e583a9c94b99139db68fa330302817.jpeg](https://i-blog.csdnimg.cn/blog_migrate/7925d2d49ce76e551017770bd94133e0.jpeg)