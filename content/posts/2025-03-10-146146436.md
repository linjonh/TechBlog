---
layout: post
title: "刚刚微调-DeepSeek-满血版正式开源"
date: 2025-03-10 10:26:52 +0800
description: "近期，由与联合推出的 DeepSeek-V3/R1 671B 全参数微调开源方案正式发布！该项目完整公开了从模型训练到推理的全流程代码与脚本，并附带了实际训练中的经验总结与优化建议，为大模型开发者提供了一套可直接落地实战的解决方案。基于DeepSeek-V3论文，并结合DeepSeek-V2代码，该项目实现了包含训练核心逻辑的文件，确保与官方架构兼容。支持与，在32台H100服务器集群上完成671B模型的满血版模型全参数微调。"
keywords: "刚刚！微调 DeepSeek 满血版正式开源。。。"
categories: ['未分类']
tags: ['开源']
artid: "146146436"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146146436
    alt: "刚刚微调-DeepSeek-满血版正式开源"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146146436
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146146436
cover: https://bing.ee123.net/img/rand?artid=146146436
image: https://bing.ee123.net/img/rand?artid=146146436
img: https://bing.ee123.net/img/rand?artid=146146436
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     刚刚！微调 DeepSeek 满血版正式开源。。。
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     近期，由
     <strong>
      中国科学院自动化研究所
     </strong>
     与
     <strong>
      中科闻歌
     </strong>
     联合推出的 DeepSeek-V3/R1 671B 全参数微调开源方案正式发布！该项目完整公开了从模型训练到推理的全流程代码与脚本，并附带了实际训练中的经验总结与优化建议，为大模型开发者提供了一套可直接落地实战的解决方案。
    </p>
    <h4>
     <strong>
      一、项目亮点
     </strong>
    </h4>
    <h4>
    </h4>
    <p>
     <strong>
      1、完整训练逻辑代码：
     </strong>
     基于DeepSeek-V3论文，并结合DeepSeek-V2代码，该项目实现了包含训练核心逻辑的
     <code>
      modeling_deepseek.py
     </code>
     文件，确保与官方架构兼容。
    </p>
    <p>
     <strong>
      2、高效并行训练策略：
     </strong>
     支持
     <strong>
      数据并行（DeepSpeed ZeRO）
     </strong>
     与
     <strong>
      序列并行（SP）
     </strong>
     ，在32台H100服务器集群上完成671B模型的满血版模型全参数微调。
    </p>
    <p>
     3、训练实战经验总结：提供多组实验配置对比（如不同超参数、并行策略选择下的显存占用），推荐最优训练参数，助开发者少走弯路。
    </p>
    <p>
     二、项目意义
    </p>
    <p>
     技术角度：通过全参数微调，DeepSeek 模型能更好地训练并拟合目标任务模式和数据分布，整体训练效果优于 LoRA 等低资源微调方案。
    </p>
    <p>
     应用角度：针对模型在预训练阶段已具备基础知识的领域，全参数微调能够挖掘模型在特定垂直领域（如社会计算、媒体领域等）各种下游任务的性能潜力。
    </p>
    <h4>
     <strong>
      三、硬件配置
     </strong>
    </h4>
    <h4>
    </h4>
    <h4>
     <strong>
      单台服务器配置如下表，集群共有 32 台相同配置的机器，共享 100TB 存储空间，挂载路径为
     </strong>
     <code>
      /nfs
     </code>
     。机器操作系统为 Ubuntu 22.04，机器之间使用 IB 网络进行通信，GPU 之间通过 NVLink 通信，CUDA 版本为 12.6。
    </h4>
    <h4>
    </h4>
    <p>
    </p>
    <p class="img-center">
     <img alt="图片" height="373" src="https://i-blog.csdnimg.cn/img_convert/71f975cd517058cd3d5a4691a69618db.png" width="501"/>
    </p>
    <h4>
     四、环境配置
    </h4>
    <h4>
    </h4>
    <h4>
     本项目基于 xtuner 框架进行扩展和改进，使其支持 Deepseek V3/R1（即
     <code>
      DeepseekV3ForCausalLM
     </code>
     模型架构）的全参数微调，支持数据并行（DeepSpeed ZeRO based DP）和序列并行（Sequence Parallel, SP）。安装 Python 环境，可根据项目中
     <code>
      requirements.txt
     </code>
     安装依赖包，并将
     <code>
      ./code/xtuner
     </code>
     与
     <code>
      DeepseekV3ForCausalLM
     </code>
     训练相关的核心代码覆盖原始 xtuner package 的对应代码即可。
    </h4>
    <h4>
    </h4>
    <ul>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>conda create -n ds_env python=3.10</code><code>conda activate ds_env</code><code>pip install -r requirements.txt</code><code># 覆盖核心代码 这里修改为你的环境路径</code><code>YOUR_ENV_PATH='/nfs/miniconda3/envs/ds_env/lib/python3.10/site-packages'</code><code>cp -r ./code/xtuner $YOUR_ENV_PATH</code></pre>
    <h4>
     五、数据准备
    </h4>
    <h4>
    </h4>
    <h4>
     该项目基于 OpenAI 标准数据格式进行扩展以兼容 reasoning 数据，每条原始训练数据格式如下。如果有思考过程，则 assistant 角色的 reasoning_content 字段非空。
    </h4>
    <h4>
    </h4>
    <ul>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>{<!-- --></code><code>  "messages": [</code><code>    {"role": "system", "content": "You are a helpful assistant."},</code><code>    {"role": "user", "content": "用户问题"},</code><code>    {"role": "assistant", "content": "最终回答", "reasoning_content": "思考过程"}</code><code>  ]</code><code>}</code></pre>
    <h4>
     为了简化处理逻辑，该项目将 reasoning_content 和 content 按照 Deepseek 的训练格式合并到 content 字段中。此外，为了兼容多轮对话的训练逻辑，还为 assistant 角色的每轮添加了 loss 字段，仅对值为 true 的 content 内容计算 loss。
    </h4>
    <h4>
    </h4>
    <ul>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>{<!-- --></code><code>  "messages": [</code><code>    {"role": "system", "content": "You are a helpful assistant."},</code><code>    {"role": "user", "content": "用户问题"},</code><code>    {"role": "assistant", "content": "&lt;think&gt;\n思考过程\n&lt;/think&gt;\n\n最终回答", "loss": true}</code><code>  ]</code><code>}</code></pre>
    <h4>
     为了更清晰地展示数据存储格式，该项目提供了一份转换后的数据样例文件 ./data/train_example.json 以供参考。在实际训练时，程序会根据 Deepseek V3/R1 的训练模版自动转换为如下格式，这里仅供展示：
    </h4>
    <h4>
    </h4>
    <ul>
     <li>
     </li>
    </ul>
    <pre><code>&lt;｜begin▁of▁sentence｜&gt;You are a helpful assistant.&lt;｜User｜&gt;用户问题&lt;｜Assistant｜&gt;&lt;think&gt;\n思考过程\n&lt;/think&gt;\n\n最终回答&lt;｜end▁of▁sentence｜&gt;</code></pre>
    <p>
     六、启动训练
    </p>
    <p>
     该项目提供了训练代码和训练启动脚本，其中：
    </p>
    <p>
     ./code/scripts/sft_deepseek.py：sft训练所需的配置文件，包括超参数设置、model和tokenizer配置、训练策略等，模型训练相关的配置均在此文件修改。
    </p>
    <p>
     ./code/scripts/sft_deepseek.sh：sft训练启动脚本，该脚本为单个节点的执行文件，因此需要通过 slurm 或 pdsh 在每台机器执行。对于每台机器，训练启动命令的唯一不同为 NODE_RANK 值，如果共 32 台机器，则该编号分别从 0 到 31。
    </p>
    <p>
     以下是该项目提供的几组实验的结论，包括在不同并行策略等配置下模型训练的可行性。训练数据 ~100k，训练上下文长度为 32k。表中报告了每次实验使用的机器数量（nodes）、序列并行度（sp）、数据并行方式（dp）、单卡 batch size（bs）、迭代轮次（epoch）、学习率（lr）、单卡显存（mem）、实验记录和备注（notes）。
    </p>
    <p>
    </p>
    <p class="img-center">
     <img alt="图片" height="367" src="https://i-blog.csdnimg.cn/img_convert/56c2f3433422b62b14ea075838749de2.png" width="687"/>
    </p>
    <h4>
     以下是训练过程中的一个截图，从 DeepSeek V3 对该项目使用的 reasoning 数据进行全参数微调时，起始 loss 通常在 3.5 左右，经过 1 epoch 训练后，loss 收敛到 1.2 左右。
    </h4>
    <h4>
    </h4>
    <p>
    </p>
    <p class="img-center">
     <img alt="图片" height="289" src="https://i-blog.csdnimg.cn/img_convert/7946ab2ba32c7f08dc5603bc91140d24.png" width="1080"/>
    </p>
    <p>
     七、模型权重转换
    </p>
    <p>
     训练过程中建议使用至少 100TB 的 SSD 大容量存储，因为单个 pth 中间结果大约占 7.4TB 硬盘空间。训练完成后，需要将 pth 转换为主流推理框架（如vllm等）较好兼容的 huggingface 格式。在单台机器节点执行 bash ./code/scripts/convert_pth_to_hf.sh 即可完成模型权重格式转换，可根据实际情况修改脚本中的 pth 路径和权重保存路径。
    </p>
    <p>
     需要注意的是，由于本过程对 CPU 内存有较大需求，因此可以通过虚拟内存进行扩展，防止 Out-of-memory。Swap（交换分区） 是 Linux 的虚拟内存，作用是当物理内存（RAM）不够用时，把部分数据存入磁盘，释放 RAM。
    </p>
    <ul>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>sudo fallocate -l 8192G /tmp/swapfile  # 创建 8T 交换文件</code><code>sudo chmod 600 /tmp/swapfile</code><code>sudo mkswap /tmp/swapfile</code><code>sudo swapon /tmp/swapfile</code><code>free -h  # 检查 swap 是否增加</code></pre>
    <p>
     八、模型推理部署
    </p>
    <p>
     该项目使用 vLLM 对全参数微调后的模型进行简单部署测试。如果使用 slurm 集群，可参考该项目提供的脚本并执行 sbatch 命令 sbatch ./code/scripts/vllm_deploy_slurm.sh 即可提交作业。半精度（bf16/fp16）模型建议使用4台机器32卡进行部署，如需配置 ray 或 api server 的端口号，可自行修改 sh 文件。如果需要通过 pdsh 启动部署（假设使用 node0~node3 四台机器），可参考以下步骤：
    </p>
    <p>
     1、设置环境变量（node0~node3）。
    </p>
    <ul>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>export HEAD_ADDR="node0"</code><code>export DASHBOARD_PORT=8265</code><code>export HEAD_PORT=6379</code><code>export RAY_TMPDIR=/tmp/ray_tmp/</code><code>export RAY_ADDRESS=$HEAD_ADDR:$HEAD_PORT</code></pre>
    <p>
     2、启动 Ray Head（node0）。
    </p>
    <ul>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>pdsh -R ssh -w node0 "source /nfs/miniconda3/etc/profile.d/conda.sh &amp;&amp; conda activate vllm &amp;&amp; \</code><code>ray start --block --head --port=$HEAD_PORT --dashboard-port=$DASHBOARD_PORT --temp-dir=$RAY_TMPDIR"</code></pre>
    <h4>
     3、启动 Ray Worker（node1~node3）。
    </h4>
    <h4>
    </h4>
    <ul>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>pdsh -R ssh -w node1,node2,node3 "source /nfs/miniconda3/etc/profile.d/conda.sh &amp;&amp; conda activate vllm &amp;&amp; \</code><code>ray start --block --address=$HEAD_ADDR:$HEAD_PORT"</code></pre>
    <h4>
     4、启动 vLLM（node0）。
    </h4>
    <h4>
    </h4>
    <ul>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
     <li>
     </li>
    </ul>
    <pre><code>pdsh -R ssh -w node0 "source /nfs/miniconda3/etc/profile.d/conda.sh &amp;&amp; conda activate vllm &amp;&amp; \</code><code>vllm serve /path/of/your/deepseek_sft_ckpt \</code><code>    --tensor-parallel-size 8 \</code><code>    --pipeline-parallel-size 4 \</code><code>    --served-model-name deepseek-r1-sft \</code><code>    --max-model-len 32768 \</code><code>    --trust-remote-code \</code><code>    --enable-reasoning \</code><code>    --reasoning-parser deepseek_r1"</code></pre>
    <h4>
     启动完成后，可通过 curl 命令测试接口是否正常启动：
    </h4>
    <ul>
     <li>
     </li>
    </ul>
    <pre><code>curl -X POST http://node0:8000/v1/chat/completions -d '{"model": "deepseek-r1-sft", "messages":[{"role":"user", "content": "hello"}]}' -H "Content-Type: application/json"</code></pre>
    <p>
     稍等片刻后，如果终端输出符合预期的响应结果，则说明从训练到部署到整个过程顺利完成！🎉
    </p>
    <p>
     Github开源地址：
    </p>
    <p>
     https://github.com/ScienceOne-AI/DeepSeek-671B-SFT-Guide
    </p>
    <p>
     来源 | AI有道
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f:626c6f672e6373646e2e6e65742f41494269674d6f64656c2f:61727469636c652f64657461696c732f313436313436343336" class_="artid" style="display:none">
 </p>
</div>


