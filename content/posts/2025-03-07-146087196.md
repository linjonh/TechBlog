---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36353633393030392f:61727469636c652f64657461696c732f313436303837313936"
layout: post
title: "大语言模型从理论到实践第二版-学习笔记绪论"
date: 2025-03-07 13:39:58 +08:00
description: "快速了解大语言模型"
keywords: "含约 7 万单词,句子长度按照 20 个词计算,语言模型参数量达 到 7.9792 ×1096 的"
categories: ['未分类']
tags: ['语言模型', '自然语言处理', '笔记', '深度学习', '机器学习', '学习', 'Python']
artid: "146087196"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146087196
    alt: "大语言模型从理论到实践第二版-学习笔记绪论"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146087196
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146087196
cover: https://bing.ee123.net/img/rand?artid=146087196
image: https://bing.ee123.net/img/rand?artid=146087196
img: https://bing.ee123.net/img/rand?artid=146087196
---

# 大语言模型从理论到实践（第二版）-学习笔记（绪论）

## 大语言模型的基本概念

1.理解语言是人工智能算法获取知识的前提

2.语言模型的目标就是对自然语言的概率分布建模

3.词汇表 V 上的语言模型，由函数 P(w1w2 ·· · wm) 表示，可以形式化地构建为词序列 w1w2 ·· · wm 的概率分布，表示词序列 w1w2 ·· · wm 作为一个句子出现的可能性的大小，
**参数量大，计算困难。**

4.为了减小 P(w1w2 ·· · wm) 模型的参数空间，可以利用句子序列（通常是从左至右）的生成过
  
程将其进行分解，使用链式法则可以得到

![](https://i-blog.csdnimg.cn/direct/3dcf809252ca40c0a4dcc5ab0a57d9e5.png)

5.但是，仅通过上述过程模型的
**参数空间依然没有减小**
，P(wm|w1w2 ·· · wm−1) 的参数空间依然是天文数字。为了解决上述问题，可以进一步假设任意单词 wi 出现的概率只与过去 n − 1 个词相关

![](https://i-blog.csdnimg.cn/direct/a326006831e44d69a5c82c4b210c0eb7.png)

虽然 n 元语言模型能缓解句子概率为零的问题，但语言是由人和时代创造的，具备无尽的可
  
能性，再庞大的训练数据也无法覆盖所有的 n-gram，而训练数据中的零频率并不代表零概率

平滑处理的基本思想是提高低概率事件，降低高概率事件
**使整体的概率分布趋于均匀**
。这类方法通常被称为统计语言模型（Statistical Language Models，SLM）。相关平滑算法细节可以参考《自然语言处理导论》的第 6 章。缺点：

**（1）无法对长度超过 n 的上下文建模。
  
（2）依赖人工设计规则的平滑技术。
  
（3）当 n 增大时，数据的稀疏性随之增大，模型的参数量更是呈指数级增加，受数据稀疏问
  
题的影响，其参数难以被准确学习。
  
此外，n 元文法中单词的离散表示也忽略了单词之间的相似性。因此，基于分布式表示和神经
  
网络的语言模型逐渐成为研究热点**

6.词的独热编码被映射为一个低维稠密的实数向量，称为词向量。估计词概率。相较于 n 元语言模型，神经网络方法可以在一定程度上
**避免数据稀疏问题**
，有些模型还可以摆脱对历史文本长度的限制，从而更好地对
**长距离依赖关系建模**
。这类方法通常被称为神经语言模型

## 大语言模型的发展历程

大语言模型的发展历程虽然只有不到 5 年，但是发展速度相当惊人，截至 2025 年 2 月，国内
  
外有超过百种大语言模型相继发布。特别是 2024 年 12 月 DeepSeek V3 和 2025 年 1 月 DeepSeek R1 模型的开源

大语言模型的发展可以粗略地分为如下三个阶段：基础模型阶段、能力探索阶段和突破发展阶段

![](https://i-blog.csdnimg.cn/direct/f42aed826e8b4135b314bc69735e279e.png)

### 1.基础模型阶段（2018-2021）

![](https://i-blog.csdnimg.cn/direct/dfc3ce50166d423194c57aed2f5d190d.png)

2017 年，Vaswani 等人提出了 Transformer[12]架构，在机器翻译任务上取得了突破性进展。2018 年，Google 和 OpenAI 分别提出了 BERT[1] 和GPT开启了
**预训练语言模型时代（模型的训练仅需要大规模无标注文本。语言模型也成了典型的自监督学习（Self-supervised Learning）任务。互联网的发展，使得大规模文本非常容易获取，因此训练超大规模的基于神经网络的语言模型成为可能）**
。BERT-Base 版本的参数量为 1.1 亿个，BERT-Large 版本的参数量为 3.4 亿个，GPT-1 的参数量为 1.17 亿个。这在当时，比其他深度神经网络的参数量，已经有了数量级上的提升。2019 年 OpenAI 发布了 GPT-2[11]，其参数量达到 15 亿个。此后，Google也发布了参数规模为 110 亿个的 T5[19] 模型。2020 年，OpenAI 进一步将语言模型的参数量扩展到 1750 亿个，发布了 GPT-3[13]。此后，国内也相继推出了一系列的大语言模型，包括清华大学的ERNIE[20]、百度的 ERNIE[21]、华为的 PanGU-α[22] 等。

此阶段的研究主要集中在语言模型本身，对仅编码器（Encoder Only）、编码器-解码（Encoder-Decoder）、仅解码器（Decoder Only）等各种类型的
**模型结构**
都有相应的研究。
**模型大小**
与 BERT 类似，
**通常采用预训练微调范式（使用具体任务的标注数据在预训练语言模型上进行监督训练），针对不同下游任务进行微调**
。这些模型参数量大都在 10 亿个以上，由于微调的计算量很大，这类模型的影响力在当时相较 BERT 类模型有不小的差距。

### 2.能力探索阶段（2019 -2022 ）

![](https://i-blog.csdnimg.cn/direct/908beb5a6ac046a5ade43ddd6a9cb3fe.png)

由于大语言模型很难针对特定任务进行微调，研究人员开始探索在不针对单一任务进行微调的情况下如何发挥大语言模型的能力。

2019 年，Radford等人在文献 [11] 中使用 GPT-2 模型研究了大语言模型在零样本情况下的任务处理能力。在此基础上，Brown 等人在 GPT-3[13] 模型上研究了
**通过语境学习进行少样本学习**
（few-shot learning）的方法，
**将不同任务的少量有标注的实例拼接到待分析的样本之前输入语言模型，语言模型根据实例理解任务并给出正确的结果。**

> 利用了语言模型的“语境学习”（in-context learning）能力。模型不需要专门针对情感分析任务重新训练，只要通过前面的少量示例，就能“学会”任务的规则并应用到新样本上。
>
> ```
> 任务：情感分析（判断句子是积极还是消极）
> 假设我们要让模型判断一句话的情感是“积极”还是“消极”，但我们没有大量标注数据来训练模型。这时可以用少样本学习的方法。
>
> 输入给模型的内容：
> 示例1：我今天很开心。 -> 积极
> 示例2：这场电影太无聊了。 -> 消极
> 待分析的样本：天气很好，我很享受。 -> ?
>
> 解释：
> 我们先给了模型两个示例：
> “我今天很开心。”标注为“积极”。
> “这场电影太无聊了。”标注为“消极”。
> 然后把待分析的句子“天气很好，我很享受。”接在后面，让模型根据前面的示例自己推断。
> 模型会“看”到前面的模式（开心=积极，无聊=消极），然后判断新句子“天气很好，我很享受”应该是“积极”。
>
> 输出：
> 模型可能会回答：积极
> ```

基于 GPT-3 的语境学习在 TriviaQA、WebQS、CoQA 等评测集合中都展示出了非常强的能力，在有些任务中甚至超过了此前的有监督方法。上述方法不需要修改语言模型的参数，模型在处理不同任务时无须花费大量计算资源进行模型微调。

仅依赖语言模型本身，其性能在
**很多任务上**
仍然很难达到有监督学习（Supervised Learning）的效果，因此研究人员提出了指令微调[23] 方案，将大量各类型任务统一为生成式自然语言理解框架，并构造训练数据进行微调。大语言模型能一次性学习数千种任务，并在未知任务上展现出很好的泛化能力。

> 指令微调的核心是把任务标准化（都变成“指令+生成”）：
>
> 训练数据：
>
> * 任务1：翻译 -> “把‘Hello’翻译成中文” -> 输出“こんにちは”
> * “把这句话翻译成法语 -> I like cats” -> “J’aime les chats”
> * 任务2：情感分析 -> “判断‘我很开心’的情感” -> 输出“积极
>
> * 任务3：“回答问题->今天是星期几？” -> “今天是星期三”
> * 任务4：“写一段关于狗的描述” -> “狗是忠诚的动物…”
>
> 用大量多样化的数据训练模型，让它变成一个“全能选手”。这样不仅能处理已知任务，还能灵活应对新任务，比单纯依赖预训练模型强很多

2022 年，Ouyang 等人提出了使用
**“有监督微调 + 强化学习”**
的 InstructGPT[24] 方法，该方法使用
**少量有监督数据就可以使大语言模型服从人类指令**
。Nakano 等人则探索了结合搜索引擎的问题回答方法 WebGPT[25]。这些方法在直接利用大语言模型进行零样本和少样本学习的基础上，逐渐扩展为
**利用生成式框架针对大量任务进行有监督微调**
的方法，有效提升了模型的性能。

> **“有监督微调 + 强化学习”：InstructGPT 方法**
>
> **有监督微调**
>
> * 数据量不需要很大（比如几百到几千个示例），但质量要高。
> * 这一步让模型初步理解“指令 -> 回答”的模式。
>
> **强化学习**
>
> * 模型尝试生成回答。
> * 奖励模型给这个回答打分（高分=好回答，低分=差回答）。
> * 模型根据分数调整自己，倾向于生成高分的回答。

### 3.突破发展阶段（ 2022 年 11 月 ChatGPT 的发布为起点）

ChatGPT 通过一个简单的对话框，利用一个大语言模型就可以实现问题回答、文稿撰写、代码生成、数学解题等过去自然语言处理系统需要大量小模型定制开发才能分别实现的能力。它在开放领域问答、各类自然语言生成式任务及对话上下文理解上所展现出来的能力远超大多数人的想象。2023 年 3 月 GPT-4 发布，相较于ChatGPT，GPT-4 有非常明显的进步，并具备了多模态理解力。GPT-4 在多种基准考试测试上的得分高于 88% 的应试者，包括美国律师资格考试（Uniform Bar Exam）、法学院入学考试（LawSchool Admission Test）、学术能力评估（Scholastic Assessment Test，SAT）等。GPT-4o 是 OpenAI于 2024 年 5 月发布的多模态大模型，其中“o”代表“omni”即“全能”。它能接受文本、音频和图像组合输入并生成文本、音频和图像的任意组合输出，可处理 50 种语言，在 232 毫秒内对音频输入做出反应，性能较 GPT-4 有显著提升。2024 年 9 月 OpenAI 又推出的全新推理模型 GPT-o1，在复杂推理任务上表现卓越，能通过内部思维链模拟人类思考，在数学、科学等领域超越人类专家及 GPT-4o。国内外各大公司和研究机构相继发布了此类系统，包括复旦大学的 MOSS、阿里巴巴的 Qwen、深度求索的 DeepSeek、Google 的 Gemini、XAI 的 Grok、科大讯飞的星火大模型、智谱的 ChatGLM 等。

### 截至 2025 年 2 月典型开源和闭源大语言模型的基本情况

模型类型中，基础模型是指仅经过预训练的模型

对话模型是指在预训练模型基础上经过有监督微调和强化学习训练的模型，具备对话和完成任务的能力

推理模型是指专注于逻辑推理增强的大语言模型

![](https://i-blog.csdnimg.cn/direct/f4b4e4d705504122ac405ccd1322cc44.png)

![](https://i-blog.csdnimg.cn/direct/15e45f53f57840698c10158c1c1dcebd.png)

![](https://i-blog.csdnimg.cn/direct/5ea161ad40ac423f86a0c599407fb0d6.png)

## 大语言模型的构建流程

OpenAI 使用的大语言模型构建流程如图1.3 所示，主要包含四个阶段：预训练、有监督微调、奖励建模和强化学习。这四个阶段都需要不同规模的数据集及不同类型的算法，会产出不同类型的模型，所需要的资源也有非常大的差别。

![](https://i-blog.csdnimg.cn/direct/275504baaafd4012bb43c732657aace1.png)

### 预训练（Pretraining）阶段

需要利用海量的
**训练数据**
（数据来自互联网网页、维基百科、书籍、
  
GitHub、论文、问答网站等），基础模型对长文本进行建模，使模型具有语言生成能力，根据输入的提示词，模型可以生成文本补全句子。有一部分研究人员认为，语言模型建模过程中隐含地构建了包括事实性知识（Factual Knowledge）和常识性知识（Commonsense）在内的世界知识（World Knowledge）由于训练过程需要消耗大量的计算资源，并很容易受到超参数影响，因此，如何
**提升分布式计算效率并使模型训练稳定收敛是本阶段的研究重点**

### 有监督微调（Supervised Fine Tuning，SFT）

也称为指令微调，利用少量高质量数据集，通过有监督训练使模型具备问题回答、翻译、写作等能力。有监督微调的数据包含用户输入的提示词和对应的理想输出结果。用户输入包括问题、闲聊对话、任务指令等多种形式和任务。

经过训练的 SFT 模型具备初步的指令理解能力和上下文理解能力，能够完成开放领域问答、阅读理解、翻译、生成代码等任务，也具备了一定的对未知任务的泛化能力。由于有监督微调阶段所需的训练数据量较少，SFT 模型的训练过程并不需要消耗大量的计算资源

SFT 模型具备了初步的任务完成能力，可以开放给用户使用，很多类 ChatGPT 的模型都属于该类
  
型，包括 Alpaca[35]、Vicuna[41]、MOSS、ChatGLM-6B 等。很多这类模型的效果非常好，甚至在一些评测中达到了 ChatGPT 的 90% 的效果[35, 41]。当前的一些研究表明，有监督微调阶段的数据选择对 SFT 模型效果有非常大的影响[42]，因此
**构造少量并且高质量的训练数据是本阶段的研究重点。**

### **奖励建模（Reward Modeling）阶段**

目标是构建一个文本质量对比模型。对于同一个提示词，SFT 模型对给出的多个不同输出结果的质量进行排序。奖励模型可以通过二分类模型，
**对输入的两个结果之间的优劣进行判断**
。奖励模型与基础模型和 SFT 模型不同，奖励模型本身并不能单独提供给用户使用。奖励模型的训练通常和 SFT 模型一样，使用数十块 GPU，通过数天时间完成训练。

由于奖励模型的准确率对强化学习阶段的效果有至关重要的影响，因此通常需要大规模的训
  
练数据对该模型进行训练。Andrej Karpathy 在报告中指出，该部分需要
**百万量级的对比数据标注**
，而且其中很多标注需要很长时间才能完成。图1.4 给出了 InstructGPT 系统中奖励模型训练样本标注示例[24]。可以看到，示例中文本表达都较为流畅，
**标注其质量排序需要制定非常详细的规范**
，
**标注者也需要认真地基于标注规范进行标注**
，需要消耗大量的人力。同时，
**保持众标注者之间的一致性**
，也是奖励建模阶段需要解决的难点问题之一。此外，奖励模型的泛化能力边界也是本阶段需要重点研究的一个问题。如果奖励模型的目标是针对系统所有的输出都能够高质量地进行判
  
断，那么该问题的难度在某种程度上与文本生成等价，因此
**限定奖励模型应用的泛化边界是本阶
  
段需要解决的问题。**

> * **奖励模型**
>   是在 InstructGPT 这种“有监督微调 + 强化学习”方法中用来评估语言模型输出的工具。它根据人类反馈（比如“好”或“不好”）预测某个输出有多符合人类期望。
> * **泛化能力**
>   指的是奖励模型能不能在没见过的新输出上也做出准确判断。
> * **难度等价于文本生成**
>
>   文本生成（比如 GPT 生成句子）本身就很难，因为它需要理解语法、语义、逻辑、世界知识等。
>   **奖励模型要判断所有生成的文本质量，相当于也要理解这些东西，甚至还要加上“人类偏好”（比如什么是“有用”“礼貌”）**
>   。所以，造一个能完美评分所有输出的奖励模型，难度不比造一个完美的语言模型低。
> * 奖励模型的泛化能力是个研究重点，因为我们希望它能评判所有输出，但这太难了（难度堪比文本生成）。
> * 所以需要明确它的“边界”：哪些输出它能判断，哪些不能，而不是让它盲目尝试。
> * 例如，研究者可能决定：奖励模型只负责判断“数学问题”和“身份问题”的回答质量，不扩展到“天气”或“宇宙”这类复杂领域。
> * 研究人员需要找到方法，比如用更多样化的训练数据、设计多个专门的奖励模型（而不是一个通用的），来平衡泛化能力和实用性。
> * 总结：奖励模型很关键，但不能指望它完美判断所有东西。它的能力有边界，这个边界有多大、怎么划定，是现在需要研究的重点。不然，奖励模型要么“自不量力”乱评分，要么“过于谨慎”没用处。

> ##### **主要内容**
>
> 1. **任务**
>    ：对关于鹦鹉研究的不同文本片段进行排序，以确定它们的相对质量和信息价值。
> 2. **排名规则**
>    ：
>    * **Rank 1（最佳）**
>      ：最具相关性和信息价值的文本。
>    * **Rank 5（最差）**
>      ：最无关或质量最低的文本。
> 3. **具体排名**
>    ：
>    * **A（最佳）**
>      ：描述鹦鹉模仿人类语言的能力，信息丰富，贴近主题，因此排名第一。
>    * **C（第二名）**
>      ：讨论了鹦鹉理解数字的能力，与主题相关，但比 A 稍微次之。
>    * **E 和 D（第三名）**
>      ：虽然描述了鹦鹉的听觉和视觉特性，但与主题（模仿能力）关联较弱，因此排名较低。
>    * **B（未排名，但可能较低）**
>      ：尽管提到了鹦鹉的发声能力，但内容可能不如 A 具体，因此未排在首位。
>    * **Rank 4 和 Rank 5 为空**
>      ，说明这些文本的质量都不算特别差。
>
> 排名 1（最佳）：
>   
> A：美国的一个研究小组发现，鹦鹉可以轻松模仿人类的语言，其中一些甚至可以像人类一样进行发声。该小组研究了鹦鹉在自然栖息地中发出的声音，并发现它们使用舌头和喙的方式与人类惊人地相似……
>
> 排名 2：
>   
> C：研究发现鹦鹉具有理解数字的能力。研究人员发现，鹦鹉可以理解最多六个数字。在一系列实验中，鹦鹉能够识别放在多个杯子下面的食物数量……
>
> 排名 3：
>   
> E：科学家发现，绿翅鹦鹉能够分辨两个除了听觉顺序不同之外完全相同的声音。这一点很重要，因为绿翅鹦鹉以模仿声音而闻名。这项研究表明，它们能够理解声音之间的区别……
>
> D：当前的研究表明，鹦鹉看见和听见世界的方式与人类不同。人类可以看到彩虹的各种颜色，而鹦鹉只能看到红色和绿色的色调。此外，鹦鹉还可以看到紫外线光，而紫外线光对人类是不可见的。许多鸟类都具备这种能力……
>
> 排名 4：（空缺）
>
> 排名 5（最差）：（空缺）

### 强化学习（Reinforcement Learning，RL）阶段

根据数十万条提示词，利用前一阶段训练的奖励模型，给出 SFT 模型
**对提示词回答结果的质量评估，并与语言模型建模目标综合**
得到更好的效果。该阶段使用的提示词数量与有监督微调阶段类似，数量在十万个量级，
**并且不需要人工提前给出该提示词所对应的理想回复**
。使用强化学习，在 SFT 模型的基础上调整参数，使最终生成的文本可以获得更高的奖励（Reward）。该阶段需要的计算量较预训练阶段也少很多，通常仅需要数十块GPU，数天即可完成训练。文献 [24] 给出了强化学习和有监督微调的对比，在模型参数量相同的情况下，强化学习可以得到相较于有监督微调好得多的效果。关于为什么强化学习相比有监督微调可以得到更好结果的问题，截至 2025 年 2 月还没有完整或得到普遍共识的解释。目前相对得到认可的观点是，强化学习使得模型具备更好的泛化能力[43]。同时，Andrej Karpathy 也指出，强化学习并不是没有问题的，它会使基础模型的熵降低，从而减少模型输出的多样性。
**经过强化学习方法训练后的 RL 模型**
，就是最终提供给用户使用、具有理解用户指令和上下文的类 ChatGPT 系统。
**由于强化学习方法稳定性不高，并且超参数众多，使得模型收敛难度大，叠加奖励模型的准确率问题，使得在大语言模型上有效应用强化学习非常困难。**