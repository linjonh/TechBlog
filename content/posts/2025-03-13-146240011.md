---
layout: post
title: "多种注意力机制文本-残差-视频"
date: 2025-03-13 21:13:55 +0800
description: "讲述多种注意机制"
keywords: "多种注意力机制（文本-＞残差-＞视频）"
categories: ['基于Prompt视觉语言模型的长视频行文理解分析']
tags: ['深度学习', '人工智能']
artid: "146240011"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146240011
    alt: "多种注意力机制文本-残差-视频"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146240011
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146240011
cover: https://bing.ee123.net/img/rand?artid=146240011
image: https://bing.ee123.net/img/rand?artid=146240011
img: https://bing.ee123.net/img/rand?artid=146240011
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     多种注意力机制（文本-＞残差-＞视频）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     1.初代自我注意机制(多头注意力机制)
    </h2>
    <h3>
     1.1原理
    </h3>
    <p>
     <strong>
      总体架构
     </strong>
     <img alt="" height="1002" src="https://i-blog.csdnimg.cn/direct/e48e6e7a3c814782bc8a4ab504758168.png" width="1422"/>
    </p>
    <p>
    </p>
    <p>
     上图是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(
     <strong>
      单词的表示向量x组成的矩阵X
     </strong>
     ) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。
    </p>
    <p id="h_338817680_7" name="h_338817680_7" style="background-color:transparent">
     <strong>
      Q, K, V 的计算  这个X无疑是由不同单词（词向量）组成的
     </strong>
    </p>
    <p>
     如下图无疑是计算出了Q K V
    </p>
    <h5 name="h_338817680_7">
     <img alt="" height="875" src="https://i-blog.csdnimg.cn/direct/664a8bc8a9d1467d8f928124189f5ce7.png" width="640"/>
    </h5>
    <p style="background-color:transparent">
     <strong>
      多头注意力的输出
     </strong>
     <img alt="" height="171" src="https://i-blog.csdnimg.cn/direct/41a7101a80904fe09196d21885c7753d.png" width="640"/>
    </p>
    <p>
     公式中计算矩阵
     <strong>
      Q
     </strong>
     和
     <strong>
      K
     </strong>
     每一行向量的内积，为了防止内积过大，因此除以
     <img alt="d_{k}" src="https://latex.csdn.net/eq?d_%7Bk%7D">
      的平方根。
      <strong>
       Q
      </strong>
      乘以
      <strong>
       K
      </strong>
      的转置后，得到的矩阵行列数都为 n，n 为句子单词数，这个矩阵可以表示单词之间的 attention 强度
     </img>
    </p>
    <p>
     （
     <strong>
      换一话来说 对图像而言 是不是patch 对视频而言 是不是帧
     </strong>
     ）！！！
    </p>
    <p>
     <img alt="" height="228" src="https://i-blog.csdnimg.cn/direct/671ba6f849054f0e810bbb2ac9f78038.png" width="640"/>
    </p>
    <p>
     <img alt="QK^{T}" src="https://latex.csdn.net/eq?QK%5E%7BT%7D">
      之后，使用 Softmax 计算每一个单词对于其他单词的 attention 系数，公式中的 Softmax 是对矩阵的每一行进行 Softmax，即每一行的和都变为 1.
     </img>
    </p>
    <p>
     <img alt="" height="247" src="https://i-blog.csdnimg.cn/direct/690a71310851405ca4a7d3139ba0c2aa.png" width="640"/>
    </p>
    <p>
     得到 Softmax 矩阵之后可以和
     <strong>
      V
     </strong>
     相乘，得到最终的输出
     <strong>
      Z
     </strong>
     。
    </p>
    <p>
     <img alt="" height="216" src="https://i-blog.csdnimg.cn/direct/03653284c7334acea5ebcb7720b8323b.png" width="640"/>
    </p>
    <p>
     就拿结果而言我们是不是得到了一个带有位置信息 和对所有样本有注意力系数的
     <strong>
      词向量 (可以一个patch 也可以是一个帧)
     </strong>
    </p>
    <p>
     <strong>
      多头注意力
     </strong>
    </p>
    <p>
     在上一步，我们已经知道怎么通过 Self-Attention 计算得到输出矩阵 Z，而 Multi-Head Attention 是由多个 Self-Attention 组合形成的，下图是论文中 Multi-Head Attention 的结构图。
    </p>
    <p class="img-center">
     <img alt="" height="859" src="https://i-blog.csdnimg.cn/direct/fbea9d9c14304d089ce11b239aebe4d5.png" width="640"/>
    </p>
    <p>
     从上图可以看到 Multi-Head Attention 包含多个 Self-Attention 层，首先将输入
     <strong>
      X
     </strong>
     分别传递到 h 个不同的 Self-Attention 中，计算得到 h 个输出矩阵
     <strong>
      Z
     </strong>
     。下图是 h=8 时候的情况，此时会得到 8 个输出矩阵
    </p>
    <p class="img-center">
     <img alt="" height="831" src="https://i-blog.csdnimg.cn/direct/62eea692b6b447e0b15835fea3f46685.png" width="640"/>
    </p>
    <p>
     得到 8 个输出矩阵 Z1 到 Z8 之后，Multi-Head Attention 将它们拼接在一起
     <strong>
      (Concat)
     </strong>
     ，然后传入一个
     <strong>
      Linear
     </strong>
     层，得到 Multi-Head Attent
    </p>
    <p class="img-center">
     <img alt="" height="388" src="https://i-blog.csdnimg.cn/direct/1dbc38ef9c5e4a2789ac3785ab1b0539.png" width="640"/>
    </p>
    <p>
     最终的输出
     <strong>
      Z
     </strong>
    </p>
    <p>
     <strong>
      掩码注意力就是看不见未来的情况
     </strong>
    </p>
    <p>
     <strong>
      参考这一篇
      <a class="link-info" href="https://blog.csdn.net/m0_73359068/article/details/145966836?spm=1001.2014.3001.5501" title="transform">
       transform
      </a>
     </strong>
    </p>
    <h3>
     <strong>
      代码实现
     </strong>
    </h3>
    <p>
     <strong>
      调用api
     </strong>
    </p>
    <pre><code class="language-python">nn.MultiheadAttention(d_model, n_head)

 def attention(self, x: torch.Tensor):
        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]

</code></pre>
    <h2>
     2.残差注意力机制(图像识别用的多)
    </h2>
    <h3>
     工作过程
    </h3>
    <ul>
     <li>
      <p>
       <strong>
        特征提取
       </strong>
       ：首先，输入数据经过一系列的卷积层和池化层等操作进行特征提取，得到初始的特征图。
      </p>
     </li>
     <li>
      <p>
       <strong>
        注意力计算
       </strong>
       ：将初始特征图输入到注意力模块中，通过一系列的计算生成注意力掩码。这个过程通常包括对特征图进行卷积操作、激活函数处理以及归一化等步骤，以得到每个位置的注意力权重。
      </p>
     </li>
     <li>
      <p>
       <strong>
        特征加权
       </strong>
       ：将注意力掩码与初始特征图相乘，使得特征图中重要的部分得到增强，不重要的部分得到抑制。
      </p>
     </li>
     <li>
      <p>
       <strong>
        残差融合
       </strong>
       ：将经过特征加权后的特征图与原始输入特征图通过残差连接相加，得到最终的输出特征图。这个输出特征图既包含了经过注意力机制筛选后的重要特征，又保留了原始输入中的信息，从而提高了特征的质量和模型的性能。
      </p>
     </li>
    </ul>
    <h3>
     代码实现
    </h3>
    <pre><code class="language-python">class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()

        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = nn.LayerNorm(d_model)
        self.attn_mask = attn_mask

    def attention(self, x: torch.Tensor):
        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]

    def forward(self, x: torch.Tensor):
        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x
</code></pre>
    <h2>
     3.跨帧注意力块（视频用的多）
    </h2>
    <p class="img-center">
     <img alt="" height="536" src="https://i-blog.csdnimg.cn/direct/10868dada2b04096acaf48fddb43c317.png" width="576"/>
    </p>
    <h4>
     <strong>
      跨帧融合注意力（CFA）
     </strong>
    </h4>
    <p>
     CFA 的作用是允许帧之间直接交换信息。具体来说：
    </p>
    <ul>
     <li>
      <p>
       每个帧都有一个
       <strong>
        消息令牌（message token）
       </strong>
       ，用于抽象当前帧的视觉信息。
      </p>
     </li>
     <li>
      <p>
       这些消息令牌会参与到全局的时间依赖性建模中。
      </p>
     </li>
     <li>
      <p>
       消息令牌的计算方式如下：
      </p>
     </li>
    </ul>
    <p class="img-center">
     <img alt="m_t^{(l)} = W_m \cdot z_t^{(l-1), [class]}" src="https://latex.csdn.net/eq?m_t%5E%7B%28l%29%7D%20%3D%20W_m%20%5Ccdot%20z_t%5E%7B%28l-1%29%2C%20%5Bclass%5D%7D"/>
    </p>
    <p>
     <img alt="" src="https://latex.csdn.net/eq?"/>
    </p>
    <p>
     <img alt="m_t^{(l)}" src="https://latex.csdn.net/eq?m_t%5E%7B%28l%29%7D"/>
     为t帧在l层的消息令牌 w是一个线性矩阵，
     <img alt="z_t^{(l-1), [class]}" src="https://latex.csdn.net/eq?z_t%5E%7B%28l-1%29%2C%20%5Bclass%5D%7D"/>
     是前一层的特征输出。
    </p>
    <p>
     所有帧的消息令牌会参与到跨帧融合注意力中
    </p>
    <p class="img-center">
     <img alt="\hat{M}^{(l)} = M^{(l)} + \text{CFA}(\text{LN}(M^{(l)}))" src="https://latex.csdn.net/eq?%5Chat%7BM%7D%5E%7B%28l%29%7D%20%3D%20M%5E%7B%28l%29%7D%20&amp;plus;%20%5Ctext%7BCFA%7D%28%5Ctext%7BLN%7D%28M%5E%7B%28l%29%7D%29%29"/>
    </p>
    <p>
     CFA 是一个自注意力模块，它允许消息令牌之间相互作用，从而捕捉帧之间的时间依赖性
    </p>
    <h4>
     帧内融合注意力（IFA）
    </h4>
    <p>
     IFA 的作用是在帧内传播全局的时间信息，增强帧的表示能力
    </p>
    <p class="img-center">
     <img alt="[\hat{z}_t^{(l)}, \bar{m}_t^{(l)}] = [z_t^{(l-1)}, \hat{m}_t^{(l)}] + \text{IFA}(\text{LN}([z_t^{(l-1)}, \hat{m}_t^{(l)}]))" src="https://latex.csdn.net/eq?%5B%5Chat%7Bz%7D_t%5E%7B%28l%29%7D%2C%20%5Cbar%7Bm%7D_t%5E%7B%28l%29%7D%5D%20%3D%20%5Bz_t%5E%7B%28l-1%29%7D%2C%20%5Chat%7Bm%7D_t%5E%7B%28l%29%7D%5D%20&amp;plus;%20%5Ctext%7BIFA%7D%28%5Ctext%7BLN%7D%28%5Bz_t%5E%7B%28l-1%29%7D%2C%20%5Chat%7Bm%7D_t%5E%7B%28l%29%7D%5D%29%29"/>
    </p>
    <ul>
     <li>
      <p>
       其中，
       <img alt="\hat{z}_t^{(l)}" src="https://latex.csdn.net/eq?%5Chat%7Bz%7D_t%5E%7B%28l%29%7D"/>
       是帧特征的更新表示，
       <em>
        m
       </em>
       ˉ
       <em>
        t
       </em>
       (
       <em>
        l
       </em>
       )​ 是消息令牌的更新表示。
      </p>
     </li>
     <li>
      <p>
       消息令牌在每个块中仅用于信息交换，不会传递到下一个块。
      </p>
     </li>
    </ul>
    <p>
     <strong>
      CFA
     </strong>
    </p>
    <p>
     首先一个数据由(b,t,c.h,w)-&gt;(bt,c,h,w)-&gt;(wxh,bt,d)（你得了解这个视频处理）
    </p>
    <pre><code class="language-python"> l, bt, d = x.size()</code></pre>
    <p>
     然后为了得到时间 一个知识点 三维是图像块输入 四维是帧输入
    </p>
    <pre><code class="language-python">b = bt // self.T
x = x.view(l, b, self.T, d) </code></pre>
    <p>
     初始化消息令牌（vit的csl）
    </p>
    <pre><code class="language-python">
msg_token = self.message_fc(x[0,:,:,:]) # # 使用线性变换初始化消息令牌 使用线性变换初始化消#息令牌 这个代表帧在不同批次和步长
msg_token = msg_token.view(b, self.T, 1, d) # 调整形状为 (b, T, 1, d)
msg_token = msg_token.permute(1,2,0,3).view(self.T, b, d)</code></pre>
    <p>
     实现跨帧传输（t不同无疑就是 不同帧）
    </p>
    <pre><code class="language-python">msg_token = msg_token + self.drop_path(self.message_attn(self.message_ln(msg_token),self.message_ln(msg_token),self.message_ln(msg_token),need_weights=False)[0])</code></pre>
    <p>
     <strong>
      IFA：帧内
     </strong>
    </p>
    <pre><code class="language-python"> x = x.view(l+1, -1, d)
        x = x + self.drop_path(self.attention(self.ln_1(x)))
        x = x[:l,:,:]</code></pre>
    <h2>
     4.补充一个知识点
    </h2>
    <pre><code class="language-python">MultiHeadAttention</code></pre>
    <p>
     传入一个输入的多头注意力机制：这种形式可以让模型关注序列内部的上下文信息，捕捉序列中不同位置之间的依赖关系。例如在文本处理中，一个句子中的每个词都可以通过自注意力机制与其他词进行交互，从而更好地理解整个句子的语义。
    </p>
    <p>
     传入三个输入的多头注意力机制：形式允许模型在不同的表示子空间中并行地计算注意力，从而捕捉输入序列之间的不同类型的依赖关系。通过分别传入 Q、K、V，模型可以灵活地根据查询去关注键值对中的不同部分，提高模型的表达能力。
    </p>
    <p>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37333335393036382f:61727469636c652f64657461696c732f313436323430303131" class_="artid" style="display:none">
 </p>
</div>


