---
layout: post
title: "ES-分词器安装与使用详解"
date: 2025-03-09 20:14:07 +0800
description: "1"
keywords: "ES-分词器安装与使用详解"
categories: ['Elasticsearch']
tags: ['Elasticsearch']
artid: "146136756"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146136756
    alt: "ES-分词器安装与使用详解"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146136756
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146136756
cover: https://bing.ee123.net/img/rand?artid=146136756
image: https://bing.ee123.net/img/rand?artid=146136756
img: https://bing.ee123.net/img/rand?artid=146136756
---

# ES-分词器安装与使用详解

##

## 安装分词器

> windows环境，分词器有2种安装方式，1.直接命令安装；2.压缩包安装

### IK分词器

查看ik分词器文档，找到安装方式介绍

![](https://i-blog.csdnimg.cn/direct/f729c59e16cf4cdb8f4256dea0c61171.png)

文档链接：

#### 方式1

    
    
    elasticsearch-plugin install https://get.infini.cloud/elasticsearch/analysis-ik/7.10.0

#### 方式2

下载压缩包，然后解压放入到 es的plugins目录

![](https://i-blog.csdnimg.cn/direct/bb238b6d0c5f434eab37cfa5f2d6f96b.png)

找到es对应的版本，然后下载

## 验证是否安装成功

    
    
    elasticsearch-plugin list
    

![](https://i-blog.csdnimg.cn/direct/c6b7246309004e459d2125fdee67f43c.png)

## 测试分词器

需要先重启ES

    
    
    POST /_analyze   
    {
      "analyzer": "ik_max_word",
      "text": "中华人民"   
    }   
    

结果

    
    
    {
      "tokens" : [
        {
          "token" : "中华人民",
          "start_offset" : 0,
          "end_offset" : 4,
          "type" : "CN_WORD",
          "position" : 0
        },
        {
          "token" : "中华",
          "start_offset" : 0,
          "end_offset" : 2,
          "type" : "CN_WORD",
          "position" : 1
        },
        {
          "token" : "华人",
          "start_offset" : 1,
          "end_offset" : 3,
          "type" : "CN_WORD",
          "position" : 2
        },
        {
          "token" : "人民",
          "start_offset" : 2,
          "end_offset" : 4,
          "type" : "CN_WORD",
          "position" : 3
        }
      ]
    }
    

## 分词器的组成

分词器（Analyzer）是用于将文本拆分为词项（Token）的工具。分词器由以下三个部分组成：

  * **字符过滤器（Character Filters）** ：对原始文本进行预处理（如去除 HTML 标签、替换字符等）。
  * **分词器（Tokenizer）** ：将文本拆分为词项。
  * **词项过滤器（Token Filters）** ：对分词后的词项进行处理（如小写转换、去除停用词等）

## 分词器种类

**分词器**| **特点**| **适用场景**| **优点**| **缺点**| **示例**  
---|---|---|---|---|---  
**Standard Analyzer**|  默认分词器，基于 Unicode 文本分割算法，按空格和标点符号分词，转换为小写。|
英文或其他基于空格分隔的语言| 简单易用，无需额外配置。| 不支持中文分词，对特殊字符敏感。| `"Hello, world!"` → `["hello",
"world"]`  
english **Analyzer**|  转换成小写，词干提取、停用词过滤| | | | "Barking a games"->["bark","game"]  
**Simple Analyzer**|  按非字母字符分词，转换为小写。| 简单的英文分词| 轻量级，适合简单场景。| 无法处理复杂文本，不支持中文。|
`"Hello, world!"` → `["hello", "world"]`  
**Whitespace Analyzer**|  按空格分词，不转换大小写。| 需要保留大小写的场景| 保留原始大小写，适合特定需求。|
无法处理标点符号，不支持中文。| `"Hello, world!"` → `["Hello,", "world!"]`  
**Keyword Analyzer**|  将整个文本作为一个词项，不进行分词。| 需要精确匹配的场景（如 ID、标签）| 保留完整文本，适合精确匹配。|
不适合全文搜索。| `"Hello, world!"` → `["Hello, world!"]`  
**Pattern Analyzer**|  基于正则表达式分词，默认按非字母字符分词，转换为小写。| 需要自定义分词规则的场景|
灵活，支持自定义正则表达式。| 配置复杂，性能较低。| `"Hello, world!"` → `["hello", "world"]`  
**Language Analyzer**|  针对特定语言（如英语、法语、德语）优化分词。| 多语言支持| 针对特定语言优化，分词效果较好。|
需要指定语言，不支持中文。| `"Hello, world!"` → `["hello", "world"]`  
**IK Analyzer**|  支持中文分词，提供 `ik_smart`（智能分词）和 `ik_max_word`（最大分词）两种模式。|
中文文本处理| 中文分词效果好，支持自定义词典。| 需要额外安装插件，重启 Elasticsearch。| `"你好世界"` → `["你好",
"世界"]`（`ik_smart`）或 `["你好", "世界", "你好世界"]`（`ik_max_word`）  
**Jieba Analyzer**|  支持中文分词，提供 `search`（搜索模式）和 `index`（索引模式）两种模式。| 中文文本处理|
中文分词效果好，支持自定义词典。| 需要额外安装插件，重启 Elasticsearch。| `"你好世界"` → `["你好",
"世界"]`（`search`）或 `["你好", "世界", "你好世界"]`（`index`）  
**Nori Analyzer**|  针对韩语优化的分词器。| 韩语文本处理| 韩语分词效果好，支持自定义词典。| 需要额外安装插件，重启
Elasticsearch。| `"안녕하세요"` → `["안녕", "하세요"]`  
**Kuromoji Analyzer**|  针对日语优化的分词器。| 日语文本处理| 日语分词效果好，支持自定义词典。| 需要额外安装插件，重启
Elasticsearch。| `"こんにちは"` → `["こんにちは"]`  
**Stempel Analyzer**|  针对波兰语优化的分词器。| 波兰语文本处理| 波兰语分词效果好，支持自定义词典。| 需要额外安装插件，重启
Elasticsearch。| `"Witaj świecie"` → `["witaj", "świecie"]`



