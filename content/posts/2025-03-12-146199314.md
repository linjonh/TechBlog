---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f4d6167696346726f6d4d652f:61727469636c652f64657461696c732f313436313939333134"
layout: post
title: "MIFNet-论文阅读笔记"
date: 2025-03-12 16:27:36 +0800
description: "遥感图像复杂的三维结构，遮挡、阴影、类内不一致性和类间模糊性等干扰因素会影响分割性能。语义分割对频率信息高度敏感，因此我们引入了频率信息，是模型能够从多个维度更全面地学习不同类别的目标特征。我们设计了一个信息融合Transformer模块，该模块能够自适应地关联局部特征、全局语义信息和频率信息，同时设计了一个相关语义聚合模块，用于聚合不同尺度的特征以构建解码器。"
keywords: "MIFNet （论文阅读笔记）"
categories: ['未分类']
tags: ['论文阅读', '笔记']
artid: "146199314"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146199314
    alt: "MIFNet-论文阅读笔记"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146199314
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146199314
cover: https://bing.ee123.net/img/rand?artid=146199314
image: https://bing.ee123.net/img/rand?artid=146199314
img: https://bing.ee123.net/img/rand?artid=146199314
---

# MIFNet （论文阅读笔记）

Frequency-aware robust multidimensional information fusion framework for remote sensing image segmentation
  
用于遥感图像分割的频率感知鲁棒多维信息融合框架
  
Junyu Fan a, Jinjiang Li b, Yepeng Liu b, Fan Zhang b
  
[论文地址](https://www.sciencedirect.com/science/article/pii/S0952197623018225)
  
[代码地址](https://github.com/JunyuFan/MIFNet)

## 1. 摘要

遥感图像复杂的三维结构，遮挡、阴影、类内不一致性和类间模糊性等干扰因素会影响分割性能。语义分割对频率信息高度敏感，因此我们引入了频率信息，是模型能够从多个维度更全面地学习不同类别的目标特征。我们设计了一个信息融合Transformer模块，该模块能够自适应地关联局部特征、全局语义信息和频率信息，同时设计了一个相关语义聚合模块，用于聚合不同尺度的特征以构建解码器。通过聚合不同深度的图像特征，可以在多个维度上对目标的具体表示以及目标之间的相关性进行建模，从而使网络能够更好地识别和理解每个类别目标的特征，以抵抗影响分割性能的各种干扰因素。

## 2. 引言

**频率信息**
可以提供图像的全局结构特征，包括纹理、边缘和轮廓。此外，
**语义分割任务对频率信息高度敏感**
，提取更多的频率信息可以增强类别间的区分度，从而减轻各种干扰因素对模型的影响。因此，我们尝试引入额外的频率信息，
**旨在使模型能够从多个维度更全面地学习不同类别目标的信息**
。这种方法旨在让模型从不同角度分析目标特征，从而降低因干扰导致类别识别混淆的风险。MIFNet利用全局信息、频率信息和局部特征，以交互方式在多个维度上对目标的语义特征进行建模。该模型结合了UNet架构和Transformer架构，使用CNN骨干网络作为编码器进行特征提取，并设计了一个基于Transformer的解码器，其中包含我们设计的信息融合Transformer模块（IFTM）。IFTM模块在多个维度上对目标特征进行建模，使模型能够实现鲁棒性。IFTM模块不仅利用自注意力机制和卷积块注意力模块（CBAM）来关联全局信息和局部特征，还引入了自适应滤波器以学习频率信息。自适应滤波器捕获不同的频率，并相应地对不同目标的特征进行建模。在跳跃连接中，我们采用了相关语义聚合模块（RSAM）来融合模型中不同深度的特征，减少因下采样导致的空间信息损失。

## 3. 网络结构

![请添加图片描述](https://i-blog.csdnimg.cn/direct/318346743f7b445a999d21b36aa65551.png#pic_center)
  
上面是网络的模型图，我们可以发现，它也是编码器-解码器的结构，并且它的Backbone就是使用CNN进行下采样，将图像分解为多尺度，解码器结合了Transformer结构，利用大感受野建模长距离依赖关系，同时还有跳连接，补偿因连续下采样导致的空间信息损失。为什么采用了U-Net的结构？我们知道Transformer计算复杂度高的原因跟输入图像的分辨率有关，分辨率太高了那么计算的复杂度一下就提起来了， 所以这里将Transformer应用于网络的深层部分，以在更小尺度的图像特征上计算自注意力机制。
  
“网络的整体结构如图2所示。我们设计了一个信息融合
**Transformer模块（IFTM）**
，用于自适应地关注图像的高频和低频语义信息以及空间信息。在跳跃连接过程中，\*\*相关语义聚合模块(RSAM)\*\*用于将不同尺度的特征信息与IFTM模块中的全局语义信息进行融合，同时补偿因下采样导致的空间信息损失。”（论文中的原话）
  
下面是MIFNet的算法流程：
  



I
n
p
u
t
:
R
e
m
o
t
e
 
s
e
s
i
n
g
i
m
a
g
e
 
X
,
L
a
b
e
l
 
Y
Input: Remote \ sesing image \ X, Label \ Y





I

n

p

u

t



:





R

e

m

o

t

e



ses

in

g

ima

g

e



X

,



L

ab

e

l



Y
  



O
u
t
p
u
t
:
S
e
g
m
e
n
t
e
d
 
i
m
a
g
e
 
S
Output: Segmented\ image\ S





O

u

tp

u

t



:





S

e

g

m

e

n

t

e

d



ima

g

e



S
  



1
:
用于特征提取的编码器
:
1:用于特征提取的编码器:





1



:





用于特征提取的编码器



:
  



f
=
b
a
c
k
b
o
n
e
(
X
)
\ \ \ \ \ \ f = backbone(X)

















f



=





ba

c

kb

o

n

e

(

X

)
  



2
:
通过计算注意力机制以及高低频滤波，获取全局语义信息并建模目标之间的关系：
2:通过计算注意力机制以及高低频滤波，获取全局语义信息并建模目标之间的关系：





2



:





通过计算注意力机制以及高低频滤波，获取全局语义信息并建模目标之间的关系：
  



a
,
k
,
v
=
f
\ \ \ \ \ \ a, k, v = f

















a

,



k

,



v



=





f
  



a
t
t
=
s
o
f
t
m
a
x
(
q
k
T
d
k
+
b
)
\ \ \ \ \ \ att = softmax(\frac{qk^T}{\sqrt{d\_k}}+b)

















a

tt



=





so

f

t

ma

x

(






















d









k

​


​













q


k









T

​




+





b

)
  



h
f
=
C
a
t
(
C
o
n
v
k
i
×
k
i
(
v
i
)
)
×
k
,
  
i
=
1
,
2
,
.
.
.
.
n
\ \ \ \ \ \ hf = Cat(Conv\_{k\_i \times k\_i}(v\_i)) \times k, \ \ i = 1, 2,....n

















h

f



=





C

a

t

(

C

o

n


v











k









i

​


×


k









i

​


​


(


v









i

​


))



×





k

,







i



=





1

,



2

,



....

n
  



l
f
=
C
a
t
(
u
p
(
a
v
g
p
o
o
l
k
i
×
k
i
(
v
i
)
)
)
,
  
i
=
1
,
2
,
.
.
.
,
n
\ \ \ \ \ \ lf = Cat(up(avgpool\_{k\_i \times k\_i}(v\_i))), \ \ i = 1, 2, ..., n

















l

f



=





C

a

t

(

u

p

(

a

vg

p

oo


l











k









i

​


×


k









i

​


​


(


v









i

​


)))

,







i



=





1

,



2

,



...

,



n
  



f
g
=
a
t
t
×
(
h
f
+
l
f
)
\ \ \ \ \ \ f\_g = att \times (hf + lf)


















f









g

​




=





a

tt



×





(

h

f



+





l

f

)
  



3
:
 将获取的全局特征映射与局部特征进行融合
3:\ 将获取的全局特征映射与局部特征进行融合





3



:





将获取的全局特征映射与局部特征进行融合
  



f
l
=
(
f
×
C
A
M
(
f
)
)
×
S
A
M
(
f
×
C
A
M
(
f
)
)
\ \ \ \ \ \ f\_l = (f \times CAM(f)) \times SAM(f\times CAM(f))


















f









l

​




=





(

f



×





C

A

M

(

f

))



×





S

A

M

(

f



×





C

A

M

(

f

))
  



F
=
f
g
+
f
l
\ \ \ \ \ \ F = f\_g + f\_l

















F



=






f









g

​




+






f









l

​

  



4
:
 与具有更丰富空间信息的浅层特征进行聚合：
4:\ 与具有更丰富空间信息的浅层特征进行聚合：





4



:





与具有更丰富空间信息的浅层特征进行聚合：
  



M
=
f
c
(
g
a
p
(
f
)
)
×
(
f
c
(
g
a
p
(
F
)
)
)
T
\ \ \ \ \ \ M = fc(gap(f)) \times (fc(gap(F)))^T

















M



=





f

c

(

g

a

p

(

f

))



×





(

f

c

(

g

a

p

(

F

))


)









T
  



F
s
=
f
l
a
t
t
e
n
(
M
)
×
f
+
u
p
(
f
l
a
t
t
e
n
(
M
)
×
F
)
\ \ \ \ \ \ F\_s = flatten(M) \times f + up(flatten(M) \times F)


















F









s

​




=





f

l

a

tt

e

n

(

M

)



×





f



+





u

p

(

f

l

a

tt

e

n

(

M

)



×





F

)
  



5
:
 最后解码器推断出分割后的图像：
5: \ 最后解码器推断出分割后的图像：





5



:





最后解码器推断出分割后的图像：
  



S
=
s
e
g
h
e
a
d
(
F
s
)
\ \ \ \ \ \ S = seghead(F\_s)

















S



=





se

g

h

e

a

d

(


F









s

​


)
  
我们先来看看它的每个部分的组成作用，再结合算法流程和结构图进行分析。

### 3.1 CNN-based encoder

这里的编码器不是简单的卷积下采样，这里使用的是ResNeXt，ResNeXt的每个模块对特征图进行下采样，生成多个不同尺度的特征图。下采样从1/4到1/32，输出特征，通过

1
×
1
1 \times 1





1



×





1
的卷积调整其维度以匹配解码器的输入维度。由最深层的ResNeXt模块生成的特征直接进入IFM模块一学习语义特征，然后通过不断聚合较浅层ResNeXt模块生成的特征图来建立目标之间的关系信息。

### 3.2 Information fusion transformer module（IFM） 请添加图片描述

在IFM模块中，包含了两个分支，一个用于提取局部特征，另一个用于建模全局语义信息。在建模全局语义信息分支中，将自适应滤波器与多头窗口注意力机制结合使用，自注意力机制可以学习全局信息并建模长距离依赖关系，而自适应滤波器则直接从空间域中捕获重要的高频和低频特征。提取局部特征的分支就是使用混合注意力机制，通过通道注意力机制和空间注意力机制的结合来得到特征图。我们接着来看全局语义信息提取的分支，它将输入特征划分为了q、k、v，这里和transformer的自注意力机制一样，会有相关的权重

W
q
,
W
k
,
W
v
W\_q, W\_k, W\_v






W









q

​


,




W









k

​


,




W









v

​

不断地学习和调整，q主要用于计算注意力，v则是通过自适应滤波器进行处理，以获得自适应调整的特征表示。k既用于自注意力的计算，也用于自适应滤波器中获取特征的计算。这使得自注意力中的k与自适应滤波器之间能够进行信息交互和调整，从而更好地适应特征表示，最后，通过可学习的权重对多维度特征进行聚合，使模型能够在多个维度上对目标进行建模。（上面是论文中的内容，一脸懵逼，继续往后面看）

#### 3.2.1 Local feature extraction branch

这一分支就是使用的CBAM来提取局部特征，也就是通道注意力机制和空间注意力机制的结合 ，不过多的解释，之前提过。

#### 3.2.2 Adaptive filter（自适应滤波器）

自适应滤波执行高通和低通滤波。高频信息指的是图像中的细节和纹理，这些事相对于较大结构快速变化的区域。高频细节包括对目标边缘和细节的描述，提供了更准确的特征以区分不同的类别。在遥感图像中，高频信息通常由地物的细节、阴影、反射、光照和纹理等因素引起。上面是论文中的内容，为了模拟不同图像的不同高通截止频率，我们使用具有不同卷积核的卷积层，对于第n组：

D
h
f
n
(
v
n
)
=
C
k
×
k
(
v
n
)
D\_{hf}^n(v\_n)=C\_{k \times k}(v\_n)






D










h

f





n

​


(


v









n

​


)



=






C










k

×

k

​


(


v









n

​


)
，

C
C





C
是一个深度卷积层，卷积核大小为

k
×
k
k \times k





k



×





k
。
  
低频信息包括图像的整体亮度和颜色等特征，这些通常反映图像的全局特性和结构，在遥感图像中，大多数语义信息由低频成分表示，这些成分可以提供有用的信息并帮助更好地理解图像的内容和边界。我们使用平均池化作为低频滤波器，为了适应不同图像的不同低通截止频率，我们使用多组不同的卷积核生成动态低通滤波器。对于第m组卷积核：

D
l
f
m
(
v
m
)
=
u
p
(
P
a
v
g
(
v
m
)
)
D\_{lf}^m(v\_m)= up(P\_{avg}(v\_m))






D










l

f





m

​


(


v









m

​


)



=





u

p

(


P










a

vg

​


(


v









m

​


))
，up是双线性插值上采样，

p
a
v
g
p\_{avg}






p










a

vg

​

表示自适应平均池化。
  
为什么要使用自适应滤波器？我们先来看看什么是滤波器，滤波器的功能是允许某一部分频率的信号顺利通过，而另外一部分频率的信号受到较大的抑制难以通过。实际上可以看作是一个选频电路。其实就是做一个筛选功能，对于语义分割来说，它其实就是像素的分类，对于分割的结果，它受比如光照、阴影等这些因素影响，那么我们使用自适应滤波器就是来做一个筛选的功能，论文中说到，高频信息指的是图像的中细节和纹理，这些事相对于较大结构快速变化的区域；低频信息包括图像的整体亮度和颜色等特征，这些通常反映图像的全局特性和结构。说白了，就是全局信息和局部信息的一个区别，对于不同的信息做了不同的处理，然后通过不同的卷积核来模拟对不同频率信息的处理。
  
![请添加图片描述](https://i-blog.csdnimg.cn/direct/ca03634861db44c88719d252c0ef2d0c.png#pic_center)
  
上图是对不同频率信息进行处理的结构图，v指的是值，用来计算权重的，我们先忽略这个v的含义，我们来看看它做了哪些操作，图中说了“Split into n groups”，将v分成了n组，这里其实跟多头自注意力机制类似，我们先看上半部分对高频信息的处理，我们将特征图用不同大小的卷积核进行操作，然后将卷积后的特征图进行拼接；下半部分是对低通频率的处理，这里进行了平均池化操作然后再上采样，再将每组给拼接起来，再与高通频率处理后的特征图进行拼接，最后输入特征图。这里的k是什么意思？key，通过对输入特征进行线性变换得到的，就是Transformer中的k，论文中就是有输入特征通过学习的权重矩阵

W
k
W\_k






W









k

​

映射得到的：

k
=
X
W
k
k=XW\_k





k



=





X


W









k

​

。拼接后的特征经过与k的乘法操作（矩阵的乘法），目的是通过key向量对拼接特征进行加权或调整。

#### 3.2.3 Window attention

采用窗口注意力机制是，如果直接对高分辨率的遥感图像进行计算，会消耗大量资源，所以将图像进行窗口划分，均匀分割成不重叠的窗口，以便在每个窗口进行自注意力计算，那么窗口之间这么交互？在执行窗口划分操作后，我们调整池化核的大小，对水平和垂直方向的窗口进行平均池化，从而生成水平和垂直特征图。通过这些特征图，模型可以在不同窗口之间建立关系。窗口内的自注意力计算：

a
t
t
=
s
o
f
t
m
a
x
(
Q
K
T
d
k
+
B
)
att=softmax(\frac{QK^T}{\sqrt{d\_k}}+B)





a

tt



=





so

f

t

ma

x

(






















d









k

​


​













Q


K









T

​




+





B

)
，B是偏置。通过平均池化生成的映射：

M
h
=
P
a
v
g
(
1
,
w
s
)
(
a
t
t
)
，
M
v
=
P
a
v
g
(
w
s
,
1
)
(
a
t
t
)
M\_h=P\_{avg}^{(1,ws)}(att)，M\_v=P\_{avg}^{(ws,1)}(att)






M









h

​




=






P










a

vg






(

1

,

w

s

)

​


(

a

tt

)

，


M









v

​




=






P










a

vg






(

w

s

,

1

)

​


(

a

tt

)
，

M
h
,
M
v
M\_h, M\_v






M









h

​


,




M









v

​

分别是水平和垂直方向的池化特征图，

w
s
ws





w

s
是窗口的大小。

### 3.3 Related semantic aggregation module（相关语义聚合模块-RSAM）

![请添加图片描述](https://i-blog.csdnimg.cn/direct/6244da45d2814983947c7ecd869b48b1.png#pic_center)
  
这一部分用于聚合从编码器不同尺度采样的特征以及由IFM模块输出的特征。较浅层的特征通常保留了详细的空间信息，而较深层的IFM模块则解析并学习高级语义信息。在RSAM模块中，全局平均池化（GAP）使来自不同尺度的特征能够在相同维度上进行计算。然后通过相关矩阵获取特征图，该矩阵捕捉不同尺度特征之间的关系。生成的相关特征图与来自不同尺度的特征图进行融合，以获得最终的相关语义聚合特征。RSAM模块能够更好地聚合不同深度特征图与目标之间的对应关系，从而获得更丰富的特征信息。
  
设

F
h
F\_h






F









h

​

和

F
l
F\_l






F









l

​

分别表示浅层的高尺度特征图和深度的低尺度特征图，特征图的计算如下：

v
=
f
c
(
G
A
P
(
F
)
)
v=f\_c(GAP(F))





v



=






f









c

​


(

G

A

P

(

F

))
，因此，

F
h
F\_h






F









h

​

和

F
l
F\_l






F









l

​

的特征映射可以表示为

v
h
v\_h






v









h

​

和

v
l
v\_l






v









l

​

，进行矩阵运算，可以获得

F
h
F\_h






F









h

​

和

F
l
F\_l






F









l

​

的相关矩阵（M）：

M
=
v
h
v
l
T
M=v\_hv\_l^T





M



=






v









h

​



v









l





T

​

，将M展平后，得到关联向量

v
m
v\_m






v









m

​

，然后将

v
m
v\_m






v









m

​

与

F
h
F\_h






F









h

​

和

F
l
F\_l






F









l

​

融合：

R
=
(
v
n
+
v
)
⨀
F
R=(v\_n+v)\bigodot F





R



=





(


v









n

​




+





v

)



⨀



F
，

⨀
\bigodot





⨀
表示点乘，

R
h
,
R
l
R\_h, R\_l






R









h

​


,




R









l

​

是

F
h
,
F
l
F\_h, F\_l






F









h

​


,




F









l

​

在与

v
m
v\_m






v









m

​

融合后的特征。聚合特征计算：

F
a
=
R
h
+
u
p
(
R
l
)
F\_a=R\_h+up(R\_l)






F









a

​




=






R









h

​




+





u

p

(


R









l

​


)
。

现在再结合网络结构图来看，首先对输入的图像进行下采样提取特征，结束后将特征图输入到IFTM中，这个结构其实就是将注意力机制换成了IFM，而IFM是将输入的图像分为两个分支，一个用于提取全局信息，采用自适应滤波器和窗口注意力机制，一个用于提取局部信息，采用CBAM，通过多层感知机，输出处理完之后的特征图，此时输出的x分为两部分，一部分包含的是局部信息

x
l
x^l






x









l
和包含全局信息的

x
h
x^h






x









h
，将二者输入到RSAM中，这里是将二者融合，在这里面，都进行全局平局池化以及多层感知机处理，然后通过相关矩阵（注意力分数矩阵）获取特征图，然后融合，获得特征图。其实我们看上面的算法流程图更清晰，它将两个尺度的特征图进行展平了 ，然后是两个残杀连接，然后在进行融合，获得输出。

以上便是全过程了。
  
（服务器崩了，还做不了实验）