---
layout: post
title: "大数据-spark-hive-总结"
date: 2025-03-10 18:42:48 +0800
description: "RDD 是 Spark 的最底层抽象，表示分布在集群节点上的不可变、可分区的数据集合。它提供。"
keywords: "大数据 spark hive 总结"
categories: ['大数据']
tags: ['大数据', 'Spark', 'Hive']
artid: "146155480"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146155480
    alt: "大数据-spark-hive-总结"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146155480
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146155480
cover: https://bing.ee123.net/img/rand?artid=146155480
image: https://bing.ee123.net/img/rand?artid=146155480
img: https://bing.ee123.net/img/rand?artid=146155480
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     大数据 spark hive 总结
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     Apache Spark
    </h2>
    <h3>
     简介
    </h3>
    <p>
     是一个开源的统一分析引擎，专为大规模数据处理而设计。它提供了高级API，支持Java、Scala、Python和R语言，并且包含了一个优化过的执行引擎，该引擎支持循环计算（如机器学习算法）和交互式查询。以下是Spark的一些关键特性和概念
    </p>
    <h3>
     核心特性
    </h3>
    <ol>
     <li>
      <p>
       <strong>
        速度
       </strong>
       ：Spark通过内存计算提高了数据处理的速度，比Hadoop MapReduce快达10到100倍。
      </p>
     </li>
     <li>
      <p>
       <strong>
        易用性
       </strong>
       ：提供丰富的高层次API，包括DataFrame和Dataset API，简化了数据操作。
      </p>
     </li>
     <li>
      <p>
       <strong>
        通用性
       </strong>
       ：除了Map和Reduce操作之外，还支持SQL查询、流处理、机器学习和图计算等多种工作负载。
      </p>
     </li>
     <li>
      <p>
       <strong>
        可扩展性
       </strong>
       ：能够有效地在数千个节点上并行运行。
      </p>
     </li>
     <li>
      <p>
       <strong>
        容错性
       </strong>
       ：使用RDD（Resilient Distributed Dataset）抽象层，自动处理节点故障恢复。
      </p>
     </li>
    </ol>
    <p>
    </p>
    <h3>
     基本概念
    </h3>
    <p>
    </p>
    <hr/>
    <h4>
     <strong>
      1. RDD (Resilient Distributed Dataset)
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        定义
       </strong>
       ：
       <br/>
       RDD 是 Spark 的最底层抽象，表示分布在集群节点上的不可变、可分区的数据集合。它提供
       <strong>
        低级别的 API
       </strong>
       ，强调对数据的
       <strong>
        细粒度控制
       </strong>
       。
      </p>
     </li>
     <li>
      <p>
       <strong>
        特点
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         <strong>
          强类型
         </strong>
         ：存储任意类型的对象（如
         <code>
          RDD[User]
         </code>
         ）。
        </p>
       </li>
       <li>
        <p>
         <strong>
          手动优化
         </strong>
         ：需要开发者自行处理序列化、分区、缓存等优化。
        </p>
       </li>
       <li>
        <p>
         <strong>
          函数式操作
         </strong>
         ：通过
         <code>
          map
         </code>
         、
         <code>
          filter
         </code>
         、
         <code>
          reduce
         </code>
         等函数式算子处理数据。
        </p>
       </li>
       <li>
        <p>
         <strong>
          容错性
         </strong>
         ：通过血统（Lineage）机制重建丢失的分区。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        适用场景
       </strong>
       ：
       <br/>
       非结构化数据、需要精细控制的分布式计算（如自定义分区策略）。
      </p>
     </li>
    </ul>
    <h4>
     <strong>
      2. DataFrame
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        定义
       </strong>
       ：
       <br/>
       DataFrame 是基于 RDD 构建的
       <strong>
        结构化数据抽象
       </strong>
       ，类似于关系型数据库的表或 Pandas 的 DataFrame。在 Spark 1.3 中引入。
      </p>
     </li>
     <li>
      <p>
       <strong>
        特点
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         <strong>
          Schema 约束
         </strong>
         ：数据具有明确的结构（列名、数据类型），通过
         <code>
          Row
         </code>
         对象表示行。
        </p>
       </li>
       <li>
        <p>
         <strong>
          优化执行
         </strong>
         ：利用 Catalyst 优化器和 Tungsten 执行引擎，自动优化查询计划。
        </p>
       </li>
       <li>
        <p>
         <strong>
          API 类型
         </strong>
         ：弱类型 API（列名在运行时检查），支持 SQL 语法。
        </p>
       </li>
       <li>
        <p>
         <strong>
          跨语言支持
         </strong>
         ：在 Java、Scala、Python、R 中接口一致。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        适用场景
       </strong>
       ：
       <br/>
       结构化/半结构化数据（如 JSON、CSV）、SQL 式查询、需要自动优化的批处理。
      </p>
     </li>
    </ul>
    <p>
    </p>
    <h4>
     <strong>
      3. Dataset
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        定义
       </strong>
       ：
       <br/>
       Dataset 是 Spark 1.6 引入的 API，结合了 RDD 的
       <strong>
        强类型特性
       </strong>
       和 DataFrame 的
       <strong>
        优化引擎
       </strong>
       。仅在 Scala 和 Java 中可用。
      </p>
     </li>
     <li>
      <p>
       <strong>
        特点
       </strong>
       ：
      </p>
      <ul>
       <li>
        <p>
         <strong>
          强类型 + 结构化
         </strong>
         ：兼具
         <code>
          RDD
         </code>
         的类型安全（如
         <code>
          Dataset[User]
         </code>
         ）和
         <code>
          DataFrame
         </code>
         的优化能力。
        </p>
       </li>
       <li>
        <p>
         <strong>
          统一 API
         </strong>
         ：与 DataFrame API 兼容（DataFrame =
         <code>
          Dataset[Row]
         </code>
         ）。
        </p>
       </li>
       <li>
        <p>
         <strong>
          编码器（Encoder）
         </strong>
         ：使用高效的二进制序列化（优于 Java 序列化）。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        适用场景
       </strong>
       ：
       <br/>
       需要类型安全的复杂业务逻辑、结合函数式和关系式操作的场景。
      </p>
     </li>
    </ul>
    <p>
    </p>
    <h3>
     <strong>
      核心区别对比
     </strong>
    </h3>
    <p>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         特性
        </strong>
       </th>
       <th>
        <strong>
         RDD
        </strong>
       </th>
       <th>
        <strong>
         DataFrame
        </strong>
       </th>
       <th>
        <strong>
         Dataset
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         数据类型
        </strong>
       </td>
       <td>
        任意对象（强类型）
       </td>
       <td>
        结构化的
        <code>
         Row
        </code>
        对象（弱类型）
       </td>
       <td>
        强类型对象（如
        <code>
         Dataset[User]
        </code>
        ）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         序列化
        </strong>
       </td>
       <td>
        Java 序列化（较慢）
       </td>
       <td>
        Tungsten 二进制编码（高效）
       </td>
       <td>
        Encoder 序列化（高效）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         优化
        </strong>
       </td>
       <td>
        无自动优化，需手动调优
       </td>
       <td>
        Catalyst 优化器自动优化
       </td>
       <td>
        Catalyst 优化器自动优化
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         API 风格
        </strong>
       </td>
       <td>
        函数式（
        <code>
         map
        </code>
        ,
        <code>
         filter
        </code>
        ）
       </td>
       <td>
        声明式（SQL/DSL）
       </td>
       <td>
        混合式（强类型 API + DSL）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         类型安全
        </strong>
       </td>
       <td>
        编译时类型检查
       </td>
       <td>
        运行时类型检查
       </td>
       <td>
        编译时类型检查
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         语言支持
        </strong>
       </td>
       <td>
        所有 Spark 语言
       </td>
       <td>
        所有 Spark 语言
       </td>
       <td>
        仅 Scala/Java
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     <strong>
      演进关系
     </strong>
    </h4>
    <p>
    </p>
    <ul>
     <li>
      <p>
       <strong>
        Spark 1.x
       </strong>
       ：RDD → DataFrame（为结构化数据优化）。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Spark 2.x
       </strong>
       ：DataFrame 和 Dataset 统一为
       <code>
        Dataset[T]
       </code>
       （DataFrame =
       <code>
        Dataset[Row]
       </code>
       ）。
      </p>
     </li>
    </ul>
    <h4>
     <strong>
      如何选择？
     </strong>
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        优先用 DataFrame/Dataset
       </strong>
       ：
       <br/>
       大多数场景下，结构化数据处理更高效（Catalyst 优化 + Tungsten）。
      </p>
     </li>
     <li>
      <p>
       <strong>
        需要类型安全时用 Dataset
       </strong>
       ：
       <br/>
       如 Scala/Java 中复杂业务逻辑。
      </p>
     </li>
     <li>
      <p>
       <strong>
        仅底层控制时用 RDD
       </strong>
       ：
       <br/>
       如自定义分区、非结构化数据，或需直接操作分布式数据。
      </p>
     </li>
    </ol>
    <p>
    </p>
    <h4>
     <strong>
      代码示例
     </strong>
    </h4>
    <p>
     <img alt="" height="297" src="https://i-blog.csdnimg.cn/direct/864700a81eae4f89acd3740a50ead132.png" width="708"/>
    </p>
    <h3>
     spark-submit
     <strong>
      参数
     </strong>
    </h3>
    <p>
     在 Spark 中，
     <code>
      spark-submit
     </code>
     是提交作业到集群的核心命令。以下是常用参数及其作用，分为
     <strong>
      基础参数
     </strong>
     、
     <strong>
      资源参数
     </strong>
     和
     <strong>
      调优参数
     </strong>
     ：
    </p>
    <h4>
     <strong>
      一、基础参数
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        参数
       </th>
       <th>
        说明
       </th>
       <th>
        示例
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <code>
         --master
        </code>
       </td>
       <td>
        指定集群模式
       </td>
       <td>
        <code>
         yarn
        </code>
        ,
        <code>
         local[*]
        </code>
        ,
        <code>
         spark://host:port
        </code>
        ,
        <code>
         k8s://...
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --deploy-mode
        </code>
       </td>
       <td>
        部署模式（客户端或集群）
       </td>
       <td>
        <code>
         client
        </code>
        （默认）或
        <code>
         cluster
        </code>
        （适合生产）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --class
        </code>
       </td>
       <td>
        主类名（含包路径）
       </td>
       <td>
        <code>
         --class com.example.MainApp
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --name
        </code>
       </td>
       <td>
        作业名称（显示在集群UI）
       </td>
       <td>
        <code>
         --name "My Spark Job"
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --files
        </code>
       </td>
       <td>
        上传文件到 Executor（如配置文件）
       </td>
       <td>
        <code>
         --files config.json
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --jars
        </code>
       </td>
       <td>
        添加依赖的 JAR 包（逗号分隔）
       </td>
       <td>
        <code>
         --jars lib1.jar,lib2.jar
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --packages
        </code>
       </td>
       <td>
        从仓库自动下载依赖（Maven格式）
       </td>
       <td>
        <code>
         --packages org.apache.kafka:kafka-clients:3.4.0
        </code>
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h4>
     <strong>
      二、资源参数
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        参数
       </th>
       <th>
        说明
       </th>
       <th>
        示例
       </th>
       <th>
        注意事项
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <code>
         --executor-memory
        </code>
       </td>
       <td>
        每个 Executor 的内存
       </td>
       <td>
        <code>
         --executor-memory 4g
        </code>
       </td>
       <td>
        需预留内存给系统和开销（如总内存的10%）
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --driver-memory
        </code>
       </td>
       <td>
        Driver 进程的内存
       </td>
       <td>
        <code>
         --driver-memory 2g
        </code>
       </td>
       <td>
        客户端模式下需本地足够内存
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --num-executors
        </code>
       </td>
       <td>
        Executor 数量
       </td>
       <td>
        <code>
         --num-executors 10
        </code>
       </td>
       <td>
        根据集群资源动态调整
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --executor-cores
        </code>
       </td>
       <td>
        每个 Executor 的 CPU 核数
       </td>
       <td>
        <code>
         --executor-cores 2
        </code>
       </td>
       <td>
        总核数 =
        <code>
         num-executors * executor-cores
        </code>
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --total-executor-cores
        </code>
       </td>
       <td>
        所有 Executor 的总核数（Standalone 模式）
       </td>
       <td>
        <code>
         --total-executor-cores 20
        </code>
       </td>
       <td>
        优先级低于
        <code>
         num-executors
        </code>
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h4>
     <strong>
      三、调优参数
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        参数
       </th>
       <th>
        说明
       </th>
       <th>
        示例
       </th>
       <th>
        用途
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <code>
         --conf spark.serializer
        </code>
       </td>
       <td>
        指定序列化方式
       </td>
       <td>
        <code>
         --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
        </code>
       </td>
       <td>
        优化序列化性能
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --conf spark.sql.shuffle.partitions
        </code>
       </td>
       <td>
        调整 Shuffle 分区数
       </td>
       <td>
        <code>
         --conf spark.sql.shuffle.partitions=200
        </code>
       </td>
       <td>
        避免数据倾斜或分区过大
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --conf spark.default.parallelism
        </code>
       </td>
       <td>
        默认并行度
       </td>
       <td>
        <code>
         --conf spark.default.parallelism=100
        </code>
       </td>
       <td>
        控制 RDD 的分区数
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --conf spark.memory.fraction
        </code>
       </td>
       <td>
        Executor 内存中用于执行和存储的比例
       </td>
       <td>
        <code>
         --conf spark.memory.fraction=0.6
        </code>
       </td>
       <td>
        调整内存分配策略
       </td>
      </tr>
      <tr>
       <td>
        <code>
         --conf spark.dynamicAllocation.enabled
        </code>
       </td>
       <td>
        启用动态资源分配
       </td>
       <td>
        <code>
         --conf spark.dynamicAllocation.enabled=true
        </code>
       </td>
       <td>
        按需增减 Executor（需集群支持）
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
    <h4>
     <strong>
      示例命令
     </strong>
    </h4>
    <p>
     <img alt="" height="310" src="https://i-blog.csdnimg.cn/direct/2cf4368048264b5c8fa913741afe5050.png" width="557"/>
    </p>
    <h4>
     <strong>
      关键注意事项
     </strong>
    </h4>
    <ol>
     <li>
      <p>
       <strong>
        资源分配
       </strong>
      </p>
      <ul>
       <li>
        <p>
         总内存和核数不能超过集群资源上限。
        </p>
       </li>
       <li>
        <p>
         在 YARN 模式下，
         <code>
          --executor-memory
         </code>
         包含堆外内存，需预留约 10% 的额外内存（如申请
         <code>
          4g
         </code>
         ，实际可用约
         <code>
          4g * 0.9
         </code>
         ）。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        动态资源分配
       </strong>
      </p>
      <ul>
       <li>
        <p>
         启用
         <code>
          spark.dynamicAllocation.enabled=true
         </code>
         时需配置
         <code>
          spark.shuffle.service.enabled=true
         </code>
         （YARN 需启动 Shuffle Service）。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        依赖管理
       </strong>
      </p>
      <ul>
       <li>
        <p>
         优先使用
         <code>
          --packages
         </code>
         自动下载依赖，避免手动传 JAR。
        </p>
       </li>
       <li>
        <p>
         本地依赖用
         <code>
          --jars
         </code>
         ，集群依赖需预先上传到 HDFS 或共享存储。
        </p>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        日志与调试
       </strong>
      </p>
      <ul>
       <li>
        <p>
         添加
         <code>
          --conf spark.eventLog.enabled=true
         </code>
         记录事件日志。
        </p>
       </li>
       <li>
        <p>
         在客户端模式下，Driver 日志输出到控制台；集群模式下需通过集群 UI 查看。
        </p>
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <strong>
      参数优化场景
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        数据倾斜
       </strong>
       ：增大
       <code>
        spark.sql.shuffle.partitions
       </code>
       或使用
       <code>
        repartition
       </code>
       。
      </p>
     </li>
     <li>
      <p>
       <strong>
        OOM 错误
       </strong>
       ：增加
       <code>
        executor-memory
       </code>
       或调整
       <code>
        spark.memory.fraction
       </code>
       。
      </p>
     </li>
     <li>
      <p>
       <strong>
        CPU 瓶颈
       </strong>
       ：增加
       <code>
        num-executors
       </code>
       或
       <code>
        executor-cores
       </code>
       。
      </p>
     </li>
     <li>
      <p>
       <strong>
        网络超时
       </strong>
       ：调整
       <code>
        spark.network.timeout
       </code>
       （默认 120s）。
      </p>
     </li>
    </ul>
    <p>
    </p>
    <h2>
     hive
    </h2>
    <p>
     <code>
      Hive
     </code>
     是构建在
     <strong>
      Hadoop
     </strong>
     生态系统之上的数据仓库工具，旨在简化大规模数据的查询和管理。它通过类 SQL 语法（HiveQL）将结构化数据操作转化为 MapReduce、Tez 或 Spark 任务，适合处理海量数据（如日志、用户行为等）。以下是其核心概念和用法：
    </p>
    <h3>
     <strong>
      Hive 核心特性
     </strong>
    </h3>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         特性
        </strong>
       </th>
       <th>
        <strong>
         描述
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         SQL-like 语法
        </strong>
       </td>
       <td>
        支持类似 SQL 的查询语言（HiveQL），降低大数据处理的学习成本。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         数据存储
        </strong>
       </td>
       <td>
        数据存储在 HDFS（Hadoop 分布式文件系统）中，支持多种文件格式（如 ORC、Parquet）。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         元数据管理
        </strong>
       </td>
       <td>
        使用 Metastore（如 MySQL）存储表结构、分区等元信息。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         扩展性
        </strong>
       </td>
       <td>
        支持自定义函数（UDF）、SerDe（序列化/反序列化工具）等扩展功能。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         批处理
        </strong>
       </td>
       <td>
        基于 MapReduce 或 Tez 引擎，适合离线批处理，
        <strong>
         不适用于实时查询
        </strong>
        。
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
    <h3>
     <strong>
      Hive 架构
     </strong>
    </h3>
    <ol>
     <li>
      <p>
       <strong>
        用户接口
       </strong>
       <br/>
       CLI、JDBC、Web UI 等工具提交 HiveQL 查询。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Driver
       </strong>
       <br/>
       解析查询，生成执行计划，管理任务生命周期。
      </p>
     </li>
     <li>
      <p>
       <strong>
        编译器
       </strong>
       <br/>
       将 HiveQL 转换为 MapReduce/Tez/Spark 任务。
      </p>
     </li>
     <li>
      <p>
       <strong>
        元数据存储
       </strong>
       <br/>
       Metastore 存储表结构、分区、字段类型等信息。
      </p>
     </li>
     <li>
      <p>
       <strong>
        执行引擎
       </strong>
       <br/>
       运行编译后的任务，读写 HDFS 数据。
      </p>
     </li>
    </ol>
    <h3>
     <strong>
      Hive 数据模型
     </strong>
    </h3>
    <h4>
     <strong>
      表（Table）
     </strong>
     <br/>
     类似关系型数据库的表，支持内部表（数据由 Hive 管理）和外部表（数据由用户管理）。
    </h4>
    <blockquote>
     <p>
      CREATE TABLE users (id INT, name STRING) STORED AS ORC;
     </p>
    </blockquote>
    <h4>
     <strong>
      分区（Partition）
     </strong>
    </h4>
    <p>
     按某一列的值划分数据目录，加速查询（如按日期分区）。
    </p>
    <p>
     CREATE TABLE logs (log_time STRING, content STRING)
     <br/>
     PARTITIONED BY (dt STRING);
    </p>
    <h4>
     <strong>
      分桶（Bucket）
     </strong>
    </h4>
    <p>
     按哈希值将数据分到多个文件，优化 JOIN 和采样效率。
    </p>
    <p>
     CREATE TABLE orders (order_id INT, user_id INT)
     <br/>
     CLUSTERED BY (user_id) INTO 10 BUCKETS;
    </p>
    <h3>
     <strong>
      Hive 应用场景
     </strong>
    </h3>
    <p>
    </p>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         场景
        </strong>
       </th>
       <th>
        <strong>
         说明
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         离线数据分析
        </strong>
       </td>
       <td>
        处理 TB/PB 级历史数据（如用户行为分析、日志统计）。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         ETL 流程
        </strong>
       </td>
       <td>
        清洗、转换数据后导入数据仓库（如将 CSV 转换为 ORC 格式）。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         数据挖掘
        </strong>
       </td>
       <td>
        结合机器学习库（如 Hive + Mahout）进行聚类、分类等操作。
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         报表生成
        </strong>
       </td>
       <td>
        定时生成统计报表（如每日销售额汇总）。
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     <strong>
      Hive 优缺点
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         优点
        </strong>
       </th>
       <th>
        <strong>
         缺点
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        易用性强（SQL 语法）
       </td>
       <td>
        延迟高（分钟级响应，不适合实时查询）
       </td>
      </tr>
      <tr>
       <td>
        可扩展性高（自定义 UDF）
       </td>
       <td>
        不支持事务和行级更新（Hive 3 部分支持）
       </td>
      </tr>
      <tr>
       <td>
        兼容 Hadoop 生态（HDFS、HBase 等）
       </td>
       <td>
        需要优化分区和存储格式提升性能
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
    <h4>
     <strong>
      Hive vs 传统数据库
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         对比项
        </strong>
       </th>
       <th>
        <strong>
         Hive
        </strong>
       </th>
       <th>
        <strong>
         传统数据库（如 MySQL）
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         数据规模
        </strong>
       </td>
       <td>
        支持 PB 级数据
       </td>
       <td>
        适合 GB/TB 级数据
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         响应速度
        </strong>
       </td>
       <td>
        高延迟（批处理）
       </td>
       <td>
        低延迟（实时查询）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         事务支持
        </strong>
       </td>
       <td>
        有限支持（Hive 3+）
       </td>
       <td>
        完整支持 ACID
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         存储与计算
        </strong>
       </td>
       <td>
        分离（HDFS + 计算引擎）
       </td>
       <td>
        耦合（本地存储 + 计算
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
    <h4>
     <strong>
      Hive 使用示例
     </strong>
    </h4>
    <h5>
     <strong>
      创建表并加载数据
     </strong>
    </h5>
    <blockquote>
     <p>
      CREATE EXTERNAL TABLE user_logs (
      <br/>
      ip STRING,
      <br/>
      url STRING,
      <br/>
      time STRING
      <br/>
      ) PARTITIONED BY (dt STRING)
      <br/>
      ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
      <br/>
      LOCATION '/hive/data/user_logs';
     </p>
     <p>
      LOAD DATA INPATH '/input/log_20231001.txt' INTO TABLE user_logs PARTITION (dt='2023-10-01');
     </p>
    </blockquote>
    <h5>
     <strong>
      聚合查询
     </strong>
    </h5>
    <blockquote>
     <p>
      SELECT dt, COUNT(*) AS pv
      <br/>
      FROM user_logs
      <br/>
      WHERE dt BETWEEN '2023-10-01' AND '2023-10-07'
      <br/>
      GROUP BY dt;
     </p>
    </blockquote>
    <h5>
     <strong>
      连接多个表
     </strong>
    </h5>
    <blockquote>
     <p>
      SELECT u.name, SUM(o.amount)
      <br/>
      FROM orders o
      <br/>
      JOIN users u ON o.user_id = u.id
      <br/>
      GROUP BY u.name;
     </p>
    </blockquote>
    <h4>
     <strong>
      生态工具
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        Hive Metastore
       </strong>
       ：独立元数据服务（供 Spark、Presto 等共用）。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Hive on Spark
       </strong>
       ：用 Spark 替代 MapReduce 提升计算速度。
      </p>
     </li>
     <li>
      <p>
       <strong>
        Hive LLAP
       </strong>
       （Live Long and Process）：低延迟交互式查询。
      </p>
     </li>
    </ul>
    <h4>
     <strong>
      总结
     </strong>
    </h4>
    <ul>
     <li>
      <p>
       <strong>
        适用场景
       </strong>
       ：离线批处理、海量数据仓库管理。
      </p>
     </li>
     <li>
      <p>
       <strong>
        替代方案
       </strong>
       ：实时查询用
       <strong>
        Impala
       </strong>
       或
       <strong>
        Presto
       </strong>
       ；复杂分析用
       <strong>
        Spark SQL
       </strong>
       。
      </p>
     </li>
     <li>
      <p>
       <strong>
        学习建议
       </strong>
       ：掌握 HiveQL 语法、分区优化和存储格式（ORC/Parquet）。
      </p>
     </li>
    </ul>
    <p>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34313832363231352f:61727469636c652f64657461696c732f313436313535343830" class_="artid" style="display:none">
 </p>
</div>


