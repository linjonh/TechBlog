---
layout: post
title: "机器学习与深度学习中模型训练时常用的四种正则化技术L1,L2,L21,ElasticNet"
date: 2025-03-14 18:36:54 +08:00
description: "四种训练模型时常用的正则化技术对于模型参数的不同影响。"
keywords: "机器学习与深度学习中模型训练时常用的四种正则化技术L1，L2，L21，ElasticNet"
categories: ['未分类']
tags: ['笔记', '机器学习', '人工智能', 'Pytorch']
artid: "146264937"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146264937
    alt: "机器学习与深度学习中模型训练时常用的四种正则化技术L1,L2,L21,ElasticNet"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146264937
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146264937
cover: https://bing.ee123.net/img/rand?artid=146264937
image: https://bing.ee123.net/img/rand?artid=146264937
img: https://bing.ee123.net/img/rand?artid=146264937
---

# 机器学习与深度学习中模型训练时常用的四种正则化技术L1，L2，L21，ElasticNet
L1正则化和L2正则化是机器学习中常用的两种正则化方法，用于防止模型过拟合。它们的区别主要体现在数学形式、作用机制和应用效果上。以下是详细对比：
#### 1 **数学定义**
* **L1正则化** （也叫Lasso正则化）：
在损失函数中加入权重参数的绝对值之和，即 λ ∑ ∣ w i ∣ \lambda \sum |w_i| λ∑∣wi​∣。
公式：
L o s s = L o s s 原始 \+ λ ∑ ∣ w i ∣ Loss = Loss_{原始} + \lambda \sum |w_i|
Loss=Loss原始​+λ∑∣wi​∣
其中 w i w_i wi​ 是模型的权重参数， λ \lambda λ 是正则化强度的超参数。
* **L2正则化** （也叫Ridge正则化）：
在损失函数中加入权重参数的平方和，即 λ ∑ w i 2 \lambda \sum w_i^2 λ∑wi2​。
公式：
L o s s = L o s s 原始 \+ λ ∑ w i 2 Loss = Loss_{原始} + \lambda \sum w_i^2
Loss=Loss原始​+λ∑wi2​
#### 2 **几何解释**
* **L1正则化** ：
在参数空间中，L1正则化对应一个菱形（或高维的绝对值约束）。优化时，损失函数的最优解倾向于落在菱形的顶点上，导致部分权重被精确地压缩到0，具有稀疏性。
* **L2正则化** ：
对应一个圆形（或高维球面约束）。优化时，权重倾向于均匀缩小，但不会精确到0，而是变得很小。
#### 3 **作用效果**
* **L1正则化** ：
* 倾向于产生**稀疏解** ，即部分权重变为0。
* 适合特征选择，因为它可以自动剔除不重要的特征。
* **L2正则化** ：
* 倾向于让所有权重变小但不为0，保持权重分布更平滑。
* 更适合处理多重共线性（特征之间高度相关）的情况。
#### 4 **计算复杂度**
* **L1正则化** ：
由于绝对值函数不可导，优化时需要特殊的算法（如次梯度下降或坐标下降法），计算复杂度稍高。
* **L2正则化** ：
平方项是连续可导的，可以直接用梯度下降等方法优化，计算上更简单。
#### 5 **应用场景**
* **L1正则化** ：
* 当特征数量很多且你怀疑只有少部分特征重要时（如高维数据降维）。
* 示例：Lasso回归。
* **L2正则化** ：
* 当所有特征都可能有贡献，但需要控制权重大小以避免过拟合时。
* 示例：Ridge回归、神经网络中的权重衰减。
#### 6 **组合使用**
在实践中，L1和L2正则化可以结合使用，称为**Elastic Net** ，公式为：
L o s s = L o s s 原始 \+ λ 1 ∑ ∣ w i ∣ \+ λ 2 ∑ w i 2 Loss = Loss_{原始} +
\lambda_1 \sum |w_i| + \lambda_2 \sum w_i^2 Loss=Loss原始​+λ1​∑∣wi​∣+λ2​∑wi2​
这种方法兼具L1的稀疏性和L2的平滑性。
#### 总结
特性| L1正则化| L2正则化
---|---|---
**惩罚项**| 绝对值 w w w| 平方 w 2 w^2 w2
**权重结果**| 稀疏（部分为0）| 小但非0
**几何形状**| 菱形| 圆形
**主要用途**| 特征选择| 权重平滑、防止过拟合
**计算难度**| 稍高| 较低
简单来说，L1更像“选择性淘汰”，L2更像“整体削弱”。根据具体任务需求选择合适的正则化方法！
* * *
### 补充： L 21 n o r m L_{21}norm L21​norm范数正则化与Elastic Net 的区别
Elastic Net和L21范数正则化是两种不同的正则化方法，它们的主要区别如下：
* **原理不同**
* **Elastic Net** ：结合了L1和L2正则化的特点，在损失函数中同时引入L1范数和L2范数作为正则化项，通过一个混合参数来平衡两者的贡献。其正则化项的表达式为(R(\boldsymbol{w})=\lambda_1|\boldsymbol{w}|_1+\lambda_2|\boldsymbol{w}|_2^2)，其中(\boldsymbol{w})是模型的参数，(\lambda_1)和(\lambda_2)是正则化参数。
* **L21范数正则化** ：是对矩阵的每一行或每一列的L2范数进行求和，然后将其作为正则化项添加到损失函数中。对于一个矩阵(\boldsymbol{W})，其L21范数正则化项的表达式为(R(\boldsymbol{W})=\sum_{i}|\boldsymbol{w}_i|_2)，其中(\boldsymbol{w}_i)表示矩阵(\boldsymbol{W})的第(i)行或第(i)列。
* **应用场景不同**
* **Elastic Net** ：常用于线性回归、逻辑回归等模型中，当数据存在多个相关特征时，它可以有效地选择出重要的特征，并对模型进行正则化，防止过拟合。
* **L21范数正则化** ：在一些需要对矩阵结构进行约束的场景中更为常用，如图像处理、信号处理等领域，它可以用于特征选择、矩阵分解等任务。
* **效果不同**
* **Elastic Net** ：由于同时包含L1和L2正则化的特性，它既能够实现特征的稀疏性，又能够对模型的参数进行平滑处理，使得模型具有较好的泛化能力。
* **L21范数正则化** ：主要作用是促进矩阵的行或列的稀疏性，使得一些行或列的元素趋近于零，从而实现特征选择或矩阵结构的简化。
Elastic Net和L21范数正则化在原理、应用场景和效果等方面都存在一定的区别，在实际应用中，需要根据具体的问题和数据特点选择合适的正则化方法。