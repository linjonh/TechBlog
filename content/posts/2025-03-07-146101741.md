---
layout: post
title: "自然语言处理TransformerBERT"
date: 2025-03-07 18:03:08 +0800
description: "是一种新型的深度学习架构，利用自注意力机制处理序列数据，具有并行处理能力和捕捉长距离依赖的优势。BERT是基于Transformer架构的预训练语言模型，采用双向编码器和预训练任务（MLM和NSP），能够生成上下文相关的词嵌入，并在多种NLP任务上表现出色。通过使用Hugging Face的Transformers库，可以方便地加载和使用预训练的BERT模型，并在特定任务上进行微调。是一种通用的深度学习架构，适用于广泛的序列数据处理任务，特别是那些需要生成新序列的任务（如机器翻译、文本摘要）。"
keywords: "bidirectional encoder representations from transformers"
categories: ['深度学习记录']
tags: ['自然语言处理', 'Transformer', 'Bert']
artid: "146101741"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146101741
    alt: "自然语言处理TransformerBERT"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146101741
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146101741
cover: https://bing.ee123.net/img/rand?artid=146101741
image: https://bing.ee123.net/img/rand?artid=146101741
img: https://bing.ee123.net/img/rand?artid=146101741
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     自然语言处理：Transformer、BERT
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     BERT（Bidirectional Encoder Representations from Transformers）和Transformer是自然语言处理（NLP）领域中的两个重要概念。Transformer是一种新型的深度学习架构，而BERT是基于Transformer架构的具体模型之一，主要用于预训练语言表示。
    </p>
    <h4>
     <a id="Transformer_2">
     </a>
     Transformer架构
    </h4>
    <p>
     Transformer是由Vaswani等人在2017年的论文《Attention is All You Need》中提出的，旨在解决序列数据处理中的长依赖问题。与传统的循环神经网络（RNN）和卷积神经网络（CNN）不同，Transformer完全依赖于自注意力机制（self-attention mechanism），使得它可以并行处理序列数据，并且能够更好地捕捉长距离依赖关系。
    </p>
    <h5>
     <a id="Transformer_6">
     </a>
     Transformer的主要组成部分
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        输入嵌入层（Input Embedding Layer）
       </strong>
       ：
      </p>
      <ul>
       <li>
        将输入的词索引转换为词向量表示。
       </li>
       <li>
        通常还会添加位置编码（Positional Encoding），以保留输入序列的位置信息。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        编码器（Encoder）
       </strong>
       ：
      </p>
      <ul>
       <li>
        包含多个相同的编码器层（通常为6到12层）。
       </li>
       <li>
        每个编码器层由两部分组成：多头自注意力机制（Multi-Head Self-Attention）和前馈神经网络（Feed-Forward Neural Network）。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        解码器（Decoder）
       </strong>
       ：
      </p>
      <ul>
       <li>
        包含多个相同的解码器层（同样通常为6到12层）。
       </li>
       <li>
        每个解码器层由三部分组成：多头自注意力机制、编码器-解码器注意力机制（Encoder-Decoder Attention）和前馈神经网络。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        线性层和Softmax层
       </strong>
       ：
      </p>
      <ul>
       <li>
        解码器输出通过一个线性层映射到词汇表大小的维度。
       </li>
       <li>
        然后通过Softmax层生成下一个词的概率分布。
       </li>
      </ul>
     </li>
    </ol>
    <h5>
     <a id="Transformer_24">
     </a>
     Transformer的工作流程
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        输入嵌入和位置编码
       </strong>
       ：
      </p>
      <ul>
       <li>
        输入序列经过嵌入层转换为词向量，并添加位置编码。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        编码器层
       </strong>
       ：
      </p>
      <ul>
       <li>
        多头自注意力机制计算每个词与其他词之间的相关性，并生成加权后的表示。
       </li>
       <li>
        前馈神经网络进一步处理这些表示，增加模型的非线性能力。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        解码器层
       </strong>
       ：
      </p>
      <ul>
       <li>
        解码器首先使用多头自注意力机制处理目标序列的嵌入。
       </li>
       <li>
        然后使用编码器-解码器注意力机制将目标序列与源序列对齐。
       </li>
       <li>
        最后通过前馈神经网络生成最终的表示。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        生成输出
       </strong>
       ：
      </p>
      <ul>
       <li>
        解码器的输出通过线性层和Softmax层生成下一个词的概率分布。
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="BERT_41">
     </a>
     BERT模型
    </h4>
    <p>
     BERT（Bidirectional Encoder Representations from Transformers）是Google在2018年提出的一种预训练语言模型，它利用Transformer架构来生成上下文相关的词嵌入。BERT的主要特点是双向编码器，即同时考虑了单词左侧和右侧的上下文信息。
    </p>
    <h5>
     <a id="BERT_45">
     </a>
     BERT的关键特性
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        双向编码器
       </strong>
       ：
      </p>
      <ul>
       <li>
        BERT使用了Transformer的编码器部分，并且采用了双向训练方式，即同时考虑了单词左侧和右侧的上下文信息。
       </li>
       <li>
        这与传统的单向语言模型（如LSTM或GRU）不同，后者只能从左到右或从右到左处理文本。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        预训练任务
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         Masked Language Model (MLM)
        </strong>
        ：随机遮蔽输入序列中的一些词，并要求模型预测这些被遮蔽的词。这种方式使得模型必须理解整个句子的上下文。
       </li>
       <li>
        <strong>
         Next Sentence Prediction (NSP)
        </strong>
        ：给定两个句子A和B，要求模型判断B是否是A的下一句。这有助于模型理解句子间的关系。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        微调（Fine-tuning）
       </strong>
       ：
      </p>
      <ul>
       <li>
        预训练完成后，BERT可以在特定的任务上进行微调，例如文本分类、命名实体识别、问答系统等。
       </li>
       <li>
        微调时只需在BERT的基础上添加少量的任务特定层，并使用任务特定的数据集进行训练。
       </li>
      </ul>
     </li>
    </ol>
    <h5>
     <a id="BERT_59">
     </a>
     BERT的架构
    </h5>
    <p>
     BERT的架构基于Transformer的编码器部分，具体包括以下组件：
    </p>
    <ol>
     <li>
      <p>
       <strong>
        输入嵌入层
       </strong>
       ：
      </p>
      <ul>
       <li>
        输入包括三个部分：词嵌入（Token Embeddings）、段嵌入（Segment Embeddings）和位置嵌入（Position Embeddings）。
       </li>
       <li>
        词嵌入表示输入序列中的每个词。
       </li>
       <li>
        段嵌入用于区分输入序列中的不同句子（BERT可以处理一对句子）。
       </li>
       <li>
        位置嵌入用于保留输入序列的位置信息。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        编码器层
       </strong>
       ：
      </p>
      <ul>
       <li>
        BERT通常包含12层或24层编码器（分别对应BERT-base和BERT-large版本）。
       </li>
       <li>
        每个编码器层由多头自注意力机制和前馈神经网络组成。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        输出层
       </strong>
       ：
      </p>
      <ul>
       <li>
        编码器的最后一层输出是一个矩阵，其中每一行对应输入序列中每个词的上下文表示。
       </li>
      </ul>
     </li>
    </ol>
    <h5>
     <a id="BERT_76">
     </a>
     BERT的工作流程
    </h5>
    <ol>
     <li>
      <p>
       <strong>
        输入嵌入
       </strong>
       ：
      </p>
      <ul>
       <li>
        将输入序列中的每个词转换为词嵌入，并添加段嵌入和位置嵌入。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        编码器层
       </strong>
       ：
      </p>
      <ul>
       <li>
        输入嵌入依次通过多个编码器层，每层都包含多头自注意力机制和前馈神经网络。
       </li>
       <li>
        每个编码器层生成更丰富的上下文表示。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        预训练任务
       </strong>
       ：
      </p>
      <ul>
       <li>
        在预训练阶段，BERT使用MLM和NSP任务来训练模型。
       </li>
       <li>
        MLM任务要求模型预测被遮蔽的词，NSP任务要求模型判断句子B是否是句子A的下一句。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        微调
       </strong>
       ：
      </p>
      <ul>
       <li>
        在微调阶段，BERT在特定任务上进行训练，通常只需要在最后一层添加任务特定的输出层，并使用任务特定的数据集进行训练。
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="Hugging_FaceTransformersBERT_92">
     </a>
     示例代码：使用Hugging Face的Transformers库加载和使用BERT
    </h4>
    <p>
     Hugging Face的Transformers库提供了方便的接口来加载和使用BERT模型。以下是一个简单的示例，展示如何使用BERT进行文本分类。
    </p>
    <h5>
     <a id="_96">
     </a>
     安装依赖
    </h5>
    <pre><code class="prism language-bash">pip <span class="token function">install</span> transformers torch
</code></pre>
    <h5>
     <a id="BERT_102">
     </a>
     加载预训练的BERT模型
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertForSequenceClassification
<span class="token keyword">import</span> torch

<span class="token comment"># 加载预训练的BERT tokenizer和模型</span>
tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 示例输入文本</span>
text <span class="token operator">=</span> <span class="token string">"Hello, how are you?"</span>

<span class="token comment"># 对输入文本进行编码</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">)</span>

<span class="token comment"># 获取模型输出</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>

<span class="token comment"># 输出分类结果</span>
logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Predictions:"</span><span class="token punctuation">,</span> predictions<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="BERT_128">
     </a>
     训练BERT模型
    </h5>
    <p>
     下面是一个简单的例子，展示如何在PyTorch中使用BERT进行微调。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertForSequenceClassification<span class="token punctuation">,</span> Trainer<span class="token punctuation">,</span> TrainingArguments
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset

<span class="token comment"># 加载数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">'glue'</span><span class="token punctuation">,</span> <span class="token string">'mrpc'</span><span class="token punctuation">)</span>

<span class="token comment"># 加载预训练的BERT tokenizer和模型</span>
tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 定义数据处理函数</span>
<span class="token keyword">def</span> <span class="token function">preprocess_function</span><span class="token punctuation">(</span>examples<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tokenizer<span class="token punctuation">(</span>examples<span class="token punctuation">[</span><span class="token string">'sentence1'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> examples<span class="token punctuation">[</span><span class="token string">'sentence2'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 对数据集进行预处理</span>
encoded_dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>preprocess_function<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment"># 设置训练参数</span>
training_args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span><span class="token string">'./results'</span><span class="token punctuation">,</span>
    evaluation_strategy<span class="token operator">=</span><span class="token string">"epoch"</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">2e-5</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 初始化Trainer</span>
trainer <span class="token operator">=</span> Trainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>encoded_dataset<span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>encoded_dataset<span class="token punctuation">[</span><span class="token string">'validation'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># 开始训练</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_173">
     </a>
     总结
    </h4>
    <ul>
     <li>
      <strong>
       Transformer
      </strong>
      是一种新型的深度学习架构，利用自注意力机制处理序列数据，具有并行处理能力和捕捉长距离依赖的优势。
     </li>
     <li>
      <strong>
       BERT
      </strong>
      是基于Transformer架构的预训练语言模型，采用双向编码器和预训练任务（MLM和NSP），能够生成上下文相关的词嵌入，并在多种NLP任务上表现出色。
     </li>
    </ul>
    <p>
     通过使用Hugging Face的Transformers库，可以方便地加载和使用预训练的BERT模型，并在特定任务上进行微调。
    </p>
    <h4>
     <a id="Transformer__BERT__179">
     </a>
     Transformer 和 BERT 的主要区别
    </h4>
    <p>
     虽然BERT是基于Transformer架构构建的，但它们在设计目标、应用场景和具体实现上有显著的区别。以下是对两者的主要区别的详细解析：
    </p>
    <h5>
     <a id="1__183">
     </a>
     1.
     <strong>
      架构与目的
     </strong>
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        Transformer
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         架构
        </strong>
        ：Transformer是一种通用的深度学习架构，最初由Vaswani等人在2017年的论文《Attention is All You Need》中提出。它包括编码器（Encoder）和解码器（Decoder），主要用于处理序列数据，如机器翻译。
       </li>
       <li>
        <strong>
         目的
        </strong>
        ：旨在解决长距离依赖问题，并提供一种可以并行化处理序列数据的方法。Transformer通过自注意力机制（Self-Attention Mechanism）捕捉输入序列中的全局依赖关系。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        BERT
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         架构
        </strong>
        ：BERT（Bidirectional Encoder Representations from Transformers）仅使用了Transformer的编码器部分，没有解码器。BERT模型通常包含多个相同的编码器层（通常是12层或24层）。
       </li>
       <li>
        <strong>
         目的
        </strong>
        ：BERT是一个预训练语言模型，旨在生成上下文相关的词嵌入。它的主要特点是双向编码器，即同时考虑了单词左侧和右侧的上下文信息，从而更好地理解句子的整体含义。
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="2__193">
     </a>
     2.
     <strong>
      输入与输出
     </strong>
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        Transformer
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         输入
        </strong>
        ：Transformer可以处理一对序列（例如源语言句子和目标语言句子），并通过位置编码保留序列的位置信息。
       </li>
       <li>
        <strong>
         输出
        </strong>
        ：解码器生成一个输出序列，通常用于生成翻译结果或其他序列预测任务。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        BERT
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         输入
        </strong>
        ：BERT接收单个句子或一对句子作为输入。输入包括三个部分：词嵌入（Token Embeddings）、段嵌入（Segment Embeddings）和位置嵌入（Position Embeddings）。段嵌入用于区分输入序列中的不同句子。
       </li>
       <li>
        <strong>
         输出
        </strong>
        ：编码器的最后一层输出是一个矩阵，其中每一行对应输入序列中每个词的上下文表示。这些表示可以用于各种下游任务，如文本分类、命名实体识别等。
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="3__203">
     </a>
     3.
     <strong>
      训练方式
     </strong>
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        Transformer
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         训练任务
        </strong>
        ：Transformer通常用于有监督的任务，如机器翻译。训练过程中，模型通过最小化目标序列的真实值和预测值之间的损失来优化参数。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        BERT
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         训练任务
        </strong>
        ：BERT采用无监督的预训练方式，主要包括两个任务：
        <ul>
         <li>
          <strong>
           Masked Language Model (MLM)
          </strong>
          ：随机遮蔽输入序列中的一些词，并要求模型预测这些被遮蔽的词。这种方式使得模型必须理解整个句子的上下文。
         </li>
         <li>
          <strong>
           Next Sentence Prediction (NSP)
          </strong>
          ：给定两个句子A和B，要求模型判断B是否是A的下一句。这有助于模型理解句子间的关系。
         </li>
        </ul>
       </li>
       <li>
        <strong>
         微调
        </strong>
        ：预训练完成后，BERT可以在特定的任务上进行微调，通常只需要在最后一层添加任务特定的输出层，并使用任务特定的数据集进行训练。
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="4__214">
     </a>
     4.
     <strong>
      应用场景
     </strong>
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        Transformer
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         应用场景
        </strong>
        ：Transformer广泛应用于需要处理序列数据的任务，如机器翻译、文本摘要、问答系统等。由于其并行化的能力，Transformer在大规模数据集上的训练效率较高。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        BERT
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         应用场景
        </strong>
        ：BERT主要用于自然语言理解任务，如文本分类、情感分析、命名实体识别、问答系统等。BERT通过预训练生成强大的语言表示，然后在特定任务上进行微调，表现出色。
       </li>
      </ul>
     </li>
    </ul>
    <h5>
     <a id="5__222">
     </a>
     5.
     <strong>
      性能与效果
     </strong>
    </h5>
    <ul>
     <li>
      <p>
       <strong>
        Transformer
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         性能
        </strong>
        ：Transformer在处理长序列时表现出色，因为它能够捕捉全局依赖关系。然而，传统的Transformer模型在处理某些复杂的语言理解任务时可能不如BERT有效。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        BERT
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         性能
        </strong>
        ：BERT在多种NLP基准测试中取得了优异的成绩，特别是在理解和生成上下文相关的词嵌入方面表现突出。BERT的双向编码器使其能够更好地捕捉句子的语义信息。
       </li>
      </ul>
     </li>
    </ul>
    <h4>
     <a id="_230">
     </a>
     具体对比表
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        特性
       </th>
       <th>
        Transformer
       </th>
       <th>
        BERT
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         架构
        </strong>
       </td>
       <td>
        包含编码器和解码器
       </td>
       <td>
        仅包含编码器
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         输入
        </strong>
       </td>
       <td>
        可以处理一对序列（如源语言和目标语言）
       </td>
       <td>
        单个句子或一对句子
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         输出
        </strong>
       </td>
       <td>
        输出为序列（如翻译结果）
       </td>
       <td>
        上下文相关的词嵌入矩阵
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         训练任务
        </strong>
       </td>
       <td>
        有监督任务（如机器翻译）
       </td>
       <td>
        无监督预训练（MLM和NSP）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         应用场景
        </strong>
       </td>
       <td>
        序列到序列任务（如机器翻译、文本摘要）
       </td>
       <td>
        自然语言理解任务（如文本分类、问答系统）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         优点
        </strong>
       </td>
       <td>
        捕捉全局依赖关系，适用于长序列任务
       </td>
       <td>
        双向编码器，上下文相关词嵌入，适合复杂语言理解任务
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         缺点
        </strong>
       </td>
       <td>
        解码器部分限制了其在某些语言理解任务中的表现
       </td>
       <td>
        计算资源需求高，预训练时间较长
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     <a id="_242">
     </a>
     示例代码对比
    </h4>
    <h5>
     <a id="Transformer__244">
     </a>
     Transformer 示例（机器翻译）
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> MarianMTModel<span class="token punctuation">,</span> MarianTokenizer

<span class="token comment"># 加载预训练的MarianMT模型和tokenizer</span>
model_name <span class="token operator">=</span> <span class="token string">'Helsinki-NLP/opus-mt-en-de'</span>
tokenizer <span class="token operator">=</span> MarianTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
model <span class="token operator">=</span> MarianMTModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>

<span class="token comment"># 示例输入文本</span>
text <span class="token operator">=</span> <span class="token string">"Hello, how are you?"</span>

<span class="token comment"># 对输入文本进行编码</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">)</span>

<span class="token comment"># 获取模型输出</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>

<span class="token comment"># 解码输出</span>
translated_text <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Translated Text:"</span><span class="token punctuation">,</span> translated_text<span class="token punctuation">)</span>
</code></pre>
    <h5>
     <a id="BERT__269">
     </a>
     BERT 示例（文本分类）
    </h5>
    <pre><code class="prism language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BertTokenizer<span class="token punctuation">,</span> BertForSequenceClassification
<span class="token keyword">import</span> torch

<span class="token comment"># 加载预训练的BERT tokenizer和模型</span>
tokenizer <span class="token operator">=</span> BertTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> BertForSequenceClassification<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'bert-base-uncased'</span><span class="token punctuation">,</span> num_labels<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

<span class="token comment"># 示例输入文本</span>
text <span class="token operator">=</span> <span class="token string">"This movie was fantastic!"</span>

<span class="token comment"># 对输入文本进行编码</span>
inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">'pt'</span><span class="token punctuation">)</span>

<span class="token comment"># 获取模型输出</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    outputs <span class="token operator">=</span> model<span class="token punctuation">(</span><span class="token operator">**</span>inputs<span class="token punctuation">)</span>

<span class="token comment"># 输出分类结果</span>
logits <span class="token operator">=</span> outputs<span class="token punctuation">.</span>logits
predictions <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Predictions:"</span><span class="token punctuation">,</span> predictions<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_295">
     </a>
     总结
    </h4>
    <ul>
     <li>
      <strong>
       Transformer
      </strong>
      是一种通用的深度学习架构，适用于广泛的序列数据处理任务，特别是那些需要生成新序列的任务（如机器翻译、文本摘要）。
     </li>
     <li>
      <strong>
       BERT
      </strong>
      是基于Transformer编码器部分的预训练语言模型，专注于生成上下文相关的词嵌入，特别适用于自然语言理解任务。
     </li>
    </ul>
    <p>
     两者的主要区别在于架构设计、训练方式和应用场景
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f35313932393237302f:61727469636c652f64657461696c732f313436313031373431" class_="artid" style="display:none">
 </p>
</div>


