---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f6379626572736e6f772f:61727469636c652f64657461696c732f313436323839333834"
layout: post
title: "数字人本地部署之llama-本地推理模型"
date: 2025-03-16 03:27:59 +0800
description: "属于命令行选项，一般用来指定要加载的模型文件。是模型文件的路径。gguf格式的文件是一种用于存储语言模型权重的文件格式，服务器会加载这个文件里的模型权重，从而使用对应的语言模型开展任务。也是命令行选项，其作用是指定服务器要监听的端口号。"
keywords: "数字人本地部署之llama-本地推理模型"
categories: ['新时代办公基础']
tags: ['数字人', '人工智能', 'Llama']
artid: "146289384"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146289384
    alt: "数字人本地部署之llama-本地推理模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146289384
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146289384
cover: https://bing.ee123.net/img/rand?artid=146289384
image: https://bing.ee123.net/img/rand?artid=146289384
img: https://bing.ee123.net/img/rand?artid=146289384
---

# 数字人本地部署之llama-本地推理模型
## llama 本地服务命令
llama-server.exe -m "data/LLM/my.gguf" --port   8080
#### `-m data/LLM/my.gguf`
`-m`
属于命令行选项，一般用来指定要加载的模型文件。
`data/LLM/my.gguf`
是模型文件的路径。
`gguf`
格式的文件是一种用于存储语言模型权重的文件格式，服务器会加载这个文件里的模型权重，从而使用对应的语言模型开展任务。
`--port 8080`
`--port`
也是命令行选项，其作用是指定服务器要监听的端口号。
## 二、llama帮助命令
llama-server.exe --help
## 三、llama命令工具下载
[https://github.com/ggml-org/llama.cpp/releases](https://github.com/ggml-org/llama.cpp/releases "https://github.com/ggml-org/llama.cpp/releases")
![](https://i-blog.csdnimg.cn/direct/00c128da7ac8408195d5bacbc8de6e31.png)
如何选择下载版本
cuda
cudart-llama-bin-win-cu11.7-x64.zip
![](https://i-blog.csdnimg.cn/direct/39619ec5378c4653b8369c8d292ce638.png)
## 四、如何查看自己电脑CPU指令
[未来商城—APPSTORE](http://51.onelink.ynwlzc.net/o2o/index.php/appshow/Rsv "未来商城—APPSTORE")
![](https://i-blog.csdnimg.cn/direct/f73381a1beb648b993dbd5be2ef275e2.png)