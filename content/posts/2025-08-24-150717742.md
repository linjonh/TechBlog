---
layout: post
title: "数据挖掘-4.8-评估泛化能力"
date: 2025-08-24T22:27:08+0800
description: "评估模型的泛化能力的方法。比如，K-fold交叉验证。样本过大过小都要用别的方式。"
keywords: "数据挖掘 4.8 评估泛化能力"
categories: ['未分类']
tags: ['深度学习', '机器学习', '数据挖掘']
artid: "150717742"
arturl: "https://blog.csdn.net/moranxiao199/article/details/150717742"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150717742
    alt: "数据挖掘-4.8-评估泛化能力"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150717742
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150717742
cover: https://bing.ee123.net/img/rand?artid=150717742
image: https://bing.ee123.net/img/rand?artid=150717742
img: https://bing.ee123.net/img/rand?artid=150717742
---



# 数据挖掘 4.8 评估泛化能力



4.8 Estimating Generalization  
 4.8 评估泛化能力

## 如何合理评估模型的泛化能力

### 指导原则 (Guidelines)

训练集要有 足够多的样本 留下，不要全部拿去训练。  
 测试集的标签 在训练过程中绝对不能使用（不能直接用，也不能间接泄漏）。但是，测试数据本身（不带标签）可以用，比如做无监督的分布分析。  
 要明确模型的 应用场景和目的（intended use & application）。  
 要清楚性能评估的 目标（objective of evaluation），比如是比较算法还是用于实际部署。

### 存在的问题 (Issues)

测试集变小 → 估计的方差变大（因为样本少，评估结果波动更大）  
 训练集变小 → 偏差（bias）增加（数据不足，模型性能评估更悲观）

## K-fold 交叉验证（Cross-Validation)

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dcd9835a61264979afded254ce29864b.png#pic_center)

测量泛化性能 (Measurement of Generalization Performance)，用于估计性能的方差 (For estimation of variation)，将数据划分为 K 折 (Divide the data into K folds)

* 对于每个 k = 1…K
  + 在 K-1 个子集上训练，留出第 k 个子集作为验证集
  + 在第 k 个子集上验证，并计算性能指标
* 汇总所有 K 次实验的 **平均值** 和 **方差**

不同的K值会如何影响结果，如何影响平均准确率或平均性能指标？  
 当训练数据很小的时候，使用较大的K值； 当训练数据很大的时候，使用较小的K值。

### 留一交叉验证（Leave One Out CV）(LOOCV)

如果 
K
=
样本数
K=样本数
K=样本数，这种极端情况称为留一交叉验证（LOOCV）。适用于数据量很小的情况。

### Stratification 分层

分层（分层交叉验证）– 确保每一折包含的样本数量与整体数据相同。  
 如果某个类别在整个数据集中占 20% 的样本，那么在从数据集中抽取的所有样本中，它也应该占大约 20% 的样本。  
 **举例**  
 一个数据集合，1000个数据，800个负例，200个正例。5-fold 交叉验证。每个样本fold，都要有80%的负例，20%的正例。

如果我们保证这一点，我们本质上所做的就说分层。与非分层交叉验证相比，分层交叉验证是一种更好的评估交叉验证性能的方法。

## 训练-验证-测试拆分（training validation test split）

（10-fold cross-validation）10倍交叉验证是很好常用的。如果有10,000个或5,000样本，我们可以使用10倍交叉验证，将会提供可靠的估计值。  
 但如果有大量或者少数样本作为训练，这是比较棘手的，需要使用其他方法。如果样本数量非常大，我们继续使用10倍交叉验证，将花费很长时间。因此，这种情况，我们通常会将数据集拆分成为 训练数据集（training dataset）、验证数据集（validation dataset）、测试数据集（testing dataset）。常用占比是60%、20%、20%。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4131620cb8ce4b378f458dc3ba50440b.png#pic_center)

## 引导策略（bootstrap）

当样本数量非常少时，事情就会变得复杂。比如只有100个样本可以用于训练。通常情况下，训练样本数量很少，标记就很困难。例如，如果是生物或者生物医学信息学领域，样本数量可能非常少。因为标记成本很高。比如蛋白质结构测定实验的成本很容易达到10万美元。所以这会成为一个问题，如果样本数量非常少，那准确率的估计就会更加困难。

如果样本少，可以使用Lemon-outcross validation柠檬杂交实验，还有别的策略，这些策略称为 引导策略（bootstrap or bootstrapping），可以实现。  
 （这里不细说）



