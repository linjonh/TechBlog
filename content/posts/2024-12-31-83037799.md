---
layout: post
title: "强化学习入门总结"
date: 2024-12-31 20:02:07 +0800
description: "+次，点赞255次，收藏2k次。目录一、强化学习概述1.强化学习简介2.发展历程：3.MDP（马儿可"
keywords: "强化学习"
categories: ['强化学习']
tags: ['策略迭代', '强化学习', '值迭代', 'Mdp']
artid: "83037799"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=83037799
    alt: "强化学习入门总结"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=83037799
featuredImagePreview: https://bing.ee123.net/img/rand?artid=83037799
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     强化学习入门总结
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p id="main-toc">
     <strong>
      目录
     </strong>
    </p>
    <p id="%E4%B8%80%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0-toc" style="margin-left:0px;">
     <a href="#%E4%B8%80%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0" rel="nofollow">
      一、强化学习概述
     </a>
    </p>
    <p id="1.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B-toc" style="margin-left:40px;">
     <a href="#1.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B" rel="nofollow">
      1.强化学习简介
     </a>
    </p>
    <p id="2.%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#2.%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%EF%BC%9A" rel="nofollow">
      2.发展历程：
     </a>
    </p>
    <p id="3.MDP%EF%BC%88%E9%A9%AC%E5%84%BF%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%89-toc" style="margin-left:40px;">
     <a href="#3.MDP%EF%BC%88%E9%A9%AC%E5%84%BF%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%89" rel="nofollow">
      3.MDP（马儿可夫决策过程）
     </a>
    </p>
    <p id="4.why%20RL%EF%BC%9F-toc" style="margin-left:40px;">
     <a href="#4.why%20RL%EF%BC%9F" rel="nofollow">
      4.why RL？
     </a>
    </p>
    <p id="5.%E6%80%BB%E7%BB%93%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#5.%E6%80%BB%E7%BB%93%EF%BC%9A" rel="nofollow">
      5.总结：
     </a>
    </p>
    <p id="%E4%BA%8C%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95-toc" style="margin-left:0px;">
     <a href="#%E4%BA%8C%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95" rel="nofollow">
      二、强化学习求解方法
     </a>
    </p>
    <p id="1.%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;">
     <a href="#1.%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95" rel="nofollow">
      1.动态规划方法
     </a>
    </p>
    <p id="2.%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;">
     <a href="#2.%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95" rel="nofollow">
      2.蒙特卡洛方法
     </a>
    </p>
    <p id="3.%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;">
     <a href="#3.%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95" rel="nofollow">
      3.时间差分方法
     </a>
    </p>
    <p id="%E4%B8%89%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB-toc" style="margin-left:0px;">
     <a href="#%E4%B8%89%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB" rel="nofollow">
      三、强化学习算法分类
     </a>
    </p>
    <p id="1.%E5%88%86%E7%B1%BB%E4%B8%80%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#1.%E5%88%86%E7%B1%BB%E4%B8%80%EF%BC%9A" rel="nofollow">
      1.分类一：
     </a>
    </p>
    <p id="2.%E5%88%86%E7%B1%BB%E4%BA%8C%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#2.%E5%88%86%E7%B1%BB%E4%BA%8C%EF%BC%9A" rel="nofollow">
      2.分类二：
     </a>
    </p>
    <p id="3.%E5%88%86%E7%B1%BB%E4%B8%89%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#3.%E5%88%86%E7%B1%BB%E4%B8%89%EF%BC%9A" rel="nofollow">
      3.分类三：
     </a>
    </p>
    <p id="4.%E5%88%86%E7%B1%BB%E5%9B%9B%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#4.%E5%88%86%E7%B1%BB%E5%9B%9B%EF%BC%9A" rel="nofollow">
      4.分类四：
     </a>
    </p>
    <p id="%E5%9B%9B%E3%80%81%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%AE%97%E6%B3%95-toc" style="margin-left:0px;">
     <a href="#%E5%9B%9B%E3%80%81%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%AE%97%E6%B3%95" rel="nofollow">
      四、代表性算法
     </a>
    </p>
    <p id="1.Q-learning-toc" style="margin-left:40px;">
     <a href="#1.Q-learning" rel="nofollow">
      1.Q-learning
     </a>
    </p>
    <p id="2.Sarsa%3A-toc" style="margin-left:40px;">
     <a href="#2.Sarsa%3A" rel="nofollow">
      2.Sarsa:
     </a>
    </p>
    <p id="3.%E5%A4%A7%E5%90%8D%E9%BC%8E%E9%BC%8E%E7%9A%84DQN-toc" style="margin-left:40px;">
     <a href="#3.%E5%A4%A7%E5%90%8D%E9%BC%8E%E9%BC%8E%E7%9A%84DQN" rel="nofollow">
      3.大名鼎鼎的DQN
     </a>
    </p>
    <p id="4.Policy%20Gradients%E7%AE%97%E6%B3%95-toc" style="margin-left:40px;">
     <a href="#4.Policy%20Gradients%E7%AE%97%E6%B3%95" rel="nofollow">
      4.Policy Gradients算法
     </a>
    </p>
    <p id="5.Actor-critic-toc" style="margin-left:40px;">
     <a href="#5.Actor-critic" rel="nofollow">
      5.Actor-critic
     </a>
    </p>
    <p id="%E4%BA%94%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%EF%BC%9A-toc" style="margin-left:0px;">
     <a href="#%E4%BA%94%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%EF%BC%9A" rel="nofollow">
      五、强化学习应用：
     </a>
    </p>
    <p id="%EF%BC%881%EF%BC%89%E5%90%84%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#%EF%BC%881%EF%BC%89%E5%90%84%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8%EF%BC%9A" rel="nofollow">
      （1）各领域应用：
     </a>
    </p>
    <p id="%EF%BC%882%EF%BC%89%E5%AF%B9%E8%AF%9D-toc" style="margin-left:40px;">
     <a href="#%EF%BC%882%EF%BC%89%E5%AF%B9%E8%AF%9D" rel="nofollow">
      （2）对话
     </a>
    </p>
    <p id="%EF%BC%883%EF%BC%89%E6%B7%98%E5%AE%9D%E7%94%B5%E5%95%86%E6%90%9C%E7%B4%A2%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#%EF%BC%883%EF%BC%89%E6%B7%98%E5%AE%9D%E7%94%B5%E5%95%86%E6%90%9C%E7%B4%A2%EF%BC%9A" rel="nofollow">
      （3）淘宝电商搜索：
     </a>
    </p>
    <p id="%EF%BC%884%EF%BC%89FlappyBird%3A-toc" style="margin-left:40px;">
     <a href="#%EF%BC%884%EF%BC%89FlappyBird%3A" rel="nofollow">
      （4）FlappyBird:
     </a>
    </p>
    <p id="%EF%BC%885%EF%BC%89%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%88TSP%EF%BC%89%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#%EF%BC%885%EF%BC%89%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%88TSP%EF%BC%89%EF%BC%9A" rel="nofollow">
      （5）组合优化问题（TSP）：
     </a>
    </p>
    <p id="%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%EF%BC%9A-toc" style="margin-left:0px;">
     <a href="#%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%EF%BC%9A" rel="nofollow">
      六、总结：
     </a>
    </p>
    <p id="1.%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%AE%97%E6%B3%95%EF%BC%9A-toc" style="margin-left:40px;">
     <a href="#1.%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%AE%97%E6%B3%95%EF%BC%9A" rel="nofollow">
      1.如何设计算法：
     </a>
    </p>
    <hr id="hr-toc"/>
    <h2 id="%E4%B8%80%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0">
     一、强化学习概述
    </h2>
    <h3 id="1.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B">
     1.强化学习简介
    </h3>
    <p>
     <span style="color:#000000;">
      （1）
     </span>
     <span style="color:#ff0000;">
      强化学习是机器学习
     </span>
     <span style="color:#ff0000;">
      中的一个领域
     </span>
     <span style="color:#000000;">
      ，强调如何基于环境而行动，以取得最大化的预期利益。其灵感来源于心理学中的行为主义理论，即有机体如何在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p>
     <img alt="" class="has" height="308" src="https://i-blog.csdnimg.cn/blog_migrate/08bed36067e5deaddf38c4d310c1eb73.png" width="275"/>
    </p>
    <p>
     <span style="color:#000000;">
      （2）强化学习最早可以追溯到巴甫洛
     </span>
     <span style="color:#000000;">
      夫的条件反射实验，它从动
     </span>
     <span style="color:#000000;">
      物行为研究和优化控制两个领域独立发展
     </span>
     <span style="color:#000000;">
      ，最终经
     </span>
     <span style="color:#000000;">
      Bellman
     </span>
     <span style="color:#000000;">
      之手将其抽象为
     </span>
     <span style="color:#ff0000;">
      马尔可夫决策过程
     </span>
     <span style="color:#000000;">
      (Markov Decision Process
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#ff0000;">
      MDP
     </span>
     <span style="color:#000000;">
      )
     </span>
    </p>
    <p>
     <img alt="" class="has" height="34" src="https://i-blog.csdnimg.cn/blog_migrate/92db3e71ad8c8714db3e734f33f46a68.png" width="263"/>
    </p>
    <h3 id="2.%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B%EF%BC%9A">
     <span style="color:#000000;">
      2.发展历程：
     </span>
    </h3>
    <p>
     <span style="color:#ff0000;">
      1956
     </span>
     <span style="color:#ff0000;">
      年
     </span>
     <span style="color:#ff0000;">
      Bellman
     </span>
     <span style="color:#ff0000;">
      提出了动态规划方法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      1977
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      Werbos
     </span>
     <span style="color:#000000;">
      提出只适应动态规划算法。
     </span>
    </p>
    <p>
     <span style="color:#ff0000;">
      1988
     </span>
     <span style="color:#ff0000;">
      年
     </span>
     <span style="color:#ff0000;">
      sutton
     </span>
     <span style="color:#ff0000;">
      提出时间差分算法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      1992
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      Watkins
     </span>
     <span style="color:#000000;">
      提出
     </span>
     <span style="color:#000000;">
      Q-learning
     </span>
     <span style="color:#000000;">
      算法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      1994
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      rummery
     </span>
     <span style="color:#000000;">
      提出
     </span>
     <span style="color:#000000;">
      Saras
     </span>
     <span style="color:#000000;">
      算法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      1996
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      Bersekas
     </span>
     <span style="color:#000000;">
      提出解决随机过程中优化控制的神经动态规划方法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      2006
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      Kocsis
     </span>
     <span style="color:#000000;">
      提出了置信上限树算法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      2009
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      kewis
     </span>
     <span style="color:#000000;">
      提出反馈控制只适应动态规划算法。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      2014
     </span>
     <span style="color:#000000;">
      年
     </span>
     <span style="color:#000000;">
      silver
     </span>
     <span style="color:#000000;">
      提出确定性策略梯度（
     </span>
     <span style="color:#000000;">
      Policy
     </span>
     <span style="color:#000000;">
      Gradients
     </span>
     <span style="color:#000000;">
      ）算法。
     </span>
    </p>
    <p>
     <span style="color:#ff0000;">
      2015
     </span>
     <span style="color:#ff0000;">
      年
     </span>
     <span style="color:#ff0000;">
      Google-
     </span>
     <span style="color:#ff0000;">
      deepmind
     </span>
     <span style="color:#ff0000;">
      提出
     </span>
     <span style="color:#ff0000;">
      Deep-Q-Network
     </span>
     <span style="color:#ff0000;">
      算法
     </span>
     <span style="color:#ff0000;">
      。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      可见，强化学习已经发展了几十年，并不是一门新的技术。在2016年，AlphaGo击败李世石之后，融合了深度学习的强化学习技术大放异彩，成为这两年最火的技术之一。总结来说，强化学习就是一个古老而又时尚的技术。
     </span>
    </p>
    <h3 id="3.MDP%EF%BC%88%E9%A9%AC%E5%84%BF%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%89">
     <span style="color:#000000;">
      3.MDP（马儿可夫决策过程）
     </span>
    </h3>
    <p>
     <img alt="" class="has" height="30" src="https://i-blog.csdnimg.cn/blog_migrate/31a4c02bf7ad2e3ee91ca76d89b4a111.png" width="361"/>
    </p>
    <p>
     <span style="color:#000000;">
      S:
     </span>
     <span style="color:#000000;">
      表示状态集
     </span>
     <span style="color:#000000;">
      (states)
     </span>
     <span style="color:#000000;">
      ，有
     </span>
     <span style="color:#000000;">
      s∈S
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#000000;">
      si
     </span>
     <span style="color:#000000;">
      表示第
     </span>
     <span style="color:#000000;">
      i
     </span>
     <span style="color:#000000;">
      步的状态。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      A:
     </span>
     <span style="color:#000000;">
      表示一组动作
     </span>
     <span style="color:#000000;">
      (actions)
     </span>
     <span style="color:#000000;">
      ，有
     </span>
     <span style="color:#000000;">
      a∈A
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#000000;">
      ai
     </span>
     <span style="color:#000000;">
      表示第
     </span>
     <span style="color:#000000;">
      i
     </span>
     <span style="color:#000000;">
      步的动作。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      ?
     </span>
     <span style="color:#000000;">
      sa
     </span>
     <span style="color:#000000;">
      :
     </span>
     <span style="color:#000000;">
      表示状态转移概率。?
     </span>
     <span style="color:#000000;">
      s?
     </span>
     <span style="color:#000000;">
      表示的是在当前
     </span>
     <span style="color:#000000;">
      s ∈ S
     </span>
     <span style="color:#000000;">
      状态下，经过
     </span>
     <span style="color:#000000;">
      a ∈ A
     </span>
     <span style="color:#000000;">
      作用后，会转移到的其他状态的概率分布情况。比如，在状态
     </span>
     <span style="color:#000000;">
      s
     </span>
     <span style="color:#000000;">
      下执行动作
     </span>
     <span style="color:#000000;">
      a
     </span>
     <span style="color:#000000;">
      ，转移到
     </span>
     <span style="color:#000000;">
      s'
     </span>
     <span style="color:#000000;">
      的概率可以表示为
     </span>
     <span style="color:#000000;">
      p(s'|
     </span>
     <span style="color:#000000;">
      s,a
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      R: S×A⟼ℝ
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#000000;">
      R
     </span>
     <span style="color:#000000;">
      是回报函数
     </span>
     <span style="color:#000000;">
      (reward function)
     </span>
     <span style="color:#000000;">
      。有些回报函数状态
     </span>
     <span style="color:#000000;">
      S
     </span>
     <span style="color:#000000;">
      的函数，可以简化为
     </span>
     <span style="color:#000000;">
      R: S⟼ℝ
     </span>
     <span style="color:#000000;">
      。如果一组
     </span>
     <span style="color:#000000;">
      (
     </span>
     <span style="color:#000000;">
      s,a
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      转移到了下个状态
     </span>
     <span style="color:#000000;">
      s'
     </span>
     <span style="color:#000000;">
      ，那么回报函数可记为
     </span>
     <span style="color:#000000;">
      r(
     </span>
     <span style="color:#000000;">
      s'|s
     </span>
     <span style="color:#000000;">
      , a)
     </span>
     <span style="color:#000000;">
      。如果
     </span>
     <span style="color:#000000;">
      (
     </span>
     <span style="color:#000000;">
      s,a
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      对应的下个状态
     </span>
     <span style="color:#000000;">
      s'
     </span>
     <span style="color:#000000;">
      是唯一的，那么回报函数也可以记为
     </span>
     <span style="color:#000000;">
      r(
     </span>
     <span style="color:#000000;">
      s,a
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      γ
     </span>
     <span style="color:#000000;">
      :折现因子
     </span>
    </p>
    <h3 id="4.why%20RL%EF%BC%9F">
     4.why RL？
    </h3>
    <p>
     强化学习所解决的问题的特点：
    </p>
    <ul>
     <li>
      智能体和环境之间不断进行交互
     </li>
     <li>
      搜索和试错
     </li>
     <li>
      延迟奖励（当前所做的动作可能很多步之后才会产生相应的结果）
     </li>
    </ul>
    <p>
     目标：
    </p>
    <ul>
     <li>
      获取更多的累积奖励
     </li>
     <li>
      获得更可靠的估计
     </li>
    </ul>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      强化学习
     </span>
     <span style="color:#000000;">
      (Reinforcement Learning)
     </span>
     <span style="color:#000000;">
      是一个机器学习大家族中的分支
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      由于近些年来的技术突破
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      和深度学习
     </span>
     <span style="color:#000000;">
      (Deep Learning)
     </span>
     <span style="color:#000000;">
      的整合
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      使得强化学习有了进一步的运用
     </span>
     <span style="color:#000000;">
      。
     </span>
     <span style="color:#000000;">
      比如让计算机学着玩游戏
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      AlphaGo
     </span>
     <span style="color:#000000;">
      挑战世界围棋高手
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      都是强化学习在行的事
     </span>
     <span style="color:#000000;">
      。
     </span>
     <span style="color:#000000;">
      强化学习也是让你的程序从对当前环境完全陌生
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      成长为一个在环境中游刃有余的高手。
     </span>
    </p>
    <h3 id="5.%E6%80%BB%E7%BB%93%EF%BC%9A">
     5.总结：
    </h3>
    <p>
     <span style="color:#000000;">
      深度强化学习
     </span>
     <span style="color:#000000;">
      全称是
     </span>
     <span style="color:#000000;">
      Deep Reinforcement Learning
     </span>
     <span style="color:#000000;">
      （
     </span>
     <span style="color:#000000;">
      DRL
     </span>
     <span style="color:#000000;">
      ），其所带来的推理能力 是智能的一个关键特征衡量，真正的让机器有了自我学习、自我思考的能力。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      深度强化学习
     </span>
     <span style="color:#000000;">
      (Deep Reinforcement
     </span>
     <span style="color:#000000;">
      Learning
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#000000;">
      DRL)
     </span>
     <span style="color:#000000;">
      本质上属于采用神经网络作为值函数估计器的一类方法，其主要优势在于它能够利用深度神经网络对状态特征进行自动抽取，避免了人工 定义状态特征带来的不准确性，使得
     </span>
     <span style="color:#000000;">
      Agent
     </span>
     <span style="color:#000000;">
      能够在更原始的状态上进行学习。
     </span>
    </p>
    <h2 id="%E4%BA%8C%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E6%B1%82%E8%A7%A3%E6%96%B9%E6%B3%95">
     <span style="color:#000000;">
      二、强化学习求解方法
     </span>
    </h2>
    <h3 id="1.%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95">
     <span style="color:#000000;">
      1.动态规划方法
     </span>
    </h3>
    <p>
     <span style="color:#000000;">
      基石：贝尔曼方程
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      （1）贝尔曼方程：
     </span>
    </p>
    <p>
     <img alt="" class="has" height="54" src="https://i-blog.csdnimg.cn/blog_migrate/6dcc16701263c967bfcabd5be11ef5a6.png" width="292"/>
    </p>
    <p>
     <img alt="" class="has" height="33" src="https://i-blog.csdnimg.cn/blog_migrate/b4ee2d21fece51fd7363d2fae0ca8c1e.png" width="183"/>
    </p>
    <p>
     贝尔曼最优方程：
    </p>
    <p>
     <img alt="" class="has" height="50" src="https://i-blog.csdnimg.cn/blog_migrate/bf903291dfff0f5d0746c8bc9715fa04.png" width="352"/>
    </p>
    <p>
     <img alt="" class="has" height="50" src="https://i-blog.csdnimg.cn/blog_migrate/0b9aa3832100f4de14d5f65e520e63cf.png" width="291"/>
    </p>
    <p>
     如何求解贝尔曼方程呢？
    </p>
    <p>
     本质上就是个线性规划问题，多个方程，多个未知数，进行求解，如下，假设每一步的即使奖励是-0.2，有三个未知状态V(0,1),V(1,1),V(1,0),损失因子是1，则在如下的策略下，得到的贝尔曼方程如下：
    </p>
    <p>
     <img alt="" class="has" height="620" src="https://i-blog.csdnimg.cn/blog_migrate/af7656b5a1ffbddaaaed8239034f0af4.png" width="378"/>
    </p>
    <p>
     但是当未知变量不断增大，线性规划则很难求解，这时需要使用动态规划进行不断迭代，让其状态值收敛。
    </p>
    <p>
     （2）值迭代
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      In Value Iteration, you start with a
     </span>
     <span style="color:#000000;">
      randon
     </span>
     <span style="color:#000000;">
      value function and then find a new (improved) value function in a iterative process, until reaching the optimal value function. Notice that you can derive easily the optimal policy from the optimal value function. This process is based on the Optimality Bellman operator.
     </span>
    </p>
    <p>
     <img alt="" class="has" height="137" src="https://i-blog.csdnimg.cn/blog_migrate/a39a67e239ec398cbb5625ebfc483e6e.png" width="556"/>
    </p>
    <p>
     （3）策略迭代
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      In Policy Iteration algorithms, you start with a random policy, then find the value function of that policy (policy evaluation step), then find an new (improved) policy based on the previous value function, and so on. In this process, each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Given a policy, its value function can be obtained using the Bellman operator.
     </span>
    </p>
    <p>
     <img alt="" class="has" height="161" src="https://i-blog.csdnimg.cn/blog_migrate/6d3230dabe30e731ca19ddb03e83bc00.png" width="511"/>
    </p>
    <p>
     （4）算法详细对比：
    </p>
    <p>
     <img alt="" class="has" height="319" src="https://i-blog.csdnimg.cn/blog_migrate/f8c86aa114232befb89ba17b972ffd81.png" width="466"/>
    </p>
    <p>
    </p>
    <p>
     <img alt="" class="has" height="261" src="https://i-blog.csdnimg.cn/blog_migrate/922b4cc243fcf203d50f9f3cc465f19f.png" width="469"/>
    </p>
    <ul>
     <li>
      <span style="color:#000000;">
       根据策略迭代算法，每一次迭代都要进行策略评估和策略提升，直到二者都收敛。可我们的目标是选出最优的策略，那么有没有可能在策略评估值没有收敛的情况下，最优策略已经收敛了呢？答案是有这个可能
      </span>
     </li>
     <li>
      <span style="color:#000000;">
       策略迭代的收敛速度更快一些，在状态空间较小时，最好选用策略迭代方法。当状态空间较大时，值迭代的计算量更小一些
      </span>
     </li>
    </ul>
    <p>
     （5）GridWorld举例（分别用策略迭代和值迭代进行求解）
    </p>
    <p>
     <img alt="" class="has" height="249" src="https://i-blog.csdnimg.cn/blog_migrate/a078221899482b78be794d8752f94491.png" width="499"/>
    </p>
    <p>
     策略迭代过程：
    </p>
    <p>
     <img alt="" class="has" height="706" src="https://i-blog.csdnimg.cn/blog_migrate/16272a6b5809cfd926a194aff5f35ec3.png" width="714"/>
    </p>
    <p>
     值迭代过程：
    </p>
    <p>
     <img alt="" class="has" height="562" src="https://i-blog.csdnimg.cn/blog_migrate/ca4ac33bbf892896864b1fc2f7f8a3bb.png" width="719"/>
    </p>
    <h3 id="2.%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95">
     <span style="color:#000000;">
      2.蒙特卡洛方法
     </span>
    </h3>
    <p>
     <span style="color:#000000;">
      上面的动态规划方法，是一种较为理想的状态，即所有的参数都提前知道，比如状态转移概率，及奖励等等。然而显示情况是未知的，这时候有一种手段是采用蒙特卡洛采样，基于大数定律，基于统计计算出转移概率值；比如当你抛硬币的次数足够多，那么正面和反面的概率将会越来越接近真实情况。
     </span>
    </p>
    <h3 id="3.%E6%97%B6%E9%97%B4%E5%B7%AE%E5%88%86%E6%96%B9%E6%B3%95">
     <span style="color:#000000;">
      3.时间差分方法
     </span>
    </h3>
    <p>
     基于动态规划和蒙特卡洛
    </p>
    <h2 id="%E4%B8%89%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB">
     三、强化学习算法分类
    </h2>
    <h3 id="1.%E5%88%86%E7%B1%BB%E4%B8%80%EF%BC%9A">
     1.分类一：
    </h3>
    <p>
     <img alt="" class="has" height="295" src="https://i-blog.csdnimg.cn/blog_migrate/40d6695fd02755f4eec67e3da5b01ec4.png" width="522"/>
    </p>
    <p>
     <span style="color:#000000;">
      基于理不理解所处环境来进行分类：
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      Model-free:环境给了我们什么就是什么
     </span>
     <span style="color:#000000;">
      .
     </span>
     <span style="color:#000000;">
      我们就把这种方法叫做
     </span>
     <span style="color:#000000;">
      model-free,
     </span>
     <span style="color:#000000;">
      这里的
     </span>
     <span style="color:#000000;">
      model
     </span>
     <span style="color:#000000;">
      就是用模型来表示环境
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      M
     </span>
     <span style="color:#000000;">
      odel
     </span>
     <span style="color:#000000;">
      -based:
     </span>
     <span style="color:#000000;">
      那理解了环境也就是学会了用一个模型来代表环境
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      所以这种就是
     </span>
     <span style="color:#000000;">
      model-based
     </span>
     <span style="color:#000000;">
      方法
     </span>
    </p>
    <h3 id="2.%E5%88%86%E7%B1%BB%E4%BA%8C%EF%BC%9A">
     2.分类二：
    </h3>
    <p>
     <img alt="" class="has" height="293" src="https://i-blog.csdnimg.cn/blog_migrate/c2e4e24c930b38d7a806f88800cc994c.png" width="522"/>
    </p>
    <p>
     一类是直接输出各个动作概率，另一个是输出每个动作的价值；前者适用于连续动作情况，后者无法表示连续动作的价值。
    </p>
    <h3 id="3.%E5%88%86%E7%B1%BB%E4%B8%89%EF%BC%9A">
     3.分类三：
    </h3>
    <p>
     <img alt="" class="has" height="290" src="https://i-blog.csdnimg.cn/blog_migrate/ba3da6e454de95513526d0849d918789.png" width="513"/>
    </p>
    <h3 id="4.%E5%88%86%E7%B1%BB%E5%9B%9B%EF%BC%9A">
     4.分类四：
    </h3>
    <p>
     <img alt="" class="has" height="255" src="https://i-blog.csdnimg.cn/blog_migrate/5fadc31af853f34abe61b876da1a3452.png" width="454"/>
    </p>
    <p>
     <span style="color:#000000;">
      判断
     </span>
     <span style="color:#000000;">
      on-policy
     </span>
     <span style="color:#000000;">
      和
     </span>
     <span style="color:#000000;">
      off-
     </span>
     <span style="color:#000000;">
      policy
     </span>
     <span style="color:#000000;">
      的
     </span>
     <span style="color:#ff0000;">
      关键
     </span>
     <span style="color:#000000;">
      在于
     </span>
     <span style="color:#000000;">
      ，你所估计的
     </span>
     <span style="color:#000000;">
      policy或者value-
     </span>
     <span style="color:#000000;">
      function
     </span>
     <span style="color:#000000;">
      和你生成样本时所采用的
     </span>
     <span style="color:#ff0000;">
      policy
     </span>
     <span style="color:#ff0000;">
      是不是一样
     </span>
     <span style="color:#000000;">
      。
     </span>
     <span style="color:#000000;">
      如果一样
     </span>
     <span style="color:#000000;">
      ，那就是
     </span>
     <span style="color:#000000;">
      on-policy
     </span>
     <span style="color:#000000;">
      的，否则是
     </span>
     <span style="color:#000000;">
      off-policy
     </span>
     <span style="color:#000000;">
      的。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      总结各常用算法的分类：
     </span>
    </p>
    <p>
     <img alt="" class="has" height="193" src="https://i-blog.csdnimg.cn/blog_migrate/e3b32e8be7a61899c58f286bbd8fc1e0.png" width="720"/>
    </p>
    <h2 id="%E5%9B%9B%E3%80%81%E4%BB%A3%E8%A1%A8%E6%80%A7%E7%AE%97%E6%B3%95">
     <span style="color:#000000;">
      四、代表性算法
     </span>
    </h2>
    <h3 id="1.Q-learning">
     1.Q-learning
    </h3>
    <p>
     （1）四个基本组成成分：
    </p>
    <ul>
     <li>
      Q表：Q(s,a),状态s下执行动作a的累积价值
     </li>
     <li>
      定义动作：选择动作
     </li>
     <li>
      环境反馈：
      <span style="color:#000000;">
       做出行为后，环境的反馈
      </span>
     </li>
     <li>
      环境更新
     </li>
    </ul>
    <p>
     （2）算法公式：
    </p>
    <p>
     <img alt="" class="has" height="200" src="https://i-blog.csdnimg.cn/blog_migrate/830cd70d59ecd0f8ff8c5583ad488158.png" width="455"/>
    </p>
    <p>
     （3）算法决策：
    </p>
    <p>
     <img alt="" class="has" height="238" src="https://i-blog.csdnimg.cn/blog_migrate/668fddd19e0f08f5460f44ec3ee21117.png" width="425"/>
    </p>
    <p>
     （4）算法更新：
    </p>
    <p>
     <img alt="" class="has" height="242" src="https://i-blog.csdnimg.cn/blog_migrate/ceb8ea6c5ebc0dcd196102d8b038e20e.png" width="434"/>
    </p>
    <p>
     （5）代码实现框架：
    </p>
    <p>
     <img alt="" class="has" height="285" src="https://i-blog.csdnimg.cn/blog_migrate/3f1e7e2e22b95ba243a320a032bcf45e.png" width="452"/>
    </p>
    <h3 id="2.Sarsa%3A">
     2.Sarsa:
    </h3>
    <p>
     <span style="color:#000000;">
      与
     </span>
     <span style="color:#000000;">
      Q-learning
     </span>
     <span style="color:#000000;">
      基本类似，唯一的区别是更新方式不一样
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      （1）算法公式：
     </span>
    </p>
    <p>
     <img alt="" class="has" height="229" src="https://i-blog.csdnimg.cn/blog_migrate/98db67b9e77fbb7808bf9dba1a55c352.png" width="637"/>
    </p>
    <p>
     <span style="color:#000000;">
      （2）与Q-learning的区别：Sarsa是on-policy的，Q-learning是off-policy的
     </span>
    </p>
    <p>
     <img alt="" class="has" height="480" src="https://i-blog.csdnimg.cn/blog_migrate/61d8f0705e965fbb20b66248b608676a.png" width="647"/>
    </p>
    <p>
     （3）更新过程：
    </p>
    <p>
     <img alt="" class="has" height="192" src="https://i-blog.csdnimg.cn/blog_migrate/2ecaa00e9ff585fc9b55b18bf2015ce9.png" width="342"/>
     <img alt="" class="has" height="192" src="https://i-blog.csdnimg.cn/blog_migrate/2462d1b040bafa9e278360fa15ab88d3.png" width="338"/>
    </p>
    <p>
     前者是Sarsa，后者是Q-learning
    </p>
    <p>
     （4）代码中展现不同：
    </p>
    <p>
     Sarsa:
    </p>
    <p>
     <img alt="" class="has" height="141" src="https://i-blog.csdnimg.cn/blog_migrate/fe0a337ae0b6dcc5fadd401b05f424c7.png" width="652"/>
    </p>
    <p>
     Q-learning:
    </p>
    <p>
     <img alt="" class="has" height="146" src="https://i-blog.csdnimg.cn/blog_migrate/3b370bdabe38a3763741f708b847b7de.png" width="650"/>
    </p>
    <p>
     （5）代码实现框架：
    </p>
    <p>
     <img alt="" class="has" height="297" src="https://i-blog.csdnimg.cn/blog_migrate/c6ef73c891ef3382c468b16c00fba542.png" width="438"/>
    </p>
    <h3 id="3.%E5%A4%A7%E5%90%8D%E9%BC%8E%E9%BC%8E%E7%9A%84DQN">
     3.大名鼎鼎的DQN
    </h3>
    <p>
     Deepmind就是因为DQN这篇论文，被谷歌收购
    </p>
    <p>
     （1）由来：
    </p>
    <ul>
     <li>
      <p>
       <span style="color:#ff0000;">
        <strong>
         DQN
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         （
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         Deep
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         Q
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         Network
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         ）是一种融合了神经网络和
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         Q
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         learning
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         的方法
        </strong>
       </span>
       <span style="color:#ff0000;">
        <strong>
         .
        </strong>
       </span>
      </p>
     </li>
     <li>
      <p>
       <span style="color:#000000;">
        有些问题太复杂，
       </span>
       <span style="color:#000000;">
        Q
       </span>
       <span style="color:#000000;">
        表无法存储，即使可以存储，搜索也很麻烦。故而，将
       </span>
       <span style="color:#000000;">
        Q
       </span>
       <span style="color:#000000;">
        表用神经网络进行替代。
       </span>
      </p>
     </li>
    </ul>
    <p>
     <img alt="" class="has" height="166" src="https://i-blog.csdnimg.cn/blog_migrate/00bd4715515e28ce8d1c461369ec85c2.png" width="332"/>
    </p>
    <p>
     （2）增加了两个新特性：
    </p>
    <p>
     <span style="color:#ff0000;">
      Experience
     </span>
     <span style="color:#ff0000;">
      replay
     </span>
     <span style="color:#000000;">
      ：每次
     </span>
     <span style="color:#000000;">
      DQN
     </span>
     <span style="color:#000000;">
      更新的时候，随机抽取一些之前的经历进行学习。随机抽取这种打乱了经历之间的相关性，使得神经网络更新更有效率。
     </span>
    </p>
    <p>
     <span style="color:#ff0000;">
      Fixed Q-
     </span>
     <span style="color:#ff0000;">
      targets
     </span>
     <span style="color:#000000;">
      ：使用两个结构相同但参数不同的神经网络，预测
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      估计的神经网络具备最新的参数，而预测Q
     </span>
     <span style="color:#000000;">
      现实的神经网络使用的参数则是很久以前的。
     </span>
    </p>
    <p>
     （3）算法公式：
    </p>
    <p>
     <img alt="" class="has" height="391" src="https://i-blog.csdnimg.cn/blog_migrate/ddf9252bfa7066302fc80637d1223bb8.png" width="494"/>
    </p>
    <p>
     paper地址：
     <span style="color:#000000;">
      <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">
       Playing
      </a>
     </span>
     <span style="color:#000000;">
      <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">
       Atari with
      </a>
     </span>
     <span style="color:#000000;">
      <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">
       Deep
      </a>
     </span>
     <span style="color:#000000;">
      <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">
       Reinforcement
      </a>
     </span>
     <span style="color:#000000;">
      <a href="https://arxiv.org/pdf/1312.5602.pdf" rel="nofollow">
       Learning
      </a>
     </span>
    </p>
    <p>
     （4）DQN代码实现框架：
    </p>
    <p>
     <img alt="" class="has" height="406" src="https://i-blog.csdnimg.cn/blog_migrate/10b58809dba4551f862f510aebdc8d1a.png" width="463"/>
    </p>
    <p>
     可以看到基本上Q-learning一模一样，只是有一处不一样，就是多了存储记忆，并且批量进行学习。
    </p>
    <h3 id="4.Policy%20Gradients%E7%AE%97%E6%B3%95">
     4.Policy Gradients算法
    </h3>
    <p>
     （1）由来：
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      强化学习是一个通过奖惩来学习正确行为的机制
     </span>
     <span style="color:#000000;">
      .
     </span>
     <span style="color:#000000;">
      家族中有很多种不一样的成员
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      有学习奖惩值
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      根据自己认为的高价值选行为
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      比如
     </span>
     <span style="color:#000000;">
      Q learning, Deep Q Network,
     </span>
     <span style="color:#000000;">
      也有不通过分析奖励值
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      直接输出行为的方法
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      这就是今天要说的
     </span>
     <span style="color:#000000;">
      Policy Gradients
     </span>
     <span style="color:#000000;">
      了
     </span>
     <span style="color:#000000;">
      .
     </span>
     <span style="color:#000000;">
      甚至我们可以为
     </span>
     <span style="color:#000000;">
      Policy Gradients
     </span>
     <span style="color:#000000;">
      加上一个神经网络来输出预测的动作
     </span>
     <span style="color:#000000;">
      .
     </span>
     <span style="color:#000000;">
      对比起以值为基础的方法
     </span>
     <span style="color:#000000;">
      , Policy Gradients
     </span>
     <span style="color:#000000;">
      直接输出动作的最大好处就是
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      它能在一个连续区间内挑选动作
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      而基于值的
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      比如
     </span>
     <span style="color:#000000;">
      Q-learning,
     </span>
     <span style="color:#000000;">
      它如果在无穷多的动作中计算价值
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      从而选择行为
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      这
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      它可吃不消
     </span>
     <span style="color:#000000;">
      .
     </span>
    </p>
    <p style="margin-left:0in;">
     <img alt="" class="has" height="186" src="https://i-blog.csdnimg.cn/blog_migrate/b5c79ccf7f0c32f3781d6977fa07e863.png" width="331"/>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      （2）区别：
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      Q-learning
     </span>
     <span style="color:#000000;">
      、
     </span>
     <span style="color:#000000;">
      DQN
     </span>
     <span style="color:#000000;">
      ：学习奖惩值
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      根据自己认为
     </span>
     <span style="color:#000000;">
      的高价值选行为。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      Policy
     </span>
     <span style="color:#000000;">
      Gradients
     </span>
     <span style="color:#000000;">
      ：不通过分析奖励值
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      直接输出行为的
     </span>
     <span style="color:#000000;">
      方法
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      Policy Gradients
     </span>
     <span style="color:#000000;">
      直接输出动作的最大好处就是
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#ff0000;">
      它能在一个连续区间内挑选动作
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      而基于值的
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      比如
     </span>
     <span style="color:#000000;">
      Q-learning,
     </span>
     <span style="color:#000000;">
      它如果在无穷多的动作中计算价值
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      从而选择行为
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      这
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      它可吃不消
     </span>
     <span style="color:#000000;">
      .
     </span>
    </p>
    <p>
     （3）更新：
    </p>
    <p>
     <img alt="" class="has" height="211" src="https://i-blog.csdnimg.cn/blog_migrate/215c7d6e109296935a8690b499068af9.png" width="403"/>
    </p>
    <p>
     （4）REINFORCE算法公式：
    </p>
    <p>
     <img alt="" class="has" height="144" src="https://i-blog.csdnimg.cn/blog_migrate/e3aca97b8ae0e31c5372ac88f80683ce.png" width="335"/>
    </p>
    <p>
     推到步骤的博客：
     <span style="color:#000000;">
      <a href="https://blog.csdn.net/sysstc/article/details/77189703">
       https://blog.csdn.net/
      </a>
     </span>
     <span style="color:#000000;">
      <a href="https://blog.csdn.net/sysstc/article/details/77189703">
       sysstc
      </a>
     </span>
     <span style="color:#000000;">
      <a href="https://blog.csdn.net/sysstc/article/details/77189703">
       /article/details/77189703
      </a>
     </span>
    </p>
    <h3 id="5.Actor-critic">
     5.Actor-critic
    </h3>
    <p>
     （1）由来：
    </p>
    <p>
     <span style="color:#000000;">
      结合了
     </span>
     <span style="color:#000000;">
      Policy Gradient (Actor)
     </span>
     <span style="color:#000000;">
      和
     </span>
     <span style="color:#000000;">
      Function Approximation (Critic)
     </span>
     <span style="color:#000000;">
      的方法
     </span>
     <span style="color:#000000;">
      . Actor
     </span>
     <span style="color:#000000;">
      基于概率选行为
     </span>
     <span style="color:#000000;">
      , Critic
     </span>
     <span style="color:#000000;">
      基于
     </span>
     <span style="color:#000000;">
      Actor
     </span>
     <span style="color:#000000;">
      的行为评判行为的得分
     </span>
     <span style="color:#000000;">
      , Actor
     </span>
     <span style="color:#000000;">
      根据
     </span>
     <span style="color:#000000;">
      Critic
     </span>
     <span style="color:#000000;">
      的评分修改选行为的概率
     </span>
     <span style="color:#000000;">
      .
     </span>
    </p>
    <p>
     （2）特点：
    </p>
    <p>
     <img alt="" class="has" height="182" src="https://i-blog.csdnimg.cn/blog_migrate/50eb5a5818e20841dfbe992d41023750.png" width="320"/>
    </p>
    <p>
     <span style="color:#000000;">
      Policy
     </span>
     <span style="color:#000000;">
      Gradients
     </span>
     <span style="color:#000000;">
      +
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      -learning
     </span>
     <span style="color:#000000;">
      ：既可以在连续动作中取合适的动作，又可以进行单步更新。
     </span>
    </p>
    <p>
     （3）与Policy Gradients的区别：
    </p>
    <p>
     <span style="color:#000000;">
      Policy
     </span>
     <span style="color:#000000;">
      gradient
     </span>
     <span style="color:#000000;">
      是回合后进行奖惩计算，前期没有值函数，需要后向推算；
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      Actor
     </span>
     <span style="color:#000000;">
      -
     </span>
     <span style="color:#000000;">
      critic
     </span>
     <span style="color:#000000;">
      多了一个
     </span>
     <span style="color:#000000;">
      critic
     </span>
     <span style="color:#000000;">
      方法，可以用来每一步进行一个好坏判断；
     </span>
    </p>
    <p>
     （4）更新
    </p>
    <p>
     <img alt="" class="has" height="171" src="https://i-blog.csdnimg.cn/blog_migrate/1770b2defa9c759d1d85d518b162387b.png" width="328"/>
    </p>
    <p>
     <span style="color:#000000;">
      Actor Critic
     </span>
     <span style="color:#000000;">
      方法的劣势
     </span>
     <span style="color:#000000;">
      :
     </span>
     <span style="color:#000000;">
      取决于
     </span>
     <span style="color:#000000;">
      Critic
     </span>
     <span style="color:#000000;">
      的价值
     </span>
     <span style="color:#000000;">
      判断
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      但是
     </span>
     <span style="color:#000000;">
      Critic
     </span>
     <span style="color:#000000;">
      难收敛
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      再加上
     </span>
     <span style="color:#000000;">
      Actor
     </span>
     <span style="color:#000000;">
      的更新
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      就更难收敛
     </span>
     <span style="color:#000000;">
      .
     </span>
     <span style="color:#000000;">
      为了解决收敛问题
     </span>
     <span style="color:#000000;">
      , Google
     </span>
     <span style="color:#000000;">
      Deepmind
     </span>
     <span style="color:#000000;">
      提出
     </span>
     <span style="color:#000000;">
      了
     </span>
     <span style="color:#000000;">
      Actor
     </span>
     <span style="color:#000000;">
      Critic
     </span>
     <span style="color:#000000;">
      升级版
     </span>
     <span style="color:#000000;">
      Deep Deterministic Policy Gradient.
     </span>
     <span style="color:#000000;">
      后者融合了
     </span>
     <span style="color:#000000;">
      DQN
     </span>
     <span style="color:#000000;">
      的优势
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      解决了收敛难的问题
     </span>
     <span style="color:#000000;">
      .
     </span>
    </p>
    <p>
     （5）代码实现框架：
    </p>
    <p>
     <img alt="" class="has" height="325" src="https://i-blog.csdnimg.cn/blog_migrate/a61d2c728af2312507b704f194ca0c23.png" width="467"/>
    </p>
    <h2 id="%E4%BA%94%E3%80%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%EF%BC%9A">
     五、强化学习应用：
    </h2>
    <h3 id="%EF%BC%881%EF%BC%89%E5%90%84%E9%A2%86%E5%9F%9F%E5%BA%94%E7%94%A8%EF%BC%9A">
     （1）各领域应用：
    </h3>
    <p>
     <img alt="" class="has" height="330" src="https://i-blog.csdnimg.cn/blog_migrate/250c0e226dfd4d2db3c9d005b31698cd.png" width="357"/>
    </p>
    <h3 id="%EF%BC%882%EF%BC%89%E5%AF%B9%E8%AF%9D">
     （2）对话
    </h3>
    <p>
     <span style="color:#000000;">
      TaskBot
     </span>
     <span style="color:#000000;">
      -
     </span>
     <span style="color:#000000;">
      阿里小蜜的任务型问答技术：
     </span>
    </p>
    <p>
     <img alt="" class="has" height="204" src="https://i-blog.csdnimg.cn/blog_migrate/7a1f4d930b208cdadc4412440c8dc2a7.png" width="407"/>
    </p>
    <p>
     <img alt="" class="has" height="263" src="https://i-blog.csdnimg.cn/blog_migrate/11e8b9b7d7d539146dc3f997267e81c2.png" width="404"/>
    </p>
    <p>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      <strong>
       State.
      </strong>
     </span>
     <span style="color:#000000;">
      我们主要考虑了
     </span>
     <span style="color:#000000;">
      intent network
     </span>
     <span style="color:#000000;">
      出来的
     </span>
     <span style="color:#000000;">
      user question
     </span>
     <span style="color:#000000;">
      embeddings
     </span>
     <span style="color:#000000;">
      ，当前抽 取的
     </span>
     <span style="color:#000000;">
      slot
     </span>
     <span style="color:#000000;">
      状态，和历史的
     </span>
     <span style="color:#000000;">
      slot
     </span>
     <span style="color:#000000;">
      信息，之后接入一个全连接的神经网络，最后连
     </span>
     <span style="color:#000000;">
      softmax
     </span>
     <span style="color:#000000;">
      到各个
     </span>
     <span style="color:#000000;">
      actions
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      <strong>
       Action.
      </strong>
     </span>
     <span style="color:#000000;">
      在订机票场景，
     </span>
     <span style="color:#000000;">
      action
     </span>
     <span style="color:#000000;">
      空间是离散的，主要包括对各个
     </span>
     <span style="color:#000000;">
      slot
     </span>
     <span style="color:#000000;">
      的反问和
     </span>
     <span style="color:#000000;">
      Order(
     </span>
     <span style="color:#000000;">
      下单
     </span>
     <span style="color:#000000;">
      ):
     </span>
     <span style="color:#000000;">
      反问时间，反问出发地，反问目的地，和
     </span>
     <span style="color:#000000;">
      Order
     </span>
     <span style="color:#000000;">
      。这里的
     </span>
     <span style="color:#000000;">
      action
     </span>
     <span style="color:#000000;">
      空间可以扩展，加入一些新的信息比方询问说多少个人同行，用户偏好等。
     </span>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      对比了不同的
     </span>
     <span style="color:#000000;">
      DRL
     </span>
     <span style="color:#000000;">
      配置下的效果。在测试的时候发现，如果用户退出会话
     </span>
     <span style="color:#000000;">
      (Quit)
     </span>
     <span style="color:#000000;">
      给一个比较大的惩罚
     </span>
     <span style="color:#000000;">
      -1
     </span>
     <span style="color:#000000;">
      ，模型很难学好。这个主要原因 是，用户退出的原因比较多样化，有些不是因为系统回复的不好而退出的，如 果这个时候给比较大的惩罚，会对正确的
     </span>
     <span style="color:#000000;">
      actions
     </span>
     <span style="color:#000000;">
      有影响。
     </span>
    </p>
    <h3 id="%EF%BC%883%EF%BC%89%E6%B7%98%E5%AE%9D%E7%94%B5%E5%95%86%E6%90%9C%E7%B4%A2%EF%BC%9A">
     <span style="color:#000000;">
      （3）淘宝电商搜索：
     </span>
    </h3>
    <p>
     <img alt="" class="has" height="193" src="https://i-blog.csdnimg.cn/blog_migrate/ac0d332c9f98a54f4ecc81d893ef5c97.png" width="514"/>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      用户搜索商品是一个连续的过程。这一连续过程的不同阶段之间不是孤立的，而是有着紧密的联系。换句话说，用户最终选择购买 或不够买商品，不是由某一次排序所决定，而是一连串搜索排序的结果。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      搜索引擎（智能体）、用户（环境）、用户行为（
     </span>
     <span style="color:#000000;">
      reward
     </span>
     <span style="color:#000000;">
      ）
     </span>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      当状态空间
     </span>
     <span style="color:#000000;">
      S
     </span>
     <span style="color:#000000;">
      和动作空间
     </span>
     <span style="color:#000000;">
      A
     </span>
     <span style="color:#000000;">
      确定好之后
     </span>
     <span style="color:#000000;">
      (
     </span>
     <span style="color:#000000;">
      动作空间即
     </span>
     <span style="color:#000000;">
      Agent
     </span>
     <span style="color:#000000;">
      能够选择排序策略的 空间
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      ，状态转移函数
     </span>
     <span style="color:#000000;">
      T
     </span>
     <span style="color:#000000;">
      也随即确定
     </span>
     <span style="color:#000000;">
      ;
     </span>
    </p>
    <p style="margin-left:0in;">
     <span style="color:#000000;">
      另一个重要的步骤是把我们 要达到的目标
     </span>
     <span style="color:#000000;">
      (
     </span>
     <span style="color:#000000;">
      如
     </span>
     <span style="color:#000000;">
      :
     </span>
     <span style="color:#000000;">
      提高点击率、提高
     </span>
     <span style="color:#000000;">
      GMV
     </span>
     <span style="color:#000000;">
      等
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      转化为具体的奖赏函数
     </span>
     <span style="color:#000000;">
      R
     </span>
    </p>
    <h3 id="%EF%BC%884%EF%BC%89FlappyBird%3A" style="margin-left:0in;">
     <span style="color:#000000;">
      （4）FlappyBird:
     </span>
    </h3>
    <p style="margin-left:0in;">
     <img alt="" class="has" height="340" src="https://i-blog.csdnimg.cn/blog_migrate/3c8e229aa5a03ddce838aa9c1953be3a.png" width="449"/>
    </p>
    <h3 id="%EF%BC%885%EF%BC%89%E7%BB%84%E5%90%88%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%88TSP%EF%BC%89%EF%BC%9A" style="margin-left:0in;">
     （5）组合优化问题（TSP）：
    </h3>
    <p style="margin-left:0in;">
     <img alt="" class="has" height="187" src="https://i-blog.csdnimg.cn/blog_migrate/17cd38248a17428e91a57cfeecf85acf.png" width="353"/>
    </p>
    <p style="margin-left:0in;">
     <img alt="" class="has" height="277" src="https://i-blog.csdnimg.cn/blog_migrate/dad8d126a7be03073502372c877dff84.png" width="659"/>
    </p>
    <h2 id="%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93%EF%BC%9A" style="margin-left:0in;">
     六、总结：
    </h2>
    <h3 id="1.%E5%A6%82%E4%BD%95%E8%AE%BE%E8%AE%A1%E7%AE%97%E6%B3%95%EF%BC%9A" style="margin-left:0in;">
     1.如何设计算法：
    </h3>
    <p>
     <span style="color:#008ab8;">
      Step
     </span>
     <span style="color:#008ab8;">
      1
     </span>
     <span style="color:#008ab8;">
      ：将实际问题建模成马尔可夫决策过程，抽象出五元组，
     </span>
     <span style="color:#008ab8;">
      其中
     </span>
     <span style="color:#008ab8;">
      reward
     </span>
     <span style="color:#008ab8;">
      与实际目标相关联
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      Step
     </span>
     <span style="color:#008ab8;">
      2
     </span>
     <span style="color:#008ab8;">
      ：根据动作是否连续选择对应的算法
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      动作离散：
     </span>
     <span style="color:#008ab8;">
      DQN
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      动作连续：
     </span>
     <span style="color:#008ab8;">
      Policy
     </span>
     <span style="color:#008ab8;">
      Gradients
     </span>
     <span style="color:#008ab8;">
      ，
     </span>
     <span style="color:#008ab8;">
      Actor-
     </span>
     <span style="color:#008ab8;">
      Critic
     </span>
     <span style="color:#008ab8;">
      ，
     </span>
     <span style="color:#008ab8;">
      DDPG
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      Step
     </span>
     <span style="color:#008ab8;">
      3
     </span>
     <span style="color:#008ab8;">
      ：根据算法写代码
     </span>
    </p>
    <p>
    </p>
    <p>
     参考资料：
    </p>
    <p>
     <span style="color:#008ab8;">
      1
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      莫烦强化学习系列教程
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" rel="nofollow">
       https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" rel="nofollow">
       /
      </a>
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      2
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      DQN
     </span>
     <span style="color:#008ab8;">
      系列教程
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      <a href="https://www.zhihu.com/collection/124320144" rel="nofollow">
       https
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://www.zhihu.com/collection/124320144" rel="nofollow">
       ://www.zhihu.com/collection/
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://www.zhihu.com/collection/124320144" rel="nofollow">
       124320144
      </a>
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      3
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      Policy
     </span>
     <span style="color:#008ab8;">
      gradients
     </span>
     <span style="color:#008ab8;">
      推导：
     </span>
     <span style="color:#008ab8;">
      <a href="http://karpathy.github.io/2016/05/31/rl/" rel="nofollow">
       http://karpathy.github.io/2016/05/31/rl
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="http://karpathy.github.io/2016/05/31/rl/" rel="nofollow">
       /
      </a>
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      4
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      2017
     </span>
     <span style="color:#008ab8;">
      年
     </span>
     <span style="color:#008ab8;">
      ICLR
     </span>
     <span style="color:#008ab8;">
      论文：
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      NEURAL
     </span>
     <span style="color:#000000;">
      COMBINATORIAL OPTIMIZATION WITH REINFORCEMENT LEARNING
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      5
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      sutton
     </span>
     <span style="color:#008ab8;">
      《
     </span>
     <span style="color:#008ab8;">
      Reinforcement
     </span>
     <span style="color:#008ab8;">
      Learning:An
     </span>
     <span style="color:#008ab8;">
      I
     </span>
     <span style="color:#008ab8;">
      ntroduction
     </span>
     <span style="color:#008ab8;">
      》
     </span>
     <span style="color:#008ab8;">
      ：
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      <a href="https://zhuanlan.zhihu.com/p/35182998" rel="nofollow">
       https
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://zhuanlan.zhihu.com/p/35182998" rel="nofollow">
       ://zhuanlan.zhihu.com/p/
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://zhuanlan.zhihu.com/p/35182998" rel="nofollow">
       35182998
      </a>
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      6
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#000000;">
      Exercises and Solutions to accompany
     </span>
     <span style="color:#000000;">
      Sutton‘s
     </span>
     <span style="color:#000000;">
      Book and David
     </span>
     <span style="color:#000000;">
      Silver’s course
     </span>
     <span style="color:#000000;">
      ：
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      <a href="https://github.com/dennybritz/reinforcement-learning">
       https
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://github.com/dennybritz/reinforcement-learning">
       ://github.com/dennybritz/reinforcement-
      </a>
     </span>
     <span style="color:#008ab8;">
      <a href="https://github.com/dennybritz/reinforcement-learning">
       learning
      </a>
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      7
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      Andrew
     </span>
     <span style="color:#008ab8;">
      Ng
     </span>
     <span style="color:#008ab8;">
      机器学习视频
     </span>
     <span style="color:#008ab8;">
      16-20
     </span>
     <span style="color:#008ab8;">
      章
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      8
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      David Silver
     </span>
     <span style="color:#008ab8;">
      视频
     </span>
    </p>
    <p>
     <span style="color:#008ab8;">
      9
     </span>
     <span style="color:#008ab8;">
      、
     </span>
     <span style="color:#008ab8;">
      Reinforcement
     </span>
     <span style="color:#008ab8;">
      Learning
     </span>
     <span style="color:#008ab8;">
      Beyond
     </span>
     <span style="color:#008ab8;">
      Games:To
     </span>
     <span style="color:#008ab8;">
      M
     </span>
     <span style="color:#008ab8;">
      ake
     </span>
     <span style="color:#008ab8;">
      a
     </span>
     <span style="color:#008ab8;">
      Difference
     </span>
     <span style="color:#008ab8;">
      in
     </span>
     <span style="color:#008ab8;">
      Alibaba
     </span>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6a373534333739313137:2f61727469636c652f64657461696c732f3833303337373939" class_="artid" style="display:none">
 </p>
</div>


