---
layout: post
title: "文本数据处理最佳文本切分策略"
date: 2025-03-16 23:31:44 +0800
description: "在自然语言处理（NLP）中，数据切分（Chunking）是处理长文本的关键步骤，直接影响模型性能（如检索增强生成RAG、文本嵌入、机器阅读理解）。"
keywords: "文本数据处理——最佳文本切分策略"
categories: ['人工智能']
tags: ['语义分块', '滑动窗口分块', '文本切分', '数据切分', '固定长度分块', '人工智能']
artid: "146303421"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146303421
    alt: "文本数据处理最佳文本切分策略"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146303421
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146303421
cover: https://bing.ee123.net/img/rand?artid=146303421
image: https://bing.ee123.net/img/rand?artid=146303421
img: https://bing.ee123.net/img/rand?artid=146303421
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     文本数据处理——最佳文本切分策略
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     在自然语言处理（NLP）中，数据切分（Chunking）是处理长文本的关键步骤，直接影响模型性能（如检索增强生成RAG、文本嵌入、机器阅读理解）。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ed7b9807ade14f43b5292d291aa4b116.png" width="500"/>
    </p>
    <p>
     以下是常见的切分方式及其适用场景。
    </p>
    <h4>
     <a id="_5">
     </a>
     <strong>
      一、常见切分方式
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         方法
        </strong>
       </th>
       <th>
        <strong>
         原理
        </strong>
       </th>
       <th>
        <strong>
         优点
        </strong>
       </th>
       <th>
        <strong>
         缺点
        </strong>
       </th>
       <th>
        <strong>
         适用场景
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         1. 固定长度分块
        </strong>
       </td>
       <td>
        按固定字符数或词数切割（如每200字符）
       </td>
       <td>
        实现简单，适合批量处理
       </td>
       <td>
        可能切断完整句子或语义单元
       </td>
       <td>
        通用文本嵌入、简单检索任务
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         2. 按句子分割
        </strong>
       </td>
       <td>
        基于标点（句号、问号）划分句子
       </td>
       <td>
        保留完整句子结构
       </td>
       <td>
        忽略段落间逻辑关联
       </td>
       <td>
        机器翻译、文本摘要
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         3. 按段落分割
        </strong>
       </td>
       <td>
        以段落（换行符）为边界切分
       </td>
       <td>
        保留段落内完整语义
       </td>
       <td>
        段落长度差异大，需二次处理
       </td>
       <td>
        文档结构化分析（如论文、报告）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         4. 滑动窗口分块
        </strong>
       </td>
       <td>
        固定长度分块，相邻块部分重叠（如50%）
       </td>
       <td>
        减少语义断裂，提升上下文连续性
       </td>
       <td>
        冗余计算，存储成本增加
       </td>
       <td>
        长文本生成（如故事续写）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         5. 基于语义分块
        </strong>
       </td>
       <td>
        用NLP模型检测语义边界（如主题变化点）
       </td>
       <td>
        确保块内语义完整，块间低耦合
       </td>
       <td>
        计算复杂度高，依赖模型质量
       </td>
       <td>
        RAG系统、知识图谱构建
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         6. 按标记（Token）分块
        </strong>
       </td>
       <td>
        按模型最大Token限制切割（如GPT-4的8k）
       </td>
       <td>
        适配模型输入限制
       </td>
       <td>
        可能破坏文本逻辑结构
       </td>
       <td>
        大模型输入预处理（如GPT、LLaMA）
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     <a id="_15">
     </a>
     <strong>
      二、最佳切分策略
     </strong>
    </h4>
    <p>
     <strong>
      1. 通用推荐
     </strong>
     ：
    </p>
    <ul>
     <li>
      <strong>
       优先选择：语义分块 + 滑动窗口（重叠）
      </strong>
      <ul>
       <li>
        <strong>
         理由
        </strong>
        ：平衡语义完整性与计算效率，适合大多数场景（如RAG、问答系统）。
       </li>
       <li>
        <strong>
         实现工具
        </strong>
        ：
        <ul>
         <li>
          <strong>
           LangChain的
           <code>
            RecursiveCharacterTextSplitter
           </code>
          </strong>
          ：支持多级分割（段落→句子→固定长度）。
         </li>
         <li>
          <strong>
           spaCy/sentence-transformers
          </strong>
          ：检测语义边界。
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
    <p>
     <strong>
      2. 分块大小建议
     </strong>
     ：
    </p>
    <ul>
     <li>
      <strong>
       检索增强（RAG）
      </strong>
      ：200-500字符（或50-200词），确保信息密度。
     </li>
     <li>
      <strong>
       模型输入
      </strong>
      ：按模型Token限制调整（如GPT-3.5为4k Tokens）。
     </li>
     <li>
      <strong>
       重叠窗口
      </strong>
      ：10-20%块长度（如200字符块，重叠40字符）。
     </li>
    </ul>
    <p>
     <strong>
      3. 分步骤示例
     </strong>
     ：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>text_splitter <span class="token keyword">import</span> RecursiveCharacterTextSplitter

<span class="token comment"># 定义分块规则</span>
text_splitter <span class="token operator">=</span> RecursiveCharacterTextSplitter<span class="token punctuation">(</span>
    chunk_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>      <span class="token comment"># 目标块大小</span>
    chunk_overlap<span class="token operator">=</span><span class="token number">40</span><span class="token punctuation">,</span>    <span class="token comment"># 重叠长度</span>
    separators<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"\n\n"</span><span class="token punctuation">,</span> <span class="token string">"\n"</span><span class="token punctuation">,</span> <span class="token string">". "</span><span class="token punctuation">,</span> <span class="token string">"? "</span><span class="token punctuation">,</span> <span class="token string">"! "</span><span class="token punctuation">,</span> <span class="token string">"。"</span><span class="token punctuation">,</span> <span class="token string">"？"</span><span class="token punctuation">,</span> <span class="token string">"！"</span><span class="token punctuation">]</span>  <span class="token comment"># 分割符优先级</span>
<span class="token punctuation">)</span>

<span class="token comment"># 执行切分</span>
text <span class="token operator">=</span> <span class="token string">"长文本内容..."</span>
chunks <span class="token operator">=</span> text_splitter<span class="token punctuation">.</span>split_text<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_44">
     </a>
     <strong>
      三、场景化最佳实践
     </strong>
    </h4>
    <table>
     <thead>
      <tr>
       <th>
        <strong>
         场景
        </strong>
       </th>
       <th>
        <strong>
         推荐方法
        </strong>
       </th>
       <th>
        <strong>
         工具/库
        </strong>
       </th>
       <th>
        <strong>
         注意事项
        </strong>
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        <strong>
         RAG系统
        </strong>
       </td>
       <td>
        语义分块 + 滑动窗口
       </td>
       <td>
        LangChain、Cohere Embed
       </td>
       <td>
        避免过小分块丢失上下文关联
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         法律文档分析
        </strong>
       </td>
       <td>
        按章节/条款切分
       </td>
       <td>
        spaCy（规则匹配）
       </td>
       <td>
        保留条款编号和层级结构
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         社交媒体文本处理
        </strong>
       </td>
       <td>
        按帖子/评论分割
       </td>
       <td>
        正则表达式（如按时间戳分割）
       </td>
       <td>
        处理非结构化文本（如表情符号）
       </td>
      </tr>
      <tr>
       <td>
        <strong>
         科研论文解析
        </strong>
       </td>
       <td>
        按章节（摘要、方法、结果）
       </td>
       <td>
        PDF解析库（PyPDF2、GROBID）
       </td>
       <td>
        提取图表标题与正文关联
       </td>
      </tr>
     </tbody>
    </table>
    <h4>
     <a id="___53">
     </a>
     四、语义分块 + 滑动窗口代码示例
    </h4>
    <p>
     以下是一个结合
     <strong>
      语义分块
     </strong>
     和
     <strong>
      滑动窗口
     </strong>
     的Python代码示例，使用
     <code>
      spaCy
     </code>
     检测语义边界，并实现重叠分块，适用于RAG系统：
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 安装依赖：pip install spacy sentence-transformers chromadb</span>
<span class="token comment"># 下载spacy模型：python -m spacy download en_core_web_sm</span>

<span class="token keyword">import</span> spacy
<span class="token keyword">import</span> re
<span class="token keyword">from</span> typing <span class="token keyword">import</span> List

<span class="token comment"># 加载spacy模型（检测句子和语义边界）</span>
nlp <span class="token operator">=</span> spacy<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"en_core_web_sm"</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">semantic_chunking_with_sliding_window</span><span class="token punctuation">(</span>
    text<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> 
    chunk_size<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">300</span><span class="token punctuation">,</span> 
    overlap<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token number">50</span>
<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> List<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    语义分块 + 滑动窗口切分文本
    参数：
        text: 输入文本
        chunk_size: 目标分块大小（字符数）
        overlap: 滑动窗口重叠长度（字符数）
    返回：
        chunks: 分块后的文本列表
    """</span>
    <span class="token comment"># Step 1: 预处理（移除多余空格、换行符）</span>
    text <span class="token operator">=</span> re<span class="token punctuation">.</span>sub<span class="token punctuation">(</span><span class="token string">r'\s+'</span><span class="token punctuation">,</span> <span class="token string">' '</span><span class="token punctuation">,</span> text<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Step 2: 使用spacy分句（语义分块）</span>
    doc <span class="token operator">=</span> nlp<span class="token punctuation">(</span>text<span class="token punctuation">)</span>
    sentences <span class="token operator">=</span> <span class="token punctuation">[</span>sent<span class="token punctuation">.</span>text<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> sent <span class="token keyword">in</span> doc<span class="token punctuation">.</span>sents<span class="token punctuation">]</span>
    
    <span class="token comment"># Step 3: 合并句子为初始块（基于chunk_size）</span>
    chunks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    current_chunk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    current_length <span class="token operator">=</span> <span class="token number">0</span>
    
    <span class="token keyword">for</span> sent <span class="token keyword">in</span> sentences<span class="token punctuation">:</span>
        sent_length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sent<span class="token punctuation">)</span>
        <span class="token keyword">if</span> current_length <span class="token operator">+</span> sent_length <span class="token operator">&lt;=</span> chunk_size<span class="token punctuation">:</span>
            current_chunk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sent<span class="token punctuation">)</span>
            current_length <span class="token operator">+=</span> sent_length
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            chunks<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>current_chunk<span class="token punctuation">)</span><span class="token punctuation">)</span>
            current_chunk <span class="token operator">=</span> <span class="token punctuation">[</span>sent<span class="token punctuation">]</span>
            current_length <span class="token operator">=</span> sent_length
    
    <span class="token keyword">if</span> current_chunk<span class="token punctuation">:</span>
        chunks<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>current_chunk<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    <span class="token comment"># Step 4: 应用滑动窗口（添加重叠区域）</span>
    final_chunks <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>chunks<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        current <span class="token operator">=</span> chunks<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
        <span class="token keyword">if</span> i <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment"># 取前一块的末尾overlap部分与当前块合并</span>
            prev_chunk_tail <span class="token operator">=</span> chunks<span class="token punctuation">[</span>i<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span>overlap<span class="token punctuation">:</span><span class="token punctuation">]</span>
            current <span class="token operator">=</span> prev_chunk_tail <span class="token operator">+</span> <span class="token string">' '</span> <span class="token operator">+</span> current
        final_chunks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current<span class="token punctuation">[</span><span class="token punctuation">:</span>chunk_size<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 确保不超过chunk_size</span>
    
    <span class="token keyword">return</span> final_chunks

<span class="token comment"># 测试示例</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    sample_text <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
    Large language models (LLMs) have revolutionized AI applications. 
    However, their knowledge is static after training. Retrieval-Augmented 
    Generation (RAG) addresses this by dynamically retrieving relevant 
    documents during inference. For example, in medical diagnosis, 
    RAG systems can pull the latest research papers to provide up-to-date 
    recommendations. The key steps include: document chunking, vector 
    retrieval, and context-aware generation. Effective chunking strategies 
    like semantic segmentation with sliding windows improve retrieval accuracy.
    """</span>
    
    chunks <span class="token operator">=</span> semantic_chunking_with_sliding_window<span class="token punctuation">(</span>sample_text<span class="token punctuation">,</span> chunk_size<span class="token operator">=</span><span class="token number">150</span><span class="token punctuation">,</span> overlap<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i<span class="token punctuation">,</span> chunk <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>chunks<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Chunk </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string"> (</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">len</span><span class="token punctuation">(</span>chunk<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> chars):\n</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>chunk<span class="token punctuation">}</span></span><span class="token string">\n</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token string">'-'</span><span class="token operator">*</span><span class="token number">50</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      输出示例
     </strong>
    </p>
    <pre><code>Chunk 1 (150 chars):
Large language models (LLMs) have revolutionized AI applications. However, their knowledge is static after training. Retrieval-Augmented Generation (RAG) addresses this by dynamically retrieving relevant 
--------------------------------------------------
Chunk 2 (150 chars):
retrieving relevant documents during inference. For example, in medical diagnosis, RAG systems can pull the latest research papers to provide up-to-date recommendations. The key steps include: document chunking, vector 
--------------------------------------------------
Chunk 3 (150 chars):
vector retrieval, and context-aware generation. Effective chunking strategies like semantic segmentation with sliding windows improve retrieval accuracy.
--------------------------------------------------
</code></pre>
    <p>
     <strong>
      关键实现逻辑
     </strong>
    </p>
    <ol>
     <li>
      <p>
       <strong>
        语义分块
       </strong>
       ：
      </p>
      <ul>
       <li>
        使用
        <code>
         spaCy
        </code>
        将文本分割为完整句子（保留语义边界）。
       </li>
       <li>
        例：将长段落分割为
        <code>
         ["sentence1", "sentence2", ...]
        </code>
        。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        初始分块合并
       </strong>
       ：
      </p>
      <ul>
       <li>
        按
        <code>
         chunk_size
        </code>
        合并句子，确保每块不超过目标长度。
       </li>
       <li>
        例：合并至总字符数接近300。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        滑动窗口重叠
       </strong>
       ：
      </p>
      <ul>
       <li>
        取前一块末尾的
        <code>
         overlap
        </code>
        个字符，拼接到当前块开头。
       </li>
       <li>
        例：前一块末尾的30字符 + 当前块 → 提升上下文连续性。
       </li>
      </ul>
     </li>
    </ol>
    <p>
     <strong>
      与RAG系统集成
     </strong>
    </p>
    <p>
     将分块后的文本存入向量数据库（如Chroma），用于后续检索：
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> chromadb
<span class="token keyword">from</span> sentence_transformers <span class="token keyword">import</span> SentenceTransformer

<span class="token comment"># 1. 初始化向量数据库</span>
client <span class="token operator">=</span> chromadb<span class="token punctuation">.</span>Client<span class="token punctuation">(</span><span class="token punctuation">)</span>
collection <span class="token operator">=</span> client<span class="token punctuation">.</span>create_collection<span class="token punctuation">(</span><span class="token string">"rag_docs"</span><span class="token punctuation">)</span>

<span class="token comment"># 2. 生成嵌入向量</span>
embedder <span class="token operator">=</span> SentenceTransformer<span class="token punctuation">(</span><span class="token string">'all-MiniLM-L6-v2'</span><span class="token punctuation">)</span>

<span class="token comment"># 3. 存储分块文本</span>
chunks <span class="token operator">=</span> semantic_chunking_with_sliding_window<span class="token punctuation">(</span>sample_text<span class="token punctuation">)</span>
embeddings <span class="token operator">=</span> embedder<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>chunks<span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>

collection<span class="token punctuation">.</span>add<span class="token punctuation">(</span>
    embeddings<span class="token operator">=</span>embeddings<span class="token punctuation">,</span>
    documents<span class="token operator">=</span>chunks<span class="token punctuation">,</span>
    ids<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string-interpolation"><span class="token string">f"id</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>i<span class="token punctuation">}</span></span><span class="token string">"</span></span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>chunks<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># 4. 检索示例</span>
query <span class="token operator">=</span> <span class="token string">"How does RAG handle up-to-date information?"</span>
query_embedding <span class="token operator">=</span> embedder<span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token punctuation">[</span>query<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span>

results <span class="token operator">=</span> collection<span class="token punctuation">.</span>query<span class="token punctuation">(</span>
    query_embeddings<span class="token operator">=</span>query_embedding<span class="token punctuation">,</span>
    n_results<span class="token operator">=</span><span class="token number">2</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Top 2 relevant chunks:"</span><span class="token punctuation">,</span> results<span class="token punctuation">[</span><span class="token string">'documents'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre>
    <p>
     <strong>
      最佳实践建议
     </strong>
    </p>
    <ol>
     <li>
      <p>
       <strong>
        参数调优
       </strong>
       ：
      </p>
      <ul>
       <li>
        <strong>
         <code>
          chunk_size
         </code>
        </strong>
        ：根据下游任务调整（RAG推荐200-500字符）。
       </li>
       <li>
        <strong>
         <code>
          overlap
         </code>
        </strong>
        ：设为
        <code>
         chunk_size
        </code>
        的10-20%（如300字符块用30-60重叠）。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        复杂文本处理
       </strong>
       ：
      </p>
      <ul>
       <li>
        技术文档：优先按段落（
        <code>
         \n\n
        </code>
        ）分割，再应用滑动窗口。
       </li>
       <li>
        对话记录：按说话者切换点分割（如
        <code>
         User:
        </code>
        和
        <code>
         Bot:
        </code>
        ）。
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        性能优化
       </strong>
       ：
      </p>
      <ul>
       <li>
        缓存
        <code>
         spaCy
        </code>
        分句结果，避免重复计算。
       </li>
       <li>
        并行处理多个文档的分块和嵌入生成。
       </li>
      </ul>
     </li>
    </ol>
    <h4>
     <a id="_208">
     </a>
     <strong>
      五、总结
     </strong>
    </h4>
    <ul>
     <li>
      <strong>
       最佳方法
      </strong>
      ：
      <strong>
       语义分块 + 滑动窗口
      </strong>
      （LangChain工具链），兼顾效率与语义完整性。
     </li>
     <li>
      <strong>
       核心原则
      </strong>
      ：根据任务需求（检索、生成、分析）和数据特性（结构/非结构、长度）动态调整。
     </li>
     <li>
      <strong>
       避坑指南
      </strong>
      ：
      <ul>
       <li>
        避免过小分块（丢失上下文）或过大分块（噪声增加）。
       </li>
       <li>
        测试不同分块策略对下游任务（如检索召回率）的影响。
       </li>
      </ul>
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6c69687561796f6e672f:61727469636c652f64657461696c732f313436333033343231" class_="artid" style="display:none">
 </p>
</div>


