---
arturl_encode: "68747470733a2f2f626c6f67:2e6373646e2e6e65742f6c696861696d696e675f323030382f:61727469636c652f64657461696c732f313436303530373736"
layout: post
title: "rabbitmq版本升级并部署高可用"
date: 2025-03-05 19:14:32 +08:00
description: "Logrotate下载地址https://centos.pkgs.org/7/centos-x86_64/logrotate-3.8.6-19.el7.x86_64.rpm.html。安装rpm时增加参数–nodeps --force：rpm -ivh socat-1.7.3.2-5.el7.lux.x86_64.rpm --nodeps --force。rpm -e --nodeps $(rpm -qa|grep erlang)  //如安装了，则进行卸载。"
keywords: "rabbitmq版本升级并部署高可用"
categories: ['未分类']
tags: ['高可用', '集群', 'Rabbitmq']
artid: "146050776"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146050776
    alt: "rabbitmq版本升级并部署高可用"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146050776
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146050776
cover: https://bing.ee123.net/img/rand?artid=146050776
image: https://bing.ee123.net/img/rand?artid=146050776
img: https://bing.ee123.net/img/rand?artid=146050776
---

# rabbitmq版本升级并部署高可用

## **RabbitMQ版本升级**

### 先检查是否已经安装rabbitmq

rpm -qa|grep rabbitmq|wc -l    //如果结果是0，表示没有安装

rpm -e --nodeps $(rpm -qa|grep rabbitmq)  //如安装了，则进行卸载

### 先检查是否已经安装erlang

rpm -qa|grep erlang|wc -l    //如果结果是0，表示没有安装

rpm -e --nodeps $(rpm -qa|grep erlang)  //如安装了，则进行卸载

### 准备升级后的安装包

* 去https://dl.bintray.com/rabbitmq/rpm/下载需要rabbitmq包
* 去https://github.com/rabbitmq/erlang-rpm/releases下载对应所需要的erlang包
* 由于rabbitmq需要依赖socat所以需要去此网站https://centos.pkgs.org/查找对应系统的对应版本下载
* rabbitmq还依赖logrotate，查看下是否安装了，如没安装重复第3步

Logrotate下载地址https://centos.pkgs.org/7/centos-x86\_64/logrotate-3.8.6-19.el7.x86\_64.rpm.html

```bash
rpm -qa | grep logrotate
```

### 安装对应包

```bash
rpm -ivh erlang-23.3.4.18-1.el7.x86_64.rpm

####检查是否安装成功
erl
#输入 erl 并用 halt() . 函数退出
rpm -ivh socat-1.7.3.2-2.el7.x86_64.rpm

rpm -ivh rabbitmq-server-3.8.16-1.el7.noarch.rpm
```

* #### 安装erlang遇到的问题

1.错误：依赖检测失败：

libsystemd.so.0()(64bit) 被 erlang-21.3.8.9-1.el7.x86\_64 需要

方案一

1.1下载依赖包，网址https://pkgs.org/，搜索libcrypto.so.10，下载对应版本的安装包

1.2下载地址如下

https://centos.pkgs.org/7/centos-x86\_64/openssl-libs-1.0.2k-19.el7.x86\_64.rpm.html#:~:text=http%3A//mirror.centos.org/centos/7/os/x86\_64/Packages/openssl%2Dlibs%2D1.0.2k%2D19.el7.x86\_64.rpm

1.3安装命令

rpm -ivh openssl-libs-1.0.2k-19.el7.x86\_64.rpm

安装失败则加参数--force进行强制安装

rpm -ivh openssl-libs-1.0.2k-19.el7.x86\_64.rpm --force

方案二

在命令后面添加--nodeps --force参数。该命令的作用是，不再分析包之间的依赖关系而直接安装

rpm -ivh erlang-23.3.4.11-1.el7.x86\_64.rpm --nodeps --force

然后手动安装依赖环境

安装socat遇到的错误

错误：依赖检测失败：

libreadline.so.6()(64bit) 被 socat-1.7.3.2-2.el7.x86\_64 需要

安装rpm时增加参数–nodeps --force：rpm -ivh socat-1.7.3.2-5.el7.lux.x86\_64.rpm --nodeps --force

#### 安装rabbitmq遇到的问题

错误1：

错误：依赖检测失败：

Socat被rabbitmq-server-3.8.16-l.el7.noarch需要

方案：安装socat依赖

命令：rpm -ivh socat-1.7.3.2-2.el7.x86\_64.rpm

* 启动rabbitmq

service rabbitmq-server start//启动

service rabbitmq-server stop//停止

service rabbitmq-server restart//重启

* 启动管理界面

rabbitmq-plugins enable rabbitmq\_management

* 1

### **RabbitMQ配置**

新启动的mq还不可以远程访问，guest也只能本地访问所以需要一些配置

* 添加用户并设置密码

```bash
rabbitmqctl add_user  super super
```

添加权限（使admin用户对虚拟主机“/” 具有所有权限）:

```bash
rabbitmqctl set_permissions -p “/” super “.*” “.*” “.*”
```

* 修改用户角色（加入administrator用户组）

rabbitmqctl set\_user\_tags super administrator

rabbitmqctl set\_user\_tags {username} {tag ...}

至此就可以远程访问了
  
4. 管理界面访问地址为 ip:15672(端口号)

进入管理界面查看端口号情况

![](https://i-blog.csdnimg.cn/direct/ff42bbc694304778b49d8f86b880ed97.png)

注：工程相关配置文件使用的是amqp协议，使用的端口号是5672。

### **常用命令**

#### 应用的开启关闭

```bash
service rabbitmq-server start//启动

service rabbitmq-server stop//停止

service rabbitmq-server restart//重启
```

#### 集群的开启关闭

```bash
rabbitmqctl stop_app //仅关闭应用，不关闭节点

rabbitmqctl start_app //开启应用

rabbitmq -server -detached //启动节点和应用

rabbitmqctl stop //关闭节点和应用
```

#### 插件管理

```bash
rabbitmq-plugins enable  xxx //开启某个插件

rabbitmq-plugins disable xxx //关闭某个插件

rabbitmq-plugins list //插件列表

rabbitmq-plugins set xxx xxx //启用一个或多个插件，禁用其余插件
```

注意：重启服务器后生效。

#### 用户管理

```bash
rabbitmqctl add_user username pwd //新建用户

rabbitmqctl delete_user username //删除用户

rabbitmqctl list_users //查看用户

rabbitmqctl change_passwor change_password {username} {newpassword} //改密码

rabbitmqctl set_user_tags {username} {tag ...} //设置用户角色 Tag可以为 administrator,monitoring, management
```

#### 防火墙

```bash
#### rabbitmq普通用户不能开放防火墙，使用root提前开放5672，15672端口

firewall-cmd --zone=public --add-port=5672/tcp --permanent

firewall-cmd --zone=public --add-port=15672/tcp --permanent

firewall-cmd --reload
```

## **Rabbitmq集群**

### **修改三台主机名称**

注意：需要先设置linux机器别名称

```bash
hostnamectl set-hostname 机器别名 --static
systemctl stop rabbitmq-server          #（先停 止mq）

vim /etc/hosts
```

> 127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
>
> ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
>
> 192.168.200.108 node1
>
> 192.168.200.109 node2
>
> 192.168.200.102 node3

### **共享Erlang Cookie**

需要将所有机器的.erlang.cookie文件与集群主节点进行同步，同步前需要关闭所有机器的rabbitmq服务

# 关闭rabbitmq服务 systemctl stop rabbitmq-server

[root@node1 ~]# more /var/lib/rabbitmq/.erlang.cookie

PZIJZCVFEXCZCCXPMZFE

[root@node2 ~]# more /var/lib/rabbitmq/.erlang.cookie

GQJUECIAZXPIYQFBASUT

#对比两台主机cookie文件并不一致，集群必须保证一致性#

chmod 600 /var/lib/rabbitmq/.erlang.cookie

### **启动mq服务和erlang**

在3台节点上分别都执行命令

rabbitmq-server -detached       （3台该命令可以重启mq和erlang）

systemctl start rabbitmq-server     （3台启动mq）

### **集群搭建**

以node1为主节点，其余的node2和node3加入主节点

# 1.停止服务
  
rabbitmqctl stop\_app
  
# 2.重置状态
  
rabbitmqctl reset
  
# 3.节点加入, 在一个node加入cluster之前，必须先停止该node的rabbitmq应用，即先执行stop\_app
  
# node2加入node1, node3加入node2
  
rabbitmqctl join\_cluster --ram rabbit@node1 //加入到磁盘节点
  
# 4.启动服务
  
rabbitmqctl start\_app

#此时一个普通集群就搭建起来了，在任意一个节点查看集群状态

rabbitmqctl cluster\_status

默认的 cluster\_name 名字为 rabbit@rabbit1，如果你想进行修改，可以使用以下命令：

rabbitmqctl set\_cluster\_name rabbitmq\_cluster

### **开启镜像集群**

以上部署的这种集群，是无法在各个节点之间保存数据的，数据只会保存在接受到消息的节点上当 ram 的节点挂了之后 数据会丢失；这里我们为所有队列开启镜像配置，高可用策略。

```bash
####表示开启HA模式 适用于所有的队列

rabbitmqctl set_policy ha-all "^" '{"ha-mode":"all"}'
```

### **创建集群账号**

```bash
[root@node1 ~]# rabbitmqctl add_user admin 123
 Adding user "admin" ...
 [root@node1 ~]#  rabbitmqctl set_user_tags admin administrator
 Setting tags for user "admin" to [administrator] ...
 [root@node1 ~]# rabbitmqctl set_permissions -p "/" admin ".*" ".*" ".*"
 Setting permissions for user "admin" in vhost "/" ...
 [root@node1 ~]#
```

步骤
  
1.创建用户admin密码123        （生产环境密码必须高难度密码）
  
2.给admin用户管理员身份
  
3.授予admin用户权限

### **前期准备**

#### **开启ip转发功能**

#查看是否开启转发

cat /proc/sys/net/ipv4/ip\_forward

#返回1代表IP已开启，0 未开启

#临时开启

echo 1 > /proc/sys/net/ipv4/ip\_forward

#永久开启

vi /etc/sysctl.conf

net.ipv4.ip\_forward = 1

#立即生效

sysctl -p /etc/sysctl.conf &

#### **关闭selinux**

#查看selinux状态

getenforce

#永久关闭selinux

vi /etc/selinux/config

#将 SELINUX=enforcing 改为 SELINUX=disabled

#上一个命令需要重启生效，因此建议执行以下命令，临时关闭selinux

setenforce 0

### **HAproxy**

我们用haproxy做负载均衡，在两台虚拟机上都安装。

#解压haprpxy
  
tar xf haproxy-2.9.0.tar.gz
  
#进入目录编译安装
  
cd haproxy-2.9.0
  
make TARGET=linux-glibc  PREFIX=/opt/haproxy-2.9.0
  
make install PREFIX=/opt/haproxy-2.9.0
  
#设置环境变量并生效
  
vim /etc/profile
  
export HAPROXY\_HOME=/opt/haproxy-2.9.0
  
export PATH=$PATH:$HAPROXY\_HOME/sbin
  
source /etc/profile
  
#查看版本验证安装是否成功
  
haproxy -v

HAProxy version 2.9.0-fddb8c1 2023/12/05 - https://haproxy.org/

Status: development branch - not safe for use in production.

Known bugs: http://www.haproxy.org/bugs/bugs-2.9.0.html

Running on: Linux 3.10.0-1160.36.2.el7.x86\_64 #1 SMP Wed Jul 21 11:57:15 UTC 2021 x86\_64

#### **配置haproxy文件**

```bash
mkdir /etc/haproxy
 vim /etc/haproxy/haproxy.cfg
```

> # 全局配置
>   
> global
>   
> # 日志输出配置、所有日志都记录在本机，通过 local0 进行输出
>   
> log 127.0.0.1 local0 info
>   
> # 最大连接数
>   
> maxconn 4096
>   
> # 改变当前的工作目录
>   
> chroot /opt/haproxy-2.1.10
>   
> # 以指定的 UID 运行 haproxy 进程
>   
> uid 99
>   
> # 以指定的 GID 运行 haproxy 进程
>   
> gid 99
>   
> # 以守护进行的方式运行
>   
> daemon
>   
> # 当前进程的 pid 文件存放位置
>   
> pidfile /opt/haproxy-2.1.10/haproxy.pid
>   
> ​
>   
> # 默认配置
>   
> defaults
>   
> # 应用全局的日志配置
>   
> log global
>   
> # 使用4层代理模式，7层代理模式则为"http"
>   
> mode tcp
>   
> # 日志类别
>   
> option tcplog ##记录TCP请求日志
>   
> # 不记录健康检查的日志信息
>   
> option dontlognull
>   
> # 3次失败则认为服务不可用
>   
> retries 3
>   
> # 每个进程可用的最大连接数
>   
> maxconn 2000
>   
> # 连接超时
>   
> timeout connect 5s
>   
> # 客户端超时
>   
> timeout client 120s
>   
> # 服务端超时
>   
> timeout server 120s
>   
> ​
>   
> # 绑定配置
>   
> listen rabbitmq\_cluster
>   
> bind :5672 #绑定端口
>   
> # 配置TCP模式
>   
> mode tcp
>
> option tcpka ##是否允许想server和client发送keepalive
>   
> # 采用加权轮询的机制进行负载均衡
>   
> balance roundrobin
>   
> # RabbitMQ 集群节点配置
>   
> server mq-node1 node1:5672 check inter 5000 rise 2 fall 3 weight 1
>   
> server mq-node2 node2:5672 check inter 5000 rise 2 fall 3 weight 1
>   
> server mq-node3 node3:5672 check inter 5000 rise 2 fall 3 weight 1
>   
> ​
>   
> # 配置监控页面
>   
> listen monitor
>   
> bind :8100
>   
> mode http
>   
> option httplog
>   
> stats enable
>   
> stats uri /stats
>   
> stats refresh 5s

haproxy -f /etc/haproxy/haproxy.cfg -c   #检查配置文件

netstat -ntpl启动haproxy前检查端口占用，若占用则更改/etc/haproxy/haproxy.cfg对应的端口号

```bash
#启动haproxy配置文件
 haproxy -f /etc/haproxy/haproxy.cfg

#查看Haproxy 启动状态

ps -ef|grep haproxy
```

#### **网页访问**

访问haproxy网址
http://192.168.200.102:8100/stats
检查是否配置成功，所有节点表示绿色mq集群状态健康。

### **K** **eepalived**

**下载keepalived**

以2.2.8版本为例，如下为下载地址

https://www.keepalived.org/download.html

**安装依赖软件**

rpm -qa|grep automake|wc -l 检查没有automake则安装

yum -y install automake

**解压软件**

tar -xvf keepalived-2.2.8.tar.gz

**在keepalived-2.2.8目录执行./autogen.sh**

./autogen.sh

**使用configure命令配置安装目录与核心配置文件所在位置**

./configure --prefix=/usr/local/keepalived --sysconf=/etc

* prefix：keepalived安装的位置
* sysconf：keepalived核心配置文件所在位置，固定位置，改成其他位置则keepalived启动不了，/var/log/messages中会报错

**安装**

make && make install

**keepalived配置文件**

**I**
**fconfig查看网卡**

vim /etc/keepalived/keepalived.conf

**部分配置信息（只显示使用到的）：**

> global\_defs {
>
> # 路由id,主备节点不能相同
>
> router\_id node1
>
> vrrp\_skip\_check\_adv\_addr
>
> # 使用 unicast\_src\_ip 需要注释 vrrp\_strict，而且也可以进行 ping 测试
>
> #vrrp\_strict
>
> vrrp\_garp\_interval 0
>
> vrrp\_gna\_interval 0
>
> }
>
> # 自定义监控脚本
>
> vrrp\_script chk\_haproxy {
>
> # 脚本位置
>
> script "/etc/keepalived/haproxy\_check.sh"
>
> # 脚本执行的时间间隔
>
> interval 5
>
> # 权重
>
> weight 10
>
> }
>
> vrrp\_instance VI\_1 {
>
> # Keepalived的角色，MASTER 表示主节点，BACKUP 表示备份节点
>
> state MASTER
>
> # 指定监测的网卡，可以使用 ifconfig 进行查看
>
> interface ens192
>
> #指定发送单播的源IP
>
> mcast\_src\_ip 10.9.37.104 # 当前主机ip
>
> # 虚拟路由的id，主备节点需要设置为相同
>
> virtual\_router\_id 51
>
> # 优先级，主节点的优先级需要设置比备份节点高
>
> priority 100
>
> # 设置主备之间的检查时间，单位为秒
>
> advert\_int 1
>
> # 定义验证类型和密码
>
> authentication {
>
> auth\_type PASS
>
> auth\_pass w123456
>
> }
>
> # 调用上面自定义的监控脚本
>
> track\_script {
>
> chk\_haproxy
>
> }
>
> unicast\_peer {
>
> #指定接收单播的对方目标主机IP
>
> 172.21.9.203
>
> 172.21.9.202
>
> }
>
> virtual\_ipaddress {
>
> # 虚拟IP地址，可以设置多个
>
> 10.9.37.199
>
> }
>
> }

**/etc/keepalived/script/check\_haproxy.sh内容**

**vim**
**/etc/keepalived/script/check\_haproxy.sh**

> **#!/bin/bash**
>
> **LOGFILE="/var/log/keepalived-haproxy-status.log"**
>
> **date >> $LOGFILE**
>
> **A=`ps -C haproxy --no-header |wc -l`**
>
> **# 判断haproxy是否已经启动**
>
> **if [ $A -eq 0 ];then**
>
> **#如果没有启动，则启动**
>
> **echo "warning: restart haproxy" >> $LOGFILE**
>
> **haproxy -f /etc/haproxy/haproxy.cfg**
>
> **fi**
>
> **#睡眠3秒以便haproxy完全启动**
>
> **sleep**
> **3**
>
> **#如果haproxy还是没有启动，此时需要将本机的keepalived服务停掉，以便让VIP自动漂移到另外一台haproxy**
>
> **if [ $A -eq 0 ];then**
>
> **echo "fail: check\_haproxy status" >> $LOGFILE**
>
> **systemctl stop keepalived**
>
> **fi**
>
> **else**
>
> **echo "success: check\_haproxy status" >> $LOGFILE**
>
> **fi**

**说明：**
  
Keepalived组之间的心跳检查并不能察觉到HAproxy负载是否正常，所以需要使用此脚本。
  
在Keepalived主机上，开启此脚本检测HAproxy是否正常工作，如正常工作，记录日志。
  
如进程不存在，则尝试重启HAproxy，三秒后检测，如果还没有则关掉主Keepalived，此时备Keepalived检测到主Keepalive挂掉，接管VIP，继续服务。

#给脚本添加执行权限
  
chmod +x /etc/keepalived/script/check\_haproxy.sh

#配置ip转发
  
echo "net.ipv4.ip\_nonlocal\_bind = 1" >> /etc/sysctl.conf
  
#生效
  
sysctl -p

#启动keepalived并设置开机自启
  
systemctl start keepalived
  
systemctl enable keepalived