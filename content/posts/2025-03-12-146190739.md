---
layout: post
title: "AI论文MM-Eureka基于规则的大规模强化学习探索视觉啊哈时刻"
date: 2025-03-12 18:00:00 +0800
description: "我们提出了MM-Eureka，这是一个多模态推理模型，成功地将基于规则的大规模强化学习（RL）扩展到多模态推理领域。虽然基于规则的RL在提升大型语言模型（LLMs）在文本领域的推理能力方面已经取得了显著成功，但将其应用于多模态设置仍然充满挑战。我们的工作在多模态空间中再现了基于文本的RL系统（如DeepSeek-R1）的关键特性，包括准确率奖励和响应长度的稳步提升，以及反思行为的涌现。"
keywords: "mm-eureka"
categories: ['未分类']
tags: ['人工智能']
artid: "146190739"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146190739
    alt: "AI论文MM-Eureka基于规则的大规模强化学习探索视觉啊哈时刻"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146190739
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146190739
cover: https://bing.ee123.net/img/rand?artid=146190739
image: https://bing.ee123.net/img/rand?artid=146190739
img: https://bing.ee123.net/img/rand?artid=146190739
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【AI论文】MM-Eureka：基于规则的大规模强化学习探索视觉“啊哈”时刻
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p class="img-center">
     <img alt="" height="501" src="https://i-blog.csdnimg.cn/direct/ba456ef390834435ac2f1427f6b5fed8.png" width="874"/>
    </p>
    <p>
     <strong>
      摘要
     </strong>
     ：我们提出了MM-Eureka，这是一个多模态推理模型，成功地将基于规则的大规模强化学习（RL）扩展到多模态推理领域。虽然基于规则的RL在提升大型语言模型（LLMs）在文本领域的推理能力方面已经取得了显著成功，但将其应用于多模态设置仍然充满挑战。我们的工作在多模态空间中再现了基于文本的RL系统（如DeepSeek-R1）的关键特性，包括准确率奖励和响应长度的稳步提升，以及反思行为的涌现。我们证明，无论是指令微调模型还是预训练模型，都可以通过基于规则的RL发展出强大的多模态推理能力，而无需监督微调，与替代方法相比，这显示出了更高的数据效率。我们开源了完整的流程，以促进该领域的进一步研究。所有代码、模型、数据等均已发布在
     <a href="https://github.com/ModalMinds/MM-EUREKA%E3%80%82" title="https://github.com/ModalMinds/MM-EUREKA。">
      https://github.com/ModalMinds/MM-EUREKA。
     </a>
     Huggingface链接：
     <a href="https://huggingface.co/papers/2503.07365" rel="nofollow" title="Paper page ">
      Paper page
     </a>
     ，论文链接：
     <a href="https://arxiv.org/pdf/2503.07365" rel="nofollow" title="2503.07365">
      2503.07365
     </a>
    </p>
    <h4>
     研究背景与目的
    </h4>
    <p id="">
     <strong>
      研究背景
     </strong>
    </p>
    <p id="">
     随着人工智能技术的飞速发展，特别是大规模强化学习（RL）在提升大型语言模型（LLMs）推理能力方面的显著成效，研究人员开始探索将这一技术应用于更广泛的任务领域。在文本领域，基于规则的RL已经取得了令人瞩目的成果，显著提高了LLMs的推理能力。然而，将这种成功扩展到多模态设置（如涉及图像和文本的任务）仍然是一个巨大的挑战。多模态推理任务，如解释科学图表和几何推理，需要同时处理和分析来自不同模态的信息，这对模型的综合理解和推理能力提出了更高要求。
    </p>
    <p id="">
     尽管近年来有一些尝试将大规模RL技术应用于多模态场景的工作，但这些努力大多未能完全复现像在文本领域那样稳定增长的准确率奖励和响应长度，以及反思行为等关键特性。例如，R1-V虽然在简单的计数任务上有所改进，但未能实现响应长度和“啊哈”时刻（即模型在推理过程中突然领悟并解决问题的瞬间）的增加。R1-Multimodal-Journey虽然探索了几何问题，但响应长度随着训练的进行而下降。此外，虽然Kimi1.5在多模态推理方面取得了有竞争力的结果，但其模型和训练数据并未开源，限制了社区对该技术的进一步探索。
    </p>
    <p id="">
     <strong>
      研究目的
     </strong>
    </p>
    <p id="">
     鉴于上述背景，本研究旨在探索大规模基于规则的RL在多模态推理任务中的有效性，并开源完整的流程以促进该领域的进一步研究。具体目标包括：
    </p>
    <ol>
     <li>
      开发多模态推理模型MM-Eureka，该模型能够复现在多模态空间中基于文本的RL系统（如DeepSeek-R1）的关键特性。
     </li>
     <li>
      证明无论是指令微调模型还是预训练模型，都可以通过基于规则的RL发展出强大的多模态推理能力，而无需监督微调。
     </li>
     <li>
      通过开源完整的流程（包括代码、模型和数据），促进社区对多模态推理任务的进一步研究。
     </li>
    </ol>
    <h4>
     研究方法
    </h4>
    <p id="">
     <strong>
      基本设置
     </strong>
    </p>
    <p id="">
     本研究采用InternVL2.5作为基线模型，因为它提供了多种模型尺寸，适合进行扩展实验。为了系统地研究RL在不同尺寸模型（8B或38B）、指令微调或预训练模型以及使用蒸馏数据进行冷启动或不使用冷启动的模型上的影响，我们进行了大量实验。RL算法类似于DeepSeek-R1，使用基于规则的格式奖励（r_format∈{0,1}）和准确率奖励（r_accuracy∈{0,1}）进行训练。
    </p>
    <p id="">
     <strong>
      数据集
     </strong>
    </p>
    <p id="">
     本研究的数据集主要由开源数据组成，并手动收集了K-12级别的数学多模态推理问题，以增强数据的推理特性。数据收集过程涵盖了多个开源数据集，如GeoQA、ChartQA、MATH等，最终收集了75514个样本。为了进一步提高RL训练的数据质量，我们实施了数据清洗过程，包括排除没有清晰答案或难以用我们的基于规则的奖励函数正确解析的问题，以及使用InternVL2.5-8B-instruct模型估计问题的难度，并移除估计准确率为0或1的问题。经过清洗后，最终的训练数据集包含54931个样本。
    </p>
    <p id="">
     <strong>
      奖励函数
     </strong>
    </p>
    <p id="">
     本研究采用简单的基于规则的奖励函数，而不是使用结果或过程奖励模型，从而避免了奖励黑客攻击（reward hacking）。奖励函数包括准确率奖励和格式奖励，最终奖励定义为r=r_accuracy+λr_format，其中λ是平衡格式奖励贡献的缩放系数。
    </p>
    <p id="">
     <strong>
      优势估计和策略更新
     </strong>
    </p>
    <p id="">
     在强化学习训练阶段，我们采用REINFORCE Leave-One-Out（RLOO）算法，该算法不需要评论家模型，有效降低了训练成本。对于行动者损失，我们采用PPO-clip损失，而不是标准的REINFORCE目标。此外，虽然我们在损失计算中通常将KL散度权重αKL设置为0（因为在实验中表现更好），但我们仍然保留了KL散度作为正则化项的选项。
    </p>
    <h4>
     研究结果
    </h4>
    <p id="">
     <strong>
      模型性能
     </strong>
    </p>
    <p id="">
     我们开发了MM-Eureka-8B和MM-Eureka-Zero-38B两个模型，分别基于InternVL2.5-Instruct-8B和InternVL2.5-Pretrained-38B。实验结果表明，这两个模型在多模态推理任务上均表现出色。具体来说：
    </p>
    <ol>
     <li>
      <strong>
       训练过程中的稳定提升
      </strong>
      ：无论是基于指令微调模型还是预训练模型，MM-Eureka在训练过程中都实现了准确率奖励和响应长度的稳步提升。
     </li>
     <li>
      <strong>
       与替代方法的比较
      </strong>
      ：与MPO和SFT等替代方法相比，MM-Eureka在数据效率方面表现出色。例如，使用仅54K训练样本的MM-Eureka-8B模型在多个基准测试上的性能超过了使用1M数据训练的MPO模型，并且与使用12M数据进行COT SFT训练的模型性能相当。
     </li>
     <li>
      <strong>
       “零时刻”现象
      </strong>
      ：MM-Eureka-Zero-38B模型仅使用9.3K K-12数据样本进行训练，就在某些基准测试（如OlympiadBench和K12）上超过了使用16.3M数据进行SFT训练的指令微调模型，并且在其他基准测试上取得了可比的性能。这表明在多模态推理领域存在“零时刻”现象。
     </li>
    </ol>
    <p id="">
     <strong>
      视觉“啊哈”时刻
     </strong>
    </p>
    <p id="">
     在实验过程中，我们观察到了视觉“啊哈”时刻，即模型在推理过程中重新审视图像以寻找更多线索的行为。这表明多模态推理能力也可以通过大规模RL来培养。例如，模型首先规划了一个解决方案，然后在完成后反思其工作并采用了一种新方法来解决问题。
    </p>
    <h4>
     研究局限
    </h4>
    <p id="">
     <strong>
      数据利用效率
     </strong>
    </p>
    <p id="">
     尽管MM-Eureka在数据效率方面表现出色，但我们发现，基于难度的数据过滤策略虽然稳定了RL训练，但也浪费了一部分可用数据。我们尝试了在线数据过滤方法以提高数据利用效率，但实验结果表明，这种方法在准确率奖励或响应长度的提升方面表现不佳。我们认为这可能是由于每个训练轮次中用于更新的批次大小不同导致的梯度不稳定。
    </p>
    <p id="">
     <strong>
      小型模型的稳定性
     </strong>
    </p>
    <p id="">
     尽管一些工作成功地在纯语言设置中使用小型模型复现了R1-Zero场景，但我们发现，在多模态数学推理场景中，小型模型（如8B）难以维持稳定的基于规则的RL训练，相比之下，大型模型（如38B）则表现出更好的稳定性。因此，如何在多模态推理领域使用小型模型复现R1-Zero时刻仍然是一个需要进一步探索的问题。
    </p>
    <p id="">
     <strong>
      课程学习
     </strong>
    </p>
    <p id="">
     我们尝试了使用课程学习（curriculum learning）方法进行RL实验，但结果并不理想。尽管直觉上认为课程学习可以让模型逐渐学习，但我们发现与直接训练相比并没有优势）。我们认为这可能是由于简单的课程学习设置导致模型在早期和中期阶段固定于简单问题，从而阻碍了对困难问题的探索，并阻止了后期阶段的准确率提升。
    </p>
    <h4>
     未来研究方向
    </h4>
    <p id="">
     <strong>
      提高数据利用效率
     </strong>
    </p>
    <p id="">
     未来的研究可以探索更有效的方法来利用多模态数据，以提高RL训练的数据利用效率。这可能包括开发更智能的数据过滤策略、使用自监督学习方法来增强数据表示或结合多种数据增强技术。
    </p>
    <p id="">
     <strong>
      小型模型的稳定性训练
     </strong>
    </p>
    <p id="">
     针对小型模型在多模态推理任务中的稳定性问题，未来的研究可以探索更适合小型模型的RL算法或训练策略。这可能包括开发新的奖励函数、调整超参数或结合其他技术（如迁移学习）来提高小型模型的性能。
    </p>
    <p id="">
     <strong>
      跨模态交互的理解
     </strong>
    </p>
    <p id="">
     目前的研究主要集中在如何通过RL提高多模态模型的推理能力，但未来的研究可以进一步探索模型如何理解和处理跨模态交互。这可能包括分析模型在推理过程中如何整合来自不同模态的信息、如何识别和利用模态间的关联以及如何解释其推理过程。
    </p>
    <p id="">
     <strong>
      与人类的交互
     </strong>
    </p>
    <p id="">
     未来的研究还可以探索如何将多模态RL模型与人类用户进行更有效的交互。这可能包括开发更自然和用户友好的接口、提高模型对人类反馈的理解和利用能力以及探索模型在人类辅助下的持续学习机制。
    </p>
    <p id="">
     <strong>
      应用领域的拓展
     </strong>
    </p>
    <p id="">
     除了数学和科学推理任务外，未来的研究还可以探索将多模态RL模型应用于更广泛的领域，如医学图像诊断、自动驾驶和智能助手等。这可能需要针对特定领域的数据和任务特点进行模型定制和算法优化。
    </p>
    <p id="">
     综上所述，本研究在多模态推理领域取得了重要进展，通过开发MM-Eureka模型并探索其在大规模基于规则的RL训练中的表现，为未来的研究提供了新的思路和方法。然而，研究过程中也发现了一些局限性和挑战，这些将成为未来研究的重要方向。
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36363839393334312f:61727469636c652f64657461696c732f313436313930373339" class_="artid" style="display:none">
 </p>
</div>


