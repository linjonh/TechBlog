---
layout: post
title: "集成算法概述与分类"
date: 2025-08-25T17:35:40+0800
description: "核心思想：综合多个模型（“多个专家”）的判断，以获得比单一模型更好的预测效果。常见结合策略：简单平均法加权平均法投票法（少数服从多数）"
keywords: "集成算法概述与分类"
categories: ['未分类']
tags: ['算法']
artid: "150775257"
arturl: "https://blog.csdn.net/2401_85886980/article/details/150775257"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150775257
    alt: "集成算法概述与分类"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150775257
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150775257
cover: https://bing.ee123.net/img/rand?artid=150775257
image: https://bing.ee123.net/img/rand?artid=150775257
img: https://bing.ee123.net/img/rand?artid=150775257
---



# 集成算法概述与分类

## 集成算法概述

核心思想：综合多个模型（“多个专家”）的判断，以获得比单一模型更好的预测效果。  
常见结合策略：  
简单平均法  
加权平均法  
投票法（少数服从多数）

## 集成算法的分类

根据个体学习器之间的依赖关系和生成方式，集成学习分为三类：

### 1. Bagging（Bootstrap Aggregation）

特点：并行训练多个基学习器，彼此之间无强依赖。  
代表算法：随机森林（Random Forest）  
通过数据采样随机和特征选择随机构建多棵决策树。  
分类任务使用投票法，回归任务使用平均法。  
优势：  
处理高维数据，无需特征选择。  
可评估特征重要性。  
 支持并行化，训练速度快。  
 可可视化分析。

### 2. Boosting

特点：串行训练多个弱学习器，根据前一轮结果调整样本权重。  
代表算法：AdaBoost  
步骤：  
    1. 初始化样本权重。  
    2. 训练弱分类器，调整错分样本权重。  
    3. 组合多个弱分类器，按准确率赋予不同权重。  
 思想：逐步强化模型，重点关注难分的样本。

### 3. Stacking

特点：堆叠多种不同类型的模型，分阶段训练。  
  第一阶段：多个基模型独立预测。  
  第二阶段：使用第一阶段结果训练一个元模型（meta-model）进行最终预测。  
可融合多种模型：如KNN、SVM、随机森林等。



