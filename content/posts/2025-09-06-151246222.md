---
layout: post
title: "Scikit-learn-Python机器学习-特征降维-压缩数据-特征选择-单变量特征选择-SelectKBest-选择Top-K个特征"
date: 2025-09-06T08:25:12+0800
description: "Scikit-learn Python机器学习 - 特征降维 压缩数据 - 特征选择 - 单变量特征选择 SelectKBest - 选择Top K个特征"
keywords: "Scikit-learn Python机器学习 - 特征降维 压缩数据 - 特征选择 - 单变量特征选择 SelectKBest - 选择Top K个特征"
categories: ['未分类']
tags: ['机器学习', 'Python', 'Learn']
artid: "151246222"
arturl: "https://blog.csdn.net/caoli201314/article/details/151246222"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=151246222
    alt: "Scikit-learn-Python机器学习-特征降维-压缩数据-特征选择-单变量特征选择-SelectKBest-选择Top-K个特征"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=151246222
featuredImagePreview: https://bing.ee123.net/img/rand?artid=151246222
cover: https://bing.ee123.net/img/rand?artid=151246222
image: https://bing.ee123.net/img/rand?artid=151246222
img: https://bing.ee123.net/img/rand?artid=151246222
---



# Scikit-learn Python机器学习 - 特征降维 压缩数据 - 特征选择 - 单变量特征选择 SelectKBest - 选择Top K个特征



锋哥原创的Scikit-learn Python机器学习视频教程：

[2026版 Scikit-learn Python机器学习 视频教程(无废话版) 玩命更新中~_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV11reUzEEPH "2026版 Scikit-learn Python机器学习 视频教程(无废话版) 玩命更新中~_哔哩哔哩_bilibili")

### 课程介绍

![](https://i-blog.csdnimg.cn/direct/2995eccf645c44d1b4ab6d5bfe6a2c52.png)

### Scikit-learn Python机器学习 - 特征降维 压缩数据 - 特征选择 - 单变量特征选择 SelectKBest - 选择Top K个特征

基于统计检验选择最佳特征。

`SelectKBest` 的原理非常直观，其名称就完美概括了其工作方式： **Select（选择） + K + Best（最好的）**

顾名思义，它的目标是从原始特征集中**选择出 K 个“最好的”特征**。那么，核心问题就变成了：**如何定义“最好”？**

`SelectKBest` 的工作流程可以概括为以下三个步骤：

1. **打分（Scoring）**：

   * 对于数据集中的**每一个特征**，都使用一个特定的**评分函数** `f` 进行计算。
   * 这个评分函数会计算该特征与**目标变量** `y` 之间的某种统计关系或依赖性。关系越强，得分越高。
   * 例如，它可以使用卡方检验、相关系数、互信息等作为评分标准。
2. **排序（Ranking）**：

   * 得到所有特征及其对应的分数后，`SelectKBest` 会**根据分数从高到低**对所有特征进行排序。
3. **选择（Selecting）**：

   * 最后，它简单地**保留Top-K个得分最高的特征**，并剔除其余的所有特征。
   * 用户指定的参数 `k` 就是这里需要保留的特征数量。

**🧠 核心参数详解**

| 参数名 | 说明 | 默认值 |
| --- | --- | --- |
| **score_func** | **【最重要的参数】** 用于计算特征得分的函数。它决定了“最好”的标准。 | `f_classif` (用于分类) |
| **k** | **【核心参数】** 选择要保留的 top K 个特征。可以设置为整数 ‘all’ 来保留所有特征。 | `10` |

**常见的 score_func 评分函数：**

选择哪个评分函数取决于你的问题类型（分类还是回归）以及特征的数据类型。

| 评分函数 | 适用问题 | 说明 |
| --- | --- | --- |
| **f_classif** | **分类** | 计算每个特征与目标变量之间的 **ANOVA F值**。适用于连续特征和分类目标。默认选项。 |
| **chi2** | **分类** | **卡方检验**。计算每个特征与目标变量之间的卡方统计量。**适用于非负的特征**（如词频、布尔特征）。 |
| **mutual_info_classif** | **分类** | **互信息**。衡量特征和目标变量之间的**非线性关系**。非常强大，但计算成本更高。 |
| **f_regression** | **回归** | 计算每个特征与目标变量之间的 **F值**（线性回归模型的简单线性回归）。 |
| **mutual_info_regression** | **回归** | 互信息的回归版本，同样用于捕捉非线性关系。 |

📊 工作流程示意图

![](https://i-blog.csdnimg.cn/direct/186f230e8da04786a18404d4bd11fb8e.png)

我们来看一个示例：

```
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.datasets import load_iris
​
# 加载数据
X, y = load_iris(return_X_y=True)
​
# 选择最佳的2个特征
selector = SelectKBest(score_func=f_classif, k=3)
X_new = selector.fit_transform(X, y)
​
print(f"原始特征数: {X.shape[1]}")
print(f"筛选后特征数: {X_new.shape[1]}")
print(f"特征得分: {selector.scores_}")
```

运行结果：

```
原始特征数: 4
筛选后特征数: 3
特征得分: [ 119.26450218   49.16004009 1180.16118225  960.0071468 ]
```



