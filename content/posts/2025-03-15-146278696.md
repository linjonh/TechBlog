---
layout: post
title: "es-将知识库中的数据转换为向量存储到es并进行相似性检索"
date: 2025-03-15 15:52:26 +0800
description: "需要检查该文件的编码格式，用文本的方式打开该文件，如果不是utf-8，则另存为，并指定编码格式为utf-8，在改了编码格式后，再用excel打开，可能会变为乱码，不用管，只要保证编码为utf-8,且用文本打开能正常显示就行。然后运行脚本就可以了。（BM25 算法），但是它不适合语义匹配，比如如果用户输入的是“求解 x² - 5x + 6 = 0”，但我的文档是“二次方程求解方法”，ES 可能不会返回这个文档，因为它不包含完全匹配的关键词。而如果将数据作为向量存入ES(语义搜索)，则可以查找语义相似的内容。"
keywords: "es-将知识库中的数据转换为向量存储到es并进行相似性检索"
categories: ['Es']
tags: ['搜索引擎', '大数据', 'Elasticsearch']
artid: "146278696"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146278696
    alt: "es-将知识库中的数据转换为向量存储到es并进行相似性检索"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146278696
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146278696
cover: https://bing.ee123.net/img/rand?artid=146278696
image: https://bing.ee123.net/img/rand?artid=146278696
img: https://bing.ee123.net/img/rand?artid=146278696
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     es-将知识库中的数据转换为向量存储到es并进行相似性检索
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <hr id="hr-toc" name="tableOfContents"/>
    <p>
    </p>
    <h2 id="%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%B0%86%E6%95%B0%E6%8D%AE%E8%BD%AC%E4%B8%BA%E5%90%91%E9%87%8F%E5%AD%98%E5%85%A5es%3F" name="%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%B0%86%E6%95%B0%E6%8D%AE%E8%BD%AC%E4%B8%BA%E5%90%91%E9%87%8F%E5%AD%98%E5%85%A5es%3F">
     为什么要将数据转为向量存入es?
    </h2>
    <p>
     我之前把数据作为文档存入 ES，主要用于
     <strong>
      全文检索
     </strong>
     （BM25 算法），但是它不适合语义匹配，比如如果用户输入的是“求解 x² - 5x + 6 = 0”，但我的文档是“二次方程求解方法”，ES 可能不会返回这个文档，因为它不包含完全匹配的关键词。
    </p>
    <p>
     而如果将数据作为向量存入ES(语义搜索)，则可以查找语义相似的内容。
    </p>
    <h2 id="%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87" name="%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87">
     数据准备
    </h2>
    <p>
     我这里准备包含250条数学文档的csv文件：
    </p>
    <p>
     <img alt="" height="663" src="https://i-blog.csdnimg.cn/direct/e3b372184ee94ee2a0a101cea75c0fa9.png" width="1493"/>
    </p>
    <p>
     需要检查该文件的编码格式，用文本的方式打开该文件，如果不是utf-8，则另存为，并指定编码格式为utf-8，在改了编码格式后，再用excel打开，可能会变为乱码，不用管，只要保证编码为utf-8,且用文本打开能正常显示就行。
    </p>
    <h2 id="%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E5%BA%93" name="%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%E5%BA%93">
     创建索引库
    </h2>
    <p>
     我使用的es版本为7.12.1
    </p>
    <p>
     打开devtools工具，创建一个名为math_index的索引库
    </p>
    <pre><code class="hljs">PUT /math_index
{
    "settings": {
        "number_of_shards": 3,
        "number_of_replicas": 1
    },
    "mappings": {
        "properties": {
            "ask_vector": {  
                "type": "dense_vector",  
                "dims": 1024  
            },
			"ask": {  
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_smart"
            },
            "answer": {  
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_smart"
            }
        }
    }
}</code></pre>
    <p id="1.%20%E7%B4%A2%E5%BC%95%E8%AE%BE%E5%AE%9A%20(settings)" name="1.%20%E7%B4%A2%E5%BC%95%E8%AE%BE%E5%AE%9A%20(settings)">
     <strong>
      1. 索引设定 (
      <code>
       settings
      </code>
      )
     </strong>
    </p>
    <ul>
     <li>
      <code>
       number_of_shards: 3
      </code>
      ：该索引被分成 3 个分片，提高查询和索引的吞吐量。
     </li>
     <li>
      <code>
       number_of_replicas: 1
      </code>
      ：每个主分片有 1 个副本，提高数据的可用性和容错性。
     </li>
    </ul>
    <p id="2.%20%E5%AD%97%E6%AE%B5%E6%98%A0%E5%B0%84%20(mappings)" name="2.%20%E5%AD%97%E6%AE%B5%E6%98%A0%E5%B0%84%20(mappings)">
     <strong>
      2. 字段映射 (
      <code>
       mappings
      </code>
      )
     </strong>
    </p>
    <ul>
     <li>
      <strong>
       <code>
        ask_vector
       </code>
      </strong>
      ：
      <ul>
       <li>
        类型为
        <code>
         dense_vector
        </code>
        ，维度为
        <code>
         1024
        </code>
        ，用于存储文本的向量表示（通常用于语义搜索，如基于向量相似度的检索）。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       <code>
        ask
       </code>
      </strong>
      ：
      <ul>
       <li>
        类型为
        <code>
         text
        </code>
        ，用于存储用户问题文本。
       </li>
       <li>
        <code>
         analyzer: "ik_max_word"
        </code>
        ：索引时使用
        <code>
         ik_max_word
        </code>
        （细粒度分词）。
       </li>
       <li>
        <code>
         search_analyzer: "ik_smart"
        </code>
        ：搜索时使用
        <code>
         ik_smart
        </code>
        （较粗粒度分词，提升搜索效率）。
       </li>
      </ul>
     </li>
     <li>
      <strong>
       <code>
        answer
       </code>
      </strong>
      ：
      <ul>
       <li>
        类型为
        <code>
         text
        </code>
        ，用于存储回答文本。
       </li>
       <li>
        同样采用
        <code>
         ik_max_word
        </code>
        进行索引，
        <code>
         ik_smart
        </code>
        进行搜索。
       </li>
      </ul>
     </li>
    </ul>
    <h2 id="%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8" name="%E5%90%91%E9%87%8F%E5%AD%98%E5%82%A8">
     向量存储
    </h2>
    <pre><code class="hljs">from elasticsearch import Elasticsearch
from transformers import BertTokenizer, BertModel
import torch
import pandas as pd


def embeddings_doc(doc, tokenizer, model, max_length=300):
    encoded_dict = tokenizer.encode_plus(
        doc,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    input_id = encoded_dict['input_ids']
    attention_mask = encoded_dict['attention_mask']

    # 前向传播
    with torch.no_grad():
        outputs = model(input_id, attention_mask=attention_mask)

    # 提取最后一层的CLS向量作为文本表示
    last_hidden_state = outputs.last_hidden_state
    cls_embeddings = last_hidden_state[:, 0, :]
    return cls_embeddings[0]


def add_doc(index_name, id, embedding_ask, ask, answer, es):
    body = {
        "ask_vector": embedding_ask.tolist(),
        "ask": ask,
        "answer": answer
    }
    result = es.create(index=index_name, id=id, doc_type="_doc", body=body)
    return result


def main():
    # 模型下载的地址
    model_name = 'D:\\model\\chinese-roberta-wwm-ext-large'
    # ES 信息
    es_host = "http://your_ip"
    es_port = 9200
    es_user = ""
    es_password = ""
    index_name = "math_index"

    # 数据地址
    path = "D:\\Downloads\\知识库1.4.csv"

    # 分词器和模型
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name)

    # ES 连接
    es = Elasticsearch(
        [es_host],
        port=es_port,
        http_auth=(es_user, es_password)
    )

    # 读取数据写入ES
    data = pd.read_csv(path, encoding='utf-8')
    for index, row in data.iterrows():
        ask = row["题目"]
        answer = row["答案"]
        # 文本转向量
        embedding_ask = embeddings_doc(ask, tokenizer, model)
        result = add_doc(index_name, index, embedding_ask, ask, answer, es)
        print(result)


if __name__ == '__main__':
    main()
</code></pre>
    <p>
     里面的模型文件在这里下载：
    </p>
    <p>
     <a href="https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/" rel="nofollow" title="https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/">
      https://huggingface.co/hfl/chinese-roberta-wwm-ext-large/
     </a>
    </p>
    <p>
     把这几个文件下载下来，放到一个文件夹中：
    </p>
    <p>
     <img alt="" height="663" src="https://i-blog.csdnimg.cn/direct/2fc3ed34a8a14e58a863adf3c4d906b3.png" width="1678"/>
    </p>
    <p>
     然后运行脚本就可以了。（这里的es依赖用到7.12.1版本：pip install elasticsearch==7.12.1 -i https://pypi.tuna.tsinghua.edu.cn/simple）
    </p>
    <p>
     运行结束后，es就存储了知识库数据以及生成的向量：
    </p>
    <p>
     <img alt="" height="674" src="https://i-blog.csdnimg.cn/direct/5deeb52cf601437589f88b2103466153.png" width="872"/>
    </p>
    <h2 id="%E9%AA%8C%E8%AF%81" name="%E9%AA%8C%E8%AF%81">
     验证
    </h2>
    <p>
     这里使用余弦相似度进行相似性检索
    </p>
    <pre><code class="hljs">from elasticsearch import Elasticsearch
from transformers import BertTokenizer, BertModel
import torch


def embeddings_doc(doc, tokenizer, model, max_length=300):
    encoded_dict = tokenizer.encode_plus(
        doc,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    input_id = encoded_dict['input_ids']
    attention_mask = encoded_dict['attention_mask']

    # 前向传播
    with torch.no_grad():
        outputs = model(input_id, attention_mask=attention_mask)

    # 提取最后一层的CLS向量作为文本表示
    last_hidden_state = outputs.last_hidden_state
    cls_embeddings = last_hidden_state[:, 0, :]
    return cls_embeddings[0]


def search_similar(index_name, query_text, tokenizer, model, es, top_k=3):
    query_embedding = embeddings_doc(query_text, tokenizer, model)
    print(query_embedding.tolist())
    query = {
        "query": {
            "script_score": {
                "query": {"match_all": {}},
                "script": {
                    "source": "cosineSimilarity(params.queryVector, 'ask_vector') + 1.0",
                    "lang": "painless",
                    "params": {
                        "queryVector": query_embedding.tolist()
                    }
                }
            }
        },
        "size": top_k
    }
    res = es.search(index=index_name, body=query)
    hits = res['hits']['hits']
    similar_documents = []
    for hit in hits:
        similar_documents.append(hit['_source'])
    return similar_documents


def main():
    # 模型下载的地址
    model_name = 'D:\\model\\chinese-roberta-wwm-ext-large'
    # ES 信息
    es_host = "http://your_ip"
    es_port = 9200
    es_user = ""
    es_password = ""
    index_name = "math _index"

    # 分词器和模型
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertModel.from_pretrained(model_name)

    # ES 连接
    es = Elasticsearch(
        [es_host],
        port=es_port,
        http_auth=(es_user, es_password)
    )

    query_text = "在复平面内，(1+3i)(3-i) 对应的点位于哪个象限"

    similar_documents = search_similar(index_name, query_text, tokenizer, model, es)
    for item in similar_documents:
        print("================================")
        print('ask：', item['ask'])
        print('answer：', item['answer'])


if __name__ == '__main__':
    main()

</code></pre>
    <p>
     得到的结果（找到的相似度前十的数据）：
    </p>
    <p>
     <img alt="" height="492" src="https://i-blog.csdnimg.cn/direct/b05b06b228a7493abc60aed9f766a2e4.png" width="1156"/>
    </p>
    <p>
     可以看到，第一个数据确实最为相似。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37343436323333392f:61727469636c652f64657461696c732f313436323738363936" class_="artid" style="display:none">
 </p>
</div>


