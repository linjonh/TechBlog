---
layout: post
title: "第N4周NLP中的文本嵌入"
date: 2025-03-10 08:20:02 +0800
description: "此处定义了一个自定义的批处理函数，用于将数据加载到DataLoader时对文本数据进行填充（padding）和标签处理。这个函数的目的是确保每个批次中的文本数据长度一致，并将标签转换为适合模型输入的格式。: 将批次中的数据解包为文本和标签。: 计算批次中所有文本的最大长度。使用F.pad在文本的右侧填充（padding）零，使其长度与最大长度一致（max_len）。（1）F.pad: 这是 PyTorch 的函数，用于对张量进行填充。（2）text: 这里假设text。"
keywords: "第N4周：NLP中的文本嵌入"
categories: ['未分类']
tags: ['自然语言处理', '人工智能']
artid: "145219434"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145219434
    alt: "第N4周NLP中的文本嵌入"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145219434
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145219434
cover: https://bing.ee123.net/img/rand?artid=145219434
image: https://bing.ee123.net/img/rand?artid=145219434
img: https://bing.ee123.net/img/rand?artid=145219434
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     第N4周：NLP中的文本嵌入
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <blockquote>
     <p>
      本人往期文章可查阅：
      <a href="https://blog.csdn.net/weixin_43414521/article/details/140057978?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=140057978&amp;sharerefer=PC&amp;sharesource=weixin_43414521&amp;sharefrom=from_link" title="深度学习总结">
       深度学习总结
      </a>
     </p>
    </blockquote>
    <p id="uae5fde31">
    </p>
    <p>
     <strong>
      词嵌入是一种用于自然语言处理（NLP）的技术，用于将单词表示为数字，以便计算机可以处理它们。通俗的讲就是，一种把文本转为数值输入到计算机中的方法。
     </strong>
     之前文章中提到的将文本转换为字典序列、one-hot编码就是最早期的词嵌入方法。
    </p>
    <p>
     <span style="background-color:#d7d8d9">
      Embedding
     </span>
     和
     <span style="background-color:#d7d8d9">
      EmbeddingBag
     </span>
     则是PyTorch 中的用来处理文本数据中词嵌入（word embedding）的工具，它们将离散的词汇映射到低维的连续向量空间中，使得词汇之间的语义关系能够在向量空间中得到体现。
    </p>
    <h2>
     1. Embedding 嵌入
    </h2>
    <p>
     <span style="background-color:#d7d8d9">
      Embedding
     </span>
     是 PyTorch 中最基本的词嵌入操作，
     <span style="background-color:#d7d8d9">
      Embedding
     </span>
     的输入是一个整数张量，每个整数都代表着一个词汇的索引，输出是一个浮点型的张量，每个浮点数都代表着对应词汇的词嵌入向量。
    </p>
    <ul>
     <li id="u9fc0278a">
      <strong>
       输入shape：
       <span style="background-color:#d7d8d9">
        [batch,seqSize]
       </span>
      </strong>
      （seqSize为单个文本长度）
     </li>
     <li>
      <strong>
       输出shape：
       <span style="background-color:#d7d8d9">
        [batch,seqSize,embed_dim]
       </span>
      </strong>
      （embed_dim嵌入维度）
     </li>
    </ul>
    <p>
    </p>
    <p>
     嵌入层使用
     <strong>
      随机权重初始化
     </strong>
     ，并将学习数据集中所有词的嵌入。它是一个灵活的层，可以以各种方式使用，如：
    </p>
    <ul>
     <li>
      它可以用作深度学习模型的一部分，其中嵌入于模型本身一起被学习。
     </li>
     <li>
      它可以用于加载训练好的词嵌入模型。
     </li>
    </ul>
    <p>
     嵌入层被定义为网络的
     <strong>
      第一个隐藏层
     </strong>
     。
    </p>
    <p>
     <strong>
      函数原型：
     </strong>
    </p>
    <pre><code>import torch

torch.nn.Embedding(
    num_embeddings,
    embedding_dim,
    padding_idx=None,
    max_norm=None,
    norm_type=2.0,
    scale_grad_by_freq=False,
    sparse=False,
    _weight=None,
    _freeze=False,
    device=None,
    dtype=None
)</code></pre>
    <p>
     <strong>
      官方API地址：
      <a class="link-info" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="nofollow" title="Embedding——PyTorch 2.0 documentation">
       Embedding——PyTorch 2.0 documentation
      </a>
     </strong>
    </p>
    <p>
     <strong>
      常见参数：
     </strong>
    </p>
    <ul>
     <li>
      <p>
       <strong>
        num_embeddings
       </strong>
       ：嵌入的总数，也就是离散变量的类别数。例如在单词嵌入中，就是词汇表的大小，即，最大整数index+1。
      </p>
     </li>
     <li>
      <p>
       <strong>
        embedding_dim
       </strong>
       ：每个嵌入向量的维度，即转换后的连续向量的维度。这个维度决定了嵌入向量的表达能力，维度越高，理论上表达能力越强，但计算成本和参数量也会增加。
      </p>
     </li>
     <li>
      <p>
       <strong>
        padding_idx
       </strong>
       （可选）：如果设置，那么索引为
       <code>
        padding_idx
       </code>
       的嵌入向量会被初始化为全零向量，并且在训练过程中保持不变。这在处理填充（padding）操作时很有用，比如在处理不同长度的文本序列时，可以用
       <code>
        padding_idx
       </code>
       来填充较短的序列，使其长度一致。
      </p>
     </li>
     <li>
      <p>
       <strong>
        max_norm
       </strong>
       （可选）：如果设置，每个嵌入向量的范数会被限制在
       <code>
        max_norm
       </code>
       以内。这有助于控制嵌入向量的大小，避免过大的数值导致数值不稳定等问题。
      </p>
     </li>
     <li>
      <p>
       <strong>
        norm_type
       </strong>
       （可选）：与
       <code>
        max_norm
       </code>
       一起使用，指定范数的类型，默认为 2，即 L2 范数。
      </p>
     </li>
     <li>
      <p>
       <strong>
        scale_grad_by_freq
       </strong>
       （可选）：如果设置为
       <code>
        True
       </code>
       ，那么在反向传播时，嵌入向量的梯度会被其频率缩放。这在处理稀疏数据时可能有助于稳定训练。
      </p>
     </li>
     <li>
      <p>
       <strong>
        sparse
       </strong>
       （可选）：如果设置为
       <code>
        True
       </code>
       ，则梯度会被稀疏化，这在某些情况下可以节省内存和计算资源，但可能会导致一些优化器无法使用。
      </p>
     </li>
    </ul>
    <p>
     下面是一个简单的例子，用
     <span style="background-color:#d7d8d9">
      Embedding
     </span>
     将两个句子转换为词嵌入向量：
    </p>
    <h3>
     1.1 自定义数据集类
    </h3>
    <pre><code>import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader,Dataset

class MyDataset(Dataset):
    def __init__(self,texts,labels):
        self.texts=texts
        self.labels=labels
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self,idx):
        texts=self.texts[idx]
        labels=self.labels[idx]
        
        return texts,labels</code></pre>
    <h3>
     1.2 定义填充函数
    </h3>
    <pre><code>def collate_batch(batch):
    texts,labels=zip(*batch)
    max_len=max(len(text) for text in texts)
    padded_texts=[F.pad(text,(0,max_len-len(text)),value=0) for text in texts]
    padded_texts=torch.stack(padded_texts)
    labels=torch.tensor(labels,dtype=torch.float).unsqueeze(1)
    return padded_texts,labels</code></pre>
    <p>
     此处定义了一个自定义的批处理函数，用于将数据加载到
     <code>
      DataLoader
     </code>
     时对文本数据进行填充（padding）和标签处理。这个函数的目的是确保每个批次中的文本数据长度一致，并将标签转换为适合模型输入的格式。
    </p>
    <ul>
     <li>
      <p>
       <strong>
        <code>
         texts, labels = zip(*batch)
        </code>
       </strong>
       : 将批次中的数据解包为文本和标签。
      </p>
     </li>
     <li>
      <p>
       <strong>
        <code>
         max_len = max(len(text) for text in texts)
        </code>
       </strong>
       : 计算批次中所有文本的最大长度。
      </p>
     </li>
     <li>
      <p>
       <strong>
        <code>
         padded_texts
        </code>
       </strong>
       : 计算每个文本需要填充的长度：
       <code>
        max_len - len(text)，
       </code>
       使用
       <code>
        F.pad
       </code>
       在文本的右侧填充（padding）零，使其长度与最大长度一致（max_len）。
      </p>
     </li>
    </ul>
    <p>
     （1）
     <strong>
      <code>
       F.pad
      </code>
     </strong>
     : 这是 PyTorch 的
     <code>
      torch.nn.functional.pad
     </code>
     函数，用于对张量进行填充。
    </p>
    <p>
     （2）
     <strong>
      <code>
       text
      </code>
     </strong>
     : 这里假设
     <code>
      text
     </code>
     是一个一维张量，表示一个文本序列（例如，经过编码的
     <code>
      input_ids
     </code>
     ）
    </p>
    <p>
     （3）
     <strong>
      <code>
       max_len
      </code>
     </strong>
     : 批次中所有文本的最大长度
    </p>
    <p>
     （4）
     <strong>
      <code>
       len(text)
      </code>
     </strong>
     : 当前文本的长度
    </p>
    <p>
     （5）
     <strong>
      <code>
       value=0
      </code>
     </strong>
     : 填充的值为 0
    </p>
    <ul>
     <li>
      <p>
       <strong>
        <code>
         padded_texts=torch.stack(padded_texts)：
        </code>
       </strong>
       将填充后的张量列表堆叠成一个更高维度的张量。这一步是必要的，因为
       <code>
        torch.stack
       </code>
       可以将多个张量沿着一个新的维度堆叠起来，从而形成一个批次化的张量。
      </p>
     </li>
    </ul>
    <p>
     假设
     <code>
      padded_texts
     </code>
     是一个填充后的张量列表，每个张量的形状为
     <code>
      [sequence_length]
     </code>
     ，
     <code>
      torch.stack
     </code>
     会将这些张量堆叠成一个更高维度的张量，形状为
     <code>
      [batch_size, sequence_length]
     </code>
     。
    </p>
    <p>
     示例：
    </p>
    <pre><code>padded_texts = [
    torch.tensor([101, 2023, 2003, 102]),
    torch.tensor([101, 2003, 102, 0]),
    torch.tensor([101, 2054, 2000, 102])
]</code></pre>
    <p>
     使用
     <code>
      torch.stack(padded_texts)
     </code>
     后，结果为：
    </p>
    <pre><code>tensor([
    [101, 2023, 2003, 102],
    [101, 2003, 102,   0],
    [101, 2054, 2000, 102]
])</code></pre>
    <ul>
     <li>
      <p>
       <strong>
        <code>
         labels=torch.tensor(labels,dtype=torch.float).unsqueeze(1)：
        </code>
       </strong>
       将标签转换为浮点型张量，并增加一个维度。这种操作在某些任务中非常常见，尤其是在处理分类任务时，需要将标签转换为适合模型输出的形式。
      </p>
     </li>
    </ul>
    <p>
     （1）
     <strong>
      <code>
       torch.tensor(labels, dtype=torch.float)
      </code>
     </strong>
     : 将标签列表转换为浮点型张量。这通常用于二分类任务，其中标签通常是 0 和 1。
    </p>
    <p>
     （2）
     <strong>
      <code>
       .unsqueeze(1)
      </code>
     </strong>
     : 在第 1 维度（索引从 0 开始）增加一个维度。这会将标签从一维张量
     <code>
      [batch_size]
     </code>
     转换为二维张量
     <code>
      [batch_size, 1] 。
     </code>
    </p>
    <hr/>
    <p>
     <strong>
      为什么需要
      <code>
       .unsqueeze(1)
      </code>
     </strong>
    </p>
    <p>
     在某些情况下，模型的输出是一个二维张量，形状为
     <code>
      [batch_size, num_classes]
     </code>
     ，而标签需要与模型输出的形状一致。例如：
    </p>
    <ul>
     <li>
      <p>
       在二分类任务中，模型输出的形状可能是
       <code>
        [batch_size, 1]
       </code>
       。
      </p>
     </li>
     <li>
      <p>
       在多分类任务中，模型输出的形状可能是
       <code>
        [batch_size, num_classes]
       </code>
       ，但标签需要从
       <code>
        [batch_size]
       </code>
       转换为
       <code>
        [batch_size, 1]
       </code>
       。
      </p>
     </li>
    </ul>
    <p>
     通过使用
     <code>
      .unsqueeze(1)
     </code>
     ，可以确保标签的形状与模型输出一致，从而避免形状不匹配的错误。
    </p>
    <h3>
     1.3 准备数据和数据加载器
    </h3>
    <pre><code># 假设我们有以下三个样本，分别由不同数量的单词索引组成
text_data=[
    torch.tensor([1,1,1,1],dtype=torch.long),  # 样本1
    torch.tensor([2,2,2],dtype=torch.long),  # 样本2
    torch.tensor([3,3],dtype=torch.long)   # 样本3
]

# 对应的标签
labels=torch.tensor([4,5,6],dtype=torch.float)

# 创建数据集和数据加载器
my_dataset=MyDataset(text_data,labels)
data_loader=DataLoader(my_dataset,batch_size=2,shuffle=True,collate_fn=collate_batch)

for batch in data_loader:
    print(batch)</code></pre>
    <p>
     代码输出：
    </p>
    <pre><code>(tensor([[2, 2, 2],
        [3, 3, 0]]), tensor([[5.],
        [6.]]))
(tensor([[1, 1, 1, 1]]), tensor([[4.]]))</code></pre>
    <h3>
     1.4 定义模型
    </h3>
    <pre><code>class EmbeddingModel(nn.Module):
    def __init__(self,vocab_size,embed_dim):
        super(EmbeddingModel,self).__init__()
        self.embedding=nn.Embedding(vocab_size,embed_dim)
        self.fc=nn.Linear(embed_dim,1) # 假设我们做一个二分类任务
        
    def forward(self,text):
        print("embedding输入文本是：",text)
        print("embedding输入文本是shape：",text.shape)
        embedding=self.embedding(text)
        embedding_mean=embedding.mean(dim=1) # 对每个样本的嵌入向量进行平均
        print("embedding输出文本shape：",embedding_mean.shape)
        return self.fc(embedding_mean)</code></pre>
    <p>
     （1）
     <strong>
      <code>
       nn.Embedding(vocab_size, embed_dim)
      </code>
     </strong>
     : 这是一个嵌入层，将输入的索引序列（
     <code>
      text
     </code>
     ）映射到一个固定维度的嵌入向量
    </p>
    <ul>
     <li>
      <p>
       <code>
        vocab_size
       </code>
       : 词汇表大小，即输入索引的最大值加1。
      </p>
     </li>
     <li>
      <p>
       <code>
        embed_dim
       </code>
       : 嵌入向量的维度。
      </p>
     </li>
    </ul>
    <p>
     （2）
     <code>
      embedding_mean = embedding.mean(dim=1)
     </code>
    </p>
    <p>
    </p>
    <ul>
     <li>
      <p>
       这一步对每个样本的嵌入向量沿着序列长度维度（
       <code>
        dim=1
       </code>
       ）进行平均，得到每个样本的固定长度嵌入表示。
      </p>
     </li>
     <li>
      <p>
       输出的形状为
       <code>
        [batch_size, embed_dim]
       </code>
      </p>
     </li>
    </ul>
    <p>
    </p>
    <h5>
     （3）
     <code>
      nn.Linear(embed_dim, 1)
     </code>
    </h5>
    <ul>
     <li>
      <p>
       这是一个全连接层，将嵌入向量映射到输出维度为 1 的结果。
      </p>
     </li>
     <li>
      <p>
       输出的形状为
       <code>
        [batch_size, 1]
       </code>
       ，适用于二分类任务。
      </p>
     </li>
    </ul>
    <blockquote>
     <p>
      特别注意：
     </p>
     <p>
      如果使用
      <span style="background-color:#d7d8d9">
       embedding_mean=embedding.mean(dim=1)
      </span>
      语句对每个样本的嵌入向量求平均，输出shape为
      <span style="background-color:#d7d8d9">
       [batch,embed_dim]
      </span>
      。若注释掉该语句，输出shape则为
      <span style="background-color:#d7d8d9">
       [batch,seqSize,embed_dim]
      </span>
      。
     </p>
    </blockquote>
    <h3>
     1.5 训练模型
    </h3>
    <pre><code># 示例词典大小和嵌入维度
vocab_size=10
embed_dim=6

# 创建模型实例
model=EmbeddingModel(vocab_size,embed_dim)

# 定义一个简单的损失函数和优化器
criterion=nn.BCEWithLogitsLoss()
optimizer=optim.SGD(model.parameters(),lr=0.01)

# 训练模型
for epoch in range(1): # 训练1个epoch
    for batch in data_loader:
        texts,labels=batch
        
        # 前向传播
        outputs=model(texts)
        loss=criterion(outputs,labels)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    print(f'Epoch {epoch+1},Loss:{loss.item()}')</code></pre>
    <p>
     代码输出：
    </p>
    <pre><code>embedding输入文本是： tensor([[3, 3, 0],
        [2, 2, 2]])
embedding输入文本是shape： torch.Size([2, 3])
embedding输出文本shape： torch.Size([2, 6])
embedding输入文本是： tensor([[1, 1, 1, 1]])
embedding输入文本是shape： torch.Size([1, 4])
embedding输出文本shape： torch.Size([1, 6])
Epoch 1,Loss:3.57496976852417</code></pre>
    <h2>
     2. EmbeddingBag嵌入
    </h2>
    <p>
     <span style="background-color:#d7d8d9">
      EmbeddingBag
     </span>
     是在
     <span style="background-color:#d7d8d9">
      Embedding
     </span>
     基础上进一步优化的工具，其核心思想是将每个输入序列的嵌入向量进行合并，能够处理可变长度的输入序列，并且减少了计算和存储的开销，并且可以计算句子中所有词汇的词嵌入向量的均值或总和。
    </p>
    <p>
     在PyTorch中，
     <span style="background-color:#d7d8d9">
      EmbeddingBag
     </span>
     的输入是一个
     <strong>
      整数张量
     </strong>
     和一个
     <strong>
      偏移量张量
     </strong>
     ，每个整数都代表着一个词汇的索引，偏移量则表示句子中每个词汇的位置，输出是一个浮点型的张量，每个浮点数都代表着对应句子的词嵌入向量的均值或总和。
    </p>
    <ul>
     <li>
      <strong>
       输入shape：
       <span style="background-color:#d7d8d9">
        [seqSize]
       </span>
      </strong>
      （seqSize为单个文本长度）
     </li>
     <li>
      <strong>
       输出shape：
       <span style="background-color:#d7d8d9">
        [batch, embed_dim]
       </span>
      </strong>
      （embed_dim嵌入维度）
     </li>
    </ul>
    <blockquote>
     <p>
      假定原始输入数据为：
      <span style="background-color:#d7d8d9">
       [[1,1,1,1],[2,2,2],[3,3]]
      </span>
     </p>
     <p>
      <strong>
       1. 输入：
      </strong>
     </p>
     <ul>
      <li>
       输入是一个展平的词汇索引张量（
       <span style="background-color:#d7d8d9">
        input
       </span>
       ），例如
       <span style="background-color:#d7d8d9">
        [2,2,2,1,1,1,1]
       </span>
       。
      </li>
      <li>
       对应的偏移量（
       <span style="background-color:#d7d8d9">
        offsets
       </span>
       ），例如
       <span style="background-color:#d7d8d9">
        [0,3]
       </span>
       ，表示每个样本在展平张量中的起始位置。
      </li>
     </ul>
     <p>
      <strong>
       2. 合并操作：
      </strong>
     </p>
     <ul>
      <li>
       根据偏移量，将嵌入向量进行合并操作。
      </li>
      <li>
       合并操作可以是求和、平均或取最大值，默认是平均 （
       <span style="background-color:#d7d8d9">
        mean
       </span>
       ）。
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      函数原型：
     </strong>
    </p>
    <pre><code>torch.nn.EmbeddingBag(
    num_embeddings=*,
    embedding_dim,
    max_norm=None,
    norm_type=2.0,
    scale_grad_by_freq=False,
    mode='mean',
    sparse=False,
    _weight=None,
    include_last_offset=False,
    padding_idx=None,
    device=None,
    dtype=None
)</code></pre>
    <p>
     <strong>
      主要参数：
     </strong>
    </p>
    <ul>
     <li>
      <span style="background-color:#d7d8d9">
       num_embeddings
      </span>
      （
      <span style="background-color:#d7d8d9">
       int
      </span>
      ）：词典的大小。
     </li>
     <li>
      <span style="background-color:#d7d8d9">
       embedding_dim
      </span>
      （
      <span style="background-color:#d7d8d9">
       int
      </span>
      ）：每个词向量的维度，即嵌入向量的长度。
     </li>
     <li>
      <span style="background-color:#d7d8d9">
       mode
      </span>
      （
      <span style="background-color:#d7d8d9">
       str
      </span>
      ）：指定嵌入向量的聚合方式。可选值为
      <span style="background-color:#d7d8d9">
       'sum'
      </span>
      、
      <span style="background-color:#d7d8d9">
       'mean'
      </span>
      和
      <span style="background-color:#d7d8d9">
       'max'
      </span>
      。
     </li>
     <li>
      <ul>
       <li id="uca81eb1d">
        （假设有一个序列
        <span style="background-color:#d7d8d9">
         [2,3,1]
        </span>
        ，每个数字表示一个离散特征的索引，对应的嵌入向量分别为
        <span style="background-color:#d7d8d9">
         [[0.1,0.2,0.3],[0.2,0.3,0.4],[0.3,0.4,0.5]]
        </span>
        ）
       </li>
       <li>
        <span style="background-color:#d7d8d9">
         'sum'
        </span>
        ：对所有的嵌入向量求和，则使用
        <span style="background-color:#d7d8d9">
         'sum'
        </span>
        模式汇总后的嵌入向量为
        <span style="background-color:#d7d8d9">
         [0.6,0.9,1.2]
        </span>
        。
       </li>
       <li>
        <span style="background-color:#d7d8d9">
         'mean'
        </span>
        ：对所有的嵌入向量求平均值，使用
        <span style="background-color:#d7d8d9">
         'mean'
        </span>
        模式汇总后的嵌入向量为
        <span style="background-color:#d7d8d9">
         [0.2,0.3,0.4]
        </span>
        。
       </li>
       <li>
        <span style="background-color:#d7d8d9">
         'max'
        </span>
        ：对所有的嵌入向量求最大值，使用
        <span style="background-color:#d7d8d9">
         'max'
        </span>
        模式汇总后的嵌入向量为
        <span style="background-color:#d7d8d9">
         [0.3,0.4,0.5]
        </span>
        。
       </li>
      </ul>
     </li>
    </ul>
    <p>
     下面是一个简单的例子，用
     <strong>
      EmbeddingBag
     </strong>
     将两个句子转换为词嵌入向量并计算它们的均值。
    </p>
    <p>
     先放上上周的数据集构建的代码，不清楚的同学可以回过头看看上周的代码。
    </p>
    <h3>
     2.1 自定义数据集类
    </h3>
    <pre><code>import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader,Dataset

class MyDataset(Dataset):
    def __init__(self,texts,labels):
        self.texts=texts
        self.labels=labels
        
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self,idx):
        texts=self.texts[idx]
        labels=self.labels[idx]
        
        return texts,labels</code></pre>
    <h3>
     2.2 准备数据和数据加载器
    </h3>
    <pre><code># 假设我们有以下三个样本，分别由不同数量的单词索引组成
text_data=[
    torch.tensor([1,1,1,1],dtype=torch.long),  # 样本1
    torch.tensor([2,2,2],dtype=torch.long),  # 样本2
    torch.tensor([3,3],dtype=torch.long)   # 样本3
]

# 对应的标签
labels=torch.tensor([4,5,6],dtype=torch.float)

# 创建数据集和数据加载器
my_dataset=MyDataset(text_data,labels)
data_loader=DataLoader(my_dataset,batch_size=2,shuffle=True,collate_fn=lambda x:x)

for batch in data_loader:
    print(batch)</code></pre>
    <h3>
     2.3 定义模型
    </h3>
    <pre><code>class EmbeddingBagModel(nn.Module):
    def __init__(self,vocab_size,embed_dim):
        super(EmbeddingBagModel,self).__init__()
        self.embedding_bag=nn.EmbeddingBag(vocab_size,embed_dim,mode='mean')
        self.fc=nn.Linear(embed_dim,1)
        
    def forward(self,text,offsets):
        print("embedding_bag输入文本是：",text)
        print("embedding_bag输入文本shape：",text.shape)
        
        embedded=self.embedding_bag(text,offsets)
        print("embedding_bag输出文本shape：",embedded.shape)
        return self.fc(embedded)</code></pre>
    <h3>
     2.4 训练模型
    </h3>
    <pre><code># 示例词典大小和嵌入维度
vocab_size=10
embed_dim=6

# 创建模型实例
model=EmbeddingBagModel(vocab_size,embed_dim)

# 定义一个简单的损失函数和优化器
criterion=nn.BCEWithLogitsLoss()
optimizer=optim.SGD(model.parameters(),lr=0.01)

# 训练模型
for epoch in range(1): # 训练1个epoch
    for batch in data_loader:
        # 将批处理的数据展平并计算偏移量
        texts,labels=zip(*batch)
        
        offsets=[0]+[len(text) for text in texts[:-1]]
        offsets=torch.tensor(offsets).cumsum(dim=0)
        texts=torch.cat(texts)
        labels=torch.tensor(labels).unsqueeze(1)
        
        # 前向传播
        outputs=model(texts,offsets)
        loss=criterion(outputs,labels)
        
        # 反向传播和优化
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    print(f'Epoch {epoch+1},Loss:{loss.item()}')</code></pre>
    <p>
     代码输出：
    </p>
    <pre><code>embedding_bag输入文本是： tensor([2, 2, 2, 1, 1, 1, 1])
embedding_bag输入文本shape： torch.Size([7])
embedding_bag输出文本shape： torch.Size([2, 6])
embedding_bag输入文本是： tensor([3, 3])
embedding_bag输入文本shape： torch.Size([2])
embedding_bag输出文本shape： torch.Size([1, 6])
Epoch 1,Loss:-2.8094725608825684</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34333431343532312f:61727469636c652f64657461696c732f313435323139343334" class_="artid" style="display:none">
 </p>
</div>


