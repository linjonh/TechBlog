---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f36323839343637372f:61727469636c652f64657461696c732f313436313036363936"
layout: post
title: "BERTT5GPTs,Llama"
date: 2025-03-08 13:55:58 +0800
description: "（本系列是课程笔记）Encoder-only 架构的代表——BERT结构特点：只有编码器（1）编码器结构：每个编码器layer中包含一个多头自注意力（无mask）和FFN（2）双向：每个位置可以看到序列前后的位置（无mask）双向：每个位置可以看到序列前后的位置（无mask）BERT-base 包含 12 个 Transformer 层，每层中的多 头注意力机制 包含 12 个头， 隐状态的维度为 768，总参数 量约 110M，与 GPT 的参数量相当。"
keywords: "gpt、llama、bert"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Bert']
artid: "146106696"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146106696
    alt: "BERTT5GPTs,Llama"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146106696
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146106696
cover: https://bing.ee123.net/img/rand?artid=146106696
image: https://bing.ee123.net/img/rand?artid=146106696
img: https://bing.ee123.net/img/rand?artid=146106696
---

# BERT、T5、GPTs，Llama

📕参考：
[大模型研讨课第一期：Why LLMs?、模型结构1（共10期）\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1dfmoY2ESA/?spm_id_from=333.788.top_right_bar_window_history.content.click&vd_source=b42a01767afd2815dd173fb2970a67af "大模型研讨课第一期：Why LLMs?、模型结构1（共10期）_哔哩哔哩_bilibili")

（本系列是课程笔记）

---

![](https://i-blog.csdnimg.cn/direct/cff50715538e416c96a7b03fce421377.png)

## Encoder-only--BERT

### Encoder-only 架构的代表——BERT

结构特点：只有编码器

![](https://i-blog.csdnimg.cn/direct/c503be88ed21499aa1c5e9b3de5d7e18.png)

BERT（Bidirectional Encoder Representations from Transformers）：

（1）编码器结构：每个编码器layer中包含一个多头自注意力（无mask）和FFN

（2）双向：每个位置可以看到序列前后的位置（无mask）

![](https://i-blog.csdnimg.cn/direct/a549fdb17ef34247874d8287ea73d9fd.png)

> 双向：每个位置可以看到序列前后的位置（无mask）

![](https://i-blog.csdnimg.cn/direct/105237342c724cf8b9e5a8a0b581cdc1.png)

BERT-base 包含 12 个 Transformer 层，每层中的多 头注意力机制 包含 12 个头， 隐状态的维度为 768，总参数 量约 110M，与 GPT 的参数量相当。

BERT-large 则将 BERT-base 的规模翻倍，包含 24 个 Transformer 层，每层中的多头注意力机制包含 16 个 头，隐状态的维度为 1024， 总参数量约 340M。

### BERT的预训练-微调范式

（1）预训练：使用互联网上大量的无标签的语料数据，自监督训练

（2）微调：利用下游任务的有标签数据进行微调，使模型快速应用于各种不同的下游任务，并且取得（比不预训练）更好的效果

![](https://i-blog.csdnimg.cn/direct/4547f14278c14b4dab8ae08a9382fe5a.png)

### BERT的输入

1.将文本句子使用WordPiece分词，转换为token

2.输入是句子或者句子对

（1） 句子对：起始分类标记[CLS]，第一个句子的tokens，分隔符[Sep]，第二个句子的tokens，分隔符[Sep]

（2）单个句子：起始分类标记[CLS]，第一个句子的tokens，分隔符[Sep]

![](https://i-blog.csdnimg.cn/direct/374a39ba28c64f5a9e1e5a7e1131d891.png)

输入其实是
**Token Embeddings+**
Segment Embeddings+Position Embeddings

> **（1）Token Embeddings：划分词汇**
>
> （2）Segment Embeddings：标记句子
>
> （3）Position Embeddings：位置信息

> （1）Token Embeddings将 token映射为向量表示（token就是分词后得到的）
>
> （2）Segment Embeddings 标记句子，前句标记为 EA，后句标记为EB，如 前句为0，后句为1（如果只有前句就只标0）
>
> （3）Position Embeddings学习位置信息 ，未使用三角函数 直接学习获得。缺陷：序列长度固定 （BERT设定了最大序列 长度）

### BERT的隐层输出

![](https://i-blog.csdnimg.cn/direct/390f7ebf130e46cfb8bcac1268f2c57a.png)

### BERT的预训练

![](https://i-blog.csdnimg.cn/direct/cfdeceec709f41beac0e3d02e2155e4c.png)

**预训练任务之一：完形填空 （Masked Language Model）**

设计目的：改进token-level的NLP任务效果（命名实体识别、问答等）

预训练目标：
**（利用上下文）预测被“掩盖(masked)”的tokens**

具体做法：

对输入的tokens序列，随机选择一些待预测的tokens（占15%）， 其中80%替换为[MASK]（一种特殊的token），10%替换为字典中的 任意token， 剩余10%不变。

在这些tokens对应的隐层输出使用 softmax 估计在词典中各个词 的概率，计算预测与真值的Cross-Entropy。

![](https://i-blog.csdnimg.cn/direct/986da3f1d6f04d64818a0a06c0e50f9d.png)

> 利用上下文来预测被mask的东西

**预训练任务之二：下句预测（Next Sentence Prediction）**

设计目的：提高模型判断句子之间相对关系的能力

预训练目标：
**判断输入给定的2个句子是否为上下句关系（本质是判断是非的2分类问题）**

具体做法：

输入两个句子A、B，50%概率设置B是A的下一句，50%概率A和B是随机抽取的两个句子；使用代表分类头的特殊符号[CLS]对应的隐层输出C，预测B句是否为A句的下一句。

![](https://i-blog.csdnimg.cn/direct/b8a91806842c4c5fb45f346702c0c0f0.png)

![](https://i-blog.csdnimg.cn/direct/ad212dcb41434390b886dc17f83e5c42.png)

### BERT的微调

微调：利用下游任务数据更新整个网络的参数，可以快速灵活的适应下游任务

**注意：根据下游任务添加输出层，添加的层从头训练**

**句子对分类**

句子对分类：输入两个句子，输出分类类别（cls）

（1）MNLI：大规模分类任务，目标是预测第二个句子相对于第一个句子是包含、矛盾还是中立

（2）QQP：二分类任务，预测Quora在两个语义问题上是否等效

（3）STS-B：语义文本相似性基准，从新闻头条或其他来源提取句子对， 按分值标注两个句子在语义上的相似度

（4）MRPC：自动从在线新闻源中提取句子对，并带有人工标注，以说明句子对中的句子在语义上是否等效

（5）SWAG：对抗生成的情境数据集，包含113k个句子对完整示例，用于评估常识推理

![](https://i-blog.csdnimg.cn/direct/895154ab03d545d690a940812034af24.png)

单句分类：输入一个句子，输出分类类别（cls）

（1）SST-2：斯坦福情感树库，单句二分类任务，包括从电影评论中提取的句子及其情绪标注

（2）CoLA：语言可接受性语料库，单句二分类任务，目标是预测英语句子在语言上是否“可以接受

![](https://i-blog.csdnimg.cn/direct/d85a7b61f84c444aa7c0092d3db107bd.png)

问答任务：
**输入问题和段落，输出回答的片段start/end**

（1）SQuAD：斯坦福问答数据集，包含10万个问题/答案对，任务是预测 Wikipedia段落中的答案文本范围

要求：标出答案的span ,输出start end

![](https://i-blog.csdnimg.cn/direct/608e51d50af8470083b585646eab9f9a.png)

![](https://i-blog.csdnimg.cn/direct/87ee8c53ecec4c268cd545762b10b9df.png)

**命名实体识别任务：**
输入一个句子，输出实体的类别和begin/inside/O （共9类）

（1）NER：识别文本中具有特定意义的实体，如人名、地名、机构名等，常见的NER数据集有CoNLL-2003

（2）实体有4类（人名、地名、专有名词、机构名）

begin表示类别开始，inside表示类别中间或结尾，O表示非实体，排列组合共9类

![](https://i-blog.csdnimg.cn/direct/43f313f4cc5b496f8780c93c388b0425.png)

![](https://i-blog.csdnimg.cn/direct/e456388b7190444bb9a423f16defbc59.png)

优势：

1. 可以在大量的无标签文本语料库上进行无监督预训练
2. 挖掘了预训练阶段的潜力，通过MLM任务获取到上下文相关的
   **双向**
   特征表示
3. 可以通过微调，将预训练好的模型迅速迁移到各种下游任务
4. 适合各种
   **自然语言理解（NLU）任务**

不足：

1. 由于只使用了Transformer的编码器部分，没有进行自回归预训练，且架构上缺少解码器结构， 因此不适合直接处理
   **自然语言生成（NLG）任务**
   ，如生成对话、文章摘要、机器翻译
2. 只适合处理句子和段落级别的任务，不适合处理文档级别的任务

> encoder-only 只有编码器没有解码器，（没有mask的自注意力），没有进行自回归预训练，不适合进行生成式的任务。偏向于做理解性任务，比如实体抽取。

> 自回归预训练：用前边的词预测后边的词

## Encoder-Decoder

![](https://i-blog.csdnimg.cn/direct/3d36b3ee0ac34c338903c7b9a006645f.png)

Encoder-Decoder的起源：Seq2Seq

T5(Text-to-text Transfer Transformer)

GLM (General Language Model)

Encoder-Decoder架构的起源

![](https://i-blog.csdnimg.cn/direct/b1a365089c9c4bf2954ea4aea569e213.png)

### T5(Text-to-Text Transfer Transformer)

![](https://i-blog.csdnimg.cn/direct/3239e89074184f169189317d5ca13b54.png)

![](https://i-blog.csdnimg.cn/direct/9a98def0c9d74b2f97acf05082454adc.png)

T5的设计：

（1）

将所有NLP问题都转化为Text-to-text问题

（2）用一个统一的模型、损失函数、超参等等解决所有NLP问题

（3）构建数据集时，要在数据前加上任务的描述。

> T5本身没有算法上的亮点, 也没有模型结构上的巨大创新,它最重要的作用是为NLP预训练模型提供了一个通用的框架，用seq2seq解决所有NLP问题.

![](https://i-blog.csdnimg.cn/direct/d31ed0436f864f9b958f2a5157f47ba7.png)

> 第三个：结合了单向的和双向的  前一半双向，后一半单向

![](https://i-blog.csdnimg.cn/direct/4d6de130a63446e196af483225621d90.png)

输入文本会保留一部分原始文本，同时用特定的遮蔽符号（例如，<X>、<Y>、<Z>）替换掉要预测的部分。

目标文本则包含被遮蔽的内容。

![](https://i-blog.csdnimg.cn/direct/0da95db7f90841fdae63c75126543ba9.png)

> T5的微调是把所有的任务合到一起微调，Bert的微调是一个下游任务一个微调。

![](https://i-blog.csdnimg.cn/direct/3c76df1de12e4d37a46c8dd50440f800.png)

优点：Encoder-Decoder架构适合做需要理解输入后（即有条件）的输出相关任务，

如机器翻译，问答系统

**训练：**

由于需要训两个模块且涉及两个模块之间的交互，Encoder-Decoder架构模型复杂度往往更高，在参数效率上低于Decoder-Only架构，在同样的计算资源下更加难以训练（kv cache等也对decoder-only更加友好）

**能力：**

双向attention的注意力矩阵容易退化为低秩矩阵，限制表达能力，在同等参数量条件下，Encoder-Decoder架构的泛化性能并没有比Decoder-only更好（在T5等论文中结果更好可能是因为参数量翻倍）

## Decoder-only 架构

![](https://i-blog.csdnimg.cn/direct/191397af73db446287b7aa56ea32a02c.png)

只有解码器（有mask的）

### GPT

![](https://i-blog.csdnimg.cn/direct/6974d82a04ea4cbf9fd55bacbb3163fc.png)

> ⭐由于去掉了编码器， 所以也去掉了交叉注意力

> decoder中的
> **第一个masked多头自注意力模块**
> 输入序列为了
> **不能看到当前token之后的信息**
> ，需要对当前toekn之后的tokens进行mask;

![](https://i-blog.csdnimg.cn/direct/2e3e5a8df0d54d73b47387b66f9f173b.png)

> 预训练方式：自回归，无监督学习
>
> （其实，给定前N-1个词，预测第N个词，自带了标签，即 遇到前N-1个词对应的标签是第N个词）
>
> 输出： token 经过 经过词嵌入矩阵We(内积)+Wp(位置编码，直接相加)，经过n个解码层，最后得到输出词的概率，然后选概率最大的那个。

![](https://i-blog.csdnimg.cn/direct/03cce256d9a84132b78b2a0e094c5ad4.png)

训练方式：有监督学习，使用下游任务的数据集和标签。

参数更新：新添加的层从头训练，其他参数微调。

目标函数：下游任务的目标函数+预训练的语言模型。

![](https://i-blog.csdnimg.cn/direct/83bf50f4d6a14ee49a071b5f3cf45b9a.png)

4种下游任务

输入：对输入进行特定的转换，添加一些特殊词元

结构：对结构进行微小调整（添加线性层），即可适用于各种不同的下游任务

**分类任务：判断序列的类别**

结构调整：添加线性层输出类别

输入转换：添加开始词元start和抽取词元extract

![](https://i-blog.csdnimg.cn/direct/a326a2b1646a422f931bfb4cb25975b6.png)

**蕴含任务：判断输入的两段序列（前提文本和假设文本）是否为包含关系**

结构调整：添加线性层输出预测是否为蕴含关系

输入转换：开头添加开始词元 start，用delim连接两段文本， 最后添加抽取词元extract

![](https://i-blog.csdnimg.cn/direct/dd2b3779dd7d47e39397f45a3f4edced.png)

**相似任务：判断输入的两段序列是否相似**

结构调整：添加线性层输出预测 是否为相似关系

输入转换：分别使用两种顺序连接两段文本，开头添加开始词元 start，用delim连接两段文本，最后添加抽取词元extract

![](https://i-blog.csdnimg.cn/direct/6bfaa98e01eb40d08cc1436f201ff8a4.png)

**多选任务：给定一个问题，从多个答案中判断最佳答案**

结构调整：将n个结果分别输入到 新添加的线性层，用softmax判断 概率最大的做完最终结果

输入转换：分别对n个回答构造n 段序列，开头添加开始词元start， 用delim连接问题和答案，最后添加抽取词元extract

![](https://i-blog.csdnimg.cn/direct/fbabb2ded290401f948c2dc87df4a894.png)

GPT和BERT对比

![](https://i-blog.csdnimg.cn/direct/08490ea7eceb4f34be9bf9dece86534a.png)

BERT：编码器、双向、MLM和NSP，微调任务特定的层

GPT：解码器、单向（从左到右）、预测下一个词

> 下句预测（Next Sentence Prediction）  完形填空 （Masked Language Model）

![](https://i-blog.csdnimg.cn/direct/c1f06de0e3b54111a346b6bf5c98b2fe.png)

背景：自监督预训练+监督式微调仍然需要数据集，如何去掉监督微调阶段？

GPT2核心思想：无需微调，在预训练阶段使用语言模型建模多种下游任务，实现通用的NLP模型

![](https://i-blog.csdnimg.cn/direct/81bb788e167f4a759fcecec0a84ed20f.png)

![](https://i-blog.csdnimg.cn/direct/bf66898196a54612aee919014c4e9b56.png)

layer norm的位置：

Transformer和GPT中layer norm在子层后面的位置（post-LN）

GPT-2中将layer norm放在了子层前面的位置（pre-LN）

pre-LN可以使各子层的梯度范数保持不变，更利于稳定的优化

![](https://i-blog.csdnimg.cn/direct/93d288e9cb0740509ce003535e5c7b16.png)
![](https://i-blog.csdnimg.cn/direct/2761692967e94945a0ed13075456e9bf.png)

![](https://i-blog.csdnimg.cn/direct/5d3b06cb3f41472b9e72bc6ca60cfd2e.png)

![](https://i-blog.csdnimg.cn/direct/9ddb010481414a53ad8ead020e48ee62.png)

尺度定律(Scaling law)：当增加模型计算量、训练数据量、模型参数规模，大模型 的Loss都会单调降低，模型效果越来越好

涌现能力(Emergent ability)：当模型规模大到一定程度 (60B)，模型能力出现飞跃

![](https://i-blog.csdnimg.cn/direct/d0fe8c55da0b4bd6afe4c30db198e3cf.png)

![](https://i-blog.csdnimg.cn/direct/fe5f43faa7854308bf7635e39ba60c87.png)

![](https://i-blog.csdnimg.cn/direct/85f51a3b7a604f6a97bf8036939800c0.png)

![](https://i-blog.csdnimg.cn/direct/5aac587b547946c198573748230bf71e.png)

> 无需微调，使用任务提示(prompt)和少量示例处理下游任务（few-shot）

> 没有参数更新！！！

![](https://i-blog.csdnimg.cn/direct/0f8dae2396144ff6a19877d914c6348c.png)

![](https://i-blog.csdnimg.cn/direct/eab102bd034540e49c017d7cdc4812e2.png)
![](https://i-blog.csdnimg.cn/direct/372055c1937e4242a24c8f68deaa4bd7.png)

![](https://i-blog.csdnimg.cn/direct/3d33a75319d94501a1513dbb016bd68c.png)

### Llama系列介绍--开源

![](https://i-blog.csdnimg.cn/direct/8c5c502120064067a72c55390107dfc3.png)

LLAMA系列作为开源大模型，特别是Llama 3.1

405B登场后，在多项测试中一举超越GPT-4o

（1）

Llama 3.1 405B的性能，与最好的闭源模型性能相当 (多项指标超过GPT-4o)

（2）

开源/免费使用权重和代码，允许进行微调、蒸馏到其他模型中，以及在任何地方部署

（3）

128k的上下文，多语言，良好的代码生成能力，复杂推理能力，以及工具使用能力

（4）

Llama Stack API可以轻松集成

![](https://i-blog.csdnimg.cn/direct/6e20ba22bcb049989b747fab4370d72c.png)

Decoder-only 架构成为主流的原因探讨

![](https://i-blog.csdnimg.cn/direct/7d767dc4fe4843c581908cde73b93930.png)

![](https://i-blog.csdnimg.cn/direct/ca125e0c59b64180a4e31c91b04839e4.png)

一些原因分析

（1）

decoder-only的架构相比encoder-decoder在in-context learning上会更有优势，因为prompt可以更加直接地作用于decoder每一层的参数，无需经过编码器的转换， 微调的信号更强，
**使得decoder-only能够更准确地捕捉和利用输入序列的信息**

（2）

注意力的秩：双向attention的注意力矩阵容易退化为低秩状态，而causal attention 的注意力矩阵是下三角矩阵，必然是满秩的，建模能力更强

（3）
**causal attention （就是decoder-only的单向attention）具有隐式的位置编码功能**

模型复杂度与训练效率

（1）

decoder-only支持一直复用KV-Cache，计算效率更高

（2）

Megatron和flash attention等重要工具对causal attention的支持更好

实验表明：

（1）

decoder-only架构的zero-shot泛化性能更好

（2）
**decoder-only架构的in context learning能力更强，使得few-shot能力更强**

## 

---

## FFN  (Feed-Forward Network Sublayer)前馈神经网络

FFN是Transformer模型中的一个关键组件，通常出现在编码器和解码器的每一层中。它由两个线性变换和一个激活函数组成，

![](https://i-blog.csdnimg.cn/direct/110c2fbcb06d4bd0a0aaafe5fe21cdb7.png)

> 低维→高维→低维（512→2048→512）

主要作用

1. **特征变换**
   ：增强模型的表达能力。
2. **非线性引入**
   ：
   通过激活函数增加非线性，提升模型拟合能力
   。
3. **维度调整**
   ：通过线性变换调整特征维度，便于后续处理。

---

## 激活函数

![](https://i-blog.csdnimg.cn/direct/8c408788ecf64304ad704dcfbe0b444d.png)

理想激活函数是阶跃函数, 0表示抑制神经元而1表示激活神经元

但是阶跃函数具有不连续、不光滑等不好的性质, 常用的是 Sigmoid 函数 。

![](https://i-blog.csdnimg.cn/direct/3842a958a97f486d9103729926ae43f1.png)

![](https://i-blog.csdnimg.cn/direct/f955a0fb957f4cfcb4ad9faa5b5a9bc0.png)

![](https://i-blog.csdnimg.cn/direct/d9def261fe244410b857f1a3a1345d35.png)

> HAdamard积：按元素相乘

![](https://i-blog.csdnimg.cn/direct/7fb7b6c492864f44be8865f92065bb38.png)

Multi-head：每个head有自己单独的KV .

GQA：将Q分成N个组， 每个组共享KV【相当于两种方式的融合】

> 在多头注意力机制（Multi-Head Attention）中，
> **Q（Query）、K（Key）、V（Value）**
> 通常是从同一个输入通过不同的线性变换得到的。

![](https://i-blog.csdnimg.cn/direct/d630a1e6daf74c1093917d5593d7237f.png)
![](https://i-blog.csdnimg.cn/direct/39c532efdb9042a2ab852f4904e55cc6.png)

> Q K V 在自注意力机制中是从同一个输入序列X 计算得到的（所以称之为同源）。
>
> 但Q K V实际的值并不同，因为它们乘的线性变换矩阵 W\_() 不一样。

![](https://i-blog.csdnimg.cn/direct/47db68963c274a0a80075cdd2208deb1.png)
![](https://i-blog.csdnimg.cn/direct/a61416aab7384d6583769e60511c3c0b.png)
![](https://i-blog.csdnimg.cn/direct/5a10ae0e2116486bb99e14085cbb8a7b.png)

---

## Layer Normalization（层归一化）

Layer Normalization（层归一化，简称 Layer Norm）是一种用于神经网络中的归一化技术，旨在提高模型的训练稳定性和收敛速度。
**它通过对每一层的输入进行归一化，使得输入数据的分布更加稳定，从而缓解梯度消失或梯度爆炸问题。**

Layer Norm 对每一层的输入进行归一化，使其均值为 0，方差为 1。具体来说，它对每个样本的所有特征维度进行归一化，而不是像 Batch Normalization（批归一化）那样对每个特征维度跨样本进行归一化。

![](https://i-blog.csdnimg.cn/direct/91e69989b45f4ce3bff0daccb63706bd.png)

![](https://i-blog.csdnimg.cn/direct/b7aeb0a82b99468e9caa75f489e600cc.png)

**Layer Norm 的作用**

1. **稳定训练过程**
   ：

   * 通过对每一层的输入进行归一化，Layer Norm 可以减少内部协变量偏移（Internal Covariate Shift），使得每一层的输入分布更加稳定。
   * 这有助于缓解梯度消失或梯度爆炸问题，从而加速模型收敛。
2. **适用于小批量数据**
   ：

   * Layer Norm 对每个样本独立进行归一化，不依赖于批量大小，因此在批量较小时也能稳定工作。
   * 相比之下，Batch Normalization 在批量较小时效果较差。
3. **适用于序列数据**
   ：

   * Layer Norm 在序列数据（如文本、时间序列）中表现优异，因为它对每个时间步的特征进行归一化。
   * 在 Transformer 等模型中，Layer Norm 被广泛应用于自注意力机制和前馈神经网络中。
4. **增强模型表达能力**
   ：

   * 通过可学习的参数 γ 和 β，Layer Norm 可以在归一化的基础上保留模型的表达能力。

![](https://i-blog.csdnimg.cn/direct/14f59eaa43314c66922d7289d3557a7f.png)
![](https://i-blog.csdnimg.cn/direct/75452c2164e047308f71ad286036ceb4.png)

![](https://i-blog.csdnimg.cn/direct/ac14d460116c4d959275475b0d68be9e.png)

> LayNorm：纵向的，同一个样本内计算方差
>
> BatchNorm：横向的，样本对齐，然后对同一通道的不同样本间计算均值方差

**Layer Norm 的应用场景**

1. **Transformer 模型**
   ：

   * Layer Norm 被广泛应用于 Transformer 的每个子层（如自注意力层和前馈神经网络层）之后。
   * 例如，在 BERT、GPT 等模型中，Layer Norm 用于稳定训练过程。
2. **循环神经网络（RNN）**
   ：

   * Layer Norm 可以用于 RNN 的每个时间步，缓解梯度消失问题。
3. **生成对抗网络（GAN）**
   ：

   * Layer Norm 可以用于 GAN 的生成器和判别器中，提高训练稳定性。
4. **强化学习**
   ：

   * Layer Norm 可以用于策略网络和值函数网络中，加速收敛。

总结：

Layer Norm 是一种重要的归一化技术，通过对每一层的输入进行归一化，稳定训练过程并加速收敛。它在序列数据和小批量数据中表现优异，被广泛应用于 Transformer、RNN、GAN 等模型中。与 Batch Norm 相比，Layer Norm 不依赖批量大小，训练和推理时行为一致，具有更强的通用性。

---

**Layer Norm 的核心思想**

Layer Norm 对每一层的输入进行归一化，使其均值为 0，方差为 1。具体来说，它对每个样本的所有特征维度进行归一化，而不是像 Batch Normalization（批归一化）那样对每个特征维度跨样本进行归一化。

![](https://i-blog.csdnimg.cn/direct/91e69989b45f4ce3bff0daccb63706bd.png)

![](https://i-blog.csdnimg.cn/direct/b7aeb0a82b99468e9caa75f489e600cc.png)

![](https://i-blog.csdnimg.cn/direct/323a99d6994f40d7828bf4784513ca45.png)

> 去除平移，效果相当。