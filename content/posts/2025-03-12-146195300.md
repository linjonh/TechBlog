---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34303738303137382f:61727469636c652f64657461696c732f313436313935333030"
layout: post
title: "PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›é˜¶å­¦ä¹ è®¡åˆ’-ç¬¬20å¤©ç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿ"
date: 2025-03-12 08:49:56 +0800
description: "PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›é˜¶å­¦ä¹ è®¡åˆ’ - ç¬¬20å¤©ï¼šç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿï¼å¦‚æœæ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¿˜è¯·ç»™ä¸ªä¸‰è¿å¥½è¯„ï¼Œæ„Ÿè°¢æ„Ÿè°¢ï¼"
keywords: "PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›é˜¶å­¦ä¹ è®¡åˆ’ - ç¬¬20å¤©ï¼šç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿ"
categories: ['æœªåˆ†ç±»']
tags: ['æ·±åº¦å­¦ä¹ ', 'å­¦ä¹ ', 'äººå·¥æ™ºèƒ½', 'Pytorch', 'Python', 'Aigc', 'Ai']
artid: "146195300"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146195300
    alt: "PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›é˜¶å­¦ä¹ è®¡åˆ’-ç¬¬20å¤©ç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿ"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146195300
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146195300
cover: https://bing.ee123.net/img/rand?artid=146195300
image: https://bing.ee123.net/img/rand?artid=146195300
img: https://bing.ee123.net/img/rand?artid=146195300
---

# PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›é˜¶å­¦ä¹ è®¡åˆ’ - ç¬¬20å¤©ï¼šç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿ

## PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶è¿›é˜¶å­¦ä¹ è®¡åˆ’ - ç¬¬20å¤©

## ç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿ

ä»Šå¤©æˆ‘ä»¬å°†ç»¼åˆåº”ç”¨ä¹‹å‰å­¦ä¹ çš„çŸ¥è¯†ï¼Œæ„å»ºä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯è‰ºæœ¯é£æ ¼è½¬æ¢ç³»ç»Ÿã€‚è¿™ä¸ªç³»ç»Ÿå°†ç»“åˆCycleGANä¸é£æ ¼è¿ç§»æŠ€æœ¯ï¼Œå¹¶é€šè¿‡Gradioåˆ›å»ºäº¤äº’å¼ç•Œé¢ï¼ŒåŒæ—¶å®ç°æ¨¡å‹ä¼˜åŒ–ã€éƒ¨ç½²å’Œç›‘æ§ã€‚

#### 1. CycleGANä¸é£æ ¼è¿ç§»åŸç†å›é¡¾

CycleGANèƒ½å¤Ÿåœ¨æ²¡æœ‰é…å¯¹æ•°æ®çš„æƒ…å†µä¸‹å­¦ä¹ ä¸¤ä¸ªé¢†åŸŸä¹‹é—´çš„æ˜ å°„ï¼Œè€Œé£æ ¼è¿ç§»åˆ™èƒ½å°†ç‰¹å®šçš„è‰ºæœ¯é£æ ¼åº”ç”¨åˆ°å†…å®¹å›¾åƒä¸Šã€‚æˆ‘ä»¬å°†ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼Œåˆ›å»ºä¸€ä¸ªçµæ´»çš„è‰ºæœ¯é£æ ¼è½¬æ¢ç³»ç»Ÿã€‚

**CycleGANæ ¸å¿ƒæ¦‚å¿µ**
:

* ä¸¤ä¸ªç”Ÿæˆå™¨(G, F)å’Œä¸¤ä¸ªåˆ¤åˆ«å™¨(DX, DY)
* å¾ªç¯ä¸€è‡´æ€§æŸå¤±(Cycle Consistency Loss)
* èº«ä»½æ˜ å°„æŸå¤±(Identity Mapping Loss)

**é£æ ¼è¿ç§»æ ¸å¿ƒæ¦‚å¿µ**
:

* å†…å®¹æŸå¤±(Content Loss)
* é£æ ¼æŸå¤±(Style Loss)
* GramçŸ©é˜µè®¡ç®—

#### 2. å¤šGPUè®­ç»ƒä¸æ··åˆç²¾åº¦è®­ç»ƒ

```python
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
from torchvision import datasets, transforms
import os
from torch.cuda.amp import autocast, GradScaler

# å®šä¹‰ç®€åŒ–ç‰ˆçš„ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        self.block = nn.Sequential(
            nn.ReflectionPad2d(1),
            nn.Conv2d(channels, channels, 3),
            nn.InstanceNorm2d(channels),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(channels, channels, 3),
            nn.InstanceNorm2d(channels)
        )

    def forward(self, x):
        return x + self.block(x)

class Generator(nn.Module):
    def __init__(self, input_channels=3, output_channels=3, n_residual_blocks=9):
        super(Generator, self).__init__()
        
        # åˆå§‹å·ç§¯å±‚
        self.initial = nn.Sequential(
            nn.ReflectionPad2d(3),
            nn.Conv2d(input_channels, 64, 7),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # ä¸‹é‡‡æ ·
        self.downsample = nn.Sequential(
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 3, stride=2, padding=1),
            nn.InstanceNorm2d(256),
            nn.ReLU(inplace=True)
        )
        
        # æ®‹å·®å—
        res_blocks = []
        for _ in range(n_residual_blocks):
            res_blocks.append(ResidualBlock(256))
        self.res_blocks = nn.Sequential(*res_blocks)
        
        # ä¸Šé‡‡æ ·
        self.upsample = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),
            nn.InstanceNorm2d(64),
            nn.ReLU(inplace=True)
        )
        
        # è¾“å‡ºå±‚
        self.output = nn.Sequential(
            nn.ReflectionPad2d(3),
            nn.Conv2d(64, output_channels, 7),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.initial(x)
        x = self.downsample(x)
        x = self.res_blocks(x)
        x = self.upsample(x)
        return self.output(x)

class Discriminator(nn.Module):
    def __init__(self, input_channels=3):
        super(Discriminator, self).__init__()
        
        self.model = nn.Sequential(
            nn.Conv2d(input_channels, 64, 4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.InstanceNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.InstanceNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(256, 512, 4, padding=1),
            nn.InstanceNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Conv2d(512, 1, 4, padding=1)
        )

    def forward(self, x):
        return self.model(x)

# å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
def setup_ddp(rank, world_size):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    
    # è®¾ç½®å½“å‰è®¾å¤‡
    torch.cuda.set_device(rank)

def cleanup():
    dist.destroy_process_group()

def train_model(rank, world_size, epochs):
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    setup_ddp(rank, world_size)
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åŠ è½½æ•°æ®é›†ï¼ˆç¤ºä¾‹ä½¿ç”¨CIFAR-10ï¼‰
    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)
    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)
    
    # åˆå§‹åŒ–æ¨¡å‹
    generator = Generator().to(rank)
    discriminator = Discriminator().to(rank)
    
    # å°†æ¨¡å‹åŒ…è£…ä¸ºDDPæ¨¡å‹
    generator = DDP(generator, device_ids=[rank])
    discriminator = DDP(discriminator, device_ids=[rank])
    
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # æŸå¤±å‡½æ•°
    criterion_GAN = nn.MSELoss()
    criterion_cycle = nn.L1Loss()
    criterion_identity = nn.L1Loss()
    
    # æ¢¯åº¦ç¼©æ”¾å™¨ï¼ˆç”¨äºæ··åˆç²¾åº¦è®­ç»ƒï¼‰
    scaler_G = GradScaler()
    scaler_D = GradScaler()
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        sampler.set_epoch(epoch)  # è®¾ç½®epochä»¥ç¡®ä¿æ•°æ®æ´—ç‰Œ
        
        for i, (real_A, _) in enumerate(dataloader):
            real_A = real_A.to(rank)
            
            # è®­ç»ƒåˆ¤åˆ«å™¨
            optimizer_D.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                # ç”Ÿæˆfakeå›¾åƒ
                fake_B = generator(real_A)
                
                # åˆ¤åˆ«å™¨æŸå¤±
                pred_real = discriminator(real_A)
                pred_fake = discriminator(fake_B.detach())
                
                real_label = torch.ones_like(pred_real).to(rank)
                fake_label = torch.zeros_like(pred_fake).to(rank)
                
                loss_D_real = criterion_GAN(pred_real, real_label)
                loss_D_fake = criterion_GAN(pred_fake, fake_label)
                loss_D = (loss_D_real + loss_D_fake) * 0.5
            
            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            scaler_D.scale(loss_D).backward()
            scaler_D.step(optimizer_D)
            scaler_D.update()
            
            # è®­ç»ƒç”Ÿæˆå™¨
            optimizer_G.zero_grad()
            
            with autocast():
                # èº«ä»½æŸå¤±
                identity_B = generator(real_A)
                loss_identity = criterion_identity(identity_B, real_A) * 5.0
                
                # GANæŸå¤±
                fake_B = generator(real_A)
                pred_fake = discriminator(fake_B)
                loss_GAN = criterion_GAN(pred_fake, real_label)
                
                # æ€»æŸå¤±
                loss_G = loss_identity + loss_GAN
            
            # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
            scaler_G.scale(loss_G).backward()
            scaler_G.step(optimizer_G)
            scaler_G.update()
            
            if i % 100 == 0 and rank == 0:
                print(f"[Epoch {epoch}/{epochs}] [Batch {i}/{len(dataloader)}] "
                      f"[D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}]")
    
    # ä¿å­˜æ¨¡å‹ï¼ˆä»…åœ¨ä¸»è¿›ç¨‹ï¼‰
    if rank == 0:
        torch.save(generator.module.state_dict(), 'generator.pth')
        torch.save(discriminator.module.state_dict(), 'discriminator.pth')
    
    cleanup()

# æ‰§è¡Œå¤šGPUè®­ç»ƒ
if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    if world_size > 1:
        print(f"Using {world_size} GPUs!")
        torch.multiprocessing.spawn(train_model, args=(world_size, 10), nprocs=world_size, join=True)
    else:
        print("Only one GPU available or no GPU, running on single device")
        train_model(0, 1, 10)

```

å¤šGPUè®­ç»ƒå’Œæ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥å¤§å¹…åŠ é€ŸCycleGANç­‰å¤æ‚æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ã€‚ä¸Šè¿°ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨PyTorchçš„
`DistributedDataParallel`
å’Œ
`torch.cuda.amp`
æ¥å®ç°è¿™äº›ä¼˜åŒ–ã€‚

**å…³é”®ä¼˜åŒ–æŠ€æœ¯è¯´æ˜ï¼š**

1. **DistributedDataParallel (DDP)**
   : å®ç°æ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œæ¯ä¸ªGPUå¤„ç†æ•°æ®çš„ä¸åŒå­é›†
2. **æ··åˆç²¾åº¦è®­ç»ƒ**
   : ä½¿ç”¨FP16å’ŒFP32æ··åˆç²¾åº¦ï¼Œé™ä½å†…å­˜æ¶ˆè€—å¹¶æé«˜è®¡ç®—é€Ÿåº¦
3. **GradScaler**
   : è§£å†³æ··åˆç²¾åº¦è®­ç»ƒä¸­çš„æ¢¯åº¦ä¸‹æº¢é—®é¢˜

#### 3. ä½¿ç”¨Gradioæ„å»ºäº¤äº’å¼UI

```python
import torch
import gradio as gr
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import os

# å‡è®¾å·²ç»è®­ç»ƒå¥½çš„æ¨¡å‹
class StyleTransferModel:
    def __init__(self, model_path='generator.pth'):
        from model import Generator  # å‡è®¾Generatorç±»åœ¨model.pyä¸­å®šä¹‰
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = Generator()
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if os.path.exists(model_path):
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
            print(f"Model loaded from {model_path}")
        else:
            print(f"Model {model_path} not found. Using untrained model.")
        
        self.model.to(self.device)
        self.model.eval()
        
        # é¢„å¤„ç†å’Œåå¤„ç†
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
    def inference(self, image, style_name):
        # å›¾åƒé¢„å¤„ç†
        image = self.transform(image).unsqueeze(0).to(self.device)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            output = self.model(image)
        
        # åå¤„ç†
        output = output.squeeze().cpu().detach()
        output = output * 0.5 + 0.5  # åå½’ä¸€åŒ–
        output = output.clamp(0, 1)
        output = transforms.ToPILImage()(output)
        
        return output

# åˆå§‹åŒ–æ¨¡å‹
def load_model(style_name):
    model_path = f'models/{style_name}_generator.pth'
    return StyleTransferModel(model_path)

# åˆ›å»ºé£æ ¼è½¬æ¢å‡½æ•°
def apply_style(input_image, style_name):
    if input_image is None:
        return None
    
    # åŠ è½½å¯¹åº”é£æ ¼çš„æ¨¡å‹
    model = load_model(style_name)
    
    # æ‰§è¡Œé£æ ¼è½¬æ¢
    output_image = model.inference(input_image, style_name)
    
    return output_image

# å®šä¹‰å¯ç”¨çš„é£æ ¼
styles = ["monet", "vangogh", "cezanne", "ukiyoe"]

# åˆ›å»ºGradioç•Œé¢
def create_gradio_app():
    with gr.Blocks(title="è‰ºæœ¯é£æ ¼è½¬æ¢") as app:
        gr.Markdown("# è‰ºæœ¯é£æ ¼è½¬æ¢ç³»ç»Ÿ")
        gr.Markdown("ä¸Šä¼ å›¾åƒå¹¶é€‰æ‹©è‰ºæœ¯é£æ ¼ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨ä¸ºæ‚¨è½¬æ¢å›¾åƒé£æ ¼ã€‚")
        
        with gr.Row():
            with gr.Column():
                input_image = gr.Image(label="è¾“å…¥å›¾åƒ", type="pil")
                style_selector = gr.Dropdown(
                    choices=styles, 
                    label="é€‰æ‹©è‰ºæœ¯é£æ ¼", 
                    value="monet"
                )
                submit_btn = gr.Button("åº”ç”¨é£æ ¼")
            
            with gr.Column():
                output_image = gr.Image(label="é£æ ¼åŒ–ç»“æœ")
        
        # è®¾ç½®æäº¤æŒ‰é’®åŠ¨ä½œ
        submit_btn.click(
            fn=apply_style, 
            inputs=[input_image, style_selector], 
            outputs=output_image
        )
        
        # æ·»åŠ ç¤ºä¾‹
        gr.Examples(
            examples=[
                ["examples/landscape.jpg", "monet"],
                ["examples/portrait.jpg", "vangogh"],
                ["examples/cityscape.jpg", "cezanne"],
            ],
            inputs=[input_image, style_selector],
        )
        
        # æ·»åŠ é¢å¤–ä¿¡æ¯
        gr.Markdown("""
        ## å…³äºé£æ ¼è½¬æ¢
        
        æœ¬ç³»ç»Ÿä½¿ç”¨CycleGANç»“åˆé£æ ¼è¿ç§»æŠ€æœ¯ï¼Œå°†æ‚¨çš„å›¾åƒè½¬æ¢ä¸ºå„ç§è‰ºæœ¯é£æ ¼ã€‚
        
        å¯ç”¨é£æ ¼:
        - Monet: å°è±¡æ´¾é£æ ¼ï¼Œè‰²å½©æŸ”å’Œï¼Œç¬”è§¦å¯è§
        - Van Gogh: åå°è±¡æ´¾é£æ ¼ï¼Œè‰²å½©é²œæ˜ï¼Œç¬”è§¦æœ‰åŠ›
        - Cezanne: ç°ä»£è‰ºæœ¯å…ˆé©±ï¼Œç»“æ„åŒ–ç¬”è§¦
        - Ukiyo-e: æ—¥æœ¬æµ®ä¸–ç»˜é£æ ¼ï¼Œå¹³é¢è‰²å½©ï¼Œæ˜æ˜¾è½®å»“
        
        ç³»ç»Ÿä½¿ç”¨PyTorchå¼€å‘ï¼Œæ¨¡å‹ç»è¿‡ä¼˜åŒ–å¯åœ¨CPUä¸Šè¿è¡Œï¼Œä½†GPUä¼šæœ‰æ›´å¥½çš„æ€§èƒ½ã€‚
        """)
    
    return app

# å¯åŠ¨Gradioåº”ç”¨
if __name__ == "__main__":
    app = create_gradio_app()
    app.launch(share=True)  # è®¾ç½®share=Trueå¯è·å¾—å…¬å…±é“¾æ¥

```

Gradioæ˜¯ä¸€ä¸ªå¼ºå¤§è€Œç®€æ´çš„å·¥å…·ï¼Œå¯ä»¥å¿«é€Ÿä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹åˆ›å»ºäº¤äº’å¼Webç•Œé¢ã€‚ä¸Šé¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•ä¸ºæˆ‘ä»¬çš„é£æ ¼è½¬æ¢ç³»ç»Ÿåˆ›å»ºä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç•Œé¢ã€‚

**Gradioç•Œé¢åŠŸèƒ½ï¼š**

1. å›¾åƒä¸Šä¼ åŒºåŸŸ
2. é£æ ¼é€‰æ‹©ä¸‹æ‹‰èœå•
3. è½¬æ¢æŒ‰é’®
4. ç»“æœå±•ç¤ºåŒºåŸŸ
5. æ ·ä¾‹å›¾ç‰‡

#### 4. ONNXæ¨¡å‹å¯¼å‡ºä¸Runtimeéƒ¨ç½²

```python
import torch
import onnx
import onnxruntime as ort
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
import time
import os

# åŠ è½½PyTorchæ¨¡å‹
def load_pytorch_model(model_path):
    from model import Generator  # å‡è®¾Generatoråœ¨model.pyä¸­å®šä¹‰
    model = Generator()
    model.load_state_dict(torch.load(model_path, map_location='cpu'))
    model.eval()
    return model

# å¯¼å‡ºä¸ºONNXæ¨¡å‹
def export_to_onnx(pytorch_model, onnx_path, input_shape=(1, 3, 256, 256)):
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(input_shape)
    
    # å¯¼å‡ºæ¨¡å‹
    torch.onnx.export(
        pytorch_model,               # PyTorchæ¨¡å‹
        dummy_input,                 # ç¤ºä¾‹è¾“å…¥
        onnx_path,                   # è¾“å‡ºè·¯å¾„
        export_params=True,          # å­˜å‚¨è®­ç»ƒå¥½çš„å‚æ•°æƒé‡
        opset_version=12,            # ONNXæ“ä½œé›†ç‰ˆæœ¬
        do_constant_folding=True,    # å¸¸é‡æŠ˜å ä¼˜åŒ–
        input_names=['input'],       # è¾“å…¥åç§°
        output_names=['output'],     # è¾“å‡ºåç§°
        dynamic_axes={               # åŠ¨æ€è½´ï¼ˆæ‰¹å¤„ç†ç»´åº¦ï¼‰
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    # éªŒè¯ONNXæ¨¡å‹
    onnx_model = onnx.load(onnx_path)
    onnx.checker.check_model(onnx_model)
    print(f"Model exported to {onnx_path} and validated successfully")
    return onnx_path

# åˆ›å»ºONNXæ¨ç†ä¼šè¯
def create_onnx_session(onnx_path):
    # æ£€æŸ¥å¯ç”¨çš„æ‰§è¡Œæä¾›ç¨‹åºå¹¶é€‰æ‹©æœ€åˆé€‚çš„
    if 'CUDAExecutionProvider' in ort.get_available_providers() and torch.cuda.is_available():
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        print("Using CUDA for ONNX inference")
    else:
        providers = ['CPUExecutionProvider']
        print("Using CPU for ONNX inference")
    
    # åˆ›å»ºæ¨ç†ä¼šè¯
    session_options = ort.SessionOptions()
    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
    session = ort.InferenceSession(onnx_path, session_options, providers=providers)
    return session

# ä½¿ç”¨ONNX Runtimeè¿›è¡Œæ¨ç†
def onnx_inference(session, image_path):
    # å›¾åƒé¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    # åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0).numpy()
    
    # è·å–è¾“å…¥åç§°
    input_name = session.get_inputs()[0].name
    
    # æ‰§è¡Œæ¨ç†
    start_time = time.time()
    output = session.run(None, {input_name: input_tensor})[0]
    inference_time = time.time() - start_time
    
    # åå¤„ç†
    output = output.squeeze()
    output = output * 0.5 + 0.5  # åå½’ä¸€åŒ–
    output = np.clip(output, 0, 1)
    output = np.transpose(output, (1, 2, 0)) * 255.0
    output_image = Image.fromarray(output.astype(np.uint8))
    
    return output_image, inference_time

# æ€§èƒ½æ¯”è¾ƒå‡½æ•°ï¼šPyTorch vs ONNX
def compare_performance(pytorch_model, onnx_session, image_path, num_runs=10):
    # PyTorchæ¨ç†
    pytorch_model.eval()
    transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    
    image = Image.open(image_path).convert('RGB')
    input_tensor = transform(image).unsqueeze(0)
    
    # é¢„çƒ­
    with torch.no_grad():
        _ = pytorch_model(input_tensor)
    
    # è®¡æ—¶
    pytorch_times = []
    for _ in range(num_runs):
        start_time = time.time()
        with torch.no_grad():
            _ = pytorch_model(input_tensor)
        pytorch_times.append(time.time() - start_time)
    
    # ONNXæ¨ç†
    input_name = onnx_session.get_inputs()[0].name
    input_array = input_tensor.numpy()
    
    # é¢„çƒ­
    _ = onnx_session.run(None, {input_name: input_array})
    
    # è®¡æ—¶
    onnx_times = []
    for _ in range(num_runs):
        start_time = time.time()
        _ = onnx_session.run(None, {input_name: input_array})
        onnx_times.append(time.time() - start_time)
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    avg_pytorch_time = sum(pytorch_times) / len(pytorch_times)
    avg_onnx_time = sum(onnx_times) / len(onnx_times)
    speedup = avg_pytorch_time / avg_onnx_time
    
    results = {
        'pytorch_avg_time': avg_pytorch_time,
        'onnx_avg_time': avg_onnx_time,
        'speedup': speedup
    }
    
    return results

# ä¸»å‡½æ•°ï¼šå¯¼å‡ºå¹¶ä½¿ç”¨ONNXæ¨¡å‹
def main(model_path, onnx_path, image_path):
    # æ­¥éª¤1ï¼šåŠ è½½PyTorchæ¨¡å‹
    print("Loading PyTorch model...")
    pytorch_model = load_pytorch_model(model_path)
    
    # æ­¥éª¤2ï¼šå¯¼å‡ºä¸ºONNX
    print("Exporting to ONNX...")
    if not os.path.exists(onnx_path):
        export_to_onnx(pytorch_model, onnx_path)
    
    # æ­¥éª¤3ï¼šåˆ›å»ºONNXæ¨ç†ä¼šè¯
    print("Creating ONNX inference session...")
    onnx_session = create_onnx_session(onnx_path)
    
    # æ­¥éª¤4ï¼šä½¿ç”¨ONNXè¿›è¡Œæ¨ç†
    print("Running ONNX inference...")
    output_image, inference_time = onnx_inference(onnx_session, image_path)
    output_image.save('onnx_output.jpg')
    print(f"Inference completed in {inference_time:.4f} seconds, result saved to onnx_output.jpg")
    
    # æ­¥éª¤5ï¼šæ¯”è¾ƒæ€§èƒ½
    print("Comparing performance...")
    perf_results = compare_performance(pytorch_model, onnx_session, image_path)
    print(f"PyTorch average inference time: {perf_results['pytorch_avg_time']:.4f} seconds")
    print(f"ONNX average inference time: {perf_results['onnx_avg_time']:.4f} seconds")
    print(f"Speedup: {perf_results['speedup']:.2f}x")

if __name__ == "__main__":
    main(
        model_path="models/monet_generator.pth",
        onnx_path="models/monet_generator.onnx",
        image_path="examples/landscape.jpg"
    )

```

ONNX (Open Neural Network Exchange) æ˜¯ä¸€ç§å¼€æ”¾æ ¼å¼ï¼Œç”¨äºè¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ONNX Runtimeæ˜¯ä¸€ä¸ªé«˜æ€§èƒ½æ¨ç†å¼•æ“ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ¨¡å‹åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚

**ONNXä¼˜åŠ¿:**

1. **è·¨å¹³å°å…¼å®¹æ€§**
   : æ”¯æŒå¤šç§ç¡¬ä»¶å’Œæ¡†æ¶
2. **æ¨ç†æ€§èƒ½ä¼˜åŒ–**
   : é€šå¸¸æ¯”åŸå§‹PyTorchæ¨¡å‹æ›´å¿«
3. **å‡å°æ¨¡å‹å¤§å°**
   : é€šè¿‡é‡åŒ–å’Œä¼˜åŒ–å‡å°æ¨¡å‹ä½“ç§¯
4. **éƒ¨ç½²ç®€åŒ–**
   : ç®€åŒ–ä»è®­ç»ƒåˆ°éƒ¨ç½²çš„æµç¨‹

#### 5. æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜å®ç°

```python
import time
import threading
import queue
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from PIL import Image
import onnxruntime as ort
import psutil
import GPUtil
import datetime
import os

# åˆ›å»ºé˜Ÿåˆ—ç”¨äºå­˜å‚¨æ€§èƒ½æ•°æ®
performance_queue = queue.Queue()

# æ€§èƒ½ç›‘æ§çº¿ç¨‹å‡½æ•°
def monitor_system_performance():
    while True:
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            
            # GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœæœ‰GPUï¼‰
            gpu_data = []
            try:
                gpus = GPUtil.getGPUs()
                for gpu in gpus:
                    gpu_data.append({
                        'id': gpu.id,
                        'load': gpu.load * 100,
                        'memory_used': gpu.memoryUsed,
                        'memory_total': gpu.memoryTotal
                    })
            except:
                pass
            
            # å½“å‰æ—¶é—´
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # åˆ›å»ºæ€§èƒ½æ•°æ®ç‚¹
            perf_data = {
                'timestamp': timestamp,
                'cpu_percent': cpu_percent,
                'memory_percent': memory_percent,
                'gpu_data': gpu_data
            }
            
            # å°†æ•°æ®æ”¾å…¥é˜Ÿåˆ—
            performance_queue.put(perf_data)
            
            # æ¯ç§’æ›´æ–°ä¸€æ¬¡
            time.sleep(1)
            
        except Exception as e:
            print(f"Error in monitoring thread: {e}")
            time.sleep(5)

# æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•å‡½æ•°
def inference_performance_test(onnx_session, input_data, n_runs=10):
    input_name = onnx_session.get_inputs()[0].name
    
    # é¢„çƒ­
    _ = onnx_session.run(None, {input_name: input_data})
    
    # æµ‹é‡å¤šæ¬¡è¿è¡Œçš„æ€§èƒ½
    latencies = []
    for _ in range(n_runs):
        start_time = time.time()
        _ = onnx_session.run(None, {input_name: input_data})
        latencies.append((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
    
    return {
        'mean_latency': np.mean(latencies),
        'median_latency': np.median(latencies),
        'min_latency': np.min(latencies),
        'max_latency': np.max(latencies),
        'p95_latency': np.percentile(latencies, 95),
        'p99_latency': np.percentile(latencies, 99)
    }

# Streamlitä»ªè¡¨ç›˜åº”ç”¨
def create_monitoring_dashboard():
    st.set_page_config(
        page_title="æ¨¡å‹æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜",
        page_icon="ğŸ“Š",
        layout="wide"
    )
    
    st.title("è‰ºæœ¯é£æ ¼è½¬æ¢ç³»ç»Ÿ - æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜")
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_system_performance, daemon=True)
    monitor_thread.start()
    
    # å­˜å‚¨å†å²æ•°æ®
    if 'history' not in st.session_state:
        st.session_state.history = {
            'timestamp': [],
            'cpu_percent': [],
            'memory_percent': [],
            'gpu_load': [],
            'inference_latency': []
        }
    
    # åˆå§‹åŒ–æ¨¡å‹
    @st.cache_resource
    def load_model():
        model_path = "models/monet_generator.onnx"
        if os.path.exists(model_path):
            # é€‰æ‹©åˆé€‚çš„æ‰§è¡Œæä¾›ç¨‹åº
            if 'CUDAExecutionProvider' in ort.get_available_providers():
                providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
            else:
                providers = ['CPUExecutionProvider']
                
            session = ort.InferenceSession(model_path, providers=providers)
            return session
        else:
            st.error(f"æ¨¡å‹æ–‡ä»¶ {model_path} ä¸å­˜åœ¨!")
            return None
    
    onnx_session = load_model()
    
    # åˆ›å»ºä»ªè¡¨ç›˜å¸ƒå±€
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ç³»ç»Ÿèµ„æºç›‘æ§")
        
        # CPUåˆ©ç”¨ç‡å›¾è¡¨
        cpu_chart = st.empty()
        
        # å†…å­˜åˆ©ç”¨ç‡å›¾è¡¨
        memory_chart = st.empty()
        
        if any(GPUtil.getGPUs()):
            # GPUåˆ©ç”¨ç‡å›¾è¡¨
            gpu_chart = st.empty()
    
    with col2:
        st.subheader("æ¨¡å‹æ¨ç†æ€§èƒ½")
        
        # æ¨¡å‹é€‰æ‹©
        model_options = ["monet", "vangogh", "cezanne", "ukiyoe"]
        selected_model = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options)
        
        # ä¸Šä¼ æµ‹è¯•å›¾åƒ
        uploaded_file = st.file_uploader("ä¸Šä¼ æµ‹è¯•å›¾åƒ", type=["jpg", "jpeg", "png"])
        
        # æ¨ç†æ€§èƒ½å›¾è¡¨
        latency_chart = st.empty()
        
        # æ€§èƒ½è¯¦æƒ…
        perf_metrics = st.empty()
        
        # è¿è¡Œæµ‹è¯•æŒ‰é’®
        test_button = st.button("è¿è¡Œæ¨ç†æ€§èƒ½æµ‹è¯•")
    
    # åˆ›å»ºå›¾è¡¨æ›´æ–°å‡½æ•°
    def update_charts():
        # ä»é˜Ÿåˆ—è·å–æ‰€æœ‰æ•°æ®
        while not performance_queue.empty():
            try:
                data = performance_queue.get_nowait()
                
                # æ›´æ–°å†å²æ•°æ®
                st.session_state.history['timestamp'].append(data['timestamp'])
                st.session_state.history['cpu_percent'].append(data['cpu_percent'])
                st.session_state.history['memory_percent'].append(data['memory_percent'])
                
                # å¦‚æœæœ‰GPUæ•°æ®
                if data['gpu_data']:
                    gpu_load = data['gpu_data'][0]['load']  # å‡è®¾ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
                    st.session_state.history['gpu_load'].append(gpu_load)
                else:
                    # å¦‚æœæ²¡æœ‰GPUï¼Œæ·»åŠ Noneæˆ–0
                    st.session_state.history['gpu_load'].append(0)
                
                # é™åˆ¶å†å²æ•°æ®é•¿åº¦
                max_history = 100
                if len(st.session_state.history['timestamp']) > max_history:
                    for key in st.session_state.history:
                        st.session_state.history[key] = st.session_state.history[key][-max_history:]
            
            except queue.Empty:
                break
        
        # åˆ›å»ºæ•°æ®æ¡†
        df = pd.DataFrame(st.session_state.history)
        
        # æ›´æ–°CPUå›¾è¡¨
        cpu_fig = px.line(
            df, x='timestamp', y='cpu_percent',
            title='CPU åˆ©ç”¨ç‡ (%)',
            labels={'cpu_percent': 'CPU %', 'timestamp': 'æ—¶é—´'}
        )
        cpu_fig.update_layout(yaxis_range=[0, 100])
        cpu_chart.plotly_chart(cpu_fig, use_container_width=True)
        
        # æ›´æ–°å†…å­˜å›¾è¡¨
        memory_fig = px.line(
            df, x='timestamp', y='memory_percent',
            title='å†…å­˜åˆ©ç”¨ç‡ (%)',
            labels={'memory_percent': 'å†…å­˜ %', 'timestamp': 'æ—¶é—´'}
        )
        memory_fig.update_layout(yaxis_range=[0, 100])
        memory_chart.plotly_chart(memory_fig, use_container_width=True)
        
        # å¦‚æœæœ‰GPUæ•°æ®ï¼Œæ›´æ–°GPUå›¾è¡¨
        if any(GPUtil.getGPUs()):
            gpu_fig = px.line(
                df, x='timestamp', y='gpu_load',
                title='GPU åˆ©ç”¨ç‡ (%)',
                labels={'gpu_load': 'GPU %', 'timestamp': 'æ—¶é—´'}
            )
            gpu_fig.update_layout(yaxis_range=[0, 100])
            gpu_chart.plotly_chart(gpu_fig, use_container_width=True)
        
        # å¦‚æœæœ‰æ¨ç†æ•°æ®ï¼Œæ›´æ–°æ¨ç†æ€§èƒ½å›¾è¡¨
        if 'inference_latency' in df.columns and not df['inference_latency'].empty():
            latency_fig = px.line(
                df, x='timestamp', y='inference_latency',
                title='æ¨ç†å»¶è¿Ÿ (ms)',
                labels={'inference_latency': 'å»¶è¿Ÿ (ms)', 'timestamp': 'æ—¶é—´'}
            )
            latency_chart.plotly_chart(latency_fig, use_container_width=True)
    
    # å¤„ç†æ¨ç†æ€§èƒ½æµ‹è¯•
    if test_button and uploaded_file is not None and onnx_session is not None:
        with st.spinner("è¿è¡Œæ¨ç†æ€§èƒ½æµ‹è¯•ä¸­..."):
            # å¤„ç†ä¸Šä¼ çš„å›¾åƒ
            image = Image.open(uploaded_file).convert('RGB')
            
            # é¢„å¤„ç†
            from torchvision import transforms
            transform = transforms.Compose([
                transforms.Resize((256, 256)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
            ])
            input_tensor = transform(image).unsqueeze(0).numpy()
            
            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            perf_results = inference_performance_test(onnx_session, input_tensor, n_runs=20)
            
            # å±•ç¤ºæ€§èƒ½æŒ‡æ ‡
            col1, col2, col3 = perf_metrics.columns(3)
            col1.metric("å¹³å‡å»¶è¿Ÿ (ms)", f"{perf_results['mean_latency']:.2f}")
            col2.metric("ä¸­ä½å»¶è¿Ÿ (ms)", f"{perf_results['median_latency']:.2f}")
            col3.metric("P95å»¶è¿Ÿ (ms)", f"{perf_results['p95_latency']:.2f}")
            
            # è®°å½•æ¨ç†å»¶è¿Ÿå†å²
            for i in range(min(20, len(st.session_state.history['timestamp']))):
                st.session_state.history['inference_latency'].append(perf_results['mean_latency'])
    
    # å®šæœŸæ›´æ–°å›¾è¡¨
    update_charts()
    time.sleep(0.1)
    st.experimental_rerun()

# ä¸»å‡½æ•°
if __name__ == "__main__":
    create_monitoring_dashboard()

```

ç°åœ¨è®©æˆ‘å®Œæˆå…³äºCI/CDæµæ°´çº¿ä¸å•å…ƒæµ‹è¯•çš„éƒ¨åˆ†ï¼š

```yaml
name: Style Transfer CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu
        pip install pytest pytest-cov flake8 black isort
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Format check with black
      run: |
        black --check .
    
    - name: Import order check with isort
      run: |
        isort --check-only --profile black .
    
    - name: Run unit tests
      run: |
        pytest --cov=./ --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu
        pip install -r requirements.txt
        pip install onnx onnxruntime
    
    - name: Build and export ONNX models
      run: |
        python scripts/export_models.py
    
    - name: Package application
      run: |
        mkdir -p release
        cp -r models release/
        cp -r scripts release/
        cp -r app.py release/
        cp -r requirements.txt release/
        tar -czvf style-transfer-app.tar.gz release/
    
    - name: Upload artifacts
      uses: actions/upload-artifact@v3
      with:
        name: style-transfer-app
        path: style-transfer-app.tar.gz

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v3
    
    - name: Download artifact
      uses: actions/download-artifact@v3
      with:
        name: style-transfer-app
    
    - name: Deploy to staging
      run: |
        # è¿™é‡Œå¯ä»¥æ·»åŠ éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒçš„è„šæœ¬
        echo "Deploying to staging environment"
    
    - name: Run integration tests
      run: |
        # åœ¨æµ‹è¯•ç¯å¢ƒè¿è¡Œé›†æˆæµ‹è¯•
        echo "Running integration tests"
    
    - name: Deploy to production
      if: success()
      run: |
        # éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
        echo "Deploying to production environment"

```

```python
import pytest
import torch
import numpy as np
import os
import sys
from PIL import Image

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æ¨¡å‹å’Œå·¥å…·å‡½æ•°
from models.generator import Generator
from models.discriminator import Discriminator
from utils.transforms import preprocess_image, postprocess_image
from utils.onnx_utils import export_to_onnx

class TestGenerator:
    @pytest.fixture
    def generator(self):
        """åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„ç”Ÿæˆå™¨å®ä¾‹"""
        return Generator()
    
    def test_generator_output_shape(self, generator):
        """æµ‹è¯•ç”Ÿæˆå™¨è¾“å‡ºçš„å½¢çŠ¶æ˜¯å¦æ­£ç¡®"""
        batch_size = 2
        input_shape = (batch_size, 3, 256, 256)
        x = torch.randn(input_shape)
        output = generator(x)
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦ä¸è¾“å…¥ä¸€è‡´
        assert output.shape == input_shape
        
    def test_generator_output_range(self, generator):
        """æµ‹è¯•ç”Ÿæˆå™¨è¾“å‡ºçš„å€¼åŸŸæ˜¯å¦åœ¨[-1, 1]ä¹‹é—´"""
        batch_size = 2
        input_shape = (batch_size, 3, 256, 256)
        x = torch.randn(input_shape)
        
        # ç¡®ä¿è¾“å…¥åœ¨åˆç†èŒƒå›´å†…
        x = torch.clamp(x, -1, 1)
        
        output = generator(x)
        
        # æ£€æŸ¥è¾“å‡ºåœ¨[-1.1, 1.1]èŒƒå›´å†…ï¼ˆå…è®¸ä¸€ç‚¹æ•°å€¼è¯¯å·®ï¼‰
        assert output.min() >= -1.1
        assert output.max() <= 1.1

class TestDiscriminator:
    @pytest.fixture
    def discriminator(self):
        """åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„åˆ¤åˆ«å™¨å®ä¾‹"""
        return Discriminator()
    
    def test_discriminator_output_shape(self, discriminator):
        """æµ‹è¯•åˆ¤åˆ«å™¨è¾“å‡ºçš„å½¢çŠ¶æ˜¯å¦æ­£ç¡®"""
        batch_size = 2
        input_shape = (batch_size, 3, 256, 256)
        x = torch.randn(input_shape)
        output = discriminator(x)
        
        # PatchGANåˆ¤åˆ«å™¨è¾“å‡ºåº”è¯¥æ˜¯(batch_size, 1, H/16, W/16)
        expected_shape = (batch_size, 1, 256//16, 256//16)
        assert output.shape == expected_shape

class TestImageProcessing:
    @pytest.fixture
    def sample_image(self):
        """åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç”¨çš„æ ·ä¾‹å›¾åƒ"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„RGBå›¾åƒ
        img = np.zeros((100, 100, 3), dtype=np.uint8)
        img[:50, :50] = [255, 0, 0]  # å·¦ä¸Šè§’æ˜¯çº¢è‰²
        img[50:, :50] = [0, 255, 0]  # å·¦ä¸‹è§’æ˜¯ç»¿è‰²
        img[:50, 50:] = [0, 0, 255]  # å³ä¸Šè§’æ˜¯è“è‰²
        img[50:, 50:] = [255, 255, 0]  # å³ä¸‹è§’æ˜¯é»„è‰²
        return Image.fromarray(img)
    
    def test_preprocess_image(self, sample_image):
        """æµ‹è¯•å›¾åƒé¢„å¤„ç†å‡½æ•°"""
        tensor = preprocess_image(sample_image)
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºPyTorchå¼ é‡
        assert isinstance(tensor, torch.Tensor)
        
        # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦æ­£ç¡® (C, H, W)
        assert tensor.dim() == 3
        assert tensor.shape[0] == 3  # RGBé€šé“
        
        # æ£€æŸ¥å€¼åŸŸæ˜¯å¦åœ¨[-1, 1]ä¹‹é—´
        assert tensor.min() >= -1.1
        assert tensor.max() <= 1.1
    
    def test_postprocess_image(self):
        """æµ‹è¯•å›¾åƒåå¤„ç†å‡½æ•°"""
        # åˆ›å»ºä¸€ä¸ªåœ¨[-1, 1]èŒƒå›´å†…çš„å¼ é‡
        tensor = torch.rand(3, 64, 64) * 2 - 1
        
        img = postprocess_image(tensor)
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦ä¸ºPILå›¾åƒ
        assert isinstance(img, Image.Image)
        
        # æ£€æŸ¥å›¾åƒå¤§å°æ˜¯å¦æ­£ç¡®
        assert img.size == (64, 64)
        
        # æ£€æŸ¥å›¾åƒæ¨¡å¼æ˜¯å¦ä¸ºRGB
        assert img.mode == "RGB"

class TestONNXExport:
    @pytest.fixture
    def temp_onnx_path(self, tmp_path):
        """åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„ç”¨äºONNXæ¨¡å‹å¯¼å‡º"""
        return str(tmp_path / "temp_model.onnx")
    
    def test_onnx_export(self, temp_onnx_path):
        """æµ‹è¯•ONNXæ¨¡å‹å¯¼å‡ºåŠŸèƒ½"""
        # è·³è¿‡å¦‚æœæ²¡æœ‰å®‰è£…onnx
        pytest.importorskip("onnx")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹
        generator = Generator()
        
        # å¯¼å‡ºæ¨¡å‹
        success = export_to_onnx(generator, temp_onnx_path)
        
        # æ£€æŸ¥å¯¼å‡ºæ˜¯å¦æˆåŠŸ
        assert success
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        assert os.path.exists(temp_onnx_path)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦åˆç† (è‡³å°‘1MB)
        assert os.path.getsize(temp_onnx_path) > 1_000_000

class TestCycleGANTraining:
    def test_cycle_consistency_loss(self):
        """æµ‹è¯•å¾ªç¯ä¸€è‡´æ€§æŸå¤±è®¡ç®—æ˜¯å¦æ­£ç¡®"""
        from utils.losses import cycle_consistency_loss
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        real_images = torch.rand(2, 3, 64, 64)
        cycled_images = real_images + 0.1 * torch.rand(2, 3, 64, 64)
        
        # è®¡ç®—æŸå¤±
        loss = cycle_consistency_loss(real_images, cycled_images)
        
        # æŸå¤±åº”è¯¥æ˜¯æ­£æ•°
        assert loss > 0
        
        # ç›¸åŒå›¾åƒçš„æŸå¤±åº”è¯¥ä¸º0
        identical_loss = cycle_consistency_loss(real_images, real_images)
        assert identical_loss < 1e-5

    def test_identity_loss(self):
        """æµ‹è¯•èº«ä»½æ˜ å°„æŸå¤±è®¡ç®—æ˜¯å¦æ­£ç¡®"""
        from utils.losses import identity_mapping_loss
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®
        real_images = torch.rand(2, 3, 64, 64)
        identity_mapped = real_images + 0.1 * torch.rand(2, 3, 64, 64)
        
        # è®¡ç®—æŸå¤±
        loss = identity_mapping_loss(real_images, identity_mapped)
        
        # æŸå¤±åº”è¯¥æ˜¯æ­£æ•°
        assert loss > 0
        
        # ç›¸åŒå›¾åƒçš„æŸå¤±åº”è¯¥ä¸º0
        identical_loss = identity_mapping_loss(real_images, real_images)
        assert identical_loss < 1e-5

# é›†æˆæµ‹è¯•
class TestEndToEnd:
    def test_style_transfer_pipeline(self, monkeypatch):
        """æµ‹è¯•å®Œæ•´çš„é£æ ¼è½¬æ¢æµç¨‹"""
        # æ¨¡æ‹Ÿå›¾åƒå¤„ç†å’Œæ¨¡å‹æ¨ç†
        def mock_load_image(*args, **kwargs):
            return Image.new('RGB', (256, 256), color='white')
        
        def mock_inference(*args, **kwargs):
            # è¿”å›ä¸€ä¸ªéšæœºå›¾åƒ
            tensor = torch.rand(3, 256, 256) * 2 - 1
            return postprocess_image(tensor)
        
        # æ‰“è¡¥ä¸æ›¿æ¢çœŸå®å‡½æ•°
        import utils.image_utils
        monkeypatch.setattr(utils.image_utils, "load_image", mock_load_image)
        monkeypatch.setattr(utils.inference, "run_inference", mock_inference)
        
        # æµ‹è¯•ç«¯åˆ°ç«¯æµç¨‹
        from pipeline import StyleTransferPipeline
        
        pipeline = StyleTransferPipeline(style="monet")
        result = pipeline.process("dummy_input.jpg")
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºPILå›¾åƒ
        assert isinstance(result, Image.Image)
        
        # æ£€æŸ¥å›¾åƒå°ºå¯¸
        assert result.size == (256, 256)

if __name__ == "__main__":
    pytest.main(["-v"])

```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://i-blog.csdnimg.cn/direct/0d43d2b71ade41358137e07961918407.png)

### å®Œæ•´å­¦ä¹ è®¡åˆ’æ€»ç»“

ä»Šå¤©æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„ç«¯åˆ°ç«¯å›¾åƒç”Ÿæˆç³»ç»Ÿï¼Œå°†CycleGANå’Œé£æ ¼è¿ç§»æŠ€æœ¯ç»“åˆèµ·æ¥ï¼Œå¹¶é€šè¿‡å¤šç§å·¥ç¨‹å®è·µå®ç°äº†é«˜æ•ˆçš„è®­ç»ƒã€éƒ¨ç½²å’Œç›‘æ§ã€‚

#### 1. CycleGANä¸é£æ ¼è¿ç§»ç»“åˆ

æˆ‘ä»¬å›é¡¾äº†CycleGANçš„æ ¸å¿ƒåŸç†ï¼Œå®ƒé€šè¿‡ä¸¤ä¸ªç”Ÿæˆå™¨å’Œä¸¤ä¸ªåˆ¤åˆ«å™¨å®ç°äº†æ— é…å¯¹æ•°æ®çš„åŸŸé—´è½¬æ¢ï¼ŒåŒæ—¶å¼•å…¥äº†å¾ªç¯ä¸€è‡´æ€§æŸå¤±å’Œèº«ä»½æ˜ å°„æŸå¤±æ¥ä¿è¯è½¬æ¢è´¨é‡ã€‚é£æ ¼è¿ç§»çš„æ ¸å¿ƒåœ¨äºå†…å®¹æŸå¤±å’Œé£æ ¼æŸå¤±ï¼Œé€šè¿‡GramçŸ©é˜µè®¡ç®—å®ç°äº†é£æ ¼ç‰¹å¾çš„æå–å’Œåº”ç”¨ã€‚

è¿™ä¸¤ç§æŠ€æœ¯ç»“åˆåï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ªçµæ´»çš„è‰ºæœ¯é£æ ¼è½¬æ¢ç³»ç»Ÿï¼Œæ—¢èƒ½å¤Ÿä¿æŒå†…å®¹çš„å®Œæ•´æ€§ï¼Œåˆèƒ½å¤Ÿåº”ç”¨å¤šç§è‰ºæœ¯é£æ ¼ã€‚

#### 2. å¤šGPUè®­ç»ƒä¸æ··åˆç²¾åº¦è®­ç»ƒ

ä¸ºäº†åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨äº†PyTorchçš„
`DistributedDataParallel`
å®ç°æ•°æ®å¹¶è¡Œè®­ç»ƒï¼Œè®©å¤šä¸ªGPUåŒæ—¶å¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ··åˆç²¾åº¦è®­ç»ƒï¼Œä½¿ç”¨
`torch.cuda.amp`
æ¨¡å—çš„
`autocast`
å’Œ
`GradScaler`
ï¼Œé€šè¿‡FP16å’ŒFP32æ··åˆè®¡ç®—å¤§å¹…æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨ã€‚

#### 3. Gradioäº¤äº’å¼ç•Œé¢

æˆ‘ä»¬é€šè¿‡Gradioåˆ›å»ºäº†ä¸€ä¸ªç®€æ´è€Œå¼ºå¤§çš„Webç•Œé¢ï¼Œæ”¯æŒå›¾åƒä¸Šä¼ ã€é£æ ¼é€‰æ‹©å’Œç»“æœå±•ç¤ºï¼Œå¤§å¤§æé«˜äº†ç³»ç»Ÿçš„å¯ç”¨æ€§ã€‚Gradioçš„ä¼˜åŠ¿åœ¨äºå¿«é€Ÿå¼€å‘ã€æ˜“äºéƒ¨ç½²ï¼Œé€‚åˆAIåº”ç”¨çš„åŸå‹å¼€å‘å’Œæ¼”ç¤ºã€‚

#### 4. ONNXæ¨¡å‹å¯¼å‡ºä¸ä¼˜åŒ–

é€šè¿‡å°†PyTorchæ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼Œæˆ‘ä»¬å®ç°äº†è·¨å¹³å°çš„æ¨¡å‹éƒ¨ç½²ï¼Œå¹¶é€šè¿‡ONNX Runtimeæ˜¾è‘—æé«˜äº†æ¨ç†é€Ÿåº¦ã€‚ONNXçš„å›¾ä¼˜åŒ–å’Œç®—å­èåˆåŠŸèƒ½è¿›ä¸€æ­¥æå‡äº†æ¨¡å‹æ€§èƒ½ï¼ŒåŒæ—¶é€šè¿‡é‡åŒ–æŠ€æœ¯å‡å°äº†æ¨¡å‹ä½“ç§¯ã€‚

#### 5. æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜

ä½¿ç”¨Streamlitæ„å»ºäº†å®æ—¶æ€§èƒ½ç›‘æ§ä»ªè¡¨ç›˜ï¼Œè·Ÿè¸ªCPUã€å†…å­˜ã€GPUä½¿ç”¨ç‡å’Œæ¨ç†å»¶è¿Ÿç­‰å…³é”®æŒ‡æ ‡ï¼Œå¸®åŠ©è¯†åˆ«æ€§èƒ½ç“¶é¢ˆå¹¶ä¼˜åŒ–ç³»ç»Ÿè¡¨ç°ã€‚

#### 6. CI/CDæµæ°´çº¿ä¸å•å…ƒæµ‹è¯•

æ„å»ºäº†å®Œæ•´çš„CI/CDæµæ°´çº¿ï¼ŒåŒ…æ‹¬ä»£ç è´¨é‡æ£€æŸ¥ã€å•å…ƒæµ‹è¯•ã€æ„å»ºä¸æ‰“åŒ…ã€éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒã€‚åŒæ—¶ï¼Œç¼–å†™äº†å…¨é¢çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–äº†æ¨¡å‹ç»“æ„ã€å›¾åƒå¤„ç†ã€æŸå¤±è®¡ç®—ç­‰æ ¸å¿ƒåŠŸèƒ½ï¼Œä¿è¯äº†ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚

### å®æˆ˜é¡¹ç›®æ¼”ç¤ºæµç¨‹

1. **ç¯å¢ƒå‡†å¤‡**
   ï¼šå®‰è£…PyTorchã€ONNXã€Gradioç­‰ä¾èµ–
2. **æ¨¡å‹è®­ç»ƒ**
   ï¼šä½¿ç”¨å¤šGPUå’Œæ··åˆç²¾åº¦è®­ç»ƒCycleGANæ¨¡å‹
3. **æ¨¡å‹ä¼˜åŒ–**
   ï¼šå¯¼å‡ºä¸ºONNXå¹¶ä½¿ç”¨RuntimeåŠ é€Ÿ
4. **äº¤äº’ç•Œé¢**
   ï¼šå¯åŠ¨Gradioç•Œé¢è¿›è¡Œæ¼”ç¤º
5. **æ€§èƒ½ç›‘æ§**
   ï¼šå¯åŠ¨ç›‘æ§ä»ªè¡¨ç›˜è§‚å¯Ÿç³»ç»Ÿæ€§èƒ½
6. **CI/CDéƒ¨ç½²**
   ï¼šé€šè¿‡GitHub Actionsè‡ªåŠ¨éƒ¨ç½²

---

**æ¸…åå¤§å­¦å…¨ä¸‰ç‰ˆçš„ã€ŠDeepSeekæ•™ç¨‹ã€‹å®Œæ•´çš„æ–‡æ¡£éœ€è¦çš„æœ‹å‹ï¼Œå…³æ³¨æˆ‘ç§ä¿¡ï¼šdeepseek å³å¯è·å¾—ã€‚**

**æ€ä¹ˆæ ·ä»Šå¤©çš„å†…å®¹è¿˜æ»¡æ„å—ï¼Ÿå†æ¬¡æ„Ÿè°¢æœ‹å‹ä»¬çš„è§‚çœ‹ï¼Œå…³æ³¨GZHï¼šå‡¡äººçš„AIå·¥å…·ç®±ï¼Œå›å¤666ï¼Œé€æ‚¨ä»·å€¼199çš„AIå¤§ç¤¼åŒ…ã€‚æœ€åï¼Œç¥æ‚¨æ—©æ—¥å®ç°è´¢åŠ¡è‡ªç”±ï¼Œè¿˜è¯·ç»™ä¸ªèµï¼Œè°¢è°¢ï¼**