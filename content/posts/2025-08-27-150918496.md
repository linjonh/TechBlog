---
layout: post
title: "什么是迁移学习transfer-learning"
date: 2025-08-27T10:07:15+0800
description: "迁移学习是一种机器学习方法，其中从一个任务中使用的模型获得的知识的应用可以重复用作另一任务的基础点。"
keywords: "什么是迁移学习（transfer learning）"
categories: ['未分类']
tags: ['迁移学习', '机器学习', '人工智能']
artid: "150918496"
arturl: "https://blog.csdn.net/weixin_41560800/article/details/150918496"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=150918496
    alt: "什么是迁移学习transfer-learning"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=150918496
featuredImagePreview: https://bing.ee123.net/img/rand?artid=150918496
cover: https://bing.ee123.net/img/rand?artid=150918496
image: https://bing.ee123.net/img/rand?artid=150918496
img: https://bing.ee123.net/img/rand?artid=150918496
---



# 什么是迁移学习（transfer learning）

迁移学习是一种机器学习方法，其中从一个任务中使用的模型获得的知识的应用可以重复用作另一任务的基础点。

机器学习算法使用历史数据作为输入来进行预测并产生新的输出值。它们通常设计用于执行孤立的任务。源任务是知识从其转移到目标任务的任务。目标任务是指由于从源任务转移知识而导致学习得到改善。

在迁移学习期间，利用源任务的知识和快速进展来改进新目标任务的学习和开发。知识的应用是利用源任务的属性和特征，将其应用并映射到目标任务上。

然而，如果转移方法导致新目标任务的性能下降，则称为负迁移。使用迁移学习方法时的主要挑战之一是能够提供并确保相关任务之间的正迁移，同时避免不太相关的任务之间的负迁移。

![](https://i-blog.csdnimg.cn/direct/33c5a13b4396486b8b12e025c8fdc5fc.jpeg)

**不同类型的迁移学习**

1. 归纳迁移学习：在这种类型的迁移学习中，源任务和目标任务是相同的，但是它们仍然彼此不同。该模型将使用源任务的归纳偏差来帮助提高目标任务的性能。源任务可能包含也可能不包含标记数据，进一步导致使用多任务学习和自学学习的模型。
2. 无监督迁移学习：在这种情况下，源和目标相似，但是任务不同，源和目标中的数据都未标记。降维和聚类等技术在无监督学习中是众所周知的。
3. 转导式迁移学习：在最后一种类型的迁移学习中，源任务和目标任务有相似之处，但是领域不同。源域包含大量标记数据，而目标域中缺乏标记数据，进一步导致使用域适应的模型。

**迁移学习与微调**

微调是迁移学习中的一个可选步骤，主要用于提高模型的性能。迁移学习和微调之间的区别就在于名称。

迁移学习建立在采用从一项任务中学到的特征并将所利用的知识迁移到新任务上的基础上。迁移学习通常用于数据集太小的任务，从头开始训练全面的模型。微调的基础是对流程进行微调，以获得所需的输出，从而进一步提高性能。在微调过程中，对经过训练的模型的参数进行精确且具体的调整和定制，同时尝试验证模型以实现所需的输出。

**为什么使用迁移学习？**

不需要大量数据——由于缺乏可用性，获取数据始终是一个障碍。使用不足的数据量可能会导致性能低下。这就是迁移学习的亮点，因为机器学习模型可以使用小型训练数据集来构建，因为它是预先训练的。

节省训练时间——机器学习模型很难训练，并且会占用大量时间，导致效率低下。从头开始训练深度神经网络来完成一项复杂的任务需要很长时间，因此使用预先训练的模型可以节省构建新模型的时间。

**迁移学习的优点**

更好的基础：在迁移学习中使用预先训练的模型可以为我们提供更好的基础和起点，使我们甚至无需训练模型即可执行某些任务。

更高的学习率：由于模型事先已经接受过类似任务的训练，因此该模型具有更高的学习率。

更高的准确率：凭借更好的基础和更高的学习率，模型可以以更高的性能工作，产生更准确的输出。

**迁移学习什么时候不起作用？**

当源任务训练的权重与目标任务不同时，应避免迁移学习。例如，如果之前的网络接受过分类猫和狗的训练，而新网络正在尝试检测鞋子和袜子，那么就会出现问题，因为从源转移到目标任务的权重将无法给出最好的结果。因此，使用与我们期望的输出类似的预训练权重初始化网络比使用没有相关性的权重更好。

从预训练模型中删除层会导致模型架构出现问题。如果删除第一层，模型将具有较低的学习率，因为它必须处理低级特征。删除层会减少可训练的参数数量，这可能会导致过拟合。能够使用正确数量的层对于减少过拟合至关重要，但这也是一个及时的过程。

**迁移学习的缺点**

负迁移学习：正如我上面提到的，负迁移学习是指以前的学习方法阻碍了新任务。只有当源和目标不够相似，导致第一轮训练相差太远时，才会发生这种情况。算法不必总是与我们认为相似的东西一致，这使得我们很难理解哪种类型的训练足够的基本原理和标准。

![](https://i-blog.csdnimg.cn/direct/d7e71ac8c7384f5daec7f2e752eb7ce8.jpeg)

**迁移学习的 6 个步骤**

让我们深入了解如何实施迁移学习以及所采取的步骤。迁移学习有 6 个一般步骤，我们将逐一介绍。

1. 选择源任务：第一步是选择一个包含大量数据的预训练模型，使输入和输出数据与我们选择的目标任务之间存在关系。
2. 创建基础模型：使用预先训练的权重实例化基础模型。可以通过 Xception 等架构访问预训练的权重。
3. 冻结层：为了减少再次初始化权重，需要冻结预训练模型中的层。它将弥补已经学到的知识，并让我们免于从头开始训练模型。
4. 添加新的可训练层：在冻结层之上添加新的可训练层，会将旧特征转换为新数据集的预测。
5. 训练新层：预训练模型已包含最终输出层。预训练模型的当前输出与我们想要的模型输出不同的可能性很高。因此，我们必须使用新的输出层来训练模型。因此，根据我们的预期模型添加新的密集层和最终的密集层，将提高学习率并产生我们想要的输出。
6. 微调：我们可以通过微调来提高模型的性能，微调是通过解冻全部或部分基础模型，然后以非常低的学习率重新训练模型来完成的。在此阶段使用较低的学习率至关重要，因为我们正在训练的模型比第一轮最初的模型要大得多，而且数据集也很小。因此，如果应用较大的权重更新，您将面临过度拟合的风险，因此我们需要以增量方式进行微调。当我们更改模型的行为时重新编译模型，然后再次重新训练模型，监视任何过拟合反馈。



