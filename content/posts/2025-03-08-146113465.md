---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34353633313132332f:61727469636c652f64657461696c732f313436313133343635"
layout: post
title: "在昇腾GPU上部署DeepSeek大模型与OpenWebUI从零到生产的完整指南"
date: 2025-03-08 11:40:17 +08:00
description: "随着国产AI芯片的快速发展，昇腾（Ascend）系列GPU凭借其高性能和兼容性，逐渐成为大模型部署的重要选择。本文将以昇腾300i为例，手把手教你如何部署DeepSeek大模型，并搭配OpenWebUI构建交互式界面。无论你是AI开发者还是企业运维，都能通过本文快速搭建生产级AI服务。通过本文，你已成功在昇腾GPU上构建了从模型推理到Web交互的完整链路。随着昇腾生态的不断完善，国产AI芯片正在为开发者打开新的可能性。希望这篇指南能为你的AI应用部署提供实用参考！如有疑问，欢迎在评论区交流讨论。"
keywords: "昇腾显卡 deepseek部署调用"
categories: ['未分类']
tags: ['容器', '人工智能', 'Gpu', 'Docker']
artid: "146113465"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146113465
    alt: "在昇腾GPU上部署DeepSeek大模型与OpenWebUI从零到生产的完整指南"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146113465
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146113465
cover: https://bing.ee123.net/img/rand?artid=146113465
image: https://bing.ee123.net/img/rand?artid=146113465
img: https://bing.ee123.net/img/rand?artid=146113465
---

# 在昇腾GPU上部署DeepSeek大模型与OpenWebUI：从零到生产的完整指南

##### **引言**

随着国产AI芯片的快速发展，昇腾（Ascend）系列GPU凭借其高性能和兼容性，逐渐成为大模型部署的重要选择。本文将以昇腾300i为例，手把手教你如何部署DeepSeek大模型，并搭配OpenWebUI构建交互式界面。无论你是AI开发者还是企业运维，都能通过本文快速搭建生产级AI服务。

---

#### **一、为什么选择昇腾GPU？**

```
 信创要求，现在N卡其实便宜了

```

---

#### **二、环境准备**

##### **1. 基础配置检查**

```bash
# 确认操作系统版本（推荐OpenEuler 22.03）
cat /etc/os-release

# 检查NPU驱动状态（关键！）
npu-smi info
# 预期输出：能看到NPU设备列表和驱动版本（≥6.0.RC3）

```

##### **2. 安装依赖工具**

```bash
# 禁用防火墙
systemctl stop firewalld && systemctl disable firewalld

# 安装开发工具链
yum install -y git gcc cmake python3-devel

```

---

#### **三、Docker环境配置**

##### **1. 配置Docker镜像加速**

```bash
# 创建配置文件
vi > /etc/docker/daemon.json <<EOF
{
  "registry-mirrors": [
        "https://docker.1ms.run",
        "https://docker.xuanyuan.me",

  "data-root": "/data/docker"  # 建议挂载至大容量存储
}
EOF

# 重启生效
systemctl restart docker

```

##### **2. 安装昇腾容器插件**

```bash
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Ascend-Docker-Runtime/6.0.RC3/Ascend-docker-runtime_6.0.RC3_linux-aarch64.run
chmod +x Ascend-docker-runtime_6.0.RC3_linux-aarch64.run
./Ascend-docker-runtime_6.0.RC3_linux-aarch64.run --install

```

---

#### **四、部署GPUSTACK（NPU管理平台）**

##### **1. 启动管理服务**

```bash
docker run -d \
  --name=gpustack \
  -p 80:80 \
  -v /usr/local/Ascend/driver:/usr/local/Ascend/driver:ro \
  --device=/dev/davinci0 \
  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/gpustack/gpustack:latest-npu

```

**关键参数说明**
：

* `--device`
  ：挂载NPU设备，多个设备可重复添加
* `-v /usr/local/Ascend/driver`
  ：只读挂载驱动，避免容器内版本冲突

##### **2. 登录管理界面**

访问
`http://<服务器IP>:80`
，使用以下命令获取初始密码：

```bash
docker exec gpustack cat /var/lib/gpustack/initial_admin_password

```

---

#### **五、DeepSeek模型部署实战**

##### **1. 模型准备**

```bash
# 创建模型目录
mkdir -p /data/models/deepseek-14b && cd /data/models

# 下载模型文件（以DeepSeek-R1-Distill-Qwen-14B为例）
wget https://modelscope.cn/api/v1/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/repo?Revision=master -O deepseek-14b.tar.gz
tar zxvf deepseek-14b.tar.gz

```

##### **2. 启动vLLM推理服务**

```bash
docker run -d \
  --name=deepseek-inference \
  --runtime=ascend \  # 指定昇腾运行时
  -p 23333:8000 \
  -v /data/models/deepseek-14b:/model \
  swr.cn-south-1.myhuaweicloud.com/ascendhub/vllm-ascend:0.7.3 \
  --model=/model \
  --tensor-parallel-size=1 \
  --max-model-len=4096

```

**性能调优建议**
：

* 调整
  `--max-model-len`
  控制显存占用
* 添加
  `--quantization awq`
  启用4bit量化

---

#### **六、集成OpenWebUI**

##### **1. 部署Web界面**

```bash
docker run -d \
  --name=openwebui \
  -p 3000:8080 \
  -v /data/openwebui:/app/backend/data \
  -e OPENAI_API_BASE_URL=http://host.docker.internal:23333/v1 \
  ghcr.io/open-webui/open-webui:main

```

##### **2. 界面配置**

1. 访问
   `http://<服务器IP>:3000`
2. 进入设置 → 模型 → 添加：
   * **模型名称**
     ：DeepSeek-R1-Distill-Qwen-14B
   * **API Base URL**
     ：
     `http://host.docker.internal:23333/v1`
   * **API Key**
     ：留空

---

#### **七、性能监控与优化**

##### **1. 实时监控命令**

```bash
# 查看NPU利用率
npu-smi info -t training -i 0 -c

# 查看服务日志
docker logs -f deepseek-inference --tail 100

```

---

#### **八、常见问题排查**

##### **Q1：容器启动报错 `npu-smi command not found`**

**原因**
：驱动未正确挂载
  
**解决**
：

```bash
docker run -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi ... # 添加挂载

```

##### **Q2：模型加载缓慢**

**优化方案**
：

```bash
# 启用模型缓存
docker run -e VLLM_USE_MODELSCOPE=true ...

```

---

#### **结语**

通过本文，你已成功在昇腾GPU上构建了从模型推理到Web交互的完整链路。这种方案不仅适用于DeepSeek，也可快速迁移到其他开源模型（如Qwen、ChatGLM）。随着昇腾生态的不断完善，国产AI芯片正在为开发者打开新的可能性。

希望这篇指南能为你的AI应用部署提供实用参考！如有疑问，欢迎在评论区交流讨论。