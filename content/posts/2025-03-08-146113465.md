---
layout: post
title: "在昇腾GPU上部署DeepSeek大模型与OpenWebUI从零到生产的完整指南"
date: 2025-03-08 11:40:17 +0800
description: "随着国产AI芯片的快速发展，昇腾（Ascend）系列GPU凭借其高性能和兼容性，逐渐成为大模型部署的重要选择。本文将以昇腾300i为例，手把手教你如何部署DeepSeek大模型，并搭配OpenWebUI构建交互式界面。无论你是AI开发者还是企业运维，都能通过本文快速搭建生产级AI服务。通过本文，你已成功在昇腾GPU上构建了从模型推理到Web交互的完整链路。随着昇腾生态的不断完善，国产AI芯片正在为开发者打开新的可能性。希望这篇指南能为你的AI应用部署提供实用参考！如有疑问，欢迎在评论区交流讨论。"
keywords: "昇腾显卡 deepseek部署调用"
categories: ['未分类']
tags: ['容器', '人工智能', 'Gpu', 'Docker']
artid: "146113465"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146113465
    alt: "在昇腾GPU上部署DeepSeek大模型与OpenWebUI从零到生产的完整指南"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146113465
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146113465
cover: https://bing.ee123.net/img/rand?artid=146113465
image: https://bing.ee123.net/img/rand?artid=146113465
img: https://bing.ee123.net/img/rand?artid=146113465
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     在昇腾GPU上部署DeepSeek大模型与OpenWebUI：从零到生产的完整指南
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h5>
     <a id="_3">
     </a>
     <strong>
      引言
     </strong>
    </h5>
    <p>
     随着国产AI芯片的快速发展，昇腾（Ascend）系列GPU凭借其高性能和兼容性，逐渐成为大模型部署的重要选择。本文将以昇腾300i为例，手把手教你如何部署DeepSeek大模型，并搭配OpenWebUI构建交互式界面。无论你是AI开发者还是企业运维，都能通过本文快速搭建生产级AI服务。
    </p>
    <hr/>
    <h4>
     <a id="GPU_8">
     </a>
     <strong>
      一、为什么选择昇腾GPU？
     </strong>
    </h4>
    <pre><code> 信创要求，现在N卡其实便宜了
</code></pre>
    <hr/>
    <h4>
     <a id="_12">
     </a>
     <strong>
      二、环境准备
     </strong>
    </h4>
    <h5>
     <a id="1__13">
     </a>
     <strong>
      1. 基础配置检查
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 确认操作系统版本（推荐OpenEuler 22.03）</span>
<span class="token function">cat</span> /etc/os-release

<span class="token comment"># 检查NPU驱动状态（关键！）</span>
npu-smi info
<span class="token comment"># 预期输出：能看到NPU设备列表和驱动版本（≥6.0.RC3）</span>
</code></pre>
    <h5>
     <a id="2__23">
     </a>
     <strong>
      2. 安装依赖工具
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 禁用防火墙</span>
systemctl stop firewalld <span class="token operator">&amp;&amp;</span> systemctl disable firewalld

<span class="token comment"># 安装开发工具链</span>
yum <span class="token function">install</span> <span class="token parameter variable">-y</span> <span class="token function">git</span> gcc cmake python3-devel
</code></pre>
    <hr/>
    <h4>
     <a id="Docker_34">
     </a>
     <strong>
      三、Docker环境配置
     </strong>
    </h4>
    <h5>
     <a id="1_Docker_35">
     </a>
     <strong>
      1. 配置Docker镜像加速
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 创建配置文件</span>
<span class="token function">vi</span> <span class="token operator">&gt;</span> /etc/docker/daemon.json <span class="token operator">&lt;&lt;</span><span class="token string">EOF
{
  "registry-mirrors": [
        "https://docker.1ms.run",
        "https://docker.xuanyuan.me",

  "data-root": "/data/docker"  # 建议挂载至大容量存储
}
EOF</span>

<span class="token comment"># 重启生效</span>
systemctl restart <span class="token function">docker</span>
</code></pre>
    <h5>
     <a id="2__52">
     </a>
     <strong>
      2. 安装昇腾容器插件
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token function">wget</span> https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Ascend-Docker-Runtime/6.0.RC3/Ascend-docker-runtime_6.0.RC3_linux-aarch64.run
<span class="token function">chmod</span> +x Ascend-docker-runtime_6.0.RC3_linux-aarch64.run
./Ascend-docker-runtime_6.0.RC3_linux-aarch64.run <span class="token parameter variable">--install</span>
</code></pre>
    <hr/>
    <h4>
     <a id="GPUSTACKNPU_61">
     </a>
     <strong>
      四、部署GPUSTACK（NPU管理平台）
     </strong>
    </h4>
    <h5>
     <a id="1__62">
     </a>
     <strong>
      1. 启动管理服务
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span><span class="token operator">=</span>gpustack <span class="token punctuation">\</span>
  <span class="token parameter variable">-p</span> <span class="token number">80</span>:80 <span class="token punctuation">\</span>
  <span class="token parameter variable">-v</span> /usr/local/Ascend/driver:/usr/local/Ascend/driver:ro <span class="token punctuation">\</span>
  <span class="token parameter variable">--device</span><span class="token operator">=</span>/dev/davinci0 <span class="token punctuation">\</span>
  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/gpustack/gpustack:latest-npu
</code></pre>
    <p>
     <strong>
      关键参数说明
     </strong>
     ：
    </p>
    <ul>
     <li>
      <code>
       --device
      </code>
      ：挂载NPU设备，多个设备可重复添加
     </li>
     <li>
      <code>
       -v /usr/local/Ascend/driver
      </code>
      ：只读挂载驱动，避免容器内版本冲突
     </li>
    </ul>
    <h5>
     <a id="2__75">
     </a>
     <strong>
      2. 登录管理界面
     </strong>
    </h5>
    <p>
     访问
     <code>
      http://&lt;服务器IP&gt;:80
     </code>
     ，使用以下命令获取初始密码：
    </p>
    <pre><code class="prism language-bash"><span class="token function">docker</span> <span class="token builtin class-name">exec</span> gpustack <span class="token function">cat</span> /var/lib/gpustack/initial_admin_password
</code></pre>
    <hr/>
    <h4>
     <a id="DeepSeek_83">
     </a>
     <strong>
      五、DeepSeek模型部署实战
     </strong>
    </h4>
    <h5>
     <a id="1__84">
     </a>
     <strong>
      1. 模型准备
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 创建模型目录</span>
<span class="token function">mkdir</span> <span class="token parameter variable">-p</span> /data/models/deepseek-14b <span class="token operator">&amp;&amp;</span> <span class="token builtin class-name">cd</span> /data/models

<span class="token comment"># 下载模型文件（以DeepSeek-R1-Distill-Qwen-14B为例）</span>
<span class="token function">wget</span> https://modelscope.cn/api/v1/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B/repo?Revision<span class="token operator">=</span>master <span class="token parameter variable">-O</span> deepseek-14b.tar.gz
<span class="token function">tar</span> zxvf deepseek-14b.tar.gz
</code></pre>
    <h5>
     <a id="2_vLLM_94">
     </a>
     <strong>
      2. 启动vLLM推理服务
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span><span class="token operator">=</span>deepseek-inference <span class="token punctuation">\</span>
  <span class="token parameter variable">--runtime</span><span class="token operator">=</span>ascend <span class="token punctuation">\</span>  <span class="token comment"># 指定昇腾运行时</span>
  <span class="token parameter variable">-p</span> <span class="token number">23333</span>:8000 <span class="token punctuation">\</span>
  <span class="token parameter variable">-v</span> /data/models/deepseek-14b:/model <span class="token punctuation">\</span>
  swr.cn-south-1.myhuaweicloud.com/ascendhub/vllm-ascend:0.7.3 <span class="token punctuation">\</span>
  <span class="token parameter variable">--model</span><span class="token operator">=</span>/model <span class="token punctuation">\</span>
  --tensor-parallel-size<span class="token operator">=</span><span class="token number">1</span> <span class="token punctuation">\</span>
  --max-model-len<span class="token operator">=</span><span class="token number">4096</span>
</code></pre>
    <p>
     <strong>
      性能调优建议
     </strong>
     ：
    </p>
    <ul>
     <li>
      调整
      <code>
       --max-model-len
      </code>
      控制显存占用
     </li>
     <li>
      添加
      <code>
       --quantization awq
      </code>
      启用4bit量化
     </li>
    </ul>
    <hr/>
    <h4>
     <a id="OpenWebUI_112">
     </a>
     <strong>
      六、集成OpenWebUI
     </strong>
    </h4>
    <h5>
     <a id="1_Web_113">
     </a>
     <strong>
      1. 部署Web界面
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span><span class="token operator">=</span>openwebui <span class="token punctuation">\</span>
  <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 <span class="token punctuation">\</span>
  <span class="token parameter variable">-v</span> /data/openwebui:/app/backend/data <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">OPENAI_API_BASE_URL</span><span class="token operator">=</span>http://host.docker.internal:23333/v1 <span class="token punctuation">\</span>
  ghcr.io/open-webui/open-webui:main
</code></pre>
    <h5>
     <a id="2__123">
     </a>
     <strong>
      2. 界面配置
     </strong>
    </h5>
    <ol>
     <li>
      访问
      <code>
       http://&lt;服务器IP&gt;:3000
      </code>
     </li>
     <li>
      进入设置 → 模型 → 添加：
      <ul>
       <li>
        <strong>
         模型名称
        </strong>
        ：DeepSeek-R1-Distill-Qwen-14B
       </li>
       <li>
        <strong>
         API Base URL
        </strong>
        ：
        <code>
         http://host.docker.internal:23333/v1
        </code>
       </li>
       <li>
        <strong>
         API Key
        </strong>
        ：留空
       </li>
      </ul>
     </li>
    </ol>
    <hr/>
    <h4>
     <a id="_132">
     </a>
     <strong>
      七、性能监控与优化
     </strong>
    </h4>
    <h5>
     <a id="1__133">
     </a>
     <strong>
      1. 实时监控命令
     </strong>
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 查看NPU利用率</span>
npu-smi info <span class="token parameter variable">-t</span> training <span class="token parameter variable">-i</span> <span class="token number">0</span> <span class="token parameter variable">-c</span>

<span class="token comment"># 查看服务日志</span>
<span class="token function">docker</span> logs <span class="token parameter variable">-f</span> deepseek-inference <span class="token parameter variable">--tail</span> <span class="token number">100</span>
</code></pre>
    <hr/>
    <h4>
     <a id="_145">
     </a>
     <strong>
      八、常见问题排查
     </strong>
    </h4>
    <h5>
     <a id="Q1npusmi_command_not_found_146">
     </a>
     <strong>
      Q1：容器启动报错
      <code>
       npu-smi command not found
      </code>
     </strong>
    </h5>
    <p>
     <strong>
      原因
     </strong>
     ：驱动未正确挂载
     <br/>
     <strong>
      解决
     </strong>
     ：
    </p>
    <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-v</span> /usr/local/bin/npu-smi:/usr/local/bin/npu-smi <span class="token punctuation">..</span>. <span class="token comment"># 添加挂载</span>
</code></pre>
    <h5>
     <a id="Q2_153">
     </a>
     <strong>
      Q2：模型加载缓慢
     </strong>
    </h5>
    <p>
     <strong>
      优化方案
     </strong>
     ：
    </p>
    <pre><code class="prism language-bash"><span class="token comment"># 启用模型缓存</span>
<span class="token function">docker</span> run <span class="token parameter variable">-e</span> <span class="token assign-left variable">VLLM_USE_MODELSCOPE</span><span class="token operator">=</span>true <span class="token punctuation">..</span>.
</code></pre>
    <hr/>
    <h4>
     <a id="_162">
     </a>
     <strong>
      结语
     </strong>
    </h4>
    <p>
     通过本文，你已成功在昇腾GPU上构建了从模型推理到Web交互的完整链路。这种方案不仅适用于DeepSeek，也可快速迁移到其他开源模型（如Qwen、ChatGLM）。随着昇腾生态的不断完善，国产AI芯片正在为开发者打开新的可能性。
    </p>
    <p>
     希望这篇指南能为你的AI应用部署提供实用参考！如有疑问，欢迎在评论区交流讨论。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34353633313132332f:61727469636c652f64657461696c732f313436313133343635" class_="artid" style="display:none">
 </p>
</div>


