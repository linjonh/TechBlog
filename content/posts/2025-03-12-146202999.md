---
layout: post
title: "深入理解Linux网络随笔七容器网络虚拟化-Veth设备对"
date: 2025-03-12 13:38:21 +0800
description: "微服务架构中服务被拆分成多个独立的容器，docker网络虚拟化的核心技术为：Veth设备对、Network Namespace、Bridg。"
keywords: "veth device"
categories: ['深入理解Linux网络']
tags: ['网络', 'Linux']
artid: "146202999"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146202999
    alt: "深入理解Linux网络随笔七容器网络虚拟化-Veth设备对"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146202999
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146202999
cover: https://bing.ee123.net/img/rand?artid=146202999
image: https://bing.ee123.net/img/rand?artid=146202999
img: https://bing.ee123.net/img/rand?artid=146202999
---

# 深入理解Linux网络随笔（七）：容器网络虚拟化--Veth设备对
## 深入理解Linux网络随笔（七）：容器网络虚拟化
微服务架构中服务被拆分成多个独立的容器，docker网络虚拟化的核心技术为：Veth设备对、Network Namespace、Bridg。
### Veth设备对
`veth`设备是一种 \*\*成对\*\* 出现的虚拟网络接口，作用是 \*\*在 Linux 网络命名空间或不同网络栈之间建立一个虚拟的点对点连接\*\*
，实现数据通信。例如实现容器与宿主机间的通信、不同neths间传递流量。
如图所示：
虚拟网络设备并不会直接连接物理网络设备，而是一端连接到协议栈，另一端连接到另一个 veth 设备。从一对 veth 设备中发出的数据包会直接传送到另一个
veth 设备。每个 veth 设备都可以配置 IP 地址，并作为 路由的一个接口，可以进行IP层通信。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/5edc4a621c804c3fbd6d85905f0831d4.png)
特点：
（1）成对出现：创建时总是 两个设备\*成对出现，例如 `veth0` 和 `veth1`，它们之间类似于一条网络隧道
（2）工作方式：一端发送的流量会从另一端收到，就像网线直连一样
（3）不处理 L2/L3 转发：`veth` 设备 不会执行交换、路由\*等功能，只是简单地在两端传输数据包
##### 底层源码分析
![外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传](https://i-blog.csdnimg.cn/direct/fdddec9a8ad74401b3890caf6f660202.png)
veth设备的初始化通过函数veth\_init进行。
static \_\_init int veth\_init(void)
{
//注册veth\_link\_ops veth设备的操作方法
return rtnl\_link\_register(&veth\_link\_ops);
}
`veth\_link\_ops`中定义了veth设备的操作回调函数。
static struct rtnl\_link\_ops veth\_link\_ops = {
.kind = DRV\_NAME,//设备类型
.priv\_size = sizeof(struct veth\_priv),//私有数据大小
.setup = veth\_setup,//设备启动
.validate = veth\_validate,//检查netlink请求参数的合法性
.newlink = veth\_newlink,//处理新建的veth设备回调函数
.dellink = veth\_dellink,//处理删除的veth设备回调函数
.policy = veth\_policy,//netlink配置参数解析策略
.maxtype = VETH\_INFO\_MAX,// netlink 解析时允许的最大属性编号
.get\_link\_net = veth\_get\_link\_net,// 获取 veth 设备所在的网络命名空间
.get\_num\_tx\_queues = veth\_get\_num\_queues,// 获取设备的 TX 队列数
.get\_num\_rx\_queues = veth\_get\_num\_queues,// 获取设备的 RX 队列数
};
veth设备创建调用`veth\_newlink`函数。
static int veth\_newlink(struct net \*src\_net, struct net\_device \*dev,
struct nlattr \*tb[], struct nlattr \*data[],
struct netlink\_ext\_ack \*extack)
{
int err;
struct net\_device \*peer;
struct veth\_priv \*priv;
char ifname[IFNAMSIZ];
struct nlattr \*peer\_tb[IFLA\_MAX + 1], \*\*tbp;
unsigned char name\_assign\_type;
struct ifinfomsg \*ifmp;
struct net \*net;
.....
//创建peer设备
peer = rtnl\_create\_link(net, ifname, name\_assign\_type,
&veth\_link\_ops, tbp, extack);
if (IS\_ERR(peer)) {
put\_net(net);
return PTR\_ERR(peer);
}
....
//注册peer设备
err = register\_netdevice(peer);
...
//获取dev的私有数据priv
priv = netdev\_priv(dev);
//将dev的指针赋值给peer的私有数据peer->priv，建立连接，通过dev访问peer设备
rcu\_assign\_pointer(priv->peer, peer);
//初始化dev的TX、RX队列
err = veth\_init\_queues(dev, tb);
if (err)
goto err\_queues;
//获取 peer 设备的 priv 结构体
priv = netdev\_priv(peer);
//让 peer->priv->peer 指向 dev，建立 veth 设备对的双向连接
rcu\_assign\_pointer(priv->peer, dev);
//初始化peer设备的队列
err = veth\_init\_queues(peer, tb);
if (err)
goto err\_queues;
.....
}
启动veth设备，通过`veth\_netdev\_ops`操作表找到发送过程中的回调函数veth\_xmit。
static void veth\_setup(struct net\_device \*dev)
{
ether\_setup(dev);
dev->priv\_flags &= ~IFF\_TX\_SKB\_SHARING;
dev->priv\_flags |= IFF\_LIVE\_ADDR\_CHANGE;
dev->priv\_flags |= IFF\_NO\_QUEUE;
dev->priv\_flags |= IFF\_PHONY\_HEADROOM;
//veth操作列表
dev->netdev\_ops = &veth\_netdev\_ops;
dev->ethtool\_ops = &veth\_ethtool\_ops;
dev->features |= NETIF\_F\_LLTX;
dev->features |= VETH\_FEATURES;
dev->vlan\_features = dev->features &
~(NETIF\_F\_HW\_VLAN\_CTAG\_TX |
NETIF\_F\_HW\_VLAN\_STAG\_TX |
NETIF\_F\_HW\_VLAN\_CTAG\_RX |
NETIF\_F\_HW\_VLAN\_STAG\_RX);
dev->needs\_free\_netdev = true;
dev->priv\_destructor = veth\_dev\_free;
dev->pcpu\_stat\_type = NETDEV\_PCPU\_STAT\_TSTATS;
dev->max\_mtu = ETH\_MAX\_MTU;
dev->hw\_features = VETH\_FEATURES;
dev->hw\_enc\_features = VETH\_FEATURES;
dev->mpls\_features = NETIF\_F\_HW\_CSUM | NETIF\_F\_GSO\_SOFTWARE;
netif\_set\_tso\_max\_size(dev, GSO\_MAX\_SIZE);
}
static const struct net\_device\_ops veth\_netdev\_ops = {
.ndo\_init = veth\_dev\_init,
.ndo\_open = veth\_open,
.ndo\_stop = veth\_close,
.ndo\_start\_xmit = veth\_xmit,//veth发送函数
.ndo\_get\_stats64 = veth\_get\_stats64,
.ndo\_set\_rx\_mode = veth\_set\_multicast\_list,
.ndo\_set\_mac\_address = eth\_mac\_addr,
#ifdef CONFIG\_NET\_POLL\_CONTROLLER
.ndo\_poll\_controller = veth\_poll\_controller,
#endif
.ndo\_get\_iflink = veth\_get\_iflink,
.ndo\_fix\_features = veth\_fix\_features,
.ndo\_set\_features = veth\_set\_features,
.ndo\_features\_check = passthru\_features\_check,
.ndo\_set\_rx\_headroom = veth\_set\_rx\_headroom,
.ndo\_bpf = veth\_xdp,
.ndo\_xdp\_xmit = veth\_ndo\_xdp\_xmit,
.ndo\_get\_peer\_dev = veth\_peer\_dev,
};
##### 通信过程
数据包发送过程中到达网络设备层会进入`dev\_hard\_start\_xmit`函数，遍历链表上的所有skb包调用`xmit\_one`发送数据包。
//网络设备数据包发送路径（TX Path） 的关键部分，负责调用底层驱动的 ndo\_start\_xmit() 发送数据包
static int xmit\_one(struct sk\_buff \*skb, struct net\_device \*dev,
struct netdev\_queue \*txq, bool more)
{
unsigned int len;
int rc;
//监测是否有协议栈上层监听
if (dev\_nit\_active(dev))
//AF\_PACKET 套接字正在监听，发送数据包副本给监听进程
dev\_queue\_xmit\_nit(skb, dev);
//记录数据包长度
len = skb->len;
//触发tracepoint机制，记录数据包发送开始
trace\_net\_dev\_start\_xmit(skb, dev);
//调用底层驱动的ndo\_start\_xmit()方法发送数据包
rc = netdev\_start\_xmit(skb, dev, txq, more);
trace\_net\_dev\_xmit(skb, rc, dev, len);
return rc;
}
获取驱动设备回调函数集合ops，结构体`net\_device\_ops`，调用`\_\_netdev\_start\_xmit`发送数据包。
static inline netdev\_tx\_t netdev\_start\_xmit(struct sk\_buff \*skb, struct net\_device \*dev,
struct netdev\_queue \*txq, bool more)
{
const struct net\_device\_ops \*ops = dev->netdev\_ops;
netdev\_tx\_t rc;
//发送数据包
rc = \_\_netdev\_start\_xmit(ops, skb, dev, more);
if (rc == NETDEV\_TX\_OK)
txq\_trans\_update(txq);
return rc;
}
在这里会首先判断当前cpu发送队列是否还有数据待处理，然后调用驱动的ndo\_start\_xmit函数发送数据包，回调函数veth\_xmit，lo是loopback\_xmit。也就是在veth启动的时候注册的回调函数。
static inline netdev\_tx\_t \_\_netdev\_start\_xmit(const struct net\_device\_ops \*ops,
struct sk\_buff \*skb, struct net\_device \*dev,
bool more)
{
\_\_this\_cpu\_write(softnet\_data.xmit.more, more);
return ops->ndo\_start\_xmit(skb, dev);
}
在`veth\_xmit`核心是获取veth设备数据，将数据发送到对端设备。
static netdev\_tx\_t veth\_xmit(struct sk\_buff \*skb, struct net\_device \*dev)
{
struct veth\_priv \*rcv\_priv, \*priv = netdev\_priv(dev);
struct veth\_rq \*rq = NULL;
int ret = NETDEV\_TX\_OK;
struct net\_device \*rcv;
int length = skb->len;
bool use\_napi = false;
int rxq;
rcu\_read\_lock();
//获取veth设备
rcv = rcu\_dereference(priv->peer);
...
//获取rcv设备私有数据
rcv\_priv = netdev\_priv(rcv);
//获取skb队列索引
rxq = skb\_get\_queue\_mapping(skb);
if (rxq < rcv->real\_num\_rx\_queues) {
rq = &rcv\_priv->rq[rxq];
//rq绑定了napi，且数据包适合GRO，开启NAPI机制轮询数据包
use\_napi = rcu\_access\_pointer(rq->napi) &&
veth\_skb\_is\_eligible\_for\_gro(dev, rcv, skb);
}
skb\_tx\_timestamp(skb);
//尝试将skb转发到对端veth设备
if (likely(veth\_forward\_skb(rcv, skb, rq, use\_napi) == NET\_RX\_SUCCESS)) {
//未使用NAPI机制，更新统计信息
if (!use\_napi)
dev\_sw\_netstats\_tx\_add(dev, 1, length);
} else {
.....
}
`veth\_forward\_skb`会根据数据包选择不同路径，数据包转发到对端设备`dev\_forward\_skb`，对端开启XDP则调用`veth\_xdp\_rx`，普通数据包调用`netif\_rx`。
static int veth\_forward\_skb(struct net\_device \*dev, struct sk\_buff \*skb,
struct veth\_rq \*rq, bool xdp)
{
return \_\_dev\_forward\_skb(dev, skb) ?: xdp ?
veth\_xdp\_rx(rq, skb) :
\_\_netif\_rx(skb);
}
函数调用关系：`\_\_dev\_forward\_skb-->\_\_dev\_forward\_skb2-->\_\_\_\_dev\_forward\_skb`。
//处理dev设备的转发数据包
static int \_\_dev\_forward\_skb2(struct net\_device \*dev, struct sk\_buff \*skb,
bool check\_mtu)
{
//实际数据包转发处理
int ret = \_\_\_\_dev\_forward\_skb(dev, skb, check\_mtu);
if (likely(!ret)) {
//将skb所属设备设置成刚才取到的veth对端设备rcv
skb->protocol = eth\_type\_trans(skb, dev);
//修正skb校验和
skb\_postpull\_rcsum(skb, eth\_hdr(skb), ETH\_HLEN);
}
return ret;
}
在`eth\_type\_trans`设置完成会继续执行`\_\_netif\_rx`路径，函数调用逻辑`netif\_rx\_internal-->enqueue\_to\_backlog`，在这里获取每个CPU核心对应的softnet\_data数据结构，将skb添加到等待队列`input\_pkt\_queue`，在函数`\_\_test\_and\_set\_bit`检查`sd->backlog.state`是否已包含
`NAPI\_STATE\_SCHED`，`NAPI\_STATE\_SCHED`是NAPI轮询处理的一个 \*\*状态标志位\*\* ，\*\*防止同一个`NAPI`
任务被重复调度\*\*，未设置调用`napi\_schedule\_rps`触发NAPI调度，触发软中断 `NET\_RX\_SOFTIRQ`。
static int enqueue\_to\_backlog(struct sk\_buff \*skb, int cpu,
unsigned int \*qtail)
{
enum skb\_drop\_reason reason;
struct softnet\_data \*sd;
unsigned long flags;
unsigned int qlen;
//丢包原因：未指定
reason = SKB\_DROP\_REASON\_NOT\_SPECIFIED;
sd = &per\_cpu(softnet\_data, cpu);
if (!\_\_test\_and\_set\_bit(NAPI\_STATE\_SCHED, &sd->backlog.state))
napi\_schedule\_rps(sd);
.....
}
在这里根据是否开启RPS机制走不同的路径，\*\*\*\*RPS 是 Linux 内核的一种\*\*多核负载均衡机制\*\* ，\*\*将收到的数据包分配到多个 CPU
进行处理\*\* ，避免所有网络流量只由单个 CPU 处理，减少 \*\*CPU 瓶颈\*\*
，在这里通过rps\_ipi\_list将数据包从一个CPU转发到另外一个CPU上，提高多核环境下的负载均衡，减少CPU之间的竞争\*\*\*\*。如果没有开启RPS机制，数据包会在当前CPU软中断上下文中处理NAP任务。
static int napi\_schedule\_rps(struct softnet\_data \*sd)
{
struct softnet\_data \*mysd = this\_cpu\_ptr(&softnet\_data);
// RPS将接收的数据包调度到 不同的 CPU 进行处理
#ifdef CONFIG\_RPS
//sd不等于mysd，说明要在另一个 CPU 上执行 NAPI 任务，而不是本 CPU 处理
if (sd != mysd) {
//rps\_ipi\_list 负责存储要在其他 CPU 处理的 softnet\_data 队列，并通过 NET\_RX\_SOFTIRQ 触发软中断来完成调度。
sd->rps\_ipi\_next = mysd->rps\_ipi\_list;
mysd->rps\_ipi\_list = sd;
//出发软中断，处理softnet\_data队列的NAPI任务
\_\_raise\_softirq\_irqoff(NET\_RX\_SOFTIRQ);
return 1;
}
//本地CPU处理
#endif /\* CONFIG\_RPS \*/
//直接调度 mysd->backlog 设备的 NAPI 任务，并安排 net\_rx\_action() 来执行数据包处理。
\_\_napi\_schedule\_irqoff(&mysd->backlog);
return 0;
}
#### 实践操作
Linux 中创建 veth 设备对，设备名veth，指定的虚拟网卡类型为veth，创建的另一端设备名为veth1。
ip link add veth0 type veth peer name veth1
使用`ip link show`可以看到`veth0` 和 `veth1` 设备已创建，但还未启用。
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9004d6cec9e940f2846e8da955167e10.png)
veth设备需要配置IP地址才能进行通信，为`veth0` 和 `veth1` 设备配置ip。
sudo ip addr add 192.168.1.1/24 dev veth0
sudo ip addr add 192.168.1.2/24 dev veth1
启动设备
sudo ip link set veth1 up
sudo ip link set veth0 up
使用`ip link show`可以看到`veth0` 和 `veth1` 设备状态已变成开启
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b347720bfa954091b2c8f3645f535da1.png)
为了使veth设备之间能够顺利通信，需要关闭反向路径过滤（rp\_filter）并设置允许接收本机数据包。root角色下修改系统配置如下：
![!](https://i-blog.csdnimg.cn/direct/cb97242608124f7b897c0accc3ab183f.png)
ping测试`veth0` 和 `veth1` 设备间通信
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3dd11655c6d342bc8726fb026d0bac08.png)