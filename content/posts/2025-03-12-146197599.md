---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f33383133393235302f:61727469636c652f64657461696c732f313436313937353939"
layout: post
title: "大语言模型-1.3-GPTDeepSeek模型介绍"
date: 2025-03-12 11:13:38 +08:00
description: "本博客内容是《大语言模型》一书的读书笔记，本文主要记录datawhale的活动学习笔记，本部分主要介绍GPT和DeepSeek的进展。"
keywords: "大语言模型-1.3-GPT、DeepSeek模型介绍"
categories: ['机器学习2025', '大模型Llm']
tags: ['语言模型', '人工智能', 'Gpt', 'Datawhale']
artid: "146197599"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146197599
    alt: "大语言模型-1.3-GPTDeepSeek模型介绍"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146197599
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146197599
cover: https://bing.ee123.net/img/rand?artid=146197599
image: https://bing.ee123.net/img/rand?artid=146197599
img: https://bing.ee123.net/img/rand?artid=146197599
---

# 大语言模型-1.3-GPT、DeepSeek模型介绍

### 简介

本博客内容是《大语言模型》一书的读书笔记，该书是中国人民大学高瓴人工智能学院赵鑫教授团队出品，覆盖大语言模型训练与使用的全流程，从预训练到微调与对齐，从使用技术到评测应用，帮助学员全面掌握大语言模型的核心技术。并且，课程内容基于大量的代码实战与讲解，通过实际项目与案例，学员能将理论知识应用于真实场景，提升解决实际问题的能力。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8462d74a8d764cc7a56c909eade4c15a.png)

本文主要记录datawhale的活动学习笔记，
[可点击活动连接](https://www.datawhale.cn/activity/150)

### 参考

参考
  
[【大模型】GPT: Improving Language Understanding by Generative Pre-Training](https://www.zhihu.com/tardis/bd/art/669984257?source_id=1001)
  
[AI 世界生存手册（二）：从LR到DeepSeek，模型慢慢变大了，也变强了](https://news.qq.com/rain/a/20250307A0200M00)

[#41 AI-002-十分钟理解ChatGPT的技术逻辑及演进(前世、今生）](https://juejin.cn/post/7175753434761527356#heading-4)

[十分钟理解Transformer](https://zhuanlan.zhihu.com/p/82312421)

### 1.3.1GPT 系列模型成体系推进

> 2017年，谷歌提出Transformer
>   
> 2018年，OpenAI提出GPT（1亿+参数）
>   
> 2019年，GPT-2（15亿参数）
>   
> 2020年，GPT-3（1750亿参数）
>   
> 2021年，CodeX（基于GPT-3，代码预训练）
>   
> 2021年，WebGPT（搜索能力）
>   
> 2022年2月，InstructGPT（人类对齐）
>   
> 2022年11月，ChatGPT（对话能力）
>   
> 2023年3月，GPT-4（推理能力、多模态能力）
>   
> 2024年9月，o1（深度思考能力提升）
>   
> 2025年1月，o3（深度思考能力进一步增强）
>   
> GPT系列模型从18年开始系统迭代，对于大模型发展起到了深远影响

GPT从开始至今，其发展历程如下：

> 2017年6月，Google发布论文《Attention is all you need》，首次提出Transformer模型，成为GPT发展的基础。 论文地址：
> <https://arxiv.org/abs/1706.03762>
>   
> 2018年6月,OpenAI 发布论文《Improving Language Understanding by Generative Pre-Training》(通过生成式预训练提升语言理解能力)，首次提出GPT模型(Generative Pre-Training)。论文地址：
> [paperswithcode.com/method/gpt](https://paperswithcode.com/method/gpt)
> 。
>   
> 2019年2月，OpenAI 发布论文《Language Models are Unsupervised Multitask Learners》（语言模型应该是一个无监督多任务学习者），提出GPT-2模型。论文地址: paperswithcode.com/method/gpt-…
>   
> 2020年5月，OpenAI 发布论文《Language Models are Few-Shot Learners》(语言模型应该是一个少量样本(few-shot)学习者，提出GPT-3模型。论文地址：
> <https://paperswithcode.com/method/gpt-2>
>   
> 2022年2月底，OpenAI 发布论文《Training language models to follow instructions with human feedback》（使用人类反馈指令流来训练语言模型），公布 Instruction GPT模型。论文地址：
> <https://arxiv.org/abs/2203.02155>
>   
> 2022年11月30日，OpenAI推出ChatGPT模型，并提供试用，全网火爆。见：
> [AI-001-火爆全网的聊天机器人ChatGPT能做什么](https://mp.weixin.qq.com/s?__biz=Mzg5MDU2MzM2Mw==&mid=2247484868&idx=1&sn=14b036f1ef366f2ee04ce3d560bfb693&chksm=cfdbfb88f8ac729e9432dedf4c232114b0c1d0e06d14dfad1dca4d12ee01172174caf0011597&token=494872941&lang=zh_CN#rd)

#### GPT 系列模型发展历程

> ➢ 小模型：GPT-1，GPT-2
>   
> ➢ 大模型：GPT-3，CodeX，GPT-3.5，GPT-4
>   
> ➢ 推理大模型：o-series
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/828f03bed3a8444e954bcbc135f460e0.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/9e9e8ed3d38f44bcab78d66e803a408a.png)

#### GPT-1（1.1亿参数）

当时NLP的问题

> 此时训练一个 NLP 模型和我们之前做的推荐类似，针对某个任务，首先搞一些样本，然后对模型进行有监督训练。问题出在题面上。
>   
> 1.样本怎么来，大量的高质量的标注不太容易获得。
>   
> 2.模型训练的任务是固定的，很难学到泛化能力，没法复用到做其他任务。
>   
> 这样训练出来的模型被困在了一个特定的领域，离我们想要的 AGI（人工通用智能）有点远。

GPT-1采用的架构

> ➢ Decode-only Transformer架构
>   
> ➢ 预训练后针对特定任务微调
>   
> entailment术语翻译为“蕴涵”
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2af7c0156e3b43d7a0d3d69759825b55.png)
> 1.用了4.6GB的BookCorpus数据集（该数据集主要是小说，openai 为了验证技术可行性，特意选了未出版的 7000 本书），无监督训练一个预训练模型，即generative pre-training，GPT 名字的由来。
>   
> 2.对于子任务，用有标签的小的数据集训练一个微调模型，discriminative fine-tuning。
>   
> **微调方式具体来说，可见上图右图部分。**
>   
> 对于每个任务，输入会被构造成一个连续的 token 序列。分类任务，会将输入文本拼接成一个序列，并在开头添加一个特殊token-start，在结尾增加 extract然后经过模型+线性层后输出结果，对于相似度的文本比较有趣，比如看 A 和 B 是否相似，那么就组成个序列分别为 AB 和 BA，其输入模型后，最终通过softmax 判断，是否相似，是个二分类问题。第四个问答其实是一个多分类问题。
>   
> 这四个任务有一个共性，就是我们只需要对输入做定制化，输出做一些定制，但是中间的 transformer 模型不会去动它。
>   
> 左图：GPT是一个transformer decoder-only的结构， MHA +add&norm 的 Block 其用了 12 层，参数量 0.11B，对，此时它还很小。另外输入的token 用了word2vec做了 embedding 表征。

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b156ad169bf341dd88ff53be82edacda.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/06a910f762744e69b266c5f94e86d9ab.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ef3fcc2aae984db1bdf7816c9492a93a.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/de375120fef24a4c9d74e8ba79e42fda.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/bed9d55620484cdb91892d0a7f5b7cde.png)
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3b676540e51548c3b291640b84ff46b3.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1740805a36584a22b96edc3f0acfea11.png)

#### GPT-2 （15亿参数）

➢ 将任务形式统一为单词预测
  
➢ Pr (output | input, task)
  
➢ 预训练与下游任务一致
  
➢ 使用提示进行无监督任务求解
  
➢ 初步尝试了规模扩展
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/812cf300f44b4c349c142bae734017f5.png)

#### GPT-3(1750亿参数)

➢ 模型规模达到1750亿参数
  
➢ 涌现出上下文学习能力
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7fbfa4691eed40e68fb7ac8b9f24676a.png)

#### CodeX

➢ 代码数据训练
  
➢ 推理与代码合成能力

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b741c31b156b40e7bc9c5b5f252f3c42.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/13524c0541a64dcf87f6772aded33f2f.png)

#### WebGPT

➢ 大语言模型使用浏览器

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b5bf1ee426074273be11d70f3aea9eeb.png)
>   
> WebGPT: Browser-assisted question-answering with human feedback, Arxiv 2021

#### InstructGPT

➢ 大语言模型与人类价值观对齐
  
➢ 提出RLHF算法

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/eee74fef46b94cc983b6783b77bf9f7b.png)
>   
> Training language models to follow instructions with human feedback, NIPS 2022

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/2bb8f26af5484d82a4b2746784a6e111.webp)

> 1)、对GPT-3进行fine-tuning(监督微调)。
>   
> 2)、再训练一个Reward Model(奖励模型，RM)
>   
> 3)、最后通过增强学习优化SFT
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b0b538dec12e47a798cb225efae1c1bc.webp)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/165476709f7d4fd68aa967211d014845.webp)
>   
> 值得注意的是，第2步、第3步是完全可以迭代、循环多次进行的。

##### Instruction GPT的训练规模

> 基础数据规模同GPT-3 ，只是在其基础上增加了3个步骤（监督微调SFT、奖励模型训练Reward Model，增强学习优化RPO)。
>   
> 下图中labeler是指OpenAI雇佣或有相关关系的标注人员(labler)。
>   
> 而customer则是指GPT-3 API的调用用户（即其他一些机器学习研究者、程序员等）。
>   
> 本次ChatGPT上线后据说有百万以上的用户，我们每个人都是其customer，所以可以预见，未来GPT-4发布时，其customer规模至少是百万起。
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b95918e02b824af58a830ac849325931.webp)

#### ChatGPT

➢ 基于 InstructGPT 相似技术开发，面向对话进行优化

> ChatGPT和InstructionGPT本质上是同一代际的，仅仅是在InstructionGPT的基础上，增加了Chat功能，同时开放到公众测试训练，以便产生更多有效标注数据。
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/07479876ebce407ea491857d00d2e11b.webp)

#### GPT-4

➢ 推理能力显著提升，建立可预测的训练框架
  
➢ 可支持多模态信息的大语言模型

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0ab001ea52f4421b858de911f74cf38d.png)
>   
> GPT-4 Technical Report, Arxiv 2023

#### GPT-4o

➢ 原生多模态模型，综合模态能力显著提升
  
➢ 支持统一处理和输出文本、音频、图片、视频信息
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0573b064a1ea499ab1bcac0bd76a993b.png)

#### o系列模型

➢ 推理任务上能力大幅提升
  
➢ 长思维链推理能力

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ef1aadf5fba14cea918ee4ea12f3b760.png)

#### o-series

➢ 类似人类的“慢思考”过程

> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/dbba9ec635a14cf59b8ade2c38e3a67a.png)

### 1.3.2DeepSeek 系列模型的技术演变

> DeepSeek系列模型发展历程
>   
> ➢ 训练框架：HAI-LLM
>   
> ➢ 语言大模型：DeepSeek LLM/V2/V3、Coder/Coder-V2、Math
>   
> ➢ 多模态大模型：DeepSeek-VL
>   
> ➢ 推理大模型：DeepSeek-R1
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/ea0e5a870bde4b4aaee1e611dbb160fd.png)

DeepSeek 实现了较好的训练框架与数据准备

> ➢ 训练框架 HAI-LLM（发布于2023年6月）
>   
> ➢ 大规模深度学习训练框架，支持多种并行策略
>   
> ➢ 三代主力模型均基于该框架训练完成
>   
> ➢ 数据采集
>   
> ➢ V1和Math的报告表明清洗了大规模的Common Crawl，具备超大规模数据处理能力
>   
> ➢ Coder的技术报告表明收集了大量的代码数据
>   
> ➢ Math的技术报告表明清洗收集了大量的数学数据
>   
> ➢ VL的技术报告表明清洗收集了大量多模态、图片数据

DeepSeek 进行了重要的网络架构、训练算法、性能优化探索

> ➢ V1 探索了scaling law分析（考虑了数据质量影响），用于预估超参数性能
>   
> ➢ V2 提出了MLA高效注意力机制，提升推理性能
>   
> ➢ V2、V3都针对MoE架构提出了相关稳定性训练策略
>   
> ➢ V3 使用了MTP（多token预测）训练
>   
> ➢ Math 提出了PPO的改进算法 GRPO
>   
> ➢ V3详细介绍Infrastructure的搭建方法，并提出了高效 FP8 训练方法

#### DeepSeek-V3

> ➢ 671B参数（37B激活），14.8T训练数据
>   
> ➢ 基于V2的MoE架构，引入了MTP和新的复杂均衡损失
>   
> ➢ 对于训练效率进行了极致优化，共使用 2.788M H800 GPU时
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/08c932f6b05241498faaa5bb0022ad94.png)
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/1e14c95b22bb40b39d0c26382eca8a34.png)

#### DeepSeek-R1

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e66e42a741c04fb98846d742112048bf.png)
  
DeepSeek-V3和DeepSeek-R1均达到了同期闭源模型的最好效果

> ➢ 开源模型实现了重要突破
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/87ab4085234f48be87521150a17254ba.png)

#### 为什么 DeepSeek 会引起世界关注

> ➢ 打破了OpenAI 闭源产品的领先时效性
>   
> ➢ 国内追赶GPT-4的时间很长，然而复现o1模型的时间大大缩短
>   
> ➢ 达到了与OpenAI现有API性能可比的水平
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/0fd37e8969ee42fd8cbf6fd60e07b188.png)
>   
> Large Language Model, 2025 (Book under progress)

#### 为什么 DeepSeek 会引起世界关注

> ➢ 中国具备实现世界最前沿大模型的核心技术
>   
> ➢ 模型开源、技术开放
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/8918141a66f24a469a038b3eda0a4e93.png)

参考:
[【大模型】GPT: Improving Language Understanding by Generative Pre-Training](https://www.zhihu.com/tardis/bd/art/669984257?source_id=1001)
  
[AI 世界生存手册（二）：从LR到DeepSeek，模型慢慢变大了，也变强了](https://news.qq.com/rain/a/20250307A0200M00)