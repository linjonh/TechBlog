---
layout: post
title: 架构设计-自动化运维之架构设计六要点
date: 2024-12-30 09:46:14 +08:00
categories: ['未分类']
tags: ['无标签']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=78777819
    alt: 架构设计-自动化运维之架构设计六要点
artid: 78777819
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=78777819
featuredImagePreview: https://bing.ee123.net/img/rand?artid=78777819
---

# 架构设计 - 自动化运维之架构设计六要点

**![](https://img-blog.csdnimg.cn/img_convert/deeb7f25c70dc78dd3eca4bbe3e7b8b5.png;wx_lazy=1 "跳跃猫引导关注")**

运维自动化是我们所渴望获得的，但是我们在一味强调自动化能力时，却忽略了影响自动化落地的一个关键因素。那便是跟运维朝夕相处，让人又爱又恨的业务架构。

因为业务架构是决定运维效率和质量的关键因素之一，所以我想跟大家一起聊一下怎么样的架构设计是对运维友好的。结合这些年在腾讯遇到的业务架构和做运维规划时对业务非功能规范的思考，我们可以把面向运维的架构设计分成六大设计要点。

![](https://img-blog.csdnimg.cn/img_convert/53f4ec7c1d96bef7632cd5d8581282c6.png;wxfrom=5&wx_lazy=1)

**要点一：架构独立**

### 

任何架构的产生都是为了满足特定的业务诉求，如果我们在满足业务要求的同时，能够兼顾运维对架构管理的非功能性要求。那么我们有理由认为这样的架构是对运维友好的。

![](https://img-blog.csdnimg.cn/img_convert/997e0180e5a6340c0dbf72611ae862cd.png;wxfrom=5&wx_lazy=1)

站在运维的角度，所诉求的架构独立包含四个方面：独立部署，独立测试，组件化和技术解耦。

**① 独立部署**

指的是一份源代码，可以按照便于运维的管理要求去部署、升级、伸缩等，可通过配置来区分地域分布。服务间相互调用通过接口请求实现，部署独立性也是运维独立性的前提。

**② 独立测试**

运维能够通过一些便捷的测试用例或者工具，验证该业务架构或服务的可用性。具备该能力的业务架构或服务让运维具备了独立上线的能力，而不需要每次发布或变更都需要开发或测试人员的参与。

**③ 组件规范**

指的是在同一个公司内对相关的技术能有很好的框架支持，从而避免不同的开发团队使用不同的技术栈或者组件，造成公司内部的技术架构失控。

这种做法能够限制运维对象的无序增加，让运维对生产环境始终保持着掌控。同时也能够让运维保持更多的精力投入，来围绕着标准组件做更多的效率与质量的建设工作。

**④ 技术解耦**

指的是降低服务和服务之间相互依赖的关系，也包含了降低代码对配置文件的依赖。这也是实现微服务的基础，实现独立部署、独立测试、组件化的基础。

**要点二：部署友好**

### 

DevOps 中有大量的篇幅讲述持续交付的技术实践，希望从端到端打通开发、测试、运维的所有技术环节，以实现快速部署和交付价值的目标。可见，部署是运维日常工作很重要的组成部分，是属于计划内的工作，重复度高，必须提升效率。

![](https://i-blog.csdnimg.cn/blog_migrate/74b514256e343b07164471cf3a6836d0.jpeg)

实现高效可靠的部署能力，要做好全局规划，以保证部署以及运营阶段的全方位运维掌控。有五个纬度的内容是与部署友好相关的：

**① CMDB配置**

在每次部署操作前，运维需要清晰的掌握该应用与架构、与业务的关系，为了更好的全局理解和评估工作量和潜在风险。

在织云自动化运维平台中，我们习惯于将业务关系、集群管理、运营状态、重要级别、架构层等配置信息作为运维的管理对象纳管于CMDB配置管理数据库中。这种管理办法的好处很明显，集中存储运维对象的配置信息，对日后涉及的运维操作、监控和告警等自动化能力建设，将提供大量的配置数据支撑和决策辅助的功效。

**② 环境配置**

在运维标准化程度不高的企业中，阻碍部署交付效率的原罪之一便是环境配置，这也是容器化技术主要希望解决的运维痛点之一。

腾讯的运维实践中，对开发、测试、生产三大主要环境的标准化管理，通过枚举纳管与环境相关的资源集合与运维操作，结合自动初始化工具以实现标准环境管理的落地。

**③ 依赖管理**

解决应用软件对库、运营环境等依赖关系的管理。在织云实践经验中，我们利用包管理，将依赖的库文件或环境的配置，通过整体打包和前后置执行脚本的方案，解决应用软件在不同环境部署的难题。业界还有更轻量的容器化交付方法，也是不错的选择。

**④ 部署方式**

持续交付原则提到要打造可靠可重复的交付流水线，对应用软件的部署操作，我们也强烈按此目标来规划。业界有很多案例可以参考，如Docker的Build、Ship、Run，如织云的通过配置描述、标准化流程的一键部署等等。

**⑤ 发布自测**

发布自测包含两部分：

> 1. 应用的轻量级测试；
> 2. 发布/变更内容的校对。

建设这两种能力以应对不同的运维场景需求，如在增量发布时，使用发布内容的校对能力，运维人员可快速的获取变更文件md5，或对相关的进程和端口的配置信息进行检查比对，确保每次发布变更的可靠。

同理，轻量级测试则是满足发布时对服务可用性检测的需求，此步骤可以检测服务的连通性，也可以跑些主干的测试用例。

**⑥ 灰度上线**

在《日常运维三十六计》中有这么一句话：对不可逆的删除或修改操作，尽量延迟或慢速执行。这便是灰度的思想，无论是从用户、时间、服务器等纬度的灰度上线，都是希望尽量降低上线操作的风险，业务架构支持灰度发布的能力，让应用部署过程的风险降低，对运维更友好。

**要点三：可运维性**

### 

运维脑海中最理想的微服务架构，首当其冲的肯定是可运维性强的那类。不具可运维性的应用或架构，对运维团队带来的不仅仅是黑锅，还有对他们职业发展的深深的伤害，因为维护一个没有可运维性的架构，简直就是在浪费运维人员的生命。

![](https://i-blog.csdnimg.cn/blog_migrate/1212be20fe9bd3188eb061130ba5a921.jpeg)

可运维性按操作规范和管理规范可以被归纳为以下七点：

**① 配置管理**

在微服务架构管理中，我们提议将应用的二进制文件与配置分离管理，以便于实现独立部署的目的。

被分离出来的应用配置，有三种管理办法：

> 1. 文件模式；
> 2. 配置项模式；
> 3. 分布式配置中心模式。

限于篇幅不就以上三种方式的优劣展开讨论。不同的企业可选用最适用的配置管理办法，关键是要求各业务使用一致的方案，运维便可以有针对性的建设工具和系统来做好配置管理。

**② 版本管理**

DevOps持续交付八大原则之一“把所有的东西都纳入版本控制”。就运维对象而言，想要管理好它，就必须能够清晰的描述它。

和源代码管理的要求类似，运维也需要对日常操作的对象，如包、配置、脚本等都进行脚本化管理，以备在运维系统在完成自动化操作时，能够准确无误的选定被操作的对象和版本。

**③ 标准操作**

运维日常有大量重复度高的工作需要被执行，从精益思想的视角看，这里存在极大的浪费：学习成本、无价值操作、重复建设的脚本/工具、人肉执行的风险等等。

倘若能在企业内形成统一的运维操作规范，如文件传输、远程执行、应用启动停止等等操作都被规范化、集中化、一键化的操作，运维的效率和质量将得以极大的提升。

**④ 进程管理**

包括应用安装路径、目录结构、规范进程名、规范端口号、启停方式、监控方案等等，被收纳在进程管理的范畴。做好进程管理的全局规划，能够极大的提升自动化运维程度，减少计划外任务的发生。

**⑤ 空间管理**

做好磁盘空间使用的管理，是为了保证业务数据的有序存放，也是降低计划外任务发生的有效手段。

要求提前做好的规划：备份策略、存储方案、容量预警、清理策略等，辅以行之有效的工具，让这些任务不再困扰运维。

**⑥ 日志管理**

日志规范的推行和贯彻需要研发密切配合，在实践中得出的经验，运维理想中的日志规范要包含这些要求：

* 业务数据与日志分离
* 日志与业务逻辑解耦
* 日志格式统一
* 返回码及注释清晰
* 可获取业务指标（请求量/成功率/延时）
* 定义关键事件
* 输出级别
* 管理方案（存放时长、压缩备份等）

当具体上述条件的日志规范得以落地，开发、运维和业务都能相应的获得较好的监控分析能力。

**⑦ 集中管控**

运维的工作先天就容易被切割成不同的部分，发布变更、监控分析、故障处理、项目支持、多云管理等等，我们诉求一站式的运维管理平台，使得所有的工作信息能够衔接起来和传承经验，杜绝因为信息孤岛或人工传递信息而造成的运营风险，提升整体运维管控的效率和质量。

**要点四：容错容灾**

### 

在腾讯技术运营（运维）的四大职责：质量、效率、成本、安全。质量是首要保障的阵地，转换成架构的视角，运维眼中理想的高可用架构架构设计应该包含以下几点：

![](https://i-blog.csdnimg.cn/blog_migrate/88244a2f40422b00ba7a8d6eb847c73a.jpeg)

**① 负载均衡**

无论是软件或硬件的负责均衡的方案，从运维的角度出发，我们总希望业务架构是无状态的，路由寻址是智能化的，集群容错是自动实现的。

在腾讯多年的路由软件实践中，软件的负载均衡方案被广泛应用，为业务架构实现高可用立下汗马功劳。

**② 可调度性**

在移动互联网盛行的年代，可调度性是容灾容错的一项极其重要的运维手段。在业务遭遇无法立刻解决的故障时，将用户或服务调离异常区域，是海量运营实践中屡试不爽的技巧，也是腾讯QQ和微信保障平台业务质量的核心运维能力之一。

结合域名、VIP、接入网关等技术，让架构支持调度的能力，丰富运维管理手段，有能力更从容的应对各种故障场景。

**③ 异地多活**

异地多活是数据高可用的诉求，是可调度性的前提。针对不同的业务场景，技术实现的手段不限。

**④ 主从切换**

在数据库的高可用方案中，主从切换是最常见的容灾容错方案。通过在业务逻辑中实现读写分离，再结合智能路由选择实现无人职守的主从切换自动化，无疑是架构设计对DBA最好的馈赠。

**⑤ 柔性可用**

“先扛住再优化”是腾讯海量运营思想之一，也为我们在做业务架构的高可用设计点明了方向。

如何在业务量突增的情况下，最大程度的保障业务可用？是做架构规划和设计时不可回避的问题。巧妙的设置柔性开关，或者在架构中内置自动拒绝超额请求的逻辑，能够在关键时刻保证后端服务不雪崩，确保业务架构的高可用。

**要点五：质量监控**

### 

保障和提高业务质量是运维努力追逐的目标，而监控能力是我们实现目标的重要技术手段。运维希望架构为质量监控提供便利和数据支持，要求实现以下几点：

![](https://i-blog.csdnimg.cn/blog_migrate/e8d0a72b5d4b6797ce5b2250204dc54e.jpeg)

**① 指标度量**

每个架构都必须能被指标度量，同时，我们希望的是最好只有唯一的指标度量。对于业务日趋完善的立体化监控，监控指标的数量随之会成倍增长。因此，架构的指标度量，我们希望的是最好只有唯一的指标度量。

**② 基础监控**

指的是网络、专线、主机、系统等低层次的指标能力，这类监控点大多属于非侵入式，很容易实现数据的采集。

在自动化运维能力健全的企业，基础监控产生的告警数据绝大部分会被收敛掉。同时，这部分监控数据将为高层次的业务监控提供数据支撑和决策依据，或者被包装成更贴近上层应用场景的业务监控数据使用，如容量、多维指标等。

**③ 组件监控**

腾讯习惯把开发框架、路由服务、中间件等都统称为组件，这类监控介于基础监控和业务监控之间，运维常寄希望于在组件中内嵌监控逻辑，通过组件的推广，让组件监控的覆盖度提高，获取数据的成本属中等。如利用路由组件的监控，运维可以获得每个路由服务的请求量、延时等状态和质量指标。

**④ 业务监控**

业务监控的实现方法分主动和被动的监控，即可侵入式实现，又能以旁路的方式达到目的。这类监控方案要求开发的配合，与编码和架构相关。

通常业务监控的指标都能归纳为请求量、成功率、延时3种指标。实现手段很多，有日志监控、流数据监控、波测等等，业务监控属于高层次的监控，往往能直接反馈业务问题，但倘若要深入分析出问题的根源，就必须结合必要的运维监控管理规范，如返回码定义、日志协议等。需要业务架构在设计时，前置考虑运维监控管理的诉求，全局规划好的范畴。

**⑤ 全链路监控**

基础、组件、业务的监控手段更多的是聚焦于点的监控，在分布式架构的业务场景中，要做好监控，我们必须要考虑到服务请求链路的监控。

基于唯一的交易ID或RPC的调用关系，通过技术手段还原调用关系链，再通过模型或事件触发监控告警，来反馈服务链路的状态和质量。该监控手段属于监控的高阶应用，同样需要业务架构规划时做好前置规划和代码埋点。

**⑥ 质量考核**

任何监控能力的推进，质量的优化，都需要有管理的闭环，考核是一个不错的手段，从监控覆盖率、指标全面性、事件管理机制到报表考核打分，运维和开发可以携手打造一个持续反馈的质量管理闭环，让业务架构能够不断进化提升。

**要点六：性能成本**

### 

在腾讯，所有的技术运营人员都肩负着一个重要的职能，就是要确保业务运营成本的合理。为此，我们必须对应用吞吐性能、业务容量规划和运营成本都要有相应的管理办法。

![](https://i-blog.csdnimg.cn/blog_migrate/4e1dfa5aeda74f973d4284562522c4d2.jpeg)

**① 吞吐性能**

DevOps持续交付方法论中，在测试阶段进行的非功能需求测试，其中很重要一点便是对架构吞吐性能的压测，并以此确保应用上线后业务容量的健康。

在腾讯的实践中，不仅限于测试阶段会做性能压测，我们会结合路由组件的功能，对业务模块、业务SET进行真实请求的压测，以此建立业务容量模型的基准。也从侧面提供数据论证该业务架构的吞吐性能是否达到成本考核的要求，利用不同业务间性能数据的对比，来推动架构性能的不断提高。

**② 容量规划**

英文capacity一词可以翻译成：应用性能、服务容量、业务总请求量，运维的容量规划是指在应用性能达标的前提下，基于业务总请求量的合理的服务容量规划。对容量规划与优化的手段，可参考“运维如何为公司节省一个亿？”。

**③ 运营成本**

减少运营成本，是为公司减少现金流的投入，对企业的价值丝毫不弱于质量与效率的提升。

腾讯以社交、UGC、云计算、游戏、视频等富媒体业务为主，每年消耗在带宽、设备等运营成本的金额十分巨大。运维想要优化运营成本，常常会涉及到产品功能和业务架构的优化。因此，运维理想的业务架构设计需要有足够的成本意识。

**小结**

### 

本文纯属个人以运维视角整理的对微服务架构设计的一些愚见，要实现运维价值最大化，要确保业务质量、效率、成本的全面提高，业务架构这块硬骨头是不得不啃的。

运维人需要有架构意识，能站在不同角度对业务架构提出建议或需求，这也是DevOps 精神所提倡的，开发和运维联手，持续优化出最好的业务架构。

![](https://i-blog.csdnimg.cn/blog_migrate/7208ebf2cf446127e5f2a118bfcf08fb.jpeg)

转自：『高效运维』公众号

![](https://i-blog.csdnimg.cn/blog_migrate/77ee6b26f01b66fd6a74cf13b72d0192.jpeg "金属质感分割线")

**相关阅读：**

[高并发IM系统架构优化实践](http://mp.weixin.qq.com/s?__biz=MjM5MDAxOTk2MQ==&mid=2650272968&idx=1&sn=c2b024d792735b66566425ea3ebe0e1b&chksm=be486cde893fe5c86b24abbcdcace65f76c8f8a30671ce6e8e4fedc6dfc8267934a107265f92&scene=21#wechat_redirect)

[从AT&T到青海移动的多租户数据整合实践](http://mp.weixin.qq.com/s?__biz=MjM5MDAxOTk2MQ==&mid=2650276545&idx=1&sn=1d5a5343b1aa0bc4aeaaa4db611fcbf3&chksm=be479ad7893013c1d58054f783db7eb27371450908c0f802af6ead21e9b483443b9a03e3b5a6&scene=21#wechat_redirect)

[DBIM RAC Share Nothing架构的挑战和解决方案](http://mp.weixin.qq.com/s?__biz=MjM5MDAxOTk2MQ==&mid=2650270606&idx=1&sn=526b163db027f31f75e1fdc0164d90dd&scene=21#wechat_redirect)

[Evernote 基于Google 云平台的架构设计和技术转型](http://mp.weixin.qq.com/s?__biz=MjM5MDAxOTk2MQ==&mid=2650273553&idx=1&sn=c7f00d2fd16c55edcb6eb7b0fb15e992&chksm=be486f07893fe6119579b339558bff44d75106f4eeefbe6c42a6e489033cfceacd7ba23931d7&scene=21#wechat_redirect)

资源下载

关注公众号：数据和云（OraNews）回复关键字获取

**‘2017DTC’**

，2017DTC大会PPT

**‘DBALIFE’**

，“DBA的一天”海报

**‘DBA04’**

，DBA手记4经典篇章电子书

**‘RACV1’,**

RAC系列课程视频及ppt

**‘122ARCH’**

，Oracle 12.2体系结构图

**‘2017OOW’**

，Oracle OpenWorld资料

**‘PRELECTION’**

，大讲堂讲师课程资料

![](https://i-blog.csdnimg.cn/blog_migrate/358f5628316a0d0b61637e6459b961fb.jpeg)