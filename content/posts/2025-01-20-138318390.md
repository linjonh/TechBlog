---
layout: post
title: 盘点十大开源大模型
date: 2025-01-20 16:10:00 +0800
categories: ['开源']
tags: ['开源', '学习', '人工智能']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=138318390
    alt: 盘点十大开源大模型
artid: 138318390
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=138318390
featuredImagePreview: https://bing.ee123.net/img/rand?artid=138318390
---
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     盘点十大开源大模型
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     大模型时代，开源与闭源模型不断涌现，构成了大模型领域的双重引擎。开源大模型为AI领域注入了新的活力，基于开源大模型的应用如雨后春笋般出现，同时也为研究者和开发者提供了更广阔的创新空间。
    </p>
    <p>
     在此，
     <strong>
      我们梳理了当前开源大模型的部分论文和开源代码
     </strong>
     ，让我们共同探寻这些令人振奋的进展。
    </p>
    <h3>
     <a id="Llama_2_6">
     </a>
     Llama 2
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：Llama 2: Open Foundation and Fine-Tuned Chat Models
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://github.com/meta-llama/llama
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        Meta AI
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      Llama 2 是一系列预训练和微调的大型语言模型，参数规模从70亿到700亿不等。
     </strong>
     其中，特别推出了经过优化用于对话场景的微调模型，称为Llama 2-Chat。
    </p>
    <p>
     这些模型在测试的大多数基准测试中均胜过开源对话模型，在用户评估中获得了良好的帮助性和安全性评分，有可能成为封闭源模型的合适替代品。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/ffc0973b314359075e813b992a43fe9f.png"/>
    </p>
    <h3>
     <a id="CodeGeeX_22">
     </a>
     CodeGeeX
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        **论文：**CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://github.com/THUDM/CodeGeeX
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        清华大学、智谱AI、华为
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      CodeGeeX，是一个拥有130亿参数的多语言代码生成模型。
     </strong>
     它的优势在于能够生成语法和功能正确的代码，极大地提高了程序员的编码效率，并使我们对人工智能的普适性更加接近。
    </p>
    <p>
     CodeGeeX在2022年6月基于230亿token的23种编程语言进行了预训练。大量实验证明，CodeGeeX在HumanEval-X上的表现优于规模相似的多语言代码模型，无论是在代码生成还是翻译任务上。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/663964a290a711853e1ee2dec61457dd.png"/>
    </p>
    <h3>
     <a id="MiniGPT4_38">
     </a>
     MiniGPT-4
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        **论文：**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://minigpt-4.github.io/
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        King Abdullah University of Science and Technology
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      MiniGPT-4结合了一个冻结的视觉编码器和一个冻结的大型语言模型Vicuna，仅使用一个投影层。
     </strong>
     MiniGPT-4展现出类似于GPT-4的多种能力，比如生成详细的图像描述，从手写草稿创建网站等。
    </p>
    <p>
     此外，MiniGPT-4还展示了其他新兴的能力，包括根据给定图像创作故事和诗歌，为图像中展示的问题提供解决方案，根据食物照片教用户如何烹饪等。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/31f5050494bb4414acde50ce107df418.png"/>
    </p>
    <h3>
     <a id="OPT_54">
     </a>
     OPT
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：OPT: Open Pre-trained Transformer Language Models
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://github.com/facebookresearch/metaseq
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        Meta AI
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      OPT是一系列仅包含解码器的预训练transformers模型，参数范围从125M到175B。
     </strong>
     这些模型在零样本学习和少样本学习方面展现出卓越的能力。
    </p>
    <p>
     与其他大型语言模型相比，OPT模型具有重要优势：首先，这些模型的训练成本较低，仅需1/7的碳足迹即可达到类似GPT-3的性能水平；其次，OPT模型的权重可完全和负责任地与感兴趣的研究人员共享，这为研究者提供了更多研究和实验的机会。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/a7b794a69610a1084544a57856fe0b2a.png"/>
    </p>
    <h3>
     <a id="CPM_70">
     </a>
     CPM
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：CPM: A Large-scale Generative Chinese Pre-trained Language Model
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://github.com/TsinghuaAI/CPM-1-Generate
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        清华大学
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      CPM是中文预训练语言模型。CPM由26亿参数和100GB中文训练数据组成，是目前为止最大的中文预训练语言模型之一。
     </strong>
     它采用了大规模中文训练数据进行生成式预训练，旨在为多项下游中文自然语言处理任务提供支持。
    </p>
    <p>
     相较于其他模型，CPM专注于中文训练数据，因此在处理中文自然语言处理任务时更为得心应手。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/a0f5687d45895b82c94c3e8a7d5a7f36.png"/>
    </p>
    <h3>
     <a id="CPM2_86">
     </a>
     CPM-2
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：CPM-2: Large-scale cost-effective pre-trained language models
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://github.com/TsinghuaAI/CPM
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        清华大学
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      CPM-2是一个基于大规模预训练语言模型（PLMs）的成本效益技术套件。
     </strong>
     它通过一系列技术来解决PLMs的效率问题，包括知识继承、提示微调和推理工具包infmoe。
    </p>
    <p>
     首先，利用知识继承加速预训练过程，利用现有的PLMs而不是从头开始训练模型。其次，探索了大规模PLMs的提示微调最佳实践，显著减少了任务特定参数的数量。最后，实现了一个新的推理工具包infmoe，用于在有限的计算资源下使用大规模PLMs。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/55b85670965aacef71885286cd3f5eef.png"/>
    </p>
    <h3>
     <a id="CodeGen_102">
     </a>
     CodeGen
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis
       </p>
      </li>
      <li>
       <p>
        <strong>
         代码：
        </strong>
        https://github.com/salesforce/CodeGen
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        University of California &amp; Salesforce
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      CodeGen是一个由自然语言和编程语言数据训练而成的大型语言模型系列，拥有高达16.1B个参数。
     </strong>
     CodeGen的优势在于能够通过输入-输出示例或自然语言描述来生成计算机程序，这是程序合成的重要应用。
    </p>
    <p>
     相较于先前的技术，CodeGen展示出在零样本Python代码生成上与现有最先进技术相媲美的实力，这归功于其在人类评估上的表现。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/c37aecb753911813f359f03e9aca919d.png"/>
    </p>
    <h3>
     <a id="BLOOM_118">
     </a>
     BLOOM
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://huggingface.co/bigscience/bloom
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      BLOOM是一个拥有1760亿参数的开放获取语言模型。
     </strong>
     BLOOM由数百名研究人员合作设计和构建，是一个仅包含解码器的Transformer语言模型。
    </p>
    <p>
     它在ROOTS语料库上进行训练，该语料库包含46种自然语言和13种编程语言的数百个来源（总共59种语言）。BLOOM在各种基准测试中表现出色，经过多任务提示微调后的结果更加强大。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/9c85bc92c27a3f8334ef3f432437a904.png"/>
    </p>
    <h3>
     <a id="GLM130B_132">
     </a>
     GLM-130B
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：GLM-130B: An Open Bilingual Pre-trained Model
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        https://github.com/THUDM/GLM-130B
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        清华大学
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      GLM-130B是一个拥有1300亿参数的双语（英文和中文）预训练语言模型。
     </strong>
    </p>
    <p>
     该模型旨在开源一个至少与GPT-3一样出色的1000亿规模模型，并揭示这种规模的模型如何成功地进行预训练。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/734d4b340ce656ab6ffde09ee3774085.png"/>
    </p>
    <h3>
     <a id="mT5_148">
     </a>
     mT5
    </h3>
    <blockquote>
     <ul>
      <li>
       <p>
        <strong>
         论文
        </strong>
        ：mT5: A massively multilingual pre-trained text-to-text transformer
       </p>
      </li>
      <li>
       <p>
        <strong>
         开源：
        </strong>
        http://goo.gle/mt5-code
       </p>
      </li>
      <li>
       <p>
        <strong>
         机构：
        </strong>
        Google
       </p>
      </li>
     </ul>
    </blockquote>
    <p>
     <strong>
      mT5是T5的多语言变体，
     </strong>
     采用了统一的text-text格式和大规模训练，通过在覆盖101种语言的新Common Crawl数据集上进行预训练。
    </p>
    <p>
     mT5的设计和修改训练方法在论文中有详细描述，展示了在许多多语言基准任务上的最先进表现。
    </p>
    <p>
     <img alt="图片" src="https://i-blog.csdnimg.cn/blog_migrate/663d7a700e8d6e9b240f410a90ac3328.png"/>
     <br/>
     <strong>
      -END-
     </strong>
    </p>
    <hr/>
    <h5>
     <a id="httpsblogcsdnnet2301_76168381articledetails137261875ops_request_miscrequest_idbiz_id102utm_termE5A4A7E6A8A1E59E8Butm_mediumdistributepc_search_resultnonetaskblog2allsobaiduwebdefault0137261875142v100pc_search_result_base4spm1018222630014187AI_167">
     </a>
     <a href="https://blog.csdn.net/2301_76168381/article/details/137261875?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-137261875.142%5Ev100%5Epc_search_result_base4&amp;spm=1018.2226.3001.4187">
     </a>
     👉AI大模型学习路线汇总👈
    </h5>
    <p>
     大模型学习路线图，整体分为7个大的阶段：
     <strong>
      （全套教程文末领取哈）
     </strong>
     <br/>
     <img alt="                                                   " src="https://i-blog.csdnimg.cn/blog_migrate/1dc8bc24c6d7992933d03140b841fe10.png#pic_center"/>
     <br/>
     <strong>
      第一阶段：
     </strong>
     从大模型系统设计入手，讲解大模型的主要方法；
    </p>
    <p>
     <strong>
      第二阶段：
     </strong>
     在通过大模型提示词工程从Prompts角度入手更好发挥模型的作用；
    </p>
    <p>
     <strong>
      第三阶段：
     </strong>
     大模型平台应用开发借助阿里云PAI平台构建电商领域虚拟试衣系统；
    </p>
    <p>
     <strong>
      第四阶段：
     </strong>
     大模型知识库应用开发以LangChain框架为例，构建物流行业咨询智能问答系统；
    </p>
    <p>
     <strong>
      第五阶段：
     </strong>
     大模型微调开发借助以大健康、新零售、新媒体领域构建适合当前领域大模型；
    </p>
    <p>
     <strong>
      第六阶段：
     </strong>
     以SD多模态大模型为主，搭建了文生图小程序案例；
    </p>
    <p>
     <strong>
      第七阶段：
     </strong>
     以大模型平台应用与开发为主，通过星火大模型，文心大模型等成熟大模型构建大模型行业应用。
    </p>
    <h4>
     <a id="httpsblogcsdnnet2301_76168381articledetails137261875ops_request_miscrequest_idbiz_id102utm_termE5A4A7E6A8A1E59E8Butm_mediumdistributepc_search_resultnonetaskblog2allsobaiduwebdefault0137261875142v100pc_search_result_base4spm1018222630014187_185">
     </a>
     <a href="https://blog.csdn.net/2301_76168381/article/details/137261875?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-137261875.142%5Ev100%5Epc_search_result_base4&amp;spm=1018.2226.3001.4187">
     </a>
     👉大模型实战案例👈
    </h4>
    <p>
     光学理论是没用的，要学会跟着一起做，要动手实操，才能将自己的所学运用到实际当中去，这时候可以搞点实战案例来学习。
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/31b6d0dd95b8d8fec9917b554a3cd9c9.jpeg#pic_center"/>
    </p>
    <h4>
     <a id="httpsblogcsdnnet2301_76168381articledetails137261875ops_request_miscrequest_idbiz_id102utm_termE5A4A7E6A8A1E59E8Butm_mediumdistributepc_search_resultnonetaskblog2allsobaiduwebdefault0137261875142v100pc_search_result_base4spm1018222630014187PDF_191">
     </a>
     <a href="https://blog.csdn.net/2301_76168381/article/details/137261875?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-137261875.142%5Ev100%5Epc_search_result_base4&amp;spm=1018.2226.3001.4187">
     </a>
     👉大模型视频和PDF合集👈
    </h4>
    <p>
     观看零基础学习书籍和视频，看书籍和视频学习是最快捷也是最有效果的方式，跟着视频中老师的思路，从基础到深入，还是很容易入门的。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/08182a747176e52c2e20185242400093.png#pic_center"/>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/229901358c1a2ade26339ff0af840e62.png#pic_center"/>
    </p>
    <h4>
     <a id="httpsblogcsdnnet2301_76168381articledetails137261875ops_request_miscrequest_idbiz_id102utm_termE5A4A7E6A8A1E59E8Butm_mediumdistributepc_search_resultnonetaskblog2allsobaiduwebdefault0137261875142v100pc_search_result_base4spm1018222630014187_197">
     </a>
     <a href="https://blog.csdn.net/2301_76168381/article/details/137261875?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-137261875.142%5Ev100%5Epc_search_result_base4&amp;spm=1018.2226.3001.4187">
     </a>
     👉学会后的收获：👈
    </h4>
    <p>
     <strong>
      • 基于大模型全栈工程实现
     </strong>
     （前端、后端、产品经理、设计、数据分析等），通过这门课可获得不同能力；
    </p>
    <p>
     <strong>
      • 能够利用大模型解决相关实际项目需求：
     </strong>
     大数据时代，越来越多的企业和机构需要处理海量数据，利用大模型技术可以更好地处理这些数据，提高数据分析和决策的准确性。因此，掌握大模型应用开发技能，可以让程序员更好地应对实际项目需求；
    </p>
    <p>
     • 基于大模型和企业数据AI应用开发，
     <strong>
      实现大模型理论、掌握GPU算力、硬件、LangChain开发框架和项目实战技能，
     </strong>
     学会Fine-tuning垂直训练大模型（数据准备、数据蒸馏、大模型部署）一站式掌握；
    </p>
    <p>
     <strong>
      • 能够完成时下热门大模型垂直领域模型训练能力，提高程序员的编码能力：
     </strong>
     大模型应用开发需要掌握机器学习算法、深度学习框架等技术，这些技术的掌握可以提高程序员的编码能力和分析能力，让程序员更加熟练地编写高质量的代码。
    </p>
    <h4>
     <a id="httpsblogcsdnnet2301_76168381articledetails137261875ops_request_miscrequest_idbiz_id102utm_termE5A4A7E6A8A1E59E8Butm_mediumdistributepc_search_resultnonetaskblog2allsobaiduwebdefault0137261875142v100pc_search_result_base4spm1018222630014187_207">
     </a>
     <a href="https://blog.csdn.net/2301_76168381/article/details/137261875?ops_request_misc=&amp;request_id=&amp;biz_id=102&amp;utm_term=%E5%A4%A7%E6%A8%A1%E5%9E%8B&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-137261875.142%5Ev100%5Epc_search_result_base4&amp;spm=1018.2226.3001.4187">
     </a>
     👉获取方式：
    </h4>
    <p>
     😝有需要的小伙伴，可以保存图片到
     <strong>
      wx扫描二v码
     </strong>
     免费领取【
     <code>
      保证100%免费
     </code>
     】🆓
     <br/>
     <img src="https://i-blog.csdnimg.cn/blog_migrate/61e53f4067e8c70530d7613278c62a8b.png"/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
</div>


