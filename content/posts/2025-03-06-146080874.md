---
layout: post
title: "论文阅读多模态LSeg"
date: 2025-03-06 22:08:04 +0800
description: "本文提出基于CLIP的零样本语义分割方法(LSeg)，冻结CLIP文本编码器权重，将文本特征与ViT图像特征逐像素相乘，通过两层空间卷积融合特征。训练使用有监督分割数据，在推理阶段通过任意文本提示实现像素级分割。实验表明在PASCAL-5等数据集上显著优于传统零样本方法，但与少样本方法仍有差距。创新点在于构建语言感知特征空间，但未解释四层空间规整失效现象。"
keywords: "【论文阅读】多模态——LSeg"
categories: ['论文阅读']
tags: ['计算机视觉', '深度学习', '机器学习']
artid: "146080874"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146080874
    alt: "论文阅读多模态LSeg"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146080874
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146080874
cover: https://bing.ee123.net/img/rand?artid=146080874
image: https://bing.ee123.net/img/rand?artid=146080874
img: https://bing.ee123.net/img/rand?artid=146080874
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【论文阅读】多模态——LSeg
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     文献基本信息
    </h2>
    <ul>
     <li>
      <strong>
       标题：
      </strong>
      Language-Driven Semantic Segmentation
     </li>
     <li>
      <strong>
       作者：
      </strong>
      Boyi Li、Kilian Q. Weinberger、Serge Belongie、Vladlen Koltun、René Ranftl
     </li>
     <li>
      <strong>
       单位：
      </strong>
      Cornell University、University of Copenhagen、Apple、Intel Labs
     </li>
     <li>
      <strong>
       会议/期刊：
      </strong>
      ICLR
     </li>
     <li>
      <strong>
       发表时间：
      </strong>
      2022年4月3日
     </li>
     <li>
      <strong>
       代码：
      </strong>
      <a class="link-info" href="https://github.com/isl-org/lang-seg" title="https://github.com/isl-org/lang-seg">
       https://github.com/isl-org/lang-seg
      </a>
     </li>
    </ul>
    <h2>
     背景与意义
    </h2>
    <ul>
     <li>
      <strong>
       语义分割
      </strong>
      可以看做是
      <strong>
       像素级的分类
      </strong>
      ，因此分类的新技术、新思路，一般可以直接用过来。
     </li>
     <li>
      本文实现了
      <strong>
       zero-shot的语义分割
      </strong>
      ，实现方式与
      <strong>
       CLIP实现zero-shot
      </strong>
      的方式类似，都是通过类别prompt作为文本输入，然后计算相似度。
     </li>
    </ul>
    <p style="text-align:center">
     <img alt="" height="300" src="https://i-blog.csdnimg.cn/direct/c6a2184e39f84528a63563b5fd235f5b.png" width="515"/>
    </p>
    <ul>
     <li>
      给定一张图片，然后通过文本prompt给定任意的类别，从而实现对应的语义分割。
     </li>
     <li>
      从上图中可以看到，给定了对应的类别prompt：
     </li>
    </ul>
    <ol>
     <li>
      对于
      <strong>
       图中明确出现了的语义类别（如dog、tree）
      </strong>
      ，模型能够很清楚地分割出来。
     </li>
     <li>
      对于
      <strong>
       图中没有的类别（如vehicle）
      </strong>
      ，模型也不会误召回（容错率高）。
     </li>
     <li>
      对于
      <strong>
       图中有、但是类别prompt没给的类别（如grass）
      </strong>
      ，也能正确分类为other。
     </li>
     <li>
      同样可以检测
      <strong>
       类别的子类或父类（如dog、pet）
      </strong>
      ，模型也能够通过语义识别出来。
     </li>
    </ol>
    <ul>
     <li>
      由于CLIP类的模型实质上都是通过计算
      <strong>
       图文相似度
      </strong>
      来实现分类或分割的，因此对于
      <strong>
       “other”
      </strong>
      类的类别，prompt文本实际可以是
      <strong>
       任何无意义的文本
      </strong>
      ，如“me”、“a”或“an” 等，只要与目标类别不要太接近即可。
     </li>
    </ul>
    <h2>
     研究方法与创新点
    </h2>
    <p style="text-align:center">
     <img alt="" height="200" src="https://i-blog.csdnimg.cn/direct/e37d779d45444d76ba5a09fd32e89299.png" width="614"/>
    </p>
    <ul>
     <li>
      如上图所示，模型整体看来与
      <strong>
       CLIP
      </strong>
      模型非常相似，图像先输入
      <strong>
       图像编码器（DPT ViT+decoder）
      </strong>
      得到特征向量，再进行一些
      <strong>
       upscaling
      </strong>
      ，输出图像与原图像大小保持一致，输出再与ground-truth做
      <strong>
       交叉熵
      </strong>
      ，其中将
      <strong>
       单个的图像文本特征
      </strong>
      换成
      <strong>
       语义分割中逐像素的密集特征
      </strong>
      。
     </li>
     <li>
      <strong>
       文本编码器
      </strong>
      提取
      <img alt="$N \times C$" class="mathcode" src="https://latex.csdn.net/eq?%24N%20%5Ctimes%20C%24">
       的文本特征（
       <img alt="$N$" class="mathcode" src="https://latex.csdn.net/eq?%24N%24">
        个类别，
        <img alt="$C$" class="mathcode" src="https://latex.csdn.net/eq?%24C%24">
         为特征维度），
         <strong>
          图像编码器
         </strong>
         提取
         <img alt="$\tilde H \times \tilde W \times C$" class="mathcode" src="https://latex.csdn.net/eq?%24%5Ctilde%20H%20%5Ctimes%20%5Ctilde%20W%20%5Ctimes%20C%24">
          的密集图像特征（跟原来相比有所降维，比如1/4、1/16），文本-图像二者相乘得到
          <img alt="$\tilde H \times \tilde W \times N$" class="mathcode" src="https://latex.csdn.net/eq?%24%5Ctilde%20H%20%5Ctimes%20%5Ctilde%20W%20%5Ctimes%20N%24">
           的特征，再经过
           <strong>
            空间规整模块
           </strong>
           上采样回原图尺寸，完成
           <strong>
            语义分割
           </strong>
           ，其中
           <img alt="$N$" class="mathcode" src="https://latex.csdn.net/eq?%24N%24">
            、
            <img alt="$C$" class="mathcode" src="https://latex.csdn.net/eq?%24C%24"/>
            、
            <img alt="$\tilde H$" class="mathcode" src="https://latex.csdn.net/eq?%24%5Ctilde%20H%24"/>
            和
            <img alt="$\tilde W$" class="mathcode" src="https://latex.csdn.net/eq?%24%5Ctilde%20W%24"/>
            分别是类别prompt个数（可变）、通道数和特征图的高、宽。除了上面的
            <strong>
             文本编码器提取的文本特征要与密集图像特征相乘来计算像素级的图文相似度
            </strong>
            之外，
            <strong>
             整个网络与传统的有监督网络完全一致
            </strong>
            。
           </img>
          </img>
         </img>
        </img>
       </img>
      </img>
     </li>
     <li>
      在训练过程中，模型是以
      <strong>
       有监督
      </strong>
      的方式进行训练的，也就是说训练过程中是存在
      <strong>
       标注的分割图
      </strong>
      的，模型在7个分割数据集上进行训练。
     </li>
     <li>
      在推理时，可以指定
      <strong>
       任意个数、任意内容
      </strong>
      的类别prompt来进行
      <strong>
       zero-shot
      </strong>
      的语义分割。
     </li>
     <li>
      <strong>
       创新：
      </strong>
      通过在
      <strong>
       传统的有监督分割模型
      </strong>
      上加入
      <strong>
       文本特征
      </strong>
      ，通过
      <strong>
       特征相乘
      </strong>
      把文本特征和图像特征结合起来，学到一些
      <strong>
       languge-aware的特征
      </strong>
      ，在最后就能用
      <strong>
       文本prompt
      </strong>
      得到
      <strong>
       任意的分割效果
      </strong>
      。
     </li>
     <li>
      LSeg整个
      <strong>
       文本编码器
      </strong>
      就是
      <strong>
       CLIP的文本编码器的模型和权重
      </strong>
      ，并且训练、推理全程中都是
      <strong>
       冻结
      </strong>
      的；LSeg的
      <strong>
       图像编码器
      </strong>
      可以是
      <strong>
       任何网络（CNN/ViT）
      </strong>
      ，需要进行训练。
     </li>
     <li>
      <strong>
       空间规整模块
      </strong>
      是本文提出的一个模块，为了在计算完像素级图文相似度后有一些可学习的参数来理解计算结果，由一些
      <strong>
       卷积
      </strong>
      和
      <strong>
       逐深度卷积
      </strong>
      组成。
     </li>
    </ul>
    <h2>
     研究结论
    </h2>
    <ul>
     <li>
      在PASCAL-5、COCO20、FSS-1000上作评价，如PASCAL-5有20类，现在把20类分成4份，每份5类，将其中5类作为已知，其他15类未知，做zero-shot实验。
     </li>
    </ul>
    <p style="text-align:center">
     <img alt="" height="250" src="https://i-blog.csdnimg.cn/direct/1089924d18e24296bf007bac99a5f295.png" width="407"/>
    </p>
    <ul>
     <li>
      由上图可知，
      <strong>
       LSeg在zero-shot的语义分割上确实大幅领先之前方法
      </strong>
      ，但是
      <strong>
       与few-shot哪怕是one-shot相比，还是有很大的提升空间
      </strong>
      。
     </li>
    </ul>
    <p style="text-align:center">
     <img alt="" height="200" src="https://i-blog.csdnimg.cn/direct/28749dd1b8cd4eadb7b2966ae9232a3b.png" width="418"/>
    </p>
    <ul>
     <li>
      由上图可知，LSeg
      <strong>
       在zero-shot的语义分割上确实大幅领先之前方法
      </strong>
      ，但是
      <strong>
       与few-shot哪怕是one-shot相比，还是有很大的提升空间
      </strong>
      。
     </li>
    </ul>
    <h2>
     存在的问题
    </h2>
    <ol>
     <li>
      空间规整层是简单的conv卷积或者DWconv，这一层进一步学习文本图像融合后的特征，理解文本与图像如何交互。
      <strong>
       消融实验证明，两层空间规整层效果最好，但是四层空间规整层突然就崩了，本文中并没有对此解释原因
      </strong>
      ，因此无法得知空间规整是否是一个稳定有效的技巧。
     </li>
    </ol>
    <h2>
     启发与思考
    </h2>
    <ol>
     <li>
      <strong>
       图像分类任务
      </strong>
      和
      <strong>
       图像分割任务
      </strong>
      很像，无非就是把图像级别的分类转变成像素级别的分类，前者的技术往往都能直接应用到后者。
     </li>
     <li>
      提供了一种利用CLIP的新思路，可以
      <strong>
       单独
      </strong>
      使用其中的文本或图像编码器，并且也可以用
      <strong>
       有监督
      </strong>
      的方式进行训练。
     </li>
    </ol>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f35383730313939352f:61727469636c652f64657461696c732f313436303830383734" class_="artid" style="display:none">
 </p>
</div>


