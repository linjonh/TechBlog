---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34333530343839322f:61727469636c652f64657461696c732f313233383537383430"
layout: post
title: "k8s网络模式"
date: 2025-01-13 18:49:58 +08:00
description: "一些基本概念Kubernetes集群里有三种IP地址，分别如下：- 外部 IP：一般使用Node节点"
keywords: "kubernetes的网络模式"
categories: ['K']
tags: ['运维', 'Zookeeper', 'Linux']
artid: "123857840"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=123857840
    alt: "k8s网络模式"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=123857840
featuredImagePreview: https://bing.ee123.net/img/rand?artid=123857840
---

# k8s—网络模式

## 一些基本概念

1. Kubernetes集群里有三种IP地址，分别如下：

```bash
- 外部 IP：一般使用Node节点的IP地址，即物理网卡的IP地址。
 -  PodIP：Pod的IP地址，即docker容器的IP地址，此为虚拟IP地址
 - ClusterIP：Service的IP地址，此为虚拟IP地址。

```

2. **网桥**
   ：网桥是一个二层网络设备,通过网桥可以将linux支持的不同的端口连接起来,并实现类似交换机那样的多对多的通信。
3. **网络的命名空间**
   ：Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；docker利用这一特性，实现不容器间的网络隔离。
4. **Veth设备对**
   ：Veth设备对的引入是为了实现在不同网络命名空间的通信。
5. **Iptables/Netfilter**
   ：Netfilter负责在内核中执行各种挂接的规则(过滤、修改、丢弃等)，运行在内核 模式中；Iptables模式是在用户模式下运行的进程，负责协助维护内核中Netfilter的各种规则表；通过二者的配合来实现整个Linux网络协议栈中灵活的数据包处理机制。
6. **路由**
   ：Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里。（与转发区分开来）

### Pod IP

> 每个Pod的IP地址，他是DockerEngine根据docker网桥的IP地址段进行分配的，通常是一个虚拟的二层网络，Kubernetes的最小部署单元是Pod。利用Flannel作为不同HOST之间容器互通技术时，由Flannel和etcd维护了一张节点间的路由表。Flannel的设计目的就是为集群中的所有节点重新规划IP地址的使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且”不重复的”IP地址，并让属于不同节点上的容器能够直接通过内网IP通信。
>
> 每个Pod启动时，会自动创建一个镜像（pause容器）gcr.io/google\_containers/pause:0.8.0的容器，容器内部与外部的通信经由此容器代理，该容器的IP也可以称为Pod IP。

### Cluster IP

> Pod IP 地址是实际存在于某个网卡(可以是虚拟设备docker0)上的，但Service Cluster
>   
> IP就不一样了，没有网络设备为这个地址负责。它是由kube-proxy使用Iptables规则重新定向到其本地端口，再均衡到后端Pod的。
>   
> 当我们的Service被创建时，Kubernetes给它分配一个地址10.0.0.1。这个地址从我们启动API的service-cluster-ip-range参数(旧版本为portal\_net参数)指定的地址池中分配，比如–service-cluster-ip-range=10.0.0.0/16。假设这个Service的端口是1234。集群内的所有kube-proxy都会注意到这个Service。当proxy发现一个新的service后，它会在本地节点打开一个任意端口，建相应的iptables规则，重定向服务的IP和port到这个新建的端口，开始接受到达这个服务的连接。

当一个客户端访问这个service时，这些iptable规则就开始起作用，客户端的流量被重定向到kube-proxy为这个service打开的端口上，kube-proxy随机选择一个后端pod来服务客户。这个流程如下图所示：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/108696a718e351ef68ebded94d385b05.png)
  
根据Kubernetes的网络模型，使用Service Cluster IP和Port访问Service的客户端可以坐落在任意代理节点上。外部要访问Service，我们就需要给Service外部访问IP

### 外部IP

> Service对象在Cluster IP range池中分配到的IP只能在内部访问，如果服务作为一个应用程序内部的层次，还是很合适的。如果这个Service作为前端服务，准备为集群外的客户提供业务，我们就需要给这个服务提供公共IP了。
>   
> 外部访问者是访问集群代理节点的访问者。为这些访问者提供服务，我们可以在定义Service时指定其spec.publicIPs，一般情况下publicIP是代理节点的物理IP地址。和先前的Cluster IP range上分配到的虚拟的IP一样，kube-proxy同样会为这些publicIP提供Iptables 重定向规则，把流量转发到后端的Pod上。有了publicIP，我们就可以使用load balancer等常用的互联网技术来组织外部对服务的访问了。 spec.publicIPs在新的版本中标记为过时了，代替它的是spec.type=NodePort，这个类型的service，系统会给它在集群的各个代理节点上分配一个节点级别的端口，能访问到代理节点的客户端都能访问这个端口，从而访问到服务。涉及服务
> **暴露方式（ingress，nodeport，LB。。。）**

## Docker网络实现

位置架构
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/526f1b98ddd7e6771a694538e6bc3450.png)

1、单机网络模式：Bridge 、Host、Container、None

2、多机网络模式：一类是 Docker 在 1.9 版本中引入Libnetwork项目，对跨节点网络的原生支持；一类是通过插件（plugin）方式引入的第三方实现方案，比如 Flannel，Calico 等等。

### 1、容器间通信：

同一个Pod的容器共享同一个网络命名空间，它们之间的访问可以用localhost地址 + 容器端口就可以访问。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/35f1a6f05ae51c33f84d4b2deab39e18.png)

### 2、同一Node中Pod间通信：

同一Node中Pod的默认路由都是docker0的地址，由于它们关联在同一个docker0网桥上，地址网段相同，所有它们之间应当是能直接通信的。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/fb3d7ba24ca64c7c73800a13197fb860.png)

### 3、不同Node中Pod间通信：

不同Node中Pod间通信要满足2个条件： Pod的IP不能冲突； 将Pod的IP和所在的Node的IP关联起来，通过这个关联让Pod可以互相访问。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4ea99c9397f47a1c1523a5be6a6c915e.png)

## Kube-proxy介绍

Kube-proxy是一个简单的网络代理和负载均衡器，它的作用主要是负责Service的实现，具体来说，就是实现了内部从Pod到Service和外部的从NodePort向Service的访问。

### 实现方式

* userspace是在用户空间，通过kuber-proxy实现LB的代理服务，这个是kube-proxy的最初的版本，较为稳定，但是效率也自然不太高。
* iptables是纯采用iptables来实现LB，是目前kube-proxy默认的方式。

iptables模式下Kube-proxy的实现方式：
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/cc6cd01b0eecca6ab1457182c7dafacf.png)

* 在这种模式下，kube-proxy监视Kubernetes主服务器添加和删除服务和端点对象。对于每个服务，它安装iptables规则，捕获到服务的clusterIP（虚拟）和端口的流量，并将流量重定向到服务的后端集合之一。对于每个Endpoints对象，它安装选择后端Pod的iptables规则。
* 默认情况下，后端的选择是随机的。可以通过将service.spec.sessionAffinity设置为“ClientIP”（默认为“无”）来选择基于客户端IP的会话关联。
* 与用户空间代理一样，最终结果是绑定到服务的IP:端口的任何流量被代理到适当的后端，而客户端不知道关于Kubernetes或服务或Pod的任何信息。这应该比用户空间代理更快，更可靠。然而，与用户空间代理不同，如果最初选择的Pod不响应，则iptables代理不能自动重试另一个Pod，因此它取决于具有工作准备就绪探测。

[引用地址1](https://www.kubernetes.org.cn/2059.html)
  
[引用地址2](https://www.2cto.com/net/201702/595150.html)

后续–Flannel