---
layout: post
title: "本地部署项目记录deepseekQWQ"
date: 2025-03-07 14:05:12 +0800
description: "问题：解决：【跳过问题】"
keywords: "本地部署项目记录【deepseek、QWQ】"
categories: ['个人笔记']
tags: ['服务器', '持续部署']
artid: "146092532"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146092532
    alt: "本地部署项目记录deepseekQWQ"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146092532
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146092532
cover: https://bing.ee123.net/img/rand?artid=146092532
image: https://bing.ee123.net/img/rand?artid=146092532
img: https://bing.ee123.net/img/rand?artid=146092532
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     本地部署项目记录【deepseek、QWQ】
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     1-DeepSeek
    </h2>
    <p>
     参考：
     <a href="https://imaginemiracle.blog.csdn.net/article/details/145746316?spm=1001.2101.3001.6650.14&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7ECtr-14-145746316-blog-145455641.235%5Ev43%5Epc_blog_bottom_relevance_base4&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7ECtr-14-145746316-blog-145455641.235%5Ev43%5Epc_blog_bottom_relevance_base4&amp;utm_relevant_index=15" rel="nofollow" title="【Deepseek】Linux 本地部署 Deepseek_linux部署deepseek-CSDN博客">
      【Deepseek】Linux 本地部署 Deepseek_linux部署deepseek-CSDN博客
     </a>
    </p>
    <table border="1" cellpadding="1" cellspacing="1" style="width:500px">
     <tbody>
      <tr>
       <td>
        <p>
         问题：
        </p>
        <p>
         (base) root@QiuKu_303:~/Documents/Ollama# sh ollama_install.sh &gt;&gt;&gt; Cleaning up old version at /usr/local/lib/ollama &gt;&gt;&gt; Installing ollama to /usr/local &gt;&gt;&gt; Downloading Linux amd64 bundle ######################################################################### 100.0%
        </p>
       </td>
      </tr>
      <tr>
       <td>
        <p>
         解决：【跳过问题】
        </p>
        <p>
         export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH
        </p>
       </td>
      </tr>
     </tbody>
    </table>
    <hr/>
    <h2>
     2-QWQ-32B
    </h2>
    <p>
     参考：消费级显卡也能跑！QwQ-32B本地部署教程来了！【视频号】
    </p>
    <p>
     参考：
     <a href="https://blog.csdn.net/qq_23997827/article/details/145495591?ops_request_misc=%257B%2522request%255Fid%2522%253A%25228bfcbccc58bd7eddad9553b649b28e87%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=8bfcbccc58bd7eddad9553b649b28e87&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-4-145495591-null-null.142^v102^pc_search_result_base3&amp;utm_term=vLLM%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2&amp;spm=1018.2226.3001.4187" title="Linux环境下使用vLLM部署本地大模型_vllm加载本地模型-CSDN博客">
      Linux环境下使用vLLM部署本地大模型_vllm加载本地模型-CSDN博客
     </a>
    </p>
    <p>
     参考：
     <a href="https://blog.csdn.net/m0_48891301/article/details/145491228?ops_request_misc=%257B%2522request%255Fid%2522%253A%25220f63982d41caaa2541b2e7ea84a55800%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=0f63982d41caaa2541b2e7ea84a55800&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~top_positive~default-1-145491228-null-null.nonecase&amp;utm_term=vllm%E9%83%A8%E7%BD%B2deepseek&amp;spm=1018.2226.3001.4450" title="DeepSeek 部署指南 (使用 vLLM 本地部署)_vllm部署deepseek-CSDN博客">
      DeepSeek 部署指南 (使用 vLLM 本地部署)_vllm部署deepseek-CSDN博客
     </a>
    </p>
    <table border="1" cellpadding="1" cellspacing="1" style="width:500px">
     <tbody>
      <tr>
       <td>
        conda create -n QWQ-32B python=3.12
       </td>
      </tr>
      <tr>
       <td>
        pip install vllm
       </td>
      </tr>
      <tr>
       <td>
        pip install git+https://github.com/huggingface/transformers
       </td>
      </tr>
      <tr>
       <td>
        pip install modelscope
       </td>
      </tr>
      <tr>
       <td>
        modelscope download --model 'Qwen/QwQ-32B' --local_dir '目标目录'
       </td>
      </tr>
      <tr>
       <td>
        vllm serve /home74/liguangzhen/folder/QwQ-32B
       </td>
      </tr>
     </tbody>
    </table>
    <table border="1" cellpadding="1" cellspacing="1" style="width:700px">
     <tbody>
      <tr>
       <td>
        <p>
         <strong>
          方案 1：使用 vLLM 部署 DeepSeek
         </strong>
        </p>
        <p>
         vLLM 具有高吞吐量，支持 PagedAttention，高效利用多张 GPU。
        </p>
        <p>
         <strong>
          1. 安装 vLLM
         </strong>
        </p>
        <pre><code>pip install vllm

pip install modelscope</code></pre>
        <p>
         <strong>
          2. 下载 DeepSeek 模型
         </strong>
        </p>
        <p>
         拉取 DeepSeek 相关模型，例如：# 以 deepseek-ai/deepseek-llm-7b-chat 为例
        </p>
        <pre><code>modelscope download --model 'deepseek-ai/deepseek-llm-7b-chat' --local_dir '/home74/liguangzhen/folder/DeepSeek'</code></pre>
        <p>
         <strong>
          3. 启动 vLLM 服务器
         </strong>
        </p>
        <pre><code>python -m vllm.entrypoints.openai.api_server \
    --model deepseek-7b-chat \
    --tensor-parallel-size 4  # 4 张 GPU 进行张量并行
</code></pre>
        <ul>
         <li>
          <code>
           tensor-parallel-size
          </code>
          设为 4，可以让 4 张 3090 共同运行一个模型。
         </li>
        </ul>
        <p>
         启动后，API 服务会运行在
         <code>
          http://localhost:8000/v1/completions
         </code>
         ，可以用 OpenAI API 兼容方式调用。
        </p>
        <p>
         <strong>
          4. 测试 API
         </strong>
        </p>
        <pre><code>import requests

url = "http://localhost:8000/v1/completions"
headers = {"Content-Type": "application/json"}
data = {
    "model": "deepseek-7b-chat",
    "prompt": "请介绍一下深度学习。",
    "max_tokens": 200
}

response = requests.post(url, headers=headers, json=data)
print(response.json())
</code></pre>
       </td>
      </tr>
     </tbody>
    </table>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34353832313238352f:61727469636c652f64657461696c732f313436303932353332" class_="artid" style="display:none">
 </p>
</div>


