---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34353832313238352f:61727469636c652f64657461696c732f313436303932353332"
layout: post
title: "本地部署项目记录deepseekQWQ"
date: 2025-03-07 14:05:12 +0800
description: "问题：解决：【跳过问题】"
keywords: "本地部署项目记录【deepseek、QWQ】"
categories: ['个人笔记']
tags: ['服务器', '持续部署']
artid: "146092532"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146092532
    alt: "本地部署项目记录deepseekQWQ"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146092532
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146092532
cover: https://bing.ee123.net/img/rand?artid=146092532
image: https://bing.ee123.net/img/rand?artid=146092532
img: https://bing.ee123.net/img/rand?artid=146092532
---

# 本地部署项目记录【deepseek、QWQ】

## 1-DeepSeek

参考：
[【Deepseek】Linux 本地部署 Deepseek\_linux部署deepseek-CSDN博客](https://imaginemiracle.blog.csdn.net/article/details/145746316?spm=1001.2101.3001.6650.14&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7ECtr-14-145746316-blog-145455641.235%5Ev43%5Epc_blog_bottom_relevance_base4&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7Ebaidujs_baidulandingword%7ECtr-14-145746316-blog-145455641.235%5Ev43%5Epc_blog_bottom_relevance_base4&utm_relevant_index=15 "【Deepseek】Linux 本地部署 Deepseek_linux部署deepseek-CSDN博客")

|  |
| --- |
| 问题：  (base) root@QiuKu\_303:~/Documents/Ollama# sh ollama\_install.sh >>> Cleaning up old version at /usr/local/lib/ollama >>> Installing ollama to /usr/local >>> Downloading Linux amd64 bundle ######################################################################### 100.0% |
| 解决：【跳过问题】  export LD\_LIBRARY\_PATH=/usr/lib/x86\_64-linux-gnu:$LD\_LIBRARY\_PATH |

---

## 2-QWQ-32B

参考：消费级显卡也能跑！QwQ-32B本地部署教程来了！【视频号】

参考：
[Linux环境下使用vLLM部署本地大模型\_vllm加载本地模型-CSDN博客](https://blog.csdn.net/qq_23997827/article/details/145495591?ops_request_misc=%257B%2522request%255Fid%2522%253A%25228bfcbccc58bd7eddad9553b649b28e87%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=8bfcbccc58bd7eddad9553b649b28e87&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-4-145495591-null-null.142^v102^pc_search_result_base3&utm_term=vLLM%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2&spm=1018.2226.3001.4187 "Linux环境下使用vLLM部署本地大模型_vllm加载本地模型-CSDN博客")

参考：
[DeepSeek 部署指南 (使用 vLLM 本地部署)\_vllm部署deepseek-CSDN博客](https://blog.csdn.net/m0_48891301/article/details/145491228?ops_request_misc=%257B%2522request%255Fid%2522%253A%25220f63982d41caaa2541b2e7ea84a55800%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=0f63982d41caaa2541b2e7ea84a55800&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~top_positive~default-1-145491228-null-null.nonecase&utm_term=vllm%E9%83%A8%E7%BD%B2deepseek&spm=1018.2226.3001.4450 "DeepSeek 部署指南 (使用 vLLM 本地部署)_vllm部署deepseek-CSDN博客")

|  |
| --- |
| conda create -n QWQ-32B python=3.12 |
| pip install vllm |
| pip install git+https://github.com/huggingface/transformers |
| pip install modelscope |
| modelscope download --model 'Qwen/QwQ-32B' --local\_dir '目标目录' |
| vllm serve /home74/liguangzhen/folder/QwQ-32B |

|  |
| --- |
| **方案 1：使用 vLLM 部署 DeepSeek**  vLLM 具有高吞吐量，支持 PagedAttention，高效利用多张 GPU。  **1. 安装 vLLM**   ``` pip install vllm  pip install modelscope ```   **2. 下载 DeepSeek 模型**  拉取 DeepSeek 相关模型，例如：# 以 deepseek-ai/deepseek-llm-7b-chat 为例   ``` modelscope download --model 'deepseek-ai/deepseek-llm-7b-chat' --local_dir '/home74/liguangzhen/folder/DeepSeek' ```   **3. 启动 vLLM 服务器**   ``` python -m vllm.entrypoints.openai.api_server \     --model deepseek-7b-chat \     --tensor-parallel-size 4  # 4 张 GPU 进行张量并行  ```  * `tensor-parallel-size`   设为 4，可以让 4 张 3090 共同运行一个模型。   启动后，API 服务会运行在 `http://localhost:8000/v1/completions` ，可以用 OpenAI API 兼容方式调用。  **4. 测试 API**   ``` import requests  url = "http://localhost:8000/v1/completions" headers = {"Content-Type": "application/json"} data = {     "model": "deepseek-7b-chat",     "prompt": "请介绍一下深度学习。",     "max_tokens": 200 }  response = requests.post(url, headers=headers, json=data) print(response.json())  ``` |