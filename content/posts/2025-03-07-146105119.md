---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34333532363434332f:61727469636c652f64657461696c732f313436313035313139"
layout: post
title: "如何在语言模型的参数中封装知识以T5模型为例"
date: 2025-03-07 21:04:59 +08:00
description: "近年来，预训练的神经语言模型在未标记文本上训练后，能够隐式地存储和检索知识，使用自然语言查询。本文通过微调预训练模型来回答问题，而无需任何外部上下文或知识，来测量这一方法的实际使用价值。结果显示，该方法随着模型规模的增加而扩展，并在回答问题时与从外部知识源显式检索答案的开放式系统竞争。为了促进可重现性和未来工作，我们发布了代码和训练模型。本文研究了大型语言模型通过预训练存储和检索知识的能力。"
keywords: "如何在语言模型的参数中封装知识？——以T5模型为例"
categories: ['人工智能Ai']
tags: ['语言模型', '深度学习', '人工智能']
artid: "146105119"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146105119
    alt: "如何在语言模型的参数中封装知识以T5模型为例"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146105119
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146105119
cover: https://bing.ee123.net/img/rand?artid=146105119
image: https://bing.ee123.net/img/rand?artid=146105119
img: https://bing.ee123.net/img/rand?artid=146105119
---

# 如何在语言模型的参数中封装知识？——以T5模型为例

## 【摘要】

这篇论文探讨了大型语言模型在无需外部知识的情况下，能否通过预训练来存储和检索知识以回答开放领域的问题。作者通过微调预训练模型来回答问题，而这些模型在训练时并未提供任何额外的知识或上下文。这种方法随着模型规模的增加而表现出良好的扩展性，并在某些任务上达到了与使用外部知识源的开放领域系统相当的性能。

论文的主要贡献包括：

1. 研究了大语言模型在开放领域问答任务上的表现，特别是这些模型是否能够存储和检索足够的知识以回答问题。
2. 通过使用预训练的T5模型进行实验，展示了模型规模与知识存储能力之间的关系。
3. 探讨了模型参数中存储知识的能力，以及这种能力是否能随着模型复杂性的增加而增加。
4. 分享了代码和训练好的模型，以便其他研究人员能够复制实验结果。

实验展示了在不同规模的预训练模型上进行微调的结果，发现较大的模型在开放领域问答任务上表现更好。此外，使用“显著短语掩码”（SSM）预训练目标可以显著提高模型的性能。尽管模型能够回答复杂的问题，但它们在处理多义或上下文依赖性更强的问题时表现不如使用外部知识源的方法。

论文还通过对人进行的评估，发现模型在某些情况下可能会错过正确答案，这可能是由于答案的细微差别、缺失的正确答案或者需要特定上下文的问题。作者还讨论了未来工作的方向，包括更有效的语言模型设计、解释性模型以及需要推理能力的复杂任务。

总的来说，这篇论文展示了大语言模型在开放领域知识存储和检索方面的能力，并提出了未来研究的方向。

## 【数据来源】

本文的主要数据来源包括以下几个方面：

1. **预训练数据**
   ：主要使用了C4数据集，这是一个包含大量非结构化网页内容的大规模多样化数据集。此外，还尝试了在英语维基百科上进行预训练，使用与T5相同的目标（无监督“片段替换”）进行进一步训练，但结果未见明显改善。
2. **实验数据集**
   ：

   * **Natural Questions**
     ：一个来自网络查询的问题数据集，每个问题都可以被标注为不可回答、简短答案或二元答案。问题验证集可以被多次标注，有些问题有多个答案。
   * **WebQuestions**
     ：一个从网络查询中提取的问题数据集，匹配到免费基（FreeBase）中的相应条目。
   * **TriviaQA**
     ：一个来自问答竞赛网站的问题集合，每个问题都附有包含答案的网页和维基百科搜索结果。
3. **模型预训练和微调**
   ：使用了T5系列模型的不同版本，包括T5-Base、T5-Large、T5-3B和T5-11B，以及T5.1.1系列模型，通过调整模型大小来测试其性能。
4. **评估方法**
   ：在验证集上进行模型评估，使用精确匹配（Exact Match）作为评估指标。对于无法回答的问题，从验证集中移除并重新计算模型的准确率。
5. **附加实验**
   ：

   * **进一步预训练**
     ：尝试在维基百科上进行进一步预训练，使用片段替换和显著片段掩蔽等技术。
   * **多种任务微调**
     ：尝试同时对多种问答任务进行微调，但发现效果并不明显提升。
   * **随机采样答案**
     ：在开放域Natural Questions中，仅对第一个标注的答案进行训练，实验结果显示随机采样答案对性能无显著影响。

综上所述，本文利用预训练的大型语言模型在不提供外部知识的情况下，对开放域问答任务进行微调，并通过不同规模的模型测试其性能，验证了模型在回答开放域问题时的能力。

## 【模型架构】

该论文主要研究了预训练的语言模型在不利用外部知识的情况下回答开放域问题的能力。论文提出了一个名为T5的模型架构，并通过实验展示了模型大小对性能的影响。以下是模型架构的总结：

1. **模型架构（T5）**
   ：

   * T5是一个基于Transformer的模型，被训练用于填充文本中缺失的部分（用
     `<M>`
     表示）。
   * T5在大规模未结构化文本数据上进行预训练，学习如何在没有外部知识的情况下回答问题。
   * 通过调整模型大小（从Base到11B），研究模型性能随参数数量增加的变化。
2. **预训练过程**
   ：

   * T5模型首先在C4数据集上进行多任务预训练，包括未监督的“片段填充”任务、监督翻译、摘要、分类和阅读理解任务。
   * 为评估不同大小模型的表现，进行了Base（2.2亿参数）、Large（7.7亿参数）、3B（3亿参数）和11B（11亿参数）版本的实验。
   * 使用Salient Span Masking (SSM)技术进一步预训练，特别针对包含实体或日期的片段进行掩码，以增强模型在世界知识方面的理解能力。
3. **微调过程**
   ：

   * 通过在不同数据集（如Natural Questions、WebQuestions和TriviaQA）上的微调来评估模型性能。
   * 使用AdaFactor优化器，设置学习率为0.001，使用10%的dropout率和196,608个token的批量大小。
   * 对于WebQuestions，由于数据较小，调整了批量大小和dropout率。
4. **实验结果**
   ：

   * 随着模型参数量的增加，T5模型在开放域问题上的表现逐渐提高，尤其是在11B和11B + SSM模型上表现最佳。
   * 使用SSM进行额外预训练显著提高了性能，特别是在WebQuestions和TriviaQA上取得了最先进的结果。
5. **结论**
   ：

   * 大型预训练语言模型可以在没有外部知识的情况下取得竞争性的开放域问答性能。
   * 这表明设计问答系统时可以采用不同的方法，模型越大，存储的知识越多，回答问题的能力越强。
   * 需要更高效的模型设计，并进一步研究如何确保模型在预训练过程中获得特定知识以及如何更新或删除预训练模型中的知识。

总结来说，该模型架构通过预训练和微调过程展示了语言模型在开放域问答任务中的潜力，特别是通过增加模型规模来提升性能。

## 【创新点】

该论文的创新点主要包括：

1. **大型语言模型的知识存储能力**
   ：研究表明，通过预训练的大规模语言模型可以在不依赖外部知识库的情况下，回答开放领域的问答任务，表现出色。这表明这些模型能够隐式地存储和检索大量的知识。
2. **参数规模的扩展性**
   ：实验表明，随着模型规模的增加，其知识检索能力也随之增强。最强大的模型（约110亿参数）在多个开放领域问答任务中表现出最佳效果。
3. **知识存储的不可解释性**
   ：与需要从外部知识库检索信息的模型不同，这些大型语言模型是通过预训练将知识存储在其参数中，这种存储方式是不可解释的。当模型不确定时，它会生成看起来合理但实际上是虚构的答案。
4. **开放领域问题回答的新方法**
   ：这为设计问答系统提供了一种新的方法，即模型无需访问外部知识库即可回答问题。这种方法在资源受限的环境中可能过于昂贵，因此需要开发更高效的模型。
5. **增强的鲁棒性**
   ：通过使用极大似然估计目标进行模型训练，虽然可以预训练模型学习特定的事实，但无法保证模型总是学习到所需的知识，这使得难以确保模型在预训练过程中获得特定知识。
6. **改进的数据集和评估方法**
   ：引入了新的数据集（如Natural Questions）和评估方法，以更准确地评估模型在开放领域问答任务中的表现。通过手动评估，发现当前评估方法可能低估了封闭式问答系统的性能。

这些创新点展示了大型语言模型在开放领域问答任务上的潜力，以及这种方法在未来研究中的应用前景。

## 【应用场景】

论文《How Much Knowledge Can You Pack Into the Parameters of a Language Model?》探讨了语言模型在没有外部知识的情况下，通过预训练来存储和检索知识的有效性。具体应用场景如下：

#### 1. **应用场景概述**

* **开放域问答（Open-domain Question Answering，简称ODQA）**
  ：模型在没有外部知识源的情况下，直接从其参数中检索知识以回答自然语言查询。
* **零样本提问**
  ：模型能够在没有任何上下文信息的情况下回答问题，就像学生在闭卷考试中需要独立回忆和应用已学知识。

#### 2. **主要技术及其应用场景**

* **T5模型**
  ：T5是一种预训练的文本到文本变换器模型，用于生成自然语言文本。在本研究中，T5被用来预训练以填充文档中的缺失文本跨度（denoted by ），并通过微调来回答问题。
* **预训练和微调（Pre-training and Fine-tuning）**
  ：模型首先在大规模的未标记文本上进行预训练，学习到一定的知识库，然后通过微调来解决特定任务，如开放域问答。
* **参数掩码预训练（Salient Span Masking Pre-training）**
  ：通过掩码关键实体或日期等关键短语，进一步增强模型在开放域问答任务上的表现。

#### 3. **应用场景的具体描述**

* **自然问题（Natural Questions）**
  ：模型被用来回答从网络查询中提取的问题，这些问题可能没有明确的答案，模型需要从其内部存储的知识中提取信息。
* **维基百科问答（TriviaQA）**
  ：模型从维基百科文章中获取信息，回答与维基百科相关的开放域问答问题。
* **网络问题（WebQuestions）**
  ：模型从网页查询中提取信息，回答与网站内容相关的开放域问答问题。

#### 4. **实验和结果**

* **数据集**
  ：研究使用了多个开放域问答数据集，包括自然问题、维基百科问题和TrivaQA。
* **模型大小的影响**
  ：模型大小的增加提高了其在开放域问答任务上的性能，尤其是在使用大量参数的模型上表现更佳。
* **参数掩码预训练的效果**
  ：使用参数掩码预训练的方法显著提高了模型在开放域问答任务上的表现。
* **闭卷问答**
  ：模型在没有外部知识的情况下表现良好，类似于学生在闭卷考试中需要独立回忆和应用已学知识。

#### 5. **结论**

* **知识存储与检索**
  ：大型语言模型可以通过预训练隐式地存储大量知识，而无需外部知识源。
* **未来工作方向**
  ：研究指出未来工作的几个方向，包括更高效的语言模型设计、更具解释性的模型以及更复杂推理能力的评估。
* **人类评估**
  ：研究还通过人类评估验证了模型在闭卷问答任务上的表现，揭示了模型在某些情况下仍存在局限性。

这些应用展示了大型语言模型在开放域问答任务中的潜力，同时也指出了未来研究的方向。

## 【未来展望】

#### 技术未来展望：How Much Knowledge Can You Pack Into the Parameters of a Language Model?

##### 摘要

近年来，预训练的神经语言模型在未标记文本上训练后，能够隐式地存储和检索知识，使用自然语言查询。本文通过微调预训练模型来回答问题，而无需任何外部上下文或知识，来测量这一方法的实际使用价值。结果显示，该方法随着模型规模的增加而扩展，并在回答问题时与从外部知识源显式检索答案的开放式系统竞争。为了促进可重现性和未来工作，我们发布了代码和训练模型。本文研究了大型语言模型通过预训练存储和检索知识的能力。

##### 1. 引言

大型、深度神经语言模型在未经标记文本上预训练后，在下游自然语言处理（NLP）任务上表现出极高的性能。有趣的是，这些模型在预训练后还能内化一种隐式的“知识库”。这在两个方面具有潜在用处：1) 知识是通过预训练在大量未标记文本数据上积累的，这些数据在网络上广泛可用；2) 可以使用自然语言查询检索信息，因为这些预训练的语言模型在自然语言理解任务上表现出色。

本文通过微调模型来回答问题，而不需要任何外部知识或上下文，来评估语言模型在开放式领域问答任务上的能力。这种方法要求模型解析自然语言查询，并在其参数中“查找”信息。以往关于“语言模型作为知识库”的工作通常通过合成任务来理解模型中存储的信息范围或评估推理能力。本文采用不同的方法，通过评估语言模型在开放式领域问答任务上的能力，来研究模型规模对知识检索能力的影响。

##### 2. 背景

**问答任务**
  
训练模型以选择或输出给定问题的正确答案的任务称为“问答”。最流行的变体是为模型提供一些包含答案的“上下文”（例如，从维基百科文章中获取的段落）以及问题。模型可以被训练以指示包含答案的上下文段落，或者直接输出答案文本。这种格式可以被视为阅读一些文本并回答关于它的问题，因此被称为“阅读理解”。

更困难的变体是“开放式领域问答”，模型可以被问到任意上下文无关的问题（例如，知名事实或历史细节）。通常假设模型在回答问题时可以访问外部知识库（例如，结构化的知识库或未结构化的文本库），但不会给模型提供关于答案出现的具体位置的信息。阅读理解可以被视为一种简化版本的开放式领域问答，模型可以提供“Oracle”上下文来回答问题。开放式领域问答系统可以被视为类似开放书考试，可以找到并在外部知识源中使用信息。

##### 3. 实验

**数据集**
  
本文考虑了以下开放式领域问答数据集：

* **Natural Questions**
  ：包含从网络查询中提取的问题及其对应的维基百科文章。
* **WebQuestions**
  ：从网络查询匹配到FreeBase中的对应条目。
* **TriviaQA**
  ：来自问答网站的问题集合，每个问题配有网页搜索结果，可能包含答案。

本文仅使用每个数据集的问题，而不考虑为每个问题提供的匹配文档。对于WebQuestions和TriviaQA，遵循标准评估程序，将预测答案与标准答案进行比较。对于Natural Questions，使用两种评估方法：一种是标准的“开放式领域”版本，模型仅需生成一个归一化答案；另一种是标准的多答案版本，用于阅读理解系统。

**训练**
  
本文使用Raffel等人提供的预训练模型——“Text-to-Text Transfer Transformer”（T5）。T5模型在包含未监督“跨度破坏”任务的多任务混合数据集（C4）上进行了预训练，还包括翻译、总结、分类和阅读理解等任务。T5模型在预训练过程中接触到了问答数据集，因此报告了“T5.1.1”检查点的性能。对于T5模型的微调，遵循Raffel等人（2019）的方法，使用AdaFactor优化器，设置常数学习率为0.001，10%的dropout率和196,608个令牌的批量大小。对于WebQuestions，由于其规模较小，将批量大小减半，dropout率加倍。对于T5.1.1检查点，使用相同的程序，但所有三个数据集的dropout率设置为5%。

##### 4. 结论

本文展示了大型语言模型通过预训练在未标记文本上，能够在没有外部知识的情况下，达到开放式领域问答基准的竞争力结果。这表明设计问答系统的不同方法，引发了未来工作的多个方向：1) 我们仅用最大的模型（约110亿参数）获得了最先进的结果。这种模型规模在资源受限的环境中可能过于昂贵，需要更高效的语言模型。2) “开放式领域”模型通常在回答问题时会提供关于访问了哪些信息的指示。这可以提供有用的可解释性。相比之下，我们的模型以不可解释的方式分布在参数中存储知识，并在不确定时生成看起来真实的答案。3) 用于训练模型的最大似然目标不能保证模型会学习到特定的事实。这使得确保模型在预训练过程中获得特定知识变得困难，并且无法明确更新或从预训练模型中删除知识。4) 本文使用的任务主要测量“琐碎”型知识。因此，我们对需要推理能力（如DROP）的问答任务感兴趣。

##### 未来展望

随着模型规模的增加，语言模型能够存储和检索更多知识的能力得到了验证。虽然目前的模型在开放式问答任务上表现良好，但未来的研究可以探索如何更高效地训练和使用这些模型。此外，如何理解和改进模型的推理能力也是未来研究的重点。未来的工作可以进一步研究如何在更复杂的数据集上应用这些模型，以及如何使这些模型更好地适应不同的应用需求。未来的研究还可以探索如何通过更好的训练策略和模型架构来提高模型的性能。

总之，本文的工作展示了大型语言模型在开放式领域问答任务上的潜力，为进一步的研究和应用奠定了基础。未来的研究将继续探索如何更好地利用这些模型的能力，以应对更复杂和多样的任务。

## 【附录】

为了实现论文中提到的关键技术，我们可以使用T5模型来填充文档中的缺失部分。以下是一个简化的伪代码实现，用于说明如何训练和微调T5模型来完成开放域问答任务。

#### 伪代码实现

```plaintext
# 1. 准备数据集
# - 选择开放域问题回答数据集，如Natural Questions, WebQuestions, TriviaQA
# - 对数据集进行预处理，提取问题和答案

# 2. 加载预训练的T5模型
model = T5.load_pretrained_model("t5-base")  # 可以选择不同大小的T5模型

# 3. 配置训练参数
# - 学习率
learning_rate = 0.001
# - 优化器
optimizer = Adam(learning_rate=learning_rate)
# - 预处理和后处理函数
preprocess_fn = lambda text: text.replace("<M>", "[MASK]")  # 替换缺失部分
postprocess_fn = lambda text: text.replace("[MASK]", "<M>")  # 恢复缺失部分
# - 批量大小
batch_size = 196608  # 以词为单位

# 4. 训练模型
for epoch in range(epochs):
    for batch in DataLoader(dataset, batch_size=batch_size, shuffle=True):
        # 4.1 前向传播
        outputs = model(batch, return_dict=True)
        loss = outputs.loss
        
        # 4.2 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # 4.3 后处理和验证
        if epoch % validation_interval == 0:
            # 验证模型在验证集上的性能
            validate(model, validation_dataset, preprocess_fn, postprocess_fn)

```

#### 详细说明

1. **数据集准备**
   ：

   * 选择开放域问题回答数据集，如Natural Questions, WebQuestions, TriviaQA。
   * 对数据集进行预处理，提取问题和答案。对于每个问题，需要将其转换为可以输入T5模型的形式，例如，将缺失部分用
     `<M>`
     标记。
2. **加载预训练的T5模型**
   ：

   * 使用
     `T5`
     类加载预训练的T5模型。可以选择不同大小的模型，如
     `T5-base`
     ,
     `T5-large`
     ,
     `T5-3B`
     等。
3. **配置训练参数**
   ：

   * 设置学习率、优化器、预处理和后处理函数。预处理函数将
     `<M>`
     替换为
     `[MASK]`
     ，后处理函数将
     `[MASK]`
     替换回
     `<M>`
     。
4. **训练模型**
   ：

   * 通过迭代训练数据集来训练模型。
   * 在每个epoch结束后，使用验证集验证模型性能。

#### 代码实现

以下是一个使用Hugging Face
`transformers`
库实现的简化代码示例：

```python
from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW
from torch.utils.data import DataLoader
from torch.optim import AdamW

# 1. 加载预训练的T5模型
model = T5ForConditionalGeneration.from_pretrained('t5-base')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

# 2. 配置训练参数
learning_rate = 0.001
optimizer = AdamW(model.parameters(), lr=learning_rate)

# 3. 准备数据集
def preprocess_example(example):
    question = example['question']
    answer = example['answer']
    input_text = f"fill {question} with {answer}"
    input_ids = tokenizer.encode(input_text, return_tensors='pt')
    target_text = f"answer: {answer}"
    target_ids = tokenizer.encode(target_text, return_tensors='pt')
    return input_ids, target_ids

dataset = [...]  # 假设已经准备好数据集
data_loader = DataLoader(dataset, batch_size=16, shuffle=True)

# 4. 训练模型
epochs = 10
for epoch in range(epochs):
    for batch in data_loader:
        input_ids, target_ids = batch
        outputs = model(input_ids=input_ids, labels=target_ids)
        loss = outputs.loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item()}")

# 5. 评估模型
def evaluate(model, data_loader):
    model.eval()
    total_loss = 0
    for batch in data_loader:
        input_ids, target_ids = batch
        with torch.no_grad():
            outputs = model(input_ids=input_ids, labels=target_ids)
            loss = outputs.loss
            total_loss += loss.item()
    avg_loss = total_loss / len(data_loader)
    print(f"Validation Loss: {avg_loss}")
    model.train()

evaluate(model, validation_loader)

```

这个代码示例展示了如何使用Hugging Face的
`transformers`
库来训练和评估T5模型，以完成开放域问答任务。实际使用中可能需要根据具体需求调整数据预处理和模型配置。

## 【OpenSpace】

#### 开放性讨论：语言模型能够存储多少知识？

##### 背景

近年来，预训练的语言模型在无监督环境下，通过对大量未标注文本进行训练，展现出强大的性能。特别是经过微调后的语言模型在下游自然语言处理任务（NLP）中取得了显著成果。然而，最近的研究发现，这些模型在预训练过程中还能够内部化一种隐式的“知识库”，并且可以在不依赖外部知识库的情况下回答问题。这种现象引发了对语言模型作为知识库能力的深入研究。

##### 思考问题

1. **模型大小与知识存储量的关系**
   ：

   * 研究表明，模型的参数量越大，其内部化知识的能力越强。例如，T5模型从Base到11B参数版本，性能随着参数量的增加而逐步提升。
   * 这是因为大规模模型能够更好地捕捉和存储大量的语言模式和信息。因此，模型的参数量可以作为衡量其知识存储能力的一个指标。
2. **闭卷问答（Closed-Book QA）与开放问答（Open-Book QA）**
   ：

   * 闭卷问答是指模型在回答问题时不能访问任何外部知识，必须依靠其内部化的知识来回答问题。这种设置可以更真实地评估模型的内部知识存储能力。
   * 开放问答模型则可以访问外部知识库，这为它们提供了更多的信息来源，但同时也降低了评估的难度。因此，闭卷问答提供了一种更为严格的评估标准。
3. **知识存储的隐秘性**
   ：

   * 语言模型内部化知识的方式通常是隐秘的，模型在回答问题时会根据其内部的知识库生成答案。这不同于传统的知识存储方式，模型不需要显式地存储知识，而是通过训练过程中学习到的模式来推断和生成答案。
   * 这种隐秘性使得评估模型的知识存储能力更加困难，需要设计特定的任务和评估标准来衡量模型的性能。
4. **知识存储的深度和广度**
   ：

   * 研究表明，模型能够存储大量的知识，但这些知识的深度和广度仍有待进一步探究。例如，模型在回答封闭领域的问题时可能表现良好，但在处理更复杂、需要推理能力的问题时可能表现不佳。
   * 这种现象提示我们，语言模型内部化知识的过程可能涉及复杂的模式学习和推理机制，需要进一步研究来揭示其背后的机制。
5. **未来研究方向**
   ：

   * 需要进一步研究如何提高模型的知识存储能力，包括设计更有效的预训练任务、探索新的知识表示方法等。
   * 同时，需要设计更复杂的评估任务来全面评估模型的知识存储和推理能力，包括那些需要多层次推理和背景理解的任务。

##### 结论

语言模型在预训练过程中能够隐式地存储大量知识，并在闭卷问答任务中表现出强大的能力。这为自然语言处理领域提供了一种新的知识表示和存储方式。未来的研究将进一步探索这一现象背后的机制，并设计更为复杂的评估任务来全面评估模型的能力。

---

以上讨论为开放性问题，旨在引发更多的思考和探讨。如果您有具体的问题或观点，欢迎进一步交流！