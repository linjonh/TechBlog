---
layout: post
title: "第三十天Scrapy-框架-分布式"
date: 2025-03-06 10:33:51 +0800
description: "分布式"
keywords: "第三十天：Scrapy 框架-分布式"
categories: ['Python']
tags: ['爬虫', '分布式', 'Scrapy']
artid: "145709028"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145709028
    alt: "第三十天Scrapy-框架-分布式"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145709028
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145709028
cover: https://bing.ee123.net/img/rand?artid=145709028
image: https://bing.ee123.net/img/rand?artid=145709028
img: https://bing.ee123.net/img/rand?artid=145709028
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     第三十天：Scrapy 框架-分布式
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
    </p>
    <h2 id="%E4%B8%80%E3%80%81%E4%BB%8B%E7%BB%8Dscrapy-redis%E6%A1%86%E6%9E%B6" name="%E4%B8%80%E3%80%81%E4%BB%8B%E7%BB%8Dscrapy-redis%E6%A1%86%E6%9E%B6">
     一、介绍scrapy-redis框架
    </h2>
    <p>
     scrapy-redis
    </p>
    <blockquote>
     <p>
      一个三方的基于redis的分布式爬虫框架，配合scrapy使用，让爬虫具有了分布式爬取的功能。
     </p>
    </blockquote>
    <p>
     github地址：
     <a class="link-info" href="https://github.com/darkrho/scrapy-redis" title="https://github.com/darkrho/scrapy-redis">
      https://github.com/darkrho/scrapy-redis
     </a>
    </p>
    <h2 id="%E4%BA%8C%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86" name="%E4%BA%8C%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%8E%9F%E7%90%86">
     二、分布式原理
    </h2>
    <p>
     scrapy-redis实现分布式，其实从原理上来说很简单，这里为描述方便，我们把自己的核心服务器称为master，而把用于跑爬虫程序的机器称为slave
    </p>
    <p>
     我们知道，采用scrapy框架抓取网页，我们需要首先给定它一些start_urls，爬虫首先访问start_urls里面的url，再根据我们的具体逻辑，对里面的元素、或者是其他的二级、三级页面进行抓取。而要实现分布式，我们只需要在这个starts_urls里面做文章就行了
    </p>
    <p>
     我们在master上搭建一个redis数据库`（注意这个数据库只用作url的存储)，并对每一个需要爬取的网站类型，都开辟一个单独的列表字段。通过设置slave上scrapy-redis获取url的地址为master地址。这样的结果就是，尽管有多个slave，然而大家获取url的地方只有一个，那就是服务器master上的redis数据库
    </p>
    <p>
     并且，由于scrapy-redis自身的队列机制，slave获取的链接不会相互冲突。这样各个slave在完成抓取任务之后，再把获取的结果汇总到服务器上
    </p>
    <p>
     <strong>
      好处
     </strong>
    </p>
    <p>
     程序移植性强，只要处理好路径问题，把slave上的程序移植到另一台机器上运行，基本上就是复制粘贴的事情
    </p>
    <h2 id="%E4%B8%89%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E7%9A%84%E5%AE%9E%E7%8E%B0" name="%E4%B8%89%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E7%9A%84%E5%AE%9E%E7%8E%B0">
     三、分布式爬虫的实现
    </h2>
    <ol>
     <li>
      使用三台机器，一台是win10，两台是centos6，分别在两台机器上部署scrapy来进行分布式抓取一个网站
     </li>
     <li>
      win10的ip地址为192.168.31.245，用来作为redis的master端，centos的机器作为slave
     </li>
     <li>
      master的爬虫运行时会把提取到的url封装成request放到redis中的数据库：“dmoz:requests”，并且从该数据库中提取request后下载网页，再把网页的内容存放到redis的另一个数据库中“dmoz:items”
     </li>
     <li>
      slave从master的redis中取出待抓取的request，下载完网页之后就把网页的内容发送回master的redis
     </li>
     <li>
      重复上面的3和4，直到master的redis中的“dmoz:requests”数据库为空，再把master的redis中的“dmoz:items”数据库写入到mongodb中
     </li>
     <li>
      master里的reids还有一个数据“dmoz:dupefilter”是用来存储抓取过的url的指纹（使用哈希函数将url运算后的结果），是防止重复抓取的
     </li>
    </ol>
    <h2 id="%E5%9B%9B%E3%80%81scrapy-redis%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AE%89%E8%A3%85" name="%E5%9B%9B%E3%80%81scrapy-redis%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AE%89%E8%A3%85">
     四、scrapy-redis框架的安装
    </h2>
    <pre><code>pip install scrapy-redis</code></pre>
    <h2 id="%E4%BA%94%E3%80%81%E9%83%A8%E7%BD%B2scrapy-redis" name="%E4%BA%94%E3%80%81%E9%83%A8%E7%BD%B2scrapy-redis">
     五、部署scrapy-redis
    </h2>
    <p>
     1. slave端
     <br/>
     在windows上的settings.py文件的最后增加如下一行
    </p>
    <pre><code>REDIS_HOST = 'localhost' #master IP
​
REDIS_PORT = 6379</code></pre>
    <p>
     配置好了远程的redis地址后启动两个爬虫（启动爬虫没有顺序限制）
    </p>
    <h2 id="%E5%85%AD%E3%80%81%E7%BB%99%E7%88%AC%E8%99%AB%E5%A2%9E%E5%8A%A0%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF" name="%E5%85%AD%E3%80%81%E7%BB%99%E7%88%AC%E8%99%AB%E5%A2%9E%E5%8A%A0%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF">
     六、给爬虫增加配置信息
    </h2>
    <pre><code>DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
SCHEDULER_PERSIST = True
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"
#SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"
​
ITEM_PIPELINES = {
    'example.pipelines.ExamplePipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}</code></pre>
    <h2 id="%E4%B8%83%E3%80%81%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F" name="%E4%B8%83%E3%80%81%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F">
     七、运行程序
    </h2>
    <p>
     运行slave
    </p>
    <pre><code>lpush (redis_key)  url #括号不用写</code></pre>
    <p>
     <strong>
      说明
     </strong>
    </p>
    <ul>
     <li>
      这个命令是在redis-cli中运行
     </li>
     <li>
      redis_key 是 spider.py文件中的redis_key的值
     </li>
     <li>
      url 开始爬取地址，不加双引号
     </li>
    </ul>
    <h2 id="%E5%85%AB%E3%80%81%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0mongodb%E4%B8%AD" name="%E5%85%AB%E3%80%81%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0mongodb%E4%B8%AD">
     八、数据导入到mongodb中
    </h2>
    <p>
     等到爬虫结束后,如果要把数据存储到mongodb中，就应该修改master端process_items.py文件，如下
    </p>
    <pre><code>import redis
​
import pymongo
​
def main():
​
    r = redis.Redis(host='192.168.31.245',port=6379,db=0)
​
    client = pymongo.MongoClient(host='localhost', port=27017)
​
    db = client.dmoz
​
    sheet = db.sheet
​
    while True:
​
​
        source, data = r.blpop(["dmoz:items"])
​
        item = json.loads(data)
​
        sheet.insert(item)
​
if __name__ == '__main__':
​
    main()</code></pre>
    <h2 id="%E4%B9%9D%E3%80%81%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0MySQL%E4%B8%AD" name="%E4%B9%9D%E3%80%81%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0MySQL%E4%B8%AD">
     九、数据导入到MySQL中
    </h2>
    <p>
     等到爬虫结束后,如果要把数据存储到MySQL中，就应该修改master端process_items.py文件，如下
    </p>
    <pre><code>import redis
import pymysql
import json
def process_item():
    r_client = redis.Redis(host="127.0.0.1",port=6379,db =0)
    m_client = pymysql.connect(host="127.0.0.1",port=3306,user="root",passowrd="123456",db="lianjia")
    source,data =r_client.blpop("lianjia:item")
    item = json.loads(data)
​
    cursor = m_client.cursor()
    values = []
    cursor.execute(sql,values)</code></pre>
    <h2 id="%E5%8D%81%E3%80%81setting%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE" name="%E5%8D%81%E3%80%81setting%E6%96%87%E4%BB%B6%E9%85%8D%E7%BD%AE">
     十、setting文件配置
    </h2>
    <pre><code>#启用Redis调度存储请求队列
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
 
#确保所有的爬虫通过Redis去重
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
 
#默认请求序列化使用的是pickle 但是我们可以更改为其他类似的。PS：这玩意儿2.X的可以用。3.X的不能用
#SCHEDULER_SERIALIZER = "scrapy_redis.picklecompat"
 
#不清除Redis队列、这样可以暂停/恢复 爬取
#SCHEDULER_PERSIST = True
 
#使用优先级调度请求队列 （默认使用）
#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.PriorityQueue'
#可选用的其它队列
#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.FifoQueue'
#SCHEDULER_QUEUE_CLASS = 'scrapy_redis.queue.LifoQueue'
 
#最大空闲时间防止分布式爬虫因为等待而关闭
#这只有当上面设置的队列类是SpiderQueue或SpiderStack时才有效
#并且当您的蜘蛛首次启动时，也可能会阻止同一时间启动（由于队列为空）
#SCHEDULER_IDLE_BEFORE_CLOSE = 10
 
#将清除的项目在redis进行处理
ITEM_PIPELINES = {
    'scrapy_redis.pipelines.RedisPipeline': 300
}
 
#序列化项目管道作为redis Key存储
#REDIS_ITEMS_KEY = '%(spider)s:items'
 
#默认使用ScrapyJSONEncoder进行项目序列化
#You can use any importable path to a callable object.
#REDIS_ITEMS_SERIALIZER = 'json.dumps'
 
#指定连接到redis时使用的端口和地址（可选）
#REDIS_HOST = 'localhost'
#REDIS_PORT = 6379
 
#指定用于连接redis的URL（可选）
#如果设置此项，则此项优先级高于设置的REDIS_HOST 和 REDIS_PORT
#REDIS_URL = 'redis://user:pass@hostname:9001'
 
#自定义的redis参数（连接超时之类的）
#REDIS_PARAMS  = {}
 
#自定义redis客户端类
#REDIS_PARAMS['redis_cls'] = 'myproject.RedisClient'
 
#如果为True，则使用redis的'spop'进行操作。
#如果需要避免起始网址列表出现重复，这个选项非常有用。开启此选项urls必须通过sadd添加，否则会出现类型错误。
#REDIS_START_URLS_AS_SET = False
 
#RedisSpider和RedisCrawlSpider默认 start_usls 键
#REDIS_START_URLS_KEY = '%(name)s:start_urls'
 
#设置redis使用utf-8之外的编码
#REDIS_ENCODING = 'latin1'</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6b6f757765697a68752f:61727469636c652f64657461696c732f313435373039303238" class_="artid" style="display:none">
 </p>
</div>


