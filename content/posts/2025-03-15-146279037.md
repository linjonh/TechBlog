---
layout: post
title: "如何设置爬虫的延时避免被封禁"
date: 2025-03-15 14:31:42 +0800
description: "合理设置延时是避免爬虫被封禁的重要策略。通过使用设置固定延时、使用随机延时、结合 Scrapy 框架的、使用代理服务器以及结合其他策略，可以有效降低爬虫被封禁的风险。在实际应用中，建议根据目标网站的实际情况灵活调整延时策略。"
keywords: "如何设置爬虫的延时避免被封禁"
categories: ['未分类']
tags: ['爬虫']
artid: "146279037"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146279037
    alt: "如何设置爬虫的延时避免被封禁"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146279037
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146279037
cover: https://bing.ee123.net/img/rand?artid=146279037
image: https://bing.ee123.net/img/rand?artid=146279037
img: https://bing.ee123.net/img/rand?artid=146279037
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     如何设置爬虫的延时避免被封禁
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     在使用爬虫获取数据时，合理设置延时是避免被目标网站封禁的关键策略之一。以下是一些常见的方法和技巧：
    </p>
    <h3>
     一、使用
     <code>
      time.sleep()
     </code>
     设置固定延时
    </h3>
    <p>
     <code>
      time.sleep()
     </code>
     是 Python 中最常用的延时方法，可以在每次请求之间设置固定的延时，从而降低请求频率。
    </p>
    <p>
     Python
    </p>
    <pre><code class="language-python">import time
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
}

urls = ["http://example.com/page1", "http://example.com/page2"]  # 示例URL列表

for url in urls:
    response = requests.get(url, headers=headers)
    time.sleep(1)  # 每次请求间隔1秒</code></pre>
    <h3>
     二、使用随机延时
    </h3>
    <p>
     为了更好地模拟真实用户的行为，可以设置随机延时。这可以通过
     <code>
      random.uniform()
     </code>
     或
     <code>
      random.randint()
     </code>
     实现。
    </p>
    <p>
     Python
    </p>
    <pre><code class="language-python">import time
import random
import requests

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
}

urls = ["http://example.com/page1", "http://example.com/page2"]  # 示例URL列表

for url in urls:
    response = requests.get(url, headers=headers)
    delay = random.uniform(0.5, 2.0)  # 随机延时0.5到2秒
    time.sleep(delay)</code></pre>
    <h3>
     三、使用 Scrapy 框架的
     <code>
      DOWNLOAD_DELAY
     </code>
     设置延时
    </h3>
    <p>
     如果你使用的是 Scrapy 框架，可以通过设置
     <code>
      DOWNLOAD_DELAY
     </code>
     来控制请求间隔。
    </p>
    <p>
     Python
    </p>
    <pre><code class="language-python"># 在 settings.py 中设置
DOWNLOAD_DELAY = 2  # 每次请求间隔2秒</code></pre>
    <p>
     此外，还可以结合
     <code>
      RandomDelayMiddleware
     </code>
     实现随机延时。
    </p>
    <h3>
     四、使用代理服务器
    </h3>
    <p>
     使用代理服务器可以隐藏真实 IP 地址，降低被封禁的风险。可以结合
     <code>
      requests
     </code>
     库使用代理。
    </p>
    <p>
     Python
    </p>
    <pre><code class="language-python">import requests
import random

proxy_list = [
    {"http": "http://proxy1.example.com:8080"},
    {"http": "http://proxy2.example.com:8080"},
]

url = "http://example.com/data"

for _ in range(5):  # 示例：发送5次请求
    proxy = random.choice(proxy_list)
    response = requests.get(url, proxies=proxy)
    time.sleep(1)  # 每次请求间隔1秒</code></pre>
    <h3>
     五、结合其他策略
    </h3>
    <p>
     除了设置延时，还可以结合其他策略来降低被封禁的风险：
    </p>
    <ul>
     <li>
      <p>
       <strong>
        随机更换 User-Agent
       </strong>
       ：模拟不同的浏览器访问。
      </p>
     </li>
     <li>
      <p>
       <strong>
        遵守
        <code>
         robots.txt
        </code>
        文件
       </strong>
       ：遵循目标网站的爬取规则。
      </p>
     </li>
     <li>
      <p>
       <strong>
        使用会话和 Cookies
       </strong>
       ：模拟真实用户的浏览行为。
      </p>
     </li>
    </ul>
    <h3>
     六、总结
    </h3>
    <p>
     合理设置延时是避免爬虫被封禁的重要策略。通过使用
     <code>
      time.sleep()
     </code>
     设置固定延时、使用随机延时、结合 Scrapy 框架的
     <code>
      DOWNLOAD_DELAY
     </code>
     、使用代理服务器以及结合其他策略，可以有效降低爬虫被封禁的风险。在实际应用中，建议根据目标网站的实际情况灵活调整延时策略。
    </p>
   </div>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f323430315f38373834393330382f:61727469636c652f64657461696c732f313436323739303337" class_="artid" style="display:none">
 </p>
</div>


