---
layout: post
title: "01-简单几步在Windows上用llama.cpp运行DeepSeek-R1模型"
date: 2025-03-08 18:59:47 +0800
description: "Llama.cpp 是一个开源的、轻量级的项目，旨在实现 Meta 推出的开源大语言模型 Llama 的推理（inference）。Llama 是 Meta 在 2023 年开源的一个 70B 参数的高质量大语言模型，而 llama.cpp 是一个用 C++ 实现的轻量化推理端解决方案，适用于运行和测试 Llama 模型。1.轻量化：llama.cpp 是一个非常轻量级的项目，代码简洁且易于编译，适合快速上手和测试。2.开源：完全开源，代码和模型权重都可以自由获取和使用。"
keywords: "01-简单几步！在Windows上用llama.cpp运行DeepSeek-R1模型"
categories: ['Ai']
tags: ['Llama']
artid: "146120311"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146120311
    alt: "01-简单几步在Windows上用llama.cpp运行DeepSeek-R1模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146120311
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146120311
cover: https://bing.ee123.net/img/rand?artid=146120311
image: https://bing.ee123.net/img/rand?artid=146120311
img: https://bing.ee123.net/img/rand?artid=146120311
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     01-简单几步！在Windows上用llama.cpp运行DeepSeek-R1模型
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/b0e364bd41c142c3a91ee5e07e579d06.png"/>
    </p>
    <h2>
     <a id="1llamacpp_2">
     </a>
     1.llama.cpp介绍
    </h2>
    <p>
     Llama.cpp 是一个开源的、轻量级的项目，旨在实现 Meta 推出的开源大语言模型 Llama 的推理（inference）。Llama 是 Meta 在 2023 年开源的一个 70B 参数的高质量大语言模型，而 llama.cpp 是一个用 C++ 实现的轻量化推理端解决方案，适用于运行和测试 Llama 模型。
    </p>
    <p>
     <strong>
      特点
     </strong>
    </p>
    <p>
     1.轻量化：llama.cpp 是一个非常轻量级的项目，代码简洁且易于编译，适合快速上手和测试。
     <br/>
     2.开源：完全开源，代码和模型权重都可以自由获取和使用。
     <br/>
     3.支持多种模型：支持 Llama 的不同版本（如 7B、14B、30B、70B 参数量），用户可以根据需求选择。
     <br/>
     4.跨平台：支持在多种操作系统（如 Linux、Windows、macOS）上运行。
     <br/>
     5.易于集成：代码结构简单，适合开发者快速集成到自己的项目中。
    </p>
    <p>
     <strong>
      应用场景
     </strong>
    </p>
    <ul>
     <li>
      个人项目：开发者可以快速使用 Llama 模型进行文本生成、对话机器人等实验。
     </li>
     <li>
      教育和研究：适合学习和研究大语言模型的实现和应用。
     </li>
     <li>
      小规模部署：对于小型项目或个人用途，llama.cpp 提供了一个方便的解决方案。
     </li>
    </ul>
    <h2>
     <a id="2llamacpp_18">
     </a>
     2.编译llama.cpp
    </h2>
    <h3>
     <a id="21__19">
     </a>
     2.1 环境准备
    </h3>
    <ul>
     <li>
      安装cmake:https://cmake.org/download/
     </li>
     <li>
      安装visual studio 2022
     </li>
     <li>
      安装git
     </li>
    </ul>
    <h3>
     <a id="22_llamacpp_25">
     </a>
     2.2 编译llama.cpp
    </h3>
    <p>
     确保环境准备ok，就可以执行下述命令进行编译了！
    </p>
    <pre><code>git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build
cd build
cmake .. -G "Visual Studio 17 2022" -A x64
cmake --build . --config Release -- /m
</code></pre>
    <p>
     编译成功后，这里生成了很多成果物，后面我们就要用其中的一些来运行我们的模型。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/b129aed7f28c4348a16cba13bf2ba837.png"/>
    </p>
    <h2>
     <a id="3_38">
     </a>
     3.运行模型
    </h2>
    <p>
     运行模型前需要下载好模型文件，llama.cpp支持gguf格式的模型文件。我们可以去huggineface上面下载。下面是一个比较小的模型，有多个不同的量化版本，下载其中一个就行。
     <br/>
     下载页面如下：https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/tree/main
    </p>
    <h3>
     <a id="31__41">
     </a>
     3.1 命令模式
    </h3>
    <p>
     使用llama-cli.exe来运行模型，命令如下(记住要换成模型文件的实际路径)：
    </p>
    <pre><code>./llama-cli -m "C:\Users\51559\AppData\Local\nomic.ai\GPT4All\DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf"
</code></pre>
    <p>
     上述命令执行后，就可以愉快地玩耍了～
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/d753bf5509ca4878be2eefc644d4b1b1.png"/>
    </p>
    <h3>
     <a id="32__49">
     </a>
     3.2 服务模式
    </h3>
    <p>
     如果你觉得命令模式运起来不舒服，可以用服务模式，服务模式会运行一个web，可以直接在ｗｅｂ上进行对话。
    </p>
    <pre><code>./llama-server -m "C:\Users\51559\AppData\Local\nomic.ai\GPT4All\DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf" --port 8080
</code></pre>
    <p>
     <img alt="" src="https://i-blog.csdnimg.cn/direct/3993c8f3e481491198a903c3de8e8d68.png">
      <br/>
      当我们看到上面的输出后，就可以在浏览器通过这个地址进行对话了：http://127.0.0.1:8080/
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/3f983a0ae01c4c6babb6533a28a225e2.png"/>
     </img>
    </p>
    <p>
     好了，deepseek-r1模型就在本地部署起来了。但是我们今天使用的是cpu，如果你下载的模型比较大，而你的电脑配置又比较差，可能运行起来不是那么流畅。
     <br/>
     下一节，我来教大家如何使用本地GPU，让模型更加流畅地运行！
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f616e6461303130392f:61727469636c652f64657461696c732f313436313230333131" class_="artid" style="display:none">
 </p>
</div>


