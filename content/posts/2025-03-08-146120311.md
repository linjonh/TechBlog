---
arturl_encode: "68747470733a:2f2f626c6f672e6373646e2e6e65742f616e6461303130392f:61727469636c652f64657461696c732f313436313230333131"
layout: post
title: "01-简单几步在Windows上用llama.cpp运行DeepSeek-R1模型"
date: 2025-03-08 18:59:47 +0800
description: "Llama.cpp 是一个开源的、轻量级的项目，旨在实现 Meta 推出的开源大语言模型 Llama 的推理（inference）。Llama 是 Meta 在 2023 年开源的一个 70B 参数的高质量大语言模型，而 llama.cpp 是一个用 C++ 实现的轻量化推理端解决方案，适用于运行和测试 Llama 模型。1.轻量化：llama.cpp 是一个非常轻量级的项目，代码简洁且易于编译，适合快速上手和测试。2.开源：完全开源，代码和模型权重都可以自由获取和使用。"
keywords: "01-简单几步！在Windows上用llama.cpp运行DeepSeek-R1模型"
categories: ['Ai']
tags: ['Llama']
artid: "146120311"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146120311
    alt: "01-简单几步在Windows上用llama.cpp运行DeepSeek-R1模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146120311
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146120311
cover: https://bing.ee123.net/img/rand?artid=146120311
image: https://bing.ee123.net/img/rand?artid=146120311
img: https://bing.ee123.net/img/rand?artid=146120311
---

# 01-简单几步！在Windows上用llama.cpp运行DeepSeek-R1模型

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b0e364bd41c142c3a91ee5e07e579d06.png)

## 1.llama.cpp介绍

Llama.cpp 是一个开源的、轻量级的项目，旨在实现 Meta 推出的开源大语言模型 Llama 的推理（inference）。Llama 是 Meta 在 2023 年开源的一个 70B 参数的高质量大语言模型，而 llama.cpp 是一个用 C++ 实现的轻量化推理端解决方案，适用于运行和测试 Llama 模型。

**特点**

1.轻量化：llama.cpp 是一个非常轻量级的项目，代码简洁且易于编译，适合快速上手和测试。
  
2.开源：完全开源，代码和模型权重都可以自由获取和使用。
  
3.支持多种模型：支持 Llama 的不同版本（如 7B、14B、30B、70B 参数量），用户可以根据需求选择。
  
4.跨平台：支持在多种操作系统（如 Linux、Windows、macOS）上运行。
  
5.易于集成：代码结构简单，适合开发者快速集成到自己的项目中。

**应用场景**

* 个人项目：开发者可以快速使用 Llama 模型进行文本生成、对话机器人等实验。
* 教育和研究：适合学习和研究大语言模型的实现和应用。
* 小规模部署：对于小型项目或个人用途，llama.cpp 提供了一个方便的解决方案。

## 2.编译llama.cpp

### 2.1 环境准备

* 安装cmake:https://cmake.org/download/
* 安装visual studio 2022
* 安装git

### 2.2 编译llama.cpp

确保环境准备ok，就可以执行下述命令进行编译了！

```
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
mkdir build
cd build
cmake .. -G "Visual Studio 17 2022" -A x64
cmake --build . --config Release -- /m

```

编译成功后，这里生成了很多成果物，后面我们就要用其中的一些来运行我们的模型。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/b129aed7f28c4348a16cba13bf2ba837.png)

## 3.运行模型

运行模型前需要下载好模型文件，llama.cpp支持gguf格式的模型文件。我们可以去huggineface上面下载。下面是一个比较小的模型，有多个不同的量化版本，下载其中一个就行。
  
下载页面如下：https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/tree/main

### 3.1 命令模式

使用llama-cli.exe来运行模型，命令如下(记住要换成模型文件的实际路径)：

```
./llama-cli -m "C:\Users\51559\AppData\Local\nomic.ai\GPT4All\DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf"

```

上述命令执行后，就可以愉快地玩耍了～
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/d753bf5509ca4878be2eefc644d4b1b1.png)

### 3.2 服务模式

如果你觉得命令模式运起来不舒服，可以用服务模式，服务模式会运行一个web，可以直接在ｗｅｂ上进行对话。

```
./llama-server -m "C:\Users\51559\AppData\Local\nomic.ai\GPT4All\DeepSeek-R1-Distill-Qwen-14B-Q4_0.gguf" --port 8080

```

![](https://i-blog.csdnimg.cn/direct/3993c8f3e481491198a903c3de8e8d68.png)
  
当我们看到上面的输出后，就可以在浏览器通过这个地址进行对话了：http://127.0.0.1:8080/
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/3f983a0ae01c4c6babb6533a28a225e2.png)

好了，deepseek-r1模型就在本地部署起来了。但是我们今天使用的是cpu，如果你下载的模型比较大，而你的电脑配置又比较差，可能运行起来不是那么流畅。
  
下一节，我来教大家如何使用本地GPU，让模型更加流畅地运行！