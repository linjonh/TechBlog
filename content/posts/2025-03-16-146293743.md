---
arturl_encode: "68747470:733a2f2f626c6f672e6373646e2e6e65742f7169777369722f:61727469636c652f64657461696c732f313436323933373433"
layout: post
title: "LLM3-Transformer-架构"
date: 2025-03-16 12:56:58 +0800
description: "大多数现代的大规模语言模型（LLMs）依赖于 Transformer 架构，这是一种在 2017 年的论文《注意力就是你所需要的》（https://arxiv.org/abs/1706.03762）中提出的深度神经网络架构。为了理解 LLMs，必须要先了解最初的 Transformer，它是为机器翻译任务而开发的，用于将英文文本翻译成德文和法文。简化版的 Transformer 架构如图 1.4 所示。图 1.4简化版的原始 Transformer 架构图示，它是一个用于语言翻译的深度学习模型。"
keywords: "LLM(3)： Transformer 架构"
categories: ['大模型']
tags: ['深度学习', '架构', '大语言模型', 'Transformer', 'Llm']
artid: "146293743"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146293743
    alt: "LLM3-Transformer-架构"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146293743
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146293743
cover: https://bing.ee123.net/img/rand?artid=146293743
image: https://bing.ee123.net/img/rand?artid=146293743
img: https://bing.ee123.net/img/rand?artid=146293743
---

# LLM(3)： Transformer 架构

Transformer 架构是当前大语言模型的主力架构和基础技术，本文以通俗易懂的方式，对此作简要介绍。

### 1.4 介绍 Transformer 架构

大多数现代的大规模语言模型（LLMs）依赖于 Transformer 架构，这是一种在 2017 年的论文
[《注意力就是你所需要的》](https://arxiv.org/abs/1706.03762)
（https://arxiv.org/abs/1706.03762）中提出的深度神经网络架构。为了理解 LLMs，必须要先了解最初的 Transformer，它是为机器翻译任务而开发的，用于将英文文本翻译成德文和法文。简化版的 Transformer 架构如图 1.4 所示。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/868e6ed9f55540a6aaaa76b12b83caf6.png#pic_center)

图 1.4
*简化版的原始 Transformer 架构图示，它是一个用于语言翻译的深度学习模型。Transformer 由两部分组成：(a) 编码器，处理输入文本并生成文本的嵌入表示（一种在不同维度中捕捉许多不同因素的数值表示），以及 (b) 解码器，可以使用这些嵌入表示逐词生成翻译后的文本。此图展示了翻译过程的最后阶段，其中解码器需要在给定原始输入文本（“This is an example”）和部分翻译的句子（“Das ist ein”）的情况下，仅生成最终单词（“Beispiel”），以完成整个翻译。*

Transformer 架构由两个子模块组成：编码器和解码器。编码器模块处理输入文本，并将其编码成一系列捕捉输入上下文信息的数值表示或向量。然后，解码器模块接收这些编码后的向量并生成输出文本。例如，在翻译任务中，编码器会将源语言的文本编码成向量，而解码器则会解码这些向量以生成目标语言的文本。编码器和解码器都包含许多层，它们通过所谓的自注意力机制相连。对于输入是如何预处理和编码的，您可能会有许多疑问。这些问题将在后续章节的逐步实现中得到解答。

Transformer 和 LLMs 的一个关键组件是自注意力机制（the self-attention mechanism，图 1.4 中未展示），它允许模型对序列中的不同单词或标记的重要性进行相对权衡。这种机制使模型能够捕捉输入数据中的长程依赖性和上下文关系，增强其生成连贯且上下文相关的输出的能力。然而，由于其复杂性，我们将在后续内容中进一步讨论并逐步实施。

Transformer 架构的后期变种，如 BERT（来自 Transformers 的双向编码表示的简称）和各种 GPT 模型（生成预训练变换器的简称），都是基于这一概念构建的，旨在适应不同的任务。

基于原始 Transformer 的编码器子模块构建的 BERT 在其训练方法上与 GPT 有所不同。虽然 GPT 被设计用于生成任务，但 BERT 及其变体专注于掩码词预测，即模型预测给定句子中的掩码或隐藏单词，如图 1.5 所示。这种独特的训练策略使 BERT 在文本分类任务中表现出色，包括情感预测和文档分类。作为其实力的应用实例，截至此时，X（前身为Twitter）使用 BERT 来检测有害内容。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/4c7f64577537465aa4830bee72e24ae7.png#pic_center)

图 1.5
*transformer 编码器和解码器子模块的可视化表示。左侧是编码器部分，它例证了类似于 BERT 的大型语言模型（LLM），这些模型专注于被遮蔽单词预测，主要用于文本分类等任务。右侧是解码器部分，展示了类似于 GPT 的大型语言模型，这些模型设计用于生成性任务并生成连贯的文本序列。*

另一方面，GPT专注于原始 transformer 架构的解码器部分，且设计用于需要生成文本的任务。这包括机器翻译、文本摘要、小说写作、编写计算机代码等。

主要设计和训练用于执行文本补全任务的 GPT 模型，在其能力上也显示出了显著的多功能性。这些模型擅长执行零样本学习和少量样本学习任务。零样本学习指的是在没有任何先前具体示例的情况下，能够推广到完全未见过的任务。而少量样本学习则涉及从用户作为输入提供的最少数目的示例中学习，如图 1.6 所示。

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6a5ac9d27312487a9046808ab11203e4.png#pic_center)

图 1.6
*除了文本补全之外，类似于 GPT 的大型语言模型可以根据其输入解决各种任务，而无需重新训练、微调或更改特定于任务的模型架构。有时，在输入中提供目标示例是有帮助的，这被称为少量样本设置。然而，类似于 GPT 的大型语言模型也能够在没有具体示例的情况下执行任务，这种情况被称为零样本设置。*

> **比较：Transformers 与 LLMs**
>
> 当今的大型语言模型（LLMs）基于 transformer 架构。因此，在文献中，transformers 和 LLMs 这两个术语经常被互换使用。然而，请注意，并非所有 transformers 都是 LLMs，因为 transformers 也可用于计算机视觉领域。同样，并非所有 LLMs 都是基于 transformer 的，因为存在基于递归和卷积架构的 LLMs。这些替代方法的主要动机是为了提高 LLMs 的计算效率。至于这些替代的 LLM 架构是否能够与基于 transformer 的 LLM 能力相竞争，以及它们是否会在实践中得到采用，仍有待观察。为简单起见，本文使用“LLM”一词来指代类似于 GPT 的基于 transformer 的 LLM。

---

原文：Sebastian Raschka. Build a Large Language Model(From Scratch)，此处为原文的中文翻译，为了阅读方便，有适当修改。