---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f753031323337343031322f:61727469636c652f64657461696c732f313436313937383938"
layout: post
title: "视觉语言模型VLM发展脉络"
date: 2025-03-12 10:29:53 +08:00
description: "现代社会，双眼摄入的信息量大概可能是其他所有感官之和，而视觉信息最重要的两种载体——图片和文本是我们打开AGI的大门。当前LLM时代已经将人类的语言和思维掌握的很好，但是目前来看在图像感知、理解、生成方面仍有很大不足。"
keywords: "视觉语言模型VLM发展脉络"
categories: ['多模态大模型']
tags: ['语言模型', '自然语言处理', '人工智能']
artid: "146197898"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146197898
    alt: "视觉语言模型VLM发展脉络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146197898
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146197898
cover: https://bing.ee123.net/img/rand?artid=146197898
image: https://bing.ee123.net/img/rand?artid=146197898
img: https://bing.ee123.net/img/rand?artid=146197898
---

# 视觉语言模型VLM发展脉络

## 1、大模型时代的多模态

现代社会，双眼摄入的信息量大概可能是其他所有感官之和，而视觉信息最重要的两种载体——图片和文本是我们打开AGI的大门。当前LLM时代已经将人类的语言和思维掌握的很好，但是目前来看在图像感知、理解、生成方面仍有很大不足。

## 2、数据集

因为数据的敏感性和解释门槛，很多公司在技术报告或者论文中很少会交代详细的数据来源。我们用国内做的最好的Qwen系列来看一下，做多模态的视觉语言模型，我们一般需要多少数据量？以及在核心测试数据上，模型的迭代提升情况。

### 1）QwenVL

原始数据集包含总计50亿对图文数据，经过清洗后剩余14亿对数据，其中77.3%为英文数据，22.7%为中文数据。

![](https://i-blog.csdnimg.cn/direct/fbb175b8217640e9b629a35c88871946.png)

找了一圈也没有发现QwenVL在MMMU上的指标，可能太低了，没有列出来。后续都有一个详细的对比表格，可以从DocVQA来详细对比下qwenVL7B系列的性能提升

DocVQA：

QwenVL-Qwen2VL-Qwen2.5VL

=65%-94.5%-95.7%

原始数据集包含总计50亿对图文数据，经过清洗后剩余14亿对数据，其中77.3%为英文数据，22.7%为中文数据。

下表中列出了不同数据集的原始数据量、清洗后的数据量以及剩余百分比。英文数据集包括LAION-en、LAION-COCO、DataComp、Coyo、CC12M、CC3M、SBU和COCO Caption，中文数据集包括LAION-zh和内部数据。表格详细说明了每个数据集在清洗前后的数据量变化及其在整个预训练数据中的占比。这些数据用于Qwen-VL模型的第一阶段预训练，主要优化视觉编码器和VL适配器。

![](https://i-blog.csdnimg.cn/direct/6ad1ade755f346798b4f40adbf4d5f74.png)

下表包含了Qwen-VL多任务预训练数据的详细信息，该表格包含了多个任务的样本数量和对应的数据集来源。具体来说，任务包括图像描述(Captioning)、视觉问答(VQA)、图像定位(Grounding)、参考定位(Ref Grounding)、带定位的图像描述(Grounded Captioning)、光学字符识别(OCR)以及纯文本自回归(Pure-text Autoregression)。

例如，图像描述任务包含19.7M样本，数据来源包括LAION- en & zh、DataComp、Coyo、CC12M & 3M、SBU、COCO以及内部数据；视觉问答任包含3.6M样本，数据来源包括GQA、VGQA、VQAv2、DVQA、OCR-VQA、DocVQA、TextVQA、ChartQA和AI2D；OCR任务包含24.8M样本，数据来源为SynthDoG-en &zh以及CommonCrawl的PDF和HTML文件。通过这些多样化的数据集，Qwen-VL模型能够在多模态任务中进行全面的预训练，从而提升其视觉理解能力。

![](https://i-blog.csdnimg.cn/direct/76d3c323a59745d6b8a8d8d69b786ac1.png)

### 2）Qwen-VL2

没有看到具体的数据量变化的说明，更多的看起来是模型的创新。

![](https://i-blog.csdnimg.cn/direct/e4f96d5a8dc243e886d98725edbe1f8f.png)

### 3）Qwen2.5-VL

Qwen2.5-VL的预训练数据通过多种方法构建，数据量从Qwen2-VL的1.2万亿token扩展到约4万亿token。

![](https://i-blog.csdnimg.cn/direct/22655501268648bf9992cca605238274.png)

数据来源包括清理的网络数据、合成数据等，涵盖多种多模态数据类型，如图像字幕、交错图文数据、OCR数据、视觉知识（如名人、地标、动植物识别）、多模态学术问题、本地化数据、文档解析数据、视频描述、视频定位和基于代理的交互数据。

数据清理和评分采用四阶段评分系统，评分标准包括文本质量、图文相关性、图文互补性和信息密度平衡。此外，数据集还包括扩展的10,000多个对象类别的训练数据，以及合成的非存在对象类别和多实例对象数据，以增强模型的开放词汇检测能力和极端场景下的表现。

qwen2.5的整体效果表现:

![](https://i-blog.csdnimg.cn/direct/e28e82e9719e4387b97d359301988928.png)

## 3、模型脉络

### 3.1 经典架构

先引用一张来自B站李沐-朱毅老师的PPT，括号里是论文当前引用量：

![](https://i-blog.csdnimg.cn/direct/9233e0b240ad4b378acbdb526c383d8c.png)

其中blip1&blip2：ALBEF，BLIP，BLIP2 都是来自Saleforce研究院，都是 Junnan Li 大佬的杰作。

自从多模态模型（特别是图文多模态模型）出现，模态之间怎么配合就是个问题。19-20年的时候，ViLBERT和Uniter采用了Object-Text对来提升模型对图片的理解能力。目标检测的引入，带来了一个笨重的检测器，去检测各种框，而且会存在漏检导致严重的问题。

到了21-22年，去掉检测器成了主流，ViLT，ALBEF，VLMo，BLIP 等等都抛弃了检测器，彻底摆脱了CNN网络的舒服，全面拥抱Transformer，当然这也得益于本身ViT模型在CV领域的大放光彩，让两个模态的有机融合成为了可能。

![](https://i-blog.csdnimg.cn/direct/54ab8925182c4ba2aa96d539660ae99a.png)

**BLIP**

![](https://i-blog.csdnimg.cn/direct/cb7aa60de0434d87badc50123e83d7e1.png)

![](https://i-blog.csdnimg.cn/direct/32bb254e90934f628a3b0fe4eacdbf8a.png)

**BLIP2**

![](https://i-blog.csdnimg.cn/direct/d20fa032b6c24a05933136515a6eefa9.png)

![](https://i-blog.csdnimg.cn/direct/09915a33a14844a5869085b3a319792f.png)

### 3.2 视觉语言VLM架构

1）QwenVL架构

以 Qwen-7B 的预训练模型作为语言模型的初始化，并以 Openclip ViT-bigG 作为视觉编码器的初始化，中间加入单层随机初始化的 cross-attention，经过约1.5B的图文数据训练得到。最终图像输入分辨率为448。在四大类多模态任务的标准英文测评中（Zero-shot Captioning/VQA/DocVQA/Grounding）上，均取得同等通用模型大小下最好效果，可以用于知识问答、图像问答、文档问答、细粒度视觉定位等场景。

![](https://i-blog.csdnimg.cn/direct/e5a456c8adbe45508c84e98dff59d66d.png)

**三步训练：**

**预训练**
：只优化视觉编码器和视觉语言适配器，冻结语言模型。使用大规模图像-文本配对数据，输入图像分辨率为224x224。

**多任务预训练**
：引入更高分辨率（448x448）的多任务视觉语言数据，如VQA、文本VQA、指称理解等，进行多任务联合预训练。

**监督微调**
：冻结视觉编码器，优化语言模型和适配器。使用对话交互数据进行提示调优，得到最终的带交互能力的Qwen-VL-Chat模型。

2）Qwen2VL

Qwen2-VL延续了其上一代Qwen-VL中ViT加Qwen的串联结构，在三个不同规模的模型上，Qwen2-VL都采用了600M规模大小的ViT，并且支持图像和视频统一输入。为了让模型更清楚地感知视觉信息和理解视频，Qwen2-VL新增了对
**原生动态分辨率**
的全面支持。与上一代模型相比，Qwen2-VL能够处理任意分辨率的图像输入，不同大小图片被转换为动态数量的tokens，最少只需要4个。这种设计不仅确保了模型输入与图像原始信息之间的一致性，也模拟了人类视觉感知的自然方式，让模型在图像处理任务上更加灵活高效。

![](https://i-blog.csdnimg.cn/direct/25eceecd2ff14df59af6de01ec076dbd.png)

Qwen2-VL在架构上的另一项创新，是
**多模态旋转位置嵌入（M-ROPE）**
。传统的旋转位置嵌入只能捕捉一维序列的位置信息，而M-ROPE通过将原始旋转嵌入分解为代表时间、高度和宽度的三个部分。这使得大规模语言模型能够同时捕捉和整合一维文本序列、二维视觉图像以及三维视频的位置信息。这一创新有助于提升模型的多模态处理和推理能力，能够更好地理解和建模复杂的多模态数据。

3）Qwen2.5VL

设计了一种更全面的文档解析格式，称为 QwenVL HTML 格式，它既可以将文档中的文本精准地识别出来，也能够提取文档元素（如图片、表格等）的位置信息，从而准确地将文档中的版面布局进行精准还原。基于精心构建的海量数据，QwenVL HTML 可以对广泛的场景进行鲁棒的文档解析，比如杂志、论文、网页、甚至手机截屏等等。

![](https://i-blog.csdnimg.cn/direct/8ca838ab92764c96afcb913d4dc066a2.png)

Qwen2.5-VL 的视频理解能力经过全面升级，在时间处理上引入了
**动态帧率（FPS）**
训练和
**绝对时间编码**
技术。模型不仅能够支持小时级别的超长视频理解，还具备秒级的事件定位能力。它不仅能够准确地理解小时级别的长视频内容，还可以在视频中搜索具体事件，并对视频的不同时间段进行要点总结，从而快速、高效地帮助用户提取视频中蕴藏的关键信息。

在空间维度上，Qwen2.5-VL 不仅能够动态地将不同尺寸的图像转换为不同长度的 token，还直接使用图像的实际尺寸来表示检测框和点等坐标，而不进行传统的坐标归一化。这使得模型能够直接学习图像的尺度。在时间维度上，引入了动态 FPS (每秒帧数)训练和绝对时间编码，将 mRoPE id 直接与时间流速对齐。这使得模型能够通过时间维度 id 的间隔来学习时间的节奏。

4）Pixtral12B

我们再选择一个国外做得最好的模型—Mixtral的视觉版本Pixtral为例进行说明。Pixtral的视觉编码器通过两层全连接网络连接到多模态解码器（LLM），MLP层维度不变，用于将视觉编码器的输出转换为解码器所需的输入嵌入大小，激活函数为GeLU。多模态解码器对图像token的处理方式与文本token相同，包括所有token的RoPE-1D位置编码。解码器使用了因果自注意力机制，能够平滑地促进多图像对话等能力。

![](https://i-blog.csdnimg.cn/direct/8f919c8d73df4cb6815862b144da9180.png)

采用ViT架构打底，4亿参数量。同时为了能够处理各种分辨率和纵横比的图像，作者对标准架构进行了四项关键更改：

![](https://i-blog.csdnimg.cn/direct/7924a44b94254eab8518898cc097702f.png)

**基础结构：**

**Break tokens**
：为了帮助模型区分具有相同patch数量（相同区域）但纵横比不同的图像，需要在图像行之间加入[IMAGE BREAK]，在图像序列的末尾加上[IMAGE END]。

**FFN中的门控**
：在隐藏层中使用门控，而非注意力块中的标准前馈层。

**序列打包**
：为了在单个批次中有效地处理图像，作者沿序列维度将图像展平并连接起来，并构建了一个块对角掩码，以确保来自不同图像的patch之间没有注意力泄漏。

**RoPE-2D**
：在自注意层中用相对旋转位置编码代替传统的绝对位置嵌入。虽然必须对学习到的位置嵌入进行插值以处理新的图像大小（通常以牺牲性能为代价），但相对位置编码自然而然地适合可变的图像大小。

## 4、额外带来的效果

### 4.1 ocr识别

从Qwen2.5VL的各项指标我们可以看的出来OCR识别领域是遥遥领先的，无论是gpt4o或者claude3.5，我们实际测试下来也确实这样，无论是多语言还是负责场景识别，qwen-max几乎都是全方位碾压国外的两个大模型。

![](https://i-blog.csdnimg.cn/direct/f24b3c5c9f934d15b62f7b311c299e17.png)

Qwen2.5-VL 将 OCR 识别能力提升至一个新的水平，增强了多场景、多语言和多方向的文本识别和文本定位能力。同时，在信息抽取能力上进行大幅度增强，以满足日益增长的资质审核、金融商务等数字化、智能化需求。

![](https://i-blog.csdnimg.cn/direct/008024c3adc14ec3bfe7db2c3145327b.png)

### 4.2 更多其他能力

1）精准的视觉定位

![](https://i-blog.csdnimg.cn/direct/7b0d40066a0f4a6bb528813c33eb4ab3.png)

2）视频理解

Qwen2.5VL能够准确地理解小时级别的长视频内容，还可以在视频中搜索具体事件，并对视频的不同时间段进行要点总结，从而快速、高效地帮助用户提取视频中蕴藏的关键信息。比如下图视频中罗列的论文列表：

![](https://i-blog.csdnimg.cn/direct/fc10d9437d0f44d590ca2ecf1fecbdef.png)

![](https://i-blog.csdnimg.cn/direct/574c44216d9f48c483e67b12f1de2360.png)

3）智能文档和页面布局还原

QwenVL HTML 格式既可以将文档中的文本精准地识别出来，也能够提取文档元素（如图片、表格等）的位置信息，从而准确地将文档中的版面布局进行精准还原。基于精心构建的海量数据，QwenVL HTML 可以对广泛的场景进行鲁棒的文档解析，比如杂志、论文、网页、甚至手机截屏等等。

![](https://i-blog.csdnimg.cn/direct/bf069234ec4d49c8a7b59c0929d6f6d0.png)

![](https://i-blog.csdnimg.cn/direct/ddad3224842d4263aebec7973110847d.png)

## 5、商业化VL系统的问题和挑战

### 5.1 复读机问题

试了下阿里云的qwenvlocr识别，很多时候再ocr的最后会出现很多无效字符，类似于llm初期的复读机问题，笔者在最早用的机器翻译Marian模型很经常地出现这种问题。

![](https://i-blog.csdnimg.cn/direct/fd2e93dfa8a24c7f9c27ffc241124f51.png)

### 5.2 幻觉问题

1）提示词攻击

一是最明显的视觉提示注入，也就是在图片中加入明显的文字误导。试了下最新的gpt4o和sonnet这些模型已经不存在这种问题了，但是国内的模型比如qwen-vl-max依然存在，而且可能问题还比较严重。

case1: gpt4v的攻击，现在已经没有问题了：

![](https://i-blog.csdnimg.cn/direct/8c694b4b45004dc4a3cf0555ace037f2.png)

case2:

右图是的气泡中包含攻击指令，4个顶尖模型，国外的3个模型都避免了注入攻击，而qwen-vl-max没能绕开注入攻击，而是按照气泡里的指令回答了问题。

![](https://i-blog.csdnimg.cn/direct/51bb3ad81f1e4b9796fad2dd19dc707a.png)

第一次实验结果：

![](https://i-blog.csdnimg.cn/direct/a543ee0d7e6347cc91e487be1cbeecb0.png)

第二次实验结果：

![](https://i-blog.csdnimg.cn/direct/fb2d5b5e1ea049989ed8000b3f7cd74e.png)

## 6、参考

**blip2**

https://zhuanlan.zhihu.com/p/606364639

**gpt4v离谱bug**

[https://mp.weixin.qq.com/s/mkesn-\_4QX-z8FDFS9PL1w](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247699263&idx=1&sn=d3e240f41b3137c0ee06977b768a17db&scene=21#wechat_redirect "https://mp.weixin.qq.com/s/mkesn-_4QX-z8FDFS9PL1w")

**pixtral**

[https://mp.weixin.qq.com/s/WGEmjALHVPJ8ahjhUcDVFw](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652541727&idx=4&sn=b7c432ccefebee555474aef5990e0b84&scene=21#wechat_redirect "https://mp.weixin.qq.com/s/WGEmjALHVPJ8ahjhUcDVFw")

**QwenVL**

[https://mp.weixin.qq.com/s/Hpwdlb5VB7FLU3t5Ccgx9Q](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247692885&idx=2&sn=8aacbd168fcfcc53bc22f6a5e3dd0396&scene=21#wechat_redirect "https://mp.weixin.qq.com/s/Hpwdlb5VB7FLU3t5Ccgx9Q")

**Qwen2VL**

[https://mp.weixin.qq.com/s/42M913ulJ\_P3E8-CYeqrsA](https://mp.weixin.qq.com/s?__biz=MzIzNjc1NzUzMw==&mid=2247746503&idx=3&sn=36cfa7b56871e6c6f482a90826b8516c&scene=21#wechat_redirect "https://mp.weixin.qq.com/s/42M913ulJ_P3E8-CYeqrsA")

**Qwen2.5VL**

[https://mp.weixin.qq.com/s/RhRcULJrEGwasMLoNYXPOw](https://mp.weixin.qq.com/s?__biz=Mzk0ODg4NDI5NA==&mid=2247484266&idx=1&sn=0f5c065774c4629f61fc207e27b721b8&scene=21#wechat_redirect "https://mp.weixin.qq.com/s/RhRcULJrEGwasMLoNYXPOw")

**Qwen2.5-blog**

https://qwenlm.github.io/blog/qwen2.5-vl/