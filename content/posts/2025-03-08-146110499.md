---
arturl_encode: "68747470733a2f2f:626c6f672e6373646e2e6e65742f796f726b68756e7465722f:61727469636c652f64657461696c732f313436313130343939"
layout: post
title: "CarPlanner用于自动驾驶大规模强化学习的一致性自回归轨迹规划"
date: 2025-03-08 08:54:00 +08:00
description: "25年2月来自浙大和菜鸟网络的论文“CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving”。轨迹规划对于自动驾驶至关重要，可确保在复杂环境中安全高效地导航。虽然最近基于学习的方法，特别是强化学习 (RL)，在特定场景中显示出良好的前景，但 RL 规划器在训练效率低下和管理大规模真实驾驶场景方面仍存在困难。本文介绍 Car"
keywords: "carplanner"
categories: ['计算机视觉', '自动驾驶', '机器学习']
tags: ['计算机视觉', '自动驾驶', '深度学习', '机器学习', '人工智能']
artid: "146110499"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146110499
    alt: "CarPlanner用于自动驾驶大规模强化学习的一致性自回归轨迹规划"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146110499
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146110499
cover: https://bing.ee123.net/img/rand?artid=146110499
image: https://bing.ee123.net/img/rand?artid=146110499
img: https://bing.ee123.net/img/rand?artid=146110499
---

# CarPlanner：用于自动驾驶大规模强化学习的一致性自回归轨迹规划

25年2月来自浙大和菜鸟网络的论文“CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving”。

轨迹规划对于自动驾驶至关重要，可确保在复杂环境中安全高效地导航。虽然最近基于学习的方法，特别是强化学习 (RL)，在特定场景中显示出良好的前景，但 RL 规划器在训练效率低下和管理大规模真实驾驶场景方面仍存在困难。本文介绍 CarPlanner，一种使用 RL 生成多模态轨迹的一致自回归规划器。自回归结构可实现高效的大规模 RL 训练，而一致性的结合可通过在时间步骤中保持一致的时间一致性来确保稳定的策略学习。此外，CarPlanner 采用具有专家指导的奖励函数和不变-视图模块的生成选择框架，简化 RL 训练并提高策略性能。

轨迹规划 [41] 在自动驾驶中至关重要，它利用感知和轨迹预测模块的输出来生成自车的未来姿态。控制器跟踪该规划轨迹，产生闭环驾驶的控制命令。最近，基于学习的轨迹规划备受关注，因其能够自动化算法迭代、消除繁琐的规则设计并确保各种现实场景中的安全性和舒适性 [41]。

大多数现有研究 [3, 13, 19, 34] 采用模仿学习 (IL) 来使规划轨迹与人类专家的轨迹保持一致。然而，这种方法存在分布漂移 [33] 和因果混淆 [10] 的问题。强化学习 (RL) 提供一种潜在的解决方案，它解决这些挑战并通过奖励函数提供更丰富的监督。尽管强化学习在游戏 [39]、机器人技术 [22] 和语言模型 [28] 等领域表现出有效性，但它在大规模驾驶任务中仍然存在训练效率低下和性能问题。

如图（a）所示，该种方法涉及生成初始轨迹估计，然后通过 RL 的迭代应用对其进行细化。然而，包括 Gen-Drive [21] 在内的最新研究表明，它仍然落后于最先进 IL 和基于规则的规划器。这种方法的一个显着限制是它忽略轨迹规划任务中固有的时间因果关系。此外，在高维轨迹空间上直接优化的复杂性可能会阻碍 RL 算法的性能。

自回归模型，如图 （b）所示，其使用过渡模型中的单步策略循环地生成自车的姿态。在此类方法中，所有时间步骤中的自车姿态被合并以形成整体规划轨迹。考虑到时间因果关系，当前的自回归模型允许交互行为。然而，一个常见的限制是它们依赖于从动作分布中进行自回归随机采样来生成多模态轨迹。这种普通的自回归程序可能会损害长期一致性，并不必要地扩大 RL 中的探索空间，从而导致性能不佳。

为了解决自回归模型的局限性，本文引入 CarPlanner，这是一种一致的自回归模型，旨在进行高效、大规模的基于 RL 规划器训练（见图 ©）。CarPlanner 的关键见解是它将一致的模式表示作为自回归模型的条件。

![请添加图片描述](https://i-blog.csdnimg.cn/direct/f59fd231e10e4ff6bb49c5bb7401da98.png)

MDP 用于对顺序决策问题进行建模，其形式为一个元组 ⟨S, A, P, R, ρ\_0, γ, T⟩。S 是状态空间。A 是动作空间。P 是状态转换概率。R 表示奖励函数，有界。ρ\_0 是初始状态分布。T 是时间范围，γ 是未来奖励的折扣因子。状态-动作序列定义为 τ =(s\_0,a\_0,s\_1,a\_1,…,s\_T)，其中 s\_t 和 a\_t 是时间步骤 t 时的状态和动作。RL 的目标是最大化预期回报。

状态 s\_t 包含矢量表示的地图和智体信息。地图信息 m 包括道路网络、交通信号灯等，用折线和多边形表示。智体信息包括自车和其他交通智体的当前和过去姿态，用折线表示。自车的智体索引为 0，交通智体的索引范围从 1 到 N。对于每个智体 i，其历史表示为 s^i\_t−H:t，i ∈ {0,1,…,N}，其中 H 是历史时间范围。

将轨迹规划任务建模为顺序决策过程，并将自回归模型分解为策略模型和转换模型。连接轨迹规划和自回归模型的关键，是将动作定义为自车的下一个姿态，即 a\_t = s^0\_t+1。因此，在推动自回归模型后，解码后的姿势被收集为自车规划的轨迹，状态序列进一步分解为策略模型和转换模型。这里典型的自回归方法带有固有的相关问题：跨时间的不一致行为源于策略分布，而策略分布取决于从动作分布中进行的随机抽样。

为了解决上述问题，在自回归方式中引入随时间步骤保持不变的一致性模式信息 c。由于专注于自车轨迹规划，因此一致性模式 c 不会影响转换模型。

这种一致性自回归方式揭示一个生成-选择框架，其中模式选择器根据初始状态 s\_0 对每种模式进行评分，而轨迹生成器从模式条件策略中采样来生成多模态轨迹。

该转换模型需要在每个时间步中使用，因为它会根据当前状态 s\_t 生成在时间步 t + 1 时的交通智体姿势。实际上，这个过程非常耗时，而且使用这种转换模型没有看到性能提升，因此，用轨迹预测器 P (s\_1:T^1:N |s\_0 ) 作为非反应性转换模型，该模型在给定初始状态 s\_0 的情况下一次性生成交通智体的所有未来姿势。

## 规划器架构

CarPlanner 的框架如图所示，包含四个关键组件：1) 非反应式转换模型、2) 模式选择器、3) 轨迹生成器、和 4) 规则-增强选择器。规划器在生成-选择框架内运行。

![请添加图片描述](https://i-blog.csdnimg.cn/direct/f580481eb57041e0beb56d8fcbbd5e83.png)

给定初始状态 s\_0 和所有可能的 N\_mode 模式，轨迹选择器评估并为每种模式分配分数。然后，轨迹生成器生成与各自模式相对应的 N\_mode 轨迹。

对于轨迹生成器，初始状态 s\_0 被复制 N\_mode 次，每次都与 N\_mode 模式之一相关联，从而有效地创建 N\_mode 平行世界。策略在这些模拟环境中执行。在策略展开期间，轨迹预测器充当状态转换模型，生成所有时间范围内交通智体的未来位置。

### 非反应式转换模型

该模块以初始状态 s\_0 作为输入，输出交通智体的未来轨迹。初始状态由智体和地图编码器处理，然后由自注意 Transformer 编码器 [43] 融合智体和地图特征。然后将智体特征解码为未来轨迹。

智体和地图编码器。状态 s\_0 包含地图和智体信息。地图信息 m 由 N\_m,1 条折线和 N\_m,2 个多边形组成。折线描述车道中心和车道边界，每条折线包含 3 N\_p 个点，其中 3 对应车道中心、左边界和右边界。每个点的维度为 D\_m = 9，并包含以下属性：x、y、航向、速度限制和类别。连接时，左边界和右边界的点与中心点一起产生 N\_m,1 × N\_p × 3 D\_m 的维度。

利用 PointNet [30] 从每条折线的点中提取特征，得到 N\_m,1 × D 的维数，其中 D 表示特征维数。多边形表示交叉路口、人行横道、停车线等，每个多边形包含 N\_p 个点。利用另一个 PointNet 从每个多边形的点中提取特征，得到 N\_m,2 × D 的维数。然后，将折线和多边形的特征连​​接起来形成整体地图特征，得到 N\_m × D 的维数。智体信息 A 由 N 个智体组成，每个智体保持过去 H 个时间步长的姿势。每个姿势的维度为 D\_a = 10，包括以下属性：x、y、航向、速度、边界框、时间步长和类别。因此，智体信息的维度为 N × H × D\_a。应用另一个 PointNet 从每个智体的姿势中提取特征，故有 N × D 的智体特征维度。

### 模式选择器

该模块将 s\_0 和纵向-横向分解模式信息作为输入，并输出每种模式的概率。

路线-速度的分解模式。为了捕捉纵向行为，生成 N\_lon 模式，表示与每种模式相关的轨迹平均速度。每个纵向模式 c\_lon,j 定义为 j 的标量值，在维度 D 上重复。因此，纵向模式的维数为 N\_lon × D。对于横向行为，使用图搜索算法从地图中识别 N\_lat 条可能的路线。这些路线对应于自车可用的车道。这些路线的维数为 N\_lat × N\_r × D\_m。为了提取有意义的表示，用另一个 PointNet 来聚合每条路线上 N\_r 个点的特征，从而生成维度为 N\_lat ×D 的横向模式。为了创建全面的模式表示 c，结合横向和纵向模式，得到 N\_lat × N\_lon × 2D 的组合维度。为了将此模式信息与其他特征维度对齐，将其传递通过线性层，映射回 N\_lat × N\_lon × D。N\_mode = N\_lat × N\_lon。

基于查询的 Transformer 解码器。此解码器用于将模式特征与从 s\_0 派生的地图和智体特征融合。在此框架中，模式用作查询，而地图和智体信息用作K-V。更新后的模式特征通过多层感知器 (MLP) 解码以得出每个模式的分数，然后使用 softmax 运算符对其进行归一化。

### 轨迹生成器

该模块以自回归方式运行，在给定当前状态 s\_t 和一致模式信息 c 的情况下，反复解码自身车辆的下一个姿势 a\_t。

不变-视图模块 (IVM)。在将模式和状态输入网络之前，对它们进行预处理以消除时间信息。对于状态 s\_t 中的地图和智体信息，选择自身当前姿势的 KNN [29]，并仅将它们输入到策略中。K 分别设置为地图和智体元素的一半。对于捕捉横向行为的路线，过滤掉那些段，其最接近自身车辆当前姿势的点作为起点，保留 K\_r 个点。在这种情况下，K\_r 设置为一条路线中 N\_r 个点的四分之一。最后，将路线、智体和地图姿势转换为当前时间步 t 的自车坐标系。从当前时间步长 t 中减去历史时间步长 t − H : t，得到范围为 −H : 0 的时间步长。

基于 Q 的 Transformer 解码器。采用与模式选择器相同的主干网络架构，但查询维度不同。由于 IVM 以及不同模式产生不同状态的事实，地图和智体信息不能在模式之间共享。因此，融合每个单独模式的信息。具体而言，Q维度为 1 × D，而 K-V 的维度为 (N + N\_m) × D。输出特征维度保持为 1 × D。需要强调的是，Transformer 解码器可以并行处理来自多个模式的信息，无需使用一个 for 循环按顺序处理每个模式。

策略输出。模式特征由两个不同的头处理：策略头和 V 头。每个头包含自己的 MLP 来生成动作分布的参数和相应的 V 估计。采用高斯分布来建模动作分布，在训练过程中，动作从该分布中抽样。相反，在推理过程中，利用分布平均来确定动作。

### 规则-增强选择器

该模块首先引入一个基于规则的选择器，以初始状态 s\_0、多模态自车规划轨迹和智体的预测未来轨迹为输入。它计算驾驶导向指标，例如安全性、进度、舒适度等。基于规则的分数和模式选择器提供的模式分数的加权和，获得综合分数。得分最高的自车规划轨迹被选为轨迹规划器的输出。

## 训练

首先训练非反应式转换模型，并在模式选择器和轨迹生成器的训练期间冻结权重。不会将所有模式都输入生成器，而是采用赢者通吃策略，其中根据自车真实轨迹分配正模式，并将其作为轨迹生成器的条件。

模式分配。正的横向模式由真实轨迹的端点决定。从起始位置到该端点的纵向距离被划分为 N\_lon 间隔，正的纵向模式对应于相关距离间隔。

损失函数。对于选择器，使用交叉熵损失，即正模式的负对数似然和回归自车真实轨迹的副任务。对于生成器，使用 PPO [36] 损失，它由三部分组成：策略改进、价值估计和熵。

奖励函数。为了处理不同的场景，用自车未来姿势和真值之间负的位移误差 (DE) 作为通用奖励。还引入其他术语来提高轨迹质量：碰撞率和可驾驶区域合规性。如果未来姿势发生碰撞或超出可驾驶区域，则奖励设置为 -1；否则为 0。

模式丢弃。为了防止由于 Transformers 的残差连接而过度依赖模式或路线信息，在训练期间实现一个模式丢弃模块，该模块随机屏蔽路线以缓解此问题。

遵循 PDM [9] 来构建训练和验证分割。训练集的大小为 176,218，其中使用所有可用的场景类型，每种类型有 4,000 个场景。验证集的大小为 1,118，其中选择 100 个场景和 14 种类型。在 2 个 NVIDIA 3090 GPU 中对所有模型进行 50 次训练。每个 GPU 的批次大小为 64。用 AdamW 优化器，初始学习率为 1e-4，当验证损失停止减少时，以耐心为 0 和减少因子为 0.3 降低学习率。对于 RL 训练，设置折扣 γ = 0.1 和 GAE 参数 λ = 0.9。价值、策略和熵损失的权重分别设置为 3、100 和 0.001。纵向模式数设置为 12，横向模式的最大数量设置为 5。

如下算法 1 概述 CarPlanner 框架的训练过程。该过程涉及两个主要步骤：(1) 训练非反应性转换模型，(2) 训练模式选择器和轨迹生成器。

![请添加图片描述](https://i-blog.csdnimg.cn/direct/920be901f5f54b1ca53d539715296cfe.png)