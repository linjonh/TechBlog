---
layout: post
title: "AI第一天-自我理解笔记-微调大模型"
date: 2025-03-16 23:44:55 +0800
description: "【代码】AI第一天 自我理解笔记--微调大模型。"
keywords: "AI第一天 自我理解笔记--微调大模型"
categories: ['未分类']
tags: ['笔记', '人工智能']
artid: "146303434"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146303434
    alt: "AI第一天-自我理解笔记-微调大模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146303434
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146303434
cover: https://bing.ee123.net/img/rand?artid=146303434
image: https://bing.ee123.net/img/rand?artid=146303434
img: https://bing.ee123.net/img/rand?artid=146303434
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     AI第一天 自我理解笔记--微调大模型
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <hr id="hr-toc" name="tableOfContents"/>
    <p>
    </p>
    <h4 id="1.%20%E7%A1%AE%E5%AE%9A%E7%9B%AE%E6%A0%87%EF%BC%9A%E6%98%8E%E7%A1%AE%E4%BB%BB%E5%8A%A1%E5%92%8C%E6%95%B0%E6%8D%AE" name="1.%20%E7%A1%AE%E5%AE%9A%E7%9B%AE%E6%A0%87%EF%BC%9A%E6%98%8E%E7%A1%AE%E4%BB%BB%E5%8A%A1%E5%92%8C%E6%95%B0%E6%8D%AE">
     <strong>
      1. 确定目标：明确任务和数据
     </strong>
    </h4>
    <ul>
     <li>
      <strong>
       任务类型
      </strong>
      ：
      <br/>
      确定你要解决的问题类型，例如：
      <ul>
       <li>
        <strong>
         文本分类
        </strong>
        （如情感分析）
       </li>
       <li>
        <strong>
         图像分类
        </strong>
        （如识别猫狗）
       </li>
       <li>
        <strong>
         序列生成
        </strong>
        （如文本生成或机器翻译）
       </li>
      </ul>
     </li>
     <li>
      <strong>
       数据集
      </strong>
      ：
      <br/>
      收集或准备与任务相关的数据集，并进行初步分析：
      <ul>
       <li>
        数据规模（样本数量、类别分布）
       </li>
       <li>
        数据质量（是否有噪声、缺失值、标签错误）
       </li>
       <li>
        数据预处理需求（如文本清洗、图像归一化）
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4 id="2.%20%E9%80%89%E6%8B%A9%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" name="2.%20%E9%80%89%E6%8B%A9%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">
     <strong>
      2. 选择预训练模型
     </strong>
    </h4>
    <ul>
     <li>
      <strong>
       预训练模型
      </strong>
      ：
      <br/>
      根据任务选择合适的预训练模型：
      <ul>
       <li>
        <strong>
         文本任务
        </strong>
        ：BERT、RoBERTa、GPT、T5
       </li>
       <li>
        <strong>
         图像任务
        </strong>
        ：ResNet、EfficientNet、ViT
       </li>
       <li>
        <strong>
         多模态任务
        </strong>
        ：CLIP、Mixture of Experts（MoE）
       </li>
      </ul>
     </li>
     <li>
      <strong>
       模型来源
      </strong>
      ：
      <br/>
      常用的模型库包括：
      <ul>
       <li>
        <strong>
         Hugging Face
        </strong>
        （如
        <code>
         transformers
        </code>
        库）
       </li>
       <li>
        <strong>
         PyTorch/TensorFlow官方模型库
        </strong>
       </li>
       <li>
        <strong>
         Timm库
        </strong>
        （针对计算机视觉）
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4 id="3.%20%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86" name="3.%20%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">
     <strong>
      3. 数据预处理
     </strong>
    </h4>
    <h5 id="(1)%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E6%A0%BC%E5%BC%8F%E5%8C%96" name="(1)%20%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E4%B8%8E%E6%A0%BC%E5%BC%8F%E5%8C%96">
     <strong>
      (1) 数据清洗与格式化
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       文本数据
      </strong>
      ：
      <ul>
       <li>
        去除特殊字符、停用词
       </li>
       <li>
        统一大小写（如全小写）
       </li>
       <li>
        处理缺失值或异常标签
       </li>
      </ul>
     </li>
     <li>
      <strong>
       图像数据
      </strong>
      ：
      <ul>
       <li>
        调整尺寸（如统一为224x224）
       </li>
       <li>
        归一化（如将像素值缩放到[0,1]或[-1,1]）
       </li>
       <li>
        数据增强（如旋转、翻转、裁剪）
       </li>
      </ul>
     </li>
    </ul>
    <h5 id="(2)%20%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86" name="(2)%20%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86">
     <strong>
      (2) 划分数据集
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       训练集
      </strong>
      ：用于模型训练（通常占80%）。
     </li>
     <li>
      <strong>
       验证集
      </strong>
      ：用于调参和监控过拟合（通常占10%）。
     </li>
     <li>
      <strong>
       测试集
      </strong>
      ：最终评估模型性能（通常占10%）。
     </li>
    </ul>
    <h5 id="(3)%20%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%89%B9%E5%A4%84%E7%90%86" name="(3)%20%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E6%89%B9%E5%A4%84%E7%90%86">
     <strong>
      (3) 数据加载与批处理
     </strong>
    </h5>
    <ul>
     <li>
      使用
      <code>
       DataLoader
      </code>
      （PyTorch）或
      <code>
       tf.data
      </code>
      （TensorFlow）将数据分批次加载：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>from torch.utils.data import DataLoader, Dataset

class MyDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

train_dataset = MyDataset(train_data, train_labels)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)</code></pre>
     </li>
    </ul>
    <hr/>
    <h4 id="4.%20%E6%9E%84%E5%BB%BA%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84" name="4.%20%E6%9E%84%E5%BB%BA%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84">
     <strong>
      4. 构建微调模型架构
     </strong>
    </h4>
    <h5 id="(1)%20%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" name="(1)%20%E5%8A%A0%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">
     <strong>
      (1) 加载预训练模型
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       文本模型示例（Hugging Face）
      </strong>
      ：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>from transformers import BertModel, BertConfig

model = BertModel.from_pretrained("bert-base-uncased")</code></pre>
     </li>
     <li>
      <strong>
       图像模型示例（PyTorch）
      </strong>
      ：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>import torchvision.models as models

model = models.resnet18(pretrained=True)</code></pre>
     </li>
    </ul>
    <h5 id="(2)%20%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B%E5%B0%BE%E9%83%A8%EF%BC%88%E9%80%82%E9%85%8D%E4%BB%BB%E5%8A%A1%EF%BC%89" name="(2)%20%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B%E5%B0%BE%E9%83%A8%EF%BC%88%E9%80%82%E9%85%8D%E4%BB%BB%E5%8A%A1%EF%BC%89">
     <strong>
      (2) 修改模型尾部（适配任务）
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       分类任务
      </strong>
      ：替换最后一层全连接层（全连接层的输出维度需匹配任务类别数）：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code># 对于ResNet：
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, num_classes)  # num_classes是目标类别数

# 对于BERT：
model.classifier = nn.Linear(model.config.hidden_size, num_classes)</code></pre>
     </li>
    </ul>
    <h5 id="(3)%20%E5%86%BB%E7%BB%93%E9%83%A8%E5%88%86%E5%B1%82%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89" name="(3)%20%E5%86%BB%E7%BB%93%E9%83%A8%E5%88%86%E5%B1%82%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89">
     <strong>
      (3) 冻结部分层（可选）
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       冻结预训练层
      </strong>
      ：保留底层的通用特征提取能力，只训练新增层：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>for param in model.parameters():
    param.requires_grad = False  # 冻结所有层

# 解冻最后一层（如分类层）
for param in model.fc.parameters():  # 对BERT可能是model.classifier.parameters()
    param.requires_grad = True</code></pre>
     </li>
    </ul>
    <hr/>
    <h4 id="5.%20%E8%AE%BE%E7%BD%AE%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0" name="5.%20%E8%AE%BE%E7%BD%AE%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0">
     <strong>
      5. 设置训练参数
     </strong>
    </h4>
    <h5 id="(1)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%8E%87" name="(1)%20%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E5%AD%A6%E4%B9%A0%E7%8E%87">
     <strong>
      (1) 优化器与学习率
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       选择优化器
      </strong>
      ：
      <ul>
       <li>
        常用优化器：Adam、AdamW、SGD
       </li>
       <li>
        示例：
        <pre></pre>
        python
        <p>
         深色版本
        </p>
        <pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)</code></pre>
       </li>
      </ul>
     </li>
     <li>
      <strong>
       学习率（Learning Rate）
      </strong>
      ：
      <ul>
       <li>
        预训练模型微调时，学习率通常比从头训练低（如1e-5到1e-3）。
       </li>
       <li>
        可使用学习率调度器（如
        <code>
         torch.optim.lr_scheduler.CosineAnnealingLR
        </code>
        ）。
       </li>
      </ul>
     </li>
    </ul>
    <h5 id="(2)%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" name="(2)%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">
     <strong>
      (2) 损失函数
     </strong>
    </h5>
    <ul>
     <li>
      根据任务选择损失函数：
      <ul>
       <li>
        <strong>
         分类任务
        </strong>
        ：交叉熵损失（
        <code>
         nn.CrossEntropyLoss
        </code>
        ）
       </li>
       <li>
        <strong>
         回归任务
        </strong>
        ：均方误差（
        <code>
         nn.MSELoss
        </code>
        ）
       </li>
       <li>
        <strong>
         生成任务
        </strong>
        ：交叉熵或自定义损失（如BERT的MLM损失）
       </li>
      </ul>
     </li>
    </ul>
    <h5 id="(3)%20%E5%85%B6%E4%BB%96%E8%B6%85%E5%8F%82%E6%95%B0" name="(3)%20%E5%85%B6%E4%BB%96%E8%B6%85%E5%8F%82%E6%95%B0">
     <strong>
      (3) 其他超参数
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       批量大小（Batch Size）
      </strong>
      ：根据硬件限制选择（如32、64、128）。
     </li>
     <li>
      <strong>
       训练轮次（Epochs）
      </strong>
      ：通常5-20轮，根据验证集表现调整。
     </li>
     <li>
      <strong>
       早停（Early Stopping）
      </strong>
      ：当验证损失不再下降时停止训练，防止过拟合。
     </li>
    </ul>
    <hr/>
    <h4 id="6.%20%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B" name="6.%20%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">
     <strong>
      6. 训练模型
     </strong>
    </h4>
    <h5 id="(1)%20%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF" name="(1)%20%E5%AE%9A%E4%B9%89%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF">
     <strong>
      (1) 定义训练循环
     </strong>
    </h5>
    <pre></pre>
    <p>
     python
    </p>
    <p>
     深色版本
    </p>
    <pre><code>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        inputs, labels = batch
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    
    # 验证阶段
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            inputs, labels = batch
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            val_loss += criterion(outputs, labels).item()
    
    print(f"Epoch {epoch+1}, Train Loss: {total_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}")</code></pre>
    <h5 id="(2)%20%E7%9B%91%E6%8E%A7%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B" name="(2)%20%E7%9B%91%E6%8E%A7%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B">
     <strong>
      (2) 监控训练过程
     </strong>
    </h5>
    <ul>
     <li>
      使用工具如
      <strong>
       TensorBoard
      </strong>
      或
      <strong>
       Weights &amp; Biases
      </strong>
      记录损失、准确率等指标。
     </li>
     <li>
      定期保存模型检查点（Checkpoint）：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>torch.save(model.state_dict(), f"model_epoch{epoch}.pth")</code></pre>
     </li>
    </ul>
    <hr/>
    <h4 id="7.%20%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89" name="7.%20%E8%B0%83%E6%95%B4%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89">
     <strong>
      7. 调整超参数（可选）
     </strong>
    </h4>
    <ul>
     <li>
      <strong>
       网格搜索（Grid Search）或随机搜索（Random Search）
      </strong>
      ：
      <br/>
      调整学习率、批量大小、层冻结策略等。
     </li>
     <li>
      <strong>
       自动化工具
      </strong>
      ：
      <br/>
      使用
      <code>
       Optuna
      </code>
      或
      <code>
       Ray Tune
      </code>
      进行超参数优化。
     </li>
    </ul>
    <hr/>
    <h4 id="8.%20%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%83%A8%E7%BD%B2" name="8.%20%E8%AF%84%E4%BC%B0%E4%B8%8E%E9%83%A8%E7%BD%B2">
     <strong>
      8. 评估与部署
     </strong>
    </h4>
    <h5 id="(1)%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0" name="(1)%20%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0">
     <strong>
      (1) 模型评估
     </strong>
    </h5>
    <ul>
     <li>
      在测试集上评估最终性能：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>model.eval()
test_loss = 0
correct = 0
with torch.no_grad():
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        output = model(data)
        test_loss += criterion(output, target).item()
        pred = output.argmax(dim=1, keepdim=True)
        correct += pred.eq(target.view_as(pred)).sum().item()

print(f"Test Accuracy: {100. * correct / len(test_loader.dataset)}%")</code></pre>
     </li>
    </ul>
    <h5 id="(2)%20%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B" name="(2)%20%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%9E%8B">
     <strong>
      (2) 部署模型
     </strong>
    </h5>
    <ul>
     <li>
      导出模型为ONNX格式或使用框架工具（如TorchScript）：
      <pre></pre>
      python
      <p>
       深色版本
      </p>
      <pre><code>torch.onnx.export(model, torch.randn(1, 3, 224, 224), "model.onnx")</code></pre>
     </li>
    </ul>
    <hr/>
    <h4 id="9.%20%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88" name="9.%20%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88">
     <strong>
      9. 常见问题与解决方案
     </strong>
    </h4>
    <h5 id="(1)%20%E8%BF%87%E6%8B%9F%E5%90%88" name="(1)%20%E8%BF%87%E6%8B%9F%E5%90%88">
     <strong>
      (1) 过拟合
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       解决方法
      </strong>
      ：
      <ul>
       <li>
        增加数据增强（如随机裁剪、翻转）。
       </li>
       <li>
        添加正则化（如Dropout、L2正则化）。
       </li>
       <li>
        减少模型复杂度或训练轮次。
       </li>
      </ul>
     </li>
    </ul>
    <h5 id="(2)%20%E6%AC%A0%E6%8B%9F%E5%90%88" name="(2)%20%E6%AC%A0%E6%8B%9F%E5%90%88">
     <strong>
      (2) 欠拟合
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       解决方法
      </strong>
      ：
      <ul>
       <li>
        增加训练轮次或学习率。
       </li>
       <li>
        解冻更多层（释放模型潜力）。
       </li>
       <li>
        尝试更复杂的模型架构。
       </li>
      </ul>
     </li>
    </ul>
    <h5 id="(3)%20%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E4%B8%8D%E8%B6%B3" name="(3)%20%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%E4%B8%8D%E8%B6%B3">
     <strong>
      (3) 计算资源不足
     </strong>
    </h5>
    <ul>
     <li>
      <strong>
       解决方法
      </strong>
      ：
      <ul>
       <li>
        使用小批量（如batch_size=16）。
       </li>
       <li>
        使用模型量化或知识蒸馏（Knowledge Distillation）。
       </li>
       <li>
        选择轻量级模型（如MobileNet、DistilBERT）。
       </li>
      </ul>
     </li>
    </ul>
    <hr/>
    <h4 id="10.%20%E6%80%BB%E7%BB%93%EF%BC%9A%E5%BE%AE%E8%B0%83%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE" name="10.%20%E6%80%BB%E7%BB%93%EF%BC%9A%E5%BE%AE%E8%B0%83%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE">
     <strong>
      10. 总结：微调的流程图
     </strong>
    </h4>
    <pre></pre>
    <p>
     深色版本
    </p>
    <pre><code>1. 确定任务和数据 →  
2. 选择预训练模型 →  
3. 数据预处理 →  
4. 修改模型尾部 →  
5. 设置超参数 →  
6. 训练并监控 →  
7. 调参优化 →  
8. 评估与部署</code></pre>
    <hr/>
    <h4 id="%E9%99%84%E5%BD%95%EF%BC%9A%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B%EF%BC%88%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%89" name="%E9%99%84%E5%BD%95%EF%BC%9A%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B%EF%BC%88%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%89">
     <strong>
      附录：代码示例（文本分类）
     </strong>
    </h4>
    <p>
     以下是一个完整的微调BERT进行文本分类的代码框架：
    </p>
    <pre></pre>
    <p>
     python
    </p>
    <p>
     深色版本
    </p>
    <pre><code>import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import Dataset, DataLoader

# 加载预训练模型和分词器
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)

# 数据预处理
class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

# 加载数据
train_dataset = TextDataset(train_texts, train_labels)
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# 训练配置
device = torch.device("cuda")
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# 训练循环（参考步骤6）</code></pre>
    <hr/>
    <h4 id="%E5%85%B3%E9%94%AE%E7%82%B9%E5%9B%9E%E9%A1%BE" name="%E5%85%B3%E9%94%AE%E7%82%B9%E5%9B%9E%E9%A1%BE">
     <strong>
      关键点回顾
     </strong>
    </h4>
    <ul>
     <li>
      <strong>
       微调的核心
      </strong>
      ：利用预训练模型的通用特征，仅针对特定任务调整部分参数。
     </li>
     <li>
      <strong>
       数据质量
      </strong>
      ：垃圾进，垃圾出（Garbage In, Garbage Out）。
     </li>
     <li>
      <strong>
       超参数调优
      </strong>
      ：学习率、批量大小、层冻结策略是关键。
     </li>
    </ul>
   </div>
  </div>
 </article>
 <p alt="6874747073:3a2f2f626c6f672e6373646e2e6e65742f696973756761722f:61727469636c652f64657461696c732f313436333033343334" class_="artid" style="display:none">
 </p>
</div>


