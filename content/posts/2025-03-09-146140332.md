---
layout: post
title: "kafka-flink-mysql-案例"
date: 2025-03-09 22:39:42 +0800
description: "和，并且你希望将中的数据写入到表，而将中的数据写入到表。maven。"
keywords: "kafka+flink"
categories: ['Java']
tags: ['Kafka', 'Flink']
artid: "146140332"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146140332
    alt: "kafka-flink-mysql-案例"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146140332
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146140332
cover: https://bing.ee123.net/img/rand?artid=146140332
image: https://bing.ee123.net/img/rand?artid=146140332
img: https://bing.ee123.net/img/rand?artid=146140332
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     kafka + flink +mysql 案例
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     假设你有两个Kafka主题：
     <code>
      user_activities_topic
     </code>
     和
     <code>
      product_views_topic
     </code>
     ，并且你希望将
     <code>
      user_activities_topic
     </code>
     中的数据写入到
     <code>
      user_activities
     </code>
     表，而将
     <code>
      product_views_topic
     </code>
     中的数据写入到
     <code>
      product_views
     </code>
     表。
    </p>
    <p>
     maven
    </p>
    <pre><code class="hljs">&lt;dependencies&gt;
    &lt;!-- Apache Flink dependencies --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
        &lt;artifactId&gt;flink-streaming-java_2.12&lt;/artifactId&gt;
        &lt;version&gt;1.14.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
        &lt;artifactId&gt;flink-connector-kafka_2.12&lt;/artifactId&gt;
        &lt;version&gt;1.14.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.flink&lt;/groupId&gt;
        &lt;artifactId&gt;flink-connector-jdbc_2.12&lt;/artifactId&gt;
        &lt;version&gt;1.14.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- MySQL JDBC Driver --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;mysql&lt;/groupId&gt;
        &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;
        &lt;version&gt;8.0.26&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>
    <h4>
     Flink Job 示例代码
    </h4>
    <pre><code class="hljs">import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.connector.jdbc.JdbcConnectionOptions;
import org.apache.flink.connector.jdbc.JdbcSink;

import java.util.Properties;

public class MultipleKafkaToFlinkToMysql {

    public static void main(String[] args) throws Exception {
        final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 配置Kafka消费者属性
        Properties kafkaProperties = new Properties();
        kafkaProperties.setProperty("bootstrap.servers", "localhost:9092");
        kafkaProperties.setProperty("group.id", "test");

        // 创建第一个Kafka消费者 (User Activities)
        FlinkKafkaConsumer&lt;String&gt; userActivitiesConsumer = new FlinkKafkaConsumer&lt;&gt;(
                "user_activities_topic",
                new SimpleStringSchema(),
                kafkaProperties);

        // 创建第二个Kafka消费者 (Product Views)
        FlinkKafkaConsumer&lt;String&gt; productViewsConsumer = new FlinkKafkaConsumer&lt;&gt;(
                "product_views_topic",
                new SimpleStringSchema(),
                kafkaProperties);

        // 从Kafka获取用户活动数据流
        env.addSource(userActivitiesConsumer)
           .map(value -&gt; {
               String[] parts = value.split(",");
               return new UserActivity(parts[0], parts[1]);
           })
           .addSink(JdbcSink.sink(
                   "INSERT INTO user_activities (user_id, activity) VALUES (?, ?)",
                   (statement, userActivity) -&gt; {
                       statement.setString(1, userActivity.userId);
                       statement.setString(2, userActivity.activity);
                   },
                   new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                           .withUrl("jdbc:mysql://localhost:3306/your_database")
                           .withDriverName("com.mysql.cj.jdbc.Driver")
                           .withUsername("your_username")
                           .withPassword("your_password")
                           .build()));

        // 从Kafka获取产品浏览数据流
        env.addSource(productViewsConsumer)
           .map(value -&gt; {
               String[] parts = value.split(",");
               return new ProductView(parts[0], Integer.parseInt(parts[1]));
           })
           .addSink(JdbcSink.sink(
                   "INSERT INTO product_views (user_id, product_id) VALUES (?, ?)",
                   (statement, productView) -&gt; {
                       statement.setString(1, productView.userId);
                       statement.setInt(2, productView.productId);
                   },
                   new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                           .withUrl("jdbc:mysql://localhost:3306/your_database")
                           .withDriverName("com.mysql.cj.jdbc.Driver")
                           .withUsername("your_username")
                           .withPassword("your_password")
                           .build()));

        env.execute("Multiple Kafka to Multiple MySQL Tables with Flink");
    }

    // 用户活动类
    public static class UserActivity {
        public String userId;
        public String activity;

        public UserActivity(String userId, String activity) {
            this.userId = userId;
            this.activity = activity;
        }
    }

    // 产品浏览类
    public static class ProductView {
        public String userId;
        public int productId;

        public ProductView(String userId, int productId) {
            this.userId = userId;
            this.productId = productId;
        }
    }
}</code></pre>
    <hr/>
    <p>
     当处理多个消费者和表时，直接为每个消费者编写独立的代码会导致代码冗长且难以维护。为了提高代码的可维护性和扩展性，可以采用一些设计模式和抽象方法来简化代码结构。以下是一些改进策略：
    </p>
    <p>
     ### 1. 使用工厂模式和配置文件
    </p>
    <p>
     通过使用工厂模式和配置文件，可以将不同Kafka主题和MySQL表的映射关系抽象出来，从而减少重复代码。
    </p>
    <p>
     ### 2. 示例代码重构
    </p>
    <p>
     下面是一个示例，展示了如何通过配置文件和工厂模式来管理多个Kafka消费者和相应的MySQL输出。
    </p>
    <p>
     #### 2.1 配置文件 (`application.yaml`)
    </p>
    <p>
     首先，定义一个配置文件来描述每个消费者的配置信息，包括Kafka主题、目标MySQL表名以及字段映射等。
    </p>
    <pre><code class="hljs">consumers:
  - name: user_activities_consumer
    kafka_topic: user_activities_topic
    mysql_table: user_activities
    fields:
      - { index: 0, column: user_id }
      - { index: 1, column: activity }

  - name: product_views_consumer
    kafka_topic: product_views_topic
    mysql_table: product_views
    fields:
      - { index: 0, column: user_id }
      - { index: 1, column: product_id }</code></pre>
    <p>
     #### 2.2 工厂类 (`ConsumerFactory.java`)
    </p>
    <p>
     创建一个工厂类，根据配置文件中的信息动态生成消费者并设置其数据处理逻辑。
    </p>
    <pre><code class="hljs">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;
import org.apache.flink.connector.jdbc.JdbcSink;
import org.apache.flink.api.common.serialization.SimpleStringSchema;

import java.util.Properties;
import java.util.List;
import java.util.Map;

public class ConsumerFactory {

    public static void createAndRegisterConsumers(StreamExecutionEnvironment env, List&lt;Map&lt;String, Object&gt;&gt; consumers) {
        Properties kafkaProperties = new Properties();
        kafkaProperties.setProperty("bootstrap.servers", "localhost:9092");
        kafkaProperties.setProperty("group.id", "test");

        for (Map&lt;String, Object&gt; consumerConfig : consumers) {
            String kafkaTopic = (String) consumerConfig.get("kafka_topic");
            String mysqlTable = (String) consumerConfig.get("mysql_table");
            List&lt;Map&lt;String, Object&gt;&gt; fields = (List&lt;Map&lt;String, Object&gt;&gt;) consumerConfig.get("fields");

            FlinkKafkaConsumer&lt;String&gt; kafkaConsumer = new FlinkKafkaConsumer&lt;&gt;(
                    kafkaTopic,
                    new SimpleStringSchema(),
                    kafkaProperties);

            env.addSource(kafkaConsumer)
               .map(value -&gt; parseMessage(value, fields))
               .addSink(JdbcSink.sink(
                       generateInsertSQL(mysqlTable, fields),
                       (statement, record) -&gt; populateStatement(statement, record, fields),
                       new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                               .withUrl("jdbc:mysql://localhost:3306/your_database")
                               .withDriverName("com.mysql.cj.jdbc.Driver")
                               .withUsername("your_username")
                               .withPassword("your_password")
                               .build()));
        }
    }

    private static Map&lt;String, Object&gt; parseMessage(String value, List&lt;Map&lt;String, Object&gt;&gt; fields) {
        String[] parts = value.split(",");
        return fields.stream().collect(Collectors.toMap(
                field -&gt; (String) field.get("column"),
                field -&gt; parts[(Integer) field.get("index")]
        ));
    }

    private static String generateInsertSQL(String table, List&lt;Map&lt;String, Object&gt;&gt; fields) {
        StringBuilder columns = new StringBuilder();
        StringBuilder placeholders = new StringBuilder();

        for (int i = 0; i &lt; fields.size(); i++) {
            if (i &gt; 0) {
                columns.append(", ");
                placeholders.append(", ");
            }
            columns.append(fields.get(i).get("column"));
            placeholders.append("?");
        }

        return "INSERT INTO " + table + " (" + columns.toString() + ") VALUES (" + placeholders.toString() + ")";
    }

    private static void populateStatement(java.sql.PreparedStatement statement, Map&lt;String, Object&gt; record, List&lt;Map&lt;String, Object&gt;&gt; fields) throws Exception {
        for (int i = 0; i &lt; fields.size(); i++) {
            String column = (String) fields.get(i).get("column");
            Object value = record.get(column);
            if (value instanceof Integer) {
                statement.setInt(i + 1, (Integer) value);
            } else if (value instanceof String) {
                statement.setString(i + 1, (String) value);
            }
            // 其他类型可以根据需要添加
        }
    }
}</code></pre>
    <p>
     #### 2.3 主程序 (`Main.java`)
    </p>
    <p>
     在主程序中加载配置文件，并调用工厂类来注册所有消费者。
    </p>
    <pre><code class="hljs">import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.yaml.snakeyaml.Yaml;

import java.io.InputStream;
import java.util.List;
import java.util.Map;

public class Main {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        Yaml yaml = new Yaml();
        InputStream inputStream = Main.class.getClassLoader().getResourceAsStream("application.yaml");
        Map&lt;String, Object&gt; config = yaml.load(inputStream);

        List&lt;Map&lt;String, Object&gt;&gt; consumers = (List&lt;Map&lt;String, Object&gt;&gt;) config.get("consumers");

        ConsumerFactory.createAndRegisterConsumers(env, consumers);

        env.execute("Multiple Kafka to Multiple MySQL Tables with Flink");
    }
}</code></pre>
    <p>
     ### 关键点解释
    </p>
    <p>
     1. **配置文件**：通过配置文件定义每个消费者的信息，使得添加新的消费者变得简单，只需修改配置文件即可。
     <br/>
     <br/>
     2. **工厂模式**：使用工厂类 `ConsumerFactory` 根据配置动态创建消费者，并为其设置数据处理逻辑和输出目标。
    </p>
    <p>
     3. **通用的数据处理逻辑**：`parseMessage` 方法根据配置文件中的字段映射解析消息，`generateInsertSQL` 和 `populateStatement` 方法则用于生成插入SQL语句和填充PreparedStatement。
    </p>
    <p>
     4. **扩展性**：这种设计方式非常灵活，易于扩展。如果需要增加新的消费者或修改现有消费者的配置，只需更新配置文件而无需更改代码逻辑。
    </p>
    <p>
     这种方法不仅减少了代码量，还提高了代码的可维护性和扩展性，使得系统更容易管理和维护。
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34323537323332322f:61727469636c652f64657461696c732f313436313430333332" class_="artid" style="display:none">
 </p>
</div>


