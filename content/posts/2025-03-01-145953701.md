---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34343036343930382f:61727469636c652f64657461696c732f313435393533373031"
layout: post
title: "C-牵手DeepSeek打造本地AI超能力"
date: 2025-03-01 21:07:07 +0800
description: "C# 牵手DeepSeek：打造本地AI超能力"
keywords: "c#接入deepseek"
categories: ['未分类']
tags: ['开发语言', '人工智能', 'C']
artid: "145953701"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145953701
    alt: "C-牵手DeepSeek打造本地AI超能力"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145953701
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145953701
cover: https://bing.ee123.net/img/rand?artid=145953701
image: https://bing.ee123.net/img/rand?artid=145953701
img: https://bing.ee123.net/img/rand?artid=145953701
---

# C# 牵手DeepSeek：打造本地AI超能力

### 一、引言

在人工智能飞速发展的当下，大语言模型如 DeepSeek 正掀起新一轮的技术变革浪潮，为自然语言处理领域带来了诸多创新应用。随着数据隐私和安全意识的提升，以及对模型部署灵活性的追求，本地部署 DeepSeek 成为众多开发者和企业关注的焦点。

对于 C# 开发者而言，将 DeepSeek 模型本地部署并集成到 C# 项目中，不仅能充分发挥 C# 语言在 Windows 平台开发的优势，还能实现高度定制化的人工智能应用，无论是开发智能客服、文本生成工具，还是集成到企业内部系统，都能极大提升效率和用户体验。本文将带领大家一步步完成 DeepSeek 的本地部署，并详细介绍如何使用 C# 调用其 API，让你轻松开启属于自己的 AI 开发之旅 。

### 二、前期准备

#### 2.1 硬件需求

DeepSeek 模型有多个不同规模的版本，从参数较少的轻量级版本到参数众多的大规模版本，不同版本对硬件的要求差异较大。以下是一些常见规模模型的硬件参考配置：

* **DeepSeek-R1-1.5B**
  ：
* + **CPU**
    ：建议使用 4 核及以上的 Intel 或 AMD 处理器，如 Intel Core i5 系列或 AMD Ryzen 5 系列 ，能够在数据预处理和模型调度等方面提供基本的运算能力。
* + **内存**
    ：8GB 及以上。虽然模型较小，但在运行时仍需要一定的内存空间来加载模型参数和处理输入数据。
* + **硬盘**
    ：至少 3GB 可用存储空间，用于存放模型文件。若考虑缓存和日志等文件，建议预留更多空间。
* + **显卡**
    ：如果仅进行简单的文本推理任务，且对速度要求不是特别高，集成显卡也可以满足；若追求更高的推理速度，可配备 NVIDIA 的入门级独立显卡，如 GTX 1650 等。
* **DeepSeek-R1-8B**
  ：
* + **CPU**
    ：推荐使用 8 核及以上的高性能处理器，如 Intel Core i7 或 i9 系列，AMD Ryzen 7 或 9 系列，多核心对于多任务处理和模型的并行计算有很大帮助。
* + **内存**
    ：16GB 及以上。随着模型规模增大，需要更多内存来存储模型参数和中间计算结果。
* + **硬盘**
    ：至少 80GB 的高速固态硬盘（SSD），模型文件较大，高速存储能加快模型的加载速度。
* + **显卡**
    ：NVIDIA RTX 3060 及以上级别显卡，显存 12GB 及以上，能显著提升模型的推理速度。
* **DeepSeek-R1-32B**
  ：
* + **CPU**
    ：采用 16 核及以上的服务器级处理器，如 Intel Xeon 系列或 AMD EPYC 系列，强大的 CPU 性能可保障复杂计算任务的高效执行。
* + **内存**
    ：64GB 及以上，满足大规模模型对内存的高需求。
* + **硬盘**
    ：至少 320GB 的高速 SSD，用于存储模型文件和临时数据。
* + **显卡**
    ：需要 2 - 4 张 NVIDIA A100 80GB 或性能相当的显卡，通过多卡并行计算来加速模型的运行。

在选择硬件时，要综合考虑自己的实际需求和预算。如果只是进行简单的测试和学习，较低配置的硬件即可；若是用于企业级应用或对性能要求较高的场景，则需要配备高性能的硬件。同时，还需注意硬件之间的兼容性，确保整个系统能够稳定运行 。

#### 2.2 软件准备

* **Ollama**
  ：
* + **作用与特点**
    ：Ollama 是一个轻量级的 AI 模型运行框架，支持在 macOS、Linux 和 Windows 等多个平台上运行，并且兼容包括 DeepSeek - R1 在内的 54 种主流开源模型。它简化了模型的下载、安装和使用流程，提供统一的操作界面，使得在本地环境中运行和测试不同语言模型变得更加便捷，就像是一个模型运行的 “容器”，让模型的管理和使用更加高效。
* + **获取与安装**
    ：进入 Ollama 官网
    [https://ollama.com/do](https://ollama.com/download)
    [wnloa](https://ollama.com/download)
    [d](https://ollama.com/download)
    ，根据自己的操作系统选择对应的安装包（Windows/Linux/macOS）。点击 Download 下载安装包后直接安装即可。安装时需注意，Ollama 默认安装在 C 盘，下载的大模型数据包也默认存放在 C 盘，所以要确保 C 盘有足够的存储空间。如果想更改安装路径，可在目标路径（如 D:\Ollaman）创建一个新文件夹并放置 Ollama 安装包，然后在该路径下打开命令窗口并输入 OllamaSetup.exe/DIR=D:\Ollama，接着在安装界面点击 “Install”，即可将 Ollama 安装到指定目录，大模型数据包也会默认下载到该目录中。安装完成后，可通过 Windows+R，输入 CMD 进入命令窗口，输入：ollama -v 查看版本号，若能看到版本号则说明安装成功，同时在任务栏右下角会出现一个羊驼的图标。
* **Page Assist 浏览器插件**
  ：
* + **作用与特点**
    ：Page Assist 是一款开源的浏览器扩展程序，主要编程语言为 TypeScript。它旨在通过本地运行的 AI 模型辅助网页浏览，可接入联网搜索，并且能方便地配合嵌入模型，为用户在浏览网页时提供智能交互和信息辅助。
* + **获取与安装**
    ：如果使用 Firefox 浏览器，可在 Firefox 商店中搜索 “Page Assist” 进行下载安装；若使用 Chrome 浏览器，需先从 GitHub 上克隆项目仓库（git clone https://github.com/n4ze3m/page-assist.git ），进入项目目录（cd page-assist），使用 Bun 安装项目所需的依赖（bun install），然后构建扩展程序（bun run build ，若为 Firefox 构建则使用 bun run build:firefox ），最后在浏览器的扩展程序管理页面，启用开发者模式，点击 “加载已解压的扩展程序” 按钮，选择 build 目录完成安装。
* **C# 开发环境**
  ：
* + **作用与特点**
    ：用于开发和运行 C# 程序，编写调用 DeepSeek 模型 API 的代码。常见的 C# 开发环境有 Visual Studio 和 Visual Studio Code 等。Visual Studio 是微软推出的一套完整的开发工具集，功能强大，集成了丰富的工具和功能，适合大型项目的开发；Visual Studio Code 则是一款轻量级、跨平台的代码编辑器，通过安装相关插件也能很好地支持 C# 开发，具有灵活、高效的特点，适合快速迭代和小型项目的开发。
* + **获取与安装**
    ：
* + - **Visual Studio**
      ：访问 Visual Studio 官网https://visualstudio.microsoft.com/zh-hans/ ，下载安装包。安装过程中，可根据自己的需求选择安装组件，对于初学者和一般的 C# 开发，建议默认勾选与 C# 开发相关的核心组件，如.NET 桌面开发等。安装过程可能需要一些时间，需保持电脑电量充足且不休眠。
* + - **Visual Studio Code**
      ：先从官网https://code.visualstudio.com/ 下载安装 Visual Studio Code，安装完成后，打开软件，使用快捷键 Ctrl + Shift + x 打开扩展，搜索并安装 “C#” 插件，同时还可以安装 “Solution Explorer” 等插件来增强开发体验。此外，还需要安装.NET 开发环境，可从微软官方下载对应版本的.NET 安装包进行安装 。

### 三、本地部署 DeepSeek

#### 3.1 安装 Ollama

1. **Windows 系统**
   ：

* + **默认安装**
    ：前往 Ollama 官网https://ollama.com/download ，点击对应 Windows 系统的下载按钮，下载安装包。下载完成后，双击安装包，按照安装向导的提示逐步完成安装，安装过程中可保持默认设置，Ollama 会自动安装到 C 盘的默认路径（C:\Users\ 用户名 \AppData\Local\Programs\Ollama ）。
* + **自定义路径安装**
    ：若想更改安装路径，可先在目标磁盘（如 D 盘）创建一个新文件夹，例如 D:\Ollama。然后以管理员身份运行命令提示符（CMD）或 PowerShell，定位到 Ollama 安装包所在目录，执行命令：OllamaSetup.exe/DIR=“D:\Ollama” ，将 “D:\Ollama” 替换为你期望的安装路径。等待安装完成，程序会自动写入该指定目录。安装完成后，在任务栏右下角会出现一个羊驼图标，表示 Ollama 已成功安装并在后台运行。还可以通过按下 Windows+R 键，输入 CMD 打开命令提示符，输入 “ollama -v” 命令来验证安装，若显示出版本号，则说明安装成功。

1. **macOS 系统**
   ：

* + **默认安装**
    ：在 Ollama 官网下载适用于 macOS 的安装包，通常是一个.dmg 文件。下载完成后，双击打开.dmg 文件，将 Ollama 图标拖动到 “应用程序” 文件夹中，完成安装。
* + **使用 Homebrew 安装（可选）**
    ：如果你的 macOS 系统安装了 Homebrew 包管理器，也可以通过 Homebrew 进行安装。打开终端，输入命令 “brew install ollama” ，Homebrew 会自动下载并安装 Ollama 及其依赖项。安装完成后，在终端中输入 “ollama -v” ，若显示出版本号，则安装成功。

1. **Linux 系统（以 Ubuntu 为例）**
   ：

* + 打开终端，使用以下命令下载并安装 Ollama：

```
curl -fsSL https://ollama.com/install.sh | sh

```

* 安装完成后，通过在终端输入 “ollama -v” 来验证安装，若显示出版本号，则说明安装成功。

安装注意事项：

* 安装前务必确保系统磁盘空间充足，特别是 C 盘（对于 Windows 系统默认安装情况），因为 Ollama 及其下载的模型文件可能会占用较大空间。
* 在安装过程中，若遇到权限不足等问题，对于 Windows 系统，以管理员身份运行安装程序或命令；对于 Linux 系统，可使用 sudo 命令获取管理员权限。
* 安装完成后，如果 Ollama 没有自动启动，可手动打开命令提示符（Windows）、终端（macOS 或 Linux），输入 “ollama serve” 命令启动 Ollama 服务 。

#### 3.2 下载并安装 DeepSeek 模型

1. 打开命令提示符（Windows）、终端（macOS 或 Linux），确保 Ollama 服务已经启动（若未启动，输入 “ollama serve” 命令启动）。
2. 访问 Ollama 的模型库页面https://ollama.ai/library ，在搜索框中输入 “deepseek” ，即可找到 DeepSeek 相关模型。
3. 根据自己的硬件配置选择合适的 DeepSeek 模型版本：

* + **普通用户或硬件配置较低的情况**
    ：可以选择 DeepSeek - R1 - 1.5B 版本，该版本模型较小，对硬件要求相对较低，能够在普通配置的电脑上运行，满足基本的文本生成、对话等需求。
* + **拥有中等配置硬件的用户**
    ：如具备 8GB 及以上内存，4GB 及以上显存的显卡，可选择 DeepSeek - R1 - 8B 版本，该版本在性能和功能上有一定提升，能处理更复杂的任务，生成的文本质量也更高。
* + **高性能显卡用户（显存 16GB 以上）**
    ：推荐选择 DeepSeek - R1 - 16B 版本，此版本模型参数更多，能够展现出更强大的性能，适用于对模型性能要求较高的场景，如专业的文本创作、智能客服等应用开发 。

1. 选择好模型后，复制模型的下载命令（一般为 “ollama pull deepseek - r1: 版本号” ，例如 “ollama pull deepseek - r1:1.5b” ）。
2. 将复制的命令粘贴到命令提示符或终端中，按下回车键执行。此时，Ollama 会自动从远程仓库下载所选的 DeepSeek 模型，下载过程中会显示下载进度和速度等信息。由于模型文件较大，下载时间可能较长，请耐心等待。下载完成后，模型会自动安装到 Ollama 的默认模型存储路径（对于 Windows 系统，默认路径为 C:\Users\ 用户名.ollama\models ；对于 macOS 和 Linux 系统，默认路径为～/.ollama/models ）。若想更改模型存储路径，可通过设置环境变量 “OLLAMA\_MODELS” 来实现，具体方法为：在系统环境变量中新建一个变量，变量名为 “OLLAMA\_MODELS” ，变量值为你希望的存储路径 。

#### 3.3 验证部署是否成功

1. **命令行验证**
   ：在命令提示符或终端中输入以下命令：

```
ollama run deepseek - r1:版本号

```

将 “版本号” 替换为你实际下载的 DeepSeek 模型版本（如 “1.5b”“8b” 等）。按下回车键后，进入模型交互界面，此时输入一些简单的问题，如 “你好，今天天气怎么样？” ，如果模型能够正常回答你的问题，则说明 DeepSeek 模型部署成功。例如：

```
ollama run deepseek - r1:1.5b
> 你好，今天天气怎么样？
DeepSeek - R1 - 1.5B：很抱歉，我无法获取实时天气信息，但你可以通过天气预报类的应用或网站来了解具体的天气情况。

```

1. **浏览器访问验证**
   ：打开浏览器，在地址栏中输入 “http://127.0.0.1:11434” ，如果看到 Ollama 的欢迎页面，说明 Ollama 服务正常运行。接着，在浏览器地址栏中输入 “http://127.0.0.1:11434/api/tags” ，若返回的 JSON 数据中包含你下载的 DeepSeek 模型信息，则进一步证明 DeepSeek 模型已成功部署。例如返回的 JSON 数据可能如下：

```
{
    "models": [
        {
            "name": "deepseek - r1:1.5b",
            "id": "xxxxxxxxxxxx",
            "parameters": {
                "dim": 2048,
                "n_heads": 32,
                "n_layers": 32,
                "norm_eps": 1e - 06,
                "vocab_size": 65024
            },
            "modelfile": "xxxxxxxxxxxx",
            "format": "gguf",
            "size": "1.37 GB",
            "modified": "2024 - 07 - 15T10:30:00Z"
        }
    ]
}

```

通过以上两种方式的验证，若都能得到预期的结果，就说明 DeepSeek 模型在本地已经成功部署，可以进行后续的开发和应用了 。

### 四、C# 调用 DeepSeek API

#### 4.1 搭建 C# 开发环境

1. **安装 Visual Studio**
   ：

* + 访问 Visual Studio 官网https://visualstudio.microsoft.com/zh-hans/ ，点击 “下载” 按钮，选择适合你系统的 Visual Studio 版本，对于个人学习和小型项目开发，免费的 Visual Studio Community Edition 通常就足够了。
* + 下载完成后，双击安装程序，在安装向导中，选择 “使用 C# 的开发” 工作负载，此工作负载包含了 C# 开发所需的核心组件，如 C# 编译器、调试工具、代码编辑器等。同时，还可以根据自己的需求，勾选其他相关组件，如
    [ASP.NET](https://ASP.NET)
    和 Web 开发组件（若打算开发 Web 应用来调用 DeepSeek API）、.NET 桌面开发组件（若要开发桌面应用）等。
* + 选择好组件后，点击 “安装” 按钮，等待安装过程完成。安装过程可能需要一些时间，期间请保持网络连接稳定，并且不要关闭安装程序。

1. **配置.NET 开发环境**
   ：

* + **安装.NET SDK**
    ：如果你的系统中尚未安装.NET SDK，需要从微软官方下载对应版本的.NET SDK 安装包。访问https://dotnet.microsoft.com/en-us/download ，根据你的系统和开发需求选择合适的.NET 版本进行下载安装。例如，若你希望使用最新的功能和性能优化，可选择下载最新的稳定版本。
* + **配置环境变量**
    ：安装完成后，需要配置环境变量。在 Windows 系统中，右键点击 “此电脑”，选择 “属性”，在弹出的窗口中点击 “高级系统设置”，然后在 “系统属性” 窗口中点击 “环境变量” 按钮。在 “系统变量” 列表中找到 “Path” 变量，点击 “编辑”，在弹出的编辑环境变量窗口中，添加.NET SDK 的安装路径（例如 C:\Program Files\dotnet ），确保该路径在 Path 变量中，以便系统能够找到.NET 相关的命令和工具。
* + **验证安装**
    ：打开命令提示符（CMD），输入 “dotnet --version” 命令，如果显示出版本号，则说明.NET SDK 安装成功，并且环境变量配置正确。例如显示 “8.0.100” ，则表示当前安装的是.NET 8.0.100 版本 。

#### 4.2 安装 OllamaSharp 依赖包

1. **打开项目**
   ：在 Visual Studio 中，打开你要集成 DeepSeek API 的 C# 项目。如果是新建项目，在创建项目时选择合适的项目模板，如 “控制台应用程序”（用于简单的测试和学习）、“
   [ASP.NET](https://ASP.NET)
   Core Web 应用程序”（用于开发 Web 服务来调用 API）等。
2. **打开 NuGet 包管理器**
   ：在 Visual Studio 的菜单栏中，选择 “项目” -> “管理 NuGet 程序包” ，这将打开 NuGet 包管理器窗口。
3. **搜索并安装 OllamaSharp**
   ：在 NuGet 包管理器窗口中，切换到 “浏览” 选项卡，在搜索框中输入 “OllamaSharp” ，然后在搜索结果中找到 “OllamaSharp” 包，点击 “安装” 按钮。此时，Visual Studio 会自动下载并安装 OllamaSharp 及其依赖项。在安装过程中，你可能会看到一些提示信息，如许可证接受提示等，按照提示操作即可。安装完成后，你可以在项目的 “依赖项” -> “NuGet” 中看到 “OllamaSharp” 包，表明安装成功 。

#### 4.3 示例代码实现

以下是一个使用 C# 调用 DeepSeek API 进行提问并获取回答的示例代码：

```
using Microsoft.AspNetCore.Mvc;
using OllamaSharp;
using System.Threading.Tasks;

namespace DeepSeekAPIDemo.Controllers
{
    [ApiController]
    [Route("api/[controller]")]
    public class DeepSeekController : ControllerBase
    {
        private readonly Uri _modelEndpoint = new Uri("http://localhost:11434");
        private readonly string _modelName = "deepseek-r1:8b";//根据实际下载的模型版本修改

        [HttpPost("ask")]
        public async Task<IActionResult> AskQuestion([FromBody] string question)
        {
            if (string.IsNullOrEmpty(question))
            {
                return BadRequest("请输入您的问题");
            }

            var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
            var responseBuilder = new System.Text.StringBuilder();

            await foreach (var update in chatClient.GetStreamingResponseAsync(question))
            {
                responseBuilder.Append(update.ToString());
            }

            var response = responseBuilder.ToString();
            return Ok(new { Response = response });
        }
    }
}

```

代码关键部分解释：

* **定义模型端点和名称**
  ：

```
private readonly Uri _modelEndpoint = new Uri("http://localhost:11434");
private readonly string _modelName = "deepseek-r1:8b";

```

\_modelEndpoint定义了 Ollama 服务的地址，默认是本地的 11434 端口；\_modelName指定了要使用的 DeepSeek 模型版本，这里是 “deepseek - r1:8b” ，请根据实际下载的模型版本进行修改。

* **提问接口实现**
  ：

```
[HttpPost("ask")]
public async Task<IActionResult> AskQuestion([FromBody] string question)

```

这是一个 HTTP POST 请求接口，名为 “ask”，接受一个字符串类型的问题作为请求体。

* **创建聊天客户端并获取响应**
  ：

```
var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
var responseBuilder = new System.Text.StringBuilder();

await foreach (var update in chatClient.GetStreamingResponseAsync(question))
{
    responseBuilder.Append(update.ToString());
}

var response = responseBuilder.ToString();
return Ok(new { Response = response });

```

首先创建一个OllamaApiClient实例，用于与 Ollama 服务进行通信。然后使用GetStreamingResponseAsync方法以流式方式获取模型对问题的回答，将每次获取到的响应追加到responseBuilder中，最后将完整的响应返回给客户端 。

#### 4.4 运行与测试

1. **运行项目**
   ：在 Visual Studio 中，点击工具栏上的 “启动” 按钮（绿色三角形图标），或者按下 F5 键，启动项目。如果是 Web 项目，Visual Studio 会自动启动一个本地 Web 服务器，并打开浏览器访问项目的 URL（例如http://localhost:5000 ，具体端口号可能因项目配置而异）。
2. **测试调用**
   ：使用工具（如 Postman）向http://localhost:5000/api/DeepSeek/ask（假设项目端口为 5000，控制器名为 DeepSeek）发送 POST 请求，在请求体中输入要提问的问题，例如 “介绍一下人工智能的发展历程” ，然后点击 “发送” 按钮。
3. **分析结果**
   ：如果一切正常，你将在 Postman 的响应窗口中看到 DeepSeek 模型返回的回答。例如：

```
{
    "Response": "人工智能的发展历程可以追溯到20世纪50年代。在1956年，达特茅斯会议正式确立了“人工智能”这一术语，标志着人工智能作为一个独立的研究领域诞生。早期的人工智能主要集中在符号推理和专家系统的研究……"
}

```

1. **可能出现的问题及解决方法**
   ：

* + **连接失败**
    ：如果收到 “无法连接到服务器” 等错误信息，首先检查 Ollama 服务是否已经启动，并且端口号（11434）是否正确。可以在命令提示符中输入 “ollama serve” 确保 Ollama 服务正常运行，同时检查防火墙设置，确保没有阻止对该端口的访问。
* + **模型未找到**
    ：如果出现 “模型未找到” 的错误，检查\_modelName是否与实际下载的模型名称和版本一致，例如是否存在拼写错误，是否与 Ollama 中已安装的模型名称匹配。
* + **其他错误**
    ：如果遇到其他异常，查看 Visual Studio 的输出窗口或调试信息，获取详细的错误堆栈跟踪，根据错误信息进行排查和解决 。

### 五、集成与优化

#### 5.1 与现有项目集成思路

1. **Web 应用集成**
   ：

* + [**ASP.NET**](https://ASP.NET)
    **Core Web API**
    ：如果你的项目是基于
    [ASP.NET](https://ASP.NET)
    Core 的 Web API，可直接在控制器中添加调用 DeepSeek API 的方法。例如，在现有的控制器类中，添加一个新的 API 端点，用于接收用户的问题并返回 DeepSeek 模型的回答。代码如下：

```
[ApiController]
[Route("api/ai")]
public class AIController : ControllerBase
{
    private readonly Uri _modelEndpoint = new Uri("http://localhost:11434");
    private readonly string _modelName = "deepseek-r1:8b";

    [HttpPost("ask")]
    public async Task<IActionResult> AskQuestion([FromBody] string question)
    {
        if (string.IsNullOrEmpty(question))
        {
            return BadRequest("请输入您的问题");
        }

        var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
        var responseBuilder = new System.Text.StringBuilder();

        await foreach (var update in chatClient.GetStreamingResponseAsync(question))
        {
            responseBuilder.Append(update.ToString());
        }

        var response = responseBuilder.ToString();
        return Ok(new { Response = response });
    }
}

```

这样，前端应用可以通过 HTTP POST 请求到/api/ai/ask端点，将用户的问题发送给后端，后端调用 DeepSeek 模型获取回答后返回给前端。

* **MVC 应用**
  ：对于传统的
  [ASP.NET](https://ASP.NET)
  MVC 应用，可在控制器中创建一个视图模型，用于传递用户问题和模型回答。在视图中，通过表单将用户问题提交到控制器的某个 Action 方法，该方法调用 DeepSeek API 获取回答后，再将结果传递回视图进行显示。例如：

```
public class AIViewModel
{
    public string Question { get; set; }
    public string Answer { get; set; }
}

public class AIController : Controller
{
    private readonly Uri _modelEndpoint = new Uri("http://localhost:11434");
    private readonly string _modelName = "deepseek-r1:8b";

    public IActionResult Index()
    {
        return View(new AIViewModel());
    }

    [HttpPost]
    public async Task<IActionResult> Index(AIViewModel model)
    {
        if (ModelState.IsValid)
        {
            var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
            var responseBuilder = new System.Text.StringBuilder();

            await foreach (var update in chatClient.GetStreamingResponseAsync(model.Question))
            {
                responseBuilder.Append(update.ToString());
            }

            model.Answer = responseBuilder.ToString();
            return View(model);
        }
        return View(model);
    }
}

```

在视图中，使用 HTML 表单收集用户问题，并显示模型回答：

```
@model AIViewModel
@{
    ViewBag.Title = "AI交互";
}
<h2>AI交互</h2>
@using (Html.BeginForm())
{
    @Html.LabelFor(m => m.Question, "请输入问题")
    @Html.TextBoxFor(m => m.Question, new { @class = "form-control" })
    <input type="submit" value="提问" class="btn btn-primary" />
    if (!string.IsNullOrEmpty(Model.Answer))
    {
        <div class="mt-3">
            <strong>回答:</strong>
            <p>@Model.Answer</p>
        </div>
    }
}

```

1. **桌面应用集成**
   ：

* + **WinForms 应用**
    ：在 WinForms 应用中，可通过按钮点击事件触发调用 DeepSeek API 的操作。首先，在界面上添加一个文本框用于输入问题，一个按钮用于提交问题，以及一个文本框或 RichTextBox 用于显示回答。然后，在按钮的点击事件处理方法中编写调用代码：

```
private async void btnAsk_Click(object sender, EventArgs e)
{
    string question = textBoxQuestion.Text;
    if (string.IsNullOrEmpty(question))
    {
        MessageBox.Show("请输入问题");
        return;
    }

    var _modelEndpoint = new Uri("http://localhost:11434");
    var _modelName = "deepseek-r1:8b";
    var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
    var responseBuilder = new System.Text.StringBuilder();

    await foreach (var update in chatClient.GetStreamingResponseAsync(question))
    {
        responseBuilder.Append(update.ToString());
    }

    string response = responseBuilder.ToString();
    textBoxAnswer.Text = response;
}

```

* **WPF 应用**
  ：对于 WPF 应用，同样可以在 XAML 界面中设计输入和输出控件，然后在代码背后文件中通过命令绑定或事件处理来实现调用。例如，在 XAML 中定义界面：

```
<Window x:Class="WpfApp1.MainWindow"
        xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"
        xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
        Title="WPF与DeepSeek集成" Height="350" Width="525">
    <Grid>
        <StackPanel HorizontalAlignment="Center" VerticalAlignment="Center">
            <TextBlock Text="请输入问题:" Margin="5"/>
            <TextBox x:Name="txtQuestion" Margin="5"/>
            <Button Content="提问" Click="btnAsk_Click" Margin="5"/>
            <TextBlock Text="回答:" Margin="5"/>
            <TextBox x:Name="txtAnswer" Margin="5" IsReadOnly="True"/>
        </StackPanel>
    </Grid>
</Window>

```

在代码背后文件中编写点击事件处理方法：

```
private async void btnAsk_Click(object sender, RoutedEventArgs e)
{
    string question = txtQuestion.Text;
    if (string.IsNullOrEmpty(question))
    {
        MessageBox.Show("请输入问题");
        return;
    }

    var _modelEndpoint = new Uri("http://localhost:11434");
    var _modelName = "deepseek-r1:8b";
    var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
    var responseBuilder = new System.Text.StringBuilder();

    await foreach (var update in chatClient.GetStreamingResponseAsync(question))
    {
        responseBuilder.Append(update.ToString());
    }

    string response = responseBuilder.ToString();
    txtAnswer.Text = response;
}

```

1. **移动应用集成（Xamarin.Forms）**
   ：

* + 在 Xamarin.Forms 应用中，创建一个跨平台的页面用于用户输入和显示回答。通过 HttpClient 或 OllamaSharp 库来调用 DeepSeek API。例如，在 XAML 中设计页面：

```
<ContentPage xmlns="http://xamarin.com/schemas/2014/forms"
             xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
             x:Class="XamarinApp1.MainPage">
    <StackLayout Padding="15">
        <Label Text="请输入问题:"/>
        <Entry x:Name="entryQuestion"/>
        <Button Text="提问" Clicked="btnAsk_Click"/>
        <Label Text="回答:"/>
        <Label x:Name="labelAnswer" LineBreakMode="WordWrap"/>
    </StackLayout>
</ContentPage>

```

在代码背后文件中编写点击事件处理方法：

```
private async void btnAsk_Click(object sender, EventArgs e)
{
    string question = entryQuestion.Text;
    if (string.IsNullOrEmpty(question))
    {
        await DisplayAlert("提示", "请输入问题", "确定");
        return;
    }

    var _modelEndpoint = new Uri("http://localhost:11434");
    var _modelName = "deepseek-r1:8b";
    var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
    var responseBuilder = new System.Text.StringBuilder();

    await foreach (var update in chatClient.GetStreamingResponseAsync(question))
    {
        responseBuilder.Append(update.ToString());
    }

    string response = responseBuilder.ToString();
    labelAnswer.Text = response;
}

```

需要注意的是，在移动应用中调用本地 API 时，可能需要处理网络权限和跨域问题，确保应用能够正常访问本地的 Ollama 服务 。

#### 5.2 性能优化技巧

1. **缓存策略**
   ：

* + **内存缓存**
    ：使用 C# 的MemoryCache类来实现简单的内存缓存。在每次调用 DeepSeek API 前，先检查缓存中是否已经存在该问题的回答。如果存在，直接返回缓存结果，避免重复调用模型。例如：

```
private static MemoryCache _cache = new MemoryCache(new MemoryCacheOptions());

public async Task<IActionResult> AskQuestion([FromBody] string question)
{
    if (string.IsNullOrEmpty(question))
    {
        return BadRequest("请输入您的问题");
    }

    if (_cache.TryGetValue(question, out string cachedAnswer))
    {
        return Ok(new { Response = cachedAnswer });
    }

    var _modelEndpoint = new Uri("http://localhost:11434");
    var _modelName = "deepseek-r1:8b";
    var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
    var responseBuilder = new System.Text.StringBuilder();

    await foreach (var update in chatClient.GetStreamingResponseAsync(question))
    {
        responseBuilder.Append(update.ToString());
    }

    string response = responseBuilder.ToString();
    _cache.Set(question, response, TimeSpan.FromMinutes(30));//缓存30分钟
    return Ok(new { Response = response });
}

```

* **分布式缓存（如 Redis）**
  ：对于分布式系统或多实例部署的应用，可使用 Redis 作为分布式缓存。通过 StackExchange.Redis 库来操作 Redis 缓存。首先，安装StackExchange.Redis NuGet 包，然后在代码中实现缓存逻辑：

```
private static Lazy<ConnectionMultiplexer> _lazyConnection = new Lazy<ConnectionMultiplexer>(() =>
{
    return ConnectionMultiplexer.Connect("localhost:6379");
});

private static IDatabase _cache = _lazyConnection.Value.GetDatabase();

public async Task<IActionResult> AskQuestion([FromBody] string question)
{
    if (string.IsNullOrEmpty(question))
    {
        return BadRequest("请输入您的问题");
    }

    RedisValue cachedAnswer = _cache.StringGet(question);
    if (!cachedAnswer.IsNullOrEmpty)
    {
        return Ok(new { Response = cachedAnswer });
    }

    var _modelEndpoint = new Uri("http://localhost:11434");
    var _modelName = "deepseek-r1:8b";
    var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
    var responseBuilder = new System.Text.StringBuilder();

    await foreach (var update in chatClient.GetStreamingResponseAsync(question))
    {
        responseBuilder.Append(update.ToString());
    }

    string response = responseBuilder.ToString();
    _cache.StringSet(question, response, TimeSpan.FromMinutes(30));//缓存30分钟
    return Ok(new { Response = response });
}

```

1. **异步处理**
   ：

* + **充分利用异步方法**
    ：在调用 DeepSeek API 时，使用await关键字来等待异步操作完成，避免阻塞主线程。例如，GetStreamingResponseAsync方法是异步的，在调用时应使用await foreach来处理流式响应，确保程序在等待响应的过程中可以继续执行其他任务，提高应用的响应性。

```
var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);
var responseBuilder = new System.Text.StringBuilder();

await foreach (var update in chatClient.GetStreamingResponseAsync(question))
{
    responseBuilder.Append(update.ToString());
}

```

* **任务并行**
  ：如果需要同时处理多个问题的请求，可以使用Task.WhenAll方法来并行执行多个异步操作。例如，假设有多个问题需要同时获取回答：

```
var questions = new[] { "问题1", "问题2", "问题3" };
var tasks = new List<Task<string>>();
var chatClient = new OllamaApiClient(_modelEndpoint, _modelName);

foreach (var q in questions)
{
    tasks.Add(Task.Run(async () =>
    {
        var responseBuilder = new System.Text.StringBuilder();
        await foreach (var update in chatClient.GetStreamingResponseAsync(q))
        {
            responseBuilder.Append(update.ToString());
        }
        return responseBuilder.ToString();
    }));
}

var results = await Task.WhenAll(tasks);

```

1. **优化硬件配置**
   ：

* + **升级显卡**
    ：如果硬件支持，将显卡升级为更高性能的型号，如 NVIDIA RTX 40 系列显卡。更高性能的显卡拥有更多的 CUDA 核心和更大的显存，能够显著提升模型的推理速度，特别是对于大规模的 DeepSeek 模型。
* + **增加内存**
    ：确保系统有足够的内存来加载模型和处理数据。对于较大规模的 DeepSeek 模型，建议将内存升级到 32GB 或 64GB，避免因内存不足导致模型加载缓慢或运行出错。
* + **使用高速存储**
    ：将模型文件存储在高速固态硬盘（SSD）上，如 NVMe 协议的 M.2 SSD。高速存储可以加快模型文件的读取速度，减少模型加载时间，从而提高整体的调用性能 。

### 六、常见问题及解决方案

在部署和调用 DeepSeek 的过程中，可能会遇到各种各样的问题，以下是一些常见问题及对应的解决方案：

#### 6.1 模型下载与加载问题

* **问题**
  ：模型下载速度过慢甚至无法下载。
* + **原因**
    ：可能是网络不稳定，或者下载源服务器负载过高，也有可能是本地网络设置存在限制。
* + **解决方案**
    ：首先尝试切换网络环境，比如从 Wi-Fi 切换到移动数据，或者更换网络接入点。若使用的是公共网络，可尝试在网络使用低谷期进行下载。还可以使用代理服务器，在 Ollama 的配置中设置代理，帮助绕过本地网络限制。另外，有些用户反馈在下载过程中按下 Ctrl+C 取消下载，再重新开始时，下载速度会变快，因为重新连接可能会绕过网络拥堵或提升下载路径的速度。
* **问题**
  ：模型加载失败，提示 “error loading model: unable to allocate CUDA\_Host buffer” 。
* + **原因**
    ：通常是由于电脑的显存不足，模型无法分配到足够的 CUDA 显存来加载。
* + **解决方案**
    ：换用较小规模的模型，例如从 DeepSeek - R1 - 8B 切换到 DeepSeek - R1 - 1.5B。如果一定要使用当前模型，可以尝试在部署时指定用 CPU 来运行，不过这样速度会比较慢，在 Ollama 中运行模型的命令后添加 “–cpu” 参数，如 “ollama run deepseek - r1:8b --cpu” 。
* **问题**
  ：在 ChatBox 中无法找到已下载的模型。
* + **原因**
    ：可能是模型路径配置错误，或者模型文件没有存储在 ChatBox 预期的目录中。
* + **解决方案**
    ：检查模型文件是否在正确的目录中，默认情况下，Ollama 会将模型存储在指定的本地目录中（对于 Windows 系统，默认路径为 C:\Users\ 用户名.ollama\models ；对于 macOS 和 Linux 系统，默认路径为～/.ollama/models ）。如果模型文件确实存在，但 ChatBox 中找不到，可在 Ollama 的设置中查看并配置模型文件的路径，确保该路径指向下载模型的正确位置 。

#### 6.2 API 调用问题

* **问题**
  ：API 调用报错，提示 “Error: Post “http://127.0.0.1:11434/api/show”: dial tcp
  [127.0.0.1:11434](http://127.0.0.1:11434)
  : connectex: No connection could be made because the target machine actively refused it” 。
* + **原因**
    ：这个错误通常表示 Ollama 本地服务没有正常启动，或者网络配置有问题，比如端口被占用，防火墙拦截了 Ollama 服务的端口。
* + **解决方案**
    ：首先检查 Ollama 服务是否已启动，在命令提示符中输入 “netstat -an | findstr 11434”（Windows 系统）或 “lsof -i :11434”（macOS/Linux 系统）查看端口 11434 是否被占用。如果端口被占用，需要找到占用该端口的程序并关闭它，或者更改 Ollama 的端口配置。若怀疑是防火墙问题，可以尝试暂时关闭防火墙，或者在防火墙设置中允许端口 11434 通过。更改 Ollama 端口配置的方法是，找到 Ollama 的配置文件（Windows: C:\Users\ 用户名.ollama\config.json ；macOS/Linux: ~/.ollama/config.json ），打开配置文件，找到与端口相关的设置项，将 11434 更改为一个未被占用的端口号，例如 11435 。
* **问题**
  ：API 调用返回乱码或错误的响应内容。
* + **原因**
    ：可能是客户端和服务端之间的字符集不一致，也有可能是 API 请求格式不正确，或者模型本身出现异常。
* + **解决方案**
    ：首先确保客户端和服务端之间的字符集一致，通常采用 UTF - 8 编码能够有效减少乱码现象的发生。如果响应头中 Content - Type 包含 charset=UTF - 8，则说明服务器已经指定了正确的编码方式。接着，仔细确认 API 请求 URL 和参数是否正确无误，对于 DeepSeek 通过 Ollama 的 API 调用，base\_url 应该指向 Ollama 服务地址 “http://localhost:11434” ，同时需要正确指定模型名称，如 “deepseek - r1:8b” 。另外，查看返回的具体错误提示，根据提示信息进行排查和调整。如果是 HTTP 状态码异常（比如 4xx/5xx 类型），则需按照相应代码含义调整请求策略；对于 JSON 响应体内的 error 字段描述也要给予充分重视 。

#### 6.3 性能相关问题

* **问题**
  ：模型推理速度过慢，响应时间长。
* + **原因**
    ：可能是硬件配置不足，例如 CPU 性能低、内存不足、显卡性能差等，也有可能是模型参数设置不合理，或者系统中存在其他占用大量资源的程序。
* + **解决方案**
    ：升级硬件配置，如增加内存、更换高性能显卡等。优化模型参数设置，根据硬件实际情况调整模型的批处理大小、精度等参数。关闭其他不必要的应用程序，释放系统资源。此外，如果使用的是支持多线程或多进程的环境，可以尝试并行处理多个请求，提高整体的处理效率 。
* **问题**
  ：部署过程中出现 “CUDA 版本不兼容导致编译失败” 。
* + **原因**
    ：DeepSeek 核心组件（如自定义算子）需通过 PTX 代码动态编译，对 CUDA Toolkit 版本敏感度极高，新版 CUDA（≥12.x）默认弃用 SM\_70 以下架构，导致旧显卡（如 V100）无法识别指令集，在多 GPU 混搭场景下，驱动层对不同计算能力的硬件支持存在冲突。
* + **解决方案**
    ：根据显卡型号和 DeepSeek 的要求，选择合适的 CUDA 版本进行安装。如果是多 GPU 混搭场景，确保各显卡的计算能力与 CUDA 版本兼容，必要时调整显卡配置或更新驱动程序 。

### 七、总结与展望

#### 7.1 总结

通过上述步骤，我们成功地在本地完成了 DeepSeek 模型的部署，并实现了使用 C# 语言调用其 API 的功能。从前期对硬件和软件的细致准备，到一步步完成 Ollama 的安装、DeepSeek 模型的下载与部署，再到搭建 C# 开发环境、安装依赖包并编写示例代码进行调用，每一个环节都为我们构建高效的私有 AI 服务奠定了坚实基础。

在这个过程中，我们深刻体会到了本地部署 DeepSeek 的显著优势。它不仅能够有效保障数据的隐私与安全，避免数据在云端传输和存储过程中可能面临的泄露风险，还赋予了我们更高的自主性和灵活性，使我们可以根据实际需求对模型进行定制化开发和优化。无论是将其集成到现有的 Web 应用、桌面应用还是移动应用中，DeepSeek 都展现出了强大的自然语言处理能力，为用户提供了智能化的交互体验，极大地拓展了应用的功能边界和价值。

#### 7.2 展望

展望未来，随着 C# 生态系统的不断发展壮大以及 DeepSeek 模型的持续优化升级，我们有理由期待在 C# 开发中应用 DeepSeek 会取得更加丰硕的成果。在实际项目中，我们可以进一步探索如何将 DeepSeek 与其他先进技术，如机器学习、计算机视觉等进行深度融合，打造出更加智能、多元化的应用场景。例如，结合图像识别技术，实现对图片内容的智能描述和分析；融合机器学习算法，进行个性化的用户行为预测和推荐。

同时，希望广大读者能够基于本文的基础，积极探索更多的应用可能性。可以尝试对现有示例代码进行优化和扩展，开发出更具创新性的功能。也可以将 DeepSeek 应用到不同的行业领域中，如医疗、金融、教育等，为解决实际业务问题提供新的思路和方法。相信在不断的实践和探索中，我们能够充分挖掘 DeepSeek 的潜力，为人工智能的发展贡献自己的力量，创造出更多具有价值和影响力的应用 。