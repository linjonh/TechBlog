---
layout: post
title: "基于PyTorch的深度学习3Tensor与Autograd"
date: 2025-03-05 21:39:54 +0800
description: "可以调用.detach()或with torch.no_grad()：​，将不再计算张量的梯度，跟踪张量的历史记录。在整个过程中，PyTorch采用计算图的形式进行组织，该计算图为动态图，且在每次前向传播时，将重新构建。(Leaf Node)的Tensor，使用requires_grad（可微分性）参数指定是否记录对其的操作，以便之后利用backward()方法进行梯度求解。在神经网络中，一个重要内容就是进行参数的学习，而参数的学习离不开求导，那么PyTorch是如何进行求导的呢？该属性表示梯度函数。"
keywords: "基于PyTorch的深度学习3——Tensor与Autograd"
categories: ['未分类']
tags: ['深度学习', '人工智能', 'Pytorch']
artid: "146002088"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146002088
    alt: "基于PyTorch的深度学习3Tensor与Autograd"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146002088
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146002088
cover: https://bing.ee123.net/img/rand?artid=146002088
image: https://bing.ee123.net/img/rand?artid=146002088
img: https://bing.ee123.net/img/rand?artid=146002088
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     基于PyTorch的深度学习3——Tensor与Autograd
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     在神经网络中，一个重要内容就是进行参数的学习，而参数的学习离不开求导，那么PyTorch是如何进行求导的呢？
    </p>
    <p>
     Autograd包为张量上所有的操作提供了自动求导功能，torch.Tensor和torch.Function是Autograd的两个核心类，它们相互连接并生成一个有向非循环图。接下来我们先简单介绍Tensor如何实现自动求导，然后介绍计算图，最后用代码来实现这些功能。
    </p>
    <p>
     为实现对Tensor自动求导，需考虑如下事项：
    </p>
    <p>
     1) 创建
     <strong>
      叶节点
     </strong>
     (Leaf Node)的Tensor，使用requires_grad（可微分性）参数指定是否记录对其的操作，以便之后利用backward()方法进行梯度求解。
     <strong>
      requires_grad
     </strong>
     参数的缺省值为False，如果要对其求导需设置为True，然后与之有依赖关系的节点的requires_grad参数会自动变为True。
    </p>
    <p>
     2) 可利用requires_grad_()方法修改Tensor的requires_grad属性。可以调用.detach()或with torch.no_grad()：​，将不再计算张量的梯度，跟踪张量的历史记录。这点在评估模型、测试模型阶段中常常用到。
    </p>
    <p>
     3) 通过计算创建的Tensor（非叶子节点）​，会自动被赋予
     <strong>
      grad_fn（保存了函数关系）
     </strong>
     属性。该属性表示梯度函数。叶节点的
     <strong>
      grad_fn为None，因为是手动创建的初始的张量
     </strong>
     。
    </p>
    <p>
     4) 最后得到的Tensor执行backward()函数，此时自动计算各变量的
     <strong>
      梯度
     </strong>
     ，并将累加结果保存到
     <strong>
      grad
     </strong>
     属性中。计算完成后，非叶节点的梯度自动释放。
    </p>
    <p>
     5) backward()函数接收参数，该
     <strong>
      参数应和调用backward()函数的Tensor的维度相同
     </strong>
     ，或者是可broadcast的维度。如果求导的Tensor为标量（即一个数字）​，则backward中的参数可省略。
    </p>
    <p>
     6) 反向传播的中间缓存会被清空，如果需要进行多次反向传播，需要指定backward中的参数
     <strong>
      retain_graph=True。多次反向传播
     </strong>
     时，梯度是累加的。
    </p>
    <p>
     7) 非叶节点的梯度backward调用后即被清空。
    </p>
    <p>
     8) 可以通过用torch.no_grad()包裹代码块的形式来阻止autograd去跟踪那些标记为.requesgrad=True的张量的历史记录。这步在测试阶段经常使用。
    </p>
    <p>
     在整个过程中，PyTorch采用计算图的形式进行组织，该计算图为动态图，且在每次前向传播时，将重新构建。其他深度学习架构，如TensorFlow、Keras一般为静态图。接下来我们介绍计算图，用图的形式来描述就更直观了，该计算图为有向无环图(DAG)
     <img alt="" height="658" src="https://i-blog.csdnimg.cn/direct/bd632ab5f18b4eee8a72c2f8ad42d7da.png" width="1320">
      <img alt="" height="707" src="https://i-blog.csdnimg.cn/direct/f894066e07dd40089f52c8547ea09f06.png" width="1324">
       <img alt="" height="910" src="https://i-blog.csdnimg.cn/direct/ad30ae991d754a2fb4829650e0766802.png" width="1320">
        <img alt="" height="967" src="https://i-blog.csdnimg.cn/direct/f347014730e745e6a2f8651c92723d60.png" width="1263"/>
       </img>
      </img>
     </img>
    </p>
   </div>
  </div>
 </article>
 <p alt="687474:70733a2f2f626c6f672e6373646e2e6e65742f57697334652f:61727469636c652f64657461696c732f313436303032303838" class_="artid" style="display:none">
 </p>
</div>


