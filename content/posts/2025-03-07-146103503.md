---
layout: post
title: "PyTorch中的损失函数F.nll_loss-与-nn.CrossEntropyLoss"
date: 2025-03-07 19:41:28 +0800
description: "无论是图像分类、文本分类还是其他类型的分类任务，交叉熵损失（Cross Entropy Loss）都是最常用的一种损失函数。它衡量的是模型预测的概率分布与真实标签之间的差异。F.nll_loss和。F.nll_loss是负对数似然损失（Negative Log Likelihood Loss），主要用于多类分类问题。它的输入是对数概率（log-probabilities），这意味着在使用F.nll_loss之前，我们需要先对模型的输出应用函数，将原始输出转换为对数概率形式。# 创建一些虚拟数据。"
keywords: "PyTorch中的损失函数：F.nll_loss 与 nn.CrossEntropyLoss"
categories: ['未分类']
tags: ['损失函数', '人工智能', '交叉熵', 'Pytorch', 'Python']
artid: "146103503"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146103503
    alt: "PyTorch中的损失函数F.nll_loss-与-nn.CrossEntropyLoss"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146103503
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146103503
cover: https://bing.ee123.net/img/rand?artid=146103503
image: https://bing.ee123.net/img/rand?artid=146103503
img: https://bing.ee123.net/img/rand?artid=146103503
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     PyTorch中的损失函数：F.nll_loss 与 nn.CrossEntropyLoss
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <p>
    </p>
    <h3>
     <a id="_1">
     </a>
     背景介绍
    </h3>
    <p>
     无论是图像分类、文本分类还是其他类型的分类任务，交叉熵损失（Cross Entropy Loss）都是最常用的一种损失函数。它衡量的是模型预测的概率分布与真实标签之间的差异。在 PyTorch 中，有两个特别值得注意的实现：
     <code>
      F.nll_loss
     </code>
     和
     <code>
      nn.CrossEntropyLoss
     </code>
     。
    </p>
    <h3>
     <a id="Fnll_loss_5">
     </a>
     F.nll_loss
    </h3>
    <h4>
     <a id="_7">
     </a>
     什么是负对数似然损失？
    </h4>
    <p>
     <code>
      F.nll_loss
     </code>
     是负对数似然损失（Negative Log Likelihood Loss），主要用于多类分类问题。它的输入是对数概率（log-probabilities），这意味着在使用
     <code>
      F.nll_loss
     </code>
     之前，我们需要先对模型的输出应用
     <code>
      log_softmax
     </code>
     函数，将原始输出转换为对数概率形式。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> TensorDataset

<span class="token comment"># 创建一些虚拟数据</span>
features <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有100个样本，每个样本有20个特征</span>
labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有3个类别</span>

<span class="token comment"># 创建数据加载器</span>
dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">SimpleModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 输入维度为20，输出维度为3（对应3个类别）</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

model_nll <span class="token operator">=</span> SimpleModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model_nll<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> inputs<span class="token punctuation">,</span> targets <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清除梯度</span>
    outputs <span class="token operator">=</span> model_nll<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>  <span class="token comment"># 模型前向传播</span>
    log_softmax_outputs <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 应用 log_softmax</span>
    loss <span class="token operator">=</span> F<span class="token punctuation">.</span>nll_loss<span class="token punctuation">(</span>log_softmax_outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>  <span class="token comment"># 计算 nll_loss</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 反向传播</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新权重</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Batch Loss with F.nll_loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_47">
     </a>
     应用场景
    </h4>
    <p>
     由于
     <code>
      F.nll_loss
     </code>
     需要预先计算
     <code>
      log_softmax
     </code>
     ，这为用户提供了一定程度的灵活性，尤其是在需要复用
     <code>
      log_softmax
     </code>
     结果的情况下。
    </p>
    <h3>
     <a id="nnCrossEntropyLoss_51">
     </a>
     nn.CrossEntropyLoss
    </h3>
    <h4>
     <a id="_53">
     </a>
     简化工作流程
    </h4>
    <p>
     相比之下，
     <code>
      nn.CrossEntropyLoss
     </code>
     更加直接和易用。它结合了
     <code>
      log_softmax
     </code>
     和
     <code>
      nll_loss
     </code>
     的功能，因此可以直接接受未经归一化的原始输出作为输入，内部自动完成这两个步骤。
    </p>
    <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> TensorDataset

<span class="token comment"># 创建一些虚拟数据</span>
features <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有100个样本，每个样本有20个特征</span>
labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 假设有3个类别</span>

<span class="token comment"># 创建数据加载器</span>
dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>features<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">SimpleModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SimpleModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 输入维度为20，输出维度为3（对应3个类别）</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

model_ce <span class="token operator">=</span> SimpleModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model_ce<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> inputs<span class="token punctuation">,</span> targets <span class="token keyword">in</span> data_loader<span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 清除梯度</span>
    outputs <span class="token operator">=</span> model_ce<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>  <span class="token comment"># 模型前向传播</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>  <span class="token comment"># 直接计算交叉熵损失，内部包含 log_softmax</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 反向传播</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 更新权重</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Batch Loss with nn.CrossEntropyLoss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.4f</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre>
    <h4>
     <a id="_93">
     </a>
     内部机制
    </h4>
    <p>
     实际上，
     <code>
      nn.CrossEntropyLoss
     </code>
     =
     <code>
      log_softmax
     </code>
     +
     <code>
      nll_loss
     </code>
     。这种设计简化了用户的代码编写过程，特别是当不需要对中间结果进行额外操作时。
    </p>
    <h3>
     <a id="_97">
     </a>
     区别与联系
    </h3>
    <ul>
     <li>
      <p>
       <strong>
        输入要求
       </strong>
       ：
       <code>
        F.nll_loss
       </code>
       要求输入为
       <code>
        log_softmax
       </code>
       后的结果；而
       <code>
        nn.CrossEntropyLoss
       </code>
       可以直接接受未经
       <code>
        softmax
       </code>
       处理的原始输出。
      </p>
     </li>
     <li>
      <p>
       <strong>
        灵活性
       </strong>
       ：如果需要对
       <code>
        log_softmax
       </code>
       结果进行进一步处理或调试，那么
       <code>
        F.nll_loss
       </code>
       提供了更大的灵活性。
      </p>
     </li>
     <li>
      <p>
       <strong>
        便捷性
       </strong>
       ：对于大多数用户而言，
       <code>
        nn.CrossEntropyLoss
       </code>
       因其简洁性和内置的
       <code>
        log_softmax
       </code>
       步骤，是更方便的选择。
      </p>
     </li>
    </ul>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f35313532343530342f:61727469636c652f64657461696c732f313436313033353033" class_="artid" style="display:none">
 </p>
</div>


