---
layout: post
title: 人工智能深度学习
date: 2025-01-15 09:26:41 +08:00
categories: ['Ai']
tags: ['神经网络', '深度学习', '人工智能']
image:
    path: https://img-blog.csdnimg.cn/d397c079c2eb4ebf9b44c9ce35260b5a.png?x-oss-process&#61;image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5ZK46bG8566Y,size_20,color_FFFFFF,t_70,g_se,x_16
    alt: 人工智能深度学习
artid: 123180862
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=123180862
featuredImagePreview: https://bing.ee123.net/img/rand?artid=123180862
---

# 人工智能——深度学习

#### 人工智能——深度学习

* [深度学习与机器学习的关系](#_7)
* [深度学习怎么来的](#_12)
* [浅层学习（Shallow Learning）](#Shallow_Learning_30)
* + [感知机](#_31)
  + [BP算法（误差反向传播算法）](#BP_44)
  + [支持向量机](#_55)
* [深度学习（Deep Learning）](#Deep_Learning_67)
* + [深度学习的方法](#_72)
  + [卷积神经网络（CNN）](#CNN_76)
  + [循环神经网络（RNN）](#RNN_81)
  + [玻尔兹曼机（BM）](#BM_89)

**深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。深度学习是无监督学习的一种。**

---

## 深度学习与机器学习的关系

可以理解为应用了多层神经网络的机器学习，就是深度学习。（中间层2层，加上输入输出层，共4层）
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/2de62276a37549867bbc0d6769b99c6f.png)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/49a495a20a19821a37b09edf7bd9fb61.png)
  
深度学习的概念源于人工神经网络的研究。
**含多隐层的多层感知器就是一种深度学习结构**
。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

## 深度学习怎么来的

从机器学习的模型结构层次来分，机器学习经历了两次浪潮：

1. 浅层学习（Shallow Learning）：机器学习第一次浪潮
     
   ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/245ba47c690602f1958775046e1f61e7.png)
2. 深度学习（Deep Learning）：机器学习第二次浪潮

> 2006年，加拿大多伦多大学教授、机器学习领域的泰斗Geoffrey Hinton和他的学生Ruslan
>   
> Salakhutdinov在《科学》上发表了一篇文章，开启了深度学习在学术界和工业界的浪潮。
>   
> **这篇文章有两个主要观点**
> ：
>   
> 1）多隐层的人工神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；
>   
> 2）深度神经网络在训练上的难度，可以通过“
> **逐层初始化**
> ”来有效克服，在这篇文章中，逐层初始化是通过无监督学习实现的。

**区别于传统的浅层学习，深度学习的不同在于**
：

> 1）
> **强调了模型结构的深度**
> ，通常有5层、6层，甚至10多层的隐层节点；
>   
> 2）明确突出了
> **特征学习的重要性**
> ，也就是说，
> **通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更加容易**
> 。

## 浅层学习（Shallow Learning）

### 感知机

在第一次人工智能浪潮时，
**弗兰克.罗森布拉特(Frank.Roseblatt)1957年**
有了“感知机”的构思，他将‘’人工神经元‘’ 与心理学家唐纳德·赫布与1949 年发表的
**赫布定律**
构思结合在了一起。

> **赫布定律**
> 描述了突触可塑性的基本原理，即突触前神经元向突触后神经元的持续重复的刺激可以导致突触传递效能的增加。
>   
> 简单说就是：“
> **突触前后的神经元在同一时间被激发时，突出联系会加强**
> 。”
>   
> ![>](https://i-blog.csdnimg.cn/blog_migrate/1cae0d8bb77129933b922a800ec62832.png)

利用赫布定律，将神经元的结合转变成数理模型的就是“感知机”。
**感知机是将人工神经元排列成两层联系在一起的构造。**
  
在感知集中可以
**使用实数**
，通过
**调整结合强度（权重）**
，感知机渐渐学会了监督学习 。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/df97206ed8c052b16d465006e48058cc.png)
  
**感知机局限**
  
比如
**线性不可分**
，感知机不能处理
**一条线无法分割的数据**
。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/2a9a0e66dbf929f5a45cf603b840e981.png)

### BP算法（误差反向传播算法）

> 误差反向传播算法(英:Backpropagation algorithm，简称:BP算法)是一种监督学习算法，常被用来训练多层感知机。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/bf523ec3a9e385c28b7cdce54b8048db.png)

**BP算法的特征是将误差反向传播**
  
在计算机
**没有得出正确答案**
或者是
**偏离期待数值**
时，BP算法可以
**将误差从输出层反向传回，纠正各个神经元的错误，从而减少误差。**
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ac624e4c4ab19bb922f6014d2ce1239e.png)

BP算法通过上述顺序来
**调整权重误差**
。这样可以解决双层感知机无法解决的“非线性分离问题”

### 支持向量机

> 支持向量机(Support Vector Machine，SVM)是Corinna Cortes和Vapnik等于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。
> **用于模型识别的监督机器学习算法**

在
**间隔最大化**
的构思下，向量机不仅仅泛化能力高，而且拥有十分优秀的模型识别能力。

> 间隔最大化
>   
> 在BP算法中，稍微调整，改变神经网络的状态，在正确识别出学习数据的那一刻，
> **学习随之终止**
> 。因此，有时候会出现
> **集合体的边缘触碰到线的情况**
> 。
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/4b17cd24ec8e857cc30616e15008caf4.png)
>   
> 面对上述情况，使用支持向量机就可以找出两组数据之间距离最大的地方（最大间隔）并在其中间划线。
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c781c0889a4f2eefed26428158f78784.png)
>   
> 最红的线与2组数据距离最大，被称做合适的线。这样可以根据
> **学习数据得出的识别线，判断哪些是未学习的数据**
> 。这就是
> **泛化能力**

## 深度学习（Deep Learning）

> **深度学习是无监督学习的一种**
> 。 深度学习的概念源于人工神经网络的研究。
> **含多隐层的多层感知器就是一种深度学习结构**
> 。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。

机器学习需要人工提取，深度学习可以让
**计算机自己提取特征，并以此为基础对图像进行分类**
。



### 深度学习的方法

深度学习是多层（4层以上）神经网络的总称，其中包含很多具体的方法。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/1ffec55bb183f08d9fdf368cb9731344.png)
  
不细说了，简单了解一下就行。

### 卷积神经网络（CNN）

> 卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），是深度学习（deep learning）的代表算法之一 。卷积神经网络具有表征学习（representation learning）能力，能够按其阶层结构对输入信息进行平移不变分类（shift-invariant classification），因此也被称为“平移不变人工神经网络（Shift-Invariant Artificial Neural Networks, SIANN）” 。
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ef8ccbad78e839d13b2df3219dd4dc4a.png)

卷积神经网络已成为当前语音分析和图像识别领域的研究热点。并且是深度学习模型中最成功的模型。

### 循环神经网络（RNN）

> 循环神经网络（Recurrent Neural Network,
>   
> RNN）是一类以序列（sequence）数据为输入，在序列的演进方向进行递归（recursion）且所有节点（循环单元）按链式连接的递归神经网络（recursive
>   
> neural network） 。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9e864edba6300acd3ef8c7c12611e549.png)

### 玻尔兹曼机（BM）

玻尔兹曼机是一种
**随机神经**
网络。一般用来当做
**数据生成模型**

> 参考资料
>
> 1. 《漫画人工智能》
> 2. 《神经网络与深度学习应用实战》
> 3. 百度百科
> 4. 360百科