---
arturl_encode: "68747470733a2f2f62:6c6f672e6373646e2e6e65742f44323436303539363432312f:61727469636c652f64657461696c732f313436313238303638"
layout: post
title: "初识Bert"
date: 2025-03-09 15:40:42 +08:00
description: "在学习Bert之前我们先了解“递归神经网络（RNN和 “长短期记忆（LSTM)”我们如果仅仅识别每个字的含义，那么在一句话中没有相同的字还是可以的但是如果一句话中有相同的字，那么我们进不能识别每个字的含义，因为有可能相同的字，位置不一样，词性也不一样，那么含义就会不一样。因此就引出了 递归神经网络（RNN和长短期记忆（LSTM。"
keywords: "初识Bert"
categories: ['未分类']
tags: ['深度学习', 'Lstm', 'Bert']
artid: "146128068"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146128068
    alt: "初识Bert"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146128068
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146128068
cover: https://bing.ee123.net/img/rand?artid=146128068
image: https://bing.ee123.net/img/rand?artid=146128068
img: https://bing.ee123.net/img/rand?artid=146128068
---

# 初识Bert

在学习Bert之前我们先了解“递归神经网络（RNN


Recurrent neural network)”
和 “长短期记忆（LSTM
Long short-term memory
)”

我们如果仅仅识别每个字的含义，那么在一句话中没有相同的字还是可以的但是如果一句话中有相同的字，那么我们进不能识别每个字的含义，因为有可能相同的字，位置不一样，词性也不一样，那么含义就会不一样。

![](https://i-blog.csdnimg.cn/direct/491986e1644f44fd9ecb18deef929013.png)

![](https://i-blog.csdnimg.cn/direct/f1e01f07678c45bfb18a643a3c40b6dc.png)

因此就引出了 递归神经网络（RNN


Recurrent neural network)
和长短期记忆（LSTM
Long short-term memory
)

### 递归神经网络

RNN就是在传入模型的时候，也传入一个向量，就是一个记忆单元，我们可以形象的称为“传家宝”，因为传家宝会一直串行的传下去这样就能。有了这个传家宝，就可以根据上文判断当前文字的词性。

![](https://i-blog.csdnimg.cn/direct/0316c093d20247a6a814222d386e8a6d.png)

因为是串行传递，如果一句话很长，这也导致RRN模型中记忆单元也就是“传家宝”中记录的东西过多，导致最前面的信息和最后面的信息很难对应，如：

![](https://i-blog.csdnimg.cn/direct/123a1e8ff4844b9e812f91e6721bd1b9.png)

当我们问“
谁与赛罕塔拉结下了不解之缘？”时，“传家宝”里的东西已经记录前面的太多信息了，无法准确的找到主语“我”。

为了解决这个问题，因此引出
“长短期记忆（LSTM
Long short-term memory
)”模型。

### 长短期记忆

LSTM模型就是，还是那上面的例子，问“
谁与赛罕塔拉结下了不解之缘？”时，我们就会把“传家宝”上锁，不会让中间无关紧要的信息进入“传家宝”，这样我们就能在“传家宝”当中找到我们想要的答案。

![](https://i-blog.csdnimg.cn/direct/c4bbd23015ae4d5d91c26045ce32efef.png)

### 自注意力机制

通过学习RNN和LSTM模型，虽然问题已经解决，但是速度还是太慢了，先辈的巨人们就创造了Bart模型，在学习Bart模型前，我们要先了解自注意力机制(Self-attention)

所谓的注意力机制就是
是一个特征转换器，考虑了句子整体的前后关系。
注意力机制可以让模型在生成每个目标词的时候，选择性地“关注”源句子中相关联的部分，而不是等同对待整个源句子。

![](https://i-blog.csdnimg.cn/direct/7670d6b7c76547df9396a3962c148f49.png)

通俗一点，注意力机制就是对于每个事件所分担注意力的比例是多少。
![](https://i-blog.csdnimg.cn/direct/43e6b0c0f54d44cf8c27a1068e3c7b3f.png)
![](https://i-blog.csdnimg.cn/direct/b1ac6ab5f1ea41b9809ed3912e101f1a.png)

那么
𝛼1,1,

𝛼

1,2

,

𝛼

1,3

,

𝛼

1,4

这些都是怎么求的呢？

![](https://i-blog.csdnimg.cn/direct/1860342e29ba423ab4b23e39c98abe64.png)

![](https://i-blog.csdnimg.cn/direct/f3b68b4b58894a09999c36840a0c7173.png)

得到的
𝛼1,1,

𝛼

1,2

,

𝛼

1,3

,

𝛼1,4

其概率和不为1，因此在经过一次Soft-max，得到概率和为1，这样我们就得到了

𝛼1

分配给每个字多少的注意力

![](https://i-blog.csdnimg.cn/direct/0509bbd84d2f4c53836e2bf59c7aad18.png)

b1就是
𝛼1

看过整句话结合注意力得到的值
，这样可以求得b2,b3,b4，在计算得过程中都是全部都是并行的，完全不需要等待前面得内容，因此速度很快。

### 下面是具体得计算过程，

![](https://i-blog.csdnimg.cn/direct/582ef8f8fe2d46dbbd5d228438cb634d.png)

![](https://i-blog.csdnimg.cn/direct/2cf95e8bbc5e41188f05566996a3d8c6.png)
![](https://i-blog.csdnimg.cn/direct/835dc38b81004577ac8961c5cac60ba2.png)
![](https://i-blog.csdnimg.cn/direct/7fa57074d1664c71841f2f54a8631ce2.png)
![](https://i-blog.csdnimg.cn/direct/df69696a5c444a87a21c48c5a5e5c1f2.png)

![](https://i-blog.csdnimg.cn/direct/82430dfe194d4bb1bd3bd7cc56d7b9e8.png)

我们把每个字，也就是表示成768维向量称之为token

### 

### Bert

Bert就是一个编码器，与他对应的就是GPT-2解码器，先逐步分析Bert的过程：

Inputs就是将一个字以one-hot的编码形式展示出来，传去Input Embeadding中，就是对应的“词Embeadding”进行的操作就是进行一次全连接Linear（22128，768）；在进行一次Positional Encoding，就是对应的“位置 Embeadding”，进行的操作是一次全连接Linear（512，768）。

经过两次全连接，进入的就是Muti-Head Attention（多头注意力机制），下面的三个箭头就是Wq、Wk、Wv，都是（768，768）的Linear，后面进行Norm,就是归一化，观察到还有一个箭头指向Add，这就是ResNet残差连接，因为有残差连接，网络才能走很深。传出还是768维。

传入Feed Forward，Feed Forward的操作就是进行两次全连接：nn.linear(768,3072)、nn.linear(3072,768),得到768维，在进行一次归一化和残差连接，这就算把特征提取出来了。

这就是Bert的一次完整的过程，当然，因为Bert整体的过程不改变维度，输入768维，输出768维，所以可以根据具体的情况进行N次。

![](https://i-blog.csdnimg.cn/direct/63468116e87c4b41bb72e9d4e6243d59.png)
![](https://i-blog.csdnimg.cn/direct/47a01da8ec8d429f8c9c983203cbb287.png)

![](https://i-blog.csdnimg.cn/direct/11bc361e3d60486691d599a41c353133.png)

![](https://i-blog.csdnimg.cn/direct/d69d2d0448c647d3984ed0352635b324.png)

这就是他们之间的关系

![](https://i-blog.csdnimg.cn/direct/49f839fbd32f4ef880d16affb23ab446.png)