---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f33373939303138362f:61727469636c652f64657461696c732f313436323237363236"
layout: post
title: "清华大学城市空间具身连续视觉感知问答基准测试UrbanVideo-Bench首个针对多模态大模型的运动认知评估数据集"
date: 2025-03-13 12:40:10 +08:00
description: "论文提出了UrbanVideo-Bench，首个针对城市开放空间中运动具身认知的基准测试。实验结果表明，当前最好的Video-LLMs在城市开放空间中的具身认知能力仍有很大提升空间。分析发现，因果推理与其他任务高度相关，微调大模型可以提高其在真实世界具身视频任务上的性能。"
keywords: "清华大学城市空间具身连续视觉感知问答基准测试！UrbanVideo-Bench：首个针对多模态大模型的运动认知评估数据集"
categories: ['具身智能Benchmark']
tags: ['深度学习', '具身智能', '人工智能']
artid: "146227626"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146227626
    alt: "清华大学城市空间具身连续视觉感知问答基准测试UrbanVideo-Bench首个针对多模态大模型的运动认知评估数据集"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146227626
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146227626
cover: https://bing.ee123.net/img/rand?artid=146227626
image: https://bing.ee123.net/img/rand?artid=146227626
img: https://bing.ee123.net/img/rand?artid=146227626
---

# 清华大学城市空间具身连续视觉感知问答基准测试！UrbanVideo-Bench：首个针对多模态大模型的运动认知评估数据集

![](https://i-blog.csdnimg.cn/direct/86900aaf33794d8c80002b137c663712.png)

![图片](https://i-blog.csdnimg.cn/img_convert/ae5fd013e9c9b2c6183babc38e81f4b4.gif)

* 作者：Baining Zhao, Jianjie Fang, Zichao Dai, Ziyou Wang, Jirong Zha, Weichen Zhang, Chen Gao, Yue Wang, Jinqiang Cui, Xinlei Chen, Yong Li
* 单位：清华大学，鹏城实验室
* 论文标题：UrbanVideo-Bench: Benchmarking Vision-Language Models on Embodied Intelligence with Video Data in Urban Spaces
* 论文链接：https://arxiv.org/pdf/2503.06157
* 项目主页：https://embodiedcity.github.io/UrbanVideo-Bench/
* 代码链接：https://github.com/EmbodiedCity/UrbanVideo-Bench.code

### 主要贡献

* 论文提出UrbanVideo-Bench，首个用于评估视频-语言大模型（Video-LLMs）在城市开放空间中的具身认知能力的基准，特别关注在三维城市环境中进行连续的第一人称视觉观察和导航的能力。
* 通过手动控制无人机在现实世界城市和模拟环境中收集视频数据，生成了约1500个视频片段，并设计了生成5200+个多项选择题的流程，旨在评估Video-LLMs在具身认知任务中的表现。
* 设计了包含16个任务的多样化任务集，涵盖回忆、感知、推理和导航四个方面的具身认知能力，反映了在城市开放空间中可能遇到的实际挑战。
* 对17种广泛使用的Video-LLMs进行了定量和定性评估，揭示了当前模型在具身认知任务中的局限性。

### 研究背景

![图片](https://i-blog.csdnimg.cn/img_convert/c85aaeda54a44ccc2a88fb05eb55b8ca.jpeg)

#### **研究问题**

* 人类能够处理连续的第一人称视觉观察，从而在现实世界的三维空间中进行方向判断、距离判断和导航。
* 然而，现有的多模态大模型在城市开放空间的三维运动中表现出的具身认知能力尚未得到充分探索。

#### **研究难点**

该问题的研究难点包括：

* 创建一个评估城市空间中具身认知能力的任务集；
* 获取视频数据，特别是无人机在城市环境中飞行的视频；
* 设计逻辑性和目的性的运动路线以确保连贯的视觉观察。

#### **相关工作**

![图片](https://i-blog.csdnimg.cn/img_convert/2ae4b11caa9accd3c6fd7c1add93f1d1.png)

* **具身能力的LMMs**
  ：

  + 具身智能是指认知过程深深植根于身体与世界的互动中。近年来，多模态大模型（LMMs）展示了前所未有的视觉理解能力，被认为是开发具智能体的“大脑”。
  + 现有研究主要集中在二维图像、静态点云或基于语言的空间理解上，而人类对世界的理解是基于连续的视觉感知。因此，需要多样化的视频基准来全面评估LMMs在各种具身场景中的潜力。
* **视频LMMs**
  ：

  + 传统的视频基准涵盖了各种任务，如抽象理解和时空分析，主要集中在理解视频内容上，缺乏从具身、自我中心的视角探索Video-LLMs的具身认知能力。
  + 现有研究在室内或地面场景中探索了具身能力，但在城市开放的三维空间中对具身能力的探索不足。
  + 论文提出的基准独立记录了具身运动视频及其对应的多项选择题，以评估Video-LLMs在城市三维空间中的认知能力。
* **本文特点**
  ：

  + 论文提出的UrbanVideo-Bench基准与其他基准相比，其视频来源和场景不同，专注于评估与城市三维空中运动相关的Video-LLMs的具身认知能力。
  + 通过这种比较，论文强调了其基准在复杂动态城市三维空间中评估模型认知能力的独特性。

### UrbanVideo-Bench设计与构建

#### **任务集**

![图片](https://i-blog.csdnimg.cn/img_convert/cd8e843d40edc4eefaeb25bdfa405d17.png)

* **任务分类**
  ：

  + 论文将具身认知能力分为四个主要类别：回忆（Recall）、感知（Perception）、推理（Reasoning）和导航（Navigation）。
  + 这些类别涵盖了模型在城市开放空间中进行具身认知所需的不同能力。
* **具体任务**
  ：

  + **回忆**
    ：评估模型记忆城市环境中关键信息的能力。包括轨迹描述、序列回忆、对象回忆和场景回忆等任务。
  + **感知**
    ：评估模型对静态和动态空间关系的理解能力。包括地标位置、目标检测、距离变化、持续时间、认知地图等任务。
  + **推理**
    ：评估模型分析和理解其行为和环境的能力。包括因果推理、反事实推理和关联推理等任务。
  + **导航**
    ：评估模型在城市空间中进行路线规划和直接输出动作的能力。包括路径导向的导航和高层次的规划等任务。

#### **数据集生成流程**

![图片](https://i-blog.csdnimg.cn/img_convert/d74ccc5549ea241c702d17b291ca0d44.png)

* **收集方案**
  ：

  + **数据收集**
    ：在广东省的深圳和肇庆市使用两架大疆Mini 4K无人机收集视频数据。此外，使用两个模拟器（EmbodiedCity和AerialVLN）扩展数据集。
  + **视频质量**
    ：确保视频数据具有高质量，能够反映城市环境的复杂性和动态性。
* **多项选择问题生成**
  ：

  + **思维链Prompt**
    ：利用LMM生成高质量的多项选择题。过程包括描述、关键信息提取、角色扮演和提供问题和选项模板。
  + **辅助工具**
    ：使用Gemini-1.5 Flash等模型协助生成问题，以提高问题质量。
* **问题筛选**
  ：

  + **过滤标准**
    ：去除那些仅凭常识即可回答的问题，确保评估的是模型的具身认知能力而非常识或世界知识。
* **人工调整**
  ：

  + **问题修正**
    ：对生成的多项选择题进行人工检查和修正，解决无效问题、歧义选项、错误答案等问题。
  + **质量控制**
    ：确保问题的准确性和一致性，提高数据集的整体质量。

#### **数据集统计**

* **数据规模**
  ：数据集包括约1500个视频片段和超过5200个多项选择题。视频分辨率和时长各异，覆盖了从短时精细运动到长距离导航的多种场景。
* **任务分布**
  ：低级和高级任务各占大约50%，确保全面评估模型在不同认知任务中的表现。
* **词汇丰富性**
  ：问题文本的字数范围从50到250，展示了城市元素的丰富性。

### 实验

#### **实验设置**

* **评估指标**
  ：由于每个问题都是多项选择题形式，可以直接计算每个任务类型以及整体任务的准确率。
* **基线模型**
  ：包括专有和开源的Video-LLMs。

  + 专有模型包括GPT-4o、Gemini系列和Qwen-VL-Max等；
  + 开源模型包括LLaVA-NeXT-Video、Kangaroo、Qwen2-VL系列和InternVL2系列等。
* **输入参数**
  ：根据模型的要求调整输入帧数或fps，以确保公平比较并考虑本地计算资源限制。

#### **模型比较**

![图片](https://i-blog.csdnimg.cn/img_convert/9830170cf938edd42e0f58c3da7c49e9.png)

* **结果分析**
  ：实验结果表明，无论是专有模型还是开源模型，在城市开放空间中的具身认知能力都相对较弱。表现最好的模型Qwen-VL-Max的平均准确率仅为45.5%。
* **开源模型表现**
  ：一些优化过视频数据的开源模型在某些任务上优于专有模型，显示出视频数据优化的优势。

#### **认知能力相关性**

![图片](https://i-blog.csdnimg.cn/img_convert/6494eb6b16e5800e8ee337a062bcdc4d.png)

* **任务相关性**
  ：论文分析了不同任务之间的相关性，发现因果推理任务与其他任务高度相关，表明理解因果关系可能是多种认知过程的基础。
* **记忆与导航**
  ：回忆任务之间以及导航任务与回忆和感知任务之间存在强相关性，强调了记忆在认知任务中的重要性。

#### **Sim-to-Real**

![图片](https://i-blog.csdnimg.cn/img_convert/f0e3d1250039b15a080284a3c5ed125e.png)

* **迁移潜力**
  ：论文探索了模拟数据对现实任务的迁移潜力。通过在模拟环境中进行微调，模型在现实任务上的表现有所提升，特别是在目标检测和关联推理任务上。

![图片](https://i-blog.csdnimg.cn/img_convert/f97fda244efd5cc885d03a85bb429ced.gif)

#### **错误分析**

![图片](https://i-blog.csdnimg.cn/img_convert/6c2be44d3035847f46e436041f3db9ba.png)

论文总结了三种常见的错误类型：

* **城市元素/场景理解错误**
  ：模型在复杂城市场景中难以正确识别物体，导致理解不准确或产生幻觉。
* **运动理解错误**
  ：模型在区分方向和解释相机云台角度变化时存在困难，显示出空间意识有限。
* **自我中心思考错误**
  ：模型在复杂的具身推理任务中表现不佳，如路线规划和推断。

### 总结

* 论文提出了UrbanVideo-Bench，首个针对城市开放空间中运动具身认知的基准测试。
* 实验结果表明，当前最好的Video-LLMs在城市开放空间中的具身认知能力仍有很大提升空间。
* 分析发现，因果推理与其他任务高度相关，微调大模型可以提高其在真实世界具身视频任务上的性能。

![](https://i-blog.csdnimg.cn/direct/26c659ff9d554873a225166c1ff007d5.png)