---
arturl_encode: "68747470733a2f:2f626c6f672e6373646e2e6e65742f4c414e4e59383538382f:61727469636c652f64657461696c732f313339303133363232"
layout: post
title: "OllamaOpenWebUI本地部署大模型"
date: 2025-03-15 23:56:49 +08:00
description: "Ollama是一个开源项目，用于在本地计算机上运行大型语言模型（LLMs）的工具，它的底层是使用Docker，所以支持类似Docker的构建方式，大模型就是它的镜像。它支持多种模型格式，包括但不限于GGUF，允许用户在没有高性能GPU或不希望使用云服务的情况下，利用个人计算机的资源来执行复杂的语言任务。Ollama极大简化了大模型私有部署步骤，使得大模型运行像Docker一样简单方便。"
keywords: "Ollama+OpenWebUI本地部署大模型"
categories: ['未分类']
tags: ['Openwebui', 'Ollama', 'Ollama']
artid: "139013622"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=139013622
    alt: "OllamaOpenWebUI本地部署大模型"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=139013622
featuredImagePreview: https://bing.ee123.net/img/rand?artid=139013622
cover: https://bing.ee123.net/img/rand?artid=139013622
image: https://bing.ee123.net/img/rand?artid=139013622
img: https://bing.ee123.net/img/rand?artid=139013622
---

# Ollama+OpenWebUI本地部署大模型

## 前言

Ollama是一个开源项目，用于在本地计算机上运行大型语言模型（LLMs）的工具，它的底层是使用Docker，所以支持类似Docker的构建方式，大模型就是它的镜像。它支持多种模型格式，包括但不限于GGUF，允许用户在没有高性能GPU或不希望使用云服务的情况下，利用个人计算机的资源来执行复杂的语言任务。

## Ollama使用

### Ollama安装

根据自己的平台选择下载对应的工具，
[下载地址](https://ollama.com/download)
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/80c0b09254a80bc63865937a5e94e597.png)
  
右击使用管理员权限安装，成功后会弹出下面框。
  
输入ollama list 可以查看本地有哪些模型

> ollama list
>   
> ![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e29a5098bf1d6ce1da5e6f7e6dade601.png)

想要查看支持哪些模型，可以点击官网
[中央仓库](https://ollama.com/library)
，支持的模型很多。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/a70e18b7e0a44b28a388e056ef4f3d14.png)

Ollama支持的参数：

```
Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

```

### Ollama修改配置

在拉取Ollama 大模型之前，需要修改Ollma两个配置，直接在电脑环境变量中添加以下两个系统变量,一个是方便局域网法访问，一个是避免C盘被占用过大：

* OLLAMA\_HOST：修改为0.0.0.0:11434，以便局域网访问
* OLLAMA\_MODELS：默认为C盘，需要修改为其他磁盘。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/d06b0d5988f62dfa913a81474ab1fa68.png)

退出重登Ollama，重新打开PowelShell窗口，输入本机的ip，返回running即为修改Ip成功 :
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/30b46f2fdcbef0061b70be388fdda8a5.png)

### Ollama 拉取远程大模型

ollama pull以及ollama run都可以拉取大模型，run拉取后会直接运行大模型

```
ollama run codellama:7b

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/509e30614a285e83965d483e735a9833.png)
  
`注意`
：可能会遇到以下类似的问题，大部分是网络环境问题。多试几次或者切换代理网络即可。

```
Error: pull model manifest: Get "https://registry.ollama.ai/v2/library/codellama/manifests/7b": dial tcp: lookup registry.ollama.ai: no such host

```

### Ollama 构建本地大模型

Ollama除了可以使用官方自带的模型，也可以使用我们已经量化好的gguf模型。在模型所在位置，新建个Modelfile，内容如下：

```
FROM ./llama-2-7b-chat.Q4_K_M.gguf

# set prompt template
TEMPLATE """[INST] <<SYS>>{{ .System }}<</SYS>>

{{ .Prompt }} [/INST]
"""

# set parameters
PARAMETER stop "[INST]"
PARAMETER stop "[/INST]"
PARAMETER stop "<<SYS>>"
PARAMETER stop "<</SYS>>"

# set system message
SYSTEM """
You are a helpful assistant.
"""

```

使用PowerShell，切换到模型目录下，执行以下命令，构建模型

```
ollama create my-model2 -f .\Modelfile

```

创建成功后：

```
ollama list

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/2c3ea3f59cf7e72c46ea48d18a28976f.png)

Ollama 下载的模型默认情况下都是经过量化的，如果要使用未经量化的模型，可以在
[huggingface](https://huggingface.co/)
（Hugging Face 起初是NLP机器学习服务商，开源了非常出名的自然语言处理应用构建的 transformers 库。随着大模型流行，Hugging Face转向了机器学习的社区服务，类似于代码服务的github） 下载指定的模型，并使用上述方式进行运行。

### Ollama 运行本地模型：

#### 命令行交互

```
ollama run my-model2

```

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ee2481978be6b1a28ce80a2dcccc9e5d.png)

#### Api调用

Ollama run之后就可以直接使用api接口调用，调用方式POST:
  
url:

```
http://127.0.0.1:11434/api/generate

```

请求体,修改成你执行的model：

```
{
  "model": "codellama:7b",
  "prompt": "Why is the sky blue?",
    "format": "json",
  "stream": false
}

```

返回数据：

```
{
    "model": "codellama:7b",
    "created_at": "2024-05-18T08:05:25.3502395Z",
    "response": "{\n\"The sky appears blue because of a phenomenon called Rayleigh scattering, which occurs when sunlight interacts with the Earth's atmosphere. When light travels through a medium, such as air or water, it encounters particles that can scatter it in different directions. In the case of the Earth's atmosphere, the tiny molecules of gases, such as nitrogen and oxygen, scatter shorter wavelengths of light (like blue and violet) more than longer wavelengths (like red and orange). This is known as the Rayleigh scattering effect.\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "done": false
}

```

#### Web 端调用

Ollama 如果想要在Web端使用，需要借助OpenWebUI 这个工具。OpenWebUI 是一个为大型语言模型（LLM）设计的开源Web界面，它提供了一个用户友好的交互方式来管理和运行这些模型。OpenWebUI 可以与不同的LLM运行程序集成，包括但不限于Ollama和OpenAI兼容的API

推荐使用docker 启动OpenWebUI，如果你的Ollama与OpenWebUI部署在同一服务器上，则使用以下命令

```
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name ollama-web --restart always ghcr.io/open-webui/open-webui:main

```

如果不是在同一机器，则加个参数OLLAMA\_BASE\_URL

```
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=http://10.11.24.27 -v open-webui:/app/backend/data --name ollama-web --restart always ghcr.io/open-webui/open-webui:main

```

启动后输入
  
http://localhost:3000/或者http://10.11.24.27:3000/会跳转到如下页面
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/862f844f12830361cff63dfab2e82145.png)
  
点击注册，注册信息可以随意写，注册后跳转到该页面
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/48fc03b3b4142bd17718c6ecba31494c.png)

选择模型后可以开始对话

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/385539951edecb536f97d21545bb4c43.png)
  
OpenWebUI 也支持下载模型，点击setting 页面，输入模型qwen:0.5b

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a9438e52bc0e0598d43b1e3c3f686fa3.png)
  
成功后会有提示
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/c5988dae08774b2f70c54fcb3f5d869f.png)
  
切换到qwen:0.5b问答。
  
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/b6f2d227c549e059484d0e08427090c4.png)
  
整体来说OpenWebUI用起来会比较方便。

## 总结

Ollama极大简化了大模型私有部署步骤，使得大模型运行像Docker一样简单方便。Ollama 还提供Api的方式，集成Langchain等应用也就方便了很多，结合OpenWebUI可以体验类似ChatGpt的交互，方便使用。