---
layout: post
title: 10.基于FFMPEGSDL2播放video-音视频同步参考音频时钟
date: 2024-07-26 16:50:16 +08:00
categories: ['Ffmpeg']
tags: ['同步', 'Sdl', 'Ffmpeg']
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=82626615
    alt: 10.基于FFMPEGSDL2播放video-音视频同步参考音频时钟
artid: 82626615
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=82626615
featuredImagePreview: https://bing.ee123.net/img/rand?artid=82626615
---

# 10.基于FFMPEG+SDL2播放video---音视频同步（参考音频时钟）

继续FFMPEG学习之路。。。

参考资料：

1. An ffmpeg and SDL Tutorial

#### 文章目录

* [1 综述](#1__6)
* [2 音视频同步](#2__11)
* [3 DTS 和 PTS](#3_DTS__PTS_23)
* [4 音频时钟](#4__28)
* [5 视频PTS](#5_PTS_86)
* [6 同步](#6__115)
* [7 不足](#7__174)
* [8 工程](#8__184)

## 1 综述

前面在写了使用FFMPEG+SDL2播放音频，视频的demo，接下来则需要将音频视频合入同时进行播放，在简单的将两份代码合入之后，调试了一番，发现音频视频可以正常播放，但是并没有同步，两者之间的独立的两个部分，这样就会导致画面和人的口型对不上，看着很不舒服，这时候就需要音视频同步了。

所谓的音视频同步，顾名思义就是要音频的播放速度跟上视频的播放速度，或者说视频的播放速度跟上音频的播放速度，这样就会使当前的画面和播放的声音一致。

## 2 音视频同步

我们知道，视频有帧率的概念，表示一秒显示的帧数，例如25FPS表示一秒显示25帧图像；音频有采样率的概念，表示每秒播放的样本的个数，例如一段音频音频参数为：48khz 32bit, 则每秒播放的音频字节数为 48000*（32/8）= 192000 。因此如果我们保持每秒视频播放25帧，音频播放192000字节数据，这样即便音频和视频播放是独立的，理论上也应该可以达到同步的目的。理想是美好的，但是随着时间的逐渐增大，误差就会逐渐增大，慢慢就会出现音视频不会同步的现象。

那么如何才能做到音视频同步呐？既然音频和视频都有与时间相关的一些概念，那么能否有一个独立的时间参数，音频和视频都参考这个独立的时间参数，播放快了就减慢播放，播放慢了就加快播放，这样音视频就可以参考这个时间参数实现同步播放。

独立的时间参数可以有一下三个选择：
  
**参考音频时钟**
：即以音频的播放速度为准，将视频同步到音频上，视频播放快了就减慢播放速度，视频播放慢了就加快播放速度。
  
**参考视频时钟**
：即以视频的播放速度为准，将音频同步到视频上。
  
**参考外部时钟**
：即设置一个外部的独立时钟，将音频和视频同步到此时钟上。

本文中的同步是参考音频时钟的。

## 3 DTS 和 PTS

要实现上面讲的参考音频时钟，那么使用每秒播放25帧的这个参数就不容易实现了，因此我们需要视频中更精确的时间量来和音频时钟同步。在视频中，有DTS和PTS的概念，DTS(Decoding Time Stamp)解码时间戳，用于告诉解码器解码的顺序；PTS(Presentation Time Stamp)显示时间戳，用于表示解码后的数据显示顺序。

关于DTS和PTS，网上有很多详细的资料，这里不再描述，我们只需要知道我们可以使用PTS和音频时钟进行参考，从而判断下一帧的时间，从而实现音视频同步。

## 4 音频时钟

首先，我们先了解音频时钟相关的。
  
既然以音频时钟为准，那么我们就需要实时的更新音频时钟，方便视频可以获取到最新的时钟，因此在audio_decode_frame函数中，每取出一个packet，就要更新audio clock,如下：

```
		/* if update, update the audio clock w/pts */
		if(pkt->pts != AV_NOPTS_VALUE) 
		{
			/* 获取真正的时间 */
			pState->audioClock = av_q2d(pState->pAudioStream->time_base)*pkt->pts;
			//printf("pts is %f， av is %f, clock is %f\n", pkt->pts, av_q2d(pState->pAudioStream->time_base),
			//	pState->audioClock );
		}

```

根据前面的文章，我们知道可能一个音频packet里面包含多帧音频数据，因此在此函数中，每解码一帧的数据，就要更新audio clock,如下：

```
			/* Keep audio_clock up-to-date */
			pts = pState->audioClock;
			*pts_ptr = pts;
			n = 2 * pState->pAudioStream->codec->channels;
			pState->audioClock += (double)data_size /
					(double)(n * pState->pAudioStream->codec->sample_rate);

```

其中，data_size为当前帧的大小， n * pState->pAudioStream->codec->sample_rate 为每秒播放的音频数据量， (double)data_size /
  
(double)(n * pState->pAudioStream->codec->sample_rate);
  
则是当前解码的音频帧需要播放的时间，单位s.

注意：关于上面pts, dts, av_q2d相关知识，可以参考：
<https://blog.csdn.net/bixinwei22/article/details/78770090>
  
这篇博文已经介绍的很详细。

当视频需要参考时钟的时候，却不能直接返回audio clock,因为上面介绍了当前的audio clock为当前音频帧播放完时候的时钟，但是如果当前缓冲区里面还有音频数据，就需要减去这些音频数据占据的时间，才是当前音频的clock,如下：

```
double getAudioClock(VideoState *pState) 
{
	double pts;
	int hwBufSize = 0;        //当前剩余的要播放的数据
	int bytesPerSec = 0;
	int n = 0;

	pts = pState->audioClock; /* maintained in the audio thread */
	
	hwBufSize = pState->audioBufSize - pState->audioBufIndex;
	
	n = pState->pAudioStream->codec->channels * 2;
	if(pState->pAudioStream) 
	{
		bytesPerSec = pState->pAudioStream->codec->sample_rate * n;
	}
	
	if(bytesPerSec) 
	{
		pts -= (double)hwBufSize / bytesPerSec;
	}
	
	return pts;
}

```

其中pts -= (double)hwBufSize / bytesPerSec; 即为减去缓冲区里面剩余音频数据要播放需要的时间。

## 5 视频PTS

根据博文：
<https://blog.csdn.net/bixinwei22/article/details/78770090>
里面介绍，当我们获取到视频帧的PTS后，就可以得到其显示时间(单位s)，如下：
  
time(second) = st->duration * av_q2d(st->time_base)

在我们的解码线程中，在将解码后的数据放入到PacketQueue队列前，需要获取其显示时间，如下：

```
		/* get PTS */
		if (packet->dts != AV_NOPTS_VALUE)
		{
			pts = av_frame_get_best_effort_timestamp(pFrame);
		}
		else
		{
			pts = 0;
		}

		pts *= av_q2d(pState->pVideoStream->time_base);
		
		if(got_picture)
		{
			pts = synchronizeVideo(pState, pFrame, pts);
			if ((queuePicture(pState, pFrame, pts)) < 0)
				break;
		}

```

上面的流程为，获取视频PTS—>获取显示时间—>矫正时间—>放入队列

上面的矫正时间，是有可能调用av_frame_get_best_effort_timestamp时候没有获取到正确的PTS，那么就需要在synchronizeVideo中进行矫正。

## 6 同步

现在我们已经获取到每帧视频的显示时间，并且能够实时的同步音频时钟，接下来就是视频显示的该如何参考音频时钟？

我们的videoRefreshTimer函数的作用是刷新定时器并将视频显示到屏幕上，因此我们的同步工作就在这个函数里面进行。

所谓的同步其实是根据时间戳和音频时钟来计算下一帧视频显示的时间，设置定时器，从而通过快了变慢下一帧播放，慢了加快下一帧播放来实现同步。

大致思路为：
  
① 用当前PTS减去上一帧的PTS，获取一个值delay
  
② 将当前要显示的视频的pts减去当前音频的时钟，获取一个值diff
  
③ 将delay 和diff进行比较，如果diff小于0，说明当前要显示的视频播放的慢了，下一帧需要提前播放了；如果diff大于delay，说明当前要显示的视频播放的快了，下一帧需要慢点播放

代码如下：

```
			delay = pVideoPic->pts - pState->frameLastPTS;
			if ((delay <= 0) || (delay >= 1.0))
			{
				/* if incorrect delay, use previous one */
				delay = pState->frameLastDelay;
			}

			pState->frameLastDelay = delay;
			pState->frameLastPTS = pVideoPic->pts;

			/* update delay to sync to audio */
			refClock = getAudioClock(pState);
			diff = pVideoPic->pts - refClock;

			/* Skip or repeat the frame. Take delay into account
	 			FFPlay still doesn't "know if this is the best guess." */
			syncThreshold = (delay > AV_SYNC_THRESHOLD) ? delay : AV_SYNC_THRESHOLD;
			if(fabs(diff) < AV_NOSYNC_THRESHOLD) 
			{
				if(diff <= -syncThreshold)       //视频显示的慢了，下一帧快一点
				{
					delay = 0;
				} 
				else if(diff >= syncThreshold)   //视频显示的快了，下一帧慢一点
				{
					delay = 2 * delay;
				}
			}

			pState->frameTimer += delay;
			
			/* computer the REAL delay */
			actualDelay = pState->frameTimer - (av_gettime() / 1000000.0);
			if (actualDelay <= 0.01)
			{
				actualDelay = 0.01;
			}
			
			schduleRefresh(pState, (int)(actualDelay * 1000 + 0.5));

```

其中，frameTimer 为视频播放到现在的延迟时间总和，这个值减去当前的时间即为下一帧的播放时间；AV_NOSYNC_THRESHOLD为0.01，这个是参考ffplay里面来做的，保证值不能小于0.01，并且设置最小的刷新值为0.01.

如上，便是音视频同步的整个过程了。

## 7 不足

在代码中只是实现了基本的功能，在一些细节方面没有优化：
  
①去初始化功能没有做好
  
②代码在一个cpp文件中，太冗杂，可以按照其功能进行拆分
  
③没有添加必要的打印信息
  
④代码编解码细节方面还可以优化

接下来，则需要对这些方面进行修改。

## 8 工程

最后放上完整的工程，在vs2010上测试ok.
  
[基于FFMPEG_SDL2_音视频播放_参考音频时钟](https://download.csdn.net/download/u011003120/10659545)