---
arturl_encode: "68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34363434393032342f:61727469636c652f64657461696c732f313436313838343033"
layout: post
title: "202250311-WINDOWS本地4G显存Docker运行vLLM"
date: 2025-03-11 20:33:54 +08:00
description: "需要去huggingface注册账号获取token：HUGGING_FACE_HUB_TOKEN。*显存不足，可以通过参数减少最大上下文并采用量化版本。"
keywords: "202250311-WINDOWS本地4G显存Docker运行vLLM"
categories: ['Ai']
tags: ['容器', 'Java', 'Docker']
artid: "146188403"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146188403
    alt: "202250311-WINDOWS本地4G显存Docker运行vLLM"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146188403
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146188403
cover: https://bing.ee123.net/img/rand?artid=146188403
image: https://bing.ee123.net/img/rand?artid=146188403
img: https://bing.ee123.net/img/rand?artid=146188403
---

# 202250311-WINDOWS本地4G显存Docker运行vLLM

前置：

需要去huggingface注册账号获取token：HUGGING\_FACE\_HUB\_TOKEN

运行vLLM

```bash
docker run  --name LocalvLLM_qwen1.5B_Int4 --runtime nvidia --gpus all      -v D:/vLLM/.cache/huggingface:/root/.cache/huggingface      --env "HUGGING_FACE_HUB_TOKEN=changeme"      --env "HUGGINGFACE_CO_URL_HOME= https://hf-mirror.com/"      --env "_HF_DEFAULT_ENDPOINT=https://hf-mirror.com"      --env "HF_ENDPOINT=https://hf-mirror.com"      -p 8000:8000      --ipc=host      vllm/vllm-openai:latest      --model Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4 --gpu-memory-utilization=1 --max-model-len 4096
```

测试：

```bash
curl http://localhost:8000/v1/completions     -H "Content-Type: application/json"     -d '{
        "model": "Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'

```

> {"id":"cmpl-e6c75e13fd784f08b764aee18f325f65","object":"text\_completion","created":1741695843,"model":"Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4","choices":[{"index":0,"text":"
> **city with a rich history and culture**
> ","logprobs":null,"finish\_reason":"length","stop\_reason":null,"prompt\_logprobs":null}],"usage":{"prompt\_tokens":4,"total\_tokens":11,"completion\_tokens":7,"prompt\_tokens\_details":null}}

\*显存不足，可以通过参数减少最大上下文并采用量化版本。

参考资料：

## [vllm减小显存 | vllm小模型大显存问题\_gpu-memory-utilization-CSDN博客](https://blog.csdn.net/weixin_48435461/article/details/140476658?fromshare=blogdetail&sharetype=blogdetail&sharerId=140476658&sharerefer=PC&sharesource=weixin_46449024&sharefrom=from_link "vllm减小显存 | vllm小模型大显存问题_gpu-memory-utilization-CSDN博客")