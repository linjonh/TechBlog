---
layout: post
title: "202250311-WINDOWS本地4G显存Docker运行vLLM"
date: 2025-03-11 20:33:54 +0800
description: "需要去huggingface注册账号获取token：HUGGING_FACE_HUB_TOKEN。*显存不足，可以通过参数减少最大上下文并采用量化版本。"
keywords: "202250311-WINDOWS本地4G显存Docker运行vLLM"
categories: ['Ai']
tags: ['容器', 'Java', 'Docker']
artid: "146188403"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146188403
    alt: "202250311-WINDOWS本地4G显存Docker运行vLLM"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146188403
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146188403
cover: https://bing.ee123.net/img/rand?artid=146188403
image: https://bing.ee123.net/img/rand?artid=146188403
img: https://bing.ee123.net/img/rand?artid=146188403
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     202250311-WINDOWS本地4G显存Docker运行vLLM
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="./../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="./../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     前置：
    </p>
    <p>
     需要去huggingface注册账号获取token：HUGGING_FACE_HUB_TOKEN
    </p>
    <p>
    </p>
    <p>
     运行vLLM
    </p>
    <pre><code class="language-bash">docker run  --name LocalvLLM_qwen1.5B_Int4 --runtime nvidia --gpus all      -v D:/vLLM/.cache/huggingface:/root/.cache/huggingface      --env "HUGGING_FACE_HUB_TOKEN=changeme"      --env "HUGGINGFACE_CO_URL_HOME= https://hf-mirror.com/"      --env "_HF_DEFAULT_ENDPOINT=https://hf-mirror.com"      --env "HF_ENDPOINT=https://hf-mirror.com"      -p 8000:8000      --ipc=host      vllm/vllm-openai:latest      --model Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4 --gpu-memory-utilization=1 --max-model-len 4096</code></pre>
    <p>
    </p>
    <p>
     测试：
    </p>
    <pre><code class="language-bash">curl http://localhost:8000/v1/completions     -H "Content-Type: application/json"     -d '{
        "model": "Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4",
        "prompt": "San Francisco is a",
        "max_tokens": 7,
        "temperature": 0
    }'
</code></pre>
    <blockquote>
     <p>
      {"id":"cmpl-e6c75e13fd784f08b764aee18f325f65","object":"text_completion","created":1741695843,"model":"Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4","choices":[{"index":0,"text":"
      <strong>
       city with a rich history and culture
      </strong>
      ","logprobs":null,"finish_reason":"length","stop_reason":null,"prompt_logprobs":null}],"usage":{"prompt_tokens":4,"total_tokens":11,"completion_tokens":7,"prompt_tokens_details":null}}
     </p>
    </blockquote>
    <p>
     *显存不足，可以通过参数减少最大上下文并采用量化版本。
    </p>
    <p>
    </p>
    <p>
     参考资料：
    </p>
    <h2 id="articleContentId">
     <a href="https://blog.csdn.net/weixin_48435461/article/details/140476658?fromshare=blogdetail&amp;sharetype=blogdetail&amp;sharerId=140476658&amp;sharerefer=PC&amp;sharesource=weixin_46449024&amp;sharefrom=from_link" title="vllm减小显存 | vllm小模型大显存问题_gpu-memory-utilization-CSDN博客">
      vllm减小显存 | vllm小模型大显存问题_gpu-memory-utilization-CSDN博客
     </a>
    </h2>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34363434393032342f:61727469636c652f64657461696c732f313436313838343033" class_="artid" style="display:none">
 </p>
</div>


