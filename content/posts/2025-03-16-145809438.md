---
layout: post
title: "NLP-40残差神经网络"
date: 2025-03-16 22:27:20 +0800
description: "— 25.3.16。"
keywords: "【NLP 40、残差神经网络】"
categories: ['Nlp']
tags: ['自然语言处理', '神经网络', '人工智能']
artid: "145809438"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145809438
    alt: "NLP-40残差神经网络"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=145809438
featuredImagePreview: https://bing.ee123.net/img/rand?artid=145809438
cover: https://bing.ee123.net/img/rand?artid=145809438
image: https://bing.ee123.net/img/rand?artid=145809438
img: https://bing.ee123.net/img/rand?artid=145809438
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【NLP 40、残差神经网络】
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <blockquote>
     <p>
     </p>
     <p>
      —— 25.3.16
     </p>
    </blockquote>
    <h2>
     一、残差神经网络的核心思想
    </h2>
    <ol>
     <li>
      <p>
       ​
       <strong>
        残差映射
       </strong>
       ：传统神经网络直接学习输入到输出的复杂映射 H(x)，而残差网络改为学习残差函数 F(x)=H(x)−x，最终输出为 y=F(x)+x ，这种设计使得深层网络即使无法学习有效特征，也能通过恒等映射 x 保持性能不退化。
      </p>
     </li>
     <li>
      <p>
       ​
       <strong>
        跳跃连接（Shortcut Connection）​
       </strong>
       ：
       <br/>
       在残差块中，输入通过跨层连接直接传递到输出端，与经过卷积层、激活函数后的结果相加。这种设计为梯度提供了“直达路径”，缓解了反向传播中的梯度消失问题
      </p>
     </li>
    </ol>
    <h2>
     二、残差块的设计
    </h2>
    <h3>
     1.BasicBlock
    </h3>
    <ul>
     <li>
      含 ​
      <strong>
       两个 3×3 卷积层
      </strong>
      ，每层后接批量归一化（BatchNorm）和 ReLU 激活函数
     </li>
     <li>
      输入通过跳跃连接直接与第二个卷积层的输出相加，再经过一次 ReLU 激活
     </li>
    </ul>
    <pre><code class="language-python">class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Identity()  # 恒等映射（输入输出通道相同时）
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)  # 跳跃连接
        return F.relu(out)</code></pre>
    <hr/>
    <h3>
     2.Bottleneck
    </h3>
    <ul>
     <li>
      使用 ​
      <strong>
       1×1→3×3→1×1 的卷积组合
      </strong>
      ，通过降维和升维减少计算量
     </li>
     <li>
      典型应用于 ResNet-50/101/152 等深层变体
     </li>
    </ul>
    <hr/>
    <h2>
     三、残差网络如何解决梯度消失
    </h2>
    <h3>
     1.​
     <strong>
      梯度传播公式
     </strong>
     ：
    </h3>
    <p>
     根据链式法则，残差块的梯度为：
     <img alt="" height="54" src="https://i-blog.csdnimg.cn/direct/990bb8a4271643089d9974c68270e7d0.png" width="230"/>
    </p>
    <p>
     即使
     <img alt="" height="47" src="https://i-blog.csdnimg.cn/direct/638b93873b9044f78ab26f138fed928f.png" width="48">
      趋近于零，梯度仍可通过恒等项 1 有效传播
     </img>
    </p>
    <h3>
     <strong>
      2.实际效果
     </strong>
     ：
    </h3>
    <p>
     跳跃连接使得反向传播时梯度可直接回传到浅层，避免指数级衰减实验表明，ResNet-152（152层）的训练误差低于普通 CNN 的 20 层网络
    </p>
    <h3>
     3.代码示例
    </h3>
    <pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F

class ResNet18(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # 残差块组
        self.layer1 = self._make_layer(64, 64, stride=1, num_blocks=2)
        self.layer2 = self._make_layer(64, 128, stride=2, num_blocks=2)
        self.layer3 = self._make_layer(128, 256, stride=2, num_blocks=2)
        self.layer4 = self._make_layer(256, 512, stride=2, num_blocks=2)
        
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)
    
    def _make_layer(self, in_channels, out_channels, stride, num_blocks):
        layers = [BasicBlock(in_channels, out_channels, stride)]
        for _ in range(num_blocks-1):
            layers.append(BasicBlock(out_channels, out_channels, stride=1))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))  # 初始卷积层
        x = self.maxpool(x)
        
        x = self.layer1(x)  # 4组残差块
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)  # 全局平均池化
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

# 使用示例
model = ResNet18(num_classes=1000)
input_tensor = torch.randn(1, 3, 224, 224)
output = model(input_tensor)
print(output.shape)  # torch.Size([1, 1000])</code></pre>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f6d305f37333938333730372f:61727469636c652f64657461696c732f313435383039343338" class_="artid" style="display:none">
 </p>
</div>


