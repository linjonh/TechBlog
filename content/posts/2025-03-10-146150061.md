---
arturl_encode: "68747470733a2f2f626c6f67:2e6373646e2e6e65742f77656231383238353438323531322f:61727469636c652f64657461696c732f313436313530303631"
layout: post
title: "Python-爬虫实战案例-获取拉勾网招聘职位信息"
date: 2025-03-10 12:03:52 +08:00
description: "本文通过详细步骤展示了如何使用Python爬取拉勾网的职位招聘信息。我们使用了requests、BeautifulSoup、csv等常见库完成拉勾网数据的抓取、解析与存储，并且介绍了如何处理反爬虫机制、分页问题以及数据存储。在享受爬虫技术带来便利的同时，务必铭记要遵循网站规则。合理设置爬取频率，模拟真实用户行为，不恶意冲击服务器；尊重网站的 robots.txt 协议，不越界访问禁止区域。只有如此，才能确保爬虫技术在合法合规的轨道上稳健前行，实现数据获取与网站运营的和谐共生。"
keywords: "Python 爬虫实战案例 - 获取拉勾网招聘职位信息"
categories: ['面试', '阿里巴巴', '学习路线']
tags: ['爬虫', '开发语言', 'Python']
artid: "146150061"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=146150061
    alt: "Python-爬虫实战案例-获取拉勾网招聘职位信息"
render_with_liquid: false
featuredImage: https://bing.ee123.net/img/rand?artid=146150061
featuredImagePreview: https://bing.ee123.net/img/rand?artid=146150061
cover: https://bing.ee123.net/img/rand?artid=146150061
image: https://bing.ee123.net/img/rand?artid=146150061
img: https://bing.ee123.net/img/rand?artid=146150061
---

# Python 爬虫实战案例 - 获取拉勾网招聘职位信息

### 引言

拉勾网，作为互联网招聘领域的佼佼者，汇聚了海量且多样的职位招聘信息。这些信息涵盖了从新兴科技领域到传统行业转型所需的各类岗位，无论是初出茅庐的应届生，还是经验丰富的职场老手，都能在其中探寻到机遇。

对于求职者而言，能够快速、全面地掌握招聘职位的详细情况，如薪资待遇的高低、工作地点的便利性、职位描述所要求的技能与职责等，无疑能在求职路上抢占先机。而企业方，通过分析同行业职位信息的发布趋势、薪资水平的波动，也可为制定更具吸引力的招聘策略提供有力依据。

接下来，就让我们看看如何运用 Python 爬虫从拉勾网获取关键的招聘信息。

**目录**

[一、实战目标](#%E4%B8%80%E3%80%81%E5%AE%9E%E6%88%98%E7%9B%AE%E6%A0%87)

[二、技术路线](#%E4%BA%8C%E3%80%81%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF)

[三、数据爬取](#%E4%B8%89%E3%80%81%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96)

[3.1 网页分析](#3.1%20%E7%BD%91%E9%A1%B5%E5%88%86%E6%9E%90)

[3.2 网页请求](#3.2%20%E7%BD%91%E9%A1%B5%E8%AF%B7%E6%B1%82)

[3.3 网页解析](#3.3%20%E7%BD%91%E9%A1%B5%E8%A7%A3%E6%9E%90)

[3.4 保存数据](#3.4%20%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE)

[总结](#%E6%80%BB%E7%BB%93)

---

### 一、实战目标

本次实战的核心目标是精准抓取拉勾网特定职位的关键招聘信息。具体而言，要获取的信息涵盖：职位名称，它如同求职路上的指南针，能让求职者迅速定位职业方向；薪资范围，这是求职者关注的重点，也是衡量自身价值与市场行情的关键标尺；公司名称，背后关联着企业的规模、文化与发展前景；

### 二、技术路线

[requests](https://so.csdn.net/so/search=undefined&urw=&q=requests "requests")
：用于发送HTTP请求，获取网页内容。

[BeautifulSoup](https://so.csdn.net/so/search=undefined&urw=&q=BeautifulSoup "BeautifulSoup")
：用于解析HTML页面，提取所需的信息。

[csv](https://so.csdn.net/so/search=undefined&urw=&q=csv "csv")
：用于将爬取的数据存储为CSV文件，便于后续分析。

### 三、数据爬取

#### 3.1 网页分析

拉勾网的职位列表页，清晰明了的卡片式设计呈现了众多招聘信息，关键数据一目了然。仔细观察其 URL，不难发现其中蕴含的规律，如职位关键词、城市代码、页码等参数巧妙嵌入，以 “https://www.lagou.com/wn/jobsfromSearch=true&kd=python&pn=1&city=%E8%A5%BF%E5%AE%89” 职位关键词 /city = 城市代码、kd = 关键职位、pn = 页码” 为例，这种结构为精准定位不同职位、不同地区的招聘页面提供了线索，pn 参数可以协助我们获取多分页的信息。

![](https://i-blog.csdnimg.cn/direct/9c3a5f0e4d6949009836c3bc92a83f7a.png)

分析后我们可以知道，职位信息都在class\_=‘item\_\_10RTO’ 的div元素下，可以通过id=‘openWinPostion’、class\_=‘money\_\_3Lkgq’、class\_=‘company-name\_\_2-SjF’ 来分别获取职位名称、薪资范围和公司名称。

#### 3.2 网页请求

在 Python 的工具库中，requests 库能高效地向目标网站发送 HTTP 请求，帮我们牵线搭桥获取网页内容。不过，拉勾网为了维护自身数据的有序访问，设置了一些防护机制，我们得像智慧的访客一样巧妙应对。

首先，合理设置请求头（headers）至关重要，它就像是我们拜访网站时递出的名片，告知对方我们是友好且正常的浏览器访问。模拟常见浏览器的 User-Agent 字段，如 “Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36”，让服务器误以为我们是普通用户操作。同时，Referer 字段也不可忽视，它记录着请求的来源页面，保持其合理性，能避免一些不必要的拦截。最后，如果网站有动态验证的话我们需要设置 Cookie ，可以从自己浏览器访问记录中找到Cookie参数。

```
# 拉勾网职位招聘信息爬取与数据分析
import requests

# 模拟浏览器头部信息
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36",
    "Referer": "https://www.lagou.com/jobs/list_python?px=default",
    "Cookie": "index_location_city=%E5%85%A8%E5%9B%BD; user_trace_token=20250104093938-902c4ff5-f24c-45e5-aec2-3d1473035947; __lg_stoken__=a8290517006006e881e7779a471d837c8e507eea47d1d1fa6cc1ad03ccb44cefb814362b5ac691966eb8697d786c6e53e4f6c233c3d4eaa9ba7c50ac63afc8f768fef09ed4ae; JSESSIONID=ABAACCCABBFAAGBFE0BC31D870268EB481847F272508F4B; WEBTJ-ID=20250104094035-1942ef8a924bd-0dd7463ae6d789-26001851-1049088-1942ef8a9253d2; LGUID=20250104093941-930407fa-f7e8-4c42-81fc-756f97aefcb1; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1735954836; HMACCOUNT=D7BB61FCD5F74C1A; sajssdk_2015_cross_new_user=1; _ga=GA1.2.694435554.1735954836; _gid=GA1.2.74502511.1735954836; X_MIDDLE_TOKEN=bd70439f6dca25617ea4b718273bbf6d; SEARCH_ID=6e9a4c0e27e34ae3929ec6c60a10d1f7; X_HTTP_TOKEN=1226f88b1a607ea4951559537134b7e1ae0350e7f1; sensorsdata2015session=%7B%7D; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1735955277; LGRID=20250104094702-51a5d712-8f33-46b0-a616-3b54c420e4a4; _ga_DDLTLJDLHH=GS1.2.1735954837.1.1.1735955277.60.0.0; gate_login_token=v1####df3b53f43f17d1db42f281952270b469e8255336da4736f3; LG_LOGIN_USER_ID=v1####5496aa08cbf5a8587c4797982411a6af15950cd637352b0a; LG_HAS_LOGIN=1; _putrc=B6E2CB0ECCED9CDE; login=true; unick=%E7%94%B3%E7%99%BB%E5%B3%B0; showExpriedIndex=1; showExpriedCompanyHome=1; showExpriedMyPublish=1; hasDeliver=0; privacyPolicyPopup=true; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%221942ef8ab81cd-06cd07b2b912ae-26001851-1049088-1942ef8ab822d7%22%2C%22%24device_id%22%3A%221942ef8ab81cd-06cd07b2b912ae-26001851-1049088-1942ef8ab822d7%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%2C%22%24os%22%3A%22Windows%22%2C%22%24browser%22%3A%22Chrome%22%2C%22%24browser_version%22%3A%22121.0.0.0%22%7D%7D"
}

# 目标URL，以Python职位为例，搜索西安地区，第一页数据,pn为页码，kd为职位关键词
url = "https://www.lagou.com/wn/jobs?fromSearch=true&kd=python&pn=1&city=%E8%A5%BF%E5%AE%89"

# 发送get请求，获取响应
res = requests.get(url, headers=headers)
html = res.text
print(html)

```

在这段代码中，我们精心构建了请求头和请求参数，并发起 GET请求，若请求顺利，便能获得网页数据，为后续的数据解析铺就道路。

#### 3.3 网页解析

当我们成功取回网页的 HTML 内容，需要合适的工具来解读其中的奥秘。在这里，我们使用BeautifulSoup来获取想要的信息。我们已获取到拉勾网职位列表页的 HTML 内容，存储在变量 html 中，提取职位名称、薪资、公司名称等信息的代码如下：

```
from bs4 import BeautifulSoup

job_list = []
# 创建BeautifulSoup对象，选用html.parser解析器
soup = BeautifulSoup(html, 'html.parser')

# 查找所有职位列表项
job_list_items = soup.find_all('div', class_='item__10RTO')

for item in job_list_items:

    # 提取职位名称
    job_title = item.find(id='openWinPostion').text.strip()
    # 提取薪资范围
    salary = item.find('span', class_='money__3Lkgq').text.strip()
    # 提取公司名称
    company_name = item.find('div', class_='company-name__2-SjF').text.strip()
    # 将职位信息写入列表
    job_list.append({"job_title": job_title, "salary": salary, "company_name": company_name})

print(job_list)

```

在这段代码里，我们先创建了 BeautifulSoup 对象，然后定义了列表用于临时存储职位信息，接着利用 find\_all 方法依据类名找出所有职位列表项，再深入每个列表项，通过标签与属性的组合，精准抓取职位名称、薪资、公司名称等关键信息，将其清晰呈现。

![](https://i-blog.csdnimg.cn/direct/069e2143bbe34a4291a6af1b82502201.png)

#### 3.4 保存数据

辛苦抓取并解析得到的数据，需要妥善保存才能让其价值延续。常见的 CSV、JSON 等格式，各有千秋。

CSV 格式，以其简洁的表格形式，通用性强，能被 Excel 等众多软件直接打开编辑，方便数据的初步查看与简单分析。Python 内置的 csv 模块便能担此大任。以下是将获取到的拉勾网职位数据保存为 CSV 文件的示例：

```
import csv

# CSV文件路径
csv_file_path = "lagou_jobs.csv"

# 写入CSV文件
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['job_title', 'salary', 'company_name']  # 定义列名
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

    writer.writeheader()  # 写入表头
    writer.writerows(job_list)  # 写入数据行

print(f"数据已成功保存至 {csv_file_path}")

```

在这段示例中，我们首先定义了 CSV 文件路径，运用 csv.DictWriter 以字典形式将数据逐行写入 CSV 文件，同时写入表头，确保数据存储的规范性与完整性，方便后续随时调取分析。

![](https://i-blog.csdnimg.cn/direct/5894778167754765a0fdaf7aa1b83184.png)

最后对代码进行整理优化，并增加多页面处理。

```
# 爬取拉勾网职位招聘信息
import requests
from bs4 import BeautifulSoup
import csv


def get_html(url):

    # 模拟浏览器头部信息
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36",
        "Referer": "https://www.lagou.com/jobs/list_python?px=default",
        "Cookie": "index_location_city=%E5%85%A8%E5%9B%BD; user_trace_token=20250104093938-902c4ff5-f24c-45e5-aec2-3d1473035947; __lg_stoken__=a8290517006006e881e7779a471d837c8e507eea47d1d1fa6cc1ad03ccb44cefb814362b5ac691966eb8697d786c6e53e4f6c233c3d4eaa9ba7c50ac63afc8f768fef09ed4ae; JSESSIONID=ABAACCCABBFAAGBFE0BC31D870268EB481847F272508F4B; WEBTJ-ID=20250104094035-1942ef8a924bd-0dd7463ae6d789-26001851-1049088-1942ef8a9253d2; LGUID=20250104093941-930407fa-f7e8-4c42-81fc-756f97aefcb1; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1735954836; HMACCOUNT=D7BB61FCD5F74C1A; sajssdk_2015_cross_new_user=1; _ga=GA1.2.694435554.1735954836; _gid=GA1.2.74502511.1735954836; X_MIDDLE_TOKEN=bd70439f6dca25617ea4b718273bbf6d; SEARCH_ID=6e9a4c0e27e34ae3929ec6c60a10d1f7; X_HTTP_TOKEN=1226f88b1a607ea4951559537134b7e1ae0350e7f1; sensorsdata2015session=%7B%7D; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1735955277; LGRID=20250104094702-51a5d712-8f33-46b0-a616-3b54c420e4a4; _ga_DDLTLJDLHH=GS1.2.1735954837.1.1.1735955277.60.0.0; gate_login_token=v1####df3b53f43f17d1db42f281952270b469e8255336da4736f3; LG_LOGIN_USER_ID=v1####5496aa08cbf5a8587c4797982411a6af15950cd637352b0a; LG_HAS_LOGIN=1; _putrc=B6E2CB0ECCED9CDE; login=true; unick=%E7%94%B3%E7%99%BB%E5%B3%B0; showExpriedIndex=1; showExpriedCompanyHome=1; showExpriedMyPublish=1; hasDeliver=0; privacyPolicyPopup=true; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%221942ef8ab81cd-06cd07b2b912ae-26001851-1049088-1942ef8ab822d7%22%2C%22%24device_id%22%3A%221942ef8ab81cd-06cd07b2b912ae-26001851-1049088-1942ef8ab822d7%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%2C%22%24os%22%3A%22Windows%22%2C%22%24browser%22%3A%22Chrome%22%2C%22%24browser_version%22%3A%22121.0.0.0%22%7D%7D"
    }

    # 发送get请求，获取响应
    res = requests.get(url, headers=headers)
    html = res.text
    if res.status_code == 200:
        print(f"请求成功，状态码：{res.status_code}")

    else:
        print(f"请求失败，状态码：{res.status_code}")

    return html


def get_alljobs(html):
    job_list = []
    # 创建BeautifulSoup对象，选用html.parser解析器
    soup = BeautifulSoup(html, 'html.parser')

    # 查找所有职位列表项
    job_list_items = soup.find_all('div', class_='item__10RTO')

    for item in job_list_items:

        # 提取职位名称
        job_title = item.find(id='openWinPostion').text.strip()
        # 提取薪资范围
        salary = item.find('span', class_='money__3Lkgq').text.strip()
        # 提取公司名称
        company_name = item.find('div', class_='company-name__2-SjF').text.strip()
        # 将职位信息写入列表
        job_list.append({"job_title": job_title, "salary": salary, "company_name": company_name})

    return job_list


def save_to_csv(job_list):
    # CSV文件路径
    csv_file_path = "lagou_jobs.csv"

    # 写入CSV文件
    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['job_title', 'salary', 'company_name']  # 定义列名
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()  # 写入表头
        writer.writerows(job_list)  # 写入数据行

    return f"数据已成功保存至 {csv_file_path}"


if __name__ == "__main__":
    # 目标URL，以Python职位为例，搜索西安地区，第一页数据,pn为页码，kd为职位关键词
    base_url = "https://www.lagou.com/wn/jobs?fromSearch=true&kd=python&city=%E8%A5%BF%E5%AE%89"
    # 配置页码数量
    num_pages = 3
    # 定义一个空列表，存储所有的职位
    jobs = []
    for i in range(1, num_pages+1):
        url = f"{base_url}&pn={i}"
        html = get_html(url)
        job_list = get_alljobs(html)
        jobs.extend(job_list)
    save_to_csv(jobs)

    print("爬取完成，数据已保存至 lagou_jobs.csv")

```

### 总结

本文通过详细步骤展示了如何使用Python爬取拉勾网的职位招聘信息。我们使用了requests、BeautifulSoup、csv等常见库完成拉勾网数据的抓取、解析与存储，并且介绍了如何处理反爬虫机制、分页问题以及数据存储。

在享受爬虫技术带来便利的同时，务必铭记要遵循网站规则。合理设置爬取频率，模拟真实用户行为，不恶意冲击服务器；尊重网站的 robots.txt 协议，不越界访问禁止区域。只有如此，才能确保爬虫技术在合法合规的轨道上稳健前行，实现数据获取与网站运营的和谐共生。