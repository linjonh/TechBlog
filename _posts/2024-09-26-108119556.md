---
layout: post
title: "探究pytorch-model.eval测试效果远差于model.train"
date: 2024-09-26 15:56:39 +0800
description: "探究pytorch model.eval()测试效果远差于model.train()_model.e"
keywords: "model.eval()和model.train()输出差距很大"
categories: ["未分类"]
tags: ["神经网络", "深度学习", "Pytorch", "Python"]
artid: "108119556"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=108119556
  alt: "探究pytorch-model.eval测试效果远差于model.train"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     探究pytorch model.eval()测试效果远差于model.train()
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     <strong>
      前言：
     </strong>
     <br/>
     第一篇博客，记录下自己的学习心得。如有谬误，欢迎指正。
    </p>
    <h3>
     <a id="modeleval_3">
     </a>
     为什么用model.eval()
    </h3>
    <p>
     当网络中存在BN层或者Dropout，在测试的时候需要固定住固定BN层和dropout层。关于BN层的详细介绍可以参考这篇博文：
     <a href="https://blog.csdn.net/LoseInVain/article/details/86476010?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param">
      Pytorch的BatchNorm层使用中容易出现的问题
     </a>
    </p>
    <p>
     训练数据的时候acc可以达到99%，但是测试时acc只有33%，显然这是不行的。查询了一些资料给出的解决方案如下：、
    </p>
    <ol>
     <li>
      <p>
       删去相同的BN层
       <br/>
       其实我没太理解这个，怎么才算相同的BN层
      </p>
     </li>
     <li>
      <p>
       track_running_stats设为False
       <br/>
       含义为测试时用当前batch的方差和均值（为True则使用训练时得到的方差和均值）。我在这样设定后，测试效果确实变好了，但是之前训练得到方差和均值就没啥意义了。
      </p>
     </li>
     <li>
      <p>
       数据进行归一化
       <br/>
       有的人可能是没有进行数据归一化从而导致测试效果较差。我的训练集归一化什么的肯定是没问题的，将训练集作为测试集，原则上应该拟合的很好，但在加model.eval()后，效果依然不好，显然不是归一化的问题了。
      </p>
     </li>
     <li>
      <p>
       增大训练样本batch_size
       <br/>
       没想到会是这方面原因，因为我以前觉得加不加大batch_size，最后得到的方差和均值都一样，有点搞笑。
       <br/>
       网络中加入了BN层，训练时BN层中的方差和均值是根据每个batch的样本进行更新的，测试的时候这俩个参数是固定的。
       <br/>
       我把训练的batch_size设为了1，那么训练完成后获得的方差和均值是接近于最后一个样本的方差和均值的。加了model.eval(),在测试的时候相当于是用这最后一个样本的方差和均值来获得测试集的输出，显然这个方差和均值是不具备全局性的，结果肯定是不好的。在增大batch_size后，获得的方差和均值是更接近于全局特性的，所以当我将batch_size设为64后，测试集的acc提到了85%，尽管不是很高，但也足以证明增大batch_size是十分有效的。
      </p>
     </li>
    </ol>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34323336323839312f:61727469636c652f64657461696c732f313038313139353536" class_="artid" style="display:none">
 </p>
</div>
