---
layout: post
title: "Python-调用-Ollama-库本地大语言模型使用详解"
date: 2025-01-15 14:45:05 +0800
description: "是一个用于调用本地大语言模型（Large Langu"
keywords: "python ollama"
categories: ['Python']
tags: ['Python']
artid: "145160142"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145160142
    alt: "Python-调用-Ollama-库本地大语言模型使用详解"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Python 调用 Ollama 库：本地大语言模型使用详解
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     <code>
      ollama
     </code>
     是一个用于调用本地大语言模型（Large Language Models，LLMs）的 Python 库，旨在提供简单、高效的 API 接口，以便开发者能够方便地与本地的大语言模型进行交互。以下是关于如何在 Python 中使用
     <code>
      ollama
     </code>
     库的详细介绍。
    </p>
    <hr/>
    <h3>
     <strong>
      1. 安装 Ollama
     </strong>
    </h3>
    <p>
     在使用库之前，请确保安装了
     <code>
      ollama
     </code>
     。你可以通过以下命令安装：
    </p>
    <pre><code>pip install ollama</code></pre>
    <p>
    </p>
    <p>
    </p>
    <p>
     如果你尚未安装 Python 的包管理工具
     <code>
      pip
     </code>
     ，可以参考官方文档安装它。
    </p>
    <hr/>
    <h3>
     <strong>
      2. Ollama 的主要功能
     </strong>
    </h3>
    <p>
     <code>
      ollama
     </code>
     提供了与本地大语言模型（如 llama 或其他模型）交互的简单方法，主要是通过 API 调用模型来生成文本、回答问题等。
    </p>
    <hr/>
    <h3>
     <strong>
      3. 使用 Ollama 的基本示例
     </strong>
    </h3>
    <p>
     以下是
     <code>
      ollama
     </code>
     的基本用法。
    </p>
    <h4>
     <strong>
      3.1 导入库
     </strong>
    </h4>
    <p>
     在 Python 脚本中，首先需要引入
     <code>
      ollama
     </code>
     ：
    </p>
    <pre></pre>
    <pre><code>import ollama</code></pre>
    <hr/>
    <h4>
     <strong>
      3.2 使用 Ollama 调用模型
     </strong>
    </h4>
    <p>
     Ollama 的核心功能是调用本地模型进行推理和生成。你可以通过以下方式调用模型：
    </p>
    <h5>
     <strong>
      生成文本示例
     </strong>
    </h5>
    <p>
     以下是一个简单的生成文本的例子：
    </p>
    <p>
    </p>
    <pre><code>import ollama

# 调用 Ollama 使用大语言模型
response = ollama.generate(
    model="llama",  # 使用的模型名称
    prompt="你好，请简单介绍一下Python语言的特点。"
)

# 打印生成的内容
print(response)
</code></pre>
    <h5>
     <strong>
      解析模型输出
     </strong>
    </h5>
    <p>
     返回的
     <code>
      response
     </code>
     通常是一个字符串，表示模型生成的结果。你可以对其进一步处理，比如格式化输出或存储到文件中。
    </p>
    <hr/>
    <h4>
     <strong>
      3.3 设置自定义参数
     </strong>
    </h4>
    <p>
     调用模型时，可以传递一些自定义参数来调整模型的行为，比如最大生成长度、生成的温度等。
    </p>
    <h5>
     <strong>
      支持的参数
     </strong>
    </h5>
    <p>
     以下是一些常见的参数：
    </p>
    <ul>
     <li>
      <code>
       model
      </code>
      ：指定模型的名称（如 "llama" 等）。
     </li>
     <li>
      <code>
       prompt
      </code>
      ：输入提示。
     </li>
     <li>
      <code>
       temperature
      </code>
      ：影响生成内容的随机性，值范围为 0 到 1。
     </li>
     <li>
      <code>
       max_tokens
      </code>
      ：限制生成的最大 token 数量。
     </li>
    </ul>
    <h5>
     <strong>
      示例：自定义参数
     </strong>
    </h5>
    <pre><code>response = ollama.generate(
    model="llama",
    prompt="为我写一首关于春天的诗。",
    temperature=0.7,  # 生成时的随机性
    max_tokens=100    # 限制生成的最大长度
)

print(response)
</code></pre>
    <hr/>
    <h4>
     <strong>
      3.4 使用自定义模型
     </strong>
    </h4>
    <p>
     如果你已经在本地训练了自定义模型，或者下载了其他模型，可以通过指定模型路径来使用它。
    </p>
    <pre></pre>
    <pre><code>response = ollama.generate(
    model="/path/to/your/model",  # 指定本地模型路径
    prompt="如何学习机器学习？"
)

print(response)
</code></pre>
    <hr/>
    <h3>
     <strong>
      4. 集成流式生成
     </strong>
    </h3>
    <p>
     在某些场景下，你可能希望逐步接收模型生成的结果，而不是等待全部生成完成。这是通过流式生成（Streaming）实现的。
    </p>
    <pre></pre>
    <pre><code>for chunk in ollama.stream(
    model="llama",
    prompt="逐步生成一段关于人工智能的文章。"
):
    print(chunk, end="")
</code></pre>
    <p>
     在流式生成中，模型会逐步返回生成结果的部分内容，你可以实时处理这些结果。
    </p>
    <hr/>
    <h3>
     <strong>
      5. 错误处理
     </strong>
    </h3>
    <p>
     调用模型时，可能会遇到错误（例如模型文件路径不正确、请求超时等）。可以通过捕获异常来处理这些错误。
    </p>
    <pre></pre>
    <pre><code>try:
    response = ollama.generate(
        model="llama",
        prompt="请解释什么是大语言模型。"
    )
    print(response)
except Exception as e:
    print(f"发生错误：{e}")
</code></pre>
    <hr/>
    <h3>
     <strong>
      6. 高级用法：与其他工具集成
     </strong>
    </h3>
    <p>
     <code>
      ollama
     </code>
     可以与其他工具（如
     <code>
      Flask
     </code>
     、
     <code>
      FastAPI
     </code>
     ）结合，用于构建自己的 AI 应用。
    </p>
    <h4>
     <strong>
      示例：构建一个简单的 Flask 服务
     </strong>
    </h4>
    <p>
     以下代码展示了如何使用 Flask 构建一个简单的 Web 应用，调用 Ollama 进行生成：
    </p>
    <pre></pre>
    <pre><code>from flask import Flask, request, jsonify
import ollama

app = Flask(__name__)

@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    prompt = data.get("prompt", "")
    try:
        # 调用 Ollama
        response = ollama.generate(
            model="llama",
            prompt=prompt,
            max_tokens=100
        )
        return jsonify({"response": response})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
    <p>
     使用 Postman 或其他工具向
     <code>
      /generate
     </code>
     端点发送 POST 请求：
    </p>
    <pre></pre>
    <pre><code>{
    "prompt": "Python 的主要优点是什么？"
}
</code></pre>
    <p>
     返回结果会是模型生成的回答。
    </p>
    <hr/>
    <h3>
     <strong>
      7. 注意事项
     </strong>
    </h3>
    <ol>
     <li>
      <strong>
       模型兼容性
      </strong>
      ：确保本地安装的模型与
      <code>
       ollama
      </code>
      支持的格式兼容。
     </li>
     <li>
      <strong>
       硬件要求
      </strong>
      ：大型语言模型通常需要较高的硬件性能（特别是 GPU 支持）。在调用本地模型时，请确保你的环境足够满足计算需求。
     </li>
     <li>
      <strong>
       版本更新
      </strong>
      ：定期检查
      <code>
       ollama
      </code>
      的版本更新，获取最新功能和优化。
     </li>
    </ol>
    <hr/>
    <h3>
     <strong>
      8. 参考文档
     </strong>
    </h3>
    <p>
     有关更多详细用法和配置选项，可以参考
     <code>
      ollama
     </code>
     的官方文档或相关资源。
    </p>
    <ul>
     <li>
      官网文档链接（如果有）：请搜索
      <code>
       ollama
      </code>
      的官方资源。
     </li>
     <li>
      社区支持：可以通过 GitHub 或开发者社区寻求帮助。
     </li>
    </ul>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f:2f626c6f672e6373646e2e6e65742f6b6461796a6a3936362f:61727469636c652f64657461696c732f313435313630313432" class_="artid" style="display:none">
 </p>
</div>


