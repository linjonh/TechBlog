---
layout: post
title: 2025-02-06-三步实现DeepSeek本地部署,告别服务器崩溃的烦恼
date: 2025-02-06 13:25:48 +0800
categories: ['未分类']
tags: ['服务器', '运维']
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=145469702
  alt: 三步实现DeepSeek本地部署,告别服务器崩溃的烦恼
artid: 145469702
render_with_liquid: false
---
</p>
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     三步实现【DeepSeek】本地部署，告别服务器崩溃的烦恼！
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-github-gist" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <div class="toc">
     <h4>
      三步实现【DeepSeek】本地部署，告别服务器崩溃的烦恼！
     </h4>
     <ul>
      <li>
       <ul>
        <li>
         <a href="#_2" rel="nofollow">
          前言
         </a>
        </li>
        <li>
         <a href="#_Ollama_11" rel="nofollow">
          一：安装 Ollama
         </a>
        </li>
        <li>
         <a href="#_Deepseek__23" rel="nofollow">
          二：下载部署 Deepseek 模型
         </a>
        </li>
        <li>
         <a href="#_Chatbox_51" rel="nofollow">
          三：可视化图文交互界面 Chatbox
         </a>
        </li>
        <li>
         <a href="#_76" rel="nofollow">
          总结
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </div>
    <p>
    </p>
    <h3>
     <a id="_2">
     </a>
     前言
    </h3>
    <p>
     近年来，国产大模型DeepSeek备受关注，但随着用户访问量激增，频繁出现响应缓慢甚至系统宕机的现象，给使用者带来了一些不便。
     <br/>
     幸运的是，DeepSeek作为开源模型，允许开发者通过本地部署来解决这一问题。将模型部署到本地后，用户无需依赖网络即可随时进行推理操作，不仅提升了使用体验，还大幅降低了对外部服务的依赖。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/0d6fda84308d404ca39e71f4630963a1.png"/>
    </p>
    <p>
     三步实现【DeepSeek】本地部署，告别服务器崩溃的烦恼！
     <br/>
     <strong>
      1、安装ollama；
      <br/>
      2、下载deepSeek模型；
      <br/>
      3、下载可视化ai工具；
     </strong>
    </p>
    <h3>
     <a id="_Ollama_11">
     </a>
     一：安装 Ollama
    </h3>
    <p>
     要在本地运行 DeepSeek，我们需要使用 Ollama——一个开源的本地大模型运行工具。
     <br/>
     首先，访问
     <a href="https://ollama.com/" rel="nofollow">
      Ollama官网
     </a>
     下载该工具。官网提供了针对不同操作系统的安装包，用户只需选择适合自己电脑的版本即可。在本例中，我们选择下载适用于 Windows 操作系统的版本。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/4e0e286173b54fbbabc2b6e1e188ca59.png"/>
    </p>
    <p>
     Ollama 安装包下载后双击install，安装速度还是比较快的**（注：直接安装到C盘）**
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9ef81aa2848b4a4a95531969867c5339.png"/>
    </p>
    <p>
     安装完成 Ollama 后，打开电脑的 CMD（命令提示符）。只需在电脑下方的搜索框中输入“cmd”并按回车，即可打开命令提示符窗口。输入**【ollama help】**
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fa3dc975e6af4d359a50f864379a6ada.png"/>
    </p>
    <h3>
     <a id="_Deepseek__23">
     </a>
     二：下载部署 Deepseek 模型
    </h3>
    <p>
     返回
     <a href="https://ollama.com/" rel="nofollow">
      Ollama官网
     </a>
     ，在网页上方的搜索框中输入 “DeepSeek-R1”，这是我们要在本地部署的模型。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5fdf008afe7745b992bd33aa76111176.png"/>
    </p>
    <p>
     点击 “DeepSeek-R1” 后，您将进入模型的详情页面，页面中会显示多个可选择的参数规模。每个参数规模代表了模型的不同大小和计算能力，从 1.5B 到 671B 不等。这里的 “B” 代表 “Billion”，即“十亿”，因此：
     <br/>
     ● 1.5B 表示该模型具有 15 亿个参数，适合轻量级任务或资源有限的设备使用。
     <br/>
     ● 671B 则代表该模型拥有 6710 亿个参数，模型规模和计算能力都极为强大，适合大规模数据处理和高精度任务，但对硬件要求也相应更高。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5c74e18821254df3af95031a45fd8088.png"/>
    </p>
    <p>
     选择不同的参数规模意味着你可以根据自己的硬件配置和应用需求，决定使用哪个版本的模型。较小的模型需要的计算资源较少，适合快速推理或硬件资源较弱的设备；而较大的模型则在处理复杂任务时具有更高的推理能力，但需要更多的内存和显存支持。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/70be3f0d42f3417aa5a568d35dbe54ca.png"/>
    </p>
    <p>
     按下Win + X组合键或搜索“设备管理器”，展开“显示适配器”类别，可以看到已安装的显卡型号。如果需要更详细的信息，可以右键点击显卡名称，选择“属性”查看详细信息；
     <strong>
      我的是【RTX 3050】稳妥点下载1.5B
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/aaf23e6800454a6e9af421bdeaac366a.png"/>
    </p>
    <p>
     选择好模型规模后，复制右边的一个命令。【ollama run deepseek-r1:1.5b】
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/6d3cb47900d4492f9cfa188047505425.png"/>
    </p>
    <p>
     复制命令后，回到命令提示符窗口，将刚刚复制的命令粘贴进去，并按回车键即可开始下载模型。
     <strong>
      【下载大概1小时左右，有点慢】
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/abb0ecb8c0ce486facd5da473a0ce911.png"/>
    </p>
    <p>
     模型下载完成后，您可以直接在命令提示符窗口中使用它，开始进行推理操作。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9346eb2472494c56b09a157778cbecdf.png"/>
    </p>
    <p>
     以后如果我们需要再次使用 DeepSeek 模型，只需打开命令提示符窗口，直接输入之前复制的指令，按回车即可重新启动模型并进行推理操作。【ollama run deepseek-r1:1.5b】
     <strong>
      （1.5b有点不够智能，模型数量不够）
     </strong>
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/5e4b0b9edd814c9d9d1c9a84bf2848f3.png"/>
    </p>
    <h3>
     <a id="_Chatbox_51">
     </a>
     三：可视化图文交互界面 Chatbox
    </h3>
    <p>
     虽然我们可以在本地使用 DeepSeek 模型，但其默认的命令行界面较为简陋，很多用户可能不太习惯。为了提供更直观的交互体验，我们可以使用 Chatbox 这个可视化的图文界面来与 DeepSeek 进行交互。
     <br/>
     只需访问 Chatbox 官网，在这里，你可以选择使用其本地客户端，或者直接通过网页版进行操作，享受更友好的使用体验。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/90258a3bf5194098a6436bc611956ff3.png"/>
    </p>
    <p>
     进入 Chatbox 网页版本后，点击“使用自己的 API Key 或本地模型”选项。这将允许你连接到自己本地的 DeepSeek 模型或通过 API Key 访问外部服务。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/dd908df5f4144b74907ca6694e0e2276.png"/>
    </p>
    <p>
     这里需要注意的是，为了能够 Ollama 能远程链接，这里我们最好看一下
     <a href="https://chatboxai.app/zh/help-center/connect-chatbox-remote-ollama-service-guide" rel="nofollow">
      【如何将 Chatbox 连接到远程 Ollama 服务：逐步指南】
     </a>
     的教程，根据这个教程操作一下。
     <strong>
      一般无需配置
     </strong>
     ；
     <br/>
     如果你使用的是 window的PC应用【
     <strong>
      pc端下载过程不展示了下载就行
     </strong>
     】
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fd975067f55c4ad2ad5c721c2487987f.png"/>
    </p>
    <p>
     只需要配置相应的环境变量即可。配置好环境变量后，需要重启 Ollama 程序以使设置生效。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/a85aa63625ce4b2aaca73cdbf2ddc201.png"/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ec6e559697734274bfcc5d9d3a0392d7.png"/>
    </p>
    <p>
     重启 Ollama 程序后，返回到 Chatbox 设置界面，关闭当前的设置窗口，然后重新打开它。在重新打开后，你就能在界面中选择已经部署好的 DeepSeek 模型了。选择模型后，点击保存即可开始使用。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/28fed054826e41c98bc52e57fa94f80d.png"/>
    </p>
    <p>
     接下来，只需在 Chatbox 中新建对话即可开始使用 DeepSeek 模型。以下图为例，您可以看到上方是模型的思考过程，下方则是模型给出的答案。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/1b460ae4209f4d3799df2c0ce141d8ce.png"/>
    </p>
    <h3>
     <a id="_76">
     </a>
     总结
    </h3>
    <p>
     通过本文提供的三步教程——安装Ollama工具、下载DeepSeek模型、配置可视化的Chatbox界面，用户可以轻松实现本地部署。尤其是Chatbox的图文交互界面，不仅让操作变得更加直观和友好，也使得模型的使用变得更加高效。
     <br/>
     无论是普通用户，还是开发者，采用本地部署的方式都能避免服务器崩溃带来的麻烦，并且可以根据自己的硬件条件选择合适的模型版本，灵活应对不同的任务需求。
     <br/>
     总的来说，本地部署不仅是解决DeepSeek响应慢和系统宕机问题的有效方法，还为用户提供了更加稳定、便捷的使用体验。如果你也在使用DeepSeek，或者正在考虑部署类似的模型，按照这篇指南进行操作，将会大大提升你的工作效率，告别频繁的宕机和不稳定的服务，带来更顺畅的体验。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
</div>


<p class="artid" style="display:none">68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34333739323838322f:61727469636c652f64657461696c732f313435343639373032
