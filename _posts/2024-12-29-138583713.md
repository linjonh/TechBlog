---
layout: post
title: "VALSE-2024-Workshop报告分享探索短视频生成与编辑的前沿技术"
date: 2024-12-29 14:00:00 +0800
description: "本文总结了VALSE 2024的Workshop报告《探索短视频生成与编辑的前沿技术》的精彩内容，方"
keywords: "VALSE 2024 Workshop报告分享┆探索短视频生成与编辑的前沿技术"
categories: ["未分类"]
tags: ["视频生成", "文本生成", "扩散模型"]
artid: "138583713"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=138583713
  alt: "VALSE-2024-Workshop报告分享探索短视频生成与编辑的前沿技术"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     VALSE 2024 Workshop报告分享┆探索短视频生成与编辑的前沿技术
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p style="text-align:justify;">
     <span style="color:#79c6cd;">
      2024年视觉与学习青年学者研讨会（VALSE 2024）于5月5日到7日在重庆悦来国际会议中心举行。本公众号将全方位地对会议的热点进行报道，方便广大读者跟踪和了解人工智能的前沿理论和技术。欢迎广大读者对文章进行关注、阅读和转发。文章是对报告人演讲内容的理解或转述，可能与报告人的原意有所不同，敬请读者理解；如报告人认为文章与自己报告的内容差别较大，可以联系公众号删除。
     </span>
    </p>
    <p style="text-align:justify;">
     本文对微软亚洲研究院的罗翀博士所做的Workshop报告《探索短视频生成与编辑的前沿技术》进行总结和分享。
    </p>
    <h2 style="text-align:justify;">
     <span style="color:#1c7331;">
      <strong>
       1.报告人简介
      </strong>
     </span>
    </h2>
    <p style="text-align:justify;">
     罗翀，上海交通大学信号与信息处理专业博士，现任微软亚洲研究院智能多媒体组高级研究员，中国科学技术大学兼职教授、博士生导师，研究领域包括计算机视觉、智能语音、跨模态视频分析、视频通信等。
    </p>
    <h2 style="text-align:justify;">
     <span style="color:#1c7331;">
      <strong>
       2.报告概览
      </strong>
     </span>
    </h2>
    <p style="text-align:justify;">
     首先，罗博士在报告中首先对近年图像和视频生成做了简要回顾，包括过去几年中图像/视频生成领域的发展和技术进本。随后，他介绍了最近的创新应用，旨在通过讨论最新技术和展望未来发展，为相关研究人员提供图像和视频生成领域的全面了解。
    </p>
    <h2 style="text-align:justify;">
     <span style="color:#1c7331;">
      <strong>
       3.内容整理
      </strong>
     </span>
    </h2>
    <p style="text-align:justify;">
     罗博士的报告主要分为三个部分，下面逐一加以详细介绍
    </p>
    <h3 style="text-align:justify;">
     <span style="color:#98c091;">
      <strong>
       (1)图像和视频生成的简要回顾
      </strong>
     </span>
    </h3>
    <p style="text-align:justify;">
     <span style="color:#0d0016;">
      <strong>
       1)文本生成图像
      </strong>
     </span>
    </p>
    <p style="text-align:justify;">
     报告中呈现了文本生成图像领域的发展历程，从2016年基于生成对抗网络的方法，到2024年3月Stability AI发布的最强图片生成模型Stable Diffusion3。从图像结果和技术视角两个方向，罗博士介绍了图像生成的发展趋势。从图像结果来看，视觉上更吸引人：从“基本上是我想要的”到 “看起来惊人”；图像的分辨率得到提升：从256x256 到 1024x1024 ，再到超过2k；模型能够更好的理解提示：包括对象关系、数量和属性等；模型能够更好的进行符号控制：能够正确地拼写复杂文本。从技术视角来看，模型的规模扩大：Stable Diffusion3最大可达到8B；此外，扩散模型成为该领域的主流技术；最新的技术已经可以实现矫正流、噪声调度等高级操作。
    </p>
    <p style="text-align:justify;">
     <strong>
      2)文本生成视频
     </strong>
    </p>
    <p style="text-align:justify;">
     罗博士总结了文本生成视频领域从2021年到2024年的关键技术，从微软亚洲研究院和北京大学于2021年11月联合提出的NUWA模型，到2024年3月OpenAI发布的震惊业界的Sora。随后讨论了Sora发布之前，视频生成技术的现状和挑战。例如，存在场景单一、动作简单或者细微、难以在大幅度运动时保持表现的一致性等问题。
    </p>
    <h3 style="text-align:justify;">
     <span style="color:#98c091;">
      <strong>
       (2)图像和视频生成的技术进展
      </strong>
     </span>
    </h3>
    <p style="text-align:justify;">
     <strong>
      1)MicroCinema
     </strong>
    </p>
    <p style="text-align:justify;">
     MicroCinema是一个用于文本生成视频的有效框架，引入了一种分而治之的策略，将文本到视频的过程分为两个阶段：文本生成图像和图像/文本生成视频。这种策略具有两个显著优势：一是充分利用了Stable Diffusion、Midjourney 和 DALLE 等文本生成图像模型的最新技术；二是利用生成的图使模型可以较少关注细粒度的外观细节，更优先考虑运动动力学的高效学习。图1展示了MicroCinema创建连贯且高质量视频的能力。
    </p>
    <p style="text-align:justify;">
     论文下载链接：
     <a class="link-info" href="https://arxiv.org/abs/2311.18829" rel="nofollow" title="https://arxiv.org/abs/2311.18829">
      https://arxiv.org/abs/2311.18829
     </a>
     。
    </p>
    <p style="text-align:justify;">
     视频样本下载链接：
     <a class="link-info" href="https://wangyanhui666.github.io/MicroCinema.github.io/" rel="nofollow" title="https://wangyanhui666.github.io/MicroCinema.github.io/">
      https://wangyanhui666.github.io/MicroCinema.github.io/
     </a>
     。
    </p>
    <p style="text-align:justify;">
    </p>
    <p class="img-center">
     <img alt="" height="518" src="https://i-blog.csdnimg.cn/blog_migrate/1f3eb66488b3c6a6472dbe93964ca1fa.png" width="865"/>
    </p>
    <p style="text-align:center;">
     <strong>
      图 1 MicroCinema生成的样本视频
     </strong>
    </p>
    <p style="text-align:justify;">
     MicroCinema目前面临的关键挑战包含两个方面，一是如何将图像条件注入到视频生成网络中，以确保图像条件得到忠实保留、视频生成能力不受影响；二是如何确保稳定生成，避免外观损坏或动作不一致。
    </p>
    <p style="text-align:justify;">
     <strong>
      2)CCEdit
     </strong>
    </p>
    <p style="text-align:justify;">
     CCEdit是一种基于扩散模型的多功能生成性视频编辑框架，采用了一种三叉网络结构，包括文本到视频（T2V）主分支、外观分支和结构分支。模型区分了结构和外观控制，从而确保精确和创造性的编辑能力，通过ControlNet架构保持编辑过程中视频的结构完整性。增加的外观分支使用户能够对关键帧进行细粒度控制。模型的主分支基于现有的文本到图像（T2I）生成模型构建，然后将两个侧分支无缝地整合到主分支中，并通过可学习的时间层进行连接。CCEdit在多项评估指标上取得了目前最优表现，图2展示CCEdit强大而灵活的视频编辑额能力。
    </p>
    <p style="text-align:justify;">
     论文下载链接：
     <a class="link-info" href="https://arxiv.org/abs/2309.16496" rel="nofollow" title="https://arxiv.org/abs/2309.16496">
      https://arxiv.org/abs/2309.16496
     </a>
     。
    </p>
    <p style="text-align:justify;">
     视频样本下载链接：
     <a class="link-info" href="https://ruoyufeng.github.io/CCEdit.github.io/" rel="nofollow" title="https://ruoyufeng.github.io/CCEdit.github.io/">
      https://ruoyufeng.github.io/CCEdit.github.io/
     </a>
     。
    </p>
    <p style="text-align:justify;">
    </p>
    <p class="img-center">
     <img alt="" height="616" src="https://i-blog.csdnimg.cn/blog_migrate/32b138e9e34a7859765e0ef535f5113b.png" width="865"/>
    </p>
    <p style="text-align:center;">
     <strong>
      图 2  CCEdit为用户提供了一套强大而灵活的视频编辑功能，包括风格迁移(第1 ~ 3行)、前景修改(第4行)和背景替换(第5行)。
     </strong>
    </p>
    <h3 style="text-align:justify;">
     <span style="color:#98c091;">
      <strong>
       (3)讨论与展望
      </strong>
     </span>
    </h3>
    <p style="text-align:justify;">
     <strong>
      1)讨论
     </strong>
    </p>
    <p style="text-align:justify;">
     尽管已解决了一些存在的问题，但关于自动编码器/分词器的重要性、高描述性文本标题， DiT相比于U-net在强度和扩展性方面的优势等仍需进一步研究。此外，视频生成模型在可控性、一致性以及多轮视频生成与编辑方面的问题依然存在，这些都是当前亟需克服的技术难题。
    </p>
    <p style="text-align:justify;">
     <strong>
      2)展望
     </strong>
    </p>
    <p style="text-align:justify;">
     视频生成技术正处于一个多元化创新和快速发展的阶段，正面临着多种技术挑战和行业变革的机遇。在资源有限的情况下，解耦外观与动作建模是一种有效的可行策略。当前，视频生成模型还需要解决可控性、场景与物体一致性及多轮编辑的可行性等问题。研究统一理解与生成、多模态融合的最佳模型是重点方向之一。此外，视频生成技术对人工通用智能的发展具有重要推动作用，但仍需评估Sora技术作为实现世界模拟器的潜在有效性。
    </p>
   </div>
  </div>
  <div class="blog-extension-box" id="blogExtensionBox" style="width:400px;margin:auto;margin-top:12px">
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f617564797869616f3030312f:61727469636c652f64657461696c732f313338353833373133" class_="artid" style="display:none">
 </p>
</div>
