---
layout: post
title: "DeepSeek-R1强化学习驱动的LLM推理能力提升"
date: 2025-02-22 17:13:39 +0800
description: "个人觉得这本书非常的不错，是一本不可多得的好书，值得拥有去学习。感兴趣的朋友可以购买，多谢支持！就在"
keywords: "deepseek r1 32k"
categories: ["未分类"]
tags: ["深度学习", "人工智能"]
artid: "145478160"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=145478160
  alt: "DeepSeek-R1强化学习驱动的LLM推理能力提升"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     DeepSeek-R1：强化学习驱动的LLM推理能力提升
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h2>
     前言
    </h2>
    <p>
     先来推荐一本书《揭秘大模型：从原理到实战》，传送门：
     <a href="https://item.jd.com/14893696.html" rel="nofollow" title="https://item.jd.com/14893696.html">
      https://item.jd.com/14893696.html
     </a>
     ，个人觉得这本书非常的不错，是一本不可多得的好书，值得拥有去学习。感兴趣的朋友可以购买，多谢支持！
    </p>
    <p>
     就在2025年01月20日，deepseek 正式发布 DeepSeek-R1，并同步开源模型权重。
    </p>
    <ul>
     <li>
      开源 DeepSeek-R1 推理大模型，与 o1 性能相近。‍‍
     </li>
     <li>
      开源 DeepSeek-R1-Zero，预训练模型直接 RL，不走 SFT。
     </li>
     <li>
      开源用 R1 数据蒸馏的 Qwen、Llama 系列小模型，蒸馏模型超过 o1-mini 和 QWQ。
     </li>
    </ul>
    <p>
     模型开源的同时，技术报告也同步放出： DeepSeek-R1: Incentivizing Reasoning Capability in LLMs viaReinforcement Learning
     <a href="https://link.zhihu.com/?target=https%3A//github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf" rel="nofollow" title="跳转中...">
      跳转中...
     </a>
    </p>
    <p>
     下面，就来详细解读这篇论文的内容及实际应用的实践探索。
    </p>
    <h2>
     前提背景
    </h2>
    <h3>
     1、研究问题
    </h3>
    <p>
     如何通过强化学习（RL）有效提升大型语言模型（LLM）的推理能力？
    </p>
    <h3>
     2、问题背景
    </h3>
    <blockquote>
     <p>
      近年来，LLM在自然语言处理、计算机视觉等多个领域取得了显著进展，但在推理能力方面仍有提升空间。以往的研究大多依赖于大量的监督微调（SFT）数据，但获取高质量的SFT数据成本高昂。OpenAI的o1系列模型通过增加思维链（Chain-of-Thought, CoT）推理过程的长度来提升推理能力，但如何在测试时（test-time）有效扩展推理长度仍是一个开放问题。一些研究尝试使用基于过程的奖励模型（PRM）、强化学习和蒙特卡洛树搜索（MCTS）等方法来解决推理问题，但这些方法尚未达到OpenAI o1系列模型的通用推理性能水平。
     </p>
    </blockquote>
    <h3>
     3、创作动机
    </h3>
    <p>
     探索是否可以通过纯强化学习让LLM自主发展推理能力，而无需依赖SFT数据。
    </p>
    <h2>
     相关的研究
    </h2>
    <ul>
     <li>
      SFT：以往的研究通常依赖SFT来增强模型性能，但SFT需要大量标注数据，成本高且耗时。
     </li>
     <li>
      推理时扩展：OpenAI的o1系列模型通过增加CoT推理长度来实现推理能力扩展，但测试时扩展的挑战仍然存在。
     </li>
     <li>
      基于过程的奖励模型（PRM）：一些研究采用PRM来引导模型进行推理，但这些模型在实际应用中存在局限性。
     </li>
     <li>
      强化学习：强化学习已被用于提升推理能力，但通常与SFT数据结合使用，难以探索纯RL的潜力。
     </li>
     <li>
      搜索算法：如蒙特卡洛树搜索（MCTS）等算法也被用于增强推理，但效果有限。
     </li>
    </ul>
    <h2>
     内容核心
    </h2>
    <p>
     本文主要介绍一种新的训练策略，通过纯强化学习显著提升了LLM的推理能力，主要包括下面几点：
    </p>
    <table>
     <tbody>
      <tr>
       <td>
        <p>
         模型
        </p>
       </td>
       <td>
        <p>
         方法
        </p>
       </td>
      </tr>
      <tr>
       <td>
        <p>
         DeepSeek-R1-Zero
        </p>
       </td>
       <td>
        <p>
         纯强化学习
        </p>
       </td>
      </tr>
      <tr>
       <td>
        <p>
         DeepSeek-R1
        </p>
       </td>
       <td>
        <p>
         冷启动 SFT -&gt; RL -&gt; COT + 通用数据 SFT（80w）-&gt;全场景 RL
        </p>
       </td>
      </tr>
      <tr>
       <td>
        <p>
         蒸馏小模型
        </p>
       </td>
       <td>
        <p>
         直接用上面的 80w 数据进行SFT
        </p>
       </td>
      </tr>
     </tbody>
    </table>
    <ul>
     <li>
      DeepSeek-R1-Zero：首次验证了纯强化学习在LLM中显著增强推理能力的可行性，DeepSeek-R1-Zero无需预先的SFT数据，仅通过RL即可激励模型学会长链推理和反思等能力。
     </li>
     <li>
      多阶段训练策略：提出了多阶段训练策略（冷启动-&gt;RL-&gt;SFT-&gt;全场景RL），有效兼顾准确率与可读性，产出DeepSeek-R1，性能比肩OpenAI-o1-1217。
     </li>
     <li>
      知识蒸馏：展示了知识蒸馏在提升小模型推理能力方面的潜力，并开源多个大小不一的蒸馏模型（1.5B~70B），为社区提供了可在低资源环境中也能获得高推理能力的模型选择。
     </li>
    </ul>
    <h2>
     DeepSeek-R1-Zero
    </h2>
    <p>
     先来说DeepSeek-R1-Zero，它直接在基础模型上应用强化学习，不使用任何SFT数据，在训练过程中，DeepSeek采用了一种基于规则的奖励系统，主要包括以下两种奖励：
    </p>
    <ul>
     <li>
      准确率奖励：评估响应是否正确，比如在具有确定性结果的数学问题中，模型需要以指定的格式（box）提供最终答案，从而能够通过基于规则的验证来可靠地确认正确性。与此同样，对于LeetCode问题，可以使用编译器根据预定义的测试用例生成反馈。
     </li>
     <li>
      格式奖励：要求模型将其思考过程放在特定的标签之间。
     </li>
    </ul>
    <p>
     还有就是，DeepSeek-R1-Zero在训练过程中没有使用结果奖励（ORM）或过程奖励（PRM），在通过“结果判定”的方式，模型在没有大量带“过程标签”数据的情况下，依然能够通过最终答案的正确与否来获得奖励，从而学会更好的推理。
    </p>
    <h3>
     1、性能
    </h3>
    <p>
     关于DeepSeek-R1-Zero的性能，在AIME 2024基准测试中，DeepSeek-R1-Zero的强化学习训练性能轨迹显示，其性能随着训练的推进稳步提升。尤其是在AIME 2024的pass@1分数上，从最初的15.6%飙升至71.0%，达到了与OpenAI-o1-0912相媲美的性能水平。具体DeepSeek-R1-Zero 训练期间 AIME 准确率如下图所示：
    </p>
    <p class="img-center">
     <img alt="" height="788" src="https://i-blog.csdnimg.cn/direct/59b510454a8a415aa1ea13893f681a30.png" width="1340"/>
    </p>
    <h3>
     2、自我演化过程
    </h3>
    <p>
     接下来看一下DeepSeek-R1-Zero的自我演化过程，这一过程生动地展现了强化学习如何自主推动模型提升推理能力，从基座模型直接开始RL训练，绕过了SFT阶段，从而能够紧密跟踪模型的成长轨迹。这种方法为模型随时间推移不断演进提供了清晰视角，尤其是其在处理复杂推理任务方面的能力提升，具体的DeepSeek-R1-Zero 在 RL 过程中的平均响应长度（输出长度不断增加）如下所示：
    </p>
    <p class="img-center">
     <img alt="" height="746" src="https://i-blog.csdnimg.cn/direct/bb9eced1cd064af9858b0d3757fb8797.png" width="1310"/>
    </p>
    <h3>
     3、“顿悟”时刻
    </h3>
    <p>
     另外，在训练历程中DeepSeek-R1-Zero出现了一个特别引人注目的现象——“顿悟时刻”，这一关键时刻发生在模型的中间发展阶段，模型通过重新审视其初始策略，学会了为问题分配更多思考时间。我觉得这一行为不仅彰显了模型推理能力的显著提升，也是强化学习如何催生意外且复杂成果的一个生动例证。
    </p>
    <p>
     在大规模强化学习中，模型的「思考过程」会不断与最终的正确率奖励相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「回头反省」，尝试补充或修正先前的思路，从而获得更高的奖励。
    </p>
    <p>
     随着强化学习的迭代，这种「主动回溯、推翻先前想法并重新推理」的行为逐渐巩固，便在输出中表现为所谓的「aha moment」。但是在本质上，这是 RL 为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。这也展示了 RL 的强大潜力，它可以让模型在没有明确指导的情况下，自主学习并改进。具体的DeepSeek-R1-Zero 的“顿悟时刻”，模型学会了用拟人化的语气重新思考如下所示：
    </p>
    <p class="img-center">
     <img alt="" height="618" src="https://i-blog.csdnimg.cn/direct/84f7a9801fa74098b44de83cc6a33aa5.png" width="1318"/>
    </p>
    <h2>
     DeepSeek-R1：冷启动强化学习
    </h2>
    <p>
     关于DeepSeek-R1的冷启动强化学习方面，主要是采用了冷启动结合多阶段训练的方法，通过逐步优化模型的推理能力和综合性能，最终实现模型在多种场景下的稳定表现。
    </p>
    <h3>
     阶段1：冷启动
    </h3>
    <p>
     在训练初期，DeepSeek-R1通过少量高质量的因果思维（CoT）数据进行冷启动，为模型预热，这一阶段的目标是让模型掌握基本的CoT推理能力，并确保输出内容更具可读性。为了获取这些冷启动数据，采用了多种策略包括利用长思维回答作为少样本示例、直接提示模型生成包含反思和验证步骤的详细答案，以及对DeepSeek-R1-Zero的输出进行人工标注和细化。最终收集了数千条冷启动数据，用于微调DeepSeek-V3-Base模型，作为后续强化学习的起点。这些冷启动数据采用了一种易于理解的格式，明确将输出定义为推理过程和总结两部分。
    </p>
    <h3>
     阶段2：推理导向的强化学习
    </h3>
    <p>
     在冷启动的基础上，DeepSeek-R1进入推理导向的强化学习阶段，重点提升模型在推理任务上的表现，这个阶段引入了语言一致性奖励，根据思维链中目标语言单词的比例计算奖励值，以减少推理过程中的语言混合问题。尽管消融实验显示，语言一致性奖励可能会略微降低模型性能，但它更符合人类偏好，提高了内容的可读性。最后推理任务的准确性和语言一致性奖励相结合，形成了综合奖励函数。模型在这一阶段继续进行强化学习，直至在推理任务上达到收敛。
    </p>
    <h3>
     阶段3：拒绝采样与监督微调
    </h3>
    <p>
     接下来，DeepSeek-R1利用上一阶段的强化学习模型进行拒绝采样，生成高质量的推理和非推理数据，并使用这些数据对模型进行监督微调（SFT）。这一阶段的目标是提升模型的综合能力，使其在写作、事实问答等多种任务上都能表现出色。当强化学习训练接近收敛时，使用中间的checkpoint来采样SFT数据。与冷启动阶段主要关注推理能力的数据不同，这一阶段加入了其他领域的数据，以增强模型在写作、角色扮演以及其他通用任务上的表现。
    </p>
    <p>
     具体步骤包括：对于推理数据，构建推理提示，并从强化学习训练的checkpoint中进行拒绝采样，生成推理轨迹；对于非推理数据，比如写作、问答、翻译等任务，使用DeepSeek-V3 SFT数据集的一部分，通过筛选和整理，最终收集了约80万个训练样本（60万推理样本和20万通用样本），并使用这些样本对DeepSeek-v3-Base模型进行了两轮SFT。
    </p>
    <h3>
     阶段4：全场景强化学习
    </h3>
    <p>
     在SFT模型的基础上，DeepSeek-R1再次进行强化学习，确保模型在所有场景下都能表现良好，包括推理任务和非推理任务，并保证模型的安全性和无害性。
    </p>
    <h2>
     冷启动数据的作用
    </h2>
    <p>
     还有就是使用冷启动数据的主要目的是，解决DeepSeek-R1-Zero在训练早期出现的不稳定问题，与直接在基础模型上进行强化学习相比，使用少量监督微调（SFT）数据进行冷启动可以让模型更快地进入稳定训练阶段。关于冷启动数据的优势，可以从下面几点来看：
    </p>
    <ul>
     <li>
      可读性：冷启动数据采用更易于理解的格式，使模型输出更适合人类阅读，避免了DeepSeek-R1-Zero输出的语言混合和格式混乱问题。
     </li>
     <li>
      潜在性能：通过精心设计冷启动数据的模式，可以引导模型产生更好的推理能力。
     </li>
     <li>
      稳定训练：以SFT数据为起点，可以避免强化学习训练早期阶段的不稳定问题。
     </li>
    </ul>
    <h2>
     蒸馏小模型
    </h2>
    <p>
     为了打造更高效且具备DeepSeek-R1推理能力的小型模型，这里直接对Qwen和Llama等开源模型进行了微调，所使用的数据集正是之前在SFT阶段为DeepSeek-R1准备的80万条数据。
    </p>
    <p>
     实验结果显示，这种直接蒸馏的方法显著提升了小模型的推理性能，在这一过程中选用的基座模型涵盖了Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.5-14B、Qwen2.5-32B、Llama-3.1-8B以及Llama-3.3-70B-Instruct。
    </p>
    <p>
     在蒸馏模型的训练过程中，仅实施了SFT，而未涉及强化学习（RL）阶段，虽然RL的加入能够显著提升模型性能，但是为什么在将模型蒸馏至小模型时，直接在小模型上应用RL训练的效果不如先训练大模型再进行蒸馏呢？
    </p>
    <p>
     我觉得主要原因在于，大模型在RL训练阶段能够发展出许多复杂的推理模式。相比之下，小模型由于其容量和表示能力有限，在无监督或纯RL的环境下很难学到相同水平的推理模式。
    </p>
    <p>
     通过蒸馏，我们可以将大模型的推理轨迹直接传递给小模型。小模型只需模仿大模型相对完整的推理流程，便能在较小的训练和推理成本下，取得远超自身独立强化学习的效果。
    </p>
    <p>
     在蒸馏模型的实际操作中，我们仅采用了SFT阶段，而未包含RL阶段。尽管RL的加入能够显著提升模型性能，但根据DeepSeek团队的说法，本工作的核心目标是展示蒸馏技术的有效性，而将RL阶段的深入探索留给更广泛的研究社区去完成。
    </p>
    <h2>
     为什么PRM和MCTS没有成功？
    </h2>
    <p>
     基于过程奖励模型（PRM）和蒙特卡洛树搜索（MCTS）并不适合 LLM 的推理。
    </p>
    <h4>
     PRM 的挑战
    </h4>
    <ul>
     <li>
      难以定义通用的、细粒度的推理步骤。
     </li>
     <li>
      难以准确判断中间步骤的正确性，且自动标注方法效果不佳，人工标注又难以扩展。
     </li>
     <li>
      模型化的 PRM 容易导致奖励黑客行为（Agent 利用奖励函数或环境中的漏洞来获取高奖励，而并未真正学习到预期行为），并且会增加额外的训练成本。
     </li>
    </ul>
    <h4>
     MCTS 的挑战
    </h4>
    <ul>
     <li>
      LLM 的 token 生成搜索空间巨大，远远超出棋类游戏，容易陷入局部最优解。
     </li>
     <li>
      价值模型的训练非常困难，导致难以迭代提升。
     </li>
    </ul>
    <h2>
     实验与评估
    </h2>
    <h3>
     1、基准测试
    </h3>
    <p>
     在多个领域对DeepSeek-R1及其蒸馏模型进行了全面评估，涵盖数学推理（AIME 2024、MATH-500）、编程任务（LiveCodeBench、Codeforces）、知识问答（MMLU、GPQA Diamond、SimpleQA）以及开放生成场景（AlpacaEval2.0、ArenaHard）。对于蒸馏模型，特别关注了其在AIME 2024、MATH-500、GPQA Diamond、Codeforces和LiveCodeBench等关键基准上的表现。DeepSeek-R1在每个基准测试中的输出长度被限制在32,768个标记以内，以确保评估的一致性和可比性。
    </p>
    <h3>
     2、对比模型
    </h3>
    <p>
     在评估过程中，DeepSeek-R1与多个先进模型进行了对比，包括DeepSeek-V3、Claude-Sonnet-3.5-1022、GPT-4o-0513、OpenAI-o1-mini和OpenAI-o1-1217。这些模型涵盖了不同的架构和训练策略，为评估提供了丰富的对比维度。
    </p>
    <h3>
     3、实验设置
    </h3>
    <p>
     为了确保公平比较，所有参与评估的模型均被配置为最大生成长度为32k。生成参数统一设置为temperature=0.6和top-p=0.95，每次生成64个回答以准确估计pass@1指标。这种设置旨在模拟实际应用中的生成场景，确保评估结果具有现实意义。
    </p>
    <p>
     实验设置：在多种数学推理、代码题、知识问答和开放生成场景等基准测试上进行了系统评测。所有模型的最大生成长度设置为32k，temperature=0.6，top-p=0.95，每次生成64回答以估计pass@1。
    </p>
    <p>
     DeepSeek-R1评估：DeepSeek-R1在教育知识基准、IF-Eval基准上表现出色，生成的摘要简洁，数学推理性能与OpenAI-o1-1217相当。
    </p>
    <p>
     蒸馏模型评估：通过简单蒸馏DeepSeek-R1的输出，得到的蒸馏模型在所有方面均优于非推理模型GPT-4o-0513，且在大多数基准测试中显著优于o1-mini。
    </p>
    <h2>
     DeepSeek-R1 评估
    </h2>
    <p>
     关于DeepSeek-R1的评估，具体如下所示：
    </p>
    <ul>
     <li>
      教育知识基准性能卓越：DeepSeek-R1 在 MMLU、MMLU-Pro 和 GPQA Diamond 等教育知识基准上相比 DeepSeek-V3 显示出卓越性能。
     </li>
     <li>
      IF-Eval基准结果令人印象深刻：DeepSeek-R1 在 IF-Eval 基准上取得令人印象深刻的结果，该基准评估模型遵循格式指令的能力。改进归因于 SFT 和 RL 训练最后阶段包含的指令遵循数据。
     </li>
     <li>
      摘要简洁，避免长度偏差：DeepSeek-R1 生成的摘要简洁，在 ArenaHard 和 AlpacaEval 2.0 上长度分别为 689 个 token 和 2,218 个字符。
     </li>
     <li>
      数学推理和OpenAI-o1-1217持平：DeepSeek-R1 在数学任务上的性能与 OpenAI-o1-1217 相当，远超其他模型。
     </li>
    </ul>
    <p>
     下面是DeepSeek-R1实验结果，如下所示：
    </p>
    <p class="img-center">
     <img alt="" height="1026" src="https://i-blog.csdnimg.cn/direct/00c3f559983b47c598401dbb3e78103d.png" width="1324"/>
    </p>
    <h2>
     蒸馏模型评估
    </h2>
    <p>
     关于蒸馏模型评估，主要是在下面几点：
    </p>
    <ul>
     <li>
      通过简单蒸馏 DeepSeek-R1 的输出，得到 DeepSeek-R1-Distill-Qwen-7B，其在所有方面均优于非推理模型 GPT-4o-0513。
     </li>
     <li>
      DeepSeek-R1-14B 在所有评估指标上均超越了 QwQ-32BPreview。
     </li>
     <li>
      DeepSeek-R1-32B 和 DeepSeek-R1-70B 在大多数基准测试中均显著优于 o1-mini。
     </li>
    </ul>
    <p>
     还有就是，将强化学习（RL）应用于这些蒸馏模型能够带来显著的额外提升，这一方向值得进一步探索。deepseek 仅展示了蒸馏模型的结果，下面是蒸馏模型效果：
    </p>
    <p class="img-center">
     <img alt="" height="434" src="https://i-blog.csdnimg.cn/direct/d7798c6925c441a196516638c105724b.png" width="1350"/>
    </p>
    <h2>
     延伸讨论：选择蒸馏还是强化学习？
    </h2>
    <p>
     在上图关于蒸馏模型效果图中，我们可以看到通过蒸馏 DeepSeek-R1，小模型取得了显著成效，但是这引发了一个疑问：小模型是否能在不依赖蒸馏的情况下，仅凭大规模强化学习训练就达到类似的性能水平？
    </p>
    <p>
     为了解答这一疑问，deepseek 对 Qwen-32B-Base 进行了超过 10,000 步的大规模强化学习训练，最终得到了DeepSeek-R1-Zero-Qwen-32B。经过大规模强化学习训练后，其性能与 QwQ-32B-Preview 相当，但是通过蒸馏 DeepSeek-R1 得到的 DeepSeek-R1-Distill-Qwen-32B 在所有基准测试中的表现均明显优于DeepSeek-R1-Zero-Qwen-32B。这说明一个问题，那就是在 Qwen-32B-Base 上，直接进行强化学习训练的效果不如通过蒸馏DeepSeek-R1。
    </p>
    <p>
     最后，我们可以得出以下两个结论：
    </p>
    <ul>
     <li>
      第一，将更强大的模型蒸馏到较小模型中能产生优异的结果，而依赖大规模强化学习的小模型则需消耗巨大的计算资源，且可能仍无法达到蒸馏所能达到的性能水平。
     </li>
     <li>
      第二，尽管蒸馏策略既经济又高效，但若要突破智能的界限，可能仍需依赖更强大的基础模型以及大规模强化学习。
     </li>
    </ul>
    <h2>
     结束语
    </h2>
    <p>
     通过本文的详细介绍，通过纯强化学习（RL）的方法，DeepSeek-R1不仅显著增强了LLM的推理能力，还通过多阶段训练策略和知识蒸馏技术，为开发者提供了高效且实用的模型选择。另外详细介绍了DeepSeek-R1的训练过程，包括从零开始的强化学习（DeepSeek-R1-Zero）、冷启动强化学习（DeepSeek-R1），以及知识蒸馏技术的应用，通过这些方法，DeepSeek-R1在多个基准测试中表现出色，与现有的先进模型相比，展现了卓越的推理能力和综合性能。尤其是在教育知识基准、数学推理任务以及开放生成场景中，DeepSeek-R1的表现尤为突出，验证了它在实际应用中的巨大潜力。还有就是通过将大模型的推理轨迹直接传递给小模型，DeepSeek-R1的蒸馏模型在低资源环境中也能取得优异的性能，为资源受限的场景提供了可行的解决方案。我觉得随着强化学习技术的不断进步，以及对更高效训练策略的深入研究，LLM的推理能力将得到进一步的提升，而且我们也将继续探索如何更好地结合强化学习和知识蒸馏，以实现更强大的模型性能和更广泛的应用场景。个人觉得DeepSeek-R1的出现，不仅为LLM的推理能力提升提供了一个新的方向，也为整个机器学习社区带来了新的启示。最后，一起来期待更多基于强化学习和知识蒸馏的创新研究，推动人工智能技术向更智能、更高效的方向发展。
    </p>
    <h3>
     参考文献
    </h3>
    <p>
     DeepSeek-R1 发布，性能对标 OpenAI o1 正式版
     <a href="https://mp.weixin.qq.com/s/atKyfC5l-BaStje8-F3FGQ" rel="nofollow" title="DeepSeek-R1 发布，性能对标 OpenAI o1 正式版">
      DeepSeek-R1 发布，性能对标 OpenAI o1 正式版
     </a>
    </p>
    <p>
     <a href="https://github.com/deepseek-ai/DeepSeek-R1/tree/main?tab=readme-ov-file" title="GitHub - deepseek-ai/DeepSeek-R1">
      GitHub - deepseek-ai/DeepSeek-R1
     </a>
    </p>
    <p>
     DeepSeek-R1: Incentivizing Reasoning Capability in LLMs viaReinforcement Learning
     <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf" title="DeepSeek-R1/DeepSeek_R1.pdf at main · deepseek-ai/DeepSeek-R1 · GitHub">
      DeepSeek-R1/DeepSeek_R1.pdf at main · deepseek-ai/DeepSeek-R1 · GitHub
     </a>
    </p>
    <p>
     DeepSeek是新源神！推理模型o1性能1/50价格，微调/数据/商用全免费，蒸馏1.5B小模型可比GPT-4o
     <a href="https://mp.weixin.qq.com/s/EatwWk0vfGBv6efRKXPV-w" rel="nofollow" title="DeepSeek是新源神！推理模型o1性能1/50价格，微调/数据/商用全免费，蒸馏1.5B小模型可比GPT-4o">
      DeepSeek是新源神！推理模型o1性能1/50价格，微调/数据/商用全免费，蒸馏1.5B小模型可比GPT-4o
     </a>
    </p>
    <p>
     tomsheep：【论文解读】DeepSeek-R1：通过强化学习提升LLM推理能力
     <a href="https://zhuanlan.zhihu.com/p/19551355661" rel="nofollow" title="https://zhuanlan.zhihu.com/p/19551355661">
      https://zhuanlan.zhihu.com/p/19551355661
     </a>
    </p>
    <p>
     DeepSeek-R1 技术报告解读
     <a href="https://zhuanlan.zhihu.com/p/19868935152" rel="nofollow" title="https://zhuanlan.zhihu.com/p/19868935152">
      https://zhuanlan.zhihu.com/p/19868935152
     </a>
    </p>
   </div>
  </div>
 </article>
 <p alt="6874747073:3a2f2f626c6f672e6373646e2e6e65742f4343313939315f2f:61727469636c652f64657461696c732f313435343738313630" class_="artid" style="display:none">
 </p>
</div>
