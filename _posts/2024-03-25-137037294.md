---
layout: post
title: "52个AIGC视频生成算法模型介绍"
date: 2024-03-25 16:20:51 +0800
description: "基于Diffusion模型的AIGC生成算法日益火热，其中文生图，图生图等图像生成技术普遍成熟，很多"
keywords: "aigc视频生成框架"
categories: ['未分类']
tags: ['Aigc']
artid: "137037294"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=137037294
  alt: "52个AIGC视频生成算法模型介绍"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     52个AIGC视频生成算法模型介绍
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <div id="js_content">
     <p style="text-align:center;">
      <img alt="cb4b09c79904c61c602fa64d80dd87c7.gif" src="https://i-blog.csdnimg.cn/blog_migrate/5d20aba1ad46f5fecd122bf12c0dd158.gif"/>
     </p>
     <p style="text-align:justify;">
      基于Diffusion模型的AIGC生成算法日益火热，其中文生图，图生图等图像生成技术普遍成熟，很多算法从业者开始从事视频生成算法的研究和开发，原因是视频生成领域相对空白。
     </p>
     <h3>
     </h3>
     <p style="text-align:center;">
      <img alt="1c4428405eb4326ffd97d5cbdbd82510.png" src="https://i-blog.csdnimg.cn/blog_migrate/16680a63cde3d09384ed6455b411546a.png"/>
     </p>
     <p style="text-align:center;">
      AIGC视频算法发展现状
     </p>
     <p>
      从2023年开始，AIGC+视频的新算法层出不穷，其中最直接的是把图像方面的成果引入视频领域，并结合时序信息去生成具有连续性的视频。随着Sora的出现，视频生成的效果又再次上升了一个台阶，因此有必要将去年一年到现在的视频领域进展梳理一下，为以后的视频方向的研究提供一点思路。
     </p>
     <p style="text-align:center;">
      <img alt="89c56ad60d1947e9e9f4396669cefb70.png" src="https://i-blog.csdnimg.cn/blog_migrate/807a2ae65be5c4e19422debff9c53218.png"/>
     </p>
     <p style="text-align:center;">
      AIGC视频算法分类
     </p>
     <p>
      AIGC视频算法，经过梳理发现，可以大体分为：文生视频，图生视频，视频编辑，视频风格化，人物动态化，长视频生成等方向。具体的输入和输出形式如下：
     </p>
     <ol>
      <li>
       <p>
        文生视频：输入文本，输出视频
       </p>
      </li>
      <li>
       <p>
        图生视频：输入图片（+控制条件），输出视频
       </p>
      </li>
      <li>
       <p>
        视频编辑：输入视频（+控制条件），输出视频
       </p>
      </li>
      <li>
       <p>
        视频风格化：输入视频，输出视频
       </p>
      </li>
      <li>
       <p>
        人物动态化：输入图片+姿态条件，输出视频
       </p>
      </li>
      <li>
       <p>
        长视频生成：输入文本，输出长视频
       </p>
      </li>
     </ol>
     <p>
      ﻿
     </p>
     <p style="text-align:center;">
      <img alt="51287f0d075d356969d1ab42d62df34d.png" src="https://i-blog.csdnimg.cn/blog_migrate/01c12d1c846f482fbb221c6644fba040.png"/>
     </p>
     <p style="text-align:center;">
      具体算法梳理
     </p>
     <h5>
      <strong>
       ▐
      </strong>
      <strong>
      </strong>
      <strong>
       文生视频
      </strong>
     </h5>
     <ul>
      <li>
       <h4>
        CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers
       </h4>
      </li>
     </ul>
     <p>
      机构：清华
     </p>
     <p>
      时间：2022.5.29
     </p>
     <p>
      ﻿https://github.com/THUDM/CogVideo.
     </p>
     <p>
      简单介绍：基于两阶段的transformer（生成+帧间插值）来做文生视频
     </p>
     <p style="text-align:center;">
      <img alt="ed514daa38b20aecd74c7bbe28d3ef5c.png" src="https://i-blog.csdnimg.cn/blog_migrate/de35f377914da5fef3c2206de3571817.png"/>
     </p>
     <ul>
      <li>
       <h4>
        IMAGEN VIDEO
       </h4>
      </li>
     </ul>
     <p>
      机构：Google
     </p>
     <p>
      时间：2022.10.5
     </p>
     <p>
      简单介绍：基于google的Imagen来做的时序扩展，而Imagen和Imagen video都没有开源
     </p>
     <p>
      <img alt="bbabe3ca53eaf850b5c572bfcf0fd2ef.png" src="https://i-blog.csdnimg.cn/blog_migrate/ad0235deca8e5516030a3c639a29b5c9.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators
       </h4>
      </li>
     </ul>
     <p>
      机构：Picsart AI Resarch
     </p>
     <p>
      时间：2023.3.23
     </p>
     <p>
      ﻿https://github.com/Picsart-AI-Research/Text2Video-Zero﻿
     </p>
     <p>
      简单介绍：基于图像diffusion model引入corss-frame attention来做时序建模，其次通过显著性检测来实现背景平滑。
     </p>
     <p style="text-align:center;">
      <img alt="259d391173d47f7eaa05edc1a1840f02.png" src="https://i-blog.csdnimg.cn/blog_migrate/6005db9b78bb761c94784895f7e4971c.png"/>
     </p>
     <ul>
      <li>
       <h4>
        MagicVideo: Efficient Video GenerationWith Latent Diffusion Models
       </h4>
      </li>
     </ul>
     <p>
      机构：字节
     </p>
     <p>
      时间：2023.5.11
     </p>
     <p>
      简单介绍：直接将图像SD架构扩展成视频，增加了时序信息
     </p>
     <p>
      <img alt="4b0eec415d523dd0d813e11eb173fd9d.png" src="https://i-blog.csdnimg.cn/blog_migrate/4c2405ee6bd2226c9c60b0d76362a322.png"/>
     </p>
     <ul>
      <li>
       <h4>
        AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning
       </h4>
      </li>
     </ul>
     <p>
      机构：上海 AI Lab
     </p>
     <p>
      时间：2023.7.11
     </p>
     <p>
      ﻿https://animatediff.github.io/﻿
     </p>
     <p>
      简单介绍：基于图像diffusion model，训练一个运动建模模块，来学习运动信息
     </p>
     <p style="text-align:center;">
      <img alt="f11dc8e8e9708c67ee75f2fd5badd579.png" src="https://i-blog.csdnimg.cn/blog_migrate/faa8854fc6bd9cd12bb6e36df9b58b27.png"/>
     </p>
     <ul>
      <li>
       <h4>
        VideoCrafter1: Open Diffusion Models for High-Quality Video Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：腾讯 AI Lab
     </p>
     <p>
      时间：2023.10.30
     </p>
     <p>
      ﻿https://ailab-cvc.github.io/videocrafter﻿
     </p>
     <p>
      简单介绍：基于diffusion模型，网络架构采用空间和时序attention操作来实现视频生成
     </p>
     <p>
      <img alt="68bdbf6c9e459513e19e9754482e6eae.png" src="https://i-blog.csdnimg.cn/blog_migrate/5bb158a3a730cb5db7864faf0a37a43a.png"/>
     </p>
     <h5>
      <strong>
       ▐
      </strong>
      <strong>
      </strong>
      <strong>
       图生视频
      </strong>
     </h5>
     <h5>
     </h5>
     <ul>
      <li>
       <h4>
        AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning
       </h4>
      </li>
     </ul>
     <p>
      机构：上海 AI Lab
     </p>
     <p>
      时间：2023.7.11
     </p>
     <p>
      ﻿https://animatediff.github.io/
     </p>
     <ul>
      <li>
       <h4>
        VideoCrafter1: Open Diffusion Models for High-Quality Video Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：腾讯 AI Lab
     </p>
     <p>
      时间：2023.10.30
     </p>
     <p>
      ﻿https://ailab-cvc.github.io/videocrafter
     </p>
     <ul>
      <li>
       <h4>
        stable video diffusion
       </h4>
      </li>
     </ul>
     <p>
      机构：Stability AI
     </p>
     <p>
      时间：2023.11.21
     </p>
     <p>
      ﻿https://stability.ai/news/stable-video-diffusion-open-ai-video-model﻿
     </p>
     <p>
      简单介绍：基于SD2.1增加时序层，来进行视频生成
     </p>
     <ul>
      <li>
       <h4>
        AnimateZero: Video Diffusion Models are Zero-Shot Image Animators
       </h4>
      </li>
     </ul>
     <p>
      机构：腾讯 AI Lab
     </p>
     <p>
      时间：2023.12.6
     </p>
     <p>
      ﻿https://github.com/vvictoryuki/AnimateZero（未开源）
     </p>
     <p>
      简单介绍：基于Animate Diff增加了位置相关的attention
     </p>
     <p>
      <img alt="4385371a623e9c5918c3b5197cb74776.png" src="https://i-blog.csdnimg.cn/blog_migrate/1d86f150e224409a9670a45e51f3a504.png"/>
     </p>
     <ul>
      <li>
       <h4>
        AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance
       </h4>
      </li>
     </ul>
     <p>
      机构：阿里
     </p>
     <p>
      时间：2023.12.4
     </p>
     <p>
      ﻿https://animationai.github.io/AnimateAnything/﻿
     </p>
     <p>
      简单介绍：可以针对特定位置进行动态化，通过学习运动信息实现时序信息生成
     </p>
     <p>
      <img alt="cd349d295a04bbad29315c1e01971c05.png" src="https://i-blog.csdnimg.cn/blog_migrate/d51cb5900de130da7d4de7953e501083.png"/>
     </p>
     <ul>
      <li>
       <h4>
        LivePhoto: Real Image Animation with Text-guided Motion Control
       </h4>
      </li>
     </ul>
     <p>
      机构：阿里
     </p>
     <p>
      时间：2023.12.5
     </p>
     <p>
      ﻿https://xavierchen34.github.io/LivePhoto-Page/（未开源）
     </p>
     <p>
      简单介绍：将参考图，运动信息拼接作为输入，来进行图像的动态化
     </p>
     <p>
      <img alt="29beda70ab49845c758504fbd5143105.png" src="https://i-blog.csdnimg.cn/blog_migrate/eff76bcaf07bf399128bfc70c275c3bc.png"/>
     </p>
     <h5>
      <strong>
       ▐
      </strong>
      <strong>
      </strong>
      <strong>
       视频风格化
      </strong>
     </h5>
     <h5>
     </h5>
     <ul>
      <li>
       <h4>
        Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation
       </h4>
      </li>
     </ul>
     <p>
      机构：南洋理工
     </p>
     <p>
      时间：2023.12.17
     </p>
     <p>
      https://www.mmlab-ntu.com/project/rerender/﻿
     </p>
     <p>
      简单介绍：基于SD+controlnet，结合cros-frame attention来风格化视频序列
     </p>
     <p style="text-align:center;">
      <img alt="2151b085cd71ce3141e6b1560fbcde54.png" src="https://i-blog.csdnimg.cn/blog_migrate/6930a0d67936fe9be120b7b5b075af03.png"/>
     </p>
     <ul>
      <li>
       <h4>
        DCTNet
       </h4>
      </li>
     </ul>
     <p>
      机构：阿里达摩院
     </p>
     <p>
      时间：2022.7.6
     </p>
     <p>
      ﻿https://github.com/menyifang/DCT-Net/﻿
     </p>
     <p>
      简单介绍：基于GAN的框架做的视频风格化，目前支持7种不同的风格
     </p>
     <p>
      <img alt="a7fffd1e98db4db35b1ba95a0e9ec234.png" src="https://i-blog.csdnimg.cn/blog_migrate/4994f6bfa1c7ab7589167d97922c3eb6.png"/>
     </p>
     <h5>
      <strong>
       ▐
      </strong>
      <strong>
      </strong>
      <strong>
       视频编辑
      </strong>
     </h5>
     <h5>
     </h5>
     <p>
      主要是将深度图或者其他条件图（canny/hed），通过网络注入Diffusion model中，控制整体场景生成，并通过prompt设计来控制主体目标的外观。其中controlnet被迁移进入视频编辑领域，出现了一系列controlnetvideo的工作。
     </p>
     <ul>
      <li>
       <h4>
        Structure and Content-Guided Video Synthesis with Diffusion Models
       </h4>
      </li>
     </ul>
     <p>
      机构：Runway
     </p>
     <p>
      时间：2023.2.6
     </p>
     <p>
      ﻿https://research.runwayml.com/gen1
     </p>
     <p>
      <img alt="b37bc72dd60293313435c99edb409dd3.png" src="https://i-blog.csdnimg.cn/blog_migrate/b31e83876bb2a137816d306e68cda1ff.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Animate diff+ControlNet（基于WebUI API）
       </h4>
      </li>
     </ul>
     <ul>
      <li>
       <h4>
        Video-P2P: Video Editing with Cross-attention Control
       </h4>
      </li>
     </ul>
     <p>
      机构：港中文，adobe
     </p>
     <p>
      时间：2023.3.8
     </p>
     <p>
      ﻿https://video-p2p.github.io/
     </p>
     <p>
      <img alt="85fae37f95016438e57851810786471b.png" src="https://i-blog.csdnimg.cn/blog_migrate/3426e802a982ddce437f469abb9271f6.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Pix2Video: Video Editing using Image Diffusion
       </h4>
      </li>
     </ul>
     <p>
      机构：Abode
     </p>
     <p>
      时间：2023.3.22
     </p>
     <p>
      ﻿https://duyguceylan.github.io/pix2video.github.io/
     </p>
     <p>
      <img alt="1bd8bf1cc7e918ba10ba83fb7e600ff1.png" src="https://i-blog.csdnimg.cn/blog_migrate/29abf6e5ab9d6becd6aeedf531a3318b.png"/>
     </p>
     <ul>
      <li>
       <h4>
        InstructVid2Vid: Controllable Video Editing with Natural Language Instructions
       </h4>
      </li>
     </ul>
     <p>
      机构：浙大
     </p>
     <p>
      时间：2023.5.21
     </p>
     <p>
      <img alt="16abe2b75e0126c01702bb18ab544abd.png" src="https://i-blog.csdnimg.cn/blog_migrate/19ca681dd97618747576344606143bf4.png"/>
     </p>
     <ul>
      <li>
       <h4>
        ControlVideo: Training-free Controllable Text-to-Video Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：华为
     </p>
     <p>
      时间：2023.5.22
     </p>
     <p>
      ﻿https://github.com/YBYBZhang/ControlVideo
     </p>
     <p>
      <img alt="50d8e39632b3e1a0986f0a0475589392.png" src="https://i-blog.csdnimg.cn/blog_migrate/6a04f99bd2c9aee52ed63e2cd49f684d.png"/>
     </p>
     <ul>
      <li>
       <h4>
        ControlVideo: Conditional Control for One-shot Text-driven Video Editing and Beyond
       </h4>
      </li>
     </ul>
     <p>
      机构：清华
     </p>
     <p>
      时间：2023.11.28
     </p>
     <p>
      ﻿https://github.com/thu-ml/controlvideo
     </p>
     <p>
      <img alt="5453f3be4f045507f85066c4cca86500.png" src="https://i-blog.csdnimg.cn/blog_migrate/286b32ec3131f230334d8b71e2d9f06f.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models
       </h4>
      </li>
     </ul>
     <p>
      时间：2023.12.6
     </p>
     <p>
      ﻿https://controlavideo.github.io/
     </p>
     <p>
      <img alt="9aca6c877971a73243115938b090ce65.png" src="https://i-blog.csdnimg.cn/blog_migrate/c3c2754e4021c86c8cc76411d4440083.png"/>
     </p>
     <ul>
      <li>
       <h4>
        StableVideo: Text-driven Consistency-aware Diffusion Video Editing
       </h4>
      </li>
     </ul>
     <p>
      机构：MSRA
     </p>
     <p>
      时间：2023.8.18
     </p>
     <p>
      ﻿https://github.com/rese1f/StableVideo
     </p>
     <p>
      <img alt="efe59831eacf94ee49efaf098cd5df1d.png" src="https://i-blog.csdnimg.cn/blog_migrate/5af2535f41b05d64e2685b3d010e8b91.png"/>
     </p>
     <ul>
      <li>
       <h4>
        MagicEdit: High-Fidelity and Temporally Coherent Video Editing
       </h4>
      </li>
     </ul>
     <p>
      机构：字节
     </p>
     <p>
      时间：2023.8.28
     </p>
     <p>
      ﻿https://magic-edit.github.io/（未开源）
     </p>
     <p>
      <img alt="645434c6c0e5f3bcbcb14b6b28940fed.png" src="https://i-blog.csdnimg.cn/blog_migrate/8464e0bc6c20a52daee4d7540e20a3ca.png"/>
     </p>
     <ul>
      <li>
       <h4>
        GROUND-A-VIDEO: ZERO-SHOT GROUNDED VIDEO EDITING USING TEXT-TO-IMAGE DIFFUSION MODELS
       </h4>
      </li>
     </ul>
     <p>
      机构：KAIST
     </p>
     <p>
      时间：2023.10.2
     </p>
     <p>
      ﻿https://ground-a-video.github.io/
     </p>
     <p>
      <img alt="079ea47002f01d6a73f4ad57626dcce4.png" src="https://i-blog.csdnimg.cn/blog_migrate/296a8853296cf6269032c490724a8780.png"/>
     </p>
     <ul>
      <li>
       <h4>
        FateZero: Fusing Attentions for Zero-shot Text-based Video Editing
       </h4>
      </li>
     </ul>
     <p>
      机构：腾讯AI Lab
     </p>
     <p>
      时间：2023.10.11
     </p>
     <p>
      ﻿https://fate-zero-edit.github.io
     </p>
     <p>
      <img alt="6ad95f1fc0c6af8c42f97a7bf456ecdc.png" src="https://i-blog.csdnimg.cn/blog_migrate/4c932d81bfa06f8257b4954a9e3bede4.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Motion-Conditioned Image Animation for Video Editing
       </h4>
      </li>
     </ul>
     <p>
      机构：Meta
     </p>
     <p>
      时间：2023.11.30
     </p>
     <p>
      facebookresearch.github.io/MoCA（未开源）
     </p>
     <p>
      <img alt="af2a386e6a652c9584ab060d32b159e9.png" src="https://i-blog.csdnimg.cn/blog_migrate/0515ab26d86fe1ccfb5983579259c791.png"/>
     </p>
     <ul>
      <li>
       <h4>
        VidEdit: Zero-shot and Spatially Aware Text-driven Video Editing
       </h4>
      </li>
     </ul>
     <p>
      机构：Sorbonne Université, Paris, France
     </p>
     <p>
      时间：2023.12.15
     </p>
     <p>
      ﻿https://videdit.github.io
     </p>
     <p>
      <img alt="ee91ce97234473fa0057da1c90c8f023.png" src="https://i-blog.csdnimg.cn/blog_migrate/6eb6258730b24abcebfad8c7770d4c7b.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models
       </h4>
      </li>
     </ul>
     <p>
      时间：2024.1.4
     </p>
     <p>
      ﻿https://github.com/baaivision/vid2vid-zero
     </p>
     <p>
      <img alt="433ee0231dc975bcb213f54c7d86831b.png" src="https://i-blog.csdnimg.cn/blog_migrate/f2e3db95bb8c299d2c99b4f4c39b91b5.png"/>
     </p>
     <h5>
      <strong>
       ▐
      </strong>
      <strong>
      </strong>
      <strong>
       人物动态化
      </strong>
     </h5>
     <h5>
     </h5>
     <p>
      主要是通过人体姿态作为条件性输入（结合controlnet等），将一张图作为前置参考图，或者直接使用文本描述生成图片。其中阿里和字节分别有几篇代表性论文，其中字节的代码有两篇已经开源，阿里的代码还在等待阶段。
     </p>
     <ul>
      <li>
       <p>
        Follow Your Pose
       </p>
      </li>
     </ul>
     <p>
      机构：腾讯AI Lab
     </p>
     <p>
      时间：2023.4.3
     </p>
     <p>
      ﻿https://follow-your-pose.github.io/
     </p>
     <p>
      <img alt="53410d0779eb44d606e3ee6749023bbd.png" src="https://i-blog.csdnimg.cn/blog_migrate/1767ddf48240c339d4f75070dc58b199.png"/>
     </p>
     <ul>
      <li>
       <h4>
        DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion
       </h4>
      </li>
     </ul>
     <p>
      机构：google，nvidia
     </p>
     <p>
      时间：2023.5.4
     </p>
     <p>
      ﻿https://grail.cs.washington.edu/projects/dreampose/
     </p>
     <p>
      <img alt="eb02a1d216df570e3f4b5e8504392c4b.png" src="https://i-blog.csdnimg.cn/blog_migrate/08b34ee22941eadecb3d73e762681c3d.png"/>
     </p>
     <ul>
      <li>
       <h4>
        DISCO: Disentangled Control for Realistic Human Dance Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：微软
     </p>
     <p>
      时间：2023.10.11
     </p>
     <p>
      ﻿https://disco-dance.github.io
     </p>
     <p>
      <img alt="e35df2720df43665594bc4c9b390e3f2.png" src="https://i-blog.csdnimg.cn/blog_migrate/ae694c1dc7b7bea243b100f8c5484703.png"/>
     </p>
     <ul>
      <li>
       <h4>
        MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model
       </h4>
      </li>
     </ul>
     <p>
      机构：字节
      <br/>
     </p>
     <p>
      时间：2023.11.27
     </p>
     <p>
      ﻿https://showlab.github.io/magicanimate/
     </p>
     <p>
      <img alt="47bfeac42699e27498cad77ac27be36e.png" src="https://i-blog.csdnimg.cn/blog_migrate/0c6dc0a00018bdb6340a65006a6479fa.png"/>
     </p>
     <ul>
      <li>
       <h4>
        MaigcDance
       </h4>
      </li>
     </ul>
     <p>
      机构：字节
     </p>
     <p>
      时间：2023.11.18
     </p>
     <p>
      ﻿https://boese0601.github.io/magicdance/
     </p>
     <p>
      <img alt="57e6ce3b7bb06cb0870f41cc1daebc0e.png" src="https://i-blog.csdnimg.cn/blog_migrate/0d0713775345affaff1017adf8c1d92a.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation
       </h4>
      </li>
     </ul>
     <p>
      机构：阿里
     </p>
     <p>
      时间：2023.12.7
     </p>
     <p>
      ﻿https://humanaigc.github.io/animate-anyone/（未开源）
     </p>
     <p>
      <img alt="56891634766a32c976bb74a4615436fb.png" src="https://i-blog.csdnimg.cn/blog_migrate/edee4cc4153f1ba2edcf7f08876297e9.png"/>
     </p>
     <ul>
      <li>
       <h4>
        DreaMoving: A Human Video Generation Framework based on Diffusion Model
       </h4>
      </li>
     </ul>
     <p>
      机构：阿里
     </p>
     <p>
      时间：2023.12.11
     </p>
     <p>
      ﻿https://dreamoving.github.io/dreamoving（未开源）
     </p>
     <p>
      <img alt="ee81f94b764c3424437942769027d1a5.png" src="https://i-blog.csdnimg.cn/blog_migrate/6901b38b13c529e3cfdadb9693355c48.png"/>
     </p>
     <h5>
      <strong>
       ▐
      </strong>
      <strong>
      </strong>
      <strong>
       长视频生成
      </strong>
     </h5>
     <ul>
      <li>
       <h4>
        NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：微软亚研院
     </p>
     <p>
      时间：2023.3.22
     </p>
     <p>
      ﻿https://msra-nuwa.azurewebsites.net/
     </p>
     <p>
      <img alt="06b0ed7cde1049fe9132352b2a35c980.png" src="https://i-blog.csdnimg.cn/blog_migrate/fa81ae13350dd395ddd0405865797442.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Latent Video Diffusion Models for High-Fidelity Long Video Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：腾讯AI Lab
     </p>
     <p>
      时间：2023.3.20
     </p>
     <p>
      <img alt="f86e02227936df180de817055443b3eb.png" src="https://i-blog.csdnimg.cn/blog_migrate/090eb19521980af087b53fda18dfd647.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising
       </h4>
      </li>
     </ul>
     <p>
      机构：上海AI Lab
     </p>
     <p>
      时间：2023.3.29
     </p>
     <p>
      https://github.com/G-U-N/Gen-L-Video
     </p>
     <p>
      <img alt="74e1dfdb95803238ba93e8c94fa31855.png" src="https://i-blog.csdnimg.cn/blog_migrate/488953a5ec136684b13113874c793aa7.png"/>
     </p>
     <ul>
      <li>
       <h4>
        Sora（OpenAI）
       </h4>
      </li>
     </ul>
     <p>
      时间：2024.2
     </p>
     <p>
      ﻿https://openai.com/sora（未开源）
     </p>
     <p>
      <img alt="0a7c57fd44a5b62b9df8b3eba4522070.png" src="https://i-blog.csdnimg.cn/blog_migrate/58fdf3cfcc8276a229db282f96542f5e.png"/>
     </p>
     <h4>
      <br/>
     </h4>
     <ul>
      <li>
       <h4>
        Latte: Latent Diffusion Transformer for Video Generation
       </h4>
      </li>
     </ul>
     <p>
      机构：上海AI Lab
     </p>
     <p>
      时间：2024.1.5
     </p>
     <p>
      ﻿https://maxin-cn.github.io/latte_project
     </p>
     <p>
      <img alt="a9bcfe8b09c4cbde19fe9e86ef986764.png" src="https://i-blog.csdnimg.cn/blog_migrate/eda4a425445f8df54dd9f124c4c6ba79.png"/>
     </p>
     <p style="text-align:center;">
      <img alt="18912a6cd1cc465065654094c7595747.png" src="https://i-blog.csdnimg.cn/blog_migrate/4dfb6972705b4af6cc4032e3f9787e1c.png"/>
     </p>
     <p style="text-align:center;">
      算法效果分析
     </p>
     <p>
      本章节选择了一些代表性的方法进行效果分析
      <br/>
     </p>
     <ul>
      <li>
       <h3>
        Animate Diff
       </h3>
      </li>
     </ul>
     <p>
      效果：https://animatediff.github.io/，支持文生视频，图生视频，以及和controlnet结合做视频编辑
     </p>
     <h3>
      <br/>
     </h3>
     <ul>
      <li>
       <h3>
        Animate Diff+ControlNet
       </h3>
      </li>
     </ul>
     <p>
      输入视频：moonwalk.mp4﻿
     </p>
     <p>
      输出样例
     </p>
     <p>
      <img alt="2b2a430fcb3abec184ddadc648f26b72.gif" src="https://i-blog.csdnimg.cn/blog_migrate/e3e92d5a47fe2007b93c8dda1016d77f.gif"/>
     </p>
     <p>
      <img alt="462e4cace92e179decf532fbbed0c6f0.gif" src="https://i-blog.csdnimg.cn/blog_migrate/72966b67157f756ddf760b82db604ec1.gif"/>
     </p>
     <p>
      <img alt="d712dd0df11fc76b47de52a349f166e7.gif" src="https://i-blog.csdnimg.cn/blog_migrate/24b54990c105c636d042bffb9376b7b6.gif"/>
     </p>
     <p>
      <img alt="344165979bde887beb7d711d8da1584a.gif" src="https://i-blog.csdnimg.cn/blog_migrate/7c8c28e4f9d45f9c5129b479317a5f3e.gif"/>
     </p>
     <p style="text-align:center;">
      canny和openpose
     </p>
     <p>
      注意：要输入主语保证主体一致性（比如michael jackson或者a boy）
      <br/>
     </p>
     <ul>
      <li>
       <h3>
        AnimateAnything
       </h3>
      </li>
     </ul>
     <p>
      能够指定图片的运动区域，根据文本进行图片的动态化
     </p>
     <p>
      效果：
     </p>
     <ul>
      <li>
       <h3>
        Stable Video Diffusion
       </h3>
      </li>
     </ul>
     <p>
      能够基于静止图片生成25帧的序列(576x1024)
     </p>
     <p>
      效果：
     </p>
     <p style="text-align:center;">
      <img alt="20af25f6e0877cf382440a45b391c583.gif" src="https://i-blog.csdnimg.cn/blog_migrate/d3ab93229f8afd0b9c8118101b93bc17.gif"/>
     </p>
     <h3>
      ControlVideo
     </h3>
     <p>
      输入+输出样例：500.mp4，300.mp4，整体效果不错
     </p>
     <p>
      问题：因为推理过程需要额外的训练，消耗时间久，第一个视频需要50min（32帧），第二个视频需要14min（8帧）
     </p>
     <p style="text-align:center;">
      300
      <br/>
     </p>
     <p style="text-align:center;">
      500
     </p>
     <ul>
      <li>
       <h3>
        Rerender A Video
       </h3>
      </li>
     </ul>
     <p>
      输入：﻿
     </p>
     <p>
      输出：﻿
     </p>
     <p>
      输入：
     </p>
     <p>
      输出：
     </p>
     <p>
      整体效果还可以，运行速度和视频帧数有关，10s视频大约在20min左右。
     </p>
     <ul>
      <li>
       <h3>
        DCTNet
       </h3>
      </li>
     </ul>
     <p>
      效果：整体画面稳定，支持7种风格，显存要求低（6-7G），上面视频40s左右就可以处理完
     </p>
     <ul>
      <li>
       <h3>
        DreamPose
       </h3>
      </li>
     </ul>
     <p>
      <img alt="6c056b76117e5481cab3e8203c1014c8.png" src="https://i-blog.csdnimg.cn/blog_migrate/73835f26d49db3a3518d0ee3792b9ae9.png"/>
      <br/>
     </p>
     <ul>
      <li>
       <h3>
        Animate Anyone
       </h3>
      </li>
     </ul>
     <ul>
      <li>
       <h3>
        MagicDance
       </h3>
      </li>
     </ul>
     <p>
      输入图片：
     </p>
     <p style="text-align:center;">
      <img alt="0a71ce03b5c41156ebc88587daeeb682.png" src="https://i-blog.csdnimg.cn/blog_migrate/40aadc7401252ac92a173830aafb2d46.png"/>
     </p>
     <p>
      输出：
      <br/>
     </p>
     <p style="text-align:center;">
      <img alt="40b047b225b2aeba50c7101fd6d4be05.gif" src="https://i-blog.csdnimg.cn/blog_migrate/fda8acadb17ba86ea9f86a4dc5d40b63.png"/>
     </p>
     <p style="text-align:center;">
      <img alt="ba35c54aa8893835382f46a6d759f1c6.gif" src="https://i-blog.csdnimg.cn/blog_migrate/5e2d11b605c97448638a152a1922b842.png"/>
     </p>
     <ul>
      <li>
       <h3>
        Sora
       </h3>
      </li>
     </ul>
     <p>
      效果：https://openai.com/sora﻿
     </p>
     <p>
      能够生成长视频，质量很好，但是尚未开源
     </p>
     <p style="text-align:center;">
      <img alt="616dea105688ab302a2e9f5218297496.png" src="https://i-blog.csdnimg.cn/blog_migrate/6826fcc71253c953749c67c117f429d5.png"/>
     </p>
     <p style="text-align:center;">
      总结和展望
     </p>
     <ol>
      <li>
       <p>
        文生视频和图生视频算法：其中Animate Diff，VideoCrafter等已经开源，支持文/图生成视频，并且经过测试效果还不错，同时图生视频还支持通过结合不同的base模型实现视频的风格化。不过生成的视频帧数基本都在2s以内，可以作为动图的形式进行展示。其中Stable Video Diffusion是stability ai开源的一个图生视频的算法，效果相对更加逼真，视频质量更高，但是视频长度依旧很短。
       </p>
      </li>
      <li>
       <p>
        视频编辑算法：比如基于controlnet的可控生成视频可以初步达到预期的效果，支持实现特定目标或者属性（颜色等）的更换，也支持人物的换装（比如颜色描述）等等，其中生成的视频长度和GPU显存相关。
       </p>
      </li>
      <li>
       <p>
        视频风格化：基于diffusion 模型的视频风格化效果最好的是rerender a video，可以支持prompt描述来进行视频的风格化，整体来讲这个方法对人脸和自然环境有比较好的效果，运行成本也相对较低（相较于视频编辑算法）
       </p>
      </li>
      <li>
       <p>
        特定的人物动态化算法：目前demo效果最好的animate anyone和dream moving都还没有开源。不过这两个算法都对外开放了使用接口，比如通义千问app以及modelscope平台。重点介绍一下通义实验室的Dream moving，https://www.modelscope.cn/studios/vigen/video_generation/summary是其开放的使用平台，里面支持同款的动作生成，图生视频，视频的风格化以及视频贺卡等功能，整体来讲效果很好。而目前开源的方法中，测试的效果最好的是MagicDance，但是人脸有一定的模糊，距离animate anyone和dream moving展示的效果还有差距。
       </p>
      </li>
      <li>
       <p>
        长视频算法：随着Sora的出现，Diffusion Transformer的架构后续会备受关注，目前大部分算法都局限于2s左右的短视频生成，而且质量上不如Sora。后续会有更多的算法将Sora的思路融入现有的方法中，不断提升视频质量和视频长度。不过目前sora的模型和实现细节并没有在技术报告中公开，因此在未来还会有一段的摸索路要走。
       </p>
      </li>
      <li>
       <p>
        ﻿整体总结：
       </p>
      </li>
     </ol>
     <table width="950">
      <colgroup>
       <col width="144"/>
       <col width="144"/>
       <col width="144"/>
       <col width="144"/>
       <col width="149"/>
       <col width="225"/>
      </colgroup>
      <tbody>
       <tr>
        <td colspan="1" rowspan="1">
         <br/>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          是否可用
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          优势
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          劣势
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          适用场景
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          代表性方法
         </p>
        </td>
       </tr>
       <tr>
        <td colspan="1" rowspan="1">
         <p>
          文/图生视频
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          是
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          视频质量高
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          视频长度短
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          短视频动态封面
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          Animate Diff（可扩展性强）
         </p>
         <p>
          VideoCrafter（质量较好）
         </p>
         <p>
          Stable Video Diffusion（质量更好）
         </p>
        </td>
       </tr>
       <tr>
        <td colspan="1" rowspan="1">
         <p>
          视频编辑算法
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          待定
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          算法种类多，可实现的功能多（修改任意目标的属性）
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          推理速度较慢，显存要求高，视频长度短
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          人物换装（最简单的改变衣服颜色），目标编辑，用户体验
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          ControlVideo（效果好但运行时间久）
         </p>
        </td>
       </tr>
       <tr>
        <td colspan="1" rowspan="1">
         <p>
          视频风格化
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          是
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          显存要求相对视频编辑更低，推理速度更快。
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          画面存在一定的不稳定问题。但是基于GAN的DCTNet相对更稳定
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          用户体验
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          Rerender-A-Video（更灵活）
         </p>
         <p>
          DCTNet（效果更稳定）
         </p>
        </td>
       </tr>
       <tr>
        <td colspan="1" rowspan="1">
         <p>
          人物动态化
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          待定
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          用户可玩性高
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          效果最好的代码暂时没有开源，开源的代码生成的人脸会有一定的模糊
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          用户体验
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          Animate Anyone（待开源）
         </p>
         <p>
          DreamMoving（待开源）
         </p>
         <p>
          MagicDance（已开源）
         </p>
        </td>
       </tr>
       <tr>
        <td colspan="1" rowspan="1">
         <p>
          长视频生成
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          否
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          视频长度远超2s
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          整体质量偏差
         </p>
         <p>
          （Sora还没开源）
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          影视制作
         </p>
        </td>
        <td colspan="1" rowspan="1">
         <p>
          Sora
         </p>
        </td>
       </tr>
      </tbody>
     </table>
     <p>
      ﻿
     </p>
     <p style="text-align:center;">
      <strong>
       ¤
      </strong>
      <strong>
       拓展阅读
      </strong>
      <strong>
       ¤
      </strong>
     </p>
     <p style="text-align:center;">
      <a href="" rel="nofollow">
       3DXR技术
      </a>
      |
      <a href="" rel="nofollow">
       终端技术
      </a>
      |
      <a href="" rel="nofollow">
       音视频技术
      </a>
     </p>
     <p style="text-align:center;">
      <a href="" rel="nofollow">
       服务端技术
      </a>
      |
      <a href="" rel="nofollow">
       技术质量
      </a>
      |
      <a href="" rel="nofollow">
       数据算法
      </a>
     </p>
    </div>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f54616f62616f6a697368752f:61727469636c652f64657461696c732f313337303337323934" class_="artid" style="display:none">
 </p>
</div>
