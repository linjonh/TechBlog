---
layout: post
title: 2023-03-03-YOLO系列YOLOv5超详细解读源码详解入门实践改进
date: 2023-03-03 11:47:09 +0800
categories: ['目标检测论文', 'Yolov']
tags: ['Yolo', '深度学习', '人工智能', '计算机视觉', '目标检测']
image:
  path: https://img-blog.csdnimg.cn/d6a153ab12e54987b4c41df440b1a382.gif?x-oss-process=image/resize,m_fixed,h_150
  alt: YOLO系列YOLOv5超详细解读源码详解入门实践改进
artid: 129312409
render_with_liquid: false
---
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     【YOLO系列】YOLOv5超详细解读（源码详解＋入门实践＋改进）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <h4 id="%F0%9F%8C%9F%E6%9C%AC%E4%BA%BAYOLOv5%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/60da1050eccf2c125011c7d0de9fabc4.gif"/>
    </h4>
    <h2 id="%E2%80%8B%E7%BC%96%E8%BE%91" style="text-align:center;">
     <img alt="" height="422" src="https://i-blog.csdnimg.cn/blog_migrate/4c98cb6cd208d9e3bdde823e296c3195.gif" width="750"/>
    </h2>
    <h2 id="%E5%89%8D%E8%A8%80">
     前言
    </h2>
    <p>
     吼吼！终于来到了YOLOv5啦！
    </p>
    <p>
     首先，一个热知识：YOLOv5没有发表正式论文哦~
    </p>
    <p>
     为什么呢？可能YOLOv5项目的作者Glenn Jocher还在吃帽子吧，hh
    </p>
    <p style="text-align:center;">
     <img alt="" height="386" src="https://i-blog.csdnimg.cn/blog_migrate/625a17a246004984a99d81060fe8f9fd.png" width="610"/>
    </p>
    <hr/>
    <h2>
     <strong>
      目录
     </strong>
    </h2>
    <p id="%E2%80%8B%E7%BC%96%E8%BE%91-toc" style="margin-left:0px;">
     <a href="#%E5%89%8D%E8%A8%80" rel="nofollow">
      前言
     </a>
    </p>
    <p id="%E4%B8%80%E3%80%81YOLOv5%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:0px;">
     <a href="#%E4%B8%80%E3%80%81YOLOv5%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" rel="nofollow">
      一、YOLOv5的网络结构
     </a>
    </p>
    <p id="%C2%A0%E4%BA%8C%E3%80%81%E8%BE%93%E5%85%A5%E7%AB%AF-toc" style="margin-left:0px;">
     <a href="#%C2%A0%E4%BA%8C%E3%80%81%E8%BE%93%E5%85%A5%E7%AB%AF" rel="nofollow">
      二、输入端
     </a>
    </p>
    <p id="%EF%BC%881%EF%BC%89Mosaic%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-toc" style="margin-left:40px;">
     <a href="#%EF%BC%881%EF%BC%89Mosaic%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA" rel="nofollow">
      （1）Mosaic数据增强
     </a>
    </p>
    <p id="%C2%A0%EF%BC%882%EF%BC%89%E8%87%AA%E9%80%82%E5%BA%94%E9%94%9A%E6%A1%86%E8%AE%A1%E7%AE%97-toc" style="margin-left:40px;">
     <a href="#%C2%A0%EF%BC%882%EF%BC%89%E8%87%AA%E9%80%82%E5%BA%94%E9%94%9A%E6%A1%86%E8%AE%A1%E7%AE%97" rel="nofollow">
      （2）自适应锚框计算
     </a>
    </p>
    <p id="%EF%BC%883%EF%BC%89%E8%87%AA%E9%80%82%E5%BA%94%E5%9B%BE%E7%89%87%E7%BC%A9%E6%94%BE-toc" style="margin-left:40px;">
     <a href="#%EF%BC%883%EF%BC%89%E8%87%AA%E9%80%82%E5%BA%94%E5%9B%BE%E7%89%87%E7%BC%A9%E6%94%BE" rel="nofollow">
      （3）自适应图片缩放
     </a>
    </p>
    <p id="%E4%B8%89%E3%80%81Backbone-toc" style="margin-left:0px;">
     <a href="#%E4%B8%89%E3%80%81Backbone" rel="nofollow">
      三、Backbone
     </a>
    </p>
    <p id="%EF%BC%881%EF%BC%89Focus%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;">
     <a href="#%EF%BC%881%EF%BC%89Focus%E7%BB%93%E6%9E%84" rel="nofollow">
      （1）Focus结构
     </a>
    </p>
    <p id="%C2%A0%EF%BC%882%EF%BC%89CSP%E7%BB%93%E6%9E%84-toc" style="margin-left:40px;">
     <a href="#%C2%A0%EF%BC%882%EF%BC%89CSP%E7%BB%93%E6%9E%84" rel="nofollow">
      （2）CSP结构
     </a>
    </p>
    <p id="%E5%9B%9B%E3%80%81Neck-toc" style="margin-left:0px;">
     <a href="#%E5%9B%9B%E3%80%81Neck" rel="nofollow">
      四、Neck
     </a>
    </p>
    <p id="%C2%A0%C2%A0%E4%BA%94%E3%80%81Head-toc" style="margin-left:0px;">
     <a href="#%C2%A0%C2%A0%E4%BA%94%E3%80%81Head" rel="nofollow">
      五、Head
     </a>
    </p>
    <p id="%EF%BC%881%EF%BC%89Bounding%20box%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-toc" style="margin-left:40px;">
     <a href="#%EF%BC%881%EF%BC%89Bounding%20box%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" rel="nofollow">
      （1）Bounding box损失函数
     </a>
    </p>
    <p id="%EF%BC%882%EF%BC%89NMS%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6-toc" style="margin-left:40px;">
     <a href="#%EF%BC%882%EF%BC%89NMS%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6" rel="nofollow">
      （2）NMS非极大值抑制
     </a>
    </p>
    <p id="%C2%A0%E5%85%AD%E3%80%81%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5-toc" style="margin-left:0px;">
     <a href="#%C2%A0%E5%85%AD%E3%80%81%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5" rel="nofollow">
      六、训练策略
     </a>
    </p>
    <p class="img-center">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/36cf0a9cad26c2d9feca98ec09c546d6.gif"/>
    </p>
    <p>
     <span style="color:#fe2c24;">
      <strong>
       【写论文必看】
      </strong>
     </span>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/133617849?spm=1001.2014.3001.5501" title="深度学习纯小白如何从零开始写第一篇论文？看完这篇豁然开朗！-CSDN博客">
      深度学习纯小白如何从零开始写第一篇论文？看完这篇豁然开朗！-CSDN博客
     </a>
    </p>
    <p>
     <strong>
      前期回顾：
     </strong>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129248856?spm=1001.2014.3001.5501" title="【YOLO系列】YOLOv4论文超详细解读2（网络详解）">
      【YOLO系列】YOLOv4论文超详细解读2（网络详解）
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129232468?spm=1001.2014.3001.5501" title="【YOLO系列】YOLOv4论文超详细解读1（翻译 ＋学习笔记）">
      【YOLO系列】YOLOv4论文超详细解读1（翻译 ＋学习笔记）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129143961?spm=1001.2014.3001.5501" title="​​​​​​【YOLO系列】YOLOv3论文超详细解读（翻译 ＋学习笔记）">
      ​​​​​​【YOLO系列】YOLOv3论文超详细解读（翻译 ＋学习笔记）
     </a>
     <br/>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129087464?spm=1001.2014.3001.5501" title="【YOLO系列】YOLOv2论文超详细解读（翻译 ＋学习笔记）">
      【YOLO系列】YOLOv2论文超详细解读（翻译 ＋学习笔记）
     </a>
     <br/>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129011644?spm=1001.2014.3001.5501" title="【YOLO系列】YOLOv1论文超详细解读（翻译 ＋学习笔记）">
      【YOLO系列】YOLOv1论文超详细解读（翻译 ＋学习笔记）
     </a>
    </p>
    <p>
     <img alt="" height="49" src="https://i-blog.csdnimg.cn/blog_migrate/9d66c6b496add6f9fddd9ce463aca2e1.gif" width="49">
      🍀
      <strong>
       本人YOLOv5源码详解系列：
      </strong>
     </img>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129356033?spm=1001.2014.3001.5501" title="YOLOv5源码逐行超详细注释与解读（1）——项目目录结构解析">
      YOLOv5源码逐行超详细注释与解读（1）——项目目录结构解析
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129349094?spm=1001.2014.3001.5501" title="YOLOv5源码逐行超详细注释与解读（2）——推理部分detect.py">
      YOLOv5源码逐行超详细注释与解读（2）——推理部分detect.py
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129460666?spm=1001.2014.3001.5501" title="YOLOv5源码逐行超详细注释与解读（3）——训练部分train.py">
      YOLOv5源码逐行超详细注释与解读（3）——训练部分train.py
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129649553?spm=1001.2014.3001.5501" title="YOLOv5源码逐行超详细注释与解读（4）——验证部分val（test）.py">
      YOLOv5源码逐行超详细注释与解读（4）——验证部分val（test）.py
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129697521?spm=1001.2014.3001.5501" title="YOLOv5源码逐行超详细注释与解读（5）——配置文件yolov5s.yaml">
      YOLOv5源码逐行超详细注释与解读（5）——配置文件yolov5s.yaml
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129803802?spm=1001.2014.3001.5501" title="YOLOv5源码逐行超详细注释与解读（6）——网络结构（1）yolo.py">
      YOLOv5源码逐行超详细注释与解读（6）——网络结构（1）yolo.py
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129854764" title="YOLOv5源码逐行超详细注释与解读（7）——网络结构（2）common.py">
      YOLOv5源码逐行超详细注释与解读（7）——网络结构（2）common.py
     </a>
    </p>
    <p>
     <img alt="" height="49" src="https://i-blog.csdnimg.cn/blog_migrate/9d66c6b496add6f9fddd9ce463aca2e1.gif" width="49">
      🌟
      <strong>
       本人YOLOv5入门实践系列：
      </strong>
     </img>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129981848?spm=1001.2014.3001.5501" title="YOLOv5入门实践（1）——手把手带你环境配置搭建">
      YOLOv5入门实践（1）——手把手带你环境配置搭建
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129995604?spm=1001.2014.3001.5501" title="YOLOv5入门实践（2）——手把手教你利用labelimg标注数据集">
      YOLOv5入门实践（2）——手把手教你利用labelimg标注数据集
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130025866?spm=1001.2014.3001.5501" title="YOLOv5入门实践（3）——手把手教你划分自己的数据集">
      YOLOv5入门实践（3）——手把手教你划分自己的数据集
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130043351?spm=1001.2014.3001.5501" title="YOLOv5入门实践（4）——手把手教你训练自己的数据集">
      YOLOv5入门实践（4）——手把手教你训练自己的数据集
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130044342?spm=1001.2014.3001.5501" title="YOLOv5入门实践（5）——从零开始，手把手教你训练自己的目标检测模型（包含pyqt5界面）">
      YOLOv5入门实践（5）——从零开始，手把手教你训练自己的目标检测模型（包含pyqt5界面）
     </a>
    </p>
    <p>
     <img alt="" height="49" src="https://i-blog.csdnimg.cn/blog_migrate/9d66c6b496add6f9fddd9ce463aca2e1.gif" width="49">
      🌟
      <strong>
       本人YOLOv5改进系列：
      </strong>
     </img>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130564848?spm=1001.2014.3001.5501" title="YOLOv5改进系列（0）——重要性能指标与训练结果评价及分析">
      YOLOv5改进系列（0）——重要性能指标与训练结果评价及分析
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130551913?spm=1001.2014.3001.5501" title="YOLOv5改进系列（1）——添加SE注意力机制">
      YOLOv5改进系列（1）——添加SE注意力机制
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130587102?spm=1001.2014.3001.5501" title="YOLOv5改进系列（2）——添加CBAM注意力机制">
      YOLOv5改进系列（2）——添加CBAM注意力机制
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130619604?spm=1001.2014.3001.5501" title="YOLOv5改进系列（3）——添加CA注意力机制">
      YOLOv5改进系列（3）——添加CA注意力机制
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130641318?spm=1001.2014.3001.5501" title="YOLOv5改进系列（4）——添加ECA注意力机制">
      YOLOv5改进系列（4）——添加ECA注意力机制
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/130832933?spm=1001.2014.3001.5501" title="YOLOv5改进系列（5）——替换主干网络之 MobileNetV3">
      YOLOv5改进系列（5）——替换主干网络之 MobileNetV3
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131008642?spm=1001.2014.3001.5501" title="YOLOv5改进系列（6）——替换主干网络之 ShuffleNetV2">
      YOLOv5改进系列（6）——替换主干网络之 ShuffleNetV2
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131031541?spm=1001.2014.3001.5501" title="YOLOv5改进系列（7）——添加SimAM注意力机制">
      YOLOv5改进系列（7）——添加SimAM注意力机制
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131053284?spm=1001.2014.3001.5501" title="YOLOv5改进系列（8）——添加SOCA注意力机制">
      YOLOv5改进系列（8）——添加SOCA注意力机制
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131207097?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22131207097%22%2C%22source%22%3A%22weixin_43334693%22%7D" title="YOLOv5改进系列（9）——替换主干网络之EfficientNetv2">
      YOLOv5改进系列（9）——替换主干网络之EfficientNetv2
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131235113?spm=1001.2014.3001.5501" title="​​​​​​YOLOv5改进系列（10）——替换主干网络之GhostNet">
      ​​​​​​YOLOv5改进系列（10）——替换主干网络之GhostNet
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131350224?spm=1001.2014.3001.5501" title="YOLOv5改进系列（11）——添加损失函数之EIoU、AlphaIoU、SIoU、WIoU">
      YOLOv5改进系列（11）——添加损失函数之EIoU、AlphaIoU、SIoU、WIoU
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131461294?spm=1001.2014.3001.5501" title="YOLOv5改进系列（12）——更换Neck之BiFPN">
      YOLOv5改进系列（12）——更换Neck之BiFPN
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131513850?spm=1001.2014.3001.5501" title="YOLOv5改进系列（13）——更换激活函数之SiLU，ReLU，ELU，Hardswish，Mish，Softplus，AconC系列等">
      YOLOv5改进系列（13）——更换激活函数之SiLU，ReLU，ELU，Hardswish，Mish，Softplus，AconC系列等
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131552028?spm=1001.2014.3001.5501" title="YOLOv5改进系列（14）——更换NMS（非极大抑制）之 DIoU-NMS、CIoU-NMS、EIoU-NMS、GIoU-NMS 、SIoU-NMS、Soft-NMS">
      YOLOv5改进系列（14）——更换NMS（非极大抑制）之 DIoU-NMS、CIoU-NMS、EIoU-NMS、GIoU-NMS 、SIoU-NMS、Soft-NMS
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131613721?spm=1001.2014.3001.5501" title="YOLOv5改进系列（15）——增加小目标检测层">
      YOLOv5改进系列（15）——增加小目标检测层
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131973273?spm=1001.2014.3001.5501" title="YOLOv5改进系列（16）——添加EMA注意力机制（ICASSP2023|实测涨点）">
      YOLOv5改进系列（16）——添加EMA注意力机制（ICASSP2023|实测涨点）
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/131999141?spm=1001.2014.3001.5501" title="YOLOv5改进系列（17）——更换IoU之MPDIoU（ELSEVIER 2023|超越WIoU、EIoU等|实测涨点）">
      YOLOv5改进系列（17）——更换IoU之MPDIoU（ELSEVIER 2023|超越WIoU、EIoU等|实测涨点）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/132070079?spm=1001.2014.3001.5501" title="YOLOv5改进系列（18）——更换Neck之AFPN（全新渐进特征金字塔|超越PAFPN|实测涨点）">
      YOLOv5改进系列（18）——更换Neck之AFPN（全新渐进特征金字塔|超越PAFPN|实测涨点）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/132161488?spm=1001.2014.3001.5501" title="YOLOv5改进系列（19）——替换主干网络之Swin TransformerV1（参数量更小的ViT模型）">
      YOLOv5改进系列（19）——替换主干网络之Swin TransformerV1（参数量更小的ViT模型）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/132203200?spm=1001.2014.3001.5501" title="YOLOv5改进系列（20）——添加BiFormer注意力机制（CVPR2023|小目标涨点神器）">
      YOLOv5改进系列（20）——添加BiFormer注意力机制（CVPR2023|小目标涨点神器）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/132211831?spm=1001.2014.3001.5501" title="YOLOv5改进系列（21）——替换主干网络之RepViT（清华 ICCV 2023|最新开源移动端ViT）">
      YOLOv5改进系列（21）——替换主干网络之RepViT（清华 ICCV 2023|最新开源移动端ViT）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/132367429" title="YOLOv5改进系列（22）——替换主干网络之MobileViTv1（一种轻量级的、通用的移动设备 ViT）">
      YOLOv5改进系列（22）——替换主干网络之MobileViTv1（一种轻量级的、通用的移动设备 ViT）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/132428203?spm=1001.2014.3001.5502" title="YOLOv5改进系列（23）——替换主干网络之MobileViTv2（移动视觉 Transformer 的高效可分离自注意力机制）">
      YOLOv5改进系列（23）——替换主干网络之MobileViTv2（移动视觉 Transformer 的高效可分离自注意力机制）
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/133199471?spm=1001.2014.3001.5502" title="YOLOv5改进系列（24）——替换主干网络之MobileViTv3（移动端轻量化网络的进一步升级）">
      YOLOv5改进系列（24）——替换主干网络之MobileViTv3（移动端轻量化网络的进一步升级）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/135510571" title="YOLOv5改进系列（25）——添加LSKNet注意力机制（大选择性卷积核的领域首次探索）">
      YOLOv5改进系列（25）——添加LSKNet注意力机制（大选择性卷积核的领域首次探索）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/135562865?spm=1001.2014.3001.5502" title="YOLOv5改进系列（26）——添加RFAConv注意力卷积（感受野注意力卷积运算）">
      YOLOv5改进系列（26）——添加RFAConv注意力卷积（感受野注意力卷积运算）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/135610505?spm=1001.2014.3001.5502" title="YOLOv5改进系列（27）——添加SCConv注意力卷积（CVPR 2023|即插即用的高效卷积模块）">
      YOLOv5改进系列（27）——添加SCConv注意力卷积（CVPR 2023|即插即用的高效卷积模块）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/135758781?spm=1001.2014.3001.5502" title="YOLOv5改进系列（28）——添加DSConv注意力卷积（ICCV 2023|用于管状结构分割的动态蛇形卷积）">
      YOLOv5改进系列（28）——添加DSConv注意力卷积（ICCV 2023|用于管状结构分割的动态蛇形卷积）
     </a>
    </p>
    <p>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/135845841?spm=1001.2014.3001.5501" title="YOLOv5改进系列（29）——添加DilateFormer（MSDA）注意力机制（中科院一区顶刊|即插即用的多尺度全局注意力机制）">
      YOLOv5改进系列（29）——添加DilateFormer（MSDA）注意力机制（中科院一区顶刊|即插即用的多尺度全局注意力机制）
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/139153395?spm=1001.2014.3001.5502" title="YOLOv5改进系列（30）——添加iRMB注意力机制（ICCV 2023|即插即用的反向残差注意力机制）">
      <br/>
      YOLOv5改进系列（30）——添加iRMB注意力机制（ICCV 2023|即插即用的反向残差注意力机制）
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/139834875?spm=1001.2014.3001.5501" title="YOLOv5改进系列（31）——添加Dual-ViT注意力机制（TPAMI 2023|京东提出多尺度双视觉Transformer，降低计算开销）">
      <br/>
      YOLOv5改进系列（31）——添加Dual-ViT注意力机制（TPAMI 2023|京东提出多尺度双视觉Transformer，降低计算开销）
     </a>
     <br/>
     <a href="https://blog.csdn.net/weixin_43334693/article/details/139832512?spm=1001.2014.3001.5502" title="YOLOv5改进系列（32）——替换主干网络之PKINet（CVPR2024 | 面向遥感旋转框主干，有效捕获不同尺度上的密集纹理特征）">
      <br/>
      ​​​​​​​YOLOv5改进系列（32）——替换主干网络之PKINet（CVPR2024 | 面向遥感旋转框主干，有效捕获不同尺度上的密集纹理特征）
     </a>
    </p>
    <p>
     持续更新中。。。
    </p>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/bd85d57a8763651965e1a73b3dfd8287.gif">
     </img>
    </p>
    <h2 id="%E4%B8%80%E3%80%81YOLOv5%E7%9A%84%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" style="margin-left:0px;text-align:left;">
     <strong>
      一、YOLOv5的网络结构
     </strong>
    </h2>
    <p style="margin-left:0;text-align:left;">
     <strong>
      YOLOv5特点：
     </strong>
     <span style="color:#fe2c24;">
      合适于移动端部署，模型小，速度快
     </span>
    </p>
    <p style="margin-left:0;text-align:left;">
     YOLOv5有
     <strong>
      YOLOv5s、YOLOv5m、YOLOv5l、YOLOv5x
     </strong>
     四个版本。文件中，这几个模型的结构基本一样，不同的是depth_multiple模型深度和width_multiple模型宽度这两个参数。 就和我们买衣服的尺码大小排序一样，YOLOv5s网络是YOLOv5系列中深度最小，特征图的宽度最小的网络。其他的三种都是在此基础上不断加深，不断加宽。
    </p>
    <p style="margin-left:0px;text-align:center;">
     <img alt="" height="217" src="https://i-blog.csdnimg.cn/blog_migrate/c56b42d534b692075cf5ae000747eb85.png" width="623"/>
    </p>
    <p>
     <span style="color:#4da8ee;">
      <strong>
       YOLOv5s的网络结构如下：
      </strong>
     </span>
    </p>
    <p style="text-align:center;">
     <strong>
      <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/a573a17c82a6a1e3d23469541d3b4afc.jpeg"/>
     </strong>
    </p>
    <p style="margin-left:.0001pt;text-align:left;">
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （1）输入端 ：
     </strong>
     Mosaic数据增强、自适应锚框计算、自适应图片缩放
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （2）Backbone ：
     </strong>
     Focus结构，CSP结构
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （3）Neck ：
     </strong>
     FPN+PAN结构
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （4）Head ：
     </strong>
     CIOU_Loss
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#e6b223;">
      <strong>
       基本组件：
      </strong>
     </span>
    </p>
    <ul>
     <li>
      <strong>
       Focus：
      </strong>
      基本上就是YOLO v2的passthrough。
     </li>
     <li>
      <strong>
       CBL：
      </strong>
      由Conv+Bn+Leaky_relu激活函数三者组成。
     </li>
     <li>
      <strong>
       CSP1_X：
      </strong>
      借鉴CSPNet网络结构，由三个卷积层和X个Res unint模块Concate组成。
     </li>
     <li>
      <strong>
       CSP2_X：
      </strong>
      不再用Res unint模块，而是改为CBL。
     </li>
     <li>
      <strong>
       SPP：
      </strong>
      采用1×1，5×5，9×9，13×13的最大池化的方式，进行多尺度融合。
     </li>
    </ul>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#98c091;">
      <strong>
       YOLO5算法性能测试图：
      </strong>
     </span>
    </p>
    <p class="img-center">
     <img alt="" height="238" src="https://i-blog.csdnimg.cn/blog_migrate/9c5ad986153900b975c88021a889463e.png" width="457"/>
    </p>
    <hr/>
    <h2 id="%C2%A0%E4%BA%8C%E3%80%81%E8%BE%93%E5%85%A5%E7%AB%AF">
     <strong>
      二、输入端
     </strong>
    </h2>
    <h3 id="%EF%BC%881%EF%BC%89Mosaic%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA" style="margin-left:0px;text-align:left;">
     <span style="background-color:#38d8f0;">
      （1）Mosaic数据增强
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     YOLOv5在输入端采用了
     <span style="color:#fe2c24;">
      Mosaic数据增强
     </span>
     ，
     <strong>
      Mosaic 数据增强算法将多张图片按照一定比例组合成一张图片，使模型在更小的范围内识别目标。
     </strong>
     Mosaic 数据增强算法参考 CutMix数据增强算法。CutMix数据增强算法使用两张图片进行拼接，而 Mosaic 数据增强算法一般使用
     <span style="color:#fe2c24;">
      四张
     </span>
     进行拼接，但两者的算法原理是非常相似的。
    </p>
    <p style="margin-left:.0001pt;text-align:center;">
     <img alt="" height="377" src="https://i-blog.csdnimg.cn/blog_migrate/c5db6630d97fde1e0d5599cf728244fc.png" width="532"/>
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      <span style="color:#4da8ee;">
       Mosaic数据增强的主要步骤为：
      </span>
     </strong>
    </p>
    <p style="margin-left:0;text-align:left;">
     （1）随机选取图片拼接基准点坐标（xc，yc），另随机选取四张图片。
    </p>
    <p style="margin-left:0;text-align:left;">
     （2）四张图片根据基准点，分别经过尺寸调整和比例缩放后，放置在指定尺寸的大图的左上，右上，左下，右下位置。
    </p>
    <p style="margin-left:0;text-align:left;">
     （3）根据每张图片的尺寸变换方式，将映射关系对应到图片标签上。
    </p>
    <p style="margin-left:0;text-align:left;">
     （4）依据指定的横纵坐标，对大图进行拼接。处理超过边界的检测框坐标。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      <span style="color:#faa572;">
       采用Mosaic数据增强的方式有几个优点：
      </span>
     </strong>
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （1）丰富数据集：
     </strong>
     随机使用4张图像，随机缩放后随机拼接，增加很多小目标，大大增加了数据多样性。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （2）增强模型鲁棒性：
     </strong>
     混合四张具有不同语义信息的图片，可以让模型检测超出常规语境的目标。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （3）加强批归一化层（Batch Normalization）的效果：
     </strong>
     当模型设置 BN 操作后，训练时会尽可能增大批样本总量（BatchSize），因为 BN 原理为计算每一个特征层的均值和方差，如果批样本总量越大，那么 BN 计算的均值和方差就越接近于整个数据集的均值和方差，效果越好。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      （4）Mosaic 数据增强算法有利于提升小目标检测性能：
     </strong>
     Mosaic 数据增强图像由四张原始图像拼接而成，这样每张图像会有更大概率包含小目标，从而提升了模型的检测能力。
    </p>
    <hr/>
    <h3 id="%C2%A0%EF%BC%882%EF%BC%89%E8%87%AA%E9%80%82%E5%BA%94%E9%94%9A%E6%A1%86%E8%AE%A1%E7%AE%97">
     <span style="background-color:#d4e9d5;">
      （2）自适应锚框计算
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     <strong>
      之前我们学的 YOLOv3、YOLOv4，对于不同的数据集，都会计算先验框 anchor。
     </strong>
     然后在训练时，网络会在 anchor 的基础上进行预测，输出预测框，再和标签框进行对比，最后就进行梯度的反向传播。
    </p>
    <p style="margin-left:0;text-align:left;">
     在 YOLOv3、YOLOv4 中，训练不同的数据集时，是
     <strong>
      使用单独的脚本进行初始锚框的计算
     </strong>
     ，
     <span style="color:#fe2c24;">
      在 YOLOv5 中，则是将此功能嵌入到整个训练代码里中。所以在每次训练开始之前，它都会根据不同的数据集来自适应计算 anchor。
     </span>
    </p>
    <p style="margin-left:0;text-align:left;">
     but，如果觉得计算的锚框效果并不好，那你也可以在代码中将此功能关闭哈~
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#6eaad7;">
      <strong>
       自适应的计算具体过程：
      </strong>
     </span>
    </p>
    <p>
     ①获取数据集中所有目标的宽和高。
    </p>
    <p>
     ②将每张图片中按照等比例缩放的方式到 resize 指定大小，这里保证宽高中的最大值符合指定大小。
    </p>
    <p>
     ③将 bboxes 从相对坐标改成绝对坐标，这里乘以的是缩放后的宽高。
    </p>
    <p>
     ④筛选 bboxes，保留宽高都大于等于两个像素的 bboxes。
    </p>
    <p>
     ⑤使用 k-means 聚类三方得到n个 anchors，与YOLOv3、YOLOv4 操作一样。
    </p>
    <p>
     ⑥使用遗传算法随机对 anchors 的宽高进行变异。倘若变异后的效果好，就将变异后的结果赋值给 anchors；如果变异后效果变差就跳过，默认变异1000次。这里是使用 anchor_fitness 方法计算得到的适应度 fitness，然后再进行评估。
    </p>
    <hr/>
    <h3 id="%EF%BC%883%EF%BC%89%E8%87%AA%E9%80%82%E5%BA%94%E5%9B%BE%E7%89%87%E7%BC%A9%E6%94%BE" style="margin-left:0px;text-align:left;">
     <span style="background-color:#f9eda6;">
      （3）自适应图片缩放
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#1c7892;">
      <strong>
       步骤：
      </strong>
     </span>
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      (1) 根据原始图片大小以及输入到网络的图片大小计算缩放比例
     </strong>
    </p>
    <p style="margin-left:0px;text-align:center;">
     <img alt="" height="257" src="https://i-blog.csdnimg.cn/blog_migrate/349ac3d967ca515b0f30215ff437a99e.png" width="606"/>
    </p>
    <p style="margin-left:0;text-align:left;">
     原始缩放尺寸是416*416，都除以原始图像的尺寸后，可以得到0.52，和0.69两个缩放系数，选择小的缩放系数。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      (2) 根据原始图片大小与缩放比例计算缩放后的图片大小
     </strong>
    </p>
    <p style="margin-left:0px;text-align:center;">
     <strong>
      <img alt="" height="257" src="https://i-blog.csdnimg.cn/blog_migrate/36b2a608a1741771690845c5c1c7838a.png" width="616"/>
     </strong>
    </p>
    <p style="margin-left:0;text-align:left;">
     原始图片的长宽都乘以最小的缩放系数0.52，宽变成了416，而高变成了312。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      (3) 计算黑边填充数值
     </strong>
    </p>
    <p style="margin-left:0px;text-align:center;">
     <strong>
      <img alt="" height="231" src="https://i-blog.csdnimg.cn/blog_migrate/3d8aae001880a5b6a66bba70ee653c82.png" width="604"/>
     </strong>
    </p>
    <p style="margin-left:0px;">
     将416-312=104，得到原本需要填充的高度。再采用numpy中np.mod取余数的方式，得到8个像素，再除以2，即得到图片高度两端需要填充的数值。
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      <span style="color:#ed7976;">
       注意：
      </span>
     </strong>
    </p>
    <p style="margin-left:0;text-align:left;">
     （1）Yolov5中填充的是
     <strong>
      <span style="color:#7b7f82;">
       灰色
      </span>
     </strong>
     ，即（114,114,114）。
    </p>
    <p style="margin-left:0;text-align:left;">
     （2）
     <span style="color:#fe2c24;">
      训练时没有采用缩减黑边的方式
     </span>
     ，还是采用传统填充的方式，即缩放到416*416大小。只是
     <span style="color:#fe2c24;">
      在测试，使用模型推理时，才采用缩减黑边的方式
     </span>
     ，提高目标检测，推理的速度。
    </p>
    <p style="margin-left:0;text-align:left;">
     （3）为什么np.mod函数的后面用32？
    </p>
    <p style="margin-left:0;text-align:left;">
     因为YOLOv5的网络经过5次下采样，而2的5次方，等于32。所以至少要去掉32的倍数，再进行取余。以免产生尺度太小走不完stride（filter在原图上扫描时，需要跳跃的格数）的问题，再进行取余。
    </p>
    <hr/>
    <h2 id="%E4%B8%89%E3%80%81Backbone" style="margin-left:0px;text-align:left;">
     <strong>
      三、Backbone
     </strong>
    </h2>
    <h3 id="%EF%BC%881%EF%BC%89Focus%E7%BB%93%E6%9E%84" style="margin-left:0px;text-align:left;">
     <span style="background-color:#38d8f0;">
      （1）Focus结构
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     <strong>
      Focus模块
     </strong>
     在YOLOv5中是图片进入
     <strong>
      Backbone
     </strong>
     前，对图片进行切片操作，
     <span style="color:#fe2c24;">
      具体操作是在一张图片中每隔一个像素拿到一个值
     </span>
     ，类似于邻近下采样，这样就拿到了四张图片，四张图片互补，长得差不多，但是没有信息丢失，这样一来，将W、H信息就集中到了通道空间，输入通道扩充了4倍，
     <strong>
      即拼接起来的图片相对于原先的RGB三通道模式变成了12个通道，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。
     </strong>
    </p>
    <p style="margin-left:0;text-align:left;">
     以YOLOv5s为例，原始的
     <strong>
      640 × 640 × 3
     </strong>
     的图像输入Focus结构，采用切片操作，先变成
     <strong>
      320 × 320 × 12
     </strong>
     的特征图，再经过一次卷积操作，最终变成
     <strong>
      320 × 320 × 32
     </strong>
     的特征图。
    </p>
    <p style="margin-left:0;text-align:left;">
     切片操作如下：
    </p>
    <p style="margin-left:0px;text-align:center;">
     <img alt="" height="263" src="https://i-blog.csdnimg.cn/blog_migrate/009d2d69aae2b1cb6ecede41530f82bf.png" width="661"/>
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#ff9900;">
      <strong>
       作用：
      </strong>
     </span>
     可以使信息不丢失的情况下提高计算力
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#1c7331;">
      <strong>
       不足：
      </strong>
     </span>
     Focus 对某些设备不支持且不友好，开销很大，另外切片对不齐的话模型就崩了。
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#9c8ec1;">
      <strong>
       后期改进：
      </strong>
     </span>
     <strong>
      在新版中，YOLOv5 将Focus 模块替换成了一个 6 x 6 的卷积层。
     </strong>
     两者的计算量是等价的，但是对于一些 GPU 设备，使用 6 x 6 的卷积会更加高效。
    </p>
    <p style="margin-left:.0001pt;text-align:left;">
    </p>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/2a0bc0eabec904b4df563bd08e55deef.png"/>
    </p>
    <hr/>
    <h3 id="%C2%A0%EF%BC%882%EF%BC%89CSP%E7%BB%93%E6%9E%84">
     <span style="background-color:#faa572;">
      （2）CSP结构
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     YOLOv4网络结构中，借鉴了CSPNet的设计思路，在主干网络中设计了CSP结构。
    </p>
    <p style="margin-left:0px;text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/f6512df3ec5d614f8e2a9b3a5c262a74.png"/>
    </p>
    <p style="margin-left:0;text-align:left;">
     YOLOv5与YOLOv4不同点在于，
     <strong>
      YOLOv4中只有主干网络使用了CSP结构
     </strong>
     。 而
     <span style="color:#fe2c24;">
      <strong>
       YOLOv5中设计了两种CSP结构，以YOLOv5s网络为例，CSP1_ X结构应用于Backbone主干网络，另一种CSP2_X结构则应用于Neck中。
      </strong>
     </span>
    </p>
    <p style="margin-left:0px;text-align:center;">
     <span style="color:#fe2c24;">
      <strong>
       <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/0956777f26dd025353130483c6ed8568.png"/>
      </strong>
     </span>
    </p>
    <hr/>
    <h2 id="%E5%9B%9B%E3%80%81Neck" style="margin-left:0px;text-align:left;">
     <strong>
      四、Neck
     </strong>
    </h2>
    <p style="margin-left:0;text-align:left;">
     YOLOv5现在的Neck和YOLOv4中一样，都采用
     <span style="color:#fe2c24;">
      <strong>
       FPN+PAN
      </strong>
     </span>
     的结构。但是在它的基础上做了一些改进操作：
     <strong>
      YOLOV4的Neck结构中，采用的都是普通的卷积操作
     </strong>
     ，
     <span style="color:#fe2c24;">
      而YOLOV5的Neck中，采用
      <strong>
       CSPNet设计的CSP2结构
      </strong>
      ，从而加强了网络特征融合能力。
     </span>
    </p>
    <p style="margin-left:0;text-align:left;">
     结构如下图所示，FPN层自顶向下传达强语义特征，而PAN塔自底向上传达定位特征：
    </p>
    <p class="img-center">
     <img alt="" height="327" src="https://i-blog.csdnimg.cn/blog_migrate/1848da80f05ee1ad1b950cced6983fbf.png" width="428"/>
    </p>
    <hr/>
    <h2 id="%C2%A0%C2%A0%E4%BA%94%E3%80%81Head">
     <strong>
      五、Head
     </strong>
    </h2>
    <h3 id="%EF%BC%881%EF%BC%89Bounding%20box%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0" style="margin-left:0px;text-align:left;">
     <span style="background-color:#fbd4d0;">
      （1）Bounding box损失函数
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     YOLO v5采用
     <span style="color:#fe2c24;">
      CIOU_LOSS
     </span>
     作为bounding box 的损失函数。（关于IOU_ Loss、GIOU_ Loss、DIOU_ Loss以及CIOU_Loss的介绍，请看YOLOv4那一篇：
     <a href="https://blog.csdn.net/weixin_43334693/article/details/129248856?spm=1001.2014.3001.5501" title="【YOLO系列】YOLOv4论文超详细解读2（网络详解）">
      【YOLO系列】YOLOv4论文超详细解读2（网络详解）
     </a>
     ）
    </p>
    <hr/>
    <h3 id="%EF%BC%882%EF%BC%89NMS%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6" style="margin-left:0px;text-align:left;">
     <span style="background-color:#f9eda6;">
      （2）NMS非极大值抑制
     </span>
    </h3>
    <p style="margin-left:0;text-align:left;">
     NMS 的本质是搜索局部极大值，抑制非极大值元素。
    </p>
    <p style="margin-left:0;text-align:left;">
     非极大值抑制，
     <strong>
      主要就是用来抑制检测时冗余的框
     </strong>
     。因为在目标检测中，在同一目标的位置上会产生大量的候选框，这些候选框相互之间可能会有重叠，所以我们需要利用非极大值抑制找到最佳的目标边界框，消除冗余的边界框。
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#1c7892;">
      <strong>
       算法流程：
      </strong>
     </span>
    </p>
    <p>
     1.对所有预测框的置信度降序排序
    </p>
    <p>
     2.选出置信度最高的预测框，确认其为正确预测，并计算他与其他预测框的 IOU
    </p>
    <p>
     3.根据步骤2中计算的 IOU 去除重叠度高的，IOU &gt; threshold 阈值就直接删除
    </p>
    <p>
     4.剩下的预测框返回第1步，直到没有剩下的为止
    </p>
    <hr/>
    <p>
     <span style="color:#9c8ec1;">
      <strong>
       SoftNMS：
      </strong>
     </span>
    </p>
    <p style="margin-left:0;text-align:left;">
     <strong>
      当两个目标靠的非常近时，置信度低的会被置信度高的框所抑制
     </strong>
     ，那么当两个目标靠的十分近的时候就只会识别出一个 BBox。为了解决这个问题，可以使用 softNMS。
    </p>
    <p style="margin-left:0;text-align:left;">
     它的基本思想是
     <span style="color:#fe2c24;">
      用稍低一点的分数来代替原有的分数，而不是像 NMS 一样直接置零。
     </span>
    </p>
    <p style="margin-left:.0001pt;text-align:left;">
    </p>
    <p style="margin-left:.0001pt;text-align:center;">
     <img alt="" height="319" src="https://i-blog.csdnimg.cn/blog_migrate/bf2b66f103ad82f5a3df332082361bd9.png" width="554"/>
    </p>
    <hr/>
    <h2 id="%C2%A0%E5%85%AD%E3%80%81%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5">
     <strong>
      六、训练策略
     </strong>
    </h2>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#fe2c24;">
      <strong>
       （1）多尺度训练（Multi-scale training）。
      </strong>
     </span>
     如果网络的输入是416 x 416。那么训练的时候就会从 0.5 x 416 到 1.5 x 416 中任意取值，但所取的值都是32的整数倍。
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#fe2c24;">
      <strong>
       （2）训练开始前使用 warmup 进行训练。
      </strong>
     </span>
     在模型预训练阶段，先使用较小的学习率训练一些epochs或者steps (如4个 epoch 或10000个 step)，再修改为预先设置的学习率进行训练。
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#fe2c24;">
      <strong>
       （3）使用了 cosine 学习率下降策略（Cosine LR scheduler）。
      </strong>
     </span>
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#fe2c24;">
      <strong>
       （4）采用了 EMA 更新权重(Exponential Moving Average)。
      </strong>
     </span>
     相当于训练时给参数赋予一个动量，这样更新起来就会更加平滑。
    </p>
    <p style="margin-left:0;text-align:left;">
     <span style="color:#fe2c24;">
      <strong>
       （5）使用了 amp 进行混合精度训练（Mixed precision）。
      </strong>
     </span>
     能够减少显存的占用并且加快训练速度，但是需要 GPU 支持。
    </p>
    <hr/>
    <p>
     总结一下，YOLO v5和前YOLO系列相比的改进：
    </p>
    <ul>
     <li>
      (1) 增加了正样本：方法是邻域的正样本anchor匹配策略。
     </li>
     <li>
      (2) 通过灵活的配置参数，可以得到不同复杂度的模型
     </li>
     <li>
      (3) 通过一些内置的超参优化策略，提升整体性能
     </li>
     <li>
      (4) 和yolov4一样，都用了mosaic增强，提升小物体检测性能
     </li>
    </ul>
    <p style="text-align:center;">
     <img alt="" src="https://i-blog.csdnimg.cn/blog_migrate/ba513bd3374425d0714f63720abc701b.gif"/>
    </p>
   </div>
  </div>
 </article>
</div>


<p class="artid" style="display:none">68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34333333343639332f:61727469636c652f64657461696c732f313239333132343039</p>
