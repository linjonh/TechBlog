---
layout: post
title: "报错解决undefined-symbol-_ZN15TracebackLoggerC1EPKc,-version-libcudnn_ops_infer.so.8"
date: 2024-07-03 13:20:30 +0800
description: "运行API是较高级别的接口，封装了驱动API的底层细节，提供了更简化的编程模型，隐藏了底层GPU硬件"
keywords: "报错解决：undefined symbol: _ZN15TracebackLoggerC1EPKc, version libcudnn_ops_infer.so.8"
categories: ["未分类"]
tags: ["Python"]
artid: "135660237"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=135660237
  alt: "报错解决undefined-symbol-_ZN15TracebackLoggerC1EPKc,-version-libcudnn_ops_infer.so.8"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     报错解决：undefined symbol: _ZN15TracebackLoggerC1EPKc, version libcudnn_ops_infer.so.8
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     搭建 resemble-enhance 这个项目的过程中，在 Ubuntu20.04的机器上跑，报错如下：
    </p>
    <pre><code class="prism language-bash">undefined symbol: _ZN15TracebackLoggerC1EPKc, version libcudnn_ops_infer.so.8
</code></pre>
    <p>
     这个错误是在 NVIDIA GPU 上使用 PyTorch 2.1.2 和 cuDNN 12.1 时使用 torch.nn.Conv2d 时出现符号查找错误，这意味着 PyTorch 正在尝试在库 libcudnn_ops_infer.so.8 中查找名为 _ZN15TracebackLoggerC1EPKc 的符号，但该符号不存在， 发生这种情况的原因有多种，包括：
     <br/>
     1、cuDNN 版本之间不匹配：虽然安装了 cuDNN 12.1，但用于构建 PyTorch (cu121) 的 cuDNN 版本与系统上可用的 cuDNN 版本之间可能不匹配
     <br/>
     2、库路径不正确，环境中 cuDNN 库的路径可能不正确
     <br/>
     3、cuDNN 安装损坏或不完整，cuDNN 安装可能已损坏或不完整
    </p>
    <p>
     要解决问题，有三个东西要理解清楚：nivdia-smi、nvcc以及cuDNN
    </p>
    <table>
     <thead>
      <tr>
       <th>
        nvidia-smi
       </th>
       <th>
        nvcc
       </th>
       <th>
        cuDNN
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        1、nvidia-smi是NVIDIA System Management Interface的缩写，是一个命令行工具
        <br/>
        2、用于监视和管理NVIDIA GPU设备
        <br/>
        3、nvidia-smi提供有关GPU的详细信息，例如GPU的使用情况、显存使用情况、温度、驱动程序版本等
        <br/>
        4、用于检查CUDA版本、显示GPU拓扑结构和进程信息等
       </td>
       <td>
        1、nvcc是NVIDIA CUDA Compiler的缩写，是一个用于编译和构建CUDA代码的命令行工具
        <br/>
        2、是NVIDIA CUDA工具包的一部分，用于将CUDA源代码编译为可在NVIDIA GPU上执行的机器码
        <br/>
        3、nvcc根据CUDA源代码中的CUDA扩展和C++语法，生成针对NVIDIA GPU架构优化的可执行文件
        <br/>
        4、还提供了一些编译选项，允许开发人员控制编译过程、优化级别和目标GPU架构等
       </td>
       <td>
        1、cuDNN是NVIDIA Deep Neural Network库的缩写，是一个用于深度学习的GPU加速库
        <br/>
        2、提供了高性能的深度神经网络加速，包括卷积神经网络（CNN）、循环神经网络（RNN）等
        <br/>
        3、cuDNN通过优化算法和实现，加速了深度学习任务的计算过程
        <br/>
        4、提供了一系列的函数和接口，可用于在CUDA平台上开发和部署深度学习模型
       </td>
      </tr>
     </tbody>
    </table>
    <p>
     简而言之，nvidia-smi是用于监视和管理NVIDIA GPU设备的命令行工具，nvcc是用于编译和构建CUDA代码的命令行编译器，而cuDNN是一个用于深度学习的GPU加速库
     <br/>
     基于以上概念，我先运行
     <code>
      nvidia-smi
     </code>
     ，结果如下，CUDA版本为12.2
    </p>
    <pre><code class="prism language-bash">+---------------------------------------------------------------------------------------+
<span class="token operator">|</span> NVIDIA-SMI <span class="token number">535.104</span>.12             Driver Version: <span class="token number">535.104</span>.12   CUDA Version: <span class="token number">12.2</span>     <span class="token operator">|</span>
<span class="token operator">|</span>-----------------------------------------+----------------------+----------------------+
<span class="token operator">|</span> GPU  Name                 Persistence-M <span class="token operator">|</span> Bus-Id        Disp.A <span class="token operator">|</span> Volatile Uncorr. ECC <span class="token operator">|</span>
<span class="token operator">|</span> Fan  Temp   Perf          Pwr:Usage/Cap <span class="token operator">|</span>         Memory-Usage <span class="token operator">|</span> GPU-Util  Compute M. <span class="token operator">|</span>
<span class="token operator">|</span>                                         <span class="token operator">|</span>                      <span class="token operator">|</span>               MIG M. <span class="token operator">|</span>
<span class="token operator">|</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">+=</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">+=</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">|</span>
<span class="token operator">|</span>   <span class="token number">0</span>  NVIDIA A10                     Off <span class="token operator">|</span> 00000000:00:07.0 Off <span class="token operator">|</span>                    <span class="token number">0</span> <span class="token operator">|</span>
<span class="token operator">|</span>  <span class="token number">0</span>%   45C    P0              59W / 150W <span class="token operator">|</span>   7776MiB / 23028MiB <span class="token operator">|</span>      <span class="token number">0</span>%      Default <span class="token operator">|</span>
<span class="token operator">|</span>                                         <span class="token operator">|</span>                      <span class="token operator">|</span>                  N/A <span class="token operator">|</span>
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
<span class="token operator">|</span> Processes:                                                                            <span class="token operator">|</span>
<span class="token operator">|</span>  GPU   GI   CI        PID   Type   Process name                            GPU Memory <span class="token operator">|</span>
<span class="token operator">|</span>        ID   ID                                                             Usage      <span class="token operator">|</span>
<span class="token operator">|</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">=</span><span class="token operator">|</span>
<span class="token operator">|</span>    <span class="token number">0</span>   N/A  N/A    <span class="token number">933471</span>      C   python                                     2432MiB <span class="token operator">|</span>
<span class="token operator">|</span>    <span class="token number">0</span>   N/A  N/A    <span class="token number">956741</span>      C   python                                     5326MiB <span class="token operator">|</span>
+---------------------------------------------------------------------------------------+
</code></pre>
    <p>
     接着运行
     <code>
      nvcc -V
     </code>
     ，结果如下，CUDA版本为11.4
    </p>
    <pre><code class="prism language-bash">nvcc: NVIDIA <span class="token punctuation">(</span>R<span class="token punctuation">)</span> Cuda compiler driver
Copyright <span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token number">2005</span>-2021 NVIDIA Corporation
Built on Wed_Jul_14_19:41:19_PDT_2021
Cuda compilation tools, release <span class="token number">11.4</span>, V11.4.100
Build cuda_11.4.r11.4/compiler.30188945_0
</code></pre>
    <p>
     我冒出了疑问：为什么两个命令显示的CUDA版本不一致？？？
     <br/>
     经过查阅得知，CUDA的API有两种：
     <strong>
      驱动（driver）API
     </strong>
     和
     <strong>
      运行（runtime）API
     </strong>
     <br/>
     1、驱动（driver）API：
     <br/>
     驱动API是较低级别的接口，提供了对GPU硬件和驱动程序的底层访问。驱动API需要开发人员显式地管理GPU的内存分配、数据传输和并行执行等操作，它提供了更多的灵活性和控制权，适用于需要直接操作GPU硬件的特定场景，以及对CUDA运行时环境更高级别的自定义需求，通过驱动API，开发人员可以创建和管理CUDA上下文（context）、配置GPU设备、分配和释放GPU内存等。
     <br/>
     2、运行（runtime）API：
     <br/>
     运行API是较高级别的接口，封装了驱动API的底层细节，提供了更简化的编程模型，隐藏了底层GPU硬件和驱动程序的复杂性，简化了CUDA应用程序的开发。运行API提供了一组高级函数，例如并行执行（kernel启动）、内存管理和数据传输等，以便开发人员更方便地编写CUDA应用程序，它提供了对上下文管理、内存分配和数据传输等常见操作的封装和自动化处理。因此运行API是更常用和推荐的API，适用于大多数CUDA应用程序。
    </p>
    <p>
     至此还有一个问题，这些和cuDNN有啥关系？cuDNN是一个用于深度学习的GPU加速库，CUDA和cuDNN之间存在版本兼容性的关系，一般情况下，每个cuDNN版本都支持一系列的CUDA版本，这是因为cuDNN库是针对特定的CUDA版本进行编译和优化的。
    </p>
    <p>
     <strong>
      基于以上分析得知：程序报错的原因是pytorch使用的cuDNN版本与机器安装的CUDA版本不兼容导致的错误，所以更新 运行（runtime）API 即可解决问题
     </strong>
    </p>
    <p>
     解决办法：
     <br/>
     Step1 ：下载cuda toolkit（https://developer.nvidia.com/cuda-toolkit-archive）
     <br/>
     Step2 ：运行安装脚本
     <br/>
     Step3 ：将Driver这里按ENTER勾掉！如下图
     <br/>
     <img alt="去掉Driver勾勾" src="https://i-blog.csdnimg.cn/blog_migrate/d702d625d9bc852fbdd0e078fd5fd357.png">
      <br/>
      Step4 ：配置环境变量，在
      <code>
       ~/.bashrc
      </code>
      中添加如下内容以后记得
      <code>
       source ~/.bashrc
      </code>
     </img>
    </p>
    <pre><code class="prism language-bash"><span class="token builtin class-name">export</span> <span class="token assign-left variable"><span class="token environment constant">PATH</span></span><span class="token operator">=</span><span class="token environment constant">$PATH</span>:/usr/local/cuda-12.1/bin
<span class="token builtin class-name">export</span> <span class="token assign-left variable">LD_LIBRARY_PATH</span><span class="token operator">=</span><span class="token variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda-12.1/lib64
</code></pre>
    <p>
     至此，问题解决
    </p>
    <p>
     参考：https://blog.csdn.net/qq_41368074/article/details/107785536
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f62:6c6f672e6373646e2e6e65742f71715f34353739373632352f:61727469636c652f64657461696c732f313335363630323337" class_="artid" style="display:none">
 </p>
</div>
