---
layout: post
title: 2024-02-23-音视频同步
date: 2024-02-23 17:20:44 +0800
categories: [音视频]
tags: [音视频同步]
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=122502560
  alt: 音视频同步
artid: 122502560
---
<span class="artid" style="display:none" artid=68747470733a:2f2f626c6f672e6373646e2e6e65742f77616e6762756a692f:61727469636c652f64657461696c732f313232353032353630></span>
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     音视频同步
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
    </p>
    <p>
     视频：帧率，表示视频一秒显示的帧数。
    </p>
    <p>
     音频：采样率，表示音频一秒播放的sample数。
    </p>
    <p>
     声卡和显卡均是以一帧数据来作为播放单位，如果单纯依赖帧率及采样率来进行播放，在理想条件下，应该是同步的，不会出现偏差。
    </p>
    <p>
     一个AAC音频frame每声道1024个sample，一帧播放时长duration=(1024/44100)×1000ms = 23.22ms；一个视频frame播放时长duration=1000ms/24 = 41.67ms。声卡虽然是以音频采样点为播放单位，但通常我们每次往声卡缓冲区送一个音频frame，每送一个音频frame更新一下音频的播放时刻，即每隔一个音频frame时长更新一下音频时钟，实际上ffplay就是这么做的。理想情况下，音视频完全同步，音视频播放过程如下图所示：
    </p>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/7e59f96294da52ee3c897a9f71e8ce01.png"/>
    </p>
    <p>
    </p>
    <p>
     但实际情况，往往不同步，原因如下：
    </p>
    <p>
     1一帧的播放时间，难以精准控制。音视频解码及渲染的耗时不同，可能造成每一帧输出有一点细微差距，长久累计，不同步便越来越明显。（例如受限于性能，42ms才能输出一帧）
    </p>
    <p>
     2音频输出是线性的，而视频输出可能是非线性，从而导致有偏差。
    </p>
    <p>
     3媒体流本身音视频有差距。（特别是TS实时流，音视频能播放的第一个帧起点不同）
    </p>
    <p>
     所以，解决音视频同步问题，引入了时间戳：
     <br/>
     首先选择一个参考时钟，时间是线性递增的；
     <strong>
      编码时
     </strong>
     依据参考时钟给每个音视频数据块都打上时间戳；
     <strong>
      播放时
     </strong>
     ，根据音视频时间戳PTS及参考时钟，来调整播放。视频和音频的同步实际上是一个动态的过程，以参考时钟为标准，放快了就减慢播放速度；播放快了就加快播放的速度。
    </p>
    <p>
    </p>
    <p>
     没有B帧的情况下，DTS=PTS。从流分析工具elecard eye查看，流中P帧在B帧之前，但显示在B帧之后。
    </p>
    <p>
     <img alt="" height="142" src="https://i-blog.csdnimg.cn/blog_migrate/a019ab6151426504cfa5bb9d484cd662.png" width="271"/>
    </p>
    <p>
    </p>
    <p>
     <strong>
      参考时钟的选择一般来说有以下三种：
     </strong>
    </p>
    <p>
     以音频为基准，将视频同步到音频。人对音频更敏感，音频播放时钟线性增长，
     <strong>
      常用做法
     </strong>
     <br/>
     将音频同步到视频
     <br/>
     选择一个外部时钟为基准，视频和音频的播放速度都以该时钟为标准。
    </p>
    <p>
    </p>
    <p>
     <strong>
      ffplay音视频同步方式
     </strong>
     ，以audio为参考时钟，video同步到音频：
    </p>
    <p>
     获取当前要显示的video PTS，减去上一帧视频PTS，则得出上一帧视频应该显示的时长delay；
     <br/>
     当前video PTS与参考时钟当前audio PTS比较，得出音视频差距diff；
     <br/>
     获取同步阈值sync_threshold，为一帧视频差距，范围为10ms-100ms；
     <br/>
     diff小于sync_threshold，则认为不需要同步；否则delay+diff值，则是正确纠正delay；
     <br/>
     如果超过sync_threshold，且视频落后于音频，那么需要减小delay（FFMAX(0, delay + diff)），让当前帧尽快显示。
     <br/>
     如果视频落后超过1秒，且之前10次都快速输出视频帧，那么需要反馈给音频源减慢，同时反馈视频源进行丢帧处理，让视频尽快追上音频。因为这很可能是视频解码跟不上了，再怎么调整delay也没用。
     <br/>
     如果超过sync_threshold，且视频快于音频，那么需要加大delay，让当前帧延迟显示。
     <br/>
     将delay*2慢慢调整差距，这是为了平缓调整差距，因为直接delay+diff，会让画面画面迟滞。
     <br/>
     如果视频前一帧本身显示时间很长，那么直接delay+diff一步调整到位，因为这种情况再慢慢调整也没太大意义。
     <br/>
     考虑到渲染的耗时，还需进行调整。frame_timer为一帧显示的系统时间，frame_timer+delay- curr_time，则得出正在需要延迟显示当前帧的时间。
    </p>
    <p>
     <img alt="" height="638" src="https://i-blog.csdnimg.cn/blog_migrate/2b1015310df08005a223edeede1c0d77.png" width="1115"/>
    </p>
    <p>
     小黑圆圈是代表帧的实际播放时刻，小红圆圈代表帧的理论播放时刻，小绿方块表示当前系统时间(当前时刻)，小红方块表示位于不同区间的时间点，则当前时刻处于不同区间时，视频同步策略为：
     <br/>
     [1] 当前时刻在T0位置，则重复播放上一帧，延时remaining_time后再播放当前帧
     <br/>
     [2] 当前时刻在T1位置，则立即播放当前帧
     <br/>
     [3] 当前时刻在T2位置，则忽略当前帧，立即显示下一帧，加速视频追赶
     <br/>
     上述内容是为了方便理解进行的简单而形象的描述。实际过程要计算相关值，根据compute_target_delay()和video_refresh()中的策略来控制播放过程。
    </p>
    <p>
    </p>
    <p>
    </p>
    <p>
     {
     <!-- -->
     <br/>
     video-&gt;frameq.deQueue(&amp;video-&gt;frame);
     <br/>
     //获取上一帧需要显示的时长delay
     <br/>
     double current_pts = *(double *)video-&gt;frame-&gt;opaque;
     <br/>
     double delay = current_pts - video-&gt;frame_last_pts;
     <br/>
     if (delay &lt;= 0 || delay &gt;= 1.0)
     <br/>
     {
     <!-- -->
     <br/>
     delay = video-&gt;frame_last_delay;
     <br/>
     }
     <br/>
     <br/>
     // 根据视频PTS和参考时钟调整delay
     <br/>
     double ref_clock = audio-&gt;get_audio_clock();
     <br/>
     double diff = current_pts - ref_clock;// diff &lt; 0 :video slow,diff &gt; 0 :video fast
     <br/>
     //一帧视频时间或10ms，10ms音视频差距无法察觉
     <br/>
     double sync_threshold = FFMAX(MIN_SYNC_THRESHOLD, FFMIN(MAX_SYNC_THRESHOLD, delay)) ;
     <br/>
     <br/>
     audio-&gt;audio_wait_video(current_pts,false);
     <br/>
     video-&gt;video_drop_frame(ref_clock,false);
     <br/>
     if (!isnan(diff) &amp;&amp; fabs(diff) &lt; NOSYNC_THRESHOLD) // 不同步
     <br/>
     {
     <!-- -->
     <br/>
     if (diff &lt;= -sync_threshold)//视频比音频慢，加快
     <br/>
     {
     <!-- -->
     <br/>
     delay = FFMAX(0,  delay + diff);
     <br/>
     static int last_delay_zero_counts = 0;
     <br/>
     if(video-&gt;frame_last_delay &lt;= 0)
     <br/>
     {
     <!-- -->
     <br/>
     last_delay_zero_counts++;
     <br/>
     }
     <br/>
     else
     <br/>
     {
     <!-- -->
     <br/>
     last_delay_zero_counts = 0;
     <br/>
     }
     <br/>
     if(diff &lt; -1.0 &amp;&amp; last_delay_zero_counts &gt;= 10)
     <br/>
     {
     <!-- -->
     <br/>
     printf("maybe video codec too slow, adjust video&amp;audio\n");
     <br/>
     #ifndef DORP_PACK
     <br/>
     audio-&gt;audio_wait_video(current_pts,true);//差距较大，需要反馈音频等待视频
     <br/>
     #endif
     <br/>
     video-&gt;video_drop_frame(ref_clock,true);//差距较大，需要视频丢帧追上
     <br/>
     }
     <br/>
     }
     <br/>
     //视频比音频快，减慢
     <br/>
     else if (diff &gt;= sync_threshold &amp;&amp; delay &gt; SYNC_FRAMEDUP_THRESHOLD)
     <br/>
     delay = delay + diff;//音视频差距较大，且一帧的超过帧最常时间，一步到位
     <br/>
     else if (diff &gt;= sync_threshold)
     <br/>
     delay = 2 * delay;//音视频差距较小，加倍延迟，逐渐缩小
     <br/>
     }
    </p>
    <p>
    </p>
    <p>
     video-&gt;frame_last_delay = delay;
     <br/>
     video-&gt;frame_last_pts = current_pts;
    </p>
    <p>
     double curr_time = static_cast&lt;double&gt;(av_gettime()) / 1000000.0;
     <br/>
     if(video-&gt;frame_timer == 0)
     <br/>
     {
     <!-- -->
     <br/>
     video-&gt;frame_timer = curr_time;//show first frame ,set frame timer
     <br/>
     }
    </p>
    <p>
     double actual_delay = video-&gt;frame_timer + delay - curr_time;
     <br/>
     if (actual_delay &lt;= MIN_REFRSH_S)
     <br/>
     {
     <!-- -->
     <br/>
     actual_delay = MIN_REFRSH_S;
     <br/>
     }
     <br/>
     usleep(static_cast&lt;int&gt;(actual_delay * 1000 * 1000));
     <br/>
     //printf("actual_delay[%lf] delay[%lf] diff[%lf]\n",actual_delay,delay,diff);
     <br/>
     // Display
     <br/>
     SDL_UpdateTexture(video-&gt;texture, &amp;(video-&gt;rect), video-&gt;frame-&gt;data[0], video-&gt;frame-&gt;linesize[0]);
     <br/>
     SDL_RenderClear(video-&gt;renderer);
     <br/>
     SDL_RenderCopy(video-&gt;renderer, video-&gt;texture, &amp;video-&gt;rect, &amp;video-&gt;rect);
     <br/>
     SDL_RenderPresent(video-&gt;renderer);
     <br/>
     video-&gt;frame_timer = static_cast&lt;double&gt;(av_gettime()) / 1000000.0 ;
     <br/>
     <br/>
     av_frame_unref(video-&gt;frame);
     <br/>
     <br/>
     //update next frame
     <br/>
     schedule_refresh(media, 1);
     <br/>
     }
     <br/>
    </p>
   </div>
  </div>
 </article>
</div>


