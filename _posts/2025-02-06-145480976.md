---
layout: post
title: "Mac-部署Ollama-OpenWebUI完全指南"
date: 2025-02-06 22:08:13 +0800
description: "文章浏览阅读4.8k次，点赞12次，通过本教程的配置，你可以拥有一个完全本地化的AI助手！_mac "
keywords: "mac openwebui"
categories: ['']
tags: ['人工智能', 'Macos']
artid: "145480976"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=145480976
    alt: "Mac-部署Ollama-OpenWebUI完全指南"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     Mac 部署Ollama + OpenWebUI完全指南
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-github-gist" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <div class="toc">
     <h4>
      文章目录
     </h4>
     <ul>
      <li>
       <ul>
        <li>
         <a href="#__4" rel="nofollow">
          💻 环境说明
         </a>
        </li>
        <li>
         <a href="#_Ollama_12" rel="nofollow">
          🛠️ Ollama安装配置
         </a>
        </li>
        <li>
         <ul>
          <li>
           <a href="#1_Ollamahttpsgithubcomollamaollama_21" rel="nofollow">
            1. 安装[Ollama](https://github.com/ollama/ollama)
           </a>
          </li>
          <li>
           <a href="#2_Ollama_37" rel="nofollow">
            2. 启动Ollama
           </a>
          </li>
          <li>
           <a href="#3__61" rel="nofollow">
            3. 模型存储位置
           </a>
          </li>
          <li>
           <a href="#4___Ollama__74" rel="nofollow">
            4. 配置 Ollama 环境变量
           </a>
          </li>
          <li>
           <a href="#5__Ollama__109" rel="nofollow">
            5. Ollama 常用命令
           </a>
          </li>
          <li>
           <a href="#6__Ollama__121" rel="nofollow">
            6. Ollama 终端指令
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a href="#_OpenWebUI_130" rel="nofollow">
          🌐 OpenWebUI部署
         </a>
        </li>
        <li>
         <ul>
          <li>
           <a href="#1_Docker_137" rel="nofollow">
            1. 安装Docker
           </a>
          </li>
          <li>
           <a href="#2_OpenWebUIhttpswwwopenwebuicom_146" rel="nofollow">
            2. 部署[OpenWebUI](https://www.openwebui.com/)（可视化大模型对话界面）
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a href="#__179" rel="nofollow">
          🔒 离线部署方案
         </a>
        </li>
        <li>
         <ul>
          <li>
           <a href="#1__180" rel="nofollow">
            1. 准备工作
           </a>
          </li>
          <li>
           <ul>
            <li>
             <a href="#_Docker__181" rel="nofollow">
              下载必要的 Docker 镜像
             </a>
            </li>
            <li>
             <a href="#_197" rel="nofollow">
              下载所需的模型文件
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a href="#2__207" rel="nofollow">
            2. 离线环境部署
           </a>
          </li>
          <li>
           <ul>
            <li>
             <a href="#_Docker__209" rel="nofollow">
              加载 Docker 镜像
             </a>
            </li>
            <li>
             <a href="#_Docker__217" rel="nofollow">
              创建 Docker 网络
             </a>
            </li>
            <li>
             <a href="#_Ollama__223" rel="nofollow">
              启动 Ollama 容器
             </a>
            </li>
            <li>
             <a href="#_OpenWebUI__234" rel="nofollow">
              启动 OpenWebUI 容器
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a href="#3__245" rel="nofollow">
            3. 验证部署
           </a>
          </li>
          <li>
           <ul>
            <li>
             <a href="#_247" rel="nofollow">
              检查容器状态
             </a>
            </li>
            <li>
             <a href="#_WebUI_255" rel="nofollow">
              访问 WebUI
             </a>
            </li>
           </ul>
          </li>
          <li>
           <a href="#4__259" rel="nofollow">
            4. 故障排除
           </a>
          </li>
          <li>
           <a href="#5__282" rel="nofollow">
            5. 注意事项
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a href="#__293" rel="nofollow">
          ⚡ 性能优化建议
         </a>
        </li>
        <li>
         <a href="#__310" rel="nofollow">
          ❓ 常见问题
         </a>
        </li>
        <li>
         <a href="#__325" rel="nofollow">
          🎉 结语
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </div>
    <p>
    </p>
    <blockquote>
     <p>
      想拥有一个完全属于自己的AI助手，还不依赖互联网？本教程带你从零开始搭建本地AI环境！（Apple Silicon架构）
     </p>
    </blockquote>
    <h3>
     <a id="__4">
     </a>
     💻 环境说明
    </h3>
    <table>
     <thead>
      <tr>
       <th>
        配置项
       </th>
       <th>
        Mac
       </th>
       <th>
        Windows
       </th>
      </tr>
     </thead>
     <tbody>
      <tr>
       <td>
        操作系统
       </td>
       <td>
        macOS Sonoma
       </td>
       <td>
        Windows 10/11
       </td>
      </tr>
      <tr>
       <td>
        CPU
       </td>
       <td>
        M4
       </td>
       <td>
        12核或以上
       </td>
      </tr>
      <tr>
       <td>
        内存
       </td>
       <td>
        16GB
       </td>
       <td>
        32GB或以上
       </td>
      </tr>
      <tr>
       <td>
        硬盘空间
       </td>
       <td>
        20GB
       </td>
       <td>
        20GB或以上
       </td>
      </tr>
     </tbody>
    </table>
    <h3>
     <a id="_Ollama_12">
     </a>
     🛠️ Ollama安装配置
    </h3>
    <pre><code class="prism language-python">官网：https<span class="token punctuation">:</span><span class="token operator">//</span>ollama<span class="token punctuation">.</span>com<span class="token operator">/</span>
模型：https<span class="token punctuation">:</span><span class="token operator">//</span>ollama<span class="token punctuation">.</span>com<span class="token operator">/</span>library
Github：https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>ollama<span class="token operator">/</span>ollama
Docker：https<span class="token punctuation">:</span><span class="token operator">//</span>hub<span class="token punctuation">.</span>docker<span class="token punctuation">.</span>com<span class="token operator">/</span>r<span class="token operator">/</span>ollama<span class="token operator">/</span>ollama<span class="token operator">/</span>tags
</code></pre>
    <h4>
     <a id="1_Ollamahttpsgithubcomollamaollama_21">
     </a>
     1. 安装
     <a href="https://github.com/ollama/ollama">
      Ollama
     </a>
    </h4>
    <pre><code class="prism language-bash"><span class="token comment"># 使用Homebrew安装</span>
brew <span class="token function">install</span> ollama

<span class="token comment"># 或直接下载安装包</span>
<span class="token function">curl</span> https://ollama.ai/download/Ollama-darwin.zip <span class="token parameter variable">-o</span> Ollama.zip
<span class="token function">unzip</span> Ollama.zip

<span class="token comment"># 输入`ollama`或 `ollama -v`验证安装</span>
ollama

</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/f619dbb9fe3f402b8c4b995b0fbd629a.png"/>
    </p>
    <h4>
     <a id="2_Ollama_37">
     </a>
     2. 启动Ollama
    </h4>
    <pre><code class="prism language-bash"><span class="token comment"># 启动Ollama服务</span>
ollama serve
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/15a1652352224034a6935907dbef0f1d.png"/>
    </p>
    <pre><code class="prism language-python"><span class="token comment"># 或点击浏览器访问：http://localhost:11434</span>
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/bd79f95cd7ad4d2d9900a7fa85c29487.png"/>
    </p>
    <p>
     显示
     <code>
      Ollama is running
     </code>
     代表已经运行起来了！
    </p>
    <pre><code class="prism language-python"> <span class="token comment"># 下载Llama3 8B模型</span>
ollama run llama3<span class="token punctuation">:</span>8b  <span class="token comment"># 建议先尝试小模型</span>
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/8eb258a56179470881bb40867d387d52.png">
      <br/>
      💡 小贴士：你应该至少有 8 GB 的 RAM 来运行 7B 模型，16 GB 的 RAM 来运行 13B 模型，以及 32 GB 的 RAM 来运行 33B 模型。
     </img>
    </p>
    <h4>
     <a id="3__61">
     </a>
     3. 模型存储位置
    </h4>
    <p>
     Mac下，Ollama的默认存储位置：
    </p>
    <ul>
     <li>
      模型文件：
      <code>
       ~/.ollama/models
      </code>
     </li>
     <li>
      配置文件：
      <code>
       ~/Library/Application Support/Ollama
      </code>
     </li>
    </ul>
    <p>
     Windows下，Ollama的默认存储位置：
    </p>
    <ul>
     <li>
      程序目录：
      <code>
       C:\Users\&lt;用户名&gt;\AppData\Local\Programs\Ollama
      </code>
     </li>
     <li>
      模型目录：
      <code>
       C:\Users\&lt;用户名&gt;\.ollamamodels
      </code>
     </li>
     <li>
      配置文件：
      <code>
       C:\Users\&lt;用户名&gt;\AppData\Local\Ollama
      </code>
     </li>
    </ul>
    <p>
     💡 小贴士：建议通过环境变量
     <code>
      OLLAMA_MODELS
     </code>
     自定义模型存储路径，避免占用系统盘空间。
    </p>
    <h4>
     <a id="4___Ollama__74">
     </a>
     4. 配置 Ollama 环境变量
    </h4>
    <p>
     Ollama 提供了多种环境变量以供配置：
    </p>
    <ul>
     <li>
      OLLAMA_DEBUG：是否开启调试模式，默认为 false。
     </li>
     <li>
      OLLAMA_FLASH_ATTENTION：是否闪烁注意力，默认为 true。
     </li>
     <li>
      OLLAMA_HOST：Ollama 服务器的主机地址，默认为空。
     </li>
     <li>
      OLLAMA_KEEP_ALIVE：保持连接的时间，默认为 5m。
     </li>
     <li>
      OLLAMA_LLM_LIBRARY：LLM 库，默认为空。
     </li>
     <li>
      OLLAMA_MAX_LOADED_MODELS：最大加载模型数，默认为 1。
     </li>
     <li>
      OLLAMA_MAX_QUEUE：最大队列数，默认为空。
     </li>
     <li>
      OLLAMA_MAX_VRAM：最大虚拟内存，默认为空。
     </li>
     <li>
      OLLAMA_MODELS：模型目录，默认为空。
     </li>
     <li>
      OLLAMA_NOHISTORY：是否保存历史记录，默认为 false。
     </li>
     <li>
      OLLAMA_NOPRUNE：是否启用剪枝，默认为 false。
     </li>
     <li>
      OLLAMA_NUM_PARALLEL：并行数，默认为 1。
     </li>
     <li>
      OLLAMA_ORIGINS：允许的来源，默认为空。
     </li>
     <li>
      OLLAMA_RUNNERS_DIR：运行器目录，默认为空。
     </li>
     <li>
      OLLAMA_SCHED_SPREAD：调度分布，默认为空。
     </li>
     <li>
      OLLAMA_TMPDIR：临时文件目录，默认为空。
     </li>
     <li>
      OLLAMA_DEBUG：是否开启调试模式，默认为 false。
     </li>
     <li>
      OLLAMA_FLASH_ATTENTION：是否闪烁注意力，默认为 true。
     </li>
     <li>
      OLLAMA_HOST：Ollama 服务器的主机地址，默认为空。
     </li>
     <li>
      OLLAMA_KEEP_ALIVE：保持连接的时间，默认为 5m。
     </li>
     <li>
      OLLAMA_LLM_LIBRARY：LLM 库，默认为空。
     </li>
     <li>
      OLLAMA_MAX_LOADED_MODELS：最大加载模型数，默认为 1。
     </li>
     <li>
      OLLAMA_MAX_QUEUE：最大队列数，默认为空。
     </li>
     <li>
      OLLAMA_MAX_VRAM：最大虚拟内存，默认为空。
     </li>
     <li>
      OLLAMA_MODELS：模型目录，默认为空。
     </li>
     <li>
      OLLAMA_NOHISTORY：是否保存历史记录，默认为 false。
     </li>
     <li>
      OLLAMA_NOPRUNE：是否启用剪枝，默认为 false。
     </li>
     <li>
      OLLAMA_NUM_PARALLEL：并行数，默认为 1。
     </li>
     <li>
      OLLAMA_ORIGINS：允许的来源，默认为空。
     </li>
     <li>
      OLLAMA_RUNNERS_DIR：运行器目录，默认为空。
     </li>
     <li>
      OLLAMA_SCHED_SPREAD：调度分布，默认为空。
     </li>
     <li>
      OLLAMA_TMPDIR：临时文件目录，默认为空。
     </li>
    </ul>
    <h4>
     <a id="5__Ollama__109">
     </a>
     5. Ollama 常用命令
    </h4>
    <ol>
     <li>
      启动Ollama服务： ollama serve
     </li>
     <li>
      从模型文件创建模型： ollama create
     </li>
     <li>
      显示模型信息： ollama show
     </li>
     <li>
      运行模型： ollama run 模型名称
     </li>
     <li>
      从注册表中拉去模型： ollama pull 模型名称
     </li>
     <li>
      将模型推送到注册表： ollama push
     </li>
     <li>
      列出模型： ollama list
     </li>
     <li>
      复制模型： ollama cp
     </li>
     <li>
      删除模型： ollama rm 模型名称
     </li>
     <li>
      获取有关Ollama任何命令的帮助信息： ollama help
     </li>
    </ol>
    <h4>
     <a id="6__Ollama__121">
     </a>
     6. Ollama 终端指令
    </h4>
    <ol>
     <li>
      查看支持的指令：使用命令 /?
     </li>
     <li>
      退出对话模型：使用命令 /bye
     </li>
     <li>
      显示模型信息：使用命令 /show
     </li>
     <li>
      设置对话参数：使用命令 /set 参数名 参数值，例如设置温度（temperature）或top_k值
     </li>
     <li>
      清理上下文：使用命令 /clear
     </li>
     <li>
      动态切换模型：使用命令 /load 模型名称
     </li>
     <li>
      存储模型：使用命令 /save 模型名称
     </li>
     <li>
      查看快捷键：使用命令 /?shortcuts
     </li>
    </ol>
    <h3>
     <a id="_OpenWebUI_130">
     </a>
     🌐 OpenWebUI部署
    </h3>
    <pre><code class="prism language-python">Github：https<span class="token punctuation">:</span><span class="token operator">//</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span><span class="token builtin">open</span><span class="token operator">-</span>webui<span class="token operator">/</span><span class="token builtin">open</span><span class="token operator">-</span>webui
文档：https<span class="token punctuation">:</span><span class="token operator">//</span>docs<span class="token punctuation">.</span>openwebui<span class="token punctuation">.</span>com<span class="token operator">/</span>
</code></pre>
    <h4>
     <a id="1_Docker_137">
     </a>
     1. 安装Docker
    </h4>
    <ol>
     <li>
      访问
      <a href="https://www.docker.com/products/docker-desktop" rel="nofollow">
       Docker官网
      </a>
      下载Mac版本（Apple Silicon）
     </li>
     <li>
      安装并启动Docker Desktop
     </li>
     <li>
      配置国内镜像源加速下载（我这里
      <code>
       科学上网
      </code>
      不需要）
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/46b5f0a06cdc42d68e021ba16f7b4f36.png"/>
     </li>
    </ol>
    <p>
     💡 小贴士：Windows 安装 Docker 需要开启 Hyper-V（Windows专业版必需）
    </p>
    <h4>
     <a id="2_OpenWebUIhttpswwwopenwebuicom_146">
     </a>
     2. 部署
     <a href="https://www.openwebui.com/" rel="nofollow">
      OpenWebUI
     </a>
     （可视化大模型对话界面）
    </h4>
    <pre><code class="prism language-bash"><span class="token comment"># 拉取镜像 (直接run默认会拉取 latest 标签的镜像)</span>
<span class="token function">docker</span> pull ghcr.io/open-webui/open-webui:main

<span class="token comment">#（官方文档）以上是从 GitHub Container Registry (GHCR) 上拉取镜像，而不是从 Docker Hub。</span>
<span class="token comment"># 也可以docker hub 拉取 open-webui镜像</span>
<span class="token function">docker</span> pull dyrnq/open-webui:main
</code></pre>
    <p>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/361e58474834446cb840fdd80b23d184.png"/>
    </p>
    <pre><code class="prism language-bash"><span class="token comment"># 启动容器</span>
<span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 <span class="token punctuation">\</span>
  --add-host<span class="token operator">=</span>host.docker.internal:host-gateway <span class="token punctuation">\</span>
  <span class="token parameter variable">-v</span> open-webui:/app/backend/data <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span> open-webui <span class="token punctuation">\</span>
  <span class="token parameter variable">--restart</span> always <span class="token punctuation">\</span>
  ghcr.io/open-webui/open-webui:main
</code></pre>
    <p>
     访问
     <code>
      http://localhost:3000
     </code>
     即可使用Web界面。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/c0c9cf51ff6f4c3eb67d85f730ebaea0.png">
      创建账号，这个是本地账号，随便添加账号信息即可
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/9c2e50d44be84a0ba1c6dbe3a77dc5ac.png"/>
      <br/>
      选择ollama中的模型，聊天测试
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/3232f4ad8c4f4169acb63be0cc731375.png"/>
      <br/>
      也可以在这里直接拉取模型
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/ed3edcc4a96c4c11bca4b7f558c3816f.png"/>
      <br/>
      与下载的新模型进行对话
      <br/>
      <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/direct/fb56d70945eb4bebb33f88f36a7d0121.png"/>
      <br/>
      💡 小贴士：
     </img>
    </p>
    <ul>
     <li>
      注册时邮箱可以随便填写，设置密码后注意保存！
     </li>
     <li>
      ollama后台一定要运行着模型，如：ollama run phi-4
     </li>
    </ul>
    <h3>
     <a id="__179">
     </a>
     🔒 离线部署方案
    </h3>
    <h4>
     <a id="1__180">
     </a>
     1. 准备工作
    </h4>
    <h5>
     <a id="_Docker__181">
     </a>
     下载必要的 Docker 镜像
    </h5>
    <p>
     在有网络的环境中，需要下载以下镜像:
    </p>
    <pre><code class="prism language-bash"><span class="token comment"># 下载 Ollama 镜像</span>
<span class="token function">docker</span> pull ollama/ollama:latest

<span class="token comment"># 下载 OpenWebUI 镜像</span>
<span class="token function">docker</span> pull ghcr.io/open-webui/open-webui:main

<span class="token comment"># 保存镜像到文件</span>
<span class="token function">docker</span> save ollama/ollama:latest <span class="token parameter variable">-o</span> ollama.tar
<span class="token function">docker</span> save ghcr.io/open-webui/open-webui:main <span class="token parameter variable">-o</span> openwebui.tar
</code></pre>
    <h5>
     <a id="_197">
     </a>
     下载所需的模型文件
    </h5>
    <p>
     在有网络的环境中下载模型文件:
    </p>
    <pre><code class="prism language-bash"><span class="token comment"># 使用 Ollama 下载模型</span>
ollama pull llama3
<span class="token comment"># 模型文件位于 ~/.ollama/models 目录</span>
</code></pre>
    <h4>
     <a id="2__207">
     </a>
     2. 离线环境部署
    </h4>
    <h5>
     <a id="_Docker__209">
     </a>
     加载 Docker 镜像
    </h5>
    <pre><code class="prism language-bash"><span class="token comment"># 加载保存的镜像文件</span>
<span class="token function">docker</span> load <span class="token parameter variable">-i</span> ollama.tar
<span class="token function">docker</span> load <span class="token parameter variable">-i</span> openwebui.tar
</code></pre>
    <h5>
     <a id="_Docker__217">
     </a>
     创建 Docker 网络
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> network create ollama-network
</code></pre>
    <h5>
     <a id="_Ollama__223">
     </a>
     启动 Ollama 容器
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span> ollama <span class="token punctuation">\</span>
  <span class="token parameter variable">--network</span> ollama-network <span class="token punctuation">\</span>
  <span class="token parameter variable">-v</span> ~/.ollama:/root/.ollama <span class="token punctuation">\</span>
  <span class="token parameter variable">-p</span> <span class="token number">11434</span>:11434 <span class="token punctuation">\</span>
  ollama/ollama
</code></pre>
    <h5>
     <a id="_OpenWebUI__234">
     </a>
     启动 OpenWebUI 容器
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> run <span class="token parameter variable">-d</span> <span class="token punctuation">\</span>
  <span class="token parameter variable">--name</span> open-webui <span class="token punctuation">\</span>
  <span class="token parameter variable">--network</span> ollama-network <span class="token punctuation">\</span>
  <span class="token parameter variable">-p</span> <span class="token number">3000</span>:8080 <span class="token punctuation">\</span>
  <span class="token parameter variable">-e</span> <span class="token assign-left variable">OLLAMA_API_BASE_URL</span><span class="token operator">=</span>http://ollama:11434/api <span class="token punctuation">\</span>
  ghcr.io/open-webui/open-webui:main
</code></pre>
    <h4>
     <a id="3__245">
     </a>
     3. 验证部署
    </h4>
    <h5>
     <a id="_247">
     </a>
     检查容器状态
    </h5>
    <pre><code class="prism language-bash"><span class="token function">docker</span> <span class="token function">ps</span>
<span class="token function">docker</span> logs ollama
<span class="token function">docker</span> logs open-webui
</code></pre>
    <h5>
     <a id="_WebUI_255">
     </a>
     访问 WebUI
    </h5>
    <p>
     在浏览器中访问
     <code>
      http://localhost:3000
     </code>
    </p>
    <h4>
     <a id="4__259">
     </a>
     4. 故障排除
    </h4>
    <ol>
     <li>
      如果容器无法启动，检查日志:
     </li>
    </ol>
    <pre><code class="prism language-bash"><span class="token function">docker</span> logs ollama
<span class="token function">docker</span> logs open-webui
</code></pre>
    <ol start="2">
     <li>
      检查网络连接:
     </li>
    </ol>
    <pre><code class="prism language-bash"><span class="token comment"># 确保容器间可以通信</span>
<span class="token function">docker</span> network inspect ollama-network
</code></pre>
    <ol start="3">
     <li>
      检查模型文件:
     </li>
    </ol>
    <pre><code class="prism language-bash"><span class="token comment"># 进入 Ollama 容器检查模型文件</span>
<span class="token function">docker</span> <span class="token builtin class-name">exec</span> <span class="token parameter variable">-it</span> ollama <span class="token function">ls</span> /root/.ollama/models
</code></pre>
    <h4>
     <a id="5__282">
     </a>
     5. 注意事项
    </h4>
    <ol>
     <li>
      确保模型文件已正确复制到 Ollama 容器的对应目录
     </li>
     <li>
      检查磁盘空间是否充足，容器间的网络通信是关键，需要确保正确配置
     </li>
     <li>
      确保端口 3000 和 11434 未被占用
     </li>
     <li>
      容器的启动顺序很重要，必须先启动 Ollama 再启动 OpenWebUI
     </li>
     <li>
      目前也有捆绑安装的方式，将 Open WebUI 和 Ollama 打包在一个 Docker 容器中，通过一条命令就能同时安装和启动两者。
      <pre><code class="prism language-python">docker run <span class="token operator">-</span>d <span class="token operator">-</span>p <span class="token number">3000</span><span class="token punctuation">:</span><span class="token number">8080</span> <span class="token operator">-</span><span class="token operator">-</span>gpus<span class="token operator">=</span><span class="token builtin">all</span> <span class="token operator">-</span>v ollama<span class="token punctuation">:</span><span class="token operator">/</span>root<span class="token operator">/</span><span class="token punctuation">.</span>ollama <span class="token operator">-</span>v <span class="token builtin">open</span><span class="token operator">-</span>webui<span class="token punctuation">:</span><span class="token operator">/</span>app<span class="token operator">/</span>backend<span class="token operator">/</span>data <span class="token operator">-</span><span class="token operator">-</span>name <span class="token builtin">open</span><span class="token operator">-</span>webui <span class="token operator">-</span><span class="token operator">-</span>restart always ghcr<span class="token punctuation">.</span>io<span class="token operator">/</span><span class="token builtin">open</span><span class="token operator">-</span>webui<span class="token operator">/</span><span class="token builtin">open</span><span class="token operator">-</span>webui<span class="token punctuation">:</span>ollama
</code></pre>
     </li>
    </ol>
    <h3>
     <a id="__293">
     </a>
     ⚡ 性能优化建议
    </h3>
    <ol>
     <li>
      <p>
       <strong>
        内存管理
       </strong>
      </p>
      <ul>
       <li>
        关闭不必要的后台应用
       </li>
       <li>
        使用Activity Monitor监控内存使用
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        模型选择
       </strong>
      </p>
      <ul>
       <li>
        建议从小模型开始测试
       </li>
       <li>
        推荐模型大小顺序：
        <ul>
         <li>
          qwen2:0.5b (最轻量)
         </li>
         <li>
          llama2:7b (平衡型)
         </li>
         <li>
          codellama:7b (代码专用)
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li>
      <p>
       <strong>
        温度控制
       </strong>
      </p>
      <ul>
       <li>
        保持Mac Mini通风良好
       </li>
       <li>
        可使用监控工具观察CPU温度
       </li>
      </ul>
     </li>
    </ol>
    <h3>
     <a id="__310">
     </a>
     ❓ 常见问题
    </h3>
    <ol>
     <li>
      <p>
       Q: M4芯片能跑多大的模型？
       <br/>
       A: 16GB内存的M4可以流畅运行8B参数的模型，更大的模型可能会影响性能。
      </p>
     </li>
     <li>
      <p>
       Q: Llama中文支持不好怎么办？
       <br/>
       A: 可以使用
       <a href="https://github.com/LlamaFamily/Llama-Chinese">
        Llama-Chinese
       </a>
       等经过中文优化的模型。
      </p>
     </li>
     <li>
      <p>
       Q: OpenWebUI打不开怎么办？
       <br/>
       A: 检查Docker状态：
      </p>
      <pre><code class="prism language-bash"><span class="token function">docker</span> <span class="token function">ps</span>  <span class="token comment"># 查看容器状态</span>
<span class="token function">docker</span> logs open-webui  <span class="token comment"># 查看日志</span>
</code></pre>
     </li>
    </ol>
    <h3>
     <a id="__325">
     </a>
     🎉 结语
    </h3>
    <p>
     通过本教程的配置，你已经拥有了一个完全本地化的AI助手！有任何问题欢迎在评论区讨论，让我们一起探索AI的无限可能！
    </p>
    <hr/>
    <p>
     如果觉得这篇文章对你有帮助，别忘了点赞转发哦~ 👍
    </p>
    <p>
     你用Mac Mini跑过哪些AI模型？欢迎分享你的使用体验！💭
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34363434353039302f:61727469636c652f64657461696c732f313435343830393736" class_="artid" style="display:none">
 </p>
</div>


