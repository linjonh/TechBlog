---
layout: post
title: "自动驾驶的重要一环谈谈感知前沿技术"
date: 2024-12-03 07:31:32 +0800
description: "本文总结于Waymo研发经理周寅于2021年8月29日在深蓝学院的讲座。讲座内容主要包括自动驾驶系统"
keywords: "前沿传感器介绍"
categories: ['运动规划', '自动驾驶', '人工智能']
tags: ['自动驾驶', '移动机器人', '深度学习']
artid: "120141909"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=120141909
    alt: "自动驾驶的重要一环谈谈感知前沿技术"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     自动驾驶的重要一环：谈谈感知前沿技术
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p>
     本文总结于Waymo研发经理周寅于2021年8月29日在深蓝学院的讲座。讲座内容主要包括自动驾驶系统的总览，自动驾驶感知的介绍，以及感知的前沿动态和总结。
    </p>
    <h3>
     <strong>
      1.自动驾驶系统总览
     </strong>
    </h3>
    <p>
     关于自动驾驶系统,目前主流的L4级别自动驾驶系统通常包括预先提供的地图以及众多传感器,具体传感器系统包括激光雷达系统,雷达系统以及视觉系统。 无人驾驶的目的是通过这些输入来控制汽车的行驶。近些年，由于深度学习的兴起，我们在更多的模块部署了深度学习的模块，不过虽然端对端的深度学习有很美好的前景，但是与目前工业界模块化的设计方式相比，依然有不少差距。
    </p>
    <p style="text-align:center;">
     <img alt="å¾ç" height="139" src="https://i-blog.csdnimg.cn/blog_migrate/f13f8e024baa7aa79f57182ed03d9e3d.png" width="447"/>
    </p>
    <p>
    </p>
    <p style="text-align:center;">
     图一：无人车系统主流的模块化设计
    </p>
    <p>
     关于如今无人车系统的模块,输入通常包括高精地图以及各种传感器, 然后基于传感器和地图,我们就可以知道当前车辆的位置信息 (比如汽车位于哪个车道,前方是否有斑马线等), 接下来,我们就可以进行感知,了解周围的环境信息(比如车辆、行人、信号灯、施工障碍物等) 并规划路径进行驾驶。
    </p>
    <p>
     其中，感知通常是基于地图，因为地图能为我们提供很多的先验信息，比如红绿灯的位置,十字路口的位置等，基于定位和感知的结果，我们可以进行更准确的行为预测，目的是为了判断周围的物体未来可能会有怎样的行为或者运动。在这样的预测的基础下，我们接下来就可以实时地规划出车辆的最优路径。
    </p>
    <h3>
     <strong>
      2.自动驾驶感知的介绍
     </strong>
    </h3>
    <p>
     本次讲座，我们将主要关注感知的环节。关于感知，我们的输入就是各个传感器采集到的数据以及地图，输出则是对于周围环境的各种表征。关于自动驾驶感知的路线，目前比较主流的包括Waymo为代表的多传感器感知路线，以及Tesla的纯视觉感知路线。
    </p>
    <p>
     就个人观点，从无人驾驶的角度,我们希望达到最安全的驾驶等级, 然而不管哪种传感器,都存在一定的感知”盲区”。因此，我们需要传感器弥补其他传感器在特定场合的不足。比如激光雷达在雨天和雾天通常效果不佳，因此我们装配上摄像头和雷达来弥补激光雷达的这一不足。
    </p>
    <p>
     关于感知,学术界和工业界主要关注的包括六个问题：
    </p>
    <ol>
     <li>
      <strong>
       目标检测和跟踪
      </strong>
      。包括目标的大小朝向位姿；
     </li>
     <li>
      <strong>
       物体分割
      </strong>
      。在图像上我们需要对每个像素点标定类别，对于点云，我们需要对每个点标定类别；
     </li>
     <li>
      <strong>
       流估
      </strong>
      。我们想了解物体运动的趋势；
     </li>
     <li>
      <strong>
       深度估计
      </strong>
      。我们需要得到每个点的深度信息，这对于视觉感知非常重要；
     </li>
     <li>
      <strong>
       对于行人的姿态估计
      </strong>
      ， 我们希望清晰的了解到行人的意图和行为；
     </li>
     <li>
      <strong>
       高精地图的实时生成
      </strong>
      。
     </li>
    </ol>
    <p>
     在这些问题中, 目标检测是学术界和工业界共同关注的重点。关于目标检测的讨论，我们可以分为五个维度进行讨论：
    </p>
    <ol>
     <li>
      <strong>
       普适性
      </strong>
      ,也就是它是否能在各个场景下(比如极端天气,不同城市场景等)实现满意的检测识别效果；
     </li>
     <li>
      <strong>
       识别的质量和效果
      </strong>
      ，我们希望识别的效果尽可能的准确；
     </li>
     <li>
      <strong>
       运算效率
      </strong>
      ，我们希望模型不仅效果好，并且跑的快、占用的内存资源较少；
     </li>
     <li>
      <strong>
       数据标注的自动化
      </strong>
      ，我们希望用更低的人力和金钱和时间成本获得更有效的数据进行模型训练；
     </li>
     <li>
      <strong>
       数据的灵活性
      </strong>
      ，我们希望模型能够适应各种数据类型，比如仿真的模拟数据，经过压缩的数据等。
     </li>
    </ol>
    <h3>
     <strong>
      3.感知前沿介绍
     </strong>
    </h3>
    <p>
     在感知的五个维度中，检测质量，普适性，计算效率是和自动驾驶汽车的实时性能(onboard)息息相关的, 而其他的两个维度——数据灵活性和标注自动化通常可以线下（offboard）优化，也同样得到了越来越多的关注了。接下来我们也将就这几个维度分别介绍我们的工作。
    </p>
    <p style="text-align:center;">
     <img alt="å¾ç" height="195" src="https://i-blog.csdnimg.cn/blog_migrate/5eec4b7352bee7da20f517b1b4eae3b5.png" width="376"/>
    </p>
    <p style="text-align:center;">
     图二：自动驾驶感知的五个维度
    </p>
    <p>
     <strong>
      关于模型普适性,我们不久前发布在ICCV的论文Unsupervised Domain Adaptation for 3D Object Detection via Semantic Point Generation(简称SPG)就是关于这个主题的
     </strong>
     。 它实现了有效率少参数的模型，能够通过恢复激光雷达的扫描盲区而提高点云质量。同时，它可以在不需要任何额外训练数据的情况下，显著提高检测器在遮挡情况下和在雨天的性能。
    </p>
    <p style="text-align:center;">
     <img alt="图片" height="229" src="https://i-blog.csdnimg.cn/blog_migrate/fca2b9c70fd4acc198021ba2cbab7a0c.png" width="366"/>
    </p>
    <p style="text-align:center;">
     图三：雨天激光雷达的点云显示更加分散
    </p>
    <p>
     我们这篇文章的核心思想就是在有限或者不完整的点云输入的情况下，让模型自己去猜测点云的一些细节部分， 所以SPG可以被视作一个联合学习(joint learning)的框架。具体来说，它包括两个部分：分别是3D分割和前景形状恢复。3D分割意味着我们要分类每个前景点云的栅格，看它属于希望被检测的物体还是背景。对于物体的形状，我们会生成新的点来对它进行复原。
    </p>
    <p>
     所以，这个模型的流程包括以原始点云作为输入，通过前景点的生成，和原始点云进行叠加，再把增强的点云进行输入，用于目标检测。我们把点云恢复后的数据提供给Pointpilars, PV-RCNN等模型，这些模型的总体检测效果也得到了有效提升，并且在很多困难的检测场景中，模型的检测效果得到了更为显著的提升。对PV-RCNN而言，SPG带来的时耗增长在10%左右,还有进一步优化的空间。
    </p>
    <p style="text-align:center;">
     <img alt="图片" height="174" src="https://i-blog.csdnimg.cn/blog_migrate/b82e14e4cf00ecd8f34306e137cde2d2.png" width="422"/>
    </p>
    <p style="text-align:center;">
     图四：SPG通过稀疏点云恢复物体形状
    </p>
    <p>
     <strong>
      第二个工作是关于如何提高模型的性能。
     </strong>
     在3D-MAN: 3D Multi-frame Attention Network for Object Detection这个工作中，我们提出了用注意力机制来学习和融合多帧信息。我们用了一个有效的主干网络来提取潜在特征，再用注意力机制通过参考当前帧的信息和历史帧信息来调整特征。
    </p>
    <p style="text-align:center;">
     <img alt="图片" height="257" src="https://i-blog.csdnimg.cn/blog_migrate/e797a4d152dec350ddf02c62d2d71aae.png" width="281"/>
    </p>
    <p style="text-align:center;">
     图五：3D-MAN结构框架
    </p>
    <p>
     在Waymo数据集中，我们对于方法进行了测试。我们看到此方法让检测效果有了大幅度提升。
    </p>
    <p>
     <strong>
      第三个工作是关于如何提高模型的效率——RSN: Range Sparse Net for Efficient, Accurate LIDAR 3D Object Detection。
     </strong>
     这个工作的出发点是希望在提高检测效果的同时，保证运行速度和内存效率。
    </p>
    <p>
     这个工作的核心思想是利用点云的稀疏性，最大化的提高运行效率，节省运行时间，而方式是去除去背景部分的点云，从而最大程度的提高效率。这个方法的实现是把点云以Range Image的形式呈现，然后利用网络进行分割，把属于前景点云的特征提取出来。接下来，通过进一步的稀疏特征提取以及Box regression， 我们就可以得到检测结果。
    </p>
    <p>
     我们也把效果在Waymo数据集中进行了测试,发现相比于PV-RCNN,它在提高了精度的基础上,还非常显著地减少了延时。
    </p>
    <p>
     刚才讨论的三个话题都是关于线上改进模型的工作，而线上的效果提升非常依赖于车上的计算资源，并且只有很有限的实时信息。而对于线下的效果提升，我们可以拥有更多的计算资源，并且有多种传感器的全局信息，因此在这个大方向，也有很多的工作值得尝试。
    </p>
    <p>
     针对标注自动化，在Offboard 3D Object Detection from Point Cloud Sequences中，我们提出了一种自动数据标注的方法,它可以使用结构化的信息来提高自动标注和跟踪的效果，并且，我们把算法效果和人工标注的效果进行对比，两者已经非常接近。
    </p>
    <p>
     这一工作的动机在于人工标注通常非常的耗时耗力，20秒的数据可能需要一个专业标注员数天的时间标注。因此我们的思路是让机器处理绝大多数场景，只把很难处理的场景交给人工标注员去处理或者修饰。
    </p>
    <p>
     算法流程如下，对于每一帧我们都得到通过检测得到检测框的大小类别，并且进行跟踪，对于静态和动态物体，我们根据运动状态进行分类。接着基于PointNet进行处理,从而得到物体更精确的状态。
    </p>
    <p>
     结果证明,我们的自动标注结果相比于PointPillars和PV-RCNN,检测精度有了很大的提升。
    </p>
    <p style="text-align:center;">
     <img alt="图片" height="231" src="https://i-blog.csdnimg.cn/blog_migrate/92b3223536365dc5b79c745142c05fc1.png" width="384"/>
    </p>
    <p style="text-align:center;">
     图六：自动标注和PointPillars，PV-RCNN检测效果的对比
    </p>
    <p>
     关于数据的灵活性这一主题，我们在工作SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving中，目标是根据已有的相机和激光雷达数据，让车辆在一个全新的位姿情况下，也能得到接近真实的感知信息。为了实现这个目标，第一步是利用LiDAR和图片信息进行3D重建，然后利用对抗生成网络，我们可以实现效果的增强，包括前景和背景效果的提高。这个工作的应用包括：得到新的视角的图片信息以及实现对于场景中物体的挪动和方向调整。
    </p>
    <p style="text-align:center;">
     <img alt="图片" height="223" src="https://i-blog.csdnimg.cn/blog_migrate/40826282b5b92ce13efa21a3aa018abf.png" width="468"/>
    </p>
    <p style="text-align:center;">
     图七：利用SurfelGAN实现物体的调整
    </p>
    <h3 style="text-align:justify;">
     <strong>
      4.总结
     </strong>
    </h3>
    <p>
     我们接下来来总结一下今天的课程内容，在今天的课程中，我们了解了一个完整的自动驾驶车辆的系统架构和功能：包括定位，感知和行为预测以及规划。接下来，我们在感知模块展开更详细的介绍，并介绍了比较经典的研究方向。从五个方面，我们介绍了Waymo的五篇经典论文，包括线上和线下的感知，关于这两者的关系，我认为具有强耦合的联系，通过共同推进这两方面的进步，我们可以实现自动驾驶感知能力的进一步提高。
    </p>
    <p>
     感谢深蓝学员何常鑫同学的整理，非常感谢周寅博士对本文章的审核与修改。
    </p>
    <p style="text-align:justify;">
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f:672e6373646e2e6e65742f736f6172696e675f63617369612f:61727469636c652f64657461696c732f313230313431393039" class_="artid" style="display:none">
 </p>
</div>


