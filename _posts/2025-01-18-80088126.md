---
layout: post
title: 音视频开发第一篇音视频基础概念
date: 2025-01-18 22:20:08 +0800
categories: [音视频]
tags: [音视频编解码,音视频基础,PTS&amp;amp;DTS,IPB帧,FFmpeg]
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=80088126
    alt: 音视频开发第一篇音视频基础概念
artid: 80088126
render_with_liquid: false
---
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     音视频开发第一篇——音视频基础概念。
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     研究音视频的数字化技术之前，必须对声音和图像的的物理性质有基本的了解。音视频技术的本质就是
     <strong>
      声音和图像信息的采集、存储和回放
     </strong>
     。学习音视频的数字化技术，不能上来就去编解码，这样有点本末倒置。
    </p>
    <p>
     通过阅读本文，你将会回顾中学关于光学和声学部分知识，并了解音视频技术中的一些关键
     <strong>
      概念
     </strong>
     。
    </p>
    <h3 id="声音的物理特性">
     声音的物理特性
    </h3>
    <h5 id="声音的本质是波">
     声音的本质是波：
    </h5>
    <blockquote>
     <p>
      物体通过振动，对空气（传播介质）产生挤压，使空气有节奏的振动并产生疏密变化，从而形成疏密相间的纵波。
     </p>
    </blockquote>
    <h5 id="声音的三要素">
     声音的三要素
    </h5>
    <p>
     1、频率
    </p>
    <blockquote>
     <p>
      频率决定音调的高低
      <br/>
      频率与波长成反比，频率越低，波长越长。低频波传递距离较长（可对比，寺庙的钟声和轮船的鸣笛）。
     </p>
    </blockquote>
    <p>
     2、 振幅
    </p>
    <blockquote>
     <p>
      振幅决定音量
      <br/>
      音量是能量大小的表现，通常用分贝来表示音量大小，超过一定分贝的声音对人体是有害的。
     </p>
    </blockquote>
    <p>
     3.、波形
    </p>
    <blockquote>
     <p>
      波形决定音色
      <br/>
      不同的声源，在同等频率，同等振幅的情况下，声波的形状是不同的，从而发出的声音也是有差异的。
     </p>
    </blockquote>
    <h3 id="声音的数字化">
     声音的数字化
    </h3>
    <p>
     数字化音频技术，就是声音的采集、存储和回放。
    </p>
    <p>
     1、采样
    </p>
    <blockquote>
     <ol>
      <li>
       采样就是在时间轴上数字化声音信号。
      </li>
      <li>
       为了提高采样质量，根据
       <strong>
        采样定理
       </strong>
       ，需要按比声音最高频率高2倍以上的频率对声音进行采样。人耳能听到的声音频率范围20Hz～20KHz，所有典型的采样率为44.1kHz（表示一秒钟采集44100次数据）。
      </li>
     </ol>
    </blockquote>
    <p>
     2、量化
    </p>
    <blockquote>
     <p>
      量化就是每个具体采样点的声音在振幅轴上的数字化表示。
     </p>
    </blockquote>
    <p>
     3、编码
    </p>
    <blockquote>
     <p>
      通常所说的音频裸数据就是PCM（Pulse Codec Modulation脉冲编码调制）
     </p>
    </blockquote>
    <h4 id="pcm">
     PCM
    </h4>
    <blockquote>
     <p>
      PCM数据涉及四个概念：
      <br/>
      1、
      <strong>
       sampleFormat（采样格式
      </strong>
      ）：可理解为一多少字节存储声音，典型的量化格式为16bit。
      <br/>
      2、
      <strong>
       sampleRate（采样率）
      </strong>
      ：这就不用说了吧，典型的采样率为44.1KHz。
      <br/>
      3、
      <strong>
       channel（声道数）
      </strong>
      ：为了造成立体声效果，数字声音分为左、右两个声道。
      <br/>
      4、
      <strong>
       比特率
      </strong>
      ：对于数字音频而言，比特率是个关键概念。定义为：一秒时间内的比特数，用于衡量单位时间音频数据量的大小。
     </p>
    </blockquote>
    <h4 id="编码的必要性">
     编码的必要性
    </h4>
    <p>
     我们来计算一下一分钟未经编码的数据占有多少字节：
     <br/>
     以CD音质为例：
    </p>
    <blockquote>
     <p>
      CD 音质参数：
      <br/>
      <strong>
       采样格式（sampleFormat）
      </strong>
      为16byte（2字节）
      <br/>
      <strong>
       采样率（sampleRate）
      </strong>
      为44.1KHz
      <br/>
      <strong>
       声道数（channel）
      </strong>
      为2
      <br/>
      所以，一分钟的数据大小为：
     </p>
    </blockquote>
    <p>
     44100 * 16 * 2 * 60 / 8 / 1024 = 10.09 MB
    </p>
    <p>
     也许按照现在的存储技术，这个大小勉强能够接受，倒是如果要实时传输的话，那就太勉强了。所以必须对数据进行编码。
    </p>
    <h4 id="编码原理及其它">
     编码原理及其它
    </h4>
    <p>
     编码，也称为压缩编码。它的实质是通过特殊的算法压缩掉冗余信号。
    </p>
    <blockquote>
     <p>
      冗余信号： 指不能被人耳感知的信号。
     </p>
    </blockquote>
    <p>
     <strong>
      压缩比
     </strong>
     ： 是压缩编码的基本标准之一，小于1。压缩又分为有损压缩，和无损压缩。常用压缩格式中，常用有损压缩。压缩比越小，丢失信息越多，丢失的信息不可恢复。
    </p>
    <h4 id="常见的压缩算法">
     常见的压缩算法
    </h4>
    <h5 id="1wav编码">
     1、WAV编码
    </h5>
    <ul>
     <li>
      WAV编码有多种实现方式，其中一种实现是：在PCM数据格式前加上44字节，用于表示PCM的采样率、声道数、数据格式等。也就是，并不会对PCM数据进行压缩（所有实现都不压缩）。
     </li>
     <li>
      特点：音质好
     </li>
     <li>
      场合：用于多媒体开发的中间件、或音效素材。
     </li>
    </ul>
    <h5 id="2mp3编码">
     2、MP3编码
    </h5>
    <ul>
     <li>
      同样MP3也有多种编码实现，其中LAME编码中的高码率文件，音效非常接近WAV。
     </li>
     <li>
      特点：码率128Kbit/s以上的音频上压缩比较高，兼容性好。
     </li>
     <li>
      场合：高比特率下，对兼容性有要求的音乐
     </li>
    </ul>
    <h5 id="3aac编码">
     3、AAC编码
    </h5>
    <ul>
     <li>
      有损压缩技术，通过附加编码技术，有三种主要的版本：
      <br/>
      <ol>
       <li>
        LC-AAC: 应用于中高码率场景（&gt;= 80Kbit/s)
       </li>
       <li>
        HE-AAC： 应用于中低码率场景（&lt;= 80Kbit/s)
       </li>
       <li>
        HE-AAC v2: 应用于低码率场景（&lt;=48Kbit/s)
       </li>
      </ol>
     </li>
     <li>
      特点：在小于128Kbit/s码率下表现优异，常用于视频中的音频编码。
     </li>
     <li>
      场景：128Kbit/s下的音频编码，用于视频中的音频编码
     </li>
    </ul>
    <h5 id="4ogg编码">
     4、Ogg编码
    </h5>
    <ul>
     <li>
      一种非常好的编码，在各种码率下表现都十分优异，特别是低码率下。
     </li>
     <li>
      特点：可以用比MP3更小的码率实现比它更好的音质，中高码率编码表现也毫不逊色。但兼容性不好，不支持流媒体特性
     </li>
     <li>
      场景：语音聊天
     </li>
    </ul>
    <h3 id="图像的物理特性">
     图像的物理特性
    </h3>
    <p>
     白色光能被分解为多种色光，实验证明，红绿蓝三种色光无法分解，这就是我们说的三原色。
    </p>
    <p>
     另外，物体之所以能在我们的眼中呈像，是因为物体反射的光，进入了眼睛。但在视频技术中，却稍有不同。
    </p>
    <p>
     因为手机或者电脑屏幕都是自发光原，并不需要反射光。
    </p>
    <p>
     一块分辨率为1280 * 720的屏幕，水平方向有1280个像素点，竖直方向有720个像素点。每个像素点由三个子像素组成（有条件可以通过显微镜观察），分别表示RGB。屏幕显示图片时，将每一个像素点的RGB通道分别对应屏幕的子像素点，从而显示出照片。
    </p>
    <h3 id="视频的数字化">
     视频的数字化
    </h3>
    <h4 id="图像的数字表示方式一-rgb">
     图像的数字表示方式一： RGB
    </h4>
    <p>
     不管是通过常识，还是通过阅读本文，相信你已经知道任何图像都可以由
     <strong>
      RGB
     </strong>
     组成。
     <br/>
     每个像素点的子像素有两种表示：
    </p>
    <ul>
     <li>
      浮点表示：取值范围为0.0 ～ 1.0，常见于OpenGL中的子像素表示。
     </li>
     <li>
      整数表示：取之范围为0 ～ 255或者00 ～ FF，8个bit表示一个子像素。常见的格式有
      <strong>
       RGBA-8888
      </strong>
      、Android平台上的
      <strong>
       RGB-565
      </strong>
      。
     </li>
    </ul>
    <p>
     对于一般图像，通常使用整数表示。如计算一张分辨率为1280 * 720，格式为
     <strong>
      RGBA-8888
     </strong>
     的图像大小：
     <br/>
     1280 * 720 * 4 = 3.516MB
    </p>
    <blockquote>
     <p>
      <strong>
       RGBA-8888
      </strong>
      格式：一个字节表示透明度三个字节表示RGB分量。
     </p>
    </blockquote>
    <p>
     以上计算出的大小，就是位图（bitmap）在内存中占据的大小。因为数据量较大，不利于网络传输。所以就有了各种压缩格式。
    </p>
    <h4 id="图像的数字表示方式二yuv">
     图像的数字表示方式二：YUV
    </h4>
    <p>
     对于视频而言，它的裸数据更多的使用
     <strong>
      YUV
     </strong>
     格式表示。和
     <strong>
      RGB
     </strong>
     比较，最大的优点在于占用较少的频宽（
     <strong>
      RGB
     </strong>
     要求三个独立的视频数据分量同时传输），另外
     <strong>
      YUV
     </strong>
     可以很好的向黑白电视兼容。
     <br/>
     其中：
    </p>
    <ul>
     <li>
      <strong>
       Y
      </strong>
      ：（拉丁文
      <strong>
       Luminance或Luma
      </strong>
      )表示亮度分量,通常称为亮度分量或者灰度。
     </li>
     <li>
      <strong>
       U和V
      </strong>
      ： 表示色度（
      <strong>
       Chrominance或Chroma
      </strong>
      ），作用是描述色彩和饱和度，用于指定颜色。
     </li>
    </ul>
    <p>
     <strong>
      Y
     </strong>
     亮度分量的建立，是通过叠加
     <strong>
      RGB
     </strong>
     输入信号的特定部分完成。
     <br/>
     <strong>
      U和V
     </strong>
     色度分量，定义了色调和饱和度两方面，分别用
     <strong>
      Cr
     </strong>
     和
     <strong>
      Cb
     </strong>
     表示。
     <strong>
      Cr
     </strong>
     反映
     <strong>
      RGB
     </strong>
     输入信号
     <strong>
      红色部分
     </strong>
     和亮度值之间的差异。
     <strong>
      Cb
     </strong>
     则反映
     <strong>
      RGB
     </strong>
     输入信号
     <strong>
      蓝色
     </strong>
     部分和亮度值之间的差异。
    </p>
    <p>
     <strong>
      YUV
     </strong>
     格式表示的数据，
     <strong>
      Y
     </strong>
     分量和
     <strong>
      U
     </strong>
     、
     <strong>
      V
     </strong>
     是分离的。只有
     <strong>
      Y
     </strong>
     分量的数据，表现出来就是黑白视频，这正是
     <strong>
      YUV
     </strong>
     格式能兼容黑白电视的原因。
    </p>
    <p>
     <strong>
      YUV
     </strong>
     常用的格式是
     <strong>
      4 ：2 ：0
     </strong>
     （关于
     <strong>
      YUV
     </strong>
     格式的种类和计算方式，以后单独开篇讲解）。
     <br/>
     <strong>
      Y、U、V
     </strong>
     都是使用8个bit表示。
    </p>
    <h4 id="视频编码方式">
     视频编码方式
    </h4>
    <p>
     和音频数据相似，视频的编码也是通过去除冗余数据实现。不同数据在于，视频数据在时间和空间上有较强的相关性。所以这些冗余信息包括
     <strong>
      时间冗余
     </strong>
     和
     <strong>
      空间冗余
     </strong>
     。
    </p>
    <h5 id="帧间编码">
     帧间编码
    </h5>
    <blockquote>
     <p>
      <strong>
       帧内编码
      </strong>
      用于去除时间冗余。关于
      <strong>
       帧间
      </strong>
      编码技术实现细节，可以先熟悉一下概念，暂时不用了解细节，这将在以后介绍。
     </p>
    </blockquote>
    <p>
     <strong>
      帧间编码
     </strong>
     技术，是去除
     <strong>
      时间冗余
     </strong>
     的方式，包括以下方面：
     <br/>
     *
     <strong>
      运动补偿
     </strong>
     ： 通过之前的图像来预测、补偿当前图像，是减少帧序列冗余信息的有效方法。
     <br/>
     *
     <strong>
      运动表示
     </strong>
     ： 不同区域的图像需要使用不同的运动适量来描述运动信息。
     <br/>
     *
     <strong>
      运动估计
     </strong>
     ： 是一中从视频序列中抽取运动信息的一整套技术。
    </p>
    <h5 id="帧内编码">
     帧内编码
    </h5>
    <blockquote>
     <p>
      <strong>
       帧内编码
      </strong>
      用于去除空间冗余。关于
      <strong>
       帧内
      </strong>
      编码技术实现细节，可以先熟悉一下概念，暂时不用了解细节，这将在以后介绍。
     </p>
    </blockquote>
    <p>
     <strong>
      帧内编码
     </strong>
     编码标准有很多，且都需要大量篇幅介绍，这里只作大致介绍。一类是
     <strong>
      MPEG
     </strong>
     ,主要包括四个版本：1、
     <strong>
      Mpeg1(用于VCD）
     </strong>
     。2、
     <strong>
      Mpeg2(用于DVD）
     </strong>
     。3、
     <strong>
      Mpeg4(现在流行的流媒体）
     </strong>
     。第二类是
     <strong>
      H.26*
     </strong>
     系列，包括
     <strong>
      H264
     </strong>
     。
    </p>
    <h5 id="编码中的重要概念">
     编码中的重要概念
    </h5>
    <ol>
     <li>
      <strong>
       IPB帧
      </strong>
      <br/>
      <ol>
       <li>
        <strong>
         I帧
        </strong>
        ：
        <strong>
         帧内编码帧（intra picture)
        </strong>
        ,通常是每个
        <strong>
         GOP
        </strong>
        （MPEG使用的一种视频压缩技术）的第一帧，经过适当的压缩，作为随机访问的参考点，可以当作静态图像。
        <strong>
         I帧
        </strong>
        可以得到
        <strong>
         6：1
        </strong>
        的压缩比，而不造成图像模糊，可以去除空间冗余。
        <strong>
         I帧
        </strong>
        可理解为一张独立完整的视频画面，只是进行了空间冗余的压缩而已。
       </li>
       <li>
        <strong>
         P帧
        </strong>
        ：
        <strong>
         前向预测帧（predictive-frame）
        </strong>
        ，通过图像序列中，前面已编码帧的时间冗余信息的去除来压缩数据量的编码图像，也称为
        <strong>
         预测帧
        </strong>
        。
        <strong>
         P帧
        </strong>
        可理解为需要前一个
        <strong>
         I帧
        </strong>
        或
        <strong>
         P帧
        </strong>
        来解码才能得到一张完成视频画面。
       </li>
       <li>
        <strong>
         B帧
        </strong>
        ：
        <strong>
         双向预测内插编码帧（bi-directional interpolated prediction frame)
        </strong>
        , 即考虑图像序列前已编码帧，也参照图像序列后已编码帧的时间冗余信息，来压缩数据量，也称为
        <strong>
         双向预测帧
        </strong>
        。
        <strong>
         B帧
        </strong>
        可理解为需要曹考前一个
        <strong>
         I帧
        </strong>
        或
        <strong>
         P帧
        </strong>
        ，以及后一个
        <strong>
         P帧
        </strong>
        生成一张完整的视频画面。
       </li>
       <li>
        <strong>
         IDR帧
        </strong>
        ：（
        <strong>
         instantaneous decoding refresh picture
        </strong>
        ),在
        <strong>
         H264
        </strong>
        编码中出现的概念，类似
        <strong>
         I帧
        </strong>
        ，区别在于：
        <strong>
         H264
        </strong>
        采用多帧预测，
        <strong>
         I帧
        </strong>
        之后的
        <strong>
         P帧
        </strong>
        可能参考
        <strong>
         I帧
        </strong>
        之前的帧才能解析完整图像，所以在随机访问中，就不能以
        <strong>
         I帧
        </strong>
        作为参考条件。而
        <strong>
         IDR帧
        </strong>
        就是一种特殊的
        <strong>
         I帧
        </strong>
        ，这一帧后的所有帧只会参考它，而不会参考前面的帧。在编码器中，一旦接收到一个
        <strong>
         IDR帧
        </strong>
        ，就会立即清理参考帧缓冲区，并将这个
        <strong>
         IDR帧
        </strong>
        作为参考帧使用。
       </li>
      </ol>
     </li>
     <li>
      <strong>
       PTS和DTS
      </strong>
      <br/>
      <strong>
       PTS
      </strong>
      英文全称为
      <strong>
       Presentation Time Stamp
      </strong>
      ,
      <strong>
       DTS
      </strong>
      英文全称为
      <strong>
       Decoding Time Stamp
      </strong>
      ，都是时间戳的概念。
      <br/>
      简单起见，先来介绍一下
      <strong>
       FFmpeg
      </strong>
      中关于这两者的概念。
      <strong>
       FFmpeg
      </strong>
      使用
      <strong>
       AVPacket
      </strong>
      表示解码前或编码后的压缩数据，用
      <strong>
       AVFrame
      </strong>
      表示解码后或编码前的原始数据。对于视频而言，
      <strong>
       AVFrame
      </strong>
      就是以帧图像，至于什么时候显示给用户，取决于它的
      <strong>
       PTS
      </strong>
      。
      <strong>
       DTS
      </strong>
      是
      <strong>
       AVPacket
      </strong>
      的一个成员，表示该压缩包需要什么时候被解码，如果视频中的各帧是按顺序输入，那么解码时间和显示时间一致，但实际上大多数解码标准，编码顺序和输入顺序并不一致，于是就出现了两种不同的时间戳。
     </li>
     <li>
      <p>
       <strong>
        GOP
       </strong>
       <br/>
       英文全称为
       <strong>
        Group Of Picture
       </strong>
       ,意思是，两个
       <strong>
        I帧
       </strong>
       之间形成的一组图片。通常在为解码器设置参数时，需要指定
       <strong>
        gop_size
       </strong>
       的值，因为
       <strong>
        I帧
       </strong>
       的压缩率是最低的，对一个视频源而言，
       <strong>
        gop_size
       </strong>
       越大，相对来说
       <strong>
        I帧
       </strong>
       就越少，节约出来的空间就可以保存更多的
       <strong>
        I帧
       </strong>
       ，所以画质就会越好。所以，应该根据业务场景，选择适当的
       <strong>
        gop_size
       </strong>
       值，从而提高视频质量。
      </p>
      <blockquote>
       <p>
        常见的压缩率：
        <br/>
        <strong>
         I帧
        </strong>
        ： 7
        <br/>
        <strong>
         P帧
        </strong>
        ： 20
        <br/>
        <strong>
         B帧
        </strong>
        ： 50
       </p>
      </blockquote>
     </li>
    </ol>
    <h5 id="mux和demux">
     MUX和DEMUX
    </h5>
    <p>
     <strong>
      mux
     </strong>
     的全称是
     <strong>
      multiplex
     </strong>
     ，译为
     <strong>
      多路传输
     </strong>
     。在音视频中，其实就是
     <strong>
      混流
     </strong>
     。意思是将多路流包括音频、视频流混合/封装到一个流文件中。
    </p>
    <p>
     <strong>
      demux
     </strong>
     自然就是相反的意思，表示
     <strong>
      分流
     </strong>
     ，意思是将经过混流后的流文件拆分开来，方便后续处理。
    </p>
    <p>
     <strong>
      muxer
     </strong>
     和
     <strong>
      demuxer
     </strong>
     ，则可以分别表示为
     <strong>
      混流器
     </strong>
     、
     <strong>
      分流器
     </strong>
     。
    </p>
    <p>
     <strong>
      muxing
     </strong>
     与
     <strong>
      demuxing
     </strong>
     加上ing后缀，表示动作，可以分别理解为
     <strong>
      混流操作
     </strong>
     和
     <strong>
      分流操作
     </strong>
     。
    </p>
    <blockquote>
     <p>
      编解码后的音频、视频、字幕等流文件通过混流器封装（打包）到一个文件中，同时作为数据传输，传输完毕后，又通过分流器对数据进行解封（拆包），将音频、视频、字幕等数据分流出来，进一步处理。
     </p>
    </blockquote>
    <h3 id="结语">
     结语
    </h3>
    <p>
     如有任何有描述不清，或者有误之处，欢迎流言交流。更音视频相关内容将持续更新，敬请期待。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
</div>


