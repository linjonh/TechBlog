---
layout: post
title: "使用Python做中文分词和绘制词云"
date: 2025-01-02 08:53:20 +0800
description: "使用Python做中文分词和绘制词云李小璐出轨云词图作为一门编程语言，Python的编写简单，支持库"
keywords: "分词与词云实验项目"
categories: ['数据分析']
tags: ['自然语言处理', '社交网络', '云词图', 'Python']
artid: "79004761"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=79004761
    alt: "使用Python做中文分词和绘制词云"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     使用Python做中文分词和绘制词云
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <h2 id="使用python做中文分词和绘制词云">
     使用Python做中文分词和绘制词云
    </h2>
    <div align="center">
     <img alt="cloud" src="https://img-my.csdn.net/uploads/201801/08/1515382986_7219.png" width="50%">
      <p align="center">
       李小璐出轨云词图
      </p>
     </img>
    </div>
    <p>
     作为一门编程语言，
     <strong>
      Python
     </strong>
     的编写简单，支持库强大，应用场景多，越来越多的人开始将它作为自己的编程入门语言。
    </p>
    <p>
     <strong>
      Python
     </strong>
     一个比较重要的场景是做舆情分析，比如分析社交网络上群众对某一话题的态度，分析股民的情绪作为投资参考等。最近笔者也做了一些舆情分析（八卦）方面的工作，一个完整的分析流程包括：
    </p>
    <ol>
     <li>
      数据获取：使用爬虫在相关网站上获取文本内容
     </li>
     <li>
      数据清洗：按一定格式对文本数据进行清洗和提取（文本分类，贴标签）
     </li>
     <li>
      数据呈现：多维度呈现和解读数据（计算，做表，画图）
     </li>
    </ol>
    <p>
     今天呢，作为系列分享之一，笔者就以李小璐出轨事件为例，使用从知乎热门回答上爬取的
     <strong>
      中文文本
     </strong>
     数据，绘制
     <strong>
      云词图
     </strong>
     。
    </p>
    <h3 id="中文分词">
     中文分词
    </h3>
    <p>
     所谓分词即是将文本序列按完整的意思切分成一个一个的词儿，方便进行下一步的分析（词频统计，情感分析等）。
    </p>
    <p>
     由于英文词与词自带空格作为分隔符，相比于中文分词要简单的多。我们在做中文分词时，需要把词语从一整段话中筛出来，困难之处在于，汉语表达博大精深，一段话往往有不同的切分方法。
    </p>
    <p>
     所幸这不是我们需要担心的，Python中的Jieba库提供了现成的解决方案：
    </p>
    <pre class="prettyprint"><code class="language-python hljs"><span class="hljs-keyword">import</span> jieba
text=<span class="hljs-string">"李小璐给王思聪买了微博热搜"</span>
result=jieba.cut(text)
print(<span class="hljs-string">"切分结果:  "</span>+<span class="hljs-string">","</span>.join(result))</code></pre>
    <p>
     jiaba调用了自己的分词算法，将切分好的文本按逗号分隔符分开，得到下面结果
    </p>
    <pre class="prettyprint"><code class="hljs">切分结果:  李小璐,给,王思聪,买,了,微博热,搜,。</code></pre>
    <p>
     可见切分结果不尽如人意，比较明显的是，“微博”，“热搜”就没有被识别出来，其次有一些词，比如“了” “买”以及标点符号，显然我们不想让这些词出现在云词图里，因为它们本身没什么意义。
    </p>
    <h5 id="特殊名词">
     特殊名词
    </h5>
    <p>
     对于某些特别的名词，为了使得其切分时不被分开，我们可以选择在切分前强调一下这些名词，比如：
    </p>
    <pre class="prettyprint"><code class="hljs perl">text=<span class="hljs-string">"李小璐给王思聪买了微博热搜"</span>
<span class="hljs-comment">#强调特殊名词</span>
jieba.suggest_fre<span class="hljs-string">q(('微博')</span>, True)
jieba.suggest_fre<span class="hljs-string">q(('热搜')</span>, True)
result=jieba.cut(text)
<span class="hljs-keyword">print</span>(<span class="hljs-string">"切分结果:  "</span>+<span class="hljs-string">","</span>.<span class="hljs-keyword">join</span>(result))</code></pre>
    <pre class="prettyprint"><code class="hljs">切分结果:  李小璐,给,王思聪,买,了,微博,热搜,。</code></pre>
    <p>
     还可以将特殊用词加入用户自定义词典，实现相同的效果：
    </p>
    <pre class="prettyprint"><code class="hljs avrasm">jieba<span class="hljs-preprocessor">.load</span>_userdict(<span class="hljs-string">"./utils/jieba_user_dict.txt"</span>)</code></pre>
    <h5 id="文本清洗">
     文本清洗
    </h5>
    <ol>
     <li>
      切分之后一些特殊的符号会单独成词，这些词会影响我们之后的分析。这里我们可以使用一个标点符号库
      <a href="http://download.csdn.net/download/weixin_37986926/10194654">
       <em>
        stopwords.txt
       </em>
      </a>
      （点击下载），将切分出来的特殊符号剔除掉。
     </li>
     <li>
      对于“了”，“的”这样长度为一的词，显然对我们分析文本没有任何帮助。处理的方法为将长度为1的词全部剔除掉。
     </li>
    </ol>
    <h5 id="实现代码">
     实现代码
    </h5>
    <pre class="prettyprint"><code class="language-python hljs"><span class="hljs-keyword">import</span> jieba
<span class="hljs-comment">#读取标点符号库</span>
f=open(<span class="hljs-string">"utils/stopwords.txt"</span>,<span class="hljs-string">"r"</span>)
stopwords={}.fromkeys(f.read().split(<span class="hljs-string">"\n"</span>))
f.close()
<span class="hljs-comment">#加载用户自定义词典</span>
jieba.load_userdict(<span class="hljs-string">"./utils/jieba_user_dict.txt"</span>)
segs=jieba.cut(text)
mytext_list=[]
<span class="hljs-comment">#文本清洗</span>
<span class="hljs-keyword">for</span> seg <span class="hljs-keyword">in</span> segs:
    <span class="hljs-keyword">if</span> seg <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords <span class="hljs-keyword">and</span> seg!=<span class="hljs-string">" "</span> <span class="hljs-keyword">and</span> len(seg)!=<span class="hljs-number">1</span>:
        mytext_list.append(seg.replace(<span class="hljs-string">" "</span>,<span class="hljs-string">""</span>))
cloud_text=<span class="hljs-string">","</span>.join(mytext_list)</code></pre>
    <h3 id="绘制云词图">
     绘制云词图
    </h3>
    <p>
     做好了中文分词，下一步即是绘制云词图了。这里我们使用了另一个比较强大的库
     <strong>
      WordCloud
     </strong>
     。
    </p>
    <pre class="prettyprint"><code class="language-python hljs"><span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud
wc = WordCloud(
    background_color=<span class="hljs-string">"white"</span>, <span class="hljs-comment">#背景颜色</span>
    max_words=<span class="hljs-number">200</span>, <span class="hljs-comment">#显示最大词数</span>
    font_path=<span class="hljs-string">"./font/wb.ttf"</span>,  <span class="hljs-comment">#使用字体</span>
    min_font_size=<span class="hljs-number">15</span>,
    max_font_size=<span class="hljs-number">50</span>, 
    width=<span class="hljs-number">400</span>  <span class="hljs-comment">#图幅宽度</span>
    )
wc.generate(cloud_text)
wc.to_file(<span class="hljs-string">"pic.png"</span>)
</code></pre>
    <p>
     运行以上代码就可以直接出图了：
    </p>
    <p>
     <img alt="cloud" src="https://img-my.csdn.net/uploads/201801/08/1515398736_7418.png" title=""/>
    </p>
    <h5 id="参数说明">
     参数说明
    </h5>
    <p>
     WordCloud在生成对象时，提供了多个参数：
    </p>
    <pre class="prettyprint"><code class="hljs 1c">Parameters
 <span class="hljs-string">|  ----------</span>
 <span class="hljs-string">|  font_path : string</span>
 <span class="hljs-string">|       使用的字体库</span>
 <span class="hljs-string">|  width : int (default=400)</span>
 <span class="hljs-string">|      图片宽度</span>
 <span class="hljs-string">|  height : int (default=200)</span>
 <span class="hljs-string">|      图片高度</span>
 <span class="hljs-string">|  mask : nd-array or None (default=None)</span>
 <span class="hljs-string">|      图片背景参考形状  </span>
 <span class="hljs-string">|  scale : float (default=1)</span>
 <span class="hljs-string">|      图幅放大、缩小系数  </span>
 <span class="hljs-string">|  min_font_size : int (default=4)</span>
 <span class="hljs-string">|      最小的字符</span>
 <span class="hljs-string">|  min_font_size : int (default=4)</span>
 <span class="hljs-string">|      最大的字符</span>
 <span class="hljs-string">|  max_words : number (default=200)</span>
 <span class="hljs-string">|      最多显示的词数</span>
 <span class="hljs-string">|  stopwords : set of strings or None</span>
 <span class="hljs-string">|      不需要显示的词</span>
 <span class="hljs-string">|  background_color : color value (default="</span>black<span class="hljs-string">")</span>
 <span class="hljs-string">|      背景颜色</span>
 <span class="hljs-string">|  ......</span></code></pre>
    <h5 id="绘制指定形状的云词图">
     绘制指定形状的云词图
    </h5>
    <p>
     <img alt="cloud" src="https://img-my.csdn.net/uploads/201801/08/1515399447_1460.png" width="50%"/>
    </p>
    <p>
     在绘制云词图的时候，可以通过
     <strong>
      WordCloud
     </strong>
     的mask参数，可以指定词图的轮廓，绘制成各种形状的图片。比如我们想用以上这张图片，
     <strong>
      WordCloud
     </strong>
     会识别出除纯白的的部分作为轮廓。具体实现如下：
    </p>
    <pre class="prettyprint"><code class="hljs python"><span class="hljs-comment">#加载需要使用的类库</span>
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> wordcloud <span class="hljs-keyword">import</span> WordCloud, ImageColorGenerator
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-comment">#加载背景图片</span>
cloud_mask = np.array(Image.open(<span class="hljs-string">"./bc_img/heart.jpeg"</span>))
<span class="hljs-comment">#忽略显示的词</span>
st=set([<span class="hljs-string">"东西"</span>,<span class="hljs-string">"这是"</span>])
<span class="hljs-comment">#生成wordcloud对象</span>
wc = WordCloud(background_color=<span class="hljs-string">"white"</span>, 
    mask=cloud_mask,
    max_words=<span class="hljs-number">200</span>,
    font_path=<span class="hljs-string">"./font/wb.ttf"</span>,
    min_font_size=<span class="hljs-number">15</span>,
    max_font_size=<span class="hljs-number">50</span>, 
    width=<span class="hljs-number">400</span>, 
    stopwords=st)
wc.generate(cloud_text)
wc.to_file(<span class="hljs-string">"pic.png"</span>)</code></pre>
    <p>
     调用以上代码就得到了一张心形状的云词图：
    </p>
    <div align="center">
     <img alt="cloud" src="https://img-my.csdn.net/uploads/201801/08/1515383653_7394.png" width="70%">
     </img>
    </div>
    <p>
     使用PS稍加处理，就得到了下面这张图~
    </p>
    <div align="center">
     <img alt="cloud" src="https://img-my.csdn.net/uploads/201801/08/1515382986_7219.png" width="70%">
      <p align="center">
       李小璐出轨云词图
      </p>
     </img>
    </div>
    <p>
     关注 “果果数据”公众号，获取全部代码，还可以看到更多教程O(∩_∩)O~
    </p>
    <p>
     <img alt="果果" src="https://img-my.csdn.net/uploads/201801/08/1515401204_4580.jpeg" title=""/>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f67:2e6373646e2e6e65742f77656978696e5f3337393836393236:2f61727469636c652f64657461696c732f3739303034373631" class_="artid" style="display:none">
 </p>
</div>


