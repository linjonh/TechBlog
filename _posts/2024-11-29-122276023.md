---
layout: post
title: "数据挖掘经典算法一K近邻KNN"
date: 2024-11-29 19:10:24 +0800
description: "目录一、算法思想二、算法原理（1）KNN算法原理（2）KNN算法三要素① 分类决策规则② K值的选择"
keywords: "knn搜索算法"
categories: ["未分类"]
tags: ["近邻算法", "算法", "数据挖掘", "Python"]
artid: "122276023"
image:
  path: https://api.vvhan.com/api/bing?rand=sj&artid=122276023
  alt: "数据挖掘经典算法一K近邻KNN"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     数据挖掘经典算法（一）：K近邻（KNN）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="htmledit_views" id="content_views">
    <p id="main-toc">
     <strong>
      目录
     </strong>
    </p>
    <p id="%E4%B8%80%E3%80%81%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3-toc" style="margin-left:0px;">
     <a href="#%E4%B8%80%E3%80%81%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3" rel="nofollow">
      一、算法思想
     </a>
    </p>
    <p id="%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86-toc" style="margin-left:0px;">
     <a href="#%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" rel="nofollow">
      二、算法原理
     </a>
    </p>
    <p id="%EF%BC%881%EF%BC%89KNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86-toc" style="margin-left:40px;">
     <a href="#%EF%BC%881%EF%BC%89KNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86" rel="nofollow">
      （1）KNN算法原理
     </a>
    </p>
    <p id="%EF%BC%882%EF%BC%89KNN%E7%AE%97%E6%B3%95%E4%B8%89%E8%A6%81%E7%B4%A0-toc" style="margin-left:40px;">
     <a href="#%EF%BC%882%EF%BC%89KNN%E7%AE%97%E6%B3%95%E4%B8%89%E8%A6%81%E7%B4%A0" rel="nofollow">
      （2）KNN算法三要素
     </a>
    </p>
    <p id="%E2%91%A0%20%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99-toc" style="margin-left:80px;">
     <a href="#%E2%91%A0%20%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99" rel="nofollow">
      ① 分类决策规则
     </a>
    </p>
    <p id="%E2%91%A1%20K%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9-toc" style="margin-left:80px;">
     <a href="#%E2%91%A1%20K%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9" rel="nofollow">
      ② K值的选择
     </a>
    </p>
    <p id="%E2%91%A2%20%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F-toc" style="margin-left:80px;">
     <a href="#%E2%91%A2%20%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F" rel="nofollow">
      ③ 距离度量
     </a>
    </p>
    <p id="%EF%BC%883%EF%BC%89KNN%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95-toc" style="margin-left:40px;">
     <a href="#%EF%BC%883%EF%BC%89KNN%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95" rel="nofollow">
      （3）KNN实现方法
     </a>
    </p>
    <p id="%C2%A0%E2%91%A0%20%E6%9E%84%E5%BB%BAKD%E6%A0%91-toc" style="margin-left:80px;">
     <a href="#%C2%A0%E2%91%A0%20%E6%9E%84%E5%BB%BAKD%E6%A0%91" rel="nofollow">
      ① 构建KD树
     </a>
    </p>
    <p id="%C2%A0%E2%91%A1%20KD%E6%A0%91%E7%9A%84%E6%90%9C%E7%B4%A2-toc" style="margin-left:80px;">
     <a href="#%C2%A0%E2%91%A1%20KD%E6%A0%91%E7%9A%84%E6%90%9C%E7%B4%A2" rel="nofollow">
      ② KD树的搜索
     </a>
    </p>
    <p id="%E2%91%A2%20KD%E6%A0%91%E7%9A%84%E5%88%86%E7%B1%BB%EF%BC%88%E9%A2%84%E6%B5%8B%EF%BC%89-toc" style="margin-left:80px;">
     <a href="#%E2%91%A2%20KD%E6%A0%91%E7%9A%84%E5%88%86%E7%B1%BB%EF%BC%88%E9%A2%84%E6%B5%8B%EF%BC%89" rel="nofollow">
      ③ KD树的分类（预测）
     </a>
    </p>
    <p id="%EF%BC%884%EF%BC%89KNN%E7%AE%97%E6%B3%95%E4%B9%8B%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%83%85%E5%86%B5-toc" style="margin-left:40px;">
     <a href="#%EF%BC%884%EF%BC%89KNN%E7%AE%97%E6%B3%95%E4%B9%8B%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%83%85%E5%86%B5" rel="nofollow">
      （4）KNN算法之训练样本不平衡情况
     </a>
    </p>
    <p id="%EF%BC%885%EF%BC%89%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9-toc" style="margin-left:40px;">
     <a href="#%EF%BC%885%EF%BC%89%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9" rel="nofollow">
      （5）算法优缺点
     </a>
    </p>
    <p id="%E4%B8%89%E3%80%81%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C-toc" style="margin-left:0px;">
     <a href="#%E4%B8%89%E3%80%81%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C" rel="nofollow">
      三、算法实验
     </a>
    </p>
    <p id="%EF%BC%881%EF%BC%89%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E4%B8%8E%E5%A4%84%E7%90%86-toc" style="margin-left:40px;">
     <a href="#%EF%BC%881%EF%BC%89%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E4%B8%8E%E5%A4%84%E7%90%86" rel="nofollow">
      （1）数据收集与处理
     </a>
    </p>
    <p id="%C2%A0%EF%BC%882%EF%BC%89%E5%AE%9E%E9%AA%8C%E5%8F%8A%E7%BB%93%E6%9E%9C-toc" style="margin-left:40px;">
     <a href="#%C2%A0%EF%BC%882%EF%BC%89%E5%AE%9E%E9%AA%8C%E5%8F%8A%E7%BB%93%E6%9E%9C" rel="nofollow">
      （2）实验及结果
     </a>
    </p>
    <hr id="hr-toc"/>
    <h2 id="%E4%B8%80%E3%80%81%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3">
     一、算法思想
    </h2>
    <p>
     在讲解K近邻算法之前，我们先通过一个很简单的例子进入主题。在我们日常生活中，我们会遇到很多决策。比如说，我会考虑换什么手机，或者中午吃什么。这个时候我给大家三个方案选择：
    </p>
    <p style="text-align:justify;">
     1、随便选一个
    </p>
    <p style="text-align:justify;">
     2、问问周围其他人的选择
    </p>
    <p style="text-align:justify;">
     3、做一份详细的报告，根据自身的情况量身定制选择方案
    </p>
    <p style="text-align:justify;">
     相信大部分人都会选择第二个选项，如果选择选项1，意味着你会承担更高的风险，最后选择了一个非常不靠谱的选项。选择选项3你会发现会花费更多的计算成本在一个不是很重要的事件上。
    </p>
    <p>
     <img alt="" height="310" src="https://i-blog.csdnimg.cn/blog_migrate/4412ea9e1e762851ceb028f6782a9619.png" width="691"/>
    </p>
    <p>
     在我们选择下一台手机品牌时，我们就会先问其他人在用什么！当我们发现身边其他人，用安卓的大于用苹果的，这时我们下一台手机就会考虑安卓手机，这就是K近邻算法（KNN）。总结上面的例子KNN的思想可以总结为：近朱者赤，近墨者黑。
    </p>
    <p>
     <img alt="" height="365" src="https://i-blog.csdnimg.cn/blog_migrate/76e5386b55ef5766604ea269a9394f83.png" width="661"/>
    </p>
    <p>
     KNN分辨待测样本点类型的方法，如下图。
    </p>
    <p>
     第一步：先找到待测样本周围最近的已知样本点。
    </p>
    <p>
     第二步：统计周围已知样本点分布状况。
    </p>
    <p>
     第三步：将待测样本类别归为优势方。
    </p>
    <h2 id="%C2%A0%E2%80%8B">
    </h2>
    <p style="text-align:center;">
     <img alt="" height="305" src="https://i-blog.csdnimg.cn/blog_migrate/1e38017ff5d825dee7dcf8e9c9d522bc.png" width="608"/>
    </p>
    <p>
    </p>
    <p>
    </p>
    <h2>
    </h2>
    <h2 id="%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86">
     二、算法原理
    </h2>
    <h3 id="%EF%BC%881%EF%BC%89KNN%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86">
     （1）KNN算法原理
    </h3>
    <p>
     <span style="color:#000000;">
      KNN
     </span>
     <span style="color:#000000;">
      算法是选择与输入样本在特征空间内
     </span>
     <span style="color:#ff0000;">
      最近邻
     </span>
     <span style="color:#000000;">
      的
     </span>
     <span style="color:#ff0000;">
      k
     </span>
     <span style="color:#ff0000;">
      个训练样本并
     </span>
     <span style="color:#000000;">
      根据一定的
     </span>
     <span style="color:#ff0000;">
      决策规则
     </span>
     <span style="color:#000000;">
      ，给出输出结果
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      决策规则：
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      分类任务：输出结果为k各训练样本中占大多数的类。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      回归任务：输出结果为k个训练样本值的平均值。
     </span>
    </p>
    <p>
     <img alt="" height="333" src="https://i-blog.csdnimg.cn/blog_migrate/3621cf3a703b8fac886cd3594910b44d.png" width="1069"/>
    </p>
    <h3 id="%EF%BC%882%EF%BC%89KNN%E7%AE%97%E6%B3%95%E4%B8%89%E8%A6%81%E7%B4%A0">
     （2）KNN算法三要素
    </h3>
    <p>
     K值的选择、距离度量和分类决策规则是K近邻算法的三要素。
    </p>
    <h4 id="%E2%91%A0%20%E5%88%86%E7%B1%BB%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99">
     ① 分类决策规则
    </h4>
    <p>
     <span style="color:#000000;">
      KNN
     </span>
     <span style="color:#000000;">
      算法一般是用
     </span>
     <span style="color:#ff0000;">
      多数表决方法
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#000000;">
      即由输入实例的
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      个邻近的多数类决定输入实例的类。这种思想也是经验风险最小化的结果。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      训练样本为
     </span>
     <span style="color:#000000;">
      (x
     </span>
     <span style="color:#000000;">
      i
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      y
     </span>
     <span style="color:#000000;">
      i
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      。当输入实例为
     </span>
     <span style="color:#000000;">
      x，标记为c，
     </span>
     <img alt="" height="26" src="https://i-blog.csdnimg.cn/blog_migrate/4abbebae40961a8afa60b43141d4fee5.png" width="46">
      <span style="color:#000000;">
       是输入实例x的k
      </span>
      <span style="color:#000000;">
       近邻训练样本集。
      </span>
     </img>
    </p>
    <p>
     <span style="color:#000000;">
      我们定义
     </span>
     <span style="color:#ff0000;">
      训练误差率是
     </span>
     <span style="color:#ff0000;">
      K
     </span>
     <span style="color:#ff0000;">
      近邻训练样本标记与输入标记不一致的比例，误差率表示为：
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="107" src="https://i-blog.csdnimg.cn/blog_migrate/17310f0241e2f0966ff32a4ff7d207ca.png" width="572"/>
    </p>
    <p>
     <span style="color:#000000;">
      因此，要使误差率最小化即经验风险最小，就要使上式右端的
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="107" src="https://i-blog.csdnimg.cn/blog_migrate/69eb2e5781c0bffb8ec1824eb6b40023.png" width="250"/>
    </p>
    <p>
     <span style="color:#000000;">
      最大，即
     </span>
     <span style="color:#ff0000;">
      K
     </span>
     <span style="color:#ff0000;">
      近邻的标记值尽可能的与输入标记一致
     </span>
     <span style="color:#3e3e3e;">
      ，所以多数表决规则等价于经验风险最小化。
     </span>
    </p>
    <h4 id="%E2%91%A1%20K%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9">
     ② K值的选择
    </h4>
    <p>
     <img alt="" height="349" src="https://i-blog.csdnimg.cn/blog_migrate/19863bb452c35297be8f958d17df34fd.png" width="1200"/>
    </p>
    <p>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      取值较小时，模型复杂度高，训练误差会减小，泛化能力减弱；
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      取值较大时，模型复杂度低，训练误差会增大，泛化能力有一定的提高。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      KNN
     </span>
     <span style="color:#000000;">
      模型的复杂度可以通过对
     </span>
     <span style="color:#ff0000;">
      噪声的容忍度
     </span>
     <span style="color:#000000;">
      来理解，若模型对噪声很敏感，则模型的复杂度高；反之，模型的复杂度低。为了更好理解模型复杂度的含义，我们取一个极端，分析
     </span>
     <span style="color:#000000;">
      K=1
     </span>
     <span style="color:#000000;">
      和
     </span>
     <span style="color:#000000;">
      K=“
     </span>
     <span style="color:#000000;">
      样本数
     </span>
     <span style="color:#000000;">
      ”
     </span>
     <span style="color:#000000;">
      的模型复杂度。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      K=1
     </span>
     <span style="color:#000000;">
      时，模型输出的结果受噪声的影响很大。
     </span>
    </p>
    <p>
     <img alt="" height="261" src="https://i-blog.csdnimg.cn/blog_migrate/98150504105743912bd8bb8755546210.png" width="902"/>
    </p>
    <p>
     由下图可知，
     <span style="color:#000000;">
      样本数等于7
     </span>
     <span style="color:#000000;">
      ，当
     </span>
     <span style="color:#000000;">
      K=7
     </span>
     <span style="color:#000000;">
      时，不管输入数据的噪声有多大，输出结果都是绿色类，模型对噪声极不敏感，但是模型太过简单，包含的信息太少，也是不可取的。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      通过上面两种极端的
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      选取结果可知，
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      值选择应适中，一般来说选择
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      的值是
     </span>
     <span style="color:#000000;">
      k = sqrt(N)
     </span>
     <span style="color:#000000;">
      ，其中
     </span>
     <span style="color:#000000;">
      N
     </span>
     <span style="color:#000000;">
      代表训练数据集中的样本数，建议采用交叉验证的方法选取合适的
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      值。
     </span>
    </p>
    <p>
     <img alt="" height="252" src="https://i-blog.csdnimg.cn/blog_migrate/bb45073e2e14269136ee4c75f880f417.png" width="866"/>
    </p>
    <h4 id="%E2%91%A2%20%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F">
     ③ 距离度量
    </h4>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#ff0000;">
      KNN
     </span>
     <span style="color:#ff0000;">
      算法用距离来度量两个样本间的相似度
     </span>
     <span style="color:#000000;">
      ，常用的距离表示方法：
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      1、欧式距离
     </span>
    </p>
    <p style="margin-left:0in;text-align:center;">
     <img alt="" height="117" src="https://i-blog.csdnimg.cn/blog_migrate/0e3d7f53e94884131cef4096d83bca81.png" width="483"/>
     <img alt="" height="112" src="https://i-blog.csdnimg.cn/blog_migrate/d2208aae2f042673111050bab1243e88.png" width="119"/>
    </p>
    <p>
     2、曼哈顿距离
    </p>
    <p style="text-align:center;">
     <img alt="" height="105" src="https://i-blog.csdnimg.cn/blog_migrate/d81eea844712391e6ab4d76c3c79e2b1.png" width="401"/>
     <img alt="" height="150" src="https://i-blog.csdnimg.cn/blog_migrate/cf6d3755976e314e09bc5ff68a033dd6.png" width="150"/>
    </p>
    <h3 id="%EF%BC%883%EF%BC%89KNN%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95">
     （3）KNN实现方法
    </h3>
    <p>
     <span style="color:#000000;">
      实现
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      近邻法时，主要考虑的问题是如何对训练数据进行快速
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      近邻搜索。这在特征空间的维数大及训练数据容量大时尤其必要。
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      近邻法最简单的实现是线性扫描（穷举搜索），即要计算输入实例与每一个训练实例的距离。计算并存储好以后，再查找
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      近邻。
     </span>
     <span style="color:#ff0000;">
      当训练集很大时，计算非常耗时。为了提高
     </span>
     <span style="color:#ff0000;">
      KNN
     </span>
     <span style="color:#ff0000;">
      搜索的效率，可以考虑使用特殊的结构存储训练数据，以减小计算距离的次数。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树
     </span>
     <span style="color:#000000;">
      (K-dimension tree)
     </span>
     <span style="color:#000000;">
      是一种对
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。
     </span>
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树是是一种二叉树，表示对
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      维空间的一个划分，构造
     </span>
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树相当于不断地用垂直于坐标轴的超平面将
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      维空间切分，构成一系列的
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      维超矩形区域。
     </span>
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树的每个结点对应于一个
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      维超矩形区域。利用
     </span>
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树可以省去对大部分数据点的搜索，从而减少搜索的计算量。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树的
     </span>
     <span style="color:#000000;">
      KNN
     </span>
     <span style="color:#000000;">
      算法实现包括三部分：
     </span>
     <span style="color:#ff0000;">
      KD
     </span>
     <span style="color:#ff0000;">
      树的构建，
     </span>
     <span style="color:#ff0000;">
      KD
     </span>
     <span style="color:#ff0000;">
      树的搜索和
     </span>
     <span style="color:#ff0000;">
      KD
     </span>
     <span style="color:#ff0000;">
      树的分类。
     </span>
    </p>
    <h4 id="%C2%A0%E2%91%A0%20%E6%9E%84%E5%BB%BAKD%E6%A0%91" style="margin-left:0in;text-align:left;">
     ① 构建KD树
    </h4>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树实质是二叉树，其划分思想是切分使样本复杂度降低最多的特征。
     </span>
     <span style="color:#ff0000;">
      KD
     </span>
     <span style="color:#ff0000;">
      树认为特征方差越大，则该特征的复杂度亦越大，优先对该特征进行切分，切分点是所有实例在该特征的中位数。重复该切分步骤，直到切分后无样本则终止切分，终止时的样本为叶节点。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      【
     </span>
     <span style="color:#000000;">
      例
     </span>
     <span style="color:#000000;">
      】
     </span>
     <span style="color:#000000;">
      给定一个二维空间的数据集：
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="45" src="https://i-blog.csdnimg.cn/blog_migrate/0e91d739b6def41edd9b08120e476e96.png" width="600"/>
    </p>
    <p>
     构造KD树的步骤：
    </p>
    <p>
     1、数据集在维度X与Y上的方差分别为6.9和5.3， 因此首先从X的维度进行划分；
    </p>
    <p>
     2、数据集在X维度的中位数是5，以平面X=5将空间分为左右两个矩阵；
    </p>
    <p>
     3、分别对左右两个矩阵的样本在Y维度的中位数进行切分；
    </p>
    <p>
     4、重复步骤2，3 ，直到无样本，该节点为叶子节点。
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      数据集在维度
     </span>
     <span style="color:#000000;">
      x
     </span>
     <span style="color:#000000;">
      与
     </span>
     <span style="color:#000000;">
      y
     </span>
     <span style="color:#000000;">
      上的方差分别为
     </span>
     <span style="color:#000000;">
      6.9
     </span>
     <span style="color:#000000;">
      和
     </span>
     <span style="color:#000000;">
      5.3
     </span>
     <span style="color:#000000;">
      ，因此首先从
     </span>
     <span style="color:#000000;">
      x
     </span>
     <span style="color:#000000;">
      的维度进行划分;
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      首先按x
     </span>
     <span style="color:#000000;">
      坐标排序，选中间值
     </span>
     <span style="color:#000000;">
      x=5
     </span>
     <span style="color:#000000;">
      做根节点，选出
     </span>
     <span style="color:#000000;">
      (5,4)
     </span>
     <span style="color:#000000;">
      ，以平面
     </span>
     <span style="color:#000000;">
      x=5
     </span>
     <span style="color:#000000;">
      将空间分为左右两个矩形，所有
     </span>
     <span style="color:#000000;">
      x
     </span>
     <span style="color:#000000;">
      坐标比
     </span>
     <span style="color:#000000;">
      5
     </span>
     <span style="color:#000000;">
      小的都在左边，比
     </span>
     <span style="color:#000000;">
      5
     </span>
     <span style="color:#000000;">
      大的都在右边
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="186" src="https://i-blog.csdnimg.cn/blog_migrate/4db8feba98cf2715f185aa572e7999f4.png" width="483"/>
    </p>
    <p style="text-align:center;">
     <img alt="" height="399" src="https://i-blog.csdnimg.cn/blog_migrate/5e84b6aeb706fb029145e71d2f31fc9c.png" width="482"/>
    </p>
    <p>
     <span style="color:#000000;">
      分别对左右两个矩形的样本在
     </span>
     <span style="color:#000000;">
      y
     </span>
     <span style="color:#000000;">
      维度的中位数进行切分
     </span>
     <span style="color:#000000;">
      ;
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="259" src="https://i-blog.csdnimg.cn/blog_migrate/1f91674cb90f9d176d98910bfa70095b.png" width="595"/>
    </p>
    <p style="text-align:center;">
     <img alt="" height="391" src="https://i-blog.csdnimg.cn/blog_migrate/fde18f1bd3dfc26f887280d996b5e788.png" width="484"/>
    </p>
    <p>
     <span style="color:#000000;">
      重复
     </span>
     <span style="color:#000000;">
      上述两个
     </span>
     <span style="color:#000000;">
      步骤直到无样本，得到最后的
     </span>
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="408" src="https://i-blog.csdnimg.cn/blog_migrate/36b8f1b86226fe8e412f02abf0847618.png" width="455"/>
    </p>
    <p>
    </p>
    <p style="text-align:center;">
     <img alt="" height="339" src="https://i-blog.csdnimg.cn/blog_migrate/f40601a5a6ce8c18ecd143069bd415f4.png" width="627"/>
    </p>
    <h4 id="%C2%A0%E2%91%A1%20KD%E6%A0%91%E7%9A%84%E6%90%9C%E7%B4%A2">
     ② KD树的搜索
    </h4>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      1、将查询数据
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#ff0000;">
      从根结点开始
     </span>
     <span style="color:#000000;">
      ，按照
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      与各个结点的比较结果向下访问
     </span>
     <span style="color:#000000;">
      KD
     </span>
     <span style="color:#000000;">
      树
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#ff0000;">
      直至达到叶子结点
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      其中Q
     </span>
     <span style="color:#000000;">
      与结点的比较指的是将
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      对应于结点中的
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      维度上的值与
     </span>
     <span style="color:#000000;">
      m
     </span>
     <span style="color:#000000;">
      进行比较，若
     </span>
     <span style="color:#000000;">
      Q(k) &lt; m
     </span>
     <span style="color:#000000;">
      ，则访问左子树，否则访问右子树。达到叶子结点时，计算
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      与叶子结点上保存的数据之间的距离，记录下最小距离对应的数据点，记为当前“最近邻点”
     </span>
     <span style="color:#000000;">
      和最小距离。
     </span>
    </p>
    <p>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      （
     </span>
     <span style="color:#000000;">
      2
     </span>
     <span style="color:#000000;">
      ）进行
     </span>
     <span style="color:#ff0000;">
      回溯操作
     </span>
     <span style="color:#000000;">
      ，该操作是为了找到离
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      更近的“最近邻点”。即判断未被访问过的分支里是否还有离
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      更近的点，它们之间的距离小于
     </span>
     <span style="color:#000000;">
      最小距离
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      如果
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      与其父结点下的未被访问过的分支之间的距离小于最小距离，则认为该分支中存在离
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      更近的数据，进入该结点，进行（
     </span>
     <span style="color:#000000;">
      1
     </span>
     <span style="color:#000000;">
      ）步骤一样的查找过程，如果找到更近的数据点，则更新为当前的“最近邻点”
     </span>
     <span style="color:#000000;">
      ，并更新
     </span>
     <span style="color:#000000;">
      最小距离
     </span>
     <span style="color:#000000;">
      。如果
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      与其父结点下的未被访问过的分支之间的距离大于最小距离，则说明该分支内不存在与
     </span>
     <span style="color:#000000;">
      Q
     </span>
     <span style="color:#000000;">
      更近的点。
     </span>
     <span style="color:#ff0000;">
      回溯的判断过程是从下往上进行的
     </span>
     <span style="color:#000000;">
      ，直到回溯到根结点时已经不存在与
     </span>
     <span style="color:#000000;">
      P
     </span>
     <span style="color:#000000;">
      更近的分支为止。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      例如：查询点为
     </span>
     <span style="color:#000000;">
      (8.5
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      6.5
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      ，比较到子节点
     </span>
     <span style="color:#000000;">
      (9, 6)
     </span>
     <span style="color:#000000;">
      ：
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="313" src="https://i-blog.csdnimg.cn/blog_migrate/7ec387391925a558501567c1c2261b3e.png" width="660"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点
     </span>
     <span style="color:#000000;">
      (9,6)
     </span>
     <span style="color:#000000;">
      。而找到的叶子节点并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。为了找到真正的最近邻，还需要进行
     </span>
     <span style="color:#000000;">
      ‘
     </span>
     <span style="color:#000000;">
      回溯
     </span>
     <span style="color:#000000;">
      ’
     </span>
     <span style="color:#000000;">
      操作。
     </span>
    </p>
    <p>
     <span style="color:#000000;">
      例如：查询点为
     </span>
     <span style="color:#000000;">
      (8.5
     </span>
     <span style="color:#000000;">
      ,
     </span>
     <span style="color:#000000;">
      6.5
     </span>
     <span style="color:#000000;">
      )
     </span>
     <span style="color:#000000;">
      ，回溯：
     </span>
    </p>
    <p>
    </p>
    <p style="text-align:center;">
     <img alt="" height="299" src="https://i-blog.csdnimg.cn/blog_migrate/318151be8123bc62f51aa03e309ec345.png" width="595"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      当前最近邻点：
     </span>
     <span style="color:#000000;">
      (9, 6)
     </span>
     <span style="color:#000000;">
      ， 最近邻距离：
     </span>
     <span style="color:#000000;">
      sqrt(0.5)
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      回溯：算法沿搜索路径反向查找是否有距离查询点更近的数据点。此例中先从(5,4)
     </span>
     <span style="color:#000000;">
      点开始进行二叉查找，然后到达
     </span>
     <span style="color:#000000;">
      (7,2)
     </span>
     <span style="color:#000000;">
      ，最后到达
     </span>
     <span style="color:#000000;">
      (9,6)
     </span>
     <span style="color:#000000;">
      ，此时搜索路径中的节点为大于
     </span>
     <span style="color:#000000;">
      (5,4)
     </span>
     <span style="color:#000000;">
      和
     </span>
     <span style="color:#000000;">
      (7,2)
     </span>
     <span style="color:#000000;">
      ，小于
     </span>
     <span style="color:#000000;">
      (9,6)
     </span>
     <span style="color:#000000;">
      ，首先以
     </span>
     <span style="color:#000000;">
      (9,6)
     </span>
     <span style="color:#000000;">
      作为当前最近邻点，计算其到查询点
     </span>
     <span style="color:#3e3e3e;">
      (8.5
     </span>
     <span style="color:#3e3e3e;">
      ,
     </span>
     <span style="color:#3e3e3e;">
      6.5
     </span>
     <span style="color:#3e3e3e;">
      )
     </span>
     <span style="color:#000000;">
      的距离为
     </span>
     <span style="color:#000000;">
      0.7071
     </span>
     <span style="color:#000000;">
      ，然后回溯到其父节点
     </span>
     <span style="color:#000000;">
      (7,2)
     </span>
     <span style="color:#000000;">
      ，并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。以
     </span>
     <span style="color:#3e3e3e;">
      (8.5
     </span>
     <span style="color:#3e3e3e;">
      ,
     </span>
     <span style="color:#3e3e3e;">
      6.5
     </span>
     <span style="color:#3e3e3e;">
      )
     </span>
     <span style="color:#000000;">
      为圆心，以
     </span>
     <span style="color:#000000;">
      0.7071
     </span>
     <span style="color:#000000;">
      为半径画圆，发现该圆不和超平面
     </span>
     <span style="color:#000000;">
      y =2
     </span>
     <span style="color:#000000;">
      交割，因此不用进入
     </span>
     <span style="color:#000000;">
      (7,2)
     </span>
     <span style="color:#000000;">
      节点右子空间中去搜索。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#4d4d4d;">
      再回溯到
     </span>
     <span style="color:#000000;">
      (5,4)
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#4d4d4d;">
      以
     </span>
     <span style="color:#3e3e3e;">
      (8.5
     </span>
     <span style="color:#3e3e3e;">
      ,
     </span>
     <span style="color:#3e3e3e;">
      6.5
     </span>
     <span style="color:#3e3e3e;">
      )
     </span>
     <span style="color:#000000;">
      为圆心，以
     </span>
     <span style="color:#000000;">
      0.7071
     </span>
     <span style="color:#000000;">
      为半径画圆
     </span>
     <span style="color:#4d4d4d;">
      更不会与
     </span>
     <span style="color:#4d4d4d;">
      x = 5
     </span>
     <span style="color:#4d4d4d;">
      超平面交割，因此不用进入
     </span>
     <span style="color:#000000;">
      (5,4)
     </span>
     <span style="color:#4d4d4d;">
      左子空间进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点
     </span>
     <span style="color:#000000;">
      (9,6)
     </span>
     <span style="color:#000000;">
      ，
     </span>
     <span style="color:#4d4d4d;">
      最近距离为
     </span>
     <span style="color:#000000;">
      0.7071
     </span>
     <span style="color:#4d4d4d;">
      。
     </span>
    </p>
    <h4 id="%E2%91%A2%20KD%E6%A0%91%E7%9A%84%E5%88%86%E7%B1%BB%EF%BC%88%E9%A2%84%E6%B5%8B%EF%BC%89" style="margin-left:0in;text-align:left;">
     ③ KD树的分类（预测）
    </h4>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      每一次搜寻与输入样本最近的样本节点，然后忽略该节点，重复同样步骤
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      次，找到与输入样本最近邻的
     </span>
     <span style="color:#000000;">
      K
     </span>
     <span style="color:#000000;">
      个样本 ，投票法确定输出结果。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#4d4d4d;">
      当维数较大时，直接利用
     </span>
     <span style="color:#4d4d4d;">
      k-d
     </span>
     <span style="color:#4d4d4d;">
      树快速检索的性能急剧下降。假设数据集的维数为
     </span>
     <span style="color:#4d4d4d;">
      D
     </span>
     <span style="color:#4d4d4d;">
      ，一般来说要求数据的规模
     </span>
     <span style="color:#4d4d4d;">
      N
     </span>
     <span style="color:#4d4d4d;">
      满足条件：
     </span>
     <span style="color:#4d4d4d;">
      N
     </span>
     <span style="color:#4d4d4d;">
      远大于
     </span>
     <span style="color:#4d4d4d;">
      2
     </span>
     <span style="color:#4d4d4d;">
      的
     </span>
     <span style="color:#4d4d4d;">
      D
     </span>
     <span style="color:#4d4d4d;">
      次方，才能达到高效的搜索。
     </span>
    </p>
    <h3 id="%EF%BC%884%EF%BC%89KNN%E7%AE%97%E6%B3%95%E4%B9%8B%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%83%85%E5%86%B5">
     （4）KNN算法之训练样本不平衡情况
    </h3>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      若正负样本处于不平衡状态，运用投票决策的
     </span>
     <span style="color:#000000;">
      KNN
     </span>
     <span style="color:#000000;">
      算法判断输入样本的所属类别：
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="268" src="https://i-blog.csdnimg.cn/blog_migrate/fe64cf657a4602b2e9845837d35bc317.png" width="395"/>
    </p>
    <p>
     <span style="color:#000000;">
      结果显示输入样本为绿色类 。原因是
     </span>
     <span style="color:#ff0000;">
      红色类的个数远远小于绿色样本，导致出现的分类错误
     </span>
     <span style="color:#3e3e3e;">
      。
     </span>
    </p>
    <p>
     <span style="color:#3e3e3e;">
      解决方法：
     </span>
    </p>
    <p>
     <span style="color:#3e3e3e;">
      1、
     </span>
     <span style="color:#000000;">
      若分类决策选择
     </span>
     <span style="color:#ff0000;">
      限定半径最近邻法
     </span>
     <span style="color:#000000;">
      ，即以输入样本为圆心，最大半径
     </span>
     <span style="color:#000000;">
      R
     </span>
     <span style="color:#000000;">
      的圆内选择出现次数最多的类做为输入样本的类 。如下图，黑色样本的分类结果正确。
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="281" src="https://i-blog.csdnimg.cn/blog_migrate/6e90573860099ee0ad4a00011efda459.png" width="363"/>
    </p>
    <p>
     2、
     <span style="color:#000000;">
      投票法是默认每个样本的权重相等，我们假定权重与距离成反比，
     </span>
     <span style="color:#ff0000;">
      即距离越大，对结果的影响越小，那么该样本的权重也越小，反之，权重则越大，根据权重对输入样本进行分类
     </span>
     <span style="color:#000000;">
      。这种思想与
     </span>
     <span style="color:#000000;">
      A
     </span>
     <span style="color:#000000;">
      daBoost
     </span>
     <span style="color:#000000;">
      算法相似，分类性能好的弱分类器给予一个大的权重 。
     </span>
    </p>
    <p style="text-align:center;">
     <img alt="" height="262" src="https://i-blog.csdnimg.cn/blog_migrate/d50bd62523d28084c30ed460ef4bc104.png" width="631"/>
    </p>
    <h3 id="%EF%BC%885%EF%BC%89%E7%AE%97%E6%B3%95%E4%BC%98%E7%BC%BA%E7%82%B9">
     （5）算法优缺点
    </h3>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      <strong>
       优点：
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      1
     </span>
     <span style="color:#000000;">
      ）算法简单，理论成熟，可用于分类和回归。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      2
     </span>
     <span style="color:#000000;">
      ）对异常值不敏感。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      3
     </span>
     <span style="color:#000000;">
      ）可用于非线性分类。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      4
     </span>
     <span style="color:#000000;">
      ）比较适用于容量较大的训练数据，容量较小的训练数据则很容易出现误分类情况。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#ff0000;">
      5
     </span>
     <span style="color:#ff0000;">
      ）
     </span>
     <span style="color:#ff0000;">
      KNN
     </span>
     <span style="color:#ff0000;">
      算法原理是根据邻域的
     </span>
     <span style="color:#ff0000;">
      K
     </span>
     <span style="color:#ff0000;">
      个样本来确定输出类别，因此对于不同类的样本集有交叉或重叠较多的待分样本集来说，
     </span>
     <span style="color:#ff0000;">
      KNN
     </span>
     <span style="color:#ff0000;">
      方法较其他方法更为合适。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      <strong>
       缺点：
      </strong>
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      1
     </span>
     <span style="color:#000000;">
      ）时间复杂度和空间复杂度高。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      2
     </span>
     <span style="color:#000000;">
      ）训练样本不平衡，对稀有类别的预测准确率低。
     </span>
    </p>
    <p style="margin-left:0in;text-align:justify;">
     <span style="color:#000000;">
      3
     </span>
     <span style="color:#000000;">
      ）相比决策树模型，
     </span>
     <span style="color:#000000;">
      KNN
     </span>
     <span style="color:#000000;">
      模型可解释性不强。
     </span>
    </p>
    <h2 id="%E4%B8%89%E3%80%81%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C">
     三、算法实验
    </h2>
    <h3 id="%EF%BC%881%EF%BC%89%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%E4%B8%8E%E5%A4%84%E7%90%86">
     （1）数据收集与处理
    </h3>
    <p>
     利用爬虫工具，爬取了成都和桂林两地2020年的空气质量数据，经过数据处理分别对两地空气质量进行标签化，最终得到训练集与测试集。
    </p>
    <p>
     <img alt="" height="255" src="https://i-blog.csdnimg.cn/blog_migrate/ab995684eb53a44f1542f66280e50be0.png" width="1011"/>
    </p>
    <h3 id="%C2%A0%EF%BC%882%EF%BC%89%E5%AE%9E%E9%AA%8C%E5%8F%8A%E7%BB%93%E6%9E%9C">
     （2）实验及结果
    </h3>
    <p>
     <img alt="" height="724" src="https://i-blog.csdnimg.cn/blog_migrate/6c9d22d66dc3bc4759469848130a671b.png" width="1200"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      我们取不同
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      值
     </span>
     <span style="color:#000000;">
      (1~30)
     </span>
     <span style="color:#000000;">
      使用
     </span>
     <span style="color:#000000;">
      Manhattan
     </span>
     <span style="color:#000000;">
      距离计算公式来预测测试集中空气质量，根据实验结果，当
     </span>
     <span style="color:#000000;">
      k=12
     </span>
     <span style="color:#000000;">
      时，得到的准确率最高；当
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      值大于
     </span>
     <span style="color:#000000;">
      20
     </span>
     <span style="color:#000000;">
      时，准确率会明显下降。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <img alt="" height="717" src="https://i-blog.csdnimg.cn/blog_migrate/7dc18862c36006195fbe9b2aaae7bf8b.png" width="1200"/>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      我们同样取不同
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      值
     </span>
     <span style="color:#000000;">
      (1~30)
     </span>
     <span style="color:#000000;">
      使用
     </span>
     <span style="color:#000000;">
      Euclidean
     </span>
     <span style="color:#000000;">
      距离计算公式来预测测试集中空气质量，根据实验结果，当
     </span>
     <span style="color:#000000;">
      k=4
     </span>
     <span style="color:#000000;">
      时，得到的准确率最高；当
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      值大于
     </span>
     <span style="color:#000000;">
      16
     </span>
     <span style="color:#000000;">
      时，准确率会明显下降。
     </span>
    </p>
    <p style="margin-left:0in;text-align:left;">
     <span style="color:#000000;">
      结论：
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      值的选取和距离计算方法对实验结果均有较大的影响，其中
     </span>
     <span style="color:#000000;">
      k
     </span>
     <span style="color:#000000;">
      值选取一般不能大于
     </span>
     <span style="color:#000000;">
      √
     </span>
     <span style="color:#000000;">
      样本数
     </span>
     <span style="color:#000000;">
      。
     </span>
    </p>
    <p>
    </p>
    <p>
    </p>
   </div>
  </div>
 </article>
 <p alt="68747470733a2f2f626c6f672e:6373646e2e6e65742f77656978696e5f34363332323836362f:61727469636c652f64657461696c732f313232323736303233" class_="artid" style="display:none">
 </p>
</div>
