---
layout: post
title: GPT-2-开源模型本地搭建一
date: 2023-04-07 09:52:31 +0800
categories: [深度学习]
tags: [深度学习,tensorflow,python]
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=130004414
    alt: GPT-2-开源模型本地搭建一
artid: 130004414
render_with_liquid: false
---
<p class="artid" style="display:none">$url</p>
<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     GPT-2 开源模型本地搭建（一）
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-dark" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
    </p>
    <div class="toc">
     <h4>
      GPT-2 开源模型本地搭建
     </h4>
     <ul>
      <li>
       <a href="#1GPT_2" rel="nofollow">
        1、GPT使用心得
       </a>
      </li>
      <li>
       <a href="#2py_13" rel="nofollow">
        2、py环境准备
       </a>
      </li>
      <li>
       <ul>
        <li>
         <a href="#21_Microsoft_Visual_C_140_14" rel="nofollow">
          2.1 安装Microsoft Visual C++ 14.0运行库
         </a>
        </li>
        <li>
         <a href="#22__19" rel="nofollow">
          2.2 环境安装
         </a>
        </li>
        <li>
         <ul>
          <li>
           <a href="#1_python_3637_30" rel="nofollow">
            1. 安装python 3.6或3.7
           </a>
          </li>
          <li>
           <a href="#2_pip_41" rel="nofollow">
            2. 安装pip
           </a>
          </li>
          <li>
           <a href="#3__tensorflow_46" rel="nofollow">
            3 安装tensorflow
           </a>
          </li>
          <li>
           <a href="#4_NumPyregex_54" rel="nofollow">
            4. 安装NumPy、regex
           </a>
          </li>
          <li>
           <a href="#5_pip_freeze__grep_tensorflow_62" rel="nofollow">
            5. 安装其他插件（pip freeze | grep tensorflow）
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a href="#3GPT2_Hugging_Face_76" rel="nofollow">
        3、GPT-2 开源模型本地搭建（基于Hugging Face）
       </a>
      </li>
      <li>
       <ul>
        <li>
         <a href="#31__78" rel="nofollow">
          3.1 模型介绍
         </a>
        </li>
        <li>
         <a href="#32__86" rel="nofollow">
          3.2 插件安装
         </a>
        </li>
        <li>
         <a href="#33__88" rel="nofollow">
          3.3 模型运行
         </a>
        </li>
        <li>
         <a href="#34__137" rel="nofollow">
          3.4 模型测试
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a href="#4GPT2GitHub_145" rel="nofollow">
        4、GPT-2本地模型搭建（GitHub，坑未踩完）
       </a>
      </li>
      <li>
       <ul>
        <li>
         <a href="#41__146" rel="nofollow">
          4.1 模型介绍
         </a>
        </li>
        <li>
         <a href="#42__154" rel="nofollow">
          4.2 模型下载
         </a>
        </li>
        <li>
         <a href="#43_Hugging_FaceGitHub_173" rel="nofollow">
          4.3 为什么Hugging Face下载的模型能加载，而GitHub上下载的不能用的？（要转换格式）
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a href="#5demo_238" rel="nofollow">
        5、调试demo源码
       </a>
      </li>
     </ul>
    </div>
    <p>
    </p>
    <h2>
     <a id="1GPT_2">
     </a>
     1、GPT使用心得
    </h2>
    <p>
     ChatGPT (gpt-35-turbo) 和 GPT-4 模型是针对对话接口进行了优化的语言模型，都是输入对话和输出消息模式。
     <br/>
     以上模型的行为与旧的 GPT-3、GPT-2 模型不同，旧的模型是文本输入和文本输出，这意味着它们接受了提示字符串并返回了一个会追加到提示的补全，旧的模型属于文本补全类模型。（本文验证了GPT-2的模型、本主另外还验证了GPT2-ML的中文模型，确实属于文本补全）
    </p>
    <p>
     这两种模型需要以类似聊天的具体脚本形式提供输入，然后通过聊天返回补全信息，以展示模型编写的消息。 虽然这种形式专为多回合对话而设计，但你会发现它也适用于非聊天场景。
    </p>
    <p>
     GPT-2 ：4年以前就已经开源，没多少研究价值，无法投入使用；
     <br/>
     GPT2-ML：苏剑林大佬玩的一套模型，训练了1.5B参数的体量，但能力还是一般，
     <a href="https://github.com/imcaspar/gpt2-ml">
      源码
     </a>
     。
     <br/>
     ChatGPT ：github上有不少已经对接了OpenAI官网接口的web版，可以拿来用，比如
     <a href="https://github.com/Chanzhaoyu/chatgpt-web">
      这款
     </a>
     。
     <br/>
     GPT-4 ：OpenAI就是微软公司的，目前微软云已经出了3.5与4系列的接口，比如
     <a href="https://learn.microsoft.com/zh-cn/azure/cognitive-services/openai/how-to/chatgpt?pivots=programming-language-chat-completions" rel="nofollow">
      预览版
     </a>
     ；
    </p>
    <h2>
     <a id="2py_13">
     </a>
     2、py环境准备
    </h2>
    <h3>
     <a id="21_Microsoft_Visual_C_140_14">
     </a>
     2.1 安装Microsoft Visual C++ 14.0运行库
    </h3>
    <p>
     1）
     <a href="https://visualstudio.microsoft.com/zh-hant/visual-cpp-build-tools/" rel="nofollow">
      下载vs_BuildTools.exe
     </a>
     <br/>
     <a href="https://pan.baidu.com/s/1tWJTsYWK0QSoMJjxIE9LmA?pwd=9ut7" rel="nofollow">
      百度网盘
     </a>
     ,提取码: 9ut7
     <br/>
     2）安装c++插件
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/4d1cd5f2f7512d78a16b6cdd7627de84.png"/>
    </p>
    <h3>
     <a id="22__19">
     </a>
     2.2 环境安装
    </h3>
    <p>
     TensorFlow 2.4.0 支持 Python 版本为 3.5-3.8,GPT-2是用TensorFlow实现的，因此需要安装TensorFlow库。
     <br/>
     本次验证选用基于Python 3.7、TensorFlow 2.4.0。
    </p>
    <p>
     供参考的国内镜像源
     <br/>
     中国科技大学：https://pypi.mirrors.ustc.edu.cn/simple/
     <br/>
     华中理工大学：http://pypi.hustunique.com/
     <br/>
     山东理工大学：http://pypi.sdutlinux.org/
     <br/>
     阿里云：https://mirrors.aliyun.com/pypi/simple/
     <br/>
     清华：https://pypi.tuna.tsinghua.edu.cn/simple
     <br/>
     豆瓣：http://pypi.douban.com/simple/
    </p>
    <h4>
     <a id="1_python_3637_30">
     </a>
     1. 安装python 3.6或3.7
    </h4>
    <p>
     下载对应的操作系统版本，window下载，
     <a href="https://www.python.org/downloads/windows/" rel="nofollow">
      官网
     </a>
     、
     <a href="https://pan.baidu.com/s/1Sa3T6NkMie30R-CG1aCQow?pwd=mhkq" rel="nofollow">
      百度网盘
     </a>
     提取码: mhkq
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/5afdb920bb005814481532ee8cd4d435.png"/>
    </p>
    <p>
     Windows选择“Add Python 3.x to PATH”选项，这样可以将Python添加到系统环境变量中，方便后续操作。
     <br/>
     Linux上传源代码包，解压后执行：
     <br/>
     ./configure
     <br/>
     make -j4 &amp;&amp; make install
     <br/>
     py -V
    </p>
    <h4>
     <a id="2_pip_41">
     </a>
     2. 安装pip
    </h4>
    <pre><code>  py -m ensurepip --default-pip
  pip --version
</code></pre>
    <h4>
     <a id="3__tensorflow_46">
     </a>
     3 安装tensorflow
    </h4>
    <pre><code>  pip install tensorflow==2.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install tensorflow-gpu==2.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  查看已安装的 TensorFlow 版本：
  pip list | grep tensorflow
  pip freeze | grep tensorflow
</code></pre>
    <h4>
     <a id="4_NumPyregex_54">
     </a>
     4. 安装NumPy、regex
    </h4>
    <pre><code>  pip install numpy==1.19.4 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install regex==2020.11.13 -i https://pypi.tuna.tsinghua.edu.cn/simple
  
  NumPy是一个用于数值计算的Python库，也是TensorFlow的依赖库之一，因此需要安装。
  regex是一个正则表达式库，GPT-2使用了regex库来处理文本数据。
</code></pre>
    <h4>
     <a id="5_pip_freeze__grep_tensorflow_62">
     </a>
     5. 安装其他插件（pip freeze | grep tensorflow）
    </h4>
    <pre><code>  pip install tensorflow-addons==0.12.1 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install tensorflow-estimator==2.4.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install tensorflow-hub==0.11.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip p install tensorflow-io-gcs-filesystem==0.20.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install tensorflow-text==2.4.3 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install typing-extensions==3.10.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install --upgrade Tokenizers -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install transformers==4.0.0 -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install ctypes -i https://pypi.tuna.tsinghua.edu.cn/simple
  pip install flask -i https://pypi.tuna.tsinghua.edu.cn/simple
</code></pre>
    <h2>
     <a id="3GPT2_Hugging_Face_76">
     </a>
     3、GPT-2 开源模型本地搭建（基于Hugging Face）
    </h2>
    <p>
     Hugging Face是机器学习的圣地，有非常多的模型，类似于代码界的GitHub。
    </p>
    <h3>
     <a id="31__78">
     </a>
     3.1 模型介绍
    </h3>
    <p>
     OpenAI 已将模型开源至
     <a href="https://huggingface.co/gpt2" rel="nofollow">
      Hugging Face
     </a>
     上，在上面可以找到以下的GPT-2预训练模型：
     <br/>
     gpt2
     <br/>
     gpt2-medium
     <br/>
     gpt2-large
     <br/>
     gpt2-xl
     <br/>
     其中，gpt2是最小的模型，参数量为124M；gpt2-medium, gpt2-large和gpt2-xl依次是参数量增大的模型。
    </p>
    <h3>
     <a id="32__86">
     </a>
     3.2 插件安装
    </h3>
    <p>
     pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
    </p>
    <h3>
     <a id="33__88">
     </a>
     3.3 模型运行
    </h3>
    <p>
     1）下载模型：download.py
    </p>
    <pre><code>from transformers import GPT2Tokenizer, TFGPT2LMHeadModel

model_name = 'gpt2'  # 模型名称，可以是 gpt2, gpt2-medium, gpt2-large, gpt2-xl 中的任意一个
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = TFGPT2LMHeadModel.from_pretrained(model_name)
model.save_pretrained(f"./{model_name}")  # 将模型保存到本地
tokenizer.save_pretrained(f"./{model_name}")  # 将 tokenizer 保存到本地
</code></pre>
    <pre><code>#Windows执行(py C:\01.project\gitee\gpt-2\src\download.py)
#Linux执行(python3 /root/python/gpt-2/download.py)
</code></pre>
    <p>
     2）调用模型：
    </p>
    <pre><code>import tensorflow as tf
import datetime
from transformers import GPT2Tokenizer, TFGPT2LMHeadModel
from flask import Flask, jsonify, request, render_template

model_dir = "C:/01.project/chatgpt/gpt-2/models/gpt2"
# 加载 GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(model_dir)
# 加载 GPT-2 模型
model = TFGPT2LMHeadModel.from_pretrained(model_dir)

# 创建 Flask 实例
app = Flask(__name__)

# 定义 API 接口
@app.route('/generate', methods=['POST'])
def generate_text():
    # 从请求中获取文本
    input_text = request.get_json()['text']
    # 使用 tokenizer 对文本进行编码
    input_ids = tokenizer.encode(input_text, return_tensors='tf')
    # 使用模型生成文本
    output_ids = model.generate(input_ids, max_length=100, num_return_sequences=1)
    # 使用 tokenizer 对生成的文本进行解码
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    # 返回生成的文本
    return jsonify({'generated_text': output_text})

# 启动应用
if __name__ == '__main__':
    app.run(host="0.0.0.0", port=5000)
</code></pre>
    <p>
     3）启动模型， 执行：py C:\01.project\gitee\gpt-2\src\app_huggingface_gpt2.py
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/4801eb1ba6a8e2fbfcd64a3ff406355f.png"/>
    </p>
    <h3>
     <a id="34__137">
     </a>
     3.4 模型测试
    </h3>
    <p>
     根据测试的结果，我们知道了，这是一个文本补全模型，而不是文本对话模型。
     <br/>
     <img alt="在这里插入图片描述" src="https://i-blog.csdnimg.cn/blog_migrate/3e874fa156ba4ce627b0ba5c6ec148a0.png">
      <br/>
      模型效果为什么这么差？
     </img>
    </p>
    <p>
     GPT-2模型在训练时是从大规模的文本语料库中学习到的语言模型，但它的训练目标是生成自然流畅的单向文本，而不是与人进行对话。因此，即使它被输入了一些问题或对话，它也只会反映出之前学习到的文本知识，并没有明确的答案或响应。而且，尽管GPT-2可以生成出一些有趣或合理的回复，但也会生成一些无意义或不连贯的回复，这可能会导致对话不友好。如果需要建立一个友好的对话模型，需要用到适合于对话的模型，如Seq2Seq、Transformer等，并在相应的数据集上进行训练和微调。
    </p>
    <p>
     所以，这玩意不训练就根本没法用，训练的成本不低，可以找大牛训练好的模型直接用。
    </p>
    <h2>
     <a id="4GPT2GitHub_145">
     </a>
     4、GPT-2本地模型搭建（GitHub，坑未踩完）
    </h2>
    <h3>
     <a id="41__146">
     </a>
     4.1 模型介绍
    </h3>
    <p>
     在GitHub，可以下载到
     <a href="https://github.com/openai/gpt-2">
      开源的模型
     </a>
     ，这里的模型得用TensorFlow 1.x去跑，本文没有踩这里的坑，主要介绍Hugging Face上的模型，模型大致如下：
     <br/>
     GPT-2 117M：117 million parameters
     <br/>
     GPT-2 345M：345 million parameters
     <br/>
     GPT-2 762M：762 million parameters
     <br/>
     GPT-2 1.5B：1.5 billion parameters
     <br/>
     GitHub上是居于TensorFlow 1.15.2，但使用TensorFlow 2.x环境也可以使用，
    </p>
    <h3>
     <a id="42__154">
     </a>
     4.2 模型下载
    </h3>
    <pre><code>1. 下载源代码
   https://github.com/openai/gpt-2

2. 下载模型数据
  1）进入到源码路径：cd C:\01.project\gitee\gpt-2
  2）pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
  3）下载过程中会缺少插件：ModuleNotFoundError: No module named 'tqdm'
    执行：pip3 install tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple
  4）下载对应模型
    py download_model.py 124M
    py download_model.py 355M
    py download_model.py 774M
    py download_model.py 1558M
  5）下载后的模型文件：（124M版本） 
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/1073fef06e3b49bf89bd507e7cc80b18.png)
</code></pre>
    <h3>
     <a id="43_Hugging_FaceGitHub_173">
     </a>
     4.3 为什么Hugging Face下载的模型能加载，而GitHub上下载的不能用的？（要转换格式）
    </h3>
    <p>
     两个下载的模型分别为tf_model.h5 和 model.ckpt.data-00000-of-00001，都是深度学习模型在不同框架下的存储格式，但它们的区别在于：
    </p>
    <ol>
     <li>
      存储方式：tf_model.h5 是基于 HDF5 格式存储的 Keras/TensorFlow模型文件，而model.ckpt.data-00000-of-00001 则是 TensorFlow 的 checkpoint文件格式。
     </li>
     <li>
      框架兼容性：tf_model.h5 主要用于 Keras 和 TensorFlow框架之间的模型转换和共享，而model.ckpt.data-00000-of-00001是TensorFloW框架保存模型权重和参数的默认格式。因此，在使用其他深度学习框架加载模型时，需要先将 TensorFlow模型转换为其他框架所支持的格式。
     </li>
     <li>
      存储内容：tf_model.h5 中包括了Keras/TensorFlow模型的结构、权重和优化器参数等信息；而 model.ckpt.data-00000-of-00001只包括了 TensorFlow模型的权重数据，其模型图和其他相关参数需要通过其他文件进行保存。
     </li>
    </ol>
    <p>
     但是，如果使用TensorFlow 加载model.ckpt.data-00000-of-00001模型，需要先将其转换为 TensorFlow 能够识别的格式，如 TensorFlow checkpoint 文件或 SavedModel 等。转换方式取决于你想要的最终格式。
     <br/>
     以下是一些可能有用的转换方式：
    </p>
    <ol>
     <li>
      从 TensorFlow checkpoint 文件中加载：可以利用 TensorFlow 官方提供的加载 checkpoint 文件的
      <br/>
      API，例如 tf.train.load_checkpoint 函数读取模型权重，并将其转换为 TensorFlow 张量，再利用
      <br/>
      TensorFlow 的 API 生成新的 TensorFlow 模型。
      <pre><code class="prism language-python"><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

checkpoint_path <span class="token operator">=</span> <span class="token string">'path/to/model.ckpt'</span>
<span class="token comment"># 创建一个空白的 TensorFlow 模型</span>
model <span class="token operator">=</span> YourModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 加载原始的 TensorFlow checkpoint 文件</span>
checkpoint <span class="token operator">=</span> tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>load_checkpoint<span class="token punctuation">(</span>checkpoint_path<span class="token punctuation">)</span>
<span class="token comment"># 将 TensorFlow checkpoint 文件中的权重数据加载到新模型中</span>
<span class="token keyword">for</span> var_name<span class="token punctuation">,</span> tensor_value <span class="token keyword">in</span> checkpoint<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    var_name <span class="token operator">=</span> var_name<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'module.'</span><span class="token punctuation">,</span> <span class="token string">''</span><span class="token punctuation">)</span>
    model_weights<span class="token punctuation">[</span>var_name<span class="token punctuation">]</span><span class="token punctuation">.</span>assign<span class="token punctuation">(</span>tensor_value<span class="token punctuation">)</span>
</code></pre>
     </li>
     <li>
      从 PyTorch 模型中加载：可以利用 PyTorch 和 TensorFlow 提供的模型转换工具，将 PyTorch 模型转换为
      <br/>
      TensorFlow checkpoint 文件或 SavedModel，然后在 TensorFlow 中加载。
      <pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> TFGPT2LMHeadModel

<span class="token comment"># 加载 PyTorch 模型</span>
pytorch_model <span class="token operator">=</span> TFGPT2LMHeadModel<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">'path/to/pytorch/model'</span><span class="token punctuation">)</span>
<span class="token comment"># 将 PyTorch 模型转换为 TensorFlow checkpoint 文件</span>
pytorch_weights <span class="token operator">=</span> pytorch_model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span>
tf_checkpoint_path <span class="token operator">=</span> <span class="token string">'path/to/tf/checkpoint'</span>
tf<span class="token punctuation">.</span>train<span class="token punctuation">.</span>save_checkpoint<span class="token punctuation">(</span>tf_checkpoint_path<span class="token punctuation">,</span> pytorch_weights<span class="token punctuation">)</span>
<span class="token comment"># 加载新的 TensorFlow checkpoint 文件</span>
new_model <span class="token operator">=</span> YourModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
new_model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>tf_checkpoint_path<span class="token punctuation">)</span>
</code></pre>
     </li>
     <li>
      从 ONNX 模型中加载：如果原始模型是使用 ONNX 格式保存的，可以使用 TensorFlow 提供的
      <br/>
      tf.experimental.tensorrt.Converter 将 ONNX 模型转换为 TensorFlow 格式，然后在
      <br/>
      TensorFlow 中加载。
      <pre><code class="prism language-python"><span class="token keyword">import</span> onnx
<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> tf2onnx

<span class="token comment"># 加载 ONNX 模型</span>
onnx_model <span class="token operator">=</span> onnx<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'path/to/onnx/model'</span><span class="token punctuation">)</span>
<span class="token comment"># 将 ONNX 模型转换为 TensorFlow 格式</span>
tf_model_path <span class="token operator">=</span> <span class="token string">'path/to/tf/model'</span>
tf2onnx<span class="token punctuation">.</span>convert<span class="token punctuation">.</span>from_onnx<span class="token punctuation">(</span>onnx_model<span class="token punctuation">)</span><span class="token punctuation">.</span>save_model<span class="token punctuation">(</span>tf_model_path<span class="token punctuation">)</span>
<span class="token comment"># 加载新的 TensorFlow 模型</span>
new_model <span class="token operator">=</span> YourModel<span class="token punctuation">(</span><span class="token punctuation">)</span>
new_model<span class="token punctuation">.</span>load_weights<span class="token punctuation">(</span>tf_model_path<span class="token punctuation">)</span>	
</code></pre>
     </li>
    </ol>
    <h2>
     <a id="5demo_238">
     </a>
     5、调试demo源码
    </h2>
    <p>
     demo源代码请见：
     <a href="https://gitee.com/tipsdark/gpt-2" rel="nofollow">
      Gitee
     </a>
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
</div>


