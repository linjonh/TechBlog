---
layout: post
title: "十大开源LLM大模型"
date: 2025-01-16 02:31:48 +0800
description: "文章浏览阅读1.6k次，点赞17次，本文介绍了2024年即将推出的十大开源大型语言模型，包括LLaM"
keywords: "开源llm模型"
categories: ['']
tags: ['Reactjs', 'Javascript']
artid: "135985084"
image:
    path: https://api.vvhan.com/api/bing?rand=sj&artid=135985084
    alt: "十大开源LLM大模型"
render_with_liquid: false
---

<div class="blog-content-box">
 <div class="article-header-box">
  <div class="article-header">
   <div class="article-title-box">
    <h1 class="title-article" id="articleContentId">
     十大开源LLM大模型
    </h1>
   </div>
  </div>
 </div>
 <article class="baidu_pl">
  <div class="article_content clearfix" id="article_content">
   <link href="../../assets/css/kdoc_html_views-1a98987dfd.css" rel="stylesheet"/>
   <link href="../../assets/css/ck_htmledit_views-704d5b9767.css" rel="stylesheet"/>
   <div class="markdown_views prism-atom-one-light" id="content_views">
    <svg style="display: none;" xmlns="http://www.w3.org/2000/svg">
     <path d="M5,0 0,2.5 5,5z" id="raphael-marker-block" stroke-linecap="round" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     </path>
    </svg>
    <p>
     大型
     <a href="https://www.jdon.com/tag-49140/" rel="nofollow">
      <strong>
       语言模型
      </strong>
     </a>
     （
     <strong>
      LLM）
      <strong>
       对于当前生成
       <a href="https://www.jdon.com/tag-30661/" rel="nofollow">
        <strong>
         人工智能
        </strong>
       </a>
       的革命至关重要。语言模型和解释器是基于 Transformer（一种强大的神经
       <a href="https://www.jdon.com/forum/91/" rel="nofollow">
        <strong>
         架构
        </strong>
       </a>
       ）的人工智能 (AI) 系统。它们被称为
      </strong>
      “大型”
     </strong>
     ，因为它们包含数亿（如果不是数十亿）来自大量文本
     <a href="https://www.jdon.com/tag-49580/" rel="nofollow">
      <strong>
       数据
      </strong>
     </a>
     的预训练参数。
    </p>
    <p>
     在本文中，我们将了解
     <strong>
      2024 年
     </strong>
     将推出的_
     <strong>
      十大开源
      <a href="https://www.jdon.com/tag-49140/" rel="nofollow">
       <strong>
        大模型
       </strong>
      </a>
     </strong>
     _。尽管
     <strong>
      ChatGPT
     </strong>
     和（专有）
     <strong>
      LLM
     </strong>
     只出现了一年，但开源社区已经取得了重大进展，现在有许多开源 LLM 可用于各种应用程序。继续阅读以发现最受欢迎的内容！
    </p>
    <p>
     <strong>
      1.LLaMA 2
     </strong>
     <br/>
     大多数顶级大模型公司都谨慎地开发他们的项目。元脱颖而出。 Meta 提供了有关
     <strong>
      LLaMA 2
     </strong>
     及其强大的开​​源替代方案的重要信息。LLaMA 2 是一个
     <strong>
      7-700 亿参数的
     </strong>
     生成文本模型，于
     <strong>
      2023 年 7 月
     </strong>
     完成。该模型适用于商业和学习。 RLHF 对此进行了改进。构建并训练此文本生成模型以教授聊天机器人自然语言。 Meta 提供开放、可定制的 LLaMA 2、Chat 和 Code Llama。
    </p>
    <p>
     2. BLOOM
     <br/>
     <strong>
      2022 年
     </strong>
     ，Flourish 开发了
     <strong>
      BLOOM，
      <strong>
       这是一种自回归
      </strong>
      大型语言模型 (LLM)
     </strong>
     ，可通过使用大量文本数据扩展提示来生成文本。超过 70 个国家的专家和志愿者在一年内开发了该项目。开源
     <strong>
      LLM BLOOM
     </strong>
     模型包含 1760 亿个参数。它可以流畅、连贯地使用 46 种语言和 13 种编程语言编写。
     <strong>
      BLOOM 的
     </strong>
     执行、评估和改进以及训练数据和源代码都是公开的。 Hugging Face 用户
     <strong>
      免费使用BLOOM 。
     </strong>
    </p>
    <p>
     <strong>
      3.BERT
     </strong>
     <br/>
     LLM 技术依赖于BERT（
     <strong>
      来自 Transformers 的双向编码器表示
     </strong>
     ）神经架构。谷歌研究人员发布了“注意力就是你所需要的”。 2017 年。BERT 是早期的变压器测试。 2018 年
     <strong>
      Google 语言模型 BERT
     </strong>
     作为开源软件提供。它迅速掌握了自然语言处理任务。
     <br/>
     Bert 先进的早期 LLM 开发能力和开源特性使其成为流行的
     <strong>
      语言模型 (LLM)
     </strong>
     。 2020 年，随着 Bert 的推出，Google 搜索将支持
     <strong>
      70 多种语言
     </strong>
     。许多预训练的Bert 模型都是开源的。这些模型有助于检测有害评论、临床记录和情绪。
    </p>
    <p>
     <strong>
      4.Falcon 180B
     </strong>
     <br/>
     新的
     <strong>
      Falcon 180B
     </strong>
     表明专有和开源大语言模型之间的差异正在快速缩小，如果 Falcon 40B 在 Hugging Face 的大语言模型记分牌上排名第一，还没有给开源 LLM 社区留下深刻的印象的话。 Falcon 180B由
     <strong>
      阿联酋
     </strong>
     技术创新研究所于9月20日至23日推出，正在使用
     <strong>
      3.5万亿
     </strong>
     代币和
     <strong>
      1800亿个参数
     </strong>
     进行训练。 Hugging Face表示，鉴于其惊人的处理能力，Falcon 180B可以与
     <strong>
      Google的PaLM 2
     </strong>
     （运行Google Bard的LLM）竞争。 Falcon 180B在一些 NLP 任务上已经超越了
     <strong>
      LLaMA 2
     </strong>
     和GPT-3.5 。
    </p>
    <p>
     重要的是要记住，Falcon 180B 需要强大的处理能力才能运行，同时可以免费用于商业和研究环境。
    </p>
    <p>
     <strong>
      5.OPT-175B
     </strong>
     <br/>
     2022 年，Meta 发布了开放式预训练
     <strong>
      Transformers 语言模型 (TLM
     </strong>
     )，实现了一个重要的里程碑，这是他们利用开源来解放 LLM 竞赛的目标的一部分。 OPT 由一组预训练的 Transformer 组成，仅解码器，参数范围从 1
     <strong>
      25M 到 175B
     </strong>
     。最强大的兄弟是
     <strong>
      OPT-175B
     </strong>
     ，这是一种开源 LLM，是市场上最复杂的，其
     <a href="https://www.jdon.com/tag-300/" rel="nofollow">
      <strong>
       性能
      </strong>
     </a>
     与 GPT-3 类似。公众可以访问源代码和预训练模型。但是，如果您计划与大模型建立人工智能驱动的业务，您最好考虑另一种选择，因为 OPT-175B 仅在允许该模型用于研究用例的非商业许可下可用。
    </p>
    <p>
     <strong>
      6.XGen-7B
     </strong>
     <br/>
     越来越多的企业参加 LLM 竞赛。 Salesforce 是最新进入该市场的公司之一，于 2023 年 7 月发布了 XGen-7B LLM。作者声称，大多数开源
     <strong>
      LLM
     </strong>
     专注于提供冗长的答复，但细节很少（即，简短的提示，
     <a href="https://www.jdon.com/69062.html" rel="nofollow">
      <strong>
       上下文
      </strong>
     </a>
     很少） ）。
     <strong>
      XGen-7B
     </strong>
     试图创建一个可以处理更大上下文窗口的工具。具体来说，XGen 最复杂的变体 (XGen-7B-8K-base) 支持 8K 上下文窗口，即输入和输出中的全部文本量。
    </p>
    <p>
     <strong>
      虽然 XGen 仅利用 7B 个参数进行训练（比LLaMA 2
     </strong>
     或 Falcon 等最强大的开源 LLM 少得多），但效率是另一个首要目标。尽管 XGen 尺寸很小，但它仍然可以产生出色的结果。除了
     <strong>
      XGen-7 B-{4K,8K}
     </strong>
     -inst 版本（使用教学数据和 RLHF 进行训练并在非商业许可下提供）之外，该模型可用于商业和研究用途。
    </p>
    <p>
     <strong>
      7.GPT-NeoX 和 GPT-NeoX
     </strong>
     <br/>
     <strong>
      GPT-NeoX 和 GPT-J
     </strong>
     由非营利性人工智能研究中心 EleutherAI 的科学家开发，是 GPT 的两个优秀的开源替代品。
     <strong>
      GPT-NeoX
     </strong>
     中有 200 亿个参数，GPT-J 中有 60 亿个参数。尽管大多数高级大模型可以使用超过 1000 亿个参数进行训练，但这两个大模型能够得出高精度的研究结果。它们可以用于许多不同的领域和应用场景，因为它们是在来自各种来源的 22 个高质量数据集上进行训练的。与 GPT-3 相比，
     <strong>
      GPT-NeoX 和 GPT-J
     </strong>
     尚未使用 RLHF 进行训练。
    </p>
    <p>
     <strong>
      GPT-NeoX 和 GPT-J
     </strong>
     可用于任何自然语言处理活动，包括研究、营销活动规划、情感分析和文本生成。通过 NLP Cloud
     <a href="https://www.jdon.com/tag-33426/" rel="nofollow">
      <strong>
       API
      </strong>
     </a>
     ，您可以免费获得两个大模型。
    </p>
    <p>
     <strong>
      8. Vicuna 13-B
     </strong>
     <br/>
     <strong>
      使用从ShareGPT
     </strong>
     收集的用户共享对话，LLaMa 13B 模型经过改进，创建了开源对话模型 Vicuna-13B。 Vicuna-13B 是一款具有多种用途的智能聊天机器人；下面列出了各个行业的一些例子，包括客户服务、医疗保健、教育、金融和旅行/酒店。根据使用 GPT-4 作为判断的初步评估，Vicuna-13B在 90% 以上的情况下超越了 LLaMa 和 Alpaca 等其他模型，达到了ChatGPT和
     <strong>
      Google Bar
     </strong>
     rd 90% 以上的质量。
    </p>
    <p>
     <strong>
      9. YI 34B
     </strong>
     <br/>
     <strong>
      YI 34B
     </strong>
     中国01 AI开发了一种新的语言模型，名为Yi 34B。目前，该模型在 Hugging Face Open LLM 排行榜上名列前茅。该公司的目标是开发能够说中文和英文的双语模型。与原始的 4K 令牌上下文窗口相比，该模型现在可以使用多达 32K 令牌进行训练。
    </p>
    <p>
     令人印象深刻的是，该公司最近发布了 34B 模型的 200,000 代币版本。这些模型可以被许可用于商业用途，并且可用于研究目的。 34B 模型拥有 3 万亿代币，在算术和编码方面表现出色。该公司已提供监督微调对话模型和基本模型的基准。该模型有多个 4 位和 8 位版本可用。
    </p>
    <p>
     <strong>
      10.Mistral ​​​​​​​8x7B
     </strong>
     <br/>
     <strong>
      Mistral AI
     </strong>
     于 2023 年 12 月推出了 Mixtral 8X 7B 基础模型。采用开放权重，它是一种优秀的专家模型稀疏混合 (SMoE)，在许多参数较小的基准测试中，其性能优于 Llama 2 和 GPT 3.5。
    </p>
    <p>
     它是在 Apache 2.0 下获得许可的；因此，您如何改变它并从中获利并没有太多限制。Mistral是一个仅解码器模型，是一个稀疏的专家混合网络。前馈块从八个参数组中选择。路由器网络在每一层为每个令牌选择其中两个组（“专家”），以便处理它并贡献其输出。此方法减少了延迟和成本，同时增加了模型参数的数量。 Mistral 每个代币仅使用 467 亿个参数中的 129 亿个参数。这表明它使用与 12.9B 模型相同数量的资源并以相同的速率处理输入和输出。
    </p>
    <p>
     Mixtral 8x7B 目前被Hugging Face Open LLM 排行榜评为市场上排名前 10 的大模型，平均得分为 68.42、ARC 66.04、
     <strong>
      HellaSwag 86.49、
     </strong>
     <strong>
      MMLU
     </strong>
     71.82和 TruthfulQA 46.78。在大多数基准测试中，它的性能优于 GPT 3.5 和 Llama 2；但是，不如 GPT 4。在大多数基准测试中，它的推理速度比Llama 2 70B 快 6 倍。具体来说，它在大多数常见基准测试中均达到或超过 GPT3.5。
    </p>
    <p>
     Mixtral 在 BBQ 基准上的偏差比 Llama 2 少。Mixtral 比 Llama 2 具有更大胆的积极情绪，每个维度的变异性相似。Mixtral 在几乎所有方面都优于
     <strong>
      GPT 3.5
     </strong>
     ，除了 Mt Bench 分数。 Mixtral 在科学和代码生产方面表现良好。它可能会被改进成为一个MT-Bench分数为8.3的指令跟踪模型。 Mixtral 还支持多种语言，可以使用**英语、法语、意大利语、德语和西班牙语，**这使其成为适合各种应用程序和用户的灵活模型。 Mistral AI 始终致力于扩展模型的语言能力。
    </p>
    <p>
     <strong>
      如何选择合适的开源LLM？
     </strong>
     <br/>
     选择正确的开源**LLM（大型语言模型）**涉及根据您的特定
     <a href="https://www.jdon.com/colorUML.html" rel="nofollow">
      <strong>
       需求
      </strong>
     </a>
     和偏好考虑各种因素。这取决于您使用大模型的情况和目的。不同的模型可能在不同的应用中表现出色。此外，模型架构和模型大小可能会提供更好的性能。您可以选择适合您的项目要求的大模型。
    </p>
   </div>
   <link href="../../assets/css/markdown_views-a5d25dd831.css" rel="stylesheet"/>
   <link href="../../assets/css/style-e504d6a974.css" rel="stylesheet"/>
  </div>
 </article>
 <p alt="68747470733a:2f2f626c6f672e6373646e2e6e65742f6366795f62616e712f:61727469636c652f64657461696c732f313335393835303834" class_="artid" style="display:none">
 </p>
</div>


